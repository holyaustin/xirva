[{"id": "1708.00234", "submitter": "Mateusz {\\L}\\k{a}cki", "authors": "Mateusz Krzysztof {\\L}\\k{a}cki, Frederik Lermyte, B{\\l}a\\.zej\n  Miasojedow, Miko{\\l}aj Olsza\\'nski, Micha{\\l} Startek, Frank Sobott, Dirk\n  Valkenborg, Anna Gambin", "title": "Assigning peaks and modeling ETD in top-down mass spectrometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many techniques of modern mass spectrometry, the top down methods are\nbecoming continuously more popular in the overall strive to describe the\nproteome. These techniques are based on fragmentation of ions inside mass\nspectrometers instead of being proteolytically digested. In some of these\ntechniques, the fragmentation is induced by electron transfer. It can trigger\nseveral concurring reactions: electron transfer dissociation, electron transfer\nwithout dissociation, and proton transfer reaction. The evaluation of the\nextent of these reactions is important for the proper understanding of the\nfunctioning of the instrument and, what is even more important, to know if it\ncan be used to reveal important structural information. We present a workflow\nfor assigning peaks and interpreting the results of electron transfer driven\nreactions. We also present software written in Python and available under GNU\nv3 license.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 10:32:53 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 09:34:56 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 21:03:08 GMT"}, {"version": "v4", "created": "Fri, 25 Aug 2017 09:56:37 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["\u0141\u0105cki", "Mateusz Krzysztof", ""], ["Lermyte", "Frederik", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Olsza\u0144ski", "Miko\u0142aj", ""], ["Startek", "Micha\u0142", ""], ["Sobott", "Frank", ""], ["Valkenborg", "Dirk", ""], ["Gambin", "Anna", ""]]}, {"id": "1708.00348", "submitter": "Hannah Worthington", "authors": "Hannah Worthington, Rachel S. McCrea, Ruth King, Richard A. Griffiths", "title": "Estimation of population size when capture probability depends on\n  individual states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multi-state model to estimate the size of a closed population\nfrom ecological capture-recapture studies. We consider the case where\ncapture-recapture data are not of a simple binary form, but where the state of\nan individual is also recorded upon every capture as a discrete variable. The\nproposed multi-state model can be regarded as a generalisation of the commonly\napplied set of closed population models to a multi-state form. The model\npermits individuals to move between the different discrete states, whilst\nallowing heterogeneity within the capture probabilities. A closed-form\nexpression for the likelihood is presented in terms of a set of sufficient\nstatistics. The link between existing models for capture heterogeneity are\nestablished, and simulation is used to show that the estimate of population\nsize can be biased when movement between states is not accounted for. The\nproposed unconditional approach is also compared to a conditional approach to\nassess estimation bias. The model derived in this paper is motivated by a real\necological data set on great crested newts, Triturus cristatus.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:14:27 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Worthington", "Hannah", ""], ["McCrea", "Rachel S.", ""], ["King", "Ruth", ""], ["Griffiths", "Richard A.", ""]]}, {"id": "1708.00645", "submitter": "Aurelien Hazan", "authors": "Aur\\'elien Hazan (UPEC UP12)", "title": "Stock-flow consistent macroeconomic model with nonuniform distributional\n  constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on results concerning a partially aggregated Stock Flow Consistent\n(SFC) macroeconomic model in the stationary state where the sectors of banks\nand firms are aggregated, the sector of households is dis-aggregated, and the\nprobability density function (pdf) of the wealth of households is exogenous,\nconstrained by econometric data. It is shown that the equality part of the\nconstraint can be reduced to a single constant-sum equation, which relates this\nproblem to the study of continuous mass transport problems, and to the sum of\niid random variables. Existing results can thus be applied, and provide\nmarginal probabilities, and the location of the critical point before\ncondensation occurs. Various numerical experiments are performed using Monte\nCarlo sampling of the hit-and-run type, using wealth and income data for\nFrance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:32:35 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Hazan", "Aur\u00e9lien", "", "UPEC UP12"]]}, {"id": "1708.00656", "submitter": "David Dekker", "authors": "David Dekker, David Krackhardt and Tom A.B. Snijders", "title": "Transitivity Correlation: Measuring Network Transitivity as Comparative\n  Quantity", "comments": "27 pages, 2 appendices, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes that common measures for network transitivity, based on\nthe enumeration of transitive triples, do not reflect the theoretical\nstatements about transitivity they aim to describe. These statements are often\nformulated as comparative conditional probabilities, but these are not directly\nreflected by simple functions of enumerations. We think that a better approach\nis obtained by considering the linear regression coefficient of ties\n$i\\,\\rightarrow\\,j$ on the number of two-paths\n$i\\,\\rightarrow\\,k\\rightarrow\\,j$ for the $(n-2)$ possible intermediate nodes\n$k$. Two measures of transitivity based on correlation coefficients between the\nexistence of a tie and the existence, or the number, of two-paths are\ndeveloped, and called \"Transitivity Phi\" and \"Transitivity Correlation\". Some\ndesirable properties for these measures are studied and compared to existing\nclustering coefficients, in both random (Erd\\\"os-Renyi) and in stylized\nnetworks (windmills). Furthermore, it is shown that under the condition of zero\nTransitivity Correlation, the total number of transitive triples is determined\nby four underlying features of any directed graph: size, density, reciprocity,\nand the covariance between indegrees and outdegrees. Also, it is demonstrated\nthat plotting conditional probability of ties, given the number of two-paths,\nprovides valuable insights into empirical regularities and irregularities of\ntransitivity patterns.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 09:03:04 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Dekker", "David", ""], ["Krackhardt", "David", ""], ["Snijders", "Tom A. B.", ""]]}, {"id": "1708.00842", "submitter": "Murat Uney Dr", "authors": "Murat Uney, Bernard Mulgrew, Daniel E Clark", "title": "Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods", "comments": "accepted with minor revisions, IEEE Transactions on Signal and\n  Information Processing Over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT cs.MA math.IT stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:34:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 22:45:52 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Uney", "Murat", ""], ["Mulgrew", "Bernard", ""], ["Clark", "Daniel E", ""]]}, {"id": "1708.00907", "submitter": "Adam Sales", "authors": "Adam C. Sales", "title": "Sequential Specification Tests to Choose a Model: A Change-Point\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers faced with a sequence of candidate model specifications must\noften choose the best specification that does not violate a testable\nidentification assumption. One option in this scenario is sequential\nspecification tests: hypothesis tests of the identification assumption over the\nsequence. Borrowing an idea from the change-point literature, this paper shows\nhow to use the distribution of p-values from sequential specification tests to\nestimate the point in the sequence where the identification assumption ceases\nto hold. Unlike current approaches, this method is robust to individual errant\np-values and does not require choosing a test level or tuning parameter. This\npaper demonstrates the method's properties with a simulation study, and\nillustrates it by application to the problems of choosing a bandwidth in a\nregression discontinuity design while maintaining covariate balance and of\nchoosing a lag order for a time series model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 19:47:54 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sales", "Adam C.", ""]]}, {"id": "1708.01229", "submitter": "Edward Wu", "authors": "Edward Wu, Johann Gagnon-Bartsch", "title": "The LOOP Estimator: Adjusting for Covariates in Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting a randomized controlled trial, it is common to specify in\nadvance the statistical analyses that will be used to analyze the data.\nTypically these analyses will involve adjusting for small imbalances in\nbaseline covariates. However, this poses a dilemma, since adjusting for too\nmany covariates can hurt precision more than it helps, and it is often unclear\nwhich covariates are predictive of outcome prior to conducting the experiment.\nFor example, both post-stratification and OLS regression adjustments can\nactually increase variance (relative to a simple difference in means) if too\nmany covariates are used. OLS is also biased under the Neyman-Rubin model. In\nthis paper, we introduce the LOOP (\"Leave-One-Out Potential outcomes\")\nestimator of the average treatment effect. We leave out each observation and\nthen impute that observation's treatment and control potential outcomes using a\nprediction algorithm, such as a random forest. This estimator is unbiased under\nthe Neyman-Rubin model, generally performs at least as well as the unadjusted\nestimator, and the experimental randomization largely justifies the statistical\nassumptions made. Importantly, the LOOP estimator also enables us to take\nadvantage of automatic variable selection when using random forests.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:20:52 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wu", "Edward", ""], ["Gagnon-Bartsch", "Johann", ""]]}, {"id": "1708.01481", "submitter": "Daniel Eck", "authors": "Daniel J. Eck, Christopher J. Nachtsheim, R. Dennis Cook, and Thomas\n  A. Albrecht", "title": "Multivariate Design of Experiments for Engineering Dimensional Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of dimensional analysis experiments when there is more\nthan a single response. We first give a brief overview of dimensional analysis\nexperiments and the dimensional analysis (DA) procedure. The validity of the DA\nmethod for univariate responses was established by the Buckingham $\\Pi$-Theorem\nin the early 20th century. We extend the theorem to the multivariate case,\ndevelop basic criteria for multivariate design of DA and give guidelines for\ndesign construction. Finally, we illustrate the construction of designs for DA\nexperiments for an example involving the design of a heat exchanger.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 13:09:31 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 20:38:34 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Eck", "Daniel J.", ""], ["Nachtsheim", "Christopher J.", ""], ["Cook", "R. Dennis", ""], ["Albrecht", "Thomas A.", ""]]}, {"id": "1708.01634", "submitter": "Jianhua Shi", "authors": "Huijuan Ma, Jianhua Shi, Yong Zhou", "title": "Proportional Mean Residual Life Model with Censored Survival Data under\n  Case-cohort Design", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional mean residual life model is studied for analysing survival data\nfrom the case-cohort design. To simultaneously estimate the regression\nparameters and the baseline mean residual life function, weighted estimating\nequations based on an inverse selection probability are proposed. The resulting\nregression coefficients estimates are shown to be consistent and asymptotic\nnormal with easily estimated variance-covariance. Simulation studies show that\nthe proposed estimators perform very well. An application to a real dataset\nfrom the South Welsh nickel refiners study is also given to illustrate the\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:11:52 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 12:52:43 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Ma", "Huijuan", ""], ["Shi", "Jianhua", ""], ["Zhou", "Yong", ""]]}, {"id": "1708.01767", "submitter": "RadhaKrishna Ganti", "authors": "Aroon Narayanan, Sreejith T. V and Radha Krishna Ganti", "title": "Coverage Analysis in Millimeter Wave Cellular Networks with Reflections", "comments": "Accepted for presentation in Globecom 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.SI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coverage probability of a user in a mmwave system depends on the\navailability of line-of-sight paths or reflected paths from any base station.\nMany prior works modelled blockages using random shape theory and analyzed the\nSIR distribution with and without interference. While, it is intuitive that the\nreflected paths do not significantly contribute to the coverage (because of\nlonger path lengths), there are no works which provide a model and study the\ncoverage with reflections. In this paper, we model and analyze the impact of\nreflectors using stochastic geometry. We observe that the reflectors have very\nlittle impact on the coverage probability.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:24:16 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Narayanan", "Aroon", ""], ["T.", "Sreejith", "V"], ["Ganti", "Radha Krishna", ""]]}, {"id": "1708.01772", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov, Tianchi Chen, Jacob Jaffe, Edoardo Airoldi, Steven\n  Carr, Bogdan Budnik and Nikolai Slavov", "title": "Quantifying homologous proteins and proteoforms", "comments": null, "journal-ref": "Molecular & Cellular Proteomics, 2018", "doi": "10.1074/mcp.TIR118.000947", "report-no": "mcp.TIR118.000947", "categories": "q-bio.QM q-bio.GN stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many proteoforms - arising from alternative splicing, post-translational\nmodifications (PTMs), or paralogous genes - have distinct biological functions,\nsuch as histone PTM proteoforms. However, their quantification by existing\nbottom-up mass-spectrometry (MS) methods is undermined by peptide-specific\nbiases. To avoid these biases, we developed and implemented a first-principles\nmodel (HIquant) for quantifying proteoform stoichiometries. We characterized\nwhen MS data allow inferring proteoform stoichiometries by HIquant, derived an\nalgorithm for optimal inference, and demonstrated experimentally high accuracy\nin quantifying fractional PTM occupancy without using external standards, even\nin the challenging case of the histone modification code.\n  HIquant server is implemented at:\nhttps://web.northeastern.edu/slavov/2014_HIquant/\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:52:12 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Chen", "Tianchi", ""], ["Jaffe", "Jacob", ""], ["Airoldi", "Edoardo", ""], ["Carr", "Steven", ""], ["Budnik", "Bogdan", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1708.01948", "submitter": "Shijing Yao", "authors": "Shijing Yao, Yueqing Wang, Bin Yu", "title": "Efficient Aerosol Retrieval for Multi-angle Imaging SpectroRadiometer\n  (MISR): A Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in Aerosol Optical Depth (AOD) retrieval algorithms for\nMulti-angle Imaging SpectroRadiometer (MISR) proposed a hierarchical Bayesian\nmodel. However the inference algorithm used in their work was Markov Chain\nMonte Carlo (MCMC), which was reported prohibitively slow. The poor speed of\nMCMC dramatically limited the production feasibility of the Bayesian framework\nif large scale (e.g. global scale) of aerosol retrieval is desired.\n  In this paper, we present an alternative optimization method to mitigate the\nspeed problem. In particular we adopt Maximize a Posteriori (MAP) approach, and\napply a gradient-free \"hill-climbing\" algorithm: the coordinate-wise\nstochastic-search. Our method has shown to be much (about 100 times) faster\nthan MCMC, easier to converge, and insensitive to hyper parameters. To further\nscale our approach, we parallelized our method using Apache Spark, which\nachieves linear speed-up w.r.t number of CPU cores up to 16. Due to these\nefforts, we are able to retrieve AOD at much finer resolution (1.1km) with a\ntiny fraction of time consumption compared with existing methods.\n  During our research, we find that in low AOD levels, the Bayesian network\ntends to produce overestimated retrievals. We also find that high absorbing\naerosol types are retrieved at the same time. This is likely caused by the\nDirichlet prior for aerosol types, as it is shown to encourage selecting\nabsorbing types in practice. After changing Dirichlet to uniform, the AOD\nretrievals show excellent agreement with ground measurement in all levels.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:52:49 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Yao", "Shijing", ""], ["Wang", "Yueqing", ""], ["Yu", "Bin", ""]]}, {"id": "1708.02123", "submitter": "Joshua Lukemire", "authors": "Joshua Lukemire, Suprateek Kundu, Giuseppe Pagnoni, and Ying Guo", "title": "Bayesian Joint Modeling of Multiple Brain Functional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain function is organized in coordinated modes of spatio-temporal activity\n(functional networks) exhibiting an intrinsic baseline structure with\nvariations under different experimental conditions. Existing approaches for\nuncovering such network structures typically do not explicitly model shared and\ndifferential patterns across networks, thus potentially reducing the detection\npower. We develop an integrative modeling approach for jointly modeling\nmultiple brain networks across experimental conditions. The proposed Bayesian\nJoint Network Learning approach develops flexible priors on the edge\nprobabilities involving a common intrinsic baseline structure and differential\neffects specific to individual networks. Conditional on these edge\nprobabilities, connection strengths are modeled under a Bayesian spike and slab\nprior on the off-diagonal elements of the inverse covariance matrix. The model\nis fit under a posterior computation scheme based on Markov chain Monte Carlo.\nNumerical simulations illustrate that the proposed joint modeling approach has\nincreased power to detect true differential edges while providing adequate\ncontrol on false positives and achieving greater accuracy in the estimation of\nedge strengths compared to existing methods. An application of the method to\nfMRI Stroop task data provides unique insights into brain network alterations\nbetween cognitive conditions which existing graphical modeling techniques\nfailed to reveal.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:02:25 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 18:56:02 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Lukemire", "Joshua", ""], ["Kundu", "Suprateek", ""], ["Pagnoni", "Giuseppe", ""], ["Guo", "Ying", ""]]}, {"id": "1708.02158", "submitter": "Stephan Huckemann", "authors": "Christina Imdahl, Carsten Gottschlich, Stephan Huckemann, Ken'ichi\n  Ohshika", "title": "M\\\"obius Moduli for Fingerprint Orientation Fields", "comments": "16 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fingerprint descriptor, namely M\\\"obius moduli, measuring\nlocal deviation of orientation fields (OF) of fingerprints from conformal\nfields, and we propose a method to robustly measure them, based on\ntetraquadrilaterals to approximate a conformal modulus locally with one due to\na M\\\"obius transformation. Conformal fields arise by the approximation of\nfingerprint OFs given by zero pole models, which are determined by the singular\npoints and a rotation. This approximation is very coarse, e.g. for fingerprints\nwith no singular points (arch type), the zero-pole model's OF has parallel\nlines. Quadratic differential (QD) models, which are obtained from zero-pole\nmodels by adding suitable singularities outside the observation window,\napproximate real fingerprints much better. For example, for arch type\nfingerprints, parallel lines along the distal joint change slowly into circular\nlines around the nail furrow. Still, QD models are not fully realistic because,\nfor example along the central axis of arch type fingerprints, ridge line\ncurvatures usually first increase and then decrease again. It is impossible to\nmodel this with QDs, which, due to complex analyticity, also produce conformal\nfields only. In fact, as one of many applications of the new descriptor, we\nshow, using histograms of curvature and conformality index (log of the absolute\nvalue of the M\\\"obius modulus), that local deviation from conformality in\nfingerprints occurs systematically at high curvature which is not reflected by\nstate of the art fingerprint models as are used, for instance, in the well\nknown synthetic fingerprint generation tool SFinGe and these differences\nrobustely discriminate real prints from SFinGe's synthetic prints.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:19:19 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Imdahl", "Christina", ""], ["Gottschlich", "Carsten", ""], ["Huckemann", "Stephan", ""], ["Ohshika", "Ken'ichi", ""]]}, {"id": "1708.02184", "submitter": "Dmitrii Zholud", "authors": "Holger Rootz\\'en and Dmitrii Zholud", "title": "Human life is unlimited - but short", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does the human lifespan have an impenetrable biological upper limit which\nultimately will stop further increase in life lengths? This question is\nimportant for understanding aging, and for society, and has led to intense\ncontroversies. Demographic data for humans has been interpreted as showing\nexistence of a limit, or even as an indication of a decreasing limit, but also\nas evidence that a limit does not exist. This paper studies what can be\ninferred from data about human mortality at extreme age. We show that in\nwestern countries and Japan and after age 110 the probability of dying is about\n47% per year. Hence there is no finite upper limit to the human lifespan.\nStill, given the present stage of biotechnology, it is unlikely that during the\nnext 25 years anyone will live longer than 128 years in these countries. Data,\nremarkably, shows no difference in mortality after age 110 between sexes,\nbetween ages, or between different lifestyles or genetic backgrounds. These\nresults, and the analysis methods developed in this paper, can help testing\nbiological theories of ageing and aid confirmation of success of efforts to\nfind a cure for ageing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:12:57 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rootz\u00e9n", "Holger", ""], ["Zholud", "Dmitrii", ""]]}, {"id": "1708.02196", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Huimin Chen, Shudong Sun and Juan M Corchado", "title": "Joint Smoothing, Tracking, and Forecasting Based on Continuous-Time\n  Target Trajectory Fitting", "comments": "16 pages, 8 figures, 5 tables, 80 references; Codes available", "journal-ref": "IEEE Transactions on Automation Science and Engineering, Volume:\n  16, Issue: 3, July 2019,Pages: 1476 - 1483", "doi": "10.1109/TASE.2018.2882641", "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a continuous time state estimation framework that unifies\ntraditionally individual tasks of smoothing, tracking, and forecasting (STF),\nfor a class of targets subject to smooth motion processes, e.g., the target\nmoves with nearly constant acceleration or affected by insignificant noises.\nFundamentally different from the conventional Markov transition formulation,\nthe state process is modeled by a continuous trajectory function of time (FoT)\nand the STF problem is formulated as an online data fitting problem with the\ngoal of finding the trajectory FoT that best fits the observations in a sliding\ntime-window. Then, the state of the target, whether the past (namely,\nsmoothing), the current (filtering) or the near-future (forecasting), can be\ninferred from the FoT. Our framework releases stringent statistical modeling of\nthe target motion in real time, and is applicable to a broad range of real\nworld targets of significance such as passenger aircraft and ships which move\non scheduled, (segmented) smooth paths but little statistical knowledge is\ngiven about their real time movement and even about the sensors. In addition,\nthe proposed STF framework inherits the advantages of data fitting for\naccommodating arbitrary sensor revisit time, target maneuvering and missed\ndetection. The proposed method is compared with state of the art estimators in\nscenarios of either maneuvering or non-maneuvering target.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:42:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Tiancheng", ""], ["Chen", "Huimin", ""], ["Sun", "Shudong", ""], ["Corchado", "Juan M", ""]]}, {"id": "1708.02230", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Paulina A. Rowi\\'nska", "title": "Delayed acceptance ABC-SMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is now an established technique for\nstatistical inference used in cases where the likelihood function is\ncomputationally expensive or not available. It relies on the use of a~model\nthat is specified in the form of a~simulator, and approximates the likelihood\nat a~parameter value $\\theta$ by simulating auxiliary data sets $x$ and\nevaluating the distance of $x$ from the true data $y$. However, ABC is not\ncomputationally feasible in cases where using the simulator for each $\\theta$\nis very expensive. This paper investigates this situation in cases where\na~cheap, but approximate, simulator is available. The approach is to employ\ndelayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential\nMonte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the\ncheap simulator to rule out parts of the parameter space that are not worth\nexploring, so that the ``true'' simulator is only run (in the second stage of\nthe kernel) where there is a~reasonable chance of accepting proposed values of\n$\\theta$. We show that this approach can be used quite automatically, with few\ntuning parameters. Applications to stochastic differential equation models and\nlatent doubly intractable distributions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:52:04 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 14:56:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G.", ""], ["Rowi\u0144ska", "Paulina A.", ""]]}, {"id": "1708.02486", "submitter": "Carlo Lancia PhD", "authors": "Carlo Lancia and Guglielmo Lulli", "title": "Data-driven modelling and validation of aircraft inbound-stream at some\n  major European airports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an exhaustive study on the arrivals process at eight\nimportant European airports. Using inbound traffic data, we define, compare,\nand contrast a data-driven Poisson and PSRA point process. Although, there is\nsufficient evidence that the interarrivals might follow an exponential\ndistribution, this finding does not directly translate to evidence that the\narrivals stream is Poisson. The main reason is that finite-capacity constraints\nimpose a correlation structure to the arrivals stream, which a Poisson model\ncannot capture. We show the weaknesses and somehow the difficulties of using a\nPoisson process to model with good approximation the arrivals stream. On the\nother hand, our innovative non-parametric, data-driven PSRA model, predicts\nquite well and captures important properties of the typical arrivals stream.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 13:37:52 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lancia", "Carlo", ""], ["Lulli", "Guglielmo", ""]]}, {"id": "1708.02635", "submitter": "Doyup Lee", "authors": "Doyup Lee", "title": "Anomaly Detection in Multivariate Non-stationary Time Series for\n  Automatic DBMS Diagnosis", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-126", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in database management systems (DBMSs) is difficult because\nof increasing number of statistics (stat) and event metrics in big data system.\nIn this paper, I propose an automatic DBMS diagnosis system that detects\nanomaly periods with abnormal DB stat metrics and finds causal events in the\nperiods. Reconstruction error from deep autoencoder and statistical process\ncontrol approach are applied to detect time period with anomalies. Related\nevents are found using time series similarity measures between events and\nabnormal stat metrics. After training deep autoencoder with DBMS metric data,\nefficacy of anomaly detection is investigated from other DBMSs containing\nanomalies. Experiment results show effectiveness of proposed model, especially,\nbatch temporal normalization layer. Proposed model is used for publishing\nautomatic DBMS diagnosis reports in order to determine DBMS configuration and\nSQL tuning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:04:19 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 23:54:22 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Lee", "Doyup", ""]]}, {"id": "1708.02703", "submitter": "Faranak Golestaneh", "authors": "Faranak Golestaneh, Pierre Pinson, Rasoul Azizipanah-Abarghooee and\n  Hoay Beng Gooi", "title": "Ellipsoidal Prediction Regions for Multivariate Uncertainty\n  Characterization", "comments": "8 pages, 7 Figures, Submitted to IEEE Transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While substantial advances are observed in probabilistic forecasting for\npower system operation and electricity market applications, most approaches are\nstill developed in a univariate framework. This prevents from informing about\nthe interdependence structure among locations, lead times and variables of\ninterest. Such dependencies are key in a large share of operational problems\ninvolving renewable power generation, load and electricity prices for instance.\nThe few methods that account for dependencies translate to sampling scenarios\nbased on given marginals and dependence structures. However, for classes of\ndecision-making problems based on robust, interval chance-constrained\noptimization, necessary inputs take the form of polyhedra or ellipsoids.\nConsequently, we propose a systematic framework to readily generate and\nevaluate ellipsoidal prediction regions, with predefined probability and\nminimum volume. A skill score is proposed for quantitative assessment of the\nquality of prediction ellipsoids. A set of experiments is used to illustrate\nthe discrimination ability of the proposed scoring rule for misspecification of\nellipsoidal prediction regions. Application results based on three datasets\nwith wind, PV power and electricity prices, allow us to assess the skill of the\nresulting ellipsoidal prediction regions, in terms of calibration, sharpness\nand overall skill.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 03:25:33 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Golestaneh", "Faranak", ""], ["Pinson", "Pierre", ""], ["Azizipanah-Abarghooee", "Rasoul", ""], ["Gooi", "Hoay Beng", ""]]}, {"id": "1708.03018", "submitter": "Samuel W.K. Wong", "authors": "Samuel W.K. Wong and James V. Zidek", "title": "Dimensional and statistical foundations for accumulated damage models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a framework for creating damage accumulation models for\nengineered wood products by invoking the classical theory of\nnon--dimensionalization. The result is a general class of such models. Both the\nUS and Canadian damage accumulation models are revisited. It is shown how the\nformer may be generalized within that framework while deficiencies are\ndiscovered in the latter and overcome. Use of modern Bayesian statistical\nmethods for estimating the parameters in these models is proposed along with an\nillustrative application of these methods to a ramp load dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 21:19:37 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Wong", "Samuel W. K.", ""], ["Zidek", "James V.", ""]]}, {"id": "1708.03144", "submitter": "Shantanu Desai", "authors": "Anumandla Sukrutha, Sristi Ram Dyuthi, Shantanu Desai", "title": "Multimodel Response Assessment for Monthly Rainfall Distribution in Some\n  Selected Indian Cities Using Best Fit Probability as a Tool", "comments": "14 pages, 5 figures", "journal-ref": "Applied Water Science 8:145 (2018)", "doi": "10.1007/s13201-018-0789-4", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carry out a study of the statistical distribution of rainfall\nprecipitation data for 20 cites in India. We have determined the best-fit\nprobability distribution for these cities from the monthly precipitation data\nspanning 100 years of observations from 1901 to 2002. To fit the observed data,\nwe considered 10 different distributions. The efficacy of the fits for these\ndistributions was evaluated using four empirical non-parametric goodness-of-fit\ntests namely Kolmogorov-Smirnov, Anderson-Darling, Chi-Square, Akaike\ninformation criterion, and Bayesian Information criterion. Finally, the\nbest-fit distribution using each of these tests were reported, by combining the\nresults from the model comparison tests. We then find that for most of the\ncities, Generalized Extreme-Value Distribution or Inverse Gaussian Distribution\nmost adequately fits the observed data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:50:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 05:17:28 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 18:37:13 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Sukrutha", "Anumandla", ""], ["Dyuthi", "Sristi Ram", ""], ["Desai", "Shantanu", ""]]}, {"id": "1708.03156", "submitter": "Rapha\\\"el Huser", "authors": "Luigi Lombardo, Thomas Opitz and Raphael Huser", "title": "Point process-based modeling of multiple debris flow landslides using\n  INLA: an application to the 2009 Messina disaster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a stochastic modeling approach based on spatial point processes of\nlog-Gaussian Cox type for a collection of around 5000 landslide events provoked\nby a precipitation trigger in Sicily, Italy. Through the embedding into a\nhierarchical Bayesian estimation framework, we can use the Integrated Nested\nLaplace Approximation methodology to make inference and obtain the posterior\nestimates. Several mapping units are useful to partition a given study area in\nlandslide prediction studies. These units hierarchically subdivide the\ngeographic space from the highest grid-based resolution to the stronger\nmorphodynamic-oriented slope units. Here we integrate both mapping units into a\nsingle hierarchical model, by treating the landslide triggering locations as a\nrandom point pattern. This approach diverges fundamentally from the unanimously\nused presence-absence structure for areal units since we focus on modeling the\nexpected landslide count jointly within the two mapping units. Predicting this\nlandslide intensity provides more detailed and complete information as compared\nto the classically used susceptibility mapping approach based on relative\nprobabilities. To illustrate the model's versatility, we compute absolute\nprobability maps of landslide occurrences and check its predictive power over\nspace. While the landslide community typically produces spatial predictive\nmodels for landslides only in the sense that covariates are spatially\ndistributed, no actual spatial dependence has been explicitly integrated so far\nfor landslide susceptibility. Our novel approach features a spatial latent\neffect defined at the slope unit level, allowing us to assess the spatial\ninfluence that remains unexplained by the covariates in the model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 10:31:23 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Lombardo", "Luigi", ""], ["Opitz", "Thomas", ""], ["Huser", "Raphael", ""]]}, {"id": "1708.03229", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, Luyu Wang", "title": "Automatic Selection of t-SNE Perplexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely\nused dimensionality reduction methods for data visualization, but it has a\nperplexity hyperparameter that requires manual selection. In practice, proper\ntuning of t-SNE perplexity requires users to understand the inner working of\nthe method as well as to have hands-on experience. We propose a model selection\nobjective for t-SNE perplexity that requires negligible extra computation\nbeyond that of the t-SNE itself. We empirically validate that the perplexity\nsettings found by our approach are consistent with preferences elicited from\nhuman experts across a number of datasets. The similarities of our approach to\nBayesian information criteria (BIC) and minimum description length (MDL) are\nalso analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 14:19:20 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cao", "Yanshuai", ""], ["Wang", "Luyu", ""]]}, {"id": "1708.03472", "submitter": "Todd Davies", "authors": "Chi Ling Chan, Justin Lai, Bryan Hooi, Todd Davies", "title": "The Message or the Messenger? Inferring Virality and Diffusion Structure\n  from Online Petition Signature Data", "comments": "19 pages, 6 figures, 4 tables, to appear in Giovanni Luca Ciampaglia,\n  Afra J. Mashhadi, and Taha Yasseri (Editors), Social Informatics: Proceedings\n  of the 9th International Conference, SocInfo 2017 (Oxford, UK, September\n  13-15), Springer LNCS, 2017", "journal-ref": "Lecture Notes in Computer Science 10539:499-517, 2017", "doi": "10.1007/978-3-319-67217-5_30", "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goel et al. (2016) examined diffusion data from Twitter to conclude that\nonline petitions are shared more virally than other types of content. Their\ndefinition of structural virality, which measures the extent to which diffusion\nfollows a broadcast model or is spread person to person (virally), depends on\nknowing the topology of the diffusion cascade. But often the diffusion\nstructure cannot be observed directly. We examined time-stamped signature data\nfrom the Obama White House's We the People petition platform. We developed\nmeasures based on temporal dynamics that, we argue, can be used to infer\ndiffusion structure as well as the more intrinsic notion of virality sometimes\nknown as infectiousness. These measures indicate that successful petitions are\nlikely to be higher in both intrinsic and structural virality than unsuccessful\npetitions are. We also investigate threshold effects on petition signing that\nchallenge simple contagion models, and report simulations for a theoretical\nmodel that are consistent with our data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 08:43:31 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Chan", "Chi Ling", ""], ["Lai", "Justin", ""], ["Hooi", "Bryan", ""], ["Davies", "Todd", ""]]}, {"id": "1708.03579", "submitter": "Alex Reinhart", "authors": "Alex Reinhart and Joel Greenhouse", "title": "Self-exciting point processes with spatial covariates: modeling the\n  dynamics of crime", "comments": "30 pages, 12 figures, 6 tables; updated to accepted version", "journal-ref": "Journal of the Royal Statistical Society: Series C, vol 67, no 5,\n  pp 1305-1329 (Nov 2018)", "doi": "10.1111/rssc.12277", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crime has both varying patterns in space, related to features of the\nenvironment, economy, and policing, and patterns in time arising from criminal\nbehavior, such as retaliation. Serious crimes may also be presaged by minor\ncrimes of disorder. We demonstrate that these spatial and temporal patterns are\ngenerally confounded, requiring analyses to take both into account, and propose\na spatio-temporal self-exciting point process model which incorporates spatial\nfeatures, near-repeat and retaliation effects, and triggering. We develop\ninference methods and diagnostic tools, such as residual maps, for this model,\nand through extensive simulation and crime data obtained from Pittsburgh,\nPennsylvania, demonstrate its properties and usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 15:44:53 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 17:48:37 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Reinhart", "Alex", ""], ["Greenhouse", "Joel", ""]]}, {"id": "1708.03782", "submitter": "Mohamed Laib", "authors": "Mohamed Laib and Luciano Telesca and Mikhail Kanevski", "title": "Periodic fluctuations in correlation-based connectivity density time\n  series: application to wind speed-monitoring network in Switzerland", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.11.081", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the periodic fluctuations of connectivity density\ntime series of a wind speed-monitoring network in Switzerland. By using the\ncorrelogram-based robust periodogram annual periodic oscillations were found in\nthe correlation-based network. The intensity of such annual periodic\noscillations is larger for lower correlation thresholds and smaller for higher.\nThe annual periodicity in the connectivity density seems reasonably consistent\nwith the seasonal meteo-climatic cycle.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 14:21:46 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 08:18:56 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Laib", "Mohamed", ""], ["Telesca", "Luciano", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1708.03833", "submitter": "Aditya Vempaty", "authors": "Aditya Vempaty, Lav R. Varshney, and Pramod K. Varshney", "title": "A Coupon-Collector Model of Machine-Aided Discovery", "comments": "5 pages, 9 figures, 2017 KDD Workshop on Data-Driven Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical studies of scientific discovery---so-called Eurekometrics---have\nindicated that the output of exploration proceeds as a logistic growth curve.\nAlthough logistic functions are prevalent in explaining population growth that\nis resource-limited to a given carrying capacity, their derivation do not apply\nto discovery processes. This paper develops a generative model for logistic\n\\emph{knowledge discovery} using a novel extension of coupon collection, where\nan explorer interested in discovering all unknown elements of a set is\nsupported by technology that can respond to queries. This discovery process is\nparameterized by the novelty and quality of the set of discovered elements at\nevery time step, and randomness is demonstrated to improve performance.\nSimulation results provide further intuition on the discovery process.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 00:43:28 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Vempaty", "Aditya", ""], ["Varshney", "Lav R.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1708.03859", "submitter": "Luigi Lombardo", "authors": "Luigi Lombardo, Sergio Saia, Calogero Schillaci, P. Martin Mai,\n  Rapha\\\"el Huser", "title": "Modeling soil organic carbon with Quantile Regression: Dissecting\n  predictors' effects on carbon stocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil Organic Carbon (SOC) estimation is crucial to manage both natural and\nanthropic ecosystems and has recently been put under the magnifying glass after\nthe Paris agreement 2016 due to its relationship with greenhouse gas.\nStatistical applications have dominated the SOC stock mapping at regional scale\nso far. However, the community has hardly ever attempted to implement Quantile\nRegression (QR) to spatially predict the SOC distribution. In this\ncontribution, we test QR to estimate SOC stock (0-30 $cm$ depth) in the\nagricultural areas of a highly variable semi-arid region (Sicily, Italy, around\n25,000 $km2$) by using topographic and remotely sensed predictors. We also\ncompare the results with those from available SOC stock measurement. The QR\nmodels produced robust performances and allowed to recognize dominant effects\namong the predictors with respect to the considered quantile. This information,\ncurrently lacking, suggests that QR can discern predictor influences on SOC\nstock at specific sub-domains of each predictors. In this work, the predictive\nmap generated at the median shows lower errors than those of the Joint Research\nCentre and International Soil Reference, and Information Centre benchmarks. The\nresults suggest the use of QR as a comprehensive and effective method to map\nSOC using legacy data in agro-ecosystems. The R code scripted in this study for\nQR is included.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 06:08:30 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Lombardo", "Luigi", ""], ["Saia", "Sergio", ""], ["Schillaci", "Calogero", ""], ["Mai", "P. Martin", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1708.03972", "submitter": "Arnab Hazra", "authors": "Arnab Hazra", "title": "Analysis of Annual Cyclone Frequencies over Bay of Bengal: Effect of\n  2004 Indian Ocean Tsunami", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the time series trend and variability of the cyclone\nfrequencies over Bay of Bengal, particularly in order to conclude if there is\nany significant difference in the pattern visible before and after the\ndisastrous 2004 Indian ocean tsunami based on the observed annual cyclone\nfrequency data obtained by India Meteorological Department over the years\n1891-2015. Three different categories of cyclones- depression (<34 knots),\ncyclonic storm (34-47 knots) and severe cyclonic storm (>47 knots) have been\nanalyzed separately using a non-homogeneous Poisson process approach. The\nestimated intensity functions of the Poisson processes along with their first\ntwo derivatives are discussed and all three categories show decreasing trend of\nthe intensity functions after the tsunami. Using an exact change-point\nanalysis, we show that the drops in mean intensity functions are significant\nfor all three categories. As of author's knowledge, no study so far have\ndiscussed the relation between cyclones and tsunamis. Bay of Bengal is\nsurrounded by one of the most densely populated areas of the world and any kind\nof significant change in tropical cyclone pattern has a large impact in various\nways, for example, disaster management planning and our study is immensely\nimportant from that perspective.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 22:56:47 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Hazra", "Arnab", ""]]}, {"id": "1708.03975", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Fl\\'avio B. Gon\\c{c}alves, B\\'arbara C. C. Dias, Tufi M. Soares", "title": "Bayesian Item Response model: a generalised approach for the abilities'\n  distribution using mixtures", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Item Response Theory models assume the distribution of the\nabilities of the population in study to be Gaussian. However, this may not\nalways be a reasonable assumption, which motivates the development of more\ngeneral models. This paper presents a generalised approach for the distribution\nof the abilities in dichotomous 3-parameter Item Response models. A mixture of\nnormal distributions is considered, allowing for features like skewness,\nmultimodality and heavy tails. A solution is proposed to deal with model\nidentifiability issues without compromising the flexibility and practical\ninterpretation of the model. Inference is carried out under the Bayesian\nParadigm through a novel MCMC algorithm. The algorithm is designed in a way to\nfavour good mixing and convergence properties and is also suitable for\ninference in traditional IRT models. The efficiency and applicability of our\nmethodology is illustrated in simulated and real examples.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 23:19:28 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 11:07:25 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B.", ""], ["Dias", "B\u00e1rbara C. C.", ""], ["Soares", "Tufi M.", ""]]}, {"id": "1708.04216", "submitter": "Mohamed Laib", "authors": "Mohamed Laib and Luciano Telesca and Mikhail Kanevski", "title": "Long-range fluctuations and multifractality in connectivity density time\n  series of a wind speed monitoring network", "comments": null, "journal-ref": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 28 (3),\n  2018, p. 033108", "doi": "10.1063/1.5022737", "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the daily connectivity time series of a wind\nspeed-monitoring network using multifractal detrended fluctuation analysis. It\ninvestigates the long-range fluctuation and multifractality in the residuals of\nthe connectivity time series. Our findings reveal that the daily connectivity\nof the correlation-based network is persistent for any correlation threshold.\nFurther, the multifractality degree is higher for larger absolute values of the\ncorrelation threshold\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:25:04 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 06:55:30 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Laib", "Mohamed", ""], ["Telesca", "Luciano", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1708.04217", "submitter": "Ran Zhao", "authors": "Qidi Peng and Ran Zhao", "title": "A General Class of Multifractional Processes and Stock Price\n  Informativeness", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.08.004", "report-no": null, "categories": "q-fin.MF math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general class of stochastic processes driven by a\nmultifractional Brownian motion (mBm) and study the estimation problems of\ntheir pointwise H\\\"older exponents (PHE) based on a new localized generalized\nquadratic variation approach (LGQV). By comparing our suggested approach with\nthe other two existing benchmark estimation approaches (classic GQV and\noscillation approach) through a simulation study, we show that our estimator\nhas better performance in the case where the observed process is some unknown\nbivariate function of time and mBm. Such multifractional processes, whose PHEs\nare time-varying, can be used to model stock prices under various market\nconditions, that are both time-dependent and region-dependent. As an\napplication to finance, an empirical study on modeling cross-listed stocks\nprovides new evidence that the equity path's roughness varies via time and the\nstock price informativeness properties from global stock markets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:25:41 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 08:17:43 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 17:50:19 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Peng", "Qidi", ""], ["Zhao", "Ran", ""]]}, {"id": "1708.04221", "submitter": "Axel Finke", "authors": "Axel Finke, Ruth King, Alexandros Beskos, Petros Dellaportas", "title": "Efficient sequential Monte Carlo algorithms for integrated population\n  models", "comments": "includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models are commonly used to describe different forms of\necological data. We consider the case of count data with observation errors.\nFor such data the system process is typically multi-dimensional consisting of\ncoupled Markov processes, where each component corresponds to a different\ncharacterisation of the population, such as age group, gender or breeding\nstatus. The associated system process equations describe the biological\nmechanisms under which the system evolves over time. However, there is often\nlimited information in the count data alone to sensibly estimate demographic\nparameters of interest, so these are often combined with additional ecological\nobservations leading to an integrated data analysis. Unfortunately, fitting\nthese models to the data can be challenging, especially if the state-space\nmodel for the count data is non-linear or non-Gaussian. We propose an efficient\nparticle Markov chain Monte Carlo algorithm to estimate the demographic\nparameters without the need for resorting to linear or Gaussian approximations.\nIn particular, we exploit the integrated model structure to enhance the\nefficiency of the algorithm. We then incorporate the algorithm into a\nsequential Monte Carlo sampler in order to perform model comparison with\nregards to the dependence structure of the demographic parameters. Finally, we\ndemonstrate the applicability and computational efficiency of our algorithms on\ntwo real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:37:41 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Finke", "Axel", ""], ["King", "Ruth", ""], ["Beskos", "Alexandros", ""], ["Dellaportas", "Petros", ""]]}, {"id": "1708.04339", "submitter": "Jose Figueroa-Lopez", "authors": "Jos\\'e E. Figueroa-L\\'opez and Cecilia Mancini", "title": "Optimum thresholding using mean and conditional mean square error", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a univariate semimartingale model for (the logarithm of) an asset\nprice, containing jumps having possibly infinite activity (IA). The\nnonparametric threshold estimator of the integrated variance IV proposed in\nMancini 2009 is constructed using observations on a discrete time grid, and\nprecisely it sums up the squared increments of the process when they are below\na threshold, a deterministic function of the observation step and possibly of\nthe coefficients of X. All the threshold functions satisfying given conditions\nallow asymptotically consistent estimates of IV, however the finite sample\nproperties of the truncated realized variation can depend on the specific\nchoice of the threshold. We aim here at optimally selecting the threshold by\nminimizing either the estimation mean square error (MSE) or the conditional\nmean square error (cMSE). The last criterion allows to reach a threshold which\nis optimal not in mean but for the specific volatility (and jumps paths) at\nhand. A parsimonious characterization of the optimum is established, which\nturns out to be asymptotically proportional to the L\\'evy's modulus of\ncontinuity of the underlying Brownian motion. Moreover, minimizing the cMSE\nenables us to propose a novel implementation scheme for approximating the\noptimal threshold. Monte Carlo simulations illustrate the superior performance\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 21:38:56 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Figueroa-L\u00f3pez", "Jos\u00e9 E.", ""], ["Mancini", "Cecilia", ""]]}, {"id": "1708.04354", "submitter": "Weston Viles", "authors": "Weston D. Viles and A. James O'Malley", "title": "Constrained Community Detection in Social Networks", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in networks is the process of identifying unusually\nwell-connected sub-networks and is a central component of many applied network\nanalyses. The paradigm of modularity optimization stipulates a partition of the\nnetwork's vertices which maximizes the difference between the fraction of edges\nwithin groups (communities) and the expected fraction if edges were randomly\ndistributed. The modularity objective function incorporates the network's\ntopology exclusively and has been extensively studied whereas the integration\nof constraints or external information on community composition has largely\nremained unexplored. We impose a penalty function on the modularity objective\nfunction to regulate the constitution of communities and apply our methodology\nin identifying health care communities (HCCs) within a network of hospitals\nsuch that the number of cardiac defibrillator surgeries performed within each\nHCC exceeds a minimum threshold. This restriction permits meaningful\ncomparisons in cardiac care among the resulting health care communities by\nstandardizing the distribution of cardiac care across the hospital network.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 23:16:18 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Viles", "Weston D.", ""], ["O'Malley", "A. James", ""]]}, {"id": "1708.04420", "submitter": "Peter Vogel", "authors": "Peter Vogel, Peter Knippertz, Andreas H. Fink, Andreas Schlueter and\n  Tilmann Gneiting", "title": "Skill of global raw and postprocessed ensemble predictions of rainfall\n  over northern tropical Africa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accumulated precipitation forecasts are of high socioeconomic importance for\nagriculturally dominated societies in northern tropical Africa. In this study,\nwe analyze the performance of nine operational global ensemble prediction\nsystems (EPSs) relative to climatology-based forecasts for 1 to 5-day\naccumulated precipitation based on the monsoon seasons 2007-2014 for three\nregions within northern tropical Africa. To assess the full potential of raw\nensemble forecasts across spatial scales, we apply state-of-the-art statistical\npostprocessing methods in form of Bayesian Model Averaging (BMA) and Ensemble\nModel Output Statistics (EMOS), and verify against station and spatially\naggregated, satellite-based gridded observations. Raw ensemble forecasts are\nuncalibrated, unreliable, and underperform relative to climatology,\nindependently of region, accumulation time, monsoon season, and ensemble.\nDifferences between raw ensemble and climatological forecasts are large, and\npartly stem from poor prediction for low precipitation amounts. BMA and EMOS\npostprocessed forecasts are calibrated, reliable, and strongly improve on the\nraw ensembles, but - somewhat disappointingly - typically do not outperform\nclimatology. Most EPSs exhibit slight improvements over the period 2007-2014,\nbut overall have little added value compared to climatology. We suspect that\nthe parametrization of convection is a potential cause for the sobering lack of\nensemble forecast skill in a region dominated by mesoscale convective systems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 07:48:30 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Vogel", "Peter", ""], ["Knippertz", "Peter", ""], ["Fink", "Andreas H.", ""], ["Schlueter", "Andreas", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1708.04490", "submitter": "Giles Hooker", "authors": "David Sinclair and Giles Hooker", "title": "Sparse Inverse Covariance Estimation for High-throughput microRNA\n  Sequencing Data in the Poisson Log-Normal Graphical Model", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Poisson Log-Normal Graphical Model for count data, and\npresent a normality transformation for data arising from this distribution. The\nmodel and transformation are feasible for high-throughput microRNA (miRNA)\nsequencing data and directly account for known overdispersion relationships\npresent in this data set. The model allows for network dependencies to be\nmodeled, and we provide an algorithm which utilizes a one-step EM based result\nin order to allow for a provable increase in performance in determining the\nnetwork structure. The model is shown to provide an increase in performance in\nsimulation settings over a range of network structures. The model is applied to\nhigh-throughput miRNA sequencing data from patients with breast cancer from The\nCancer Genome Atlas (TCGA). By selecting the most highly connected miRNA\nmolecules in the fitted network we find that nearly all of them are known to be\ninvolved in the regulation of breast cancer.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 13:32:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sinclair", "David", ""], ["Hooker", "Giles", ""]]}, {"id": "1708.04577", "submitter": "Rajita Menon", "authors": "Rajita Menon, Vivek Ramanan and Kirill S. Korolev", "title": "Interactions between species introduce spurious associations in\n  microbiome studies", "comments": "4 main text figures, 15 supplementary figures (i.e appendix) and 6\n  supplementary tables. Overall 49 pages including references", "journal-ref": "2018. PLoS Comput Biol 14(1): e1005939", "doi": "10.1371/journal.pcbi.1005939", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbiota contribute to many dimensions of host phenotype, including\ndisease. To link specific microbes to specific phenotypes, microbiome-wide\nassociation studies compare microbial abundances between two groups of samples.\nAbundance differences, however, reflect not only direct associations with the\nphenotype, but also indirect effects due to microbial interactions. We found\nthat microbial interactions could easily generate a large number of spurious\nassociations that provide no mechanistic insight. Using techniques from\nstatistical physics, we developed a method to remove indirect associations and\napplied it to the largest dataset on pediatric inflammatory bowel disease. Our\nmethod corrected the inflation of p-values in standard association tests and\nshowed that only a small subset of associations is directly linked to the\ndisease. Direct associations had a much higher accuracy in separating cases\nfrom controls and pointed to immunomodulation, butyrate production, and the\nbrain-gut axis as important factors in the inflammatory bowel disease.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 16:25:29 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 05:37:43 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Menon", "Rajita", ""], ["Ramanan", "Vivek", ""], ["Korolev", "Kirill S.", ""]]}, {"id": "1708.04741", "submitter": "Jia Jia", "authors": "Jia Jia, Qi Tang, Wangang Xie, Richard Rode", "title": "A Novel Method of Subgroup Identification by Combining Virtual Twins\n  with GUIDE (VG) for Development of Precision Medicines", "comments": "22 pages, 4 figures, 3 tables, all included in the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lack of understanding of human biology creates a hurdle for the development\nof precision medicines. To overcome this hurdle we need to better understand\nthe potential synergy between a given investigational treatment (vs. placebo or\nactive control) and various demographic or genetic factors, disease history and\nseverity, etc., with the goal of identifying those patients at increased risk\nof exhibiting clinically meaningful treatment benefit. For this reason, we\npropose the VG method, which combines the idea of an individual treatment\neffect (ITE) from Virtual Twins (Foster, et al., 2011) with the unbiased\nvariable selection and cutoff value determination algorithm from GUIDE (Loh, et\nal., 2015). Simulation results show the VG method has less variable selection\nbias than Virtual Twins and higher statistical power than GUIDE Interaction in\nthe presence of prognostic variables with strong treatment effects. Type I\nerror and predictive performance of Virtual Twins, GUIDE and VG are compared\nthrough the use of simulation studies. Results obtained after retrospectively\napplying VG to data from a clinical trial also are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 02:03:52 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Jia", "Jia", ""], ["Tang", "Qi", ""], ["Xie", "Wangang", ""], ["Rode", "Richard", ""]]}, {"id": "1708.04742", "submitter": "Lucas Macri", "authors": "Wenlong Yuan, Lucas M. Macri, Shiyuan He, Jianhua Z. Huang, Shashi M.\n  Kanbur and Chow-Choong Ngeow", "title": "Large Magellanic Cloud Near-Infrared Synoptic Survey. V.\n  Period-Luminosity Relations of Miras", "comments": "Accepted for publication in The Astronomical Journal", "journal-ref": null, "doi": "10.3847/1538-3881/aa86f1", "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the near-infrared properties of 690 Mira candidates in the central\nregion of the Large Magellanic Cloud, based on time-series observations at\nJHKs. We use densely-sampled I-band observations from the OGLE project to\ngenerate template light curves in the near infrared and derive robust mean\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\nstudy the Period-Luminosity-Color relations and the color excesses of\nCarbon-rich Miras, which show evidence for a substantially different reddening\nlaw.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 02:05:08 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Yuan", "Wenlong", ""], ["Macri", "Lucas M.", ""], ["He", "Shiyuan", ""], ["Huang", "Jianhua Z.", ""], ["Kanbur", "Shashi M.", ""], ["Ngeow", "Chow-Choong", ""]]}, {"id": "1708.04789", "submitter": "Norm Matloff PhD", "authors": "Norman Matloff, Reed Davis, Laurel Beckett and Paul Thompson", "title": "revisit: a Workflow Tool for Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been widespread concern in the scientific community\nover a reproducibility crisis. Among the major causes that have been identified\nis statistical: In many scientific research the statistical analysis (including\ndata preparation) suffers from a lack of transparency and methodological\nproblems, major obstructions to reproducibility. The revisit package aims\ntoward remedying this problem, by generating a \"software paper trail\" of the\nstatistical operations applied to a dataset. This record can be \"replayed\" for\nverification purposes, as well as be modified to enable alternative analyses.\nThe software also issues warnings of certain kinds of potential errors in\nstatistical methodology, again related to the reproducibility issue.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 06:53:05 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Matloff", "Norman", ""], ["Davis", "Reed", ""], ["Beckett", "Laurel", ""], ["Thompson", "Paul", ""]]}, {"id": "1708.04792", "submitter": "Marcio Diniz", "authors": "M\\'arcio Augusto Diniz, Mourad Tighiouart and Andr\\'e Rogatko", "title": "Comparison between continuous and discrete doses using Escalation With\n  Overdose Control", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is an extensive statistical literature showing the\ndisadvantages of discretizing continuous variables, categorization is a common\npractice in clinical research which results in substantial loss of information.\nA large collection of methods in cancer phase I clinical trial design\nestablishes dose of a new agent as a discrete variable. A noteworthy exception\nis the Escalation With Overdose Control (EWOC) design, where doses can be\ndefined either as continuous or as a grid of discrete doses. A Monte Carlo\nsimulation study was performed to compare the operating characteristics of\ncontinuous and discrete dose EWOC designs. Four equally spaced grids with\ndifferent interval lengths were considered. The loss of information was\nmeasured by several operating characteristics easier for clinicians to\ninterpret, in addition to the usual statistical measures of bias and mean\nsquared error. Based on the simulations, if there is not enough knowledge about\nthe true MTD value as commonly happens in phase I clinical trials, continuous\ndose scheme arises as an attractive option.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 07:08:57 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Diniz", "M\u00e1rcio Augusto", ""], ["Tighiouart", "Mourad", ""], ["Rogatko", "Andr\u00e9", ""]]}, {"id": "1708.04819", "submitter": "Alla Slynko", "authors": "Alla Slynko and Axel Benner", "title": "Measures of hydroxymethylation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydroxymethylcytosine (5hmC) methylation is known to be a possible epigenetic\nmark impacting genome stability. In this paper, we address the existing 5hmC\nmeasure $\\Delta \\beta$ and discuss its properties both analytically and\nempirically on real data. Then we introduce several alternative\nhydroxymethylation measures and compare their properties with those of $\\Delta\n\\beta$. All results will be illustrated by means of real data analyses.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 09:05:51 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 07:13:07 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Slynko", "Alla", ""], ["Benner", "Axel", ""]]}, {"id": "1708.04935", "submitter": "Lei Chu", "authors": "Robert Qiu, Lei Chu, Xing He, Zenan Ling, Haichun Liu", "title": "Spatio-Temporal Big Data Analysis for Smart Grids Based on Random Matrix\n  Theory: A Comprehensive Study", "comments": "Book chapter#23 for the book \"Transportation and Power Grid in Smart\n  Cities: Communication Networks and Services\". arXiv admin note: text overlap\n  with arXiv:1302.0885 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cornerstone of the smart grid is the advanced monitorability on its assets\nand operations. Increasingly pervasive installation of the phasor measurement\nunits (PMUs) allows the so-called synchrophasor measurements to be taken\nroughly 100 times faster than the legacy supervisory control and data\nacquisition (SCADA) measurements, time-stamped using the global positioning\nsystem (GPS) signals to capture the grid dynamics. On the other hand, the\navailability of low-latency two-way communication networks will pave the way to\nhigh-precision real-time grid state estimation and detection, remedial actions\nupon network instability, and accurate risk analysis and post-event assessment\nfor failure prevention.\n  In this chapter, we firstly modelling spatio-temporal PMU data in large scale\ngrids as random matrix sequences. Secondly, some basic principles of random\nmatrix theory (RMT), such as asymptotic spectrum laws, transforms, convergence\nrate and free probability, are introduced briefly in order to the better\nunderstanding and application of RMT technologies. Lastly, the case studies\nbased on synthetic data and real data are developed to evaluate the performance\nof the RMT-based schemes in different application scenarios (i.e., state\nevaluation and situation awareness).\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 08:55:50 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Qiu", "Robert", ""], ["Chu", "Lei", ""], ["He", "Xing", ""], ["Ling", "Zenan", ""], ["Liu", "Haichun", ""]]}, {"id": "1708.04941", "submitter": "Anirudh Acharya", "authors": "Anirudh Acharya and Madalin Guta", "title": "Minimax estimation of qubit states with Bures risk", "comments": "29 pages, 1 figure ; V2: Added section on quantum relative entropy,\n  updated introduction and abstract, added references", "journal-ref": "J. Phys. A: Math. Theor. 51 175307 (2018)", "doi": "10.1088/1751-8121/aab6f2", "report-no": null, "categories": "quant-ph math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central problem of quantum statistics is to devise measurement schemes\nfor the estimation of an unknown state, given an ensemble of $n$ independent\nidentically prepared systems. For locally quadratic loss functions, the risk of\nstandard procedures has the usual scaling of $1/n$. However, it has been\nnoticed that for fidelity based metrics such as the Bures distance, the risk of\nconventional (non-adaptive) qubit tomography schemes scales as $1/\\sqrt{n}$ for\nstates close to the boundary of the Bloch sphere. Several proposed estimators\nappear to improve this scaling, and our goal is to analyse the problem from the\nperspective of the maximum risk over all states.\n  We propose qubit estimation strategies based on separate and adaptive\nmeasurements, that achieve $1/n$ scalings for the maximum Bures risk. The\nestimator involving local measurements uses a fixed fraction of the available\nresource $n$ to estimate the Bloch vector direction; the length of the Bloch\nvector is then estimated from the remaining copies by measuring in the\nestimator eigenbasis. The estimator based on collective measurements uses local\nasymptotic normality techniques which allows us derive upper and lower bounds\nto its maximum Bures risk. We also discuss how to construct a minimax optimal\nestimator in this setup. Finally, we consider quantum relative entropy and show\nthat the risk of the estimator based on collective measurements achieves a rate\n$O(n^{-1}\\log n)$ under this loss function. Furthermore, we show that no\nestimator can achieve faster rates, in particular the `standard' rate $1/n$.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 15:32:38 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 12:53:15 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Acharya", "Anirudh", ""], ["Guta", "Madalin", ""]]}, {"id": "1708.05017", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Adrian Dobra", "title": "Measuring Human Activity Spaces from GPS Data with Density Ranking and\n  Summary Curves", "comments": "45 pages, 13 figures. Add a mixture model for GPS data, 4 new\n  theorems, and a simulation study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity spaces are fundamental to the assessment of individuals' dynamic\nexposure to social and environmental risk factors associated with multiple\nspatial contexts that are visited during activities of daily living. In this\npaper we survey existing approaches for measuring the geometry, size and\nstructure of activity spaces based on GPS data, and explain their limitations.\nWe propose addressing these shortcomings through a nonparametric approach\ncalled density ranking, and also through three summary curves: the mass-volume\ncurve, the Betti number curve, and the persistence curve. We introduce a novel\nmixture model for human activity spaces, and study its asymptotic properties.\nWe prove that the kernel density estimator which, at the present time, is one\nof the most widespread methods for measuring activity spaces is not a stable\nestimator of their structure. We illustrate the practical value of our methods\nwith a simulation study, and with a recently collected GPS dataset that\ncomprises the locations visited by ten individuals over a six months period.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:01:03 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 22:39:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Dobra", "Adrian", ""]]}, {"id": "1708.05047", "submitter": "Elizabeth Upton", "authors": "Elizabeth Upton and Luis Carvalho", "title": "Bayesian Network Regularized Regression for Modeling Urban Crime\n  Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of statistical inference and prediction for\nprocesses defined on networks. We assume that the network is known and measures\nsimilarity, and our goal is to learn about an attribute associated with its\nvertices. Classical regression methods are not immediately applicable to this\nsetting, as we would like our model to incorporate information from both\nnetwork structure and pertinent covariates. Our proposed model consists of a\ngeneralized linear model with vertex indexed predictors and a basis expansion\nof their coefficients, allowing the coefficients to vary over the network. We\nemploy a regularization procedure, cast as a prior distribution on the\nregression coefficients under a Bayesian setup, so that the predicted responses\nvary smoothly according to the topology of the network. We motivate the need\nfor this model by examining occurrences of residential burglary in Boston,\nMassachusetts. Noting that crime rates are not spatially homogeneous, and that\nthe rates appear to vary sharply across regions in the city, we construct a\nhierarchical model that addresses these issues and gives insight into spatial\npatterns of crime occurrences. Furthermore, we examine efficient\nexpectation-maximization fitting algorithms and provide\ncomputationally-friendly methods for eliciting hyper-prior parameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 19:41:01 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Upton", "Elizabeth", ""], ["Carvalho", "Luis", ""]]}, {"id": "1708.05084", "submitter": "Zhiguang Huo", "authors": "Zhiguang Huo, Shaowu Tang, Yongseok Park and George Tseng", "title": "P-value evaluation, variability index and biomarker categorization for\n  adaptively weighted Fisher's meta-analysis method in omics applications", "comments": "adaptive weights, Fisher's method, meta-analysis, differential\n  expression analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis methods have been widely used to combine results from multiple\nclinical or genomic studies to increase statistical power and ensure robust and\naccurate conclusion. Adaptively weighted Fisher's method (AW-Fisher) is an\neffective approach to combine p-values from $K$ independent studies and to\nprovide better biological interpretation by characterizing which studies\ncontribute to meta-analysis. Currently, AW-Fisher suffers from lack of fast,\naccurate p-value computation and variability estimate of AW weights. When the\nnumber of studies $K$ is large, the $3^K - 1$ possible differential expression\npattern categories can become intractable. In this paper, we apply an\nimportance sampling technique with spline interpolation to increase accuracy\nand speed of p-value calculation. Using resampling techniques, we propose a\nvariability index for the AW weight estimator and a co-membership matrix to\ncharacterize pattern similarities between genes. The co-membership matrix is\nfurther used to categorize differentially expressed genes based on their\nmeta-patterns for further biological investigation. The superior performance of\nthe proposed methods is shown in simulations. These methods are also applied to\ntwo real applications to demonstrate intriguing biological findings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 21:02:22 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Huo", "Zhiguang", ""], ["Tang", "Shaowu", ""], ["Park", "Yongseok", ""], ["Tseng", "George", ""]]}, {"id": "1708.05094", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "An Ensemble Quadratic Echo State Network for Nonlinear Spatio-Temporal\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data and processes are prevalent across a wide variety of\nscientific disciplines. These processes are often characterized by nonlinear\ntime dynamics that include interactions across multiple scales of spatial and\ntemporal variability. The data sets associated with many of these processes are\nincreasing in size due to advances in automated data measurement, management,\nand numerical simulator output. Non- linear spatio-temporal models have only\nrecently seen interest in statistics, but there are many classes of such models\nin the engineering and geophysical sciences. Tradi- tionally, these models are\nmore heuristic than those that have been presented in the statistics\nliterature, but are often intuitive and quite efficient computationally. We\nshow here that with fairly simple, but important, enhancements, the echo state\nnet- work (ESN) machine learning approach can be used to generate long-lead\nforecasts of nonlinear spatio-temporal processes, with reasonable uncertainty\nquantification, and at only a fraction of the computational expense of a\ntraditional parametric nonlinear spatio-temporal models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 22:08:25 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1708.05404", "submitter": "Swasti Khuntia", "authors": "Swasti R. Khuntia, Jose L. Rueda, Mart A.M.M. van der Meijden", "title": "Pathway for Multivariate Dependence Modeling in Long-Term Horizon of\n  Electrical Power System", "comments": "5 pages, 2 figures, 2 algorithm tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future electricity consumption is fundamentally uncertain and dependent on\nmany variables such as economic activity, weather, electricity rates and demand\nside management. The stochasticity of system load as well as power generation\nfrom renewable energy sources (RES) (i.e., wind and solar) poses special\nchallenges to power system planners. Increasing penetration levels of wind and\nsolar exacerbate the uncertainty and variability that must be addressed in\ncoming years, and can be extremely relevant to power system planners. With this\npaper, pathways for multivariate dependence modeling using vine copula is\nproposed which includes both electrical load and power generation from RES.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 18:19:40 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Khuntia", "Swasti R.", ""], ["Rueda", "Jose L.", ""], ["van der Meijden", "Mart A. M. M.", ""]]}, {"id": "1708.05508", "submitter": "Naim Rashid", "authors": "Naim U. Rashid, Quefeng Li, Jen Jen Yeh, and Joseph G. Ibrahim", "title": "Modeling Between-Study Heterogeneity for Improved Reproducibility in\n  Gene Signature Selection and Clinical Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the genomic era, the identification of gene signatures associated with\ndisease is of significant interest. Such signatures are often used to predict\nclinical outcomes in new patients and aid clinical decision-making. However,\nrecent studies have shown that gene signatures are often not replicable. This\noccurrence has practical implications regarding the generalizability and\nclinical applicability of such signatures. To improve replicability, we\nintroduce a novel approach to select gene signatures from multiple datasets\nwhose effects are consistently non-zero and account for between-study\nheterogeneity. We build our model upon some rank-based quantities, facilitating\nintegration over different genomic datasets. A high dimensional penalized\nGeneralized Linear Mixed Model (pGLMM) is used to select gene signatures and\naddress data heterogeneity. We compare our method to some commonly used\nstrategies that select gene signatures ignoring between-study heterogeneity. We\nprovide asymptotic results justifying the performance of our method and\ndemonstrate its advantage in the presence of heterogeneity through thorough\nsimulation studies. Lastly, we motivate our method through a case study\nsubtyping pancreatic cancer patients from four gene expression studies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 04:40:06 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 16:37:58 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rashid", "Naim U.", ""], ["Li", "Quefeng", ""], ["Yeh", "Jen Jen", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "1708.05677", "submitter": "Gregor Dumphart", "authors": "Gregor Dumphart, Eric Slottke, Armin Wittneben", "title": "Robust Near-Field 3D Localization of an Unaligned Single-Coil Agent\n  Using Unobtrusive Anchors", "comments": "7 pages, to be presented at IEEE PIMRC 2017", "journal-ref": null, "doi": "10.1109/PIMRC.2017.8292363", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The magnetic near-field provides a suitable means for indoor localization,\ndue to its insensitivity to the environment and strong spatial gradients. We\nconsider indoor localization setups consisting of flat coils, allowing for\nconvenient integration of the agent coil into a mobile device (e.g., a smart\nphone or wristband) and flush mounting of the anchor coils to walls. In order\nto study such setups systematically, we first express the Cram\\'er-Rao lower\nbound (CRLB) on the position error for unknown orientation and evaluate its\ndistribution within a square room of variable size, using 15 x 10cm anchor\ncoils and a commercial NFC antenna at the agent. Thereby, we find cm-accuracy\nbeing achievable in a room of 10 x 10 x 3 meters with 12 flat wall-mounted\nanchors and with 10mW used for the generation of magnetic fields. Practically\nachieving such estimation performance is, however, difficult because of the\nnon-convex 5D likelihood function. To that end, we propose a fast and accurate\nweighted least squares (WLS) algorithm which is insensitive to initialization.\nThis is enabled by effectively eliminating the orientation nuisance parameter\nin a rigorous fashion and scaling the individual anchor observations, leading\nto a smoothed 3D cost function. Using WLS estimates to initialize a\nmaximum-likelihood (ML) solver yields accuracy near the theoretical limit in up\nto 98% of cases, thus enabling robust indoor localization with unobtrusive\ninfrastructure, with a computational efficiency suitable for real-time\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 16:21:53 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Dumphart", "Gregor", ""], ["Slottke", "Eric", ""], ["Wittneben", "Armin", ""]]}, {"id": "1708.05712", "submitter": "Colleen Farrelly", "authors": "Colleen M. Farrelly", "title": "Extensions of Morse-Smale Regression with Application to Actuarial\n  Science", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of subgroups is ubiquitous in scientific research (ex. disease\nheterogeneity, spatial distributions in ecology...), and piecewise regression\nis one way to deal with this phenomenon. Morse-Smale regression offers a way to\npartition the regression function based on level sets of a defined function and\nthat function's basins of attraction. This topologically-based piecewise\nregression algorithm has shown promise in its initial applications, but the\ncurrent implementation in the literature has been limited to elastic net and\ngeneralized linear regression. It is possible that nonparametric methods, such\nas random forest or conditional inference trees, may provide better prediction\nand insight through modeling interaction terms and other nonlinear\nrelationships between predictors and a given outcome.\n  This study explores the use of several machine learning algorithms within a\nMorse-Smale piecewise regression framework, including boosted regression with\nlinear baselearners, homotopy-based LASSO, conditional inference trees, random\nforest, and a wide neural network framework called extreme learning machines.\nSimulations on Tweedie regression problems with varying Tweedie parameter and\ndispersion suggest that many machine learning approaches to Morse-Smale\npiecewise regression improve the original algorithm's performance, particularly\nfor outcomes with lower dispersion and linear or a mix of linear and nonlinear\npredictor relationships. On a real actuarial problem, several of these new\nalgorithms perform as good as or better than the original Morse-Smale\nregression algorithm, and most provide information on the nature of predictor\nrelationships within each partition to provide insight into differences between\ndataset partitions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 22:33:35 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Farrelly", "Colleen M.", ""]]}, {"id": "1708.05894", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Sanjay Hariharan, Mark Sendak, Nathan Brajer, Meredith\n  Clement, Armando Bedoya, Cara O'Brien, Katherine Heller", "title": "An Improved Multi-Output Gaussian Process RNN with Real-Time Validation\n  for Early Sepsis Detection", "comments": "Presented at Machine Learning for Healthcare 2017, Boston, MA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a poorly understood and potentially life-threatening complication\nthat can occur as a result of infection. Early detection and treatment improves\npatient outcomes, and as such it poses an important challenge in medicine. In\nthis work, we develop a flexible classifier that leverages streaming lab\nresults, vitals, and medications to predict sepsis before it occurs. We model\npatient clinical time series with multi-output Gaussian processes, maintaining\nuncertainty about the physiological state of a patient while also imputing\nmissing values. The mean function takes into account the effects of medications\nadministered on the trajectories of the physiological variables. Latent\nfunction values from the Gaussian process are then fed into a deep recurrent\nneural network to classify patient encounters as septic or not, and the overall\nmodel is trained end-to-end using back-propagation. We train and validate our\nmodel on a large dataset of 18 months of heterogeneous inpatient stays from the\nDuke University Health System, and develop a new \"real-time\" validation scheme\nfor simulating the performance of our model as it will actually be used. Our\nproposed method substantially outperforms clinical baselines, and improves on a\nprevious related model for detecting sepsis. Our model's predictions will be\ndisplayed in a real-time analytics dashboard to be used by a sepsis rapid\nresponse team to help detect and improve treatment of sepsis.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 20:14:07 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Futoma", "Joseph", ""], ["Hariharan", "Sanjay", ""], ["Sendak", "Mark", ""], ["Brajer", "Nathan", ""], ["Clement", "Meredith", ""], ["Bedoya", "Armando", ""], ["O'Brien", "Cara", ""], ["Heller", "Katherine", ""]]}, {"id": "1708.05895", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Vin\\'icius D. Mayrink and Fl\\'avio B. Gon\\c{c}alves", "title": "Identifying down and up-regulated chromosome regions using RNA-Seq data", "comments": null, "journal-ref": null, "doi": "10.1007/s10260-019-00496-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of studies dealing with RNA-Seq data analysis has experienced a\nfast increase in the past years making this type of gene expression a strong\ncompetitor to the DNA microarrays. This paper proposes a Bayesian model to\ndetect down and up-regulated chromosome regions using RNA-Seq data. The\nmethodology is based on a recent work developed to detect up-regulated regions\nin the context of microarray data. A hidden Markov model is developed by\nconsidering a mixture of Gaussian distributions with ordered means in a way\nthat first and last mixture components are supposed to accommodate the under\nand overexpressed genes, respectively. The model is flexible enough to\nefficiently deal with the highly irregular spaced configuration of the data by\nassuming a hierarchical Markov dependence structure. The analysis of four\ncancer data sets (breast, lung, ovarian and uterus) is presented. Results\nindicate that the proposed model is selective in determining the regulation\nstatus, robust with respect to prior specifications and provides tools for a\nglobal or local search of under and overexpressed chromosome regions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 20:15:54 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mayrink", "Vin\u00edcius D.", ""], ["Gon\u00e7alves", "Fl\u00e1vio B.", ""]]}, {"id": "1708.06152", "submitter": "Josef Wilz\\'en", "authors": "Josef Wilz\\'en, Anders Eklund and Mattias Villani", "title": "Physiological Gaussian Process Priors for the Hemodynamics in fMRI\n  Analysis", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.jneumeth.2020.108778", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Inference from fMRI data faces the challenge that the hemodynamic\nsystem that relates neural activity to the observed BOLD fMRI signal is\nunknown.\n  New Method: We propose a new Bayesian model for task fMRI data with the\nfollowing features: (i) joint estimation of brain activity and the underlying\nhemodynamics, (ii) the hemodynamics is modeled nonparametrically with a\nGaussian process (GP) prior guided by physiological information and (iii) the\npredicted BOLD is not necessarily generated by a linear time-invariant (LTI)\nsystem. We place a GP prior directly on the predicted BOLD response, rather\nthan on the hemodynamic response function as in previous literature. This\nallows us to incorporate physiological information via the GP prior mean in a\nflexible way, and simultaneously gives us the nonparametric flexibility of the\nGP.\n  Results: Results on simulated data show that the proposed model is able to\ndiscriminate between active and non-active voxels also when the GP prior\ndeviates from the true hemodynamics. Our model finds time varying dynamics when\napplied to real fMRI data.\n  Comparison with Existing Method(s): The proposed model is better at detecting\nactivity in simulated data than standard models, without inflating the false\npositive rate. When applied to real fMRI data, our GP model in several cases\nfinds brain activity where previously proposed LTI models does not.\n  Conclusions: We have proposed a new non-linear model for the hemodynamics in\ntask fMRI, that is able to detect active voxels, and gives the opportunity to\nask new kinds of questions related to hemodynamics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 11:19:12 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 09:24:57 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 20:26:50 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wilz\u00e9n", "Josef", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1708.06160", "submitter": "Amir Ahmadi Javid", "authors": "Amir Ahmadi-Javid and Mohsen Ebadi", "title": "Economic Design of Memory-Type Control Charts: The Fallacy of the\n  Formula Proposed by Lorenzen and Vance (1986)", "comments": "Computational Statistics, 2020", "journal-ref": "Ahmadi-Javid, A., & Ebadi, M. (2020). Economic design of\n  memory-type control charts: The fallacy of the formula proposed by Lorenzen\n  and Vance (1986). Computational Statistics, DOI: 10.1007/s00180-020-01019-6", "doi": "10.1007/s00180-020-01019-6", "report-no": null, "categories": "stat.AP cs.CE econ.GN math.OC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory-type control charts, such as EWMA and CUSUM, are powerful tools\nfor detecting small quality changes in univariate and multivariate processes.\nMany papers on economic design of these control charts use the formula proposed\nby Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The\neconomic design of control charts: A unified approach. Technometrics, 28(1),\n3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct\nfor memory-type control charts and its values can significantly deviate from\nthe original values even if the ARL values used in this formula are accurately\ncomputed. Consequently, the use of this formula can result in charts that are\nnot economically optimal. The formula is corrected for memory-type control\ncharts, but unfortunately the modified formula is not a helpful tool from a\ncomputational perspective. We show that simulation-based optimization is a\npossible alternative method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 11:38:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ahmadi-Javid", "Amir", ""], ["Ebadi", "Mohsen", ""]]}, {"id": "1708.06738", "submitter": "Asim Dey", "authors": "Asim Kumer Dey, Yulia R. Gel, H. Vincent Poor", "title": "Motif-based analysis of power grid robustness under attacks", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network motifs are often called the building blocks of networks. Analysis of\nmotifs is found to be an indispensable tool for understanding local network\nstructure, in contrast to measures based on node degree distribution and its\nfunctions that primarily address a global network topology. As a result,\nnetworks that are similar in terms of global topological properties may differ\nnoticeably at a local level. In the context of power grids, this phenomenon of\nthe impact of local structure has been recently documented in fragility\nanalysis and power system classification. At the same time, most studies of\npower system networks still tend to focus on global topological measures of\npower grids, often failing to unveil hidden mechanisms behind vulnerability of\nreal power systems and their dynamic response to malfunctions. In this paper a\npilot study on motif-based analysis of power grid robustness under various\ntypes of intentional attacks is presented, with the goal of shedding light on\nlocal dynamics and vulnerability of power systems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 17:36:36 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Dey", "Asim Kumer", ""], ["Gel", "Yulia R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1708.06792", "submitter": "Marco Due\\~nas", "authors": "Mercedes Campi, Marco Due\\~nas", "title": "Volatility and Economic Growth in the Twentieth Century", "comments": "25 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The twentieth century was a period of outstanding economic growth together\nwith an unequal income distribution. This paper analyses the international\ndistribution of growth rates and its dynamics during the twentieth century. We\nshow that the whole century is characterized by a high heterogeneity in the\ndistribution of GDP per capita growth rates, which is reflected in different\nshapes and a persistent asymmetry of the distributions at the regional level\nand for countries of different development levels. We find that in the context\nof the global conflicts that characterized the first half of the twentieth\ncentury and involved mainly large economies, the well-known negative scale\nrelation between volatility and size of countries is not significant. After the\nyear 1956, a redistribution of volatility leads to a significant negative\nscale-relation, which has been recently considered as a robust feature of the\nevolution of economic organizations. Our results contribute with more empirical\nfacts that call the attention to traditional macroeconomic theories to better\nexplain the underlying complexity of the growth process and sheds light on its\nhistorical evolution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 19:31:00 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Campi", "Mercedes", ""], ["Due\u00f1as", "Marco", ""]]}, {"id": "1708.06872", "submitter": "Yilin Zhang", "authors": "Yilin Zhang, Marie Poux-Berthe, Chris Wells, Karolina Koc-Michalska,\n  and Karl Rohe", "title": "Discovering Political Topics in Facebook Discussion threads with Graph\n  Contextualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graph contextualization method, pairGraphText, to study\npolitical engagement on Facebook during the 2012 French presidential election.\nIt is a spectral algorithm that contextualizes graph data with text data for\nonline discussion thread. In particular, we examine the Facebook posts of the\neight leading candidates and the comments beneath these posts. We find evidence\nof both (i) candidate-centered structure, where citizens primarily comment on\nthe wall of one candidate and (ii) issue-centered structure (i.e. on political\ntopics), where citizens' attention and expression is primarily directed towards\na specific set of issues (e.g. economics, immigration, etc). To identify\nissue-centered structure, we develop pairGraphText, to analyze a network with\nhigh-dimensional features on the interactions (i.e. text). This technique\nscales to hundreds of thousands of nodes and thousands of unique words. In the\nFacebook data, spectral clustering without the contextualizing text information\nfinds a mixture of (i) candidate and (ii) issue clusters. The contextualized\ninformation with text data helps to separate these two structures. We conclude\nby showing that the novel methodology is consistent under a statistical model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 02:42:59 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 17:00:57 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 03:04:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Yilin", ""], ["Poux-Berthe", "Marie", ""], ["Wells", "Chris", ""], ["Koc-Michalska", "Karolina", ""], ["Rohe", "Karl", ""]]}, {"id": "1708.06982", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Jonas Wallin and Janine B. Illian", "title": "Level set Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-Gaussian Cox process (LGCP) is a popular point process for modeling\nnon-interacting spatial point patterns. This paper extends the LGCP model to\nhandle data exhibiting fundamentally different behaviors in different\nsubregions of the spatial domain. The aim of the analyst might be either to\nidentify and classify these regions, to perform kriging, or to derive some\nproperties of the parameters driving the random field in one or several of the\nsubregions. The extension is based on replacing the latent Gaussian random\nfield in the LGCP by a latent spatial mixture model. The mixture model is\nspecified using a latent, categorically valued, random field induced by level\nset operations on a Gaussian random field. Conditional on the classification,\nthe intensity surface for each class is modeled by a set of independent\nGaussian random fields. This allows for standard stationary covariance\nstructures, such as the Mat\\'{e}rn family, to be used to model Gaussian random\nfields with some degree of general smoothness but also occasional and\nstructured sharp discontinuities.\n  A computationally efficient MCMC method is proposed for Bayesian inference\nand we show consistency of finite dimensional approximations of the model.\nFinally, the model is fitted to point pattern data derived from a tropical\nrainforest on Barro Colorado island, Panama. We show that the proposed model is\nable to capture behavior for which inference based on the standard LGCP is\nbiased.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:13:30 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 12:32:17 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Wallin", "Jonas", ""], ["Illian", "Janine B.", ""]]}, {"id": "1708.06990", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Sebastiano Stramaglia, Daniele Marinazzo", "title": "On the interpretability and computational reliability of\n  frequency-domain Granger causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment to the paper 'A study of problems encountered in Granger\ncausality analysis from a neuroscience perspective'. We agree that\ninterpretation issues of Granger Causality in Neuroscience exist (partially due\nto the historical unfortunate use of the name 'causality', as nicely described\nin previous literature). On the other hand we think that the paper uses a\nformulation of Granger causality which is outdated (albeit still used), and in\ndoing so it dismisses the measure based on a suboptimal use of it. Furthermore,\nsince data from simulated systems are used, the pitfalls that are found with\nthe used formulation are intended to be general, and not limited to\nneuroscience. It would be a pity if this paper, even written in good faith,\nbecame a wildcard against all possible applications of Granger Causality,\nregardless of the hard work of colleagues aiming to seriously address the\nmethodological and interpretation pitfalls. In order to provide a balanced\nview, we replicated their simulations used the updated State Space\nimplementation, proposed already some years ago, in which the pitfalls are\nmitigated or directly solved.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:32:24 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Faes", "Luca", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1708.07058", "submitter": "Moumita Bhattacharya", "authors": "Moumita Bhattacharya, Deborah Ehrenthal, Hagit Shatkay", "title": "Identifying Growth-Patterns in Children by Applying Cluster analysis to\n  Electronic Medical Records", "comments": "4 pages, 5 figure Published in Proc. of the IEEE Int. Conference on\n  Bioinformatics and Biomedicine (BIBM), Belfast, Ireland, November, 2014", "journal-ref": null, "doi": "10.1109/BIBM.2014.6999183", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity is one of the leading health concerns in the United States.\nResearchers and health care providers are interested in understanding factors\naffecting obesity and detecting the likelihood of obesity as early as possible.\nIn this paper, we set out to recognize children who have higher risk of obesity\nby identifying distinct growth patterns in them. This is done by using\nclustering methods, which group together children who share similar body\nmeasurements over a period of time. The measurements characterizing children\nwithin the same cluster are plotted as a function of age. We refer to these\nplots as growthpattern curves. We show that distinct growth-pattern curves are\nassociated with different clusters and thus can be used to separate children\ninto the topmost (heaviest), middle, or bottom-most cluster based on early\ngrowth measurements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:21:21 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bhattacharya", "Moumita", ""], ["Ehrenthal", "Deborah", ""], ["Shatkay", "Hagit", ""]]}, {"id": "1708.07061", "submitter": "Jesus Lago", "authors": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter", "title": "Forecasting day-ahead electricity prices in Europe: the importance of\n  considering market integration", "comments": null, "journal-ref": "Applied Energy, Volume 211, 1 February 2018, Pages 890-903", "doi": "10.1016/j.apenergy.2017.11.098", "report-no": null, "categories": "q-fin.ST cs.CE cs.LG cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing integration among electricity markets, in this\npaper we propose two different methods to incorporate market integration in\nelectricity price forecasting and to improve the predictive performance. First,\nwe propose a deep neural network that considers features from connected markets\nto improve the predictive accuracy in a local market. To measure the importance\nof these features, we propose a novel feature selection algorithm that, by\nusing Bayesian optimization and functional analysis of variance, evaluates the\neffect of the features on the algorithm performance. In addition, using market\nintegration, we propose a second model that, by simultaneously predicting\nprices from two markets, improves the forecasting accuracy even further. As a\ncase study, we consider the electricity market in Belgium and the improvements\nin forecasting accuracy when using various French electricity features. We show\nthat the two proposed models lead to improvements that are statistically\nsignificant. Particularly, due to market integration, the predictive accuracy\nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage\nerror). In addition, we show that the proposed feature selection algorithm is\nable to perform a correct assessment, i.e. to discard the irrelevant features.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 15:34:48 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 17:12:16 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 15:34:43 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Lago", "Jesus", ""], ["De Ridder", "Fjo", ""], ["Vrancx", "Peter", ""], ["De Schutter", "Bart", ""]]}, {"id": "1708.07213", "submitter": "Samuel W.K. Wong", "authors": "Samuel W.K. Wong and James V. Zidek", "title": "The duration of load effect in lumber as stochastic degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a gamma process for modelling the damage that accumulates\nover time in the lumber used in structural engineering applications when stress\nis applied. The model separates the stochastic processes representing features\ninternal to the piece of lumber on the one hand, from those representing\nexternal forces due to applied dead and live loads. The model applies those\nexternal forces through a time-varying population level function designed for\ntime-varying loads. The application of this type of model, which is standard in\nreliability analysis, is novel in this context, which has been dominated by\naccumulated damage models (ADMs) over more than half a century. The proposed\nmodel is compared with one of the traditional ADMs. Our statistical results\nbased on a Bayesian analysis of experimental data highlight the limitations of\nusing accelerated testing data to assess long-term reliability, as seen in the\nwide posterior intervals. This suggests the need for more comprehensive testing\nin future applications, or to encode appropriate expert knowledge in the priors\nused for Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 23:24:52 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Wong", "Samuel W. K.", ""], ["Zidek", "James V.", ""]]}, {"id": "1708.07357", "submitter": "Tom Broekel", "authors": "Tom Broekel", "title": "Measuring technological complexity - Current approaches and a new\n  measure of structural complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper reviews two prominent approaches for the measurement of\ntechnological complexity: the method of reflection and the assessment of\ntechnologies' combinatorial difficulty. It discusses their central underlying\nassumptions and identifies potential problems related to these. A new measure\nof structural complexity is introduced as an alternative. The paper also puts\nforward four stylized facts of technological complexity that serve as\nbenchmarks in an empirical evaluation of five complexity measures (increasing\ndevelopment over time, larger R&D efforts, more collaborative R&D, spatial\nconcentration). The evaluation utilizes European patent data for the years 1980\nto 2013 and finds the new measure of structural complexity to mirror the four\nstylized facts as good as or better than traditional measures.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 11:33:32 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 06:18:04 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 17:39:40 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Broekel", "Tom", ""]]}, {"id": "1708.07363", "submitter": "H{\\aa}vard Wahl Kongsg{\\aa}rd HwK", "authors": "H{\\aa}vard Wahl Kongsg{\\aa}rd, Geir-Arne Fuglstad, H{\\aa}vard Rue,\n  Kristian Hveem, Steinar Krokstad", "title": "Modeling water supply networks and gastrointestinal disorder symptoms\n  with CAR models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The direct modeling of water networks is not a common practice in\nmodern epidemiology. While space often serves as a proxy, it can be\nproblematic. There are multiple ways to directly model water networks, but\nthese methods are not straightforward and can be difficult to implement. This\nstudy suggests a simple approach for modeling water networks and diseases, and\napplies this method to a dataset of self-reported gastrointestinal conditions\nfrom a questionnaire-based population health survey in central Norway.\n  Method: Our approach is based on a standard conditional autoregressive (CAR)\nmodel. An inverse matrix was constructed, with nodes weighted based on the\ndistance to neighboring nodes within the networks. This matrix was then fitted\nas a generic model. To illustrate its possible use, we utilized data taken from\na questionnaire-based population health survey, the HUNT Study, to measure\nself-reported gastrointestinal complaints. For hypothesis testing, we used the\ndeviance information criterion (DIC) and included variables in a stepwise\nmanner.\n  Results: The full model converged after six hours. We found no relation\nbetween the water networks and the health conditions of people whose residences\nconnected to different parts of the network in the geographical area studied.\n  Conclusion: All water network models are simplifications of the real\nnetworks. Nevertheless, we suggest a valid approach for distinguishing between\nthe general spatial effect and the water network using a generic model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 11:54:51 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 07:07:32 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kongsg\u00e5rd", "H\u00e5vard Wahl", ""], ["Fuglstad", "Geir-Arne", ""], ["Rue", "H\u00e5vard", ""], ["Hveem", "Kristian", ""], ["Krokstad", "Steinar", ""]]}, {"id": "1708.07451", "submitter": "Martin Genzel", "authors": "Martin Genzel and Peter Jung", "title": "Recovering Structured Data From Superimposed Non-Linear Measurements", "comments": null, "journal-ref": "IEEE Trans. Inf. Theory 66.1 (2020), 453-477", "doi": "10.1109/TIT.2019.2932426", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the problem of distributed data acquisition under\nnon-linear communication constraints. More specifically, we consider a model\nsetup where $M$ distributed nodes take individual measurements of an unknown\nstructured source vector $x_0 \\in \\mathbb{R}^n$, communicating their readings\nsimultaneously to a central receiver. Since this procedure involves collisions\nand is usually imperfect, the receiver measures a superposition of non-linearly\ndistorted signals. In a first step, we will show that an $s$-sparse vector\n$x_0$ can be successfully recovered from $O(s \\cdot\\log(2n/s))$ of such\nsuperimposed measurements, using a traditional Lasso estimator that does not\nrely on any knowledge about the non-linear corruptions. This direct method\nhowever fails to work for several \"uncalibrated\" system configurations. These\nblind reconstruction tasks can be easily handled with the\n$\\ell^{1,2}$-Group-Lasso, but coming along with an increased sampling rate of\n$O(s\\cdot \\max\\{M, \\log(2n/s) \\})$ observations - in fact, the purpose of this\nlifting strategy is to extend a certain class of bilinear inverse problems to\nnon-linear acquisition. Our two algorithmic approaches are a special instance\nof a more abstract framework which includes sub-Gaussian measurement designs as\nwell as general (convex) structural constraints. These results are of\nindependent interest for various recovery and learning tasks, as they apply to\narbitrary non-linear observation models. Finally, to illustrate the practical\nscope of our theoretical findings, an application to wireless sensor networks\nis discussed, which actually serves as the prototypical example of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:16:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 16:28:47 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Genzel", "Martin", ""], ["Jung", "Peter", ""]]}, {"id": "1708.07486", "submitter": "Thomas Nussbaumer", "authors": "Thomas Nussbaumer and Julia Polzin and Alexander Platzer", "title": "KEGGexpressionMapper allows for analysis of pathways over multiple\n  conditions by integrating transcriptomics and proteomics measurements", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.MN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: In transcriptomic and proteomics-based studies, the abundance of\ngenes is often compared to functional pathways such as the Kyoto Encyclopaedia\nat Genes and Genomes (KEGG) to identify active metabolic processes. Even though\na plethora of tools allow to analyze and to compare omics data in respect to\nKEGG pathways, the analysis of multiple conditions is often limited to only a\ndefined set of conditions. Furthermore, for transcriptomic datasets, it is\ncrucial to compare the entire set of pathways in order to obtain a global\noverview of the species' metabolic functions. Results: Here, we present the\ntool KEGGexpressionMapper, a module, that is implemented in the programming\nlanguage R. The module allows to highlight the expression of transcriptomic or\nproteomic measurements in various conditions on pathways and incorporates\nmethods to analyze gene enrichment analyses and expression clustering in time\nseries data. KEGGexpressionMapper supports time series data from transcriptomic\nor proteomics measurements from different individuals. As the tool is\nimplemented in the scripting language R, it can be integrated into existing\nanalysis pipelines to obtain a global overview of the dataset. The R package\ncan be downloaded from https://github.com/nthomasCUBE/KEGGexpressionmapper.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:47:20 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Nussbaumer", "Thomas", ""], ["Polzin", "Julia", ""], ["Platzer", "Alexander", ""]]}, {"id": "1708.07534", "submitter": "Ilya Safro", "authors": "Yuliya V. Bolotova, Jie Lou and Ilya Safro", "title": "Detecting and monitoring foodborne illness outbreaks: Twitter\n  communications and the 2015 U.S. Salmonella outbreak linked to imported\n  cucumbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research uses Twitter, as a social media device, to track communications\nrelated to the 2015 U.S. foodborne illness outbreak linked to Salmonella in\nimported cucumbers from Mexico. The relevant Twitter data are analyzed in light\nof the timeline of the official announcements made by the Centers for Disease\nControl and Prevention (CDC). The largest number of registered tweets is\nassociated with the period immediately following the CDC initial announcement\nand the official release of the first recall of cucumbers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 19:37:55 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Bolotova", "Yuliya V.", ""], ["Lou", "Jie", ""], ["Safro", "Ilya", ""]]}, {"id": "1708.07587", "submitter": "Wilson Ye Chen", "authors": "Wilson Ye Chen, Richard H. Gerlach", "title": "Semiparametric GARCH via Bayesian model averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the dynamic structure of the financial markets is subject to dramatic\nchanges, a model capable of providing consistently accurate volatility\nestimates must not make strong assumptions on how prices change over time. Most\nvolatility models impose a particular parametric functional form that relates\nan observed price change to a volatility forecast (news impact function). We\npropose a new class of functional coefficient semiparametric volatility models\nwhere the news impact function is allowed to be any smooth function, and study\nits ability to estimate volatilities compared to the well known parametric\nproposals, in both a simulation study and an empirical study with real\nfinancial data. We estimate the news impact function using a Bayesian model\naveraging approach, implemented via a carefully developed Markov chain Monte\nCarlo (MCMC) sampling algorithm. Using simulations we show that our flexible\nsemiparametric model is able to learn the shape of the news impact function\nfrom the observed data. When applied to real financial time series, our new\nmodel suggests that the news impact functions are significantly different in\nshapes for different asset types, but are similar for the assets of the same\ntype.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:56:20 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Gerlach", "Richard H.", ""]]}, {"id": "1708.07592", "submitter": "Seong-Hwan Jun", "authors": "Seong-Hwan Jun, Samuel W.K. Wong, James V. Zidek and Alexandre\n  Bouchard-C\\^ot\\'e", "title": "Sequential Decision Model for Inference and Prediction on Non-Uniform\n  Hypergraphs with Application to Knot Matching from Computational Forestry", "comments": "32 pages, 14 figures, submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the knot matching problem arising in computational\nforestry. The knot matching problem is an important problem that needs to be\nsolved to advance the state of the art in automatic strength prediction of\nlumber. We show that this problem can be formulated as a quadripartite matching\nproblem and develop a sequential decision model that admits efficient parameter\nestimation along with a sequential Monte Carlo sampler on graph matching that\ncan be utilized for rapid sampling of graph matching. We demonstrate the\neffectiveness of our methods on 30 manually annotated boards and present\nfindings from various simulation studies to provide further evidence supporting\nthe efficacy of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 01:12:37 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Jun", "Seong-Hwan", ""], ["Wong", "Samuel W. K.", ""], ["Zidek", "James V.", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1708.07604", "submitter": "Panpan Zhang", "authors": "Guang Ouyang, Dipak K. Dey, Panpan Zhang", "title": "Model-Based Method for Social Network Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple mixed membership model for social network clustering in\nthis note. A flexible function is adopted to measure affinities among a set of\nentities in a social network. The model not only allows each entity in the\nnetwork to possess more than one membership, but also provides accurate\nstatistical inference about network structure. We estimate the membership\nparameters by using an MCMC algorithm. We evaluate the performance of the\nproposed algorithm by applying our model to two empirical social network data,\nthe Zachary club data and the bottlenose dolphin network data. We also conduct\nsome numerical studies for different types of simulated networks for assessing\nthe effectiveness of our algorithm. In the end, some concluding remarks and\nfuture work are addressed briefly.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 03:21:10 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 17:56:23 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 20:28:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ouyang", "Guang", ""], ["Dey", "Dipak K.", ""], ["Zhang", "Panpan", ""]]}, {"id": "1708.07609", "submitter": "Panpan Zhang", "authors": "Guang Ouyang, Dipak K. Dey, Panpan Zhang", "title": "Clique-based Method for Social Network Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop a clique-based method for social network\nclustering. We introduce a new index to evaluate the quality of clustering\nresults, and propose an efficient algorithm based on recursive bipartition to\nmaximize an objective function of the proposed index. The optimization problem\nis NP-hard, so we approximate the semi-optimal solution via an implicitly\nrestarted Lanczos method. One of the advantages of our algorithm is that the\nproposed index of each community in the clustering result is guaranteed to be\nhigher than some predetermined threshold, $p$, which is completely controlled\nby users. We also account for the situation that $p$ is unknown. A statistical\nprocedure of controlling both under-clustering and over-clustering errors\nsimultaneously is carried out to select localized threshold for each\nsubnetwork, such that the community detection accuracy is optimized.\nAccordingly, we propose a localized clustering algorithm based on binary tree\nstructure. Finally, we exploit the stochastic blockmodels to conduct simulation\nstudies and demonstrate the accuracy and efficiency of our algorithms, both\nnumerically and graphically.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 04:16:37 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 22:22:32 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Ouyang", "Guang", ""], ["Dey", "Dipak K.", ""], ["Zhang", "Panpan", ""]]}, {"id": "1708.07691", "submitter": "Hirley Alves", "authors": "Onel L. Alcaraz L\\'opez, Hirley Alves, Pedro Nardelli, Matti Latva-aho", "title": "Aggregation and Resource Scheduling in Machine-type Communication\n  Networks: A Stochastic Geometry Approach", "comments": "Accepted for publication at IEEE TWC, April 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data aggregation is a promising approach to enable massive machine-type\ncommunication (mMTC). This paper focuses on the aggregation phase where a\nmassive number of machine-type devices (MTDs) transmit to aggregators. By using\nnon-orthogonal multiple access (NOMA) principles, we allow several MTDs to\nshare the same orthogonal channel in our proposed hybrid access scheme. We\ndevelop an analytical framework based on stochastic geometry to investigate the\nsystem performance in terms of average success probability and average number\nof simultaneously served MTDs, under imperfect successive interference\ncancellation (SIC) at the aggregators, for two scheduling schemes: random\nresource scheduling (RRS) and channel-aware resource scheduling (CRS). We\nidentify the power constraints on the MTDs sharing the same channel to attain a\nfair coexistence with purely orthogonal multiple access (OMA) setups, then\npower control coefficients are found so that these MTDs perform with similar\nreliability. We show that under high access demand, the hybrid scheme with CRS\noutperforms the OMA setup by simultaneously serving more MTDs with reduced\npower consumption.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 11:24:16 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 11:21:10 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["L\u00f3pez", "Onel L. Alcaraz", ""], ["Alves", "Hirley", ""], ["Nardelli", "Pedro", ""], ["Latva-aho", "Matti", ""]]}, {"id": "1708.07890", "submitter": "Chrisovaladis Malesios", "authors": "Chrisovalantis Malesios", "title": "Some variations on the standard theoretical models for the h-index: A\n  comparative analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various mathematical models proposed in the recent literature for\nestimating the h-index through bibliometric measures, such as number of\narticles (P) and citations received (C). These models have been previously\nempirically tested assuming a mathematical model and predetermining the models\nparameter values at some fixed constant. Here, by adopting a statistical\nmodelling view I investigate alternative distributions commonly used for this\ntype of point data. I also show that the typical assumptions for the parameters\nof the h-index mathematical models in such representations are not always\nrealistic, with more suitable specifications being favorable. Prediction of the\nhindex is also demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:14:54 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Malesios", "Chrisovalantis", ""]]}, {"id": "1708.07892", "submitter": "Chrisovaladis Malesios", "authors": "Chrisovalantis Malesios", "title": "Measuring the robustness of the journal h-index with respect to\n  publication and citation values: A Bayesian sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Braun et al. (2006) recommended using the h-index as an alternative to the\njournal impact factor (IF) to qualify journals. In this paper, a Bayesian-based\nsensitivity analysis is performed with the aid of mathematical models to\nexamine the behavior of the journal h-index to changes in the\npublication/citation counts of journals. Sensitivity of the h-index was most\napparent for changes in the number of citations, revealing similar patterns of\nbehavior for almost all models and independently to the field of research. In\ngeneral, the h-index was found to be robust to changes in citations up to\napproximately the 25th percentile of the citation distribution, inflating its\nvalue afterwards.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:18:59 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Malesios", "Chrisovalantis", ""]]}, {"id": "1708.07893", "submitter": "Chrisovaladis Malesios", "authors": "C. Malesios and S. Psarakis", "title": "Comparison of the h-index for Different Fields of Research Using\n  Bootstrap Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important disadvantage of the h-index is that typically it cannot take\ninto account the specific field of research of a researcher. Usually sample\npoint estimates of the average and median h-index values for the various fields\nare reported that are highly variable and dependent of the specific samples and\nit would be useful to provide confidence intervals of prediction accuracy. In\nthis paper we apply the non-parametric bootstrap technique for constructing\nconfidence intervals for the h-index for different fields of research. In this\nway no specific assumptions about the distribution of the empirical hindex are\nrequired as well as no large samples since that the methodology is based on\nresampling from the initial sample. The results of the analysis showed\nimportant differences between the various fields. The performance of the\nbootstrap intervals for the mean and median h-index for most fields seems to be\nrather satisfactory as revealed by the performed simulation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:24:09 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Malesios", "C.", ""], ["Psarakis", "S.", ""]]}, {"id": "1708.07894", "submitter": "Chrisovaladis Malesios", "authors": "Panagiotis Pergantas, Andreas Tsatsaris, Chrisovalantis Malesios,\n  Georgia Kriparakou, Nikos Demiris, Yiannis Tselentis", "title": "A spatial predictive model for Malaria resurgence in central Greece\n  integrating entomological, environmental and Social data", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0178836", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria constitutes an important cause of human mortality. After 2009 Greece\nexperienced a resurgence of malaria. Here, we develop a modelbased framework\nthat integrates entomological, geographical, social and environmental evidence\nin order to guide the mosquito control efforts and apply this framework to data\nfrom an entomological survey study conducted in Central Greece. Our results\nindicate that malaria transmission risk in Greece is potentially substantial.\nIn addition, specific districts such as seaside, lakeside and rice field\nregions appear to represent potential malaria hotspots in Central Greece. We\nfound that appropriate maps depicting the basic reproduction number, R0 , are\nuseful tools for informing policy makers on the risk of malaria resurgence and\ncan serve as a guide to inform recommendations regarding control measures.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:38:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Pergantas", "Panagiotis", ""], ["Tsatsaris", "Andreas", ""], ["Malesios", "Chrisovalantis", ""], ["Kriparakou", "Georgia", ""], ["Demiris", "Nikos", ""], ["Tselentis", "Yiannis", ""]]}, {"id": "1708.08138", "submitter": "Chrisovaladis Malesios", "authors": "M. Schreiber, C.C. Malesios, and S. Psarakis", "title": "Categorizing Hirsch Index Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing the Hirsch index h and some of its variants for an exploratory\nfactor analysis we discuss whether one of the most important Hirsch-type\nindices, namely the g-index comprises information about not only the size of\nthe productive core but also the impact of the papers in the core. We also\nstudy the effect of logarithmic and square-root transformation of the data\nutilized in the factor analysis. To demonstrate our approach we use a real data\nexample analysing the citation records of 26 physicists compiled from the Web\nof Science.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 20:59:26 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Schreiber", "M.", ""], ["Malesios", "C. C.", ""], ["Psarakis", "S.", ""]]}, {"id": "1708.08376", "submitter": "Alireza Inanlouganji", "authors": "Alireza Inanlougani, T.Agami Reddy and Srinivas Katiamula", "title": "Evaluation of Time-Series, Regression and Neural Network Models for\n  Solar Forecasting: Part I: One-Hour Horizon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to forecast solar irradiation at a specific location over short-time\nhorizons has acquired immense importance. In this paper, we report on analyses\nresults involving statistical and machine learning techniques to predict hourly\nhorizontal solar irradiation at one-hour ahead horizon using data sets from\nthree different cities in the U.S. with different climatic conditions. A simple\nforecast approach that assumes consecutive days are identical serves as a\nbaseline model against which to compare competing forecast alternatives. One\napproach is to use seasonal ARIMA models. Surprisingly, such models are found\nto be poorer than the simple forecast. To account for seasonal variability and\ncapture short-term fluctuations, cloud cover is an obvious variable to\nconsider. Monthly models with cloud cover as regressor were found to outperform\nthe simple forecast model. More sophisticated lagged moving average (LMX)\nmodels were also evaluated, and one of the variants, LMX2, identified at\nmonthly time scales, proved to be the best choice. Finally, the LMX2 model is\ncompared against artificial neural network (ANN) models and the latter proved\nto be more accurate. The companion paper will present algorithms and results of\nhow such models can be used for 4-hr rolling horizon and 24-hr ahead\nforecasting.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:32:07 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Inanlougani", "Alireza", ""], ["Reddy", "T. Agami", ""], ["Katiamula", "Srinivas", ""]]}, {"id": "1708.08378", "submitter": "David Barajas-Solano", "authors": "Xiu Yang, David A. Barajas-Solano, W. Steven Rosenthal, Alexandre M.\n  Tartakovsky", "title": "PDF estimation for power grid systems via sparse regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a numerical approach for estimating the probability density\nfunction (PDF) of quantities of interest (QoIs) of power grid systems subject\nto uncertain power generation and load fluctuations. In our approach,\ngeneration and load fluctuations are modeled by means of autocorrelated-in-time\nrandom processes, which are approximated in terms of a finite set of random\nparameters by means of Karhunen-Lo\\`{e}ve approximations. The map from random\nparameters to QoIs is approximated by means of Hermite polynomial expansions.\nWe propose a new approach based on compressive sensing to estimate the\ncoefficients in the Hermite expansions from a small number of realizations\n(sampling points). Linear transforms identified by iterative rotations are\nintroduced to improve the sparsity of the Hermite representations, exploiting\nthe intrinsic low-dimensional structure of the map. As such, the proposed\napproach significantly reduces the required number of sampling points to\nachieve a given accuracy compared to the standard least squares method. The\nproposed approach is employed to estimate the PDF of relative angular\nvelocities and bus voltages of systems of classical machines driven by\nautocorrelated random generation. More accurate PDF estimates, as measured by\nthe Kullback-Leibler divergence, are achieved using fewer realizations than\nrequired by basic Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 18:16:00 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yang", "Xiu", ""], ["Barajas-Solano", "David A.", ""], ["Rosenthal", "W. Steven", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1708.08522", "submitter": "Edward Kao", "authors": "Edward K. Kao", "title": "Causal Inference Under Network Interference: A Framework for Experiments\n  on Social Networks", "comments": "PhD Thesis at Harvard Department of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No man is an island, as individuals interact and influence one another daily\nin our society. When social influence takes place in experiments on a\npopulation of interconnected individuals, the treatment on a unit may affect\nthe outcomes of other units, a phenomenon known as interference. This thesis\ndevelops a causal framework and inference methodology for experiments where\ninterference takes place on a network of influence (i.e. network interference).\nIn this framework, the network potential outcomes serve as the key quantity and\nflexible building blocks for causal estimands that represent a variety of\nprimary, peer, and total treatment effects. These causal estimands are\nestimated via principled Bayesian imputation of missing outcomes. The theory on\nthe unconfoundedness assumptions leading to simplified imputation highlights\nthe importance of including relevant network covariates in the potential\noutcome model. Additionally, experimental designs that result in balanced\ncovariates and sizes across treatment exposure groups further improve the\ncausal estimate, especially by mitigating potential outcome model\nmis-specification. The true potential outcome model is not typically known in\nreal-world experiments, so the best practice is to account for interference and\nconfounding network covariates through both balanced designs and model-based\nimputation. A full factorial simulated experiment is formulated to demonstrate\nthis principle by comparing performance across different randomization schemes\nduring the design phase and estimators during the analysis phase, under varying\nnetwork topology and true potential outcome models. Overall, this thesis\nasserts that interference is not just a nuisance for analysis but rather an\nopportunity for quantifying and leveraging peer effects in real-world\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 20:50:35 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Kao", "Edward K.", ""]]}, {"id": "1708.08537", "submitter": "Miguel R\\'e", "authors": "Miguel A. R\\'e and Guillermo G. Aguirre Varela", "title": "A Measure of Dependence Between Discrete and Continuous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual Information (MI) is an useful tool for the recognition of mutual\ndependence berween data sets. Differen methods for the estimation of MI have\nbeen developed when both data sets are discrete or when both data sets are\ncontinuous. The MI estimation between a discrete data set and a continuous data\nset has not received so much attention. We present here a method for the\nestimation of MI for this last case based on the kernel density approximation.\nThe calculation may be of interest in diverse contexts. Since MI is closely\nrelated to Jensen Shannon divergence, the method here developed is of\nparticular interest in the problem of sequence segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 21:43:43 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["R\u00e9", "Miguel A.", ""], ["Varela", "Guillermo G. Aguirre", ""]]}, {"id": "1708.08646", "submitter": "Santosh Kumar", "authors": "Santosh Kumar", "title": "Recursion for the smallest eigenvalue density of\n  $\\beta$-Wishart-Laguerre ensemble", "comments": "Published version", "journal-ref": "Journal of Statistical Physics, 2019", "doi": "10.1007/s10955-019-02245-z", "report-no": null, "categories": "math-ph cond-mat.stat-mech math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistics of the smallest eigenvalue of Wishart-Laguerre ensemble is\nimportant from several perspectives. The smallest eigenvalue density is\ntypically expressible in terms of determinants or Pfaffians. These results are\nof utmost significance in understanding the spectral behavior of\nWishart-Laguerre ensembles and, among other things, unveil the underlying\nuniversality aspects in the asymptotic limits. However, obtaining exact and\nexplicit expressions by expanding determinants or Pfaffians becomes impractical\nif large dimension matrices are involved. For the real matrices ($\\beta=1$)\nEdelman has provided an efficient recurrence scheme to work out exact and\nexplicit results for the smallest eigenvalue density which does not involve\ndeterminants or matrices. Very recently, an analogous recurrence scheme has\nbeen obtained for the complex matrices ($\\beta=2$). In the present work we\nextend this to $\\beta$-Wishart-Laguerre ensembles for the case when exponent\n$\\alpha$ in the associated Laguerre weight function, $\\lambda^\\alpha\ne^{-\\beta\\lambda/2}$, is a non-negative integer, while $\\beta$ is positive\nreal. This also gives access to the smallest eigenvalue density of fixed trace\n$\\beta$-Wishart-Laguerre ensemble, as well as moments for both cases. Moreover,\ncomparison with earlier results for the smallest eigenvalue density in terms of\ncertain hypergeometric function of matrix argument results in an effective way\nof evaluating these explicitly. Exact evaluations for large values of $n$ (the\nmatrix dimension) and $\\alpha$ also enable us to compare with Tracy-Widom\ndensity and large deviation results of Katzav and Castillo. We also use our\nresult to obtain the density of the largest of the proper delay times which are\neigenvalues of the Wigner-Smith matrix and are relevant to the problem of\nquantum chaotic scattering.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 09:07:45 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 18:09:52 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kumar", "Santosh", ""]]}, {"id": "1708.09021", "submitter": "Martin Sundin", "authors": "Martin Sundin, Arun Venkitaraman, Magnus Jansson, Saikat Chatterjee", "title": "A Connectedness Constraint for Learning Sparse Graphs", "comments": "5 pages, presented at the European Signal Processing Conference\n  (EUSIPCO) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are naturally sparse objects that are used to study many problems\ninvolving networks, for example, distributed learning and graph signal\nprocessing. In some cases, the graph is not given, but must be learned from the\nproblem and available data. Often it is desirable to learn sparse graphs.\nHowever, making a graph highly sparse can split the graph into several\ndisconnected components, leading to several separate networks. The main\ndifficulty is that connectedness is often treated as a combinatorial property,\nmaking it hard to enforce in e.g. convex optimization problems. In this\narticle, we show how connectedness of undirected graphs can be formulated as an\nanalytical property and can be enforced as a convex constraint. We especially\nshow how the constraint relates to the distributed consensus problem and graph\nLaplacian learning. Using simulated and real data, we perform experiments to\nlearn sparse and connected graphs from data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:45:24 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Sundin", "Martin", ""], ["Venkitaraman", "Arun", ""], ["Jansson", "Magnus", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1708.09439", "submitter": "Behnoush Garshasebi", "authors": "Behnoush Garshasebi, Rahim F. Benekohal", "title": "Effects of Arrival Type and Degree of Saturation on Queue Length\n  Estimation at Signalized Intersections", "comments": "14 pages, 13 tables/ figures, Submitted to 2018 Transportation\n  Research Board (TRB) 97th Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose of this study is evaluation of the relationship between different\narrival types and degree of saturation (X) with overestimations of HCM 2010\nprocedure for estimating the back of queue within a study area. Further\nanalysis is performed to establish the relationship between queue length and\ndelay and also between each of them individually and X in cases with\noverestimation. The analyses are based on the 50th percentile queue lengths for\ndata collected at four signalized intersections along a corridor in 4 time\nperiods (off peak period and AM, Noon and PM peak periods). Based on the\nstatistical test results, arrival type did not play a role in overestimations.\nHowever, there is a significant relationship between the overestimations on\nminor and major street and different ranges of X. On minor streets, about 59%\nof the overestimations are at X values less than half; while near 23% of the\noverestimations are at oversaturation condition with X values greater than 1.\nThe relationship between amount of overestimations and degree of saturation\nshould be established based on the numerical amount of overestimations versus X\nvalues rather than the relative amounts; since the statistical comparison\nbetween the relative amount of overestimations and X values, resulted in a\nwrong idea of the real world condition. There was a significant correlation\nbetween field queue and delay data of the cases with overestimated queue length\nin all cases on major and minor streets. Also, field queue is correlated to X,\nin all cases on minor and major streets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 19:29:14 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Garshasebi", "Behnoush", ""], ["Benekohal", "Rahim F.", ""]]}, {"id": "1708.09443", "submitter": "Luc Villandr\\'e", "authors": "Luc Villandr\\'e, Aur\\'elie Labbe, Ruxandra-Ilinca Ibanescu, Bluma\n  Brenner, Michel Roger, and David A Stephens", "title": "Transmission clusters in the HIV-1 epidemic among men who have sex with\n  men in Montreal, Quebec, Canada", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Several studies have used phylogenetics to investigate Human\nImmunodeficiency Virus (HIV) transmission among Men who have Sex with Men\n(MSMs) in Montreal, Quebec, Canada, revealing many transmission clusters. The\nQuebec HIV genotyping program sequence database now includes viral sequences\nfrom close to 4,000 HIV-positive individuals classified as MSMs. In this paper,\nwe investigate clustering in those data by comparing results from several\nmethods: the conventional Bayesian and maximum likelihood-bootstrap methods,\nand two more recent algorithms, DM-PhyClus, a Bayesian algorithm that produces\na measure of uncertainty for proposed partitions, and the Gap Procedure, a fast\ndistance-based approach. We estimate cluster growth by focusing on recent cases\nin the Primary HIV Infection (PHI) stage. Results. The analyses reveal\nconsiderable overlap between cluster estimates obtained from conventional\nmethods. The Gap Procedure and DM-PhyClus rely on different cluster definitions\nand as a result, suggest moderately different partitions. All estimates lead to\nsimilar conclusions about cluster expansion: several large clusters have\nexperienced sizeable growth, and a few new transmission clusters are likely\nemerging. Conclusions. The lack of a gold standard measure for clustering\nquality makes picking a best estimate among those proposed difficult. Work\naiming to refine clustering criteria would be required to improve estimates.\nNevertheless, the results unanimously stress the role that clusters play in\npromoting HIV incidence among MSMs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 19:39:33 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Villandr\u00e9", "Luc", ""], ["Labbe", "Aur\u00e9lie", ""], ["Ibanescu", "Ruxandra-Ilinca", ""], ["Brenner", "Bluma", ""], ["Roger", "Michel", ""], ["Stephens", "David A", ""]]}, {"id": "1708.09472", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten, Henry R. Scharf, Trevor J. Hefley, Aaron T. Pearse,\n  Mitch D. Weegman", "title": "Animal Movement Models for Migratory Individuals and Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals often exhibit changes in their behavior during migration. Telemetry\ndata provide a way to observe geographic position of animals over time, but not\nnecessarily changes in the dynamics of the movement process. Continuous-time\nmodels allow for statistical predictions of the trajectory in the presence of\nmeasurement error and during periods when the telemetry device did not record\nthe animal's position. However, continuous-time models capable of mimicking\nrealistic trajectories with sufficient detail are computationally challenging\nto fit to large data sets and basic models lack realism in their ability to\ncapture nonstationary dynamics. We present a unified class of animal movement\nmodels that are computationally efficient and provide a suite of approaches for\naccommodating nonstationarity in continuous trajectories due to migration and\ninteractions among individuals. We show how to nest convolution models to\nincorporate interactions among migrating individuals to account for\nnonstationarity and provide inference about dynamic migratory networks. We\ndemonstrate these approaches in two case studies involving migratory birds.\nSpecifically, we used process convolution models with temporal deformation to\naccount for heterogeneity in individual greater white-fronted goose migrations\nin Europe and Iceland and we used nested process convolutions to model dynamic\nmigratory networks in sandhill cranes in North America. The approach we present\naccounts for various forms of temporal heterogeneity in animal movement and is\nnot limited to migratory applications. Furthermore, our models rely on\nwell-established principles for modeling dependent data and leverage modern\napproaches for modeling dynamic networks to help explain animal movement and\nsocial interaction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 21:05:23 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 20:14:42 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Scharf", "Henry R.", ""], ["Hefley", "Trevor J.", ""], ["Pearse", "Aaron T.", ""], ["Weegman", "Mitch D.", ""]]}, {"id": "1708.09481", "submitter": "Dave Osthus", "authors": "Dave Osthus, James Gattiker, Reid Priedhorsky, Sara Y. Del Valle", "title": "Dynamic Bayesian Influenza Forecasting in the United States with\n  Hierarchical Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-17-22749", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely and accurate forecasts of seasonal influenza would assist public\nhealth decision-makers in planning intervention strategies, efficiently\nallocating resources, and possibly saving lives. For these reasons, influenza\nforecasts are consequential. Producing timely and accurate influenza forecasts,\nhowever, have proven challenging due to noisy and limited data, an incomplete\nunderstanding of the disease transmission process, and the mismatch between the\ndisease transmission process and the data-generating process. In this paper, we\nintroduce a dynamic Bayesian (DB) flu forecasting model that exploits model\ndiscrepancy through a hierarchical model. The DB model allows forecasts of\npartially observed flu seasons to borrow discrepancy information from\npreviously observed flu seasons. We compare the DB model to all models that\ncompeted in the CDC's 2015--2016 flu forecasting challenge. The DB model\noutperformed all models, indicating the DB model is a leading influenza\nforecasting model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 21:29:23 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Osthus", "Dave", ""], ["Gattiker", "James", ""], ["Priedhorsky", "Reid", ""], ["Del Valle", "Sara Y.", ""]]}, {"id": "1708.09520", "submitter": "Worapree Ole Maneesoonthorn", "authors": "Worapree Maneesoonthorn, Gael M. Martin and Catherine S. Forbes", "title": "High-Frequency Jump Tests: Which Test Should We Use?", "comments": "This is a revised and shortened version of an earlier paper by the\n  same authors, entitled:\"Dynamic Price Jumps: the Performance of High\n  Frequency Tests and Measures, and the Robustness of Inference\". This current\n  version is forthcoming in Journal of Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an extensive evaluation of price jump tests based on\nhigh-frequency financial data. After providing a concise review of multiple\nalternative tests, we document the size and power of all tests in a range of\nempirically relevant scenarios. Particular focus is given to the robustness of\ntest performance to the presence of jumps in volatility and microstructure\nnoise, and to the impact of sampling frequency. The paper concludes by\nproviding guidelines for empirical researchers about which test to choose in\nany given setting.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 01:16:17 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 11:03:09 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 21:33:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Maneesoonthorn", "Worapree", ""], ["Martin", "Gael M.", ""], ["Forbes", "Catherine S.", ""]]}, {"id": "1708.09526", "submitter": "Hau-tieng Wu", "authors": "C Shen, MG Frasch, HT Wu, CL Herry, M Cao, A Desrochers, G Fecteau, P\n  Burns", "title": "Non-invasive acquisition of fetal ECG from the maternal xyphoid process:\n  a feasibility study in pregnant sheep and a call for open data sets", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6579/aaaaa4", "report-no": null, "categories": "physics.med-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The utility of fetal heart rate (FHR) monitoring can only be\nachieved with an acquisition sampling rate that preserves the underlying\nphysiological information on the millisecond time scale (1000 Hz rather than 4\nHz). For such acquisition, fetal ECG (fECG) is required, rather than the\nultrasound to derive FHR. We tested one recently developed algorithm, SAVER,\nand two widely applied algorithms to extract fECG from a single channel\nmaternal ECG signal recorded over the xyphoid process rather than the routine\nabdominal signal. Approach: At 126dG, ECG was attached to near-term ewe and\nfetal shoulders, manubrium and xyphoid processes (n=12). FECG served as the\nground-truth to which the fetal ECG signal extracted from the\nsimultaneously-acquired maternal ECG was compared. All fetuses were in good\nhealth during surgery (pH 7.29+/-0.03, pO2 33.2+/-8.4, pCO2 56.0+/-7.8, O2Sat\n78.3+/-7.6, lactate 2.8+/-0.6, BE -0.3+/-2.4). Main result: In all animals,\nsingle lead fECG extraction algorithm could not extract fECG from the maternal\nECG signal over the xyphoid process with the F1 less than 50%. Significance:\nThe applied fECG extraction algorithms might be unsuitable for the maternal ECG\nsignal over the xyphoid process, or the latter does not contain strong enough\nfECG signal, although the lead is near the mother's abdomen. Fetal sheep model\nis widely used to mimic various fetal conditions, yet ECG recordings in a\npublic data set form are not available to test the predictive ability of fECG\nand FHR. We are making this data set openly available to other researchers to\nfoster non-invasive fECG acquisition in this animal model.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 01:34:09 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 00:26:29 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Shen", "C", ""], ["Frasch", "MG", ""], ["Wu", "HT", ""], ["Herry", "CL", ""], ["Cao", "M", ""], ["Desrochers", "A", ""], ["Fecteau", "G", ""], ["Burns", "P", ""]]}, {"id": "1708.09663", "submitter": "Gilles Guillot", "authors": "Gilles Guillot, Pierre Benoit, Savvas Kinalis, Fran\\c{c}ois Bastardie,\n  Valerio Bartolino", "title": "Enhancing and comparing methods for the detection of fishing activity\n  from Vessel Monitoring System data", "comments": "22 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vessel Monitoring System (VMS) data provide information about speed and\nposition of fishing vessels. This opens the door to methods of estimating and\nmapping fishing effort with a high level of detail. To addess this task, we\npropose a new method belonging to the class of hidden Markov models (HMM) that\naccounts for autocorrelation in time along the fishing events and offers a good\ntrade-off between model complexity and computational efficiency. We carry out\nan objective comparison between this method and two competing approaches on a\nset of VMS data from Denmark for which the true activity is known from on-board\nsensors. The DMKMG approach proposed outperformed the competitors approach with\n6% and 15% more accurate estimates in the vessel-by-vessel and trip-by-trip\ncase, respectively. In addition, these better performances are not paid in\nterms of computation time. We also showcase our method on an extensive dataset\nfrom Sweden. A quick (real-time) data processing has the potential to change\nhow fisheries can be better harmonized to other utilisation of the seas and\nfill the gap between the local scales at which fishing pressure and stock\ndepletion occurs with the large temporal and spatial scales of traditional\nfisheries assessment and management. The computer code developped in this work\nis made publicly available as an R package from\nhttp://www2.imm.dtu.dk/~gigu/HMM-VMS\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 11:06:15 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Guillot", "Gilles", ""], ["Benoit", "Pierre", ""], ["Kinalis", "Savvas", ""], ["Bastardie", "Fran\u00e7ois", ""], ["Bartolino", "Valerio", ""]]}, {"id": "1708.09695", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Ayanendranath Basu and Leandro Pardo", "title": "Robust Wald-Type Tests under Random Censoring with Applications to\n  Clinical Trial Analyses", "comments": "Pre-print, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomly censored survival data are frequently encountered in applied\nsciences including biomedical or reliability applications and clinical trial\nanalyses. Testing the significance of statistical hypotheses is crucial in such\nanalyses to get conclusive inference but the existing likelihood based tests,\nunder a fully parametric model, are extremely non-robust against outliers in\nthe data. Although, there exists a few robust parameter estimators (e.g.,\nM-estimators and minimum density power divergence estimators) given randomly\ncensored data, there is hardly any robust testing procedure available in the\nliterature in this context. One of the major difficulties in this context is\nthe construction of a suitable consistent estimator of the asymptotic variance\nof M estimators; the latter is a function of the unknown censoring\ndistribution. In this paper, we take the first step in this direction by\nproposing a consistent estimator of asymptotic variance of the M-estimators\nbased on randomly censored data without any assumption on the form of the\ncensoring scheme. We then describe and study a class of robust Wald-type tests\nfor parametric statistical hypothesis, both simple as well as composite, under\nsuch set-up, along with their general asymptotic and robustness properties.\nRobust tests for comparing two independent randomly censored samples and robust\ntests against one sided alternatives are also discussed. Their advantages and\nusefulness are demonstrated for the tests based on the minimum density power\ndivergence estimators with specific attention to clinical trial analyses.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:03:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 18:39:52 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1708.09762", "submitter": "Aina Frau-Pascual", "authors": "Michael Eickenberg, Aina Frau-Pascual, Andr\\'es Hoyos-Idrobo", "title": "Gaussian Processes for HRF estimation for BOLD fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-parametric joint estimation method for fMRI task activation\nvalues and the hemodynamic response function (HRF). The HRF is modeled as a\nGaussian process, making continuous evaluation possible for jittered paradigms\nand providing a variance estimate at each point.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 14:56:14 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Eickenberg", "Michael", ""], ["Frau-Pascual", "Aina", ""], ["Hoyos-Idrobo", "Andr\u00e9s", ""]]}, {"id": "1708.09823", "submitter": "Nicky Best", "authors": "Nigel Dallow, Nicky Best and Timothy Montague", "title": "Better Decision Making in Drug Development Through Adoption of Formal\n  Prior Elicitation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continued increase in the use of Bayesian methods in drug\ndevelopment, there is a need for statisticians to have tools to develop robust\nand defensible informative prior distributions. Whilst relevant empirical data\nshould, where possible, provide the basis for such priors, it is often the case\nthat limitations in data and/or our understanding may preclude direct\nconstruction of a data-based prior. Formal expert elicitation methods are a key\ntechnique that can be used to determine priors in these situations. Within\nGlaxoSmithKline (GSK), we have adopted a structured approach to prior\nelicitation based on the SHELF elicitation framework, and routinely use this in\nconjunction with calculation of probability of success (assurance) of the next\nstudy(s) to inform internal decision making at key project milestones. The aim\nof this paper is to share our experiences of embedding the use of prior\nelicitation within a large pharmaceutical company, highlighting both the\nbenefits and challenges of prior elicitation through a series of case studies.\nWe have found that putting team beliefs into the shape of a quantitative\nprobability distribution provides a firm anchor for all internal decision\nmaking, enabling teams to provide investment boards with formally appropriate\nestimates of the probability of trial success as well as robust plans for\ninterim decision rules where appropriate. As an added benefit, the elicitation\nprocess provides transparency about the beliefs and risks of the potential\nmedicine, ultimately enabling better portfolio and company-wide decision\nmaking.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:13:52 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Dallow", "Nigel", ""], ["Best", "Nicky", ""], ["Montague", "Timothy", ""]]}, {"id": "1708.09852", "submitter": "Wesley Pegden", "authors": "Maria Chikina, Alan Frieze, Wesley Pegden", "title": "An analysis of the Act 43 Wisconsin Assembly district map using the\n  $\\sqrt{\\varepsilon}$ test", "comments": "4 pages, 1 table (data processing notes added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work [arXiv:1608.02014], we developed a rigorous statistical test\nfor outlier status in a reversible Markov Chain, and demonstrated its\nutilization with an application to detecting gerrymandering in Pennsylvania's\nCongressional districting. In this note, we apply our test to the current (Act\n43) assembly districting of the state of Wisconsin, and find that the\ndistricting is indeed an outlier among the the landscape of valid districtings\nof Wisconsin. Outlier status is significant at betwee p=.0002 and p=.0008,\ndepending on assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:59:19 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 15:09:29 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Chikina", "Maria", ""], ["Frieze", "Alan", ""], ["Pegden", "Wesley", ""]]}]