[{"id": "1204.0151", "submitter": "M Taghizadeh-Popp Dr.", "authors": "M. Taghizadeh-Popp, K. Ozogany, Z. Racz, E. Regoes, A. S. Szalay", "title": "Distribution of Maximal Luminosity of Galaxies in the Sloan Digital Sky\n  Survey", "comments": "Accepted for publication in ApJ", "journal-ref": null, "doi": "10.1088/0004-637X/759/2/100", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme value statistics (EVS) is applied to the distribution of galaxy\nluminosities in the Sloan Digital Sky Survey (SDSS). We analyze the DR8 Main\nGalaxy Sample (MGS), as well as the Luminous Red Galaxies (LRG). Maximal\nluminosities are sampled from batches consisting of elongated pencil beams in\nthe radial direction of sight. For the MGS, results suggest a small and\npositive tail index $\\xi$, effectively ruling out the possibility of having a\nfinite maximum cutoff luminosity, and implying that the luminosity distribution\nfunction may decay as a power law at the high luminosity end. Assuming,\nhowever, $\\xi=0$, a non-parametric comparison of the maximal luminosities with\nthe Fisher-Tippett-Gumbel distribution (limit distribution for variables\ndistributed by the Schechter fit) indicates a good agreement provided\nuncertainties arising both from the finite batch size and from the batch size\ndistribution are accounted for. For a volume limited sample of LRGs, results\nshow that they can be described as being the extremes of a luminosity\ndistribution with an exponentially decaying tail, provided the uncertainties\nrelated to batch-size distribution are taken care of.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 00:43:49 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 19:19:48 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Taghizadeh-Popp", "M.", ""], ["Ozogany", "K.", ""], ["Racz", "Z.", ""], ["Regoes", "E.", ""], ["Szalay", "A. S.", ""]]}, {"id": "1204.0168", "submitter": "Wen Dong", "authors": "Wen Dong, Katherine A. Heller and Alex Sandy Pentland", "title": "Modeling Infection with Multi-agent Dynamics", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-29047-3_21", "report-no": null, "categories": "stat.AP cs.MA cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing the ability to comprehensively study infections in small\npopulations enables us to improve epidemic models and better advise individuals\nabout potential risks to their health. We currently have a limited\nunderstanding of how infections spread within a small population because it has\nbeen difficult to closely track an infection within a complete community. The\npaper presents data closely tracking the spread of an infection centered on a\nstudent dormitory, collected by leveraging the residents' use of cellular\nphones. The data are based on daily symptom surveys taken over a period of four\nmonths and proximity tracking through cellular phones. We demonstrate that\nusing a Bayesian, discrete-time multi-agent model of infection to model\nreal-world symptom reports and proximity tracking records gives us important\ninsights about infec-tions in small populations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 05:24:32 GMT"}, {"version": "v2", "created": "Sat, 11 Oct 2014 13:44:06 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Dong", "Wen", ""], ["Heller", "Katherine A.", ""], ["Pentland", "Alex Sandy", ""]]}, {"id": "1204.0307", "submitter": "Alexander Shen", "authors": "Alexander Shen", "title": "Elections and statistics: the case of \"United Russia\", 2009-2020", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey contains statistics on elections in Russia published in different\nplaces and available online. This data is discussed from the viewpoint of\nstatistical model selection. The current version is updated including the\nmaterials up to July, 2020 voting on constitutional changes, Belarus 2020\nelections and papers that appeared in 2020; most of the data are not consistent\nwith the assumption of fair elections.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 03:52:13 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 21:38:17 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 12:17:10 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 20:17:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Shen", "Alexander", ""]]}, {"id": "1204.0549", "submitter": "Tlemcani Mounir", "authors": "Zohra Benkamra, Mekki Terbeche and Mounir Tlemcani", "title": "Bayesian sequential estimation of the reliability of a parallel-series\n  system", "comments": "12 pages", "journal-ref": "Applied Mathematics and Computation 2013, Volume 219, Issue 23, 1\n  August 2013, Pages 10842--10852", "doi": "10.1016/j.amc.2013.05.010", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a risk-averse solution to the problem of estimating the reliability\nof a parallel-series system. We adopt a beta-binomial model for components\nreliabilities, and assume that the total sample size for the experience is\nfixed. The allocation at subsystems or components level may be random. Based on\nthe sampling schemes for parallel and series systems separately, we propose a\nhybrid sequential scheme for the parallel-series system. Asymptotic optimality\nof the Bayes risk associated with quadratic loss is proved with the help of\nmartingale convergence properties.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 22:51:25 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Benkamra", "Zohra", ""], ["Terbeche", "Mekki", ""], ["Tlemcani", "Mounir", ""]]}, {"id": "1204.1022", "submitter": "Petra Friederichs", "authors": "Petra Friederichs and Thordis L. Thorarinsdottir", "title": "Forecast verification for extreme value distributions with an\n  application to probabilistic peak wind prediction", "comments": null, "journal-ref": "Environmetrics 23 (2012) 579-594", "doi": "10.1002/env.2176", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions of the uncertainty associated with extreme events are a vital\ncomponent of any prediction system for such events. Consequently, the\nprediction system ought to be probabilistic in nature, with the predictions\ntaking the form of probability distributions. This paper concerns probabilistic\nprediction systems where the data is assumed to follow either a generalized\nextreme value distribution (GEV) or a generalized Pareto distribution (GPD). In\nthis setting, the properties of proper scoring rules which facilitate the\nassessment of the prediction uncertainty are investigated and closed-from\nexpressions for the continuous ranked probability score (CRPS) are provided. In\nan application to peak wind prediction, the predictive performance of a GEV\nmodel under maximum likelihood estimation, optimum score estimation with the\nCRPS, and a Bayesian framework are compared. The Bayesian inference yields the\nhighest overall prediction skill and is shown to be a valuable tool for\ncovariate selection, while the predictions obtained under optimum CRPS\nestimation are the sharpest and give the best performance for high thresholds\nand quantiles.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 18:32:41 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 09:02:16 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 12:40:48 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Friederichs", "Petra", ""], ["Thorarinsdottir", "Thordis L.", ""]]}, {"id": "1204.1131", "submitter": "Felipe Dimer de Oliveira", "authors": "Felipe Dimer de Oliveira", "title": "Power of earthquake cluster detection tests", "comments": "14 pages, 5 Figures", "journal-ref": null, "doi": "10.1029/2012GL052130", "report-no": null, "categories": "stat.AP physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the global earthquake catalogue for indications of non-Poissonian\nattributes has been an area of intense research, especially since the 2011\nTohoku earthquake. The usual approach is to test statistically for the\nhypothesis that the global earthquake catalogue is well explained by a\nPoissonian process. In this paper we analyse one aspect of this problem which\nhas been disregarded by the literature: the power of such tests to detect\nnon-Poissonian features if they existed; that is, the probability of type II\nstatistical errors. We argue that the low frequency of large events and the\nbrevity of our earthquake catalogues reduces the power of the statistical tests\nso that an unequivocal answer for this question is not granted. We do this by\nproviding a counter example of a stochastic process that is clustered by\nconstruction and by analysing the resulting distribution of p-values given by\nthe current tests.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 06:58:25 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["de Oliveira", "Felipe Dimer", ""]]}, {"id": "1204.1571", "submitter": "Luis Carvalho", "authors": "Luis E. Carvalho", "title": "Bayesian Centroid Estimation for Motif Discovery", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0080511", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological sequences may contain patterns that are signal important\nbiomolecular functions; a classical example is regulation of gene expression by\ntranscription factors that bind to specific patterns in genomic promoter\nregions. In motif discovery we are given a set of sequences that share a common\nmotif and aim to identify not only the motif composition, but also the binding\nsites in each sequence of the set. We present a Bayesian model that is an\nextended version of the model adopted by the Gibbs motif sampler, and propose a\nnew centroid estimator that arises from a refined and meaningful loss function\nfor binding site inference. We discuss the main advantages of centroid\nestimation for motif discovery, including computational convenience, and how\nits principled derivation offers further insights about the posterior\ndistribution of binding site configurations. We also illustrate, using\nsimulated and real datasets, that the centroid estimator can differ from the\nmaximum a posteriori estimator.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 21:58:47 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Carvalho", "Luis E.", ""]]}, {"id": "1204.1792", "submitter": "Huisi Tong", "authors": "Huisi Tong, Hao Zhang, Huadong Meng, Xiqin Wang", "title": "The Recursive Form of Error Bounds for RFS State and Observation with\n  Pd<1", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2245324", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the target tracking and its engineering applications, recursive state\nestimation of the target is of fundamental importance. This paper presents a\nrecursive performance bound for dynamic estimation and filtering problem, in\nthe framework of the finite set statistics for the first time. The number of\ntracking algorithms with set-valued observations and state of targets is\nincreased sharply recently. Nevertheless, the bound for these algorithms has\nnot been fully discussed. Treating the measurement as set, this bound can be\napplied when the probability of detection is less than unity. Moreover, the\nstate is treated as set, which is singleton or empty with certain probability\nand accounts for the appearance and the disappearance of the targets. When the\nexistence of the target state is certain, our bound is as same as the most\naccurate results of the bound with probability of detection is less than unity\nin the framework of random vector statistics. When the uncertainty is taken\ninto account, both linear and non-linear applications are presented to confirm\nthe theory and reveal this bound is more general than previous bounds in the\nframework of random vector statistics.In fact, the collection of such\nmeasurements could be treated as a random finite set (RFS).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 05:07:39 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Tong", "Huisi", ""], ["Zhang", "Hao", ""], ["Meng", "Huadong", ""], ["Wang", "Xiqin", ""]]}, {"id": "1204.1894", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Accounting for the Uncertainty in the Evaluation of Percentile Ranks", "comments": "Journal of the American Society for Information Science and\n  Technology (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper entitled \"Inconsistencies of Recently Proposed Citation\nImpact Indicators and how to Avoid Them,\" Schreiber (2012, at arXiv:1202.3861)\nproposed (i) a method to assess tied ranks consistently and (ii) fractional\nattribution to percentile ranks in the case of relatively small samples (e.g.,\nfor n < 100). Schreiber's solution to the problem of how to handle tied ranks\nis convincing, in my opinion (cf. Pudovkin & Garfield, 2009). The fractional\nattribution, however, is computationally intensive and cannot be done manually\nfor even moderately large batches of documents. Schreiber attributed scores\nfractionally to the six percentile rank classes used in the Science and\nEngineering Indicators of the U.S. National Science Board, and thus missed, in\nmy opinion, the point that fractional attribution at the level of hundred\npercentiles-or equivalently quantiles as the continuous random variable-is only\na linear, and therefore much less complex problem. Given the quantile-values,\nthe non-linear attribution to the six classes or any other evaluation scheme is\nthen a question of aggregation. A new routine based on these principles\n(including Schreiber's solution for tied ranks) is made available as software\nfor the assessment of documents retrieved from the Web of Science (at\nhttp://www.leydesdorff.net/software/i3).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 15:02:44 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "1204.1935", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "New Sequential Methods for Detecting Portscanners", "comments": "11 pages, 5 figures, the mathematical theory of the detection\n  algorithm has been presented in SPIE conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new sequential methods for detecting port-scan\nattackers which routinely perform random \"portscans\" of IP addresses to find\nvulnerable servers to compromise. In addition to rigorously control the\nprobability of falsely implicating benign remote hosts as malicious, our method\nperforms significantly faster than other current solutions. Moreover, our\nmethod guarantees that the maximum amount of observational time is bounded. In\ncontrast to the previous most effective method, Threshold Random Walk\nAlgorithm, which is explicit and analytical in nature, our proposed algorithm\ninvolve parameters to be determined by numerical methods. We have developed\ncomputational techniques such as iterative minimax optimization for quick\ndetermination of the parameters of the new detection algorithm. A framework of\nmulti-valued decision for testing portscanners is also proposed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 17:38:47 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1204.1937", "submitter": "Giovanni Montana", "authors": "Matt Silver, Eva Janousova, Xue Hua, Paul M. Thompson and Giovanni\n  Montana", "title": "Identification of gene pathways implicated in Alzheimer's disease using\n  longitudinal imaging phenotypes with sparse regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for the detection of gene pathways associated with a\nmultivariate quantitative trait, and use it to identify causal pathways\nassociated with an imaging endophenotype characteristic of longitudinal\nstructural change in the brains of patients with Alzheimer's disease (AD). Our\nmethod, known as pathways sparse reduced-rank regression (PsRRR), uses group\nlasso penalised regression to jointly model the effects of genome-wide single\nnucleotide polymorphisms (SNPs), grouped into functional pathways using prior\nknowledge of gene-gene interactions. Pathways are ranked in order of importance\nusing a resampling strategy that exploits finite sample variability. Our\napplication study uses whole genome scans and MR images from 464 subjects in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database. 66,182 SNPs\nare mapped to 185 gene pathways from the KEGG pathways database. Voxel-wise\nimaging signatures characteristic of AD are obtained by analysing 3D patterns\nof structural change at 6, 12 and 24 months relative to baseline. High-ranking,\nAD endophenotype-associated pathways in our study include those describing\nchemokine, Jak-stat and insulin signalling pathways, and tight junction\ninteractions. All of these have been previously implicated in AD biology. In a\nsecondary analysis, we investigate SNPs and genes that may be driving pathway\nselection, and identify a number of previously validated AD genes including\nCR1, APOE and TOMM40.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 17:43:12 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Silver", "Matt", ""], ["Janousova", "Eva", ""], ["Hua", "Xue", ""], ["Thompson", "Paul M.", ""], ["Montana", "Giovanni", ""]]}, {"id": "1204.2098", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss", "title": "Bayesian Nonstationary Spatial Modeling for Very Large Datasets", "comments": "16 pages, 2 color figures", "journal-ref": "Environmetrics 24 (2013) 189-200", "doi": "10.1002/env.2200", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of modern high-resolution measuring instruments\nmounted on satellites, planes, ground-based vehicles and monitoring stations, a\nneed has arisen for statistical methods suitable for the analysis of large\nspatial datasets observed on large spatial domains. Statistical analyses of\nsuch datasets provide two main challenges: First, traditional\nspatial-statistical techniques are often unable to handle large numbers of\nobservations in a computationally feasible way. Second, for large and\nheterogeneous spatial domains, it is often not appropriate to assume that a\nprocess of interest is stationary over the entire domain.\n  We address the first challenge by using a model combining a low-rank\ncomponent, which allows for flexible modeling of medium-to-long-range\ndependence via a set of spatial basis functions, with a tapered remainder\ncomponent, which allows for modeling of local dependence using a compactly\nsupported covariance function. Addressing the second challenge, we propose two\nextensions to this model that result in increased flexibility: First, the model\nis parameterized based on a nonstationary Matern covariance, where the\nparameters vary smoothly across space. Second, in our fully Bayesian model, all\ncomponents and parameters are considered random, including the number,\nlocations, and shapes of the basis functions used in the low-rank component.\n  Using simulated data and a real-world dataset of high-resolution soil\nmeasurements, we show that both extensions can result in substantial\nimprovements over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 10:36:45 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 10:03:30 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2012 11:28:53 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2012 10:18:06 GMT"}, {"version": "v5", "created": "Fri, 21 Dec 2012 15:58:51 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Katzfuss", "Matthias", ""]]}, {"id": "1204.2257", "submitter": "Jiahao Chen", "authors": "Jiahao Chen, Troy Van Voorhis and Alan Edelman", "title": "Partial freeness of random matrices", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the implications of free probability for random matrices. From\nrules for calculating all possible joint moments of two free random matrices,\nwe develop a notion of partial freeness which is quantified by the breakdown of\nthese rules. We provide a combinatorial interpretation for partial freeness as\nthe presence of closed paths in Hilbert space defined by particular joint\nmoments. We also discuss how asymptotic moment expansions provide an error term\non the density of states. We present MATLAB code for the calculation of moments\nand free cumulants of arbitrary random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 19:47:18 GMT"}, {"version": "v2", "created": "Tue, 21 May 2013 20:08:01 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Chen", "Jiahao", ""], ["Van Voorhis", "Troy", ""], ["Edelman", "Alan", ""]]}, {"id": "1204.2353", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard\n  Thresholding", "comments": "variable selection, pseudo-hard thresholding", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in linear models plays a pivotal role in modern\nstatistics. Hard-thresholding methods such as $l_0$ regularization are\ntheoretically ideal but computationally infeasible. In this paper, we propose a\nnew approach, called the LAGS, short for \"least absulute gradient selector\", to\nthis challenging yet interesting problem by mimicking the discrete selection\nprocess of $l_0$ regularization. To estimate $\\beta$ under the influence of\nnoise, we consider, nevertheless, the following convex program [\\hat{\\beta} =\n\\textrm{arg min}\\frac{1}{n}\\|X^{T}(y - X\\beta)\\|_1 + \\lambda_n\\sum_{i =\n1}^pw_i(y;X;n)|\\beta_i|]\n  $\\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and\n$n$ is the weights on different $\\beta_i$; $n$ is the sample size.\nSurprisingly, we shall show in the paper, both geometrically and analytically,\nthat LAGS enjoys two attractive properties: (1) LAGS demonstrates discrete\nselection behavior and hard thresholding property as $l_0$ regularization by\nstrategically chosen $w_i$, we call this property \"pseudo-hard thresholding\";\n(2) Asymptotically, LAGS is consistent and capable of discovering the true\nmodel; nonasymptotically, LAGS is capable of identifying the sparsity in the\nmodel and the prediction error of the coefficients is bounded at the noise\nlevel up to a logarithmic factor---$\\log p$, where $p$ is the number of\npredictors.\n  Computationally, LAGS can be solved efficiently by convex program routines\nfor its convexity or by simplex algorithm after recasting it into a linear\nprogram. The numeric simulation shows that LAGS is superior compared to\nsoft-thresholding methods in terms of mean squared error and parsimony of the\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 06:57:39 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 05:28:28 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2012 23:52:09 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2012 03:56:01 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1204.2420", "submitter": "Alberto Hernando", "authors": "A. Hernando, A. Plastino", "title": "Variational Principle underlying Scale Invariant Social Systems", "comments": null, "journal-ref": null, "doi": "10.1140/epjb/e2012-30313-x", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  MaxEnt's variational principle, in conjunction with Shannon's logarithmic\ninformation measure, yields only exponential functional forms in\nstraightforward fashion. In this communication we show how to overcome this\nlimitation via the incorporation, into the variational process, of suitable\ndynamical information. As a consequence, we are able to formulate a somewhat\ngeneralized Shannonian Maximum Entropy approach which provides a unifying\n\"thermodynamic-like\" explanation for the scale-invariant phenomena observed in\nsocial contexts, as city-population distributions. We confirm the MaxEnt\npredictions by means of numerical experiments with random walkers, and compare\nthem with some empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 11:39:32 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hernando", "A.", ""], ["Plastino", "A.", ""]]}, {"id": "1204.2422", "submitter": "Alberto Hernando", "authors": "A. Hernando, A. Plastino", "title": "Scale-invariance underlying the logistic equation and its social\n  applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.physleta.2012.10.054", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  On the basis of dynamical principles we derive the Logistic Equation (LE),\nwidely employed (among multiple applications) in the simulation of population\ngrowth, and demonstrate that scale-invariance and a mean-value constraint are\nsufficient and necessary conditions for obtaining it. We also generalize the LE\nto multi-component systems and show that the above dynamical mechanisms\nunderlie large number of scale-free processes. Examples are presented regarding\ncity-populations, diffusion in complex networks, and popularity of\ntechnological products, all of them obeying the multi-component logistic\nequation in an either stochastic or deterministic way. So as to assess the\npredictability-power of our present formalism, we advance a prediction,\nregarding the next 60 months, for the number of users of the three main web\nbrowsers (Explorer, Firefox and Chrome) popularly referred as \"Browser Wars\".\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 11:52:26 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hernando", "A.", ""], ["Plastino", "A.", ""]]}, {"id": "1204.2486", "submitter": "Roy  Mendelssohn", "authors": "Roy Mendelssohn and CIndy Bessey", "title": "A Reanalysis of North Pacific Sea Surface Temperatures Using State-Space\n  Techniques: The PDO In A New Light", "comments": "21 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  North Pacific sea surface temperatures (SST), as used in estimating the PDO,\nare reanalyzed using state-space decomposition and subspace identification\ntechniques. The reanalysis presents a very different picture of SST in this\nregion. The first common trend reflects a global warming signal. The second\ncommon trend modifies this for areas that underwent a sharper warming (cooling)\nstarting in the early 1970's. This trend is also related to dynamics in the\ntropics and in Arctic sea ice extent. The third common trend is a superposition\nof changes in pressure centers on the long-term global warming signal. The\nfourth common trend is the trend that is contained in the original PDO series\nif analyzed by state-space techniques, and is identical to the trend in the\nNorth Pacific High. The first two common stochastic cycles capture the original\nPDO and so-called \"Victoria mode\", showing that these series are dominated by\nstationary behavior..\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 16:03:10 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Mendelssohn", "Roy", ""], ["Bessey", "CIndy", ""]]}, {"id": "1204.2515", "submitter": "Roy  Mendelssohn", "authors": "Cindy Bessey and Roy Mendelssohn", "title": "An Analysis of North Pacific Subsurface Temperatures Using State-Space\n  Techniques", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  North Pacific subsurface temperature data from the Simple Ocean Data\nAssimilation model at 10m, 50m, 75m, 100m and 150m depths, are analyzed using a\ncombination of state-space decomposition and subspace identification techniques\nto examine the spatial structure of thermal variability within the upper water\ncolumn. We identify four common trends from our analysis that display the major\nbroad-scale patterns in the North Pacific over a 47 year period (1958-2004):\n(1) a basin-wide near-surface warming trend that identifies the mid 1980's as a\nchange point from a cooling to a warming trend; (2) a contrasting cooling in\nthe central basin and warming along the coast of North America that began in\nthe early 1970's; (3) a cooling along the transition zone and the west coast of\nNorth America that becomes dominant around 1998; (4) and contrasting\ndifferences in the subarctic and subtropical gyres displaying differences in\nprocesses at each depth. We also provide a detailed analysis of the temperature\nvariability at four chosen locations: 52.5N 142.5W (Gulf of Alaska), 37.5N\n172.5W (central basin), 37.5N 137.5W (off coast of California), and 27.5N\n137.5W (off coast of Baja California) for both 10m and 150m depths. These\nresults identify subsurface structure, regional heterogeneity, and they also\ndisplay important differences and similarities in the patterns of subsurface\ntemperature variability when compared to previously published temperature\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 18:29:41 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Bessey", "Cindy", ""], ["Mendelssohn", "Roy", ""]]}, {"id": "1204.2994", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti and Todd Zickler", "title": "Image Restoration with Signal-dependent Camera Noise", "comments": "6 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a fast iterative algorithm for image denoising and\ndeconvolution with signal-dependent observation noise. We use an optimization\nstrategy based on variable splitting that adapts traditional Gaussian\nnoise-based restoration algorithms to account for the observed image being\ncorrupted by mixed Poisson-Gaussian noise and quantization errors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 13:48:27 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Zickler", "Todd", ""]]}, {"id": "1204.3005", "submitter": "Wassim Jouini", "authors": "Wassim Jouini and Marco Di Felice and Luciano Bononi and Christophe\n  Moy", "title": "Collaboration and Coordination in Secondary Networks for Opportunistic\n  Spectrum Access", "comments": "28 pages. Paper submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we address the general case of a coordinated secondary network\nwilling to exploit communication opportunities left vacant by a licensed\nprimary network. Since secondary users (SU) usually have no prior knowledge on\nthe environment, they need to learn the availability of each channel through\nsensing techniques, which however can be prone to detection errors. We argue\nthat cooperation among secondary users can enable efficient learning and\ncoordination mechanisms in order to maximize the spectrum exploitation by SUs,\nwhile minimizing the impact on the primary network. To this goal, we provide\nthree novel contributions in this paper. First, we formulate the spectrum\nselection in secondary networks as an instance of the Multi-Armed Bandit (MAB)\nproblem, and we extend the analysis to the collaboration learning case, in\nwhich each SU learns the spectrum occupation, and shares this information with\nother SUs. We show that collaboration among SUs can mitigate the impact of\nsensing errors on system performance, and improve the convergence of the\nlearning process to the optimal solution. Second, we integrate the learning\nalgorithms with two collaboration techniques based on modified versions of the\nHungarian algorithm and of the Round Robin algorithm that allows reducing the\ninterference among SUs. Third, we derive fundamental limits to the performance\nof cooperative learning algorithms based on Upper Confidence Bound (UCB)\npolicies in a symmetric scenario where all SU have the same perception of the\nquality of the resources. Extensive simulation results confirm the\neffectiveness of our joint learning-collaboration algorithm in protecting the\noperations of Primary Users (PUs), while maximizing the performance of SUs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 14:15:53 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Jouini", "Wassim", ""], ["Di Felice", "Marco", ""], ["Bononi", "Luciano", ""], ["Moy", "Christophe", ""]]}, {"id": "1204.3339", "submitter": "Guilherme Pumi", "authors": "Guilherme Pumi and S\\'ilvia R. C. Lopes", "title": "Parameterization of Copulas and Covariance Decay of Stochastic Processes\n  with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the problem of constructing stochastic processes with a\npredetermined covariance decay by parameterizing its marginals and a given\nfamily of copulas. We present several examples to illustrate the theory,\nincluding the important Gaussian and Euclidean families of copulas. We\nassociate the theory to common applied time series models and present a general\nmethodology to estimate a given parameter of interest identifiable through the\nprocess' covariance decay. To exemplify the proposed methodology, we present\nsimple Monte Carlo applications to parameter estimation in time series. The\nmethodology is also applied to the S&P500 US stock market index.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 00:54:45 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Pumi", "Guilherme", ""], ["Lopes", "S\u00edlvia R. C.", ""]]}, {"id": "1204.3358", "submitter": "Peter Ruckdeschel", "authors": "Peter Ruckdeschel, Bernhard Spangl and Daria Pupashenko", "title": "Robust Kalman tracking and smoothing with propagating and\n  non-propagating outliers", "comments": "27 pages, 12 figures, 2 tables", "journal-ref": "Statistical Papers 55(1), 93-123", "doi": "10.1007/s00362-012-0496-4", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common situation in filtering where classical Kalman filtering does not\nperform particularly well is tracking in the presence of propagating outliers.\nThis calls for robustness understood in a distributional sense, i.e.; we\nenlarge the distribution assumptions made in the ideal model by suitable\nneighborhoods. Based on optimality results for distributional-robust Kalman\nfiltering from Ruckdeschel[01,10], we propose new robust recursive filters and\nsmoothers designed for this purpose as well as specialized versions for\nnon-propagating outliers. We apply these procedures in the context of a GPS\nproblem arising in the car industry. To better understand these filters, we\nstudy their behavior at stylized outlier patterns (for which they are not\ndesigned) and compare them to other approaches for the tracking problem.\nFinally, in a simulation study we discuss efficiency of our procedures in\ncomparison to competitors.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 04:27:21 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 19:28:11 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Ruckdeschel", "Peter", ""], ["Spangl", "Bernhard", ""], ["Pupashenko", "Daria", ""]]}, {"id": "1204.3687", "submitter": "Benjamin Shaby", "authors": "Benjamin A Shaby", "title": "The open-faced sandwich adjustment for MCMC using estimating functions", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2013.842174", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The situation frequently arises where working with the likelihood function is\nproblematic. This can happen for several reasons---perhaps the likelihood is\nprohibitively computationally expensive, perhaps it lacks some robustness\nproperty, or perhaps it is simply not known for the model under consideration.\nIn these cases, it is often possible to specify alternative functions of the\nparameters and the data that can be maximized to obtain asymptotically normal\nestimates. However, these scenarios present obvious problems if one is\ninterested in applying Bayesian techniques. Here we describe open-faced\nsandwich adjustment, a way to incorporate a wide class of non-likelihood\nobjective functions within Bayesian-like models to obtain asymptotically valid\nparameter estimates and inference via MCMC. Two simulation examples show that\nthe method provides accurate frequentist uncertainty estimates. The open-faced\nsandwich adjustment is applied to a Poisson spatio-temporal model to analyze an\nornithology dataset from the citizen science initiative eBird.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 02:15:15 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Shaby", "Benjamin A", ""]]}, {"id": "1204.3748", "submitter": "Klaus Frick", "authors": "Klaus Frick, Philipp Marnitz, Axel Munk", "title": "Statistical Multiresolution Estimation for Variational Imaging: With an\n  Application in Poisson-Biophotonics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a spatially-adaptive method for image reconstruction\nthat is based on the concept of statistical multiresolution estimation as\nintroduced in [Frick K, Marnitz P, and Munk A. \"Statistical multiresolution\nDantzig estimation in imaging: Fundamental concepts and algorithmic framework\".\nElectron. J. Stat., 6:231-268, 2012]. It constitutes a variational\nregularization technique that uses an supremum-type distance measure as\ndata-fidelity combined with a convex cost functional. The resulting convex\noptimization problem is approached by a combination of an inexact alternating\ndirection method of multipliers and Dykstra's projection algorithm. We describe\na novel method for balancing data-fit and regularity that is fully automatic\nand allows for a sound statistical interpretation. The performance of our\nestimation approach is studied for various problems in imaging. Among others,\nthis includes deconvolution problems that arise in Poisson nanoscale\nfluorescence microscopy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 09:54:32 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Frick", "Klaus", ""], ["Marnitz", "Philipp", ""], ["Munk", "Axel", ""]]}, {"id": "1204.3941", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Zhandong Liu", "title": "A Log-Linear Graphical Model for Inferring Genetic Networks from\n  High-Throughput Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are often used to infer gene networks based on\nmicroarray expression data. Many scientists, however, have begun using\nhigh-throughput sequencing technologies to measure gene expression. As the\nresulting high-dimensional count data consists of counts of sequencing reads\nfor each gene, Gaussian graphical models are not optimal for modeling gene\nnetworks based on this discrete data. We develop a novel method for estimating\nhigh-dimensional Poisson graphical models, the Log-Linear Graphical Model,\nallowing us to infer networks based on high-throughput sequencing data. Our\nmodel assumes a pair-wise Markov property: conditional on all other variables,\neach variable is Poisson. We estimate our model locally via neighborhood\nselection by fitting 1-norm penalized log-linear models. Additionally, we\ndevelop a fast parallel algorithm, an approach we call the Poisson Graphical\nLasso, permitting us to fit our graphical model to high-dimensional genomic\ndata sets. In simulations, we illustrate the effectiveness of our methods for\nrecovering network structure from count data. A case study on breast cancer\nmicroRNAs, a novel application of graphical models, finds known regulators of\nbreast cancer genes and discovers novel microRNA clusters and hubs that are\ntargets for future research.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 23:34:57 GMT"}, {"version": "v2", "created": "Mon, 28 May 2012 20:46:35 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Allen", "Genevera I.", ""], ["Liu", "Zhandong", ""]]}, {"id": "1204.3993", "submitter": "Maria Giovanna Ranalli", "authors": "Enrico Fabrizi, Giorgio E. Montanari, Maria Giovanna Ranalli", "title": "A hierarchical latent class model for predicting disability small area\n  counts from survey data", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12112", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the estimation of the number of severely disabled\npeople using data from the Italian survey on Health Conditions and Appeal to\nMedicare. Disability is indirectly measured using a set of categorical items,\nwhich survey a set of functions concerning the ability of a person to\naccomplish everyday tasks. Latent Class Models can be employed to classify the\npopulation according to different levels of a latent variable connected with\ndisability. The survey, however, is designed to provide reliable estimates at\nthe level of Administrative Regions (NUTS2 level), while local authorities are\ninterested in quantifying the amount of population that belongs to each latent\nclass at a sub-regional level. Therefore, small area estimation techniques\nshould be used. The challenge of the present application is that the variable\nof interest is not directly observed. Adopting a full Bayesian approach, we\nbase small area estimation on a Latent Class model in which the probability of\nbelonging to each latent class changes with covariates and the influence of age\nis learnt from the data using penalized splines. Deimmler-Reisch bases are\nshown to improve speed and mixing of MCMC chains used to simulate posteriors.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 07:03:04 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Fabrizi", "Enrico", ""], ["Montanari", "Giorgio E.", ""], ["Ranalli", "Maria Giovanna", ""]]}, {"id": "1204.4021", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and St\\'ephane Girard and Mathieu Fauvel", "title": "Kernel discriminant analysis and clustering with parsimonious Gaussian\n  process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a family of parsimonious Gaussian process models which\nallow to build, from a finite sample, a model-based classifier in an infinite\ndimensional space. The proposed parsimonious models are obtained by\nconstraining the eigen-decomposition of the Gaussian processes modeling each\nclass. This allows in particular to use non-linear mapping functions which\nproject the observations into infinite dimensional spaces. It is also\ndemonstrated that the building of the classifier can be directly done from the\nobservation space through a kernel function. The proposed classification method\nis thus able to classify data of various types such as categorical data,\nfunctional data or networks. Furthermore, it is possible to classify mixed data\nby combining different kernels. The methodology is as well extended to the\nunsupervised classification case. Experimental results on various data sets\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 09:18:14 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2012 15:13:49 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Bouveyron", "Charles", ""], ["Girard", "St\u00e9phane", ""], ["Fauvel", "Mathieu", ""]]}, {"id": "1204.4180", "submitter": "Joseph Richards", "authors": "Joseph W. Richards, Dan L. Starr, Adam A. Miller, Joshua S. Bloom,\n  Nathaniel R. Butler, Henrik Brink, Arien Crellin-Quick", "title": "Construction of a Calibrated Probabilistic Classification Catalog:\n  Application to 50k Variable Sources in the All-Sky Automated Survey", "comments": "56 pages, 15 figures, 8 tables, submitted. The Machine-learned ASAS\n  Classification Catalog is available at http://www.bigmacc.info", "journal-ref": null, "doi": "10.1088/0067-0049/203/2/32", "report-no": null, "categories": "astro-ph.IM astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing data volumes from synoptic surveys, astronomers must become more\nabstracted from the discovery and introspection processes. Given the scarcity\nof follow-up resources, there is a particularly sharp onus on the frameworks\nthat replace these human roles to provide accurate and well-calibrated\nprobabilistic classification catalogs. Such catalogs inform the subsequent\nfollow-up, allowing consumers to optimize the selection of specific sources for\nfurther study and permitting rigorous treatment of purities and efficiencies\nfor population studies. Here, we describe a process to produce a probabilistic\nclassification catalog of variability with machine learning from a multi-epoch\nphotometric survey. In addition to producing accurate classifications, we show\nhow to estimate calibrated class probabilities, and motivate the importance of\nprobability calibration. We also introduce a methodology for feature-based\nanomaly detection, which allows discovery of objects in the survey that do not\nfit within the predefined class taxonomy. Finally, we apply these methods to\nsources observed by the All Sky Automated Survey (ASAS), and unveil the\nMachine-learned ASAS Classification Catalog (MACC), which is a 28-class\nprobabilistic classification catalog of 50,124 ASAS sources. We estimate that\nMACC achieves a sub-20% classification error rate, and demonstrate that the\nclass posterior probabilities are reasonably calibrated. MACC classifications\ncompare favorably to the classifications of several previous domain-specific\nASAS papers and to the ASAS Catalog of Variable Stars, which had classified\nonly 24% of those sources into one of 12 science classes. The MACC is publicly\navailable at http://www.bigmacc.info.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 20:00:00 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 16:18:46 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Richards", "Joseph W.", ""], ["Starr", "Dan L.", ""], ["Miller", "Adam A.", ""], ["Bloom", "Joshua S.", ""], ["Butler", "Nathaniel R.", ""], ["Brink", "Henrik", ""], ["Crellin-Quick", "Arien", ""]]}, {"id": "1204.4656", "submitter": "Sooraj K. Ambat", "authors": "Sooraj K. Ambat, Saikat Chatterjee, K. V. S. Hari", "title": "Fusion of Greedy Pursuits for Compressed Sensing Signal Reconstruction", "comments": "Accepted, \"20th European Signal Processing Conference 2012 (EUSIPCO\n  2012)\", Bucharest, Romania,27 Aug,2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy Pursuits are very popular in Compressed Sensing for sparse signal\nrecovery. Though many of the Greedy Pursuits possess elegant theoretical\nguarantees for performance, it is well known that their performance depends on\nthe statistical distribution of the non-zero elements in the sparse signal. In\npractice, the distribution of the sparse signal may not be known a priori. It\nis also observed that performance of Greedy Pursuits degrades as the number of\navailable measurements decreases from a threshold value which is method\ndependent. To improve the performance in these situations, we introduce a novel\nfusion framework for Greedy Pursuits and also propose two algorithms for sparse\nrecovery. Through Monte Carlo simulations we show that the proposed schemes\nimprove sparse signal recovery in clean as well as noisy measurement cases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 15:34:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 15:04:11 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Ambat", "Sooraj K.", ""], ["Chatterjee", "Saikat", ""], ["Hari", "K. V. S.", ""]]}, {"id": "1204.4801", "submitter": "Mateusz Pipie\\'n", "authors": "Lukasz Lenart, Mateusz Pipien", "title": "Almost Periodically Correlated Time Series in Business Fluctuations\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-standard subsampling procedure to make formal statistical\ninference about the business cycle, one of the most important unobserved\nfeature characterising fluctuations of economic growth. We show that some\ncharacteristics of business cycle can be modelled in a non-parametric way by\ndiscrete spectrum of the Almost Periodically Correlated (APC) time series. On\nthe basis of estimated characteristics of this spectrum business cycle is\nextracted by filtering. As an illustration we characterise the man properties\nof business cycles in industrial production index for Polish economy.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 11:00:39 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Lenart", "Lukasz", ""], ["Pipien", "Mateusz", ""]]}, {"id": "1204.5459", "submitter": "Umberto Picchini", "authors": "Umberto Picchini", "title": "Inference for SDE models via Approximate Bayesian Computation", "comments": "Version accepted for publication in Journal of Computational &\n  Graphical Statistics", "journal-ref": null, "doi": "10.1080/10618600.2013.866048", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models defined by stochastic differential equations (SDEs) allow for the\nrepresentation of random variability in dynamical systems. The relevance of\nthis class of models is growing in many applied research areas and is already a\nstandard tool to model e.g. financial, neuronal and population growth dynamics.\nHowever inference for multidimensional SDE models is still very challenging,\nboth computationally and theoretically. Approximate Bayesian computation (ABC)\nallow to perform Bayesian inference for models which are sufficiently complex\nthat the likelihood function is either analytically unavailable or\ncomputationally prohibitive to evaluate. A computationally efficient ABC-MCMC\nalgorithm is proposed, halving the running time in our simulations. Focus is on\nthe case where the SDE describes latent dynamics in state-space models; however\nthe methodology is not limited to the state-space framework. Simulation studies\nfor a pharmacokinetics/pharmacodynamics model and for stochastic chemical\nreactions are considered and a MATLAB package implementing our ABC-MCMC\nalgorithm is provided.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 18:58:34 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2012 12:19:49 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2012 08:18:26 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2013 11:31:32 GMT"}, {"version": "v5", "created": "Tue, 25 Jun 2013 11:27:55 GMT"}, {"version": "v6", "created": "Fri, 8 Nov 2013 08:13:14 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Picchini", "Umberto", ""]]}, {"id": "1204.5724", "submitter": "Paul T Edlefsen", "authors": "Paul T. Edlefsen and Arthur P. Dempster", "title": "Nonparametric survival analysis and vaccine efficacy using\n  Dempster-Shafer analysis", "comments": "This is an incomplete draft (missing refs, results)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extension of nonparametric DS inference for arbitrary\nunivariate CDFs to the case in which some failure times are (right)-censored,\nand then apply this to the problem of assessing evidence regarding assertions\nabout relative risks across two populations. The approach enables exploration\nof the sensitivity of survival analyses to assumed independence of the missing\ndata process and the failure proces. We present an application to the partially\nefficacious RV144 (HIV-1) vaccine trial, and show that the strength of\nconclusions of vaccine efficacy depend on assumptions about the maximum failure\nrates of the subjects lost-to-followup.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 18:12:50 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Edlefsen", "Paul T.", ""], ["Dempster", "Arthur P.", ""]]}, {"id": "1204.5963", "submitter": "Arthur Carvalho", "authors": "Arthur Carvalho and Kate Larson", "title": "On a Reliable Peer-Review Process", "comments": "This paper has been withdrawn by the author due to some errors in the\n  basic model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an enhanced peer-review process where the reviewers are encouraged\nto truthfully disclose their reviews. We start by modelling that process using\na Bayesian model where the uncertainty regarding the quality of the manuscript\nis taken into account. After that, we introduce a scoring function to evaluate\nthe reported reviews. Under mild assumptions, we show that reviewers strictly\nmaximize their expected scores by telling the truth. We also show how those\nscores can be used in order to reach consensus.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 15:49:24 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 22:34:16 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Carvalho", "Arthur", ""], ["Larson", "Kate", ""]]}, {"id": "1204.6096", "submitter": "Drew Thomas", "authors": "Drew M. Thomas", "title": "A Not-So-Fundamental Limitation on Studying Complex Systems with\n  Statistics: Comment on Rabin (2011)", "comments": "4 pages, no figures, 11 references. Commentary on Y. Rabin, Journal\n  of Statistical Physics, 144, 213-216 (2011)", "journal-ref": "Journal of Statistical Physics, 149 (2012), 1168-1171", "doi": "10.1007/s10955-012-0647-y", "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although living organisms are affected by many interrelated and unidentified\nvariables, this complexity does not automatically impose a fundamental\nlimitation on statistical inference. Nor need one invoke such complexity as an\nexplanation of the \"Truth Wears Off\" or \"decline\" effect; similar \"decline\"\neffects occur with far simpler systems studied in physics. Selective reporting\nand publication bias, and scientists' biases in favour of reporting\neye-catching results (in general) or conforming to others' results (in physics)\nbetter explain this feature of the \"Truth Wears Off\" effect than Rabin's\nsuggested limitation on statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 01:19:22 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2012 04:53:32 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Thomas", "Drew M.", ""]]}, {"id": "1204.6735", "submitter": "Robert Brame", "authors": "Robert Brame, Michael G. Turner, Raymond Paternoster", "title": "Surveying Residential Burglaries: A Case Study of Local Crime\n  Measurement", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the incidence of residential burglaries\nthat occur over a well-defined period of time within the 10 most populous\ncities in North Carolina. Our analysis typifies some of the general issues that\narise in estimating and comparing local crime rates over time and for different\ncities. Typically, the only information we have about crime incidence within\nany particular city is what that city's police department tells us, and the\npolice can only count and describe the crimes that come to their attention. To\naddress this, our study combines information from police-based residential\nburglary counts and the National Crime Victimization Survey to obtain interval\nestimates of residential burglary incidence at the local level. We use those\nestimates as a basis for commenting on the fragility of between-city and\nover-time comparisons that often appear in both public discourse about crime\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 19:38:59 GMT"}, {"version": "v2", "created": "Wed, 23 May 2012 17:42:43 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2012 03:46:10 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2013 17:34:40 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Brame", "Robert", ""], ["Turner", "Michael G.", ""], ["Paternoster", "Raymond", ""]]}]