[{"id": "1909.00024", "submitter": "Kareem Haggag", "authors": "M. Keith Chen, Kareem Haggag, Devin G. Pope, and Ryne Rohla", "title": "Racial Disparities in Voting Wait Times: Evidence from Smartphone Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equal access to voting is a core feature of democratic government. Using data\nfrom millions of smartphone users, we quantify a racial disparity in voting\nwait times across a nationwide sample of polling places during the 2016 U.S.\npresidential election. Relative to entirely-white neighborhoods, residents of\nentirely-black neighborhoods waited 29% longer to vote and were 74% more likely\nto spend more than 30 minutes at their polling place. This disparity holds when\ncomparing predominantly white and black polling places within the same states\nand counties, and survives numerous robustness and placebo tests. We shed light\non the mechanism for these results and discuss how geospatial data can be an\neffective tool to both measure and monitor these disparities going forward.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:24:17 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 21:45:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "M. Keith", ""], ["Haggag", "Kareem", ""], ["Pope", "Devin G.", ""], ["Rohla", "Ryne", ""]]}, {"id": "1909.00061", "submitter": "John Sun", "authors": "John Sun, Christopher S. Wang, Ellie S. Krossa", "title": "Investigating Sprawl using AIC and Recursive Partitioning Trees: A\n  Machine Learning Approach to Assessing the Association between Poverty and\n  Commute Time", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sprawl, according to Glaeser and Kahn, is the 21st century phenomenon that\nsome people are not dependent on city-living due to automobiles and therefore\ncan live outside public transportation spheres and cities. This is usually seen\nas pleasant and accompanied by improved qualities of life, but as they\naddressed, the problem remains that sprawl causes loss of jobs for those who\ncannot afford luxurious alternatives but only inferior substitutes (Glaeser and\nKahn 2004). Therefore, through our question, we hope to suggest that sprawl has\noccurred in the U.S. and poverty is one of the consequences.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:35:44 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 20:09:24 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 16:29:33 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 13:31:02 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 17:51:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sun", "John", ""], ["Wang", "Christopher S.", ""], ["Krossa", "Ellie S.", ""]]}, {"id": "1909.00066", "submitter": "Amanda Coston", "authors": "Amanda Coston, Alan Mishler, Edward H. Kennedy, Alexandra Chouldechova", "title": "Counterfactual Risk Assessments, Evaluation, and Fairness", "comments": "To appear in ACM FAT* 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic risk assessments are increasingly used to help humans make\ndecisions in high-stakes settings, such as medicine, criminal justice and\neducation. In each of these cases, the purpose of the risk assessment tool is\nto inform actions, such as medical treatments or release conditions, often with\nthe aim of reducing the likelihood of an adverse event such as hospital\nreadmission or recidivism. Problematically, most tools are trained and\nevaluated on historical data in which the outcomes observed depend on the\nhistorical decision-making policy. These tools thus reflect risk under the\nhistorical policy, rather than under the different decision options that the\ntool is intended to inform. Even when tools are constructed to predict risk\nunder a specific decision, they are often improperly evaluated as predictors of\nthe target outcome.\n  Focusing on the evaluation task, in this paper we define counterfactual\nanalogues of common predictive performance and algorithmic fairness metrics\nthat we argue are better suited for the decision-making context. We introduce a\nnew method for estimating the proposed metrics using doubly robust estimation.\nWe provide theoretical results that show that only under strong conditions can\nfairness according to the standard metric and the counterfactual metric\nsimultaneously hold. Consequently, fairness-promoting methods that target\nparity in a standard fairness metric may --- and as we show empirically, do ---\ninduce greater imbalance in the counterfactual analogue. We provide empirical\ncomparisons on both synthetic data and a real world child welfare dataset to\ndemonstrate how the proposed method improves upon standard practice.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:47:20 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:15:16 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 14:08:46 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Coston", "Amanda", ""], ["Mishler", "Alan", ""], ["Kennedy", "Edward H.", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "1909.00221", "submitter": "Feng Li", "authors": "Yanfei Kang, Evangelos Spiliotis, Fotios Petropoulos, Nikolaos\n  Athiniotis, Feng Li, Vassilios Assimakopoulos", "title": "D\\'ej\\`a vu: A data-centric forecasting approach through time series\n  cross-similarity", "comments": null, "journal-ref": "Journal of Business Research (2020)", "doi": "10.1016/j.jbusres.2020.10.051", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasts are vital for supporting the decisions of modern\ncompanies. Forecasters typically select the most appropriate statistical model\nfor each time series. However, statistical models usually presume some data\ngeneration process while making strong assumptions about the errors. In this\npaper, we present a novel data-centric approach -- `forecasting with\nsimilarity', which tackles model uncertainty in a model-free manner. Existing\nsimilarity-based methods focus on identifying similar patterns within the\nseries, i.e., `self-similarity'. In contrast, we propose searching for similar\npatterns from a reference set, i.e., `cross-similarity'. Instead of\nextrapolating, the future paths of the similar series are aggregated to obtain\nthe forecasts of the target series. Building on the cross-learning concept, our\napproach allows the application of similarity-based forecasting on series with\nlimited lengths. We evaluate the approach using a rich collection of real data\nand show that it yields competitive accuracy in both points forecasts and\nprediction intervals.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 14:14:33 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 15:19:46 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 05:10:38 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kang", "Yanfei", ""], ["Spiliotis", "Evangelos", ""], ["Petropoulos", "Fotios", ""], ["Athiniotis", "Nikolaos", ""], ["Li", "Feng", ""], ["Assimakopoulos", "Vassilios", ""]]}, {"id": "1909.00306", "submitter": "Hallee Wong", "authors": "Hallee E. Wong, Brianna C. Heggeseth, Steven J. Miller", "title": "Categorical Co-Frequency Analysis: Clustering Diagnosis Codes to Predict\n  Hospital Readmissions", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting patients' risk of 30-day hospital readmission would\nenable hospitals to efficiently allocate resource-intensive interventions. We\ndevelop a new method, Categorical Co-Frequency Analysis (CoFA), for clustering\ndiagnosis codes from the International Classification of Diseases (ICD)\naccording to the similarity in relationships between covariates and readmission\nrisk. CoFA measures the similarity between diagnoses by the frequency with\nwhich two diagnoses are split in the same direction versus split apart in\nrandom forests to predict readmission risk. Applying CoFA to de-identified data\nfrom Berkshire Medical Center, we identified three groups of diagnoses that\nvary in readmission risk. To evaluate CoFA, we compared readmission risk models\nusing ICD majors and CoFA groups to a baseline model without diagnosis\nvariables. We found substituting ICD majors for the CoFA-identified clusters\nsimplified the model without compromising the accuracy of predictions. Fitting\nseparate models for each ICD major and CoFA group did not improve predictions,\nsuggesting that readmission risk may be more homogeneous that heterogeneous\nacross diagnosis groups.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 02:36:17 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wong", "Hallee E.", ""], ["Heggeseth", "Brianna C.", ""], ["Miller", "Steven J.", ""]]}, {"id": "1909.00456", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Dynamic principal component regression for forecasting functional time\n  series in a group structure", "comments": "19 pages, 7 figures, to appear in Scandinavian Actuarial Journal.\n  arXiv admin note: text overlap with arXiv:1705.08001, arXiv:1609.04222", "journal-ref": "Scandinavian Actuarial Journal, 2020, 2020(4), 307-322", "doi": "10.1080/03461238.2019.1663553", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When generating social policies and pricing annuity at national and\nsubnational levels, it is essential both to forecast mortality accurately and\nensure that forecasts at the subnational level add up to the forecasts at the\nnational level. This has motivated recent developments in forecasting\nfunctional time series in a group structure, where static principal component\nanalysis is used. In the presence of moderate to strong temporal dependence,\nstatic principal component analysis designed for independent and identically\ndistributed functional data may be inadequate. Thus, through using the dynamic\nfunctional principal component analysis, we consider a functional time series\nforecasting method with static and dynamic principal component regression to\nforecast each series in a group structure. Through using the regional\nage-specific mortality rates in Japan obtained from the Japanese Mortality\nDatabase (2019), we investigate the point and interval forecast accuracies of\nour proposed extension, and subsequently make recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 19:24:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1909.00731", "submitter": "Frederic Barraquand", "authors": "Frederic Barraquand, Coralie Picoche, Matteo Detto, Florian Hartig", "title": "Inferring species interactions using Granger causality and convergent\n  cross mapping", "comments": null, "journal-ref": null, "doi": "10.1007/s12080-020-00482-7", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying directed interactions between species from time series of their\npopulation densities has many uses in ecology. This key statistical task is\nequivalent to causal time series inference, which connects to the Granger\ncausality (GC) concept: $x$ causes $y$ if $x$ improves the prediction of $y$ in\na dynamic model. However, the entangled nature of nonlinear ecological systems\nhas led to question the appropriateness of Granger causality, especially in its\nclassical linear Multivariate AutoRegressive (MAR) model form. Convergent-cross\nmapping (CCM), a nonparametric method developed for deterministic dynamical\nsystems, has been suggested as an alternative. Here, we show that linear GC and\nCCM are able to uncover interactions with surprisingly similar performance, for\npredator-prey cycles, 2-species deterministic (chaotic) or stochastic\ncompetition, as well as 10- and 20-species interaction networks. There is no\ncorrespondence between the degree of nonlinearity of the dynamics and which\nmethod performs best. Our results therefore imply that Granger causality, even\nin its linear MAR($p$) formulation, is a valid method for inferring\ninteractions in nonlinear ecological networks; using GC or CCM (or both) can\ninstead be decided based on the aims and specifics of the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:26:44 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:19:01 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 06:27:49 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Barraquand", "Frederic", ""], ["Picoche", "Coralie", ""], ["Detto", "Matteo", ""], ["Hartig", "Florian", ""]]}, {"id": "1909.00736", "submitter": "Verena Bauer", "authors": "Verena Bauer, Dietmar Harhoff, G\\\"oran Kauermann", "title": "A smooth dynamic network model for patent collaboration data", "comments": "Major change: We had a discrepancy in the implementation and the\n  notation in the paper of the covariate vector. Further changes: Wordings and\n  combinations of some figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and application of models, which take the evolution of\nnetwork dynamics into account are receiving increasing attention. We contribute\nto this field and focus on a profile likelihood approach to model time-stamped\nevent data for a large-scale dynamic network. We investigate the collaboration\nof inventors using EU patent data. As event we consider the submission of a\njoint patent and we explore the driving forces for collaboration between\ninventors. We propose a flexible semiparametric model, which includes external\nand internal covariates, where the latter are built from the network history.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:37:04 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 08:59:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bauer", "Verena", ""], ["Harhoff", "Dietmar", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1909.00952", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Yung-Hsuan Chao, Antonio Ortega", "title": "Graph-based Transforms for Video Coding", "comments": "To appear in IEEE Trans. on Image Processing (14 pages)", "journal-ref": null, "doi": "10.1109/TIP.2020.3026627", "report-no": null, "categories": "eess.IV cs.LG cs.MM cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many state-of-the-art compression systems, signal transformation is an\nintegral part of the encoding and decoding process, where transforms provide\ncompact representations for the signals of interest. This paper introduces a\nclass of transforms called graph-based transforms (GBTs) for video compression,\nand proposes two different techniques to design GBTs. In the first technique,\nwe formulate an optimization problem to learn graphs from data and provide\nsolutions for optimal separable and nonseparable GBT designs, called GL-GBTs.\nThe optimality of the proposed GL-GBTs is also theoretically analyzed based on\nGaussian-Markov random field (GMRF) models for intra and inter predicted block\nsignals. The second technique develops edge-adaptive GBTs (EA-GBTs) in order to\nflexibly adapt transforms to block signals with image edges (discontinuities).\nThe advantages of EA-GBTs are both theoretically and empirically demonstrated.\nOur experimental results demonstrate that the proposed transforms can\nsignificantly outperform the traditional Karhunen-Loeve transform (KLT).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 04:53:53 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 08:44:44 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Chao", "Yung-Hsuan", ""], ["Ortega", "Antonio", ""]]}, {"id": "1909.01035", "submitter": "Wendy Harrison", "authors": "Wendy J. Harrison (1 and 2), Paul D. Baxter (2) and Mark S. Gilthorpe\n  (1, 2 and 3) ((1) Leeds Institute for Data Analytics, University of Leeds,\n  Leeds, UK, (2) School of Medicine, University of Leeds, Leeds, UK, (3) The\n  Alan Turing Institute, London, UK)", "title": "Multilevel latent class (MLC) modelling of healthcare provider causal\n  effects on patient outcomes: Evaluation via simulation", "comments": "19 pages, 5 figures. Abstract to be published in the conference\n  proceedings for the Society for Social Medicine & Population Health and\n  International Epidemiology Association European Congress Joint Annual\n  Scientific Meeting, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where performance comparison of healthcare providers is of interest,\ncharacteristics of both patients and the health condition of interest must be\nbalanced across providers for a fair comparison. This is unlikely to be\nfeasible within observational data, as patient population characteristics may\nvary geographically and patient care may vary by characteristics of the health\ncondition. We simulated data for patients and providers, based on a previously\nutilized real-world dataset, and separately considered both binary and\ncontinuous covariate-effects at the upper level. Multilevel latent class (MLC)\nmodelling is proposed to partition a prediction focus at the patient level\n(accommodating casemix) and a causal inference focus at the provider level. The\nMLC model recovered a range of simulated Trust-level effects. Median recovered\nvalues were almost identical to simulated values for the binary Trust-level\ncovariate, and we observed successful recovery of the continuous Trust-level\ncovariate with at least 3 latent Trust classes. Credible intervals widen as the\nerror variance increases. The MLC approach successfully partitioned modelling\nfor prediction and for causal inference, addressing the potential conflict\nbetween these two distinct analytical strategies. This improves upon strategies\nwhich only adjust for differential selection. Patient-level variation and\nmeasurement uncertainty are accommodated within the latent classes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:14:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Harrison", "Wendy J.", "", "1 and 2"], ["Baxter", "Paul D.", "", "1, 2 and 3"], ["Gilthorpe", "Mark S.", "", "1, 2 and 3"]]}, {"id": "1909.01265", "submitter": "Damian Campo", "authors": "Damian Campo, Manuela Bastidas, Olga Luc\\'ia Quintero", "title": "Multiresolution analysis (discrete wavelet transform) through Daubechies\n  family for emotion recognition in speech", "comments": "Published in: Conference, XX Congreso Argentino de Bioingenier\\'ia,\n  SABI 2015, Octubre 28-30, 2015", "journal-ref": null, "doi": "10.13140/RG.2.1.5089.1608", "report-no": null, "categories": "cs.SD eess.AS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a study of the mathematical properties of voice as an audio\nsignal. This work includes signals in which the channel conditions are not\nideal for emotion recognition. Multiresolution analysis discrete wavelet\ntransform was performed through the use of Daubechies Wavelet Family (Db1-Haar,\nDb 6, Db8, Db10) allowing the decomposition of the initial audio signal into\nsets of coefficients on which a set of features was extracted and analyzed\nstatistically in order to differentiate emotional states. ANNs proved to be a\nsystem that allows an appropriate classification of such states. This study\nshows that the extracted features using wavelet decomposition are enough to\nanalyze and extract emotional content in audio signals presenting a high\naccuracy rate in classification of emotional states without the need to use\nother kinds of classical frequency-time features. Accordingly, this paper seeks\nto characterize mathematically the six basic emotions in humans: boredom,\ndisgust, happiness, anxiety, anger and sadness, also included the neutrality,\nfor a total of seven states to identify.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:00:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Campo", "Damian", ""], ["Bastidas", "Manuela", ""], ["Quintero", "Olga Luc\u00eda", ""]]}, {"id": "1909.01268", "submitter": "Samuel Asante Gyamerah", "authors": "Samuel Asante Gyamerah", "title": "Are Bitcoins price predictable? Evidence from machine learning\n  techniques using technical indicators", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The uncertainties in future Bitcoin price make it difficult to accurately\npredict the price of Bitcoin. Accurately predicting the price for Bitcoin is\ntherefore important for decision-making process of investors and market players\nin the cryptocurrency market. Using historical data from 01/01/2012 to\n16/08/2019, machine learning techniques (Generalized linear model via penalized\nmaximum likelihood, random forest, support vector regression with linear\nkernel, and stacking ensemble) were used to forecast the price of Bitcoin. The\nprediction models employed key and high dimensional technical indicators as the\npredictors. The performance of these techniques were evaluated using mean\nabsolute percentage error (MAPE), root mean square error (RMSE), mean absolute\nerror (MAE), and coefficient of determination (R-squared). The performance\nmetrics revealed that the stacking ensemble model with two base learner (random\nforest and generalized linear model via penalized maximum likelihood) and\nsupport vector regression with linear kernel as meta-learner was the optimal\nmodel for forecasting Bitcoin price. The MAPE, RMSE, MAE, and R-squared values\nfor the stacking ensemble model were 0.0191%, 15.5331 USD, 124.5508 USD, and\n0.9967 respectively. These values show a high degree of reliability in\npredicting the price of Bitcoin using the stacking ensemble model. Accurately\npredicting the future price of Bitcoin will yield significant returns for\ninvestors and market players in the cryptocurrency market.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:03:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gyamerah", "Samuel Asante", ""]]}, {"id": "1909.01273", "submitter": "Trevor Harris", "authors": "Trevor Harris, Bo Li, Nathan Steiger, Jason Smerdon, Naveen Narisetty,\n  Derek Tucker", "title": "Evaluating proxy influence in assimilated paleoclimate reconstructions\n  -- Testing the exchangeability of two ensembles of spatial processes", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1799810", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate field reconstructions (CFR) attempt to estimate spatiotemporal fields\nof climate variables in the past using climate proxies such as tree rings, ice\ncores, and corals. Data Assimilation (DA) methods are a recent and promising\nnew means of deriving CFRs that optimally fuse climate proxies with climate\nmodel output. Despite the growing application of DA-based CFRs, little is\nunderstood about how much the assimilated proxies change the statistical\nproperties of the climate model data. To address this question, we propose a\nrobust and computationally efficient method, based on functional data depth, to\nevaluate differences in the distributions of two spatiotemporal processes. We\napply our test to study global and regional proxy influence in DA-based CFRs by\ncomparing the background and analysis states, which are treated as two samples\nof spatiotemporal fields. We find that the analysis states are significantly\naltered from the climate-model-based background states due to the assimilation\nof proxies. Moreover, the difference between the analysis and background states\nincreases with the number of proxies, even in regions far beyond proxy\ncollection sites. Our approach allows us to characterize the added value of\nproxies, indicating where and when the analysis states are distinct from the\nbackground states.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:12:35 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 00:46:07 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Harris", "Trevor", ""], ["Li", "Bo", ""], ["Steiger", "Nathan", ""], ["Smerdon", "Jason", ""], ["Narisetty", "Naveen", ""], ["Tucker", "Derek", ""]]}, {"id": "1909.01274", "submitter": "Michael Lebacher", "authors": "Michael Lebacher, Samantha Cook, Nadja Klein and G\\\"oran Kauermann", "title": "In Search of Lost Edges: A Case Study on Reconstructing Financial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the systemic complexity of international financial systems,\nnetwork data is an important prerequisite. However, dyadic data is often not\navailable, raising the need for methods that allow for reconstructing networks\nbased on limited information. In this paper, we are reviewing different methods\nthat are designed for the estimation of matrices from their marginals and\npotentially exogenous information. This includes a general discussion of the\navailable methodology that provides edge probabilities as well as models that\nare focussed on the reconstruction of edge values. Besides summarizing the\nadvantages, shortfalls and computational issues of the approaches, we put them\ninto a competitive comparison using the SWIFT (Society for Worldwide Interbank\nFinancial Telecommunication) MT 103 payment messages network (MT 103: Single\nCustomer Credit Transfer). This network is not only economically meaningful but\nalso fully observed which allows for an extensive competitive horse race of\nmethods. The comparison concerning the binary reconstruction is divided into an\nevaluation of the edge probabilities and the quality of the reconstructed\ndegree structures. Furthermore, the accuracy of the predicted edge values is\ninvestigated. To test the methods on different topologies, the application is\nsplit into two parts. The first part considers the full MT 103 network, being\nan illustration for the reconstruction of large, sparse financial networks. The\nsecond part is concerned with reconstructing a subset of the full network,\nrepresenting a dense medium-sized network. Regarding substantial outcomes, it\ncan be found that no method is superior in every respect and that the preferred\nmodel choice highly depends on the goal of the analysis, the presumed network\nstructure and the availability of exogenous information.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:14:08 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 06:51:42 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Lebacher", "Michael", ""], ["Cook", "Samantha", ""], ["Klein", "Nadja", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1909.01284", "submitter": "Yu-hsuan Wang", "authors": "Y. Samuel Wang, Carole J. Lee, Jevin D. West, Carl T. Bergstrom, Elena\n  A. Erosheva", "title": "Gender-based homophily in collaborations across a heterogeneous\n  scholarly landscape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the corpus of JSTOR articles, we investigate the role of gender in\ncollaboration patterns across the scholarly landscape by analyzing gender-based\nhomophily--the tendency for researchers to co-author with individuals of the\nsame gender. For a nuanced analysis of gender homophily, we develop methodology\nnecessitated by the fact that the data comprises heterogeneous sub-disciplines\nand that not all authorships are exchangeable. In particular, we distinguish\nthree components of gender homophily in collaborations: a structural component\nthat is due to demographics and non-gendered authorship norms of a scholarly\ncommunity, a compositional component which is driven by varying gender\nrepresentation across sub-disciplines, and a behavioral component which we\ndefine as the remainder of observed homophily after its structural and\ncompositional components have been taken into account. Using minimal modeling\nassumptions, we measure and test for behavioral homophily. We find that\nsignificant behavioral homophily can be detected across the JSTOR corpus and\nshow that this finding is robust to missing gender indicators in our data. In a\nsecondary analysis, we show that the proportion of female representation in a\nfield is positively associated with significant behavioral homophily.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Lee", "Carole J.", ""], ["West", "Jevin D.", ""], ["Bergstrom", "Carl T.", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1909.01605", "submitter": "Marcin W\\k{a}torek", "authors": "Adarsh Sankaran, Drisya Sasi Dharan, Anand Vishnu Babykuttan, Nandhu\n  Ambika Raju, Vysakh Kunju Kunju, Marcin W\\k{a}torek", "title": "Multifractal Description of Streamflow and Suspended Sediment\n  Concentration Data from Indian River Basins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the multifractality of streamflow data of 192\nstations located in 13 river basins in India using the Multifractal Detrended\nFluctuation Analysis (MF-DFA). The streamflow datasets of different river\nbasins displayed multifractality and long term persistence with a mean exponent\nof 0.585. The streamflow records of Krishna basin displayed least persistence\nand that of Godavari basin displayed strongest multifractality and complexity.\nSubsequently, the streamflow-sediment links of five major river basins are\nevaluated using the novel Multifractal Cross Correlation Analysis (MFCCA)\nmethod of cross correlation studies. The results showed that the joint\npersistence of streamflow and total suspended sediments (TSS) is approximately\nthe mean of the persistence of individual series. The streamflow displayed\nhigher persistence than TSS in 60 % of the stations while in majority of\nstations of Godavari basin the trend was opposite. The annual cross correlation\nis higher than seasonal cross correlation in majority of stations but at these\ntime scales strength of their association differs with river basin.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:00:18 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sankaran", "Adarsh", ""], ["Dharan", "Drisya Sasi", ""], ["Babykuttan", "Anand Vishnu", ""], ["Raju", "Nandhu Ambika", ""], ["Kunju", "Vysakh Kunju", ""], ["W\u0105torek", "Marcin", ""]]}, {"id": "1909.01614", "submitter": "Siddharth Ramchandran", "authors": "Siddharth Ramchandran, Miika Koskinen and Harri L\\\"ahdesm\\\"aki", "title": "Latent Gaussian process with composite likelihoods and numerical\n  quadrature", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS-2021), pp. 3718-3726. PMLR, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical patient records are an example of high-dimensional data that is\ntypically collected from disparate sources and comprises of multiple\nlikelihoods with noisy as well as missing values. In this work, we propose an\nunsupervised generative model that can learn a low-dimensional representation\namong the observations in a latent space, while making use of all available\ndata in a heterogeneous data setting with missing values. We improve upon the\nexisting Gaussian process latent variable model (GPLVM) by incorporating\nmultiple likelihoods and deep neural network parameterised back-constraints to\ncreate a non-linear dimensionality reduction technique for heterogeneous data.\nIn addition, we develop a variational inference method for our model that uses\nnumerical quadrature. We establish the effectiveness of our model and compare\nagainst existing GPLVM methods on a standard benchmark dataset as well as on\nclinical data of Parkinson's disease patients treated at the HUS Helsinki\nUniversity Hospital.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:30:22 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 11:12:01 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 18:32:04 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 14:57:11 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ramchandran", "Siddharth", ""], ["Koskinen", "Miika", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]]}, {"id": "1909.01636", "submitter": "Assia Benbihi", "authors": "Assia Benbihi and Matthieu Geist and C\\'edric Pradalier", "title": "Learning Sensor Placement from Demonstration for UAV networks", "comments": null, "journal-ref": "ISCC 2019 Worshop Proceedings", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work demonstrates how to leverage previous network expert demonstrations\nof UAV deployment to automate the drones placement in civil applications.\nOptimal UAV placement is an NP-complete problem: it requires a closed-form\nutility function that defines the environment and the UAV constraints, it is\nnot unique and must be defined for each new UAV mission. This complex and\ntime-consuming process hinders the development of UAV-networks in civil\napplications. We propose a method that leverages previous network expert\nsolutions of UAV-network deployment to learn the expert's untold utility\nfunction form demonstrations only. This is especially interesting as it may be\ndifficult for the inspection expert to explicit his expertise into such a\nfunction as it is too complex. Once learned, our model generates a utility\nfunction which maxima match expert UAV locations. We test this method on a\nWi-Fi UAV network application inside a crowd simulator and reach similar\nquality-of-service as the expert. We show that our method is not limited to\nthis UAV application and can be extended to other missions such as building\nmonitoring.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:14:19 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Benbihi", "Assia", ""], ["Geist", "Matthieu", ""], ["Pradalier", "C\u00e9dric", ""]]}, {"id": "1909.01836", "submitter": "Pulong Ma", "authors": "Pulong Ma, Georgios Karagiannis, Bledar A. Konomi, Taylor G. Asher,\n  Gabriel R. Toro, Andrew T. Cox", "title": "Multifidelity Computer Model Emulation with High-Dimensional Output: An\n  Application to Storm Surge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricane-driven storm surge is one of the most deadly and costly natural\ndisasters, making precise quantification of the surge hazard of great\nimportance. Inference of such systems is done through physics-based computer\nmodels of the process. Such surge simulators can be implemented with a wide\nrange of fidelity levels, with computational burdens varying by several orders\nof magnitude due to the nature of the system. The danger posed by surge makes\ngreater fidelity highly desirable, however such models and their high-volume\noutput tend to come at great computational cost, which can make detailed study\nof coastal flood hazards prohibitive. These needs make the development of an\nemulator combining high-dimensional output from multiple complex computer\nmodels with different fidelity levels important. We propose a parallel partial\nautoregressive cokriging model to predict highly-accurate storm surges in a\ncomputationally efficient way over a large spatial domain. This emulator has\nthe capability of predicting storm surges as accurately as a high-fidelity\ncomputer model given any storm characteristics and allows accurate assessment\nof the hazards from storm surges over a large spatial domain.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:33:23 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:12:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 18:45:34 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 19:27:51 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2020 15:56:24 GMT"}, {"version": "v6", "created": "Tue, 5 May 2020 18:27:22 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ma", "Pulong", ""], ["Karagiannis", "Georgios", ""], ["Konomi", "Bledar A.", ""], ["Asher", "Taylor G.", ""], ["Toro", "Gabriel R.", ""], ["Cox", "Andrew T.", ""]]}, {"id": "1909.01915", "submitter": "Mark Dubbelman", "authors": "Mark A. Dubbelman, Merike Verrijp, David Facal, Gonzalo\n  S\\'anchez-Benavides, Laura J.E. Brown, Wiesje M. van der Flier, Hanna\n  Jokinen, Athene Lee, Iracema Leroi, Cristina Lojo-Seoane, Vuk Milosevic,\n  Jos\\'e Lu\\'is Molinuevo, Arturo X. Pereiro Rozas, Craig Ritchie, Stephen\n  Salloway, Gemma Stringer, Stelios Zygouris, Bruno Dubois, St\\'ephane\n  Epelbaum, Philip Scheltens, Sietske A.M. Sikkes", "title": "The influence of diversity on the measurement of functional impairment:\n  An international validation of the Amsterdam IADL Questionnaire in 8\n  countries", "comments": null, "journal-ref": null, "doi": "10.1002/dad2.12021", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  INTRODUCTION: To understand the potential influence of diversity on the\nmeasurement of functional impairment in dementia, we aimed to investigate\npossible bias caused by age, gender, education, and cultural differences.\nMETHODS: 3,571 individuals (67.1 {\\pm} 9.5 years old, 44.7% female) from the\nNetherlands, Spain, France, United States, United Kingdom, Greece, Serbia and\nFinland were included. Functional impairment was measured using the Amsterdam\nIADL Questionnaire. Item bias was assessed using differential item functioning\n(DIF) analysis. RESULTS: There were some differences in activity endorsement. A\nfew items showed statistically significant DIF. However, there was no evidence\nof meaningful item bias: effect sizes were low ({\\Delta}R2 range 0-0.03).\nImpact on total scores was minimal. DISCUSSION: The results imply a limited\nbias for age, gender, education and culture in the measurement of functional\nimpairment. This study provides an important step in recognizing the potential\ninfluence of diversity on primary outcomes in dementia research.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 16:08:14 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 07:48:06 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Dubbelman", "Mark A.", ""], ["Verrijp", "Merike", ""], ["Facal", "David", ""], ["S\u00e1nchez-Benavides", "Gonzalo", ""], ["Brown", "Laura J. E.", ""], ["van der Flier", "Wiesje M.", ""], ["Jokinen", "Hanna", ""], ["Lee", "Athene", ""], ["Leroi", "Iracema", ""], ["Lojo-Seoane", "Cristina", ""], ["Milosevic", "Vuk", ""], ["Molinuevo", "Jos\u00e9 Lu\u00eds", ""], ["Rozas", "Arturo X. Pereiro", ""], ["Ritchie", "Craig", ""], ["Salloway", "Stephen", ""], ["Stringer", "Gemma", ""], ["Zygouris", "Stelios", ""], ["Dubois", "Bruno", ""], ["Epelbaum", "St\u00e9phane", ""], ["Scheltens", "Philip", ""], ["Sikkes", "Sietske A. M.", ""]]}, {"id": "1909.01919", "submitter": "David Woodruff", "authors": "Guillaume Goujard, Jean-Paul Watson, David L.Woodruff", "title": "Mape_Maker: A Scenario Creator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe algorithms for creating probabilistic scenarios for the situation\nwhen the underlying forecast methodology is modeled as being more (or less)\naccurate than it has been historically. Such scenarios can be used in studies\nthat extend into the future and may need to consider the possibility that\nforecast technology will improve. Our approach can also be used to generate\nalternative realizations of renewable energy production that are consistent\nwith historical forecast accuracy, in effect serving as a method for creating\nfamilies of realistic alternatives -- which are often critical in\nsimulation-based analysis methodologies\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 16:16:42 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Goujard", "Guillaume", ""], ["Watson", "Jean-Paul", ""], ["Woodruff", "David L.", ""]]}, {"id": "1909.01936", "submitter": "Jarrod Olson", "authors": "Jarrod Olson and Po-Hsu Allen Chen and Marissa White and Nicole\n  Brennan and Ning Gong", "title": "State Drug Policy Effectiveness: Comparative Policy Analysis of Drug\n  Overdose Mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opioid overdose rates have reached an epidemic level and state-level policy\ninnovations have followed suit in an effort to prevent overdose deaths.\nState-level drug law is a set of policies that may reinforce or undermine each\nother, and analysts have a limited set of tools for handling the policy\ncollinearity using statistical methods. This paper uses a machine learning\nmethod called hierarchical clustering to empirically generate \"policy bundles\"\nby grouping states with similar sets of policies in force at a given time\ntogether for analysis in a 50-state, 10-year interrupted time series regression\nwith drug overdose deaths as the dependent variable. Policy clusters were\ngenerated from 138 binomial variables observed by state and year from the\nPrescription Drug Abuse Policy System. Clustering reduced the policies to a set\nof 10 bundles. The approach allows for ranking of the relative effect of\ndifferent bundles and is a tool to recommend those most likely to succeed. This\nstudy shows that a set of policies balancing Medication Assisted Treatment,\nNaloxone Access, Good Samaritan Laws, Medication Assisted Treatment,\nPrescription Drug Monitoring Programs and legalization of medical marijuana\nleads to a reduced number of overdose deaths, but not until its second year in\nforce.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:41:44 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 21:17:58 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 23:14:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Olson", "Jarrod", ""], ["Chen", "Po-Hsu Allen", ""], ["White", "Marissa", ""], ["Brennan", "Nicole", ""], ["Gong", "Ning", ""]]}, {"id": "1909.02058", "submitter": "Christine Peterson", "authors": "Elin Shaddox, Christine B. Peterson, Francesco C. Stingo, Nicola A.\n  Hanania, Charmion Cruickshank-Quinn, Katerina Kechris, Russell Bowler, and\n  Marina Vannucci", "title": "Bayesian Inference of Networks Across Multiple Sample Groups and Data\n  Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a graphical modeling framework for the inference of\nnetworks across multiple sample groups and data types. In medical studies, this\nsetting arises whenever a set of subjects, which may be heterogeneous due to\ndiffering disease stage or subtype, is profiled across multiple platforms, such\nas metabolomics, proteomics, or transcriptomics data. Our proposed Bayesian\nhierarchical model first links the network structures within each platform\nusing a Markov random field prior to relate edge selection across sample\ngroups, and then links the network similarity parameters across platforms. This\nenables joint estimation in a flexible manner, as we make no assumptions on the\ndirectionality of influence across the data types or the extent of network\nsimilarity across the sample groups and platforms. In addition, our model\nformulation allows the number of variables and number of subjects to differ\nacross the data types, and only requires that we have data for the same set of\ngroups. We illustrate the proposed approach through both simulation studies and\nan application to gene expression levels and metabolite abundances on subjects\nwith varying severity levels of Chronic Obstructive Pulmonary Disease (COPD).\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:06:46 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Shaddox", "Elin", ""], ["Peterson", "Christine B.", ""], ["Stingo", "Francesco C.", ""], ["Hanania", "Nicola A.", ""], ["Cruickshank-Quinn", "Charmion", ""], ["Kechris", "Katerina", ""], ["Bowler", "Russell", ""], ["Vannucci", "Marina", ""]]}, {"id": "1909.02063", "submitter": "Bas Hofstra", "authors": "Bas Hofstra, Vivek V. Kulkarni, Sebastian Munoz-Najar Galvez, Bryan\n  He, Dan Jurafsky, Daniel A. McFarland", "title": "The Diversity-Innovation Paradox in Science", "comments": "Updated paper; tightened up terminology, added better theoretical\n  explanation, tested for a mechanism in the updated paper, added robustness\n  analyses, updated and improved metrics across the board", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prior work finds a diversity paradox: diversity breeds innovation, and yet,\nunderrepresented groups that diversify organizations have less successful\ncareers within them. Does the diversity paradox hold for scientists as well? We\nstudy this by utilizing a near-population of ~1.2 million US doctoral\nrecipients from 1977-2015 and following their careers into publishing and\nfaculty positions. We use text analysis and machine learning to answer a series\nof questions: How do we detect scientific innovations? Are underrepresented\ngroups more likely to generate scientific innovations? And are the innovations\nof underrepresented groups adopted and rewarded? Our analyses show that\nunderrepresented groups produce higher rates of scientific novelty. However,\ntheir novel contributions are devalued and discounted: e.g., novel\ncontributions by gender and racial minorities are taken up by other scholars at\nlower rates than novel contributions by gender and racial majorities, and\nequally impactful contributions of gender and racial minorities are less likely\nto result in successful scientific careers than for majority groups. These\nresults suggest there may be unwarranted reproduction of stratification in\nacademic careers that discounts diversity's role in innovation and partly\nexplains the underrepresentation of some groups in academia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:10:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 02:31:51 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Hofstra", "Bas", ""], ["Kulkarni", "Vivek V.", ""], ["Galvez", "Sebastian Munoz-Najar", ""], ["He", "Bryan", ""], ["Jurafsky", "Dan", ""], ["McFarland", "Daniel A.", ""]]}, {"id": "1909.02169", "submitter": "Abhishek Varghese", "authors": "Abhishek Varghese (1 and 2), Christopher Drovandi (1 and 2), Kerrie\n  Mengersen (1 and 2), Antonietta Mira (3 and 4) ((1) School of Mathematical\n  Sciences, Queensland University of Technology, Brisbane, Australia, (2) ARC\n  Centre for Excellence in Mathematical and Statistical Frontiers (ACEMS), (3)\n  Institute of Computational Science, Universita della Svizzera italiana,\n  Lugano, Switzerland, (4) Department of Science and High Technology,\n  Universita degli Studi dell Insubria, Como, Italy)", "title": "Estimating a novel stochastic model for within-field disease dynamics of\n  banana bunchy top virus via approximate Bayesian computation", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007878", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Banana Bunchy Top Virus (BBTV) is one of the most economically important\nvector-borne banana diseases throughout the Asia-Pacific Basin and presents a\nsignificant challenge to the agricultural sector. Current models of BBTV are\nlargely deterministic, limited by an incomplete understanding of interactions\nin complex natural systems, and the appropriate identification of parameters. A\nstochastic network-based Susceptible-Infected model has been created which\nsimulates the spread of BBTV across the subsections of a banana plantation,\nparameterising nodal recovery, neighbouring and distant infectivity across\nsummer and winter. Findings from posterior results achieved through Markov\nChain Monte Carlo approach to approximate Bayesian computation suggest\nseasonality in all parameters, which are influenced by correlated changes in\ninspection accuracy, temperatures and aphid activity. This paper demonstrates\nhow the model may be used for monitoring and forecasting of various disease\nmanagement strategies to support policy-level decision making.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:40:46 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:08:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Varghese", "Abhishek", "", "1 and 2"], ["Drovandi", "Christopher", "", "1 and 2"], ["Mengersen", "Kerrie", "", "1 and 2"], ["Mira", "Antonietta", "", "3 and 4"]]}, {"id": "1909.02256", "submitter": "Meng Li", "authors": "Wei Tang and Meng Li", "title": "Scalable Double Regularization for 3D Nano-CT Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nano-CT (computerized tomography) has emerged as a non-destructive\nhigh-resolution cross-sectional imaging technique to effectively study the\nsub-$\\mu$m pore structure of shale, which is of fundamental importance to the\nevaluation and development of shale oil and gas. Nano-CT poses unique\nchallenges to the inverse problem of reconstructing the 3D structure due to the\nlower signal-to-noise ratio (than Micro-CT) at the nano-scale, increased\nsensitivity to the misaligned geometry caused by the movement of object\nmanipulator, limited sample size, and a larger volume of data at higher\nresolution. In this paper, we propose a scalable double regularization (SDR)\nmethod to utilize the entire dataset for simultaneous 3D structural\nreconstruction across slices through total variation regularization within\nslices and $L_1$ regularization between adjacent slices. SDR allows information\nborrowing both within and between slices, contrasting with the traditional\nmethods that usually build on slice by slice reconstruction. We develop a\nscalable and memory-efficient algorithm by exploiting the systematic sparsity\nand consistent geometry induced by such Nano-CT data. We illustrate the\nproposed method using synthetic data and two Nano-CT imaging datasets of\nJiulaodong (JLD) shale and Longmaxi (LMX) shale acquired in the Sichuan Basin.\nThese numerical experiments show that the proposed method substantially\noutperforms selected alternatives both visually and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 08:27:55 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 04:33:59 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 02:06:11 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Tang", "Wei", ""], ["Li", "Meng", ""]]}, {"id": "1909.02403", "submitter": "Robert Verschuren", "authors": "Robert Matthijs Verschuren", "title": "Predictive Claim Scores for Dynamic Multi-Product Risk Classification in\n  Insurance", "comments": null, "journal-ref": "ASTIN Bulletin (2020), 51(1), 1-25", "doi": "10.1017/asb.2020.34", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become standard practice in the non-life insurance industry to employ\nGeneralized Linear Models (GLMs) for insurance pricing. However, these GLMs\ntraditionally work only with a priori characteristics of policyholders, while\nnowadays we increasingly have a posteriori information of individual customers\navailable, sometimes even across multiple product categories. In this paper, we\ntherefore consider a dynamic claim score to capture this a posteriori\ninformation over several product lines. More specifically, we extend the\nBonus-Malus-panel model of Boucher and Inoussa (2014) and Boucher and Pigeon\n(2018) to include claim scores from other product categories and to allow for\nnon-linear effects of these scores. The application of the resulting\nmulti-product framework to a Dutch property and casualty insurance portfolio\nshows that the claims experience of individual customers can have a significant\nimpact on the risk classification and that it can be very profitable to account\nfor it.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:39:17 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 15:00:58 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 13:13:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Verschuren", "Robert Matthijs", ""]]}, {"id": "1909.02492", "submitter": "Mo Huang", "authors": "Mo Huang and Nancy R. Zhang", "title": "Reply to \"Issues arising from benchmarking single-cell RNA sequencing\n  imputation methods\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our Brief Communication (DOI: 10.1038/s41592-018-0033-z), we presented the\nmethod SAVER for recovering true gene expression levels in noisy single cell\nRNA sequencing data. We evaluated the performance of SAVER, along with\ncomparable methods MAGIC and scImpute, in an RNA FISH validation experiment and\na data downsampling experiment. In a Comment [arXiv:1908.07084v1], Li & Li were\nconcerned with the use of the downsampled datasets, specifically focusing on\nclustering results obtained from the Zeisel et al. data. Here, we will address\nthese comments and, furthermore, amend the data downsampling experiment to\ndemonstrate that the findings from the data downsampling experiment in our\nBrief Communication are valid.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 15:42:57 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Huang", "Mo", ""], ["Zhang", "Nancy R.", ""]]}, {"id": "1909.02499", "submitter": "Giuseppe Sanfilippo", "authors": "Frank Lad and Giuseppe Sanfilippo", "title": "Predictive distributions that mimic frequencies over a restricted\n  subdomain (expanded preprint version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictive distribution over a sequence of $N+1$ events is said to be\n\"frequency mimicking\" whenever the probability for the final event conditioned\non the outcome of the first $N$ events equals the relative frequency of\nsuccesses among them. Infinitely extendible exchangeable distributions that\nuniversally inhere this property are known to have several annoying concomitant\nproperties. We motivate frequency mimicking assertions over a limited subdomain\nin practical problems of finite inference, and we identify their computable\ncoherent implications. We provide some computed examples using reference\ndistributions, and we introduce computational software to generate any\nspecification. The software derives from an inversion of the finite form of the\nexchangeability representation theorem. Three new theorems delineate the extent\nof the usefulness of such distributions, and we show why it may not be\nappropriate to extend the frequency mimicking assertions for a specified value\nof $N$ to any arbitrary larger size of $N$. The constructive results identify\nthe source and structure of \"adherent masses\" in the limit of a sequence of\nfinitely additive distributions. Appendices develop a novel geometrical\nrepresentation of conditional probabilities which illuminate the analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:04:33 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lad", "Frank", ""], ["Sanfilippo", "Giuseppe", ""]]}, {"id": "1909.02527", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok, Dorothea L. Floris, Andre F. Marquand", "title": "Analyzing Brain Circuits in Population Neuroscience: A Case to Be a\n  Bayesian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity fingerprints are among today's best choices to obtain\na faithful sampling of an individual's brain and cognition in health and\ndisease. Here we make a case for key advantages of analyzing such connectome\nprofiles using Bayesian analysis strategies. They (i) afford full probability\nestimates of the studied neurocognitive phenomenon (ii) provide analytical\nmachinery to separate methodological uncertainty and biological variability in\na coherent manner (iii) usher towards avenues to go beyond classical\nnull-hypothesis significance testing and (iv) enable estimates of credibility\naround all model parameters at play and thus enable predictions with\nuncertainty intervals for single subject. We pick research questions about\nautism spectrum disorder as a recurring theme to illustrate our methodological\narguments.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:02:20 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bzdok", "Danilo", ""], ["Floris", "Dorothea L.", ""], ["Marquand", "Andre F.", ""]]}, {"id": "1909.02626", "submitter": "Brennen Fagan", "authors": "Brennen T. Fagan, Marina I. Knight, Niall J. MacKay, A. Jamie Wood", "title": "Changepoint analysis of historical battle deaths", "comments": "24 pages, 11 figures", "journal-ref": "J. R. Stat. Soc. A, 183 (2020): 909-933", "doi": "10.1111/rssa.12578", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been claimed and disputed that World War II has been followed by a\n`long peace', an unprecedented decline of war. We conduct a full changepoint\nanalysis of well-documented, publicly-available battle deaths datasets, using\nnew techniques that enable the robust detection of changes in the statistical\nproperties of such heavy-tailed data. We first test and calibrate these\ntechniques. We then demonstrate the existence of changes, independent of data\npresentation, at around 1910 and 1950 CE, bracketing the World Wars, and around\nthe 1830s and 1994 CE. Our analysis provides a methodology for future\ninvestigations and an empirical basis for political and historical discussions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:44:02 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Fagan", "Brennen T.", ""], ["Knight", "Marina I.", ""], ["MacKay", "Niall J.", ""], ["Wood", "A. Jamie", ""]]}, {"id": "1909.02664", "submitter": "Babak Ravandi", "authors": "Babak Ravandi and Arash Ravandi", "title": "Network-Based Approach for Modeling and Analyzing Coronary Angiography", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-40943-2_15", "report-no": null, "categories": "q-bio.QM physics.data-an q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant intra-observer and inter-observer variability in the\ninterpretation of coronary angiograms are reported. This variability is in part\ndue to the common practices that rely on performing visual inspections by\nspecialists (e.g., the thickness of coronaries). Quantitative Coronary\nAngiography (QCA) approaches are emerging to minimize observer's error and\nfurthermore perform predictions and analysis on angiography images. However,\nQCA approaches suffer from the same problem as they mainly rely on performing\nvisual inspections by utilizing image processing techniques.\n  In this work, we propose an approach to model and analyze the entire\ncardiovascular tree as a complex network derived from coronary angiography\nimages. This approach enables to analyze the graph structure of coronary\narteries. We conduct the assessments of network integration, degree\ndistribution, and controllability on a healthy and a diseased coronary\nangiogram. Through our discussion and assessments, we propose modeling the\ncardiovascular system as a complex network is an essential phase to fully\nautomate the interpretation of coronary angiographic images. We show how\nnetwork science can provide a new perspective to look at coronary angiograms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 22:50:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ravandi", "Babak", ""], ["Ravandi", "Arash", ""]]}, {"id": "1909.02716", "submitter": "Mahdi Abolghasemi", "authors": "Mahdi Abolghasemi, Ali Eshragh, Jason Hurley, Behnam Fahimnia", "title": "Demand Forecasting in the Presence of Systematic Events: Cases in\n  Capturing Sales Promotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable demand forecasts are critical for the effective supply chain\nmanagement. Several endogenous and exogenous variables can influence the\ndynamics of demand, and hence a single statistical model that only consists of\nhistorical sales data is often insufficient to produce accurate forecasts. In\npractice, the forecasts generated by baseline statistical models are often\njudgmentally adjusted by forecasters to incorporate factors and information\nthat are not incorporated in the baseline models. There are however systematic\nevents whose effect can be effectively quantified and modeled to help minimize\nhuman intervention in adjusting the baseline forecasts. In this paper, we\ndevelop and test a novel regime-switching approach to quantify systematic\ninformation/events and objectively incorporate them into the baseline\nstatistical model. Our simple yet practical and effective model can help limit\nforecast adjustments to only focus on the impact of less systematic events such\nas sudden climate change or dynamic market activities. The proposed model and\napproach is validated empirically using sales and promotional data from two\nAustralian companies. Discussions focus on a thorough analysis of the\nforecasting and benchmarking results. Our analysis indicates that the proposed\nmodel can successfully improve the forecast accuracy when compared to the\ncurrent industry practice which heavily relies on human judgment to factor in\nall types of information/events.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 05:17:32 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Abolghasemi", "Mahdi", ""], ["Eshragh", "Ali", ""], ["Hurley", "Jason", ""], ["Fahimnia", "Behnam", ""]]}, {"id": "1909.02913", "submitter": "Lucie Biard", "authors": "Lucie Biard, Bin Cheng, Gulam A. Manji, Shing M. Lee", "title": "A simulation study of methods for handling disease progression in\n  dose-finding clinical trials", "comments": "16 pages, 1 figure, 9 tables (6 as supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional dose-finding studies, dose-limiting toxicity (DLT) is\ndetermined within a fixed time observation window where DLT is often defined as\na binary outcome. In the setting of oncology dose-finding trials, often\npatients in advanced stage of diseases are enrolled. Therefore, disease\nprogression may occur within the DLT observation window leading to treatment\ndiscontinuation and rendering the patient unevaluable for DLT assessment. As a\nresult, additional patients have to be enrolled, increasing the sample size. We\npropose and compare several practical methods for handling disease progression\nwhich occurs within the DLT observation window, in the context of the\ntime-to-event continual reassessment method (TITE-CRM) which allows using\npartial observations. The methods differ on the way they define an evaluable\npatient and in the way incomplete observations are included. The methods are\nillustrated and contrasted in the context of a single simulated trial, and\ncompared via simulations under various scenarios of dose-progression\nrelationship, in the setting of advanced soft-tissue sarcoma.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:55:03 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Biard", "Lucie", ""], ["Cheng", "Bin", ""], ["Manji", "Gulam A.", ""], ["Lee", "Shing M.", ""]]}, {"id": "1909.02996", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Ond\\v{r}ej Sokol and Vladim\\'ir Hol\\'y", "title": "The Role of Shopping Mission in Retail Customer Segmentation", "comments": null, "journal-ref": null, "doi": "10.1177/1470785320921011", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retailing, it is important to understand customer behavior and determine\ncustomer value. A useful tool to achieve such goals is the cluster analysis of\ntransaction data. Typically, a customer segmentation is based on the recency,\nfrequency and monetary value of shopping or the structure of purchased\nproducts. We take a different approach and base our segmentation on the\nshopping mission - a reason why a customer visits the shop. Shopping missions\ninclude focused purchases of specific product categories and general purchases\nof various sizes. In an application to a Czech drugstore chain, we show that\nthe proposed segmentation brings unique information about customers and should\nbe used alongside the traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:22:26 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 19:50:47 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 13:38:26 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2020 21:04:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sokol", "Ond\u0159ej", ""], ["Hol\u00fd", "Vladim\u00edr", ""]]}, {"id": "1909.03017", "submitter": "Martin Law MSc", "authors": "Martin Law, Michael J. Grayling, Adrian P. Mander", "title": "Optimal curtailed designs for single arm phase II clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single-arm phase II oncology trials, the most popular choice of design is\nSimon's two-stage design, which allows early stopping at one interim analysis.\nHowever, the expected trial sample size can be reduced further by allowing\ncurtailment. Curtailment is stopping when the final go or no-go decision is\ncertain, so-called non-stochastic curtailment, or very likely, known as\nstochastic curtailment.\n  In the context of single-arm phase II oncology trials, stochastic curtailment\nhas previously been restricted to stopping in the second stage and/or stopping\nfor a no-go decision only. We introduce two designs that incorporate stochastic\ncurtailment and allow stopping after every observation, for either a go or\nno-go decision. We obtain optimal stopping boundaries by searching over a range\nof potential conditional powers, beyond which the trial will stop for a go or\nno-go decision. This search is novel: firstly, the search is undertaken over a\nrange of values unique to each possible design realisation. Secondly, these\nvalues are evaluated taking into account the possibility of early stopping.\nFinally, each design realisation's operating characteristics are obtained\nexactly.\n  The proposed designs are compared to existing designs in a real data example.\nThey are also compared under three scenarios, both with respect to four single\noptimality criteria and using a loss function.\n  The proposed designs are superior in almost all cases. Optimising for the\nexpected sample size under either the null or alternative hypothesis, the\nsaving compared to the popular Simon's design ranges from 22% to 55%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 17:13:24 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Law", "Martin", ""], ["Grayling", "Michael J.", ""], ["Mander", "Adrian P.", ""]]}, {"id": "1909.03441", "submitter": "Chieh Wu T", "authors": "Chieh Wu, Stratis Ioannidis, Mario Sznaier, Xiangyu Li, David Kaeli,\n  Jennifer G. Dy", "title": "Iterative Spectral Method for Alternative Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset and an existing clustering as input, alternative clustering\naims to find an alternative partition. One of the state-of-the-art approaches\nis Kernel Dimension Alternative Clustering (KDAC). We propose a novel Iterative\nSpectral Method (ISM) that greatly improves the scalability of KDAC. Our\nalgorithm is intuitive, relies on easily implementable spectral decompositions,\nand comes with theoretical guarantees. Its computation time improves upon\nexisting implementations of KDAC by as much as 5 orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:15:20 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wu", "Chieh", ""], ["Ioannidis", "Stratis", ""], ["Sznaier", "Mario", ""], ["Li", "Xiangyu", ""], ["Kaeli", "David", ""], ["Dy", "Jennifer G.", ""]]}, {"id": "1909.03457", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu (1) and Benjamin Paul Chamberlain (2) ((1) ASOS.com,\n  (2) Twitter Inc)", "title": "What is the value of experimentation & measurement?", "comments": "Accepted into IEEE International Conference on Data Mining (ICDM)\n  2019. Main paper: 6 pages, 3 figures; Supplementary document: 7 pages, 2\n  figures. Code available on:\n  https://github.com/liuchbryan/value_of_experimentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimentation and Measurement (E&M) capabilities allow organizations to\naccurately assess the impact of new propositions and to experiment with many\nvariants of existing products. However, until now, the question of measuring\nthe measurer, or valuing the contribution of an E&M capability to\norganizational success has not been addressed. We tackle this problem by\nanalyzing how, by decreasing estimation uncertainty, E&M platforms allow for\nbetter prioritization. We quantify this benefit in terms of expected relative\nimprovement in the performance of all new propositions and provide guidance for\nhow much an E&M capability is worth and when organizations should invest in\none.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:01:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1909.03575", "submitter": "Hamed Nikbakht", "authors": "Hamed Nikbakht, Konstantinos G. Papakonstantinou", "title": "A direct Hamiltonian MCMC approach for reliability estimation", "comments": "UNCECOMP 2019; 3rd ECCOMAS Thematic Conference on Uncertainty\n  Quantification in Computational Sciences and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient estimation of rare events probabilities is of\nsignificant importance, since often the occurrences of such events have\nwidespread impacts. The focus in this work is on precisely quantifying these\nprobabilities, often encountered in reliability analysis of complex engineering\nsystems, by introducing a gradient-based Hamiltonian Markov Chain Monte Carlo\n(HMCMC) framework, termed Approximate Sampling Target with Post-processing\nAdjustment (ASTPA). The basic idea is to construct a relevant target\ndistribution by weighting the high-dimensional random variable space through a\none-dimensional likelihood model, using the limit-state function. To sample\nfrom this target distribution we utilize HMCMC algorithms that produce Markov\nchain samples based on Hamiltonian dynamics rather than random walks. We\ncompare the performance of typical HMCMC scheme with our newly developed\nQuasi-Newton based mass preconditioned HMCMC algorithm that can sample very\nadeptly, particularly in difficult cases with high-dimensionality and very\nsmall failure probabilities. To eventually compute the probability of interest,\nan original post-sampling step is devised at this stage, using an inverse\nimportance sampling procedure based on the samples. The involved user-defined\nparameters of ASTPA are then discussed and general default values are\nsuggested. Finally, the performance of the proposed methodology is examined in\ndetail and compared against Subset Simulation in a series of static and dynamic\nlow- and high-dimensional benchmark problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 00:41:54 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:24:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Nikbakht", "Hamed", ""], ["Papakonstantinou", "Konstantinos G.", ""]]}, {"id": "1909.03723", "submitter": "Marco Virgolin", "authors": "Marco Virgolin, Ziyuan Wang, Tanja Alderliesten, Peter A. N. Bosman", "title": "Machine learning for automatic construction of pseudo-realistic\n  pediatric abdominal phantoms", "comments": "Currently submitted to SPIE Medical Imaging journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.med-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is proving extremely beneficial in many healthcare\napplications. In pediatric oncology, retrospective studies that investigate the\nrelationship between treatment and late adverse effects still rely on simple\nheuristics. To assess the effects of radiation therapy, treatment plans are\ntypically simulated on phantoms, i.e., virtual surrogates of patient anatomy.\nCurrently, phantoms are built according to reasonable, yet simple,\nhuman-designed criteria. This often results in a lack of individualization. We\npresent a novel approach that combines imaging and ML to build individualized\nphantoms automatically. Given the features of a patient treated historically\n(only 2D radiographs available), and a database of 3D Computed Tomography (CT)\nimaging with organ segmentations and relative patient features, our approach\nuses ML to predict how to assemble a patient-specific phantom automatically.\nExperiments on 60 abdominal CTs of pediatric patients show that our approach\nconstructs significantly more representative phantoms than using current\nphantom building criteria, in terms of location and shape of the abdomen and of\ntwo considered organs, the liver and the spleen. Among several ML algorithms\nconsidered, the Gene-pool Optimal Mixing Evolutionary Algorithm for Genetic\nProgramming (GP-GOMEA) is found to deliver the best performing models, which\nare, moreover, transparent and interpretable mathematical expressions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:38:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Virgolin", "Marco", ""], ["Wang", "Ziyuan", ""], ["Alderliesten", "Tanja", ""], ["Bosman", "Peter A. N.", ""]]}, {"id": "1909.03793", "submitter": "John Kent", "authors": "John T. Kent and Shambo Bhattacharjee and Weston R. Faber and Islam I.\n  Hussein", "title": "Revisiting the orbital tracking problem", "comments": "37 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a space object in an orbit about the earth. An uncertain initial\nstate can be represented as a point cloud which can be propagated to later\ntimes by the laws of Newtonian motion. If the state of the object is\nrepresented in Cartesian earth centered inertial (Cartesian-ECI) coordinates,\nthen even if initial uncertainty is Gaussian in this coordinate system, the\ndistribution quickly becomes non-Gaussian as the propagation time increases.\nSimilar problems arise in other standard fixed coordinate systems in\nastrodynamics, e.g. Keplerian and to some extent equinoctial. To address these\nproblems, a local \"Adapted STructural (AST)'' coordinate system has been\ndeveloped in which uncertainty is represented in terms of deviations from a\n\"central state\".\n  Given a sequence of angles-only measurements, the iterated nonlinear extended\n(IEKF) and unscented (IUKF) Kalman filters are often the most appropriate\nvariants to use. In particular, they can be much more accurate than the more\ncommonly used non-iterated versions, the extended (EKF) and unscented (UKF)\nKalman filters, especially under high eccentricity. In addition, iterated\nKalman filters can often be well-approximated by two new closed form filters,\nthe observation-centered extended (OCEKF) and unscented (OCUKF) Kalman filters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:36:08 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 16:07:38 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kent", "John T.", ""], ["Bhattacharjee", "Shambo", ""], ["Faber", "Weston R.", ""], ["Hussein", "Islam I.", ""]]}, {"id": "1909.03801", "submitter": "Michael Sachs", "authors": "Michael C Sachs and Arvid Sj\\\"olander and Erin E Gabriel", "title": "Aim for clinical utility, not just predictive accuracy", "comments": "Submitted to Epidemiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The predictions from an accurate prognostic model can be of great interest to\npatients and clinicians. When predictions are reported to individuals, they may\ndecide to take action to improve their health or they may simply be comforted\nby the knowledge. However, if there is a clearly defined space of actions in\nthe clinical context, a formal decision rule based on the prediction has the\npotential to have a much broader impact. Even if it is not the intended use of\na developed prediction model, informal decision rules can often be found in\npractice. The use of a prediction-based decision rule should be formalized and\ncompared to the standard of care in a randomized trial to assess its clinical\nutility, however, evidence is needed to motivate such a trial. We outline how\nobservational data can be used to propose a decision rule based on a prognostic\nprediction model. We then propose a framework for emulating a prediction driven\ntrial to evaluate the utility of a prediction-based decision rule in\nobservational data. A split-sample structure can and should be used to develop\nthe prognostic model, define the decision rule, and evaluate its clinical\nutility.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 12:55:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sachs", "Michael C", ""], ["Sj\u00f6lander", "Arvid", ""], ["Gabriel", "Erin E", ""]]}, {"id": "1909.03802", "submitter": "Silvia Montagna", "authors": "Silvia Montagna, Vanessa Orani, Raffaele Argiento", "title": "Bayesian isotonic logistic regression via constrained splines: an\n  application to estimating the serve advantage in professional tennis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In professional tennis, it is often acknowledged that the server has an\ninitial advantage. Indeed, the majority of points are won by the server, making\nthe serve one of the most important elements in this sport. In this paper, we\nfocus on the role of the serve advantage in winning a point as a function of\nthe rally length. We propose a Bayesian isotonic logistic regression model for\nthe probability of winning a point on serve. In particular, we decompose the\nlogit of the probability of winning via a linear combination of B-splines basis\nfunctions, with athlete-specific basis function coefficients. Further, we\nensure the serve advantage decreases with rally length by imposing constraints\non the spline coefficients. We also consider the rally ability of each player,\nand study how the different types of court may impact on the player's rally\nability. We apply our methodology to a Grand Slam singles matches dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 09:07:45 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Montagna", "Silvia", ""], ["Orani", "Vanessa", ""], ["Argiento", "Raffaele", ""]]}, {"id": "1909.03813", "submitter": "Alessandro Gasparini", "authors": "Alessandro Gasparini and Tim P. Morris and Michael J. Crowther", "title": "INTEREST: INteractive Tool for Exploring REsults from Simulation sTudies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies allow us to explore the properties of statistical methods.\nThey provide a powerful tool with a multiplicity of aims; among others:\nevaluating and comparing new or existing statistical methods, assessing\nviolations of modelling assumptions, helping with the understanding of\nstatistical concepts, and supporting the design of clinical trials. The\nincreased availability of powerful computational tools and usable software has\ncontributed to the rise of simulation studies in the current literature.\nHowever, simulation studies involve increasingly complex designs, making it\ndifficult to provide all relevant results clearly. Dissemination of results\nplays a focal role in simulation studies: it can drive applied analysts to use\nmethods that have been shown to perform well in their settings, guide\nresearchers to develop new methods in a promising direction, and provide\ninsights into less established methods. It is crucial that we can digest\nrelevant results of simulation studies. Therefore, we developed INTEREST: an\nINteractive Tool for Exploring REsults from Simulation sTudies. The tool has\nbeen developed using the Shiny framework in R and is available as a web app or\nas a standalone package. It requires uploading a tidy format dataset with the\nresults of a simulation study in R, Stata, SAS, SPSS, or comma-separated\nformat. A variety of performance measures are estimated automatically along\nwith Monte Carlo standard errors; results and performance summaries are\ndisplayed both in tabular and graphical fashion, with a wide variety of\navailable plots. Consequently, the reader can focus on simulation parameters\nand estimands of most interest. In conclusion, INTEREST can facilitate the\ninvestigation of results from simulation studies and supplement the reporting\nof results, allowing researchers to share detailed results from their\nsimulations and readers to explore them freely.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:47:43 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 13:24:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gasparini", "Alessandro", ""], ["Morris", "Tim P.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "1909.03816", "submitter": "Yawen Guan", "authors": "Yawen Guan, Brian J Reich, James A Mulholland, and Howard H Chang", "title": "Multivariate spectral downscaling for PM2.5 species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine particulate matter (PM2.5) is a mixture of air pollutants that has\nadverse effects on human health. Understanding the health effects of PM2.5\nmixture and its individual species has been a research priority over the past\ntwo decades. However, the limited availability of speciated PM2.5 measurements\ncontinues to be a major challenge in exposure assessment for conducting\nlarge-scale population-based epidemiology studies. The PM2.5 species have\ncomplex spatial-temporal and cross dependence structures that should be\naccounted for in estimating the spatiotemporal distribution of each component.\nTwo major sources of air quality data are commonly used for deriving exposure\nestimates: point-level monitoring data and gridded numerical computer model\nsimulation, such as the Community Multiscale Air Quality (CMAQ) model. We\npropose a statistical method to combine these two data sources for estimating\nspeciated PM2.5 concentration. Our method models the complex relationships\nbetween monitoring measurements and the numerical model output at different\nspatial resolutions, and we model the spatial dependence and cross dependence\namong PM2.5 species. We apply the method to combine CMAQ model output with\nmajor PM2.5 species measurements in the contiguous United States in 2011.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:36:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Guan", "Yawen", ""], ["Reich", "Brian J", ""], ["Mulholland", "James A", ""], ["Chang", "Howard H", ""]]}, {"id": "1909.04196", "submitter": "Yohei Sawada", "authors": "Yohei Sawada", "title": "Machine learning accelerates parameter optimization and uncertainty\n  assessment of a land surface model", "comments": "53 pages, 19 figures", "journal-ref": null, "doi": "10.1029/2020JD032688", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of land surface models (LSMs) significantly affects the\nunderstanding of atmospheric and related processes. Many of the LSMs' soil and\nvegetation parameters were unknown so that it is crucially important to\nefficiently optimize them. Here I present a globally applicable and\ncomputationally efficient method for parameter optimization and uncertainty\nassessment of the LSM by combining Markov Chain Monte Carlo (MCMC) with machine\nlearning. First, I performed the long-term (decadal scales) ensemble simulation\nof the LSM, in which each ensemble member has different parameters' values, and\ncalculated the gap between simulation and observation, or the cost function,\nfor each ensemble member. Second, I developed the statistical machine learning\nbased surrogate model, which is computationally cheap but accurately mimics the\nrelationship between parameters and the cost function, by applying the Gaussian\nprocess regression to learn the model simulation. Third, we applied MCMC by\nrepeatedly driving the surrogate model to get the posterior probabilistic\ndistribution of parameters. Using satellite passive microwave brightness\ntemperature observations, both synthetic and real-data experiments in the Sahel\nregion of west Africa were performed to optimize unknown soil and vegetation\nparameters of the LSM. The primary findings are (1) the proposed method is\n50,000 times as fast as the direct application of MCMC to the full LSM; (2) the\nskill of the LSM to simulate both soil moisture and vegetation dynamics can be\nimproved; (3) I successfully quantify the characteristics of equifinality by\nobtaining the full non-parametric probabilistic distribution of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 23:47:03 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 00:53:45 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sawada", "Yohei", ""]]}, {"id": "1909.04222", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Uma Roy and Caroline Uhler", "title": "Covariance Matrix Estimation under Total Positivity for Portfolio\n  Selection", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the optimal Markowitz porfolio depends on estimating the covariance\nmatrix of the returns of $N$ assets from $T$ periods of historical data.\nProblematically, $N$ is typically of the same order as $T$, which makes the\nsample covariance matrix estimator perform poorly, both empirically and\ntheoretically. While various other general purpose covariance matrix estimators\nhave been introduced in the financial economics and statistics literature for\ndealing with the high dimensionality of this problem, we here propose an\nestimator that exploits the fact that assets are typically positively\ndependent. This is achieved by imposing that the joint distribution of returns\nbe multivariate totally positive of order 2 ($\\text{MTP}_2$). This constraint\non the covariance matrix not only enforces positive dependence among the\nassets, but also regularizes the covariance matrix, leading to desirable\nstatistical properties such as sparsity. Based on stock-market data spanning\nover thirty years, we show that estimating the covariance matrix under\n$\\text{MTP}_2$ outperforms previous state-of-the-art methods including\nshrinkage estimators and factor models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:16:16 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 04:32:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Agrawal", "Raj", ""], ["Roy", "Uma", ""], ["Uhler", "Caroline", ""]]}, {"id": "1909.04293", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Christoph Bergmeir and Hansika Hewamalage", "title": "LSTM-MSNet: Leveraging Forecasts on Sets of Related Time Series with\n  Multiple Seasonal Patterns", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.2985720", "report-no": null, "categories": "stat.AP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating forecasts for time series with multiple seasonal cycles is an\nimportant use-case for many industries nowadays. Accounting for the\nmulti-seasonal patterns becomes necessary to generate more accurate and\nmeaningful forecasts in these contexts. In this paper, we propose Long\nShort-Term Memory Multi-Seasonal Net (LSTM-MSNet), a decomposition based,\nunified prediction framework to forecast time series with multiple seasonal\npatterns. The current state of the art in this space are typically univariate\nmethods, in which the model parameters of each time series are estimated\nindependently. Consequently, these models are unable to include key patterns\nand structures that may be shared by a collection of time series. In contrast,\nLSTM-MSNet is a globally trained Long Short-Term Memory network (LSTM), where a\nsingle prediction model is built across all the available time series to\nexploit the cross series knowledge in a group of related time series.\nFurthermore, our methodology combines a series of state-of-the-art\nmultiseasonal decomposition techniques to supplement the LSTM learning\nprocedure. In our experiments, we are able to show that on datasets from\ndisparate data sources, like e.g. the popular M4 forecasting competition, a\ndecomposition step is beneficial, whereas in the common real-world situation of\nhomogeneous series from a single application, exogenous seasonal variables or\nno seasonal preprocessing at all are better choices. All options are readily\nincluded in the framework and allow us to achieve competitive results for both\ncases, outperforming many state-of-the-art multi-seasonal forecasting methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 05:15:24 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 12:52:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bandara", "Kasun", ""], ["Bergmeir", "Christoph", ""], ["Hewamalage", "Hansika", ""]]}, {"id": "1909.04436", "submitter": "Martin Shepperd", "authors": "Martin Shepperd, Yuchen Guo, Ning Li, Mahir Arzoky, Andrea Capiluppi,\n  Steve Counsell, Giuseppe Destefanis, Stephen Swift, Allan Tucker, and Leila\n  Yousefi", "title": "The Prevalence of Errors in Machine Learning Experiments", "comments": "20th International Conference on Intelligent Data Engineering and\n  Automated Learning (IDEAL), 14--16 November 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context: Conducting experiments is central to research machine learning\nresearch to benchmark, evaluate and compare learning algorithms. Consequently\nit is important we conduct reliable, trustworthy experiments. Objective: We\ninvestigate the incidence of errors in a sample of machine learning experiments\nin the domain of software defect prediction. Our focus is simple arithmetical\nand statistical errors. Method: We analyse 49 papers describing 2456 individual\nexperimental results from a previously undertaken systematic review comparing\nsupervised and unsupervised defect prediction classifiers. We extract the\nconfusion matrices and test for relevant constraints, e.g., the marginal\nprobabilities must sum to one. We also check for multiple statistical\nsignificance testing errors. Results: We find that a total of 22 out of 49\npapers contain demonstrable errors. Of these 7 were statistical and 16 related\nto confusion matrix inconsistency (one paper contained both classes of error).\nConclusions: Whilst some errors may be of a relatively trivial nature, e.g.,\ntranscription errors their presence does not engender confidence. We strongly\nurge researchers to follow open science principles so errors can be more easily\nbe detected and corrected, thus as a community reduce this worryingly high\nerror rate with our computational experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:32:00 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Shepperd", "Martin", ""], ["Guo", "Yuchen", ""], ["Li", "Ning", ""], ["Arzoky", "Mahir", ""], ["Capiluppi", "Andrea", ""], ["Counsell", "Steve", ""], ["Destefanis", "Giuseppe", ""], ["Swift", "Stephen", ""], ["Tucker", "Allan", ""], ["Yousefi", "Leila", ""]]}, {"id": "1909.04817", "submitter": "Matthew van Bommel", "authors": "Matthew van Bommel, Luke Bornn, Peter Chow-White, Chuancong Gao", "title": "Home Sweet Home: Quantifying Home Court Advantages For NCAA Basketball\n  Statistics", "comments": "24 pages, 4 figures", "journal-ref": "Journal of Sports Analytics, vol. 7, no. 1, pp. 25-36, 2021", "doi": "10.3233/JSA-200450", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Box score statistics are the baseline measures of performance for National\nCollegiate Athletic Association (NCAA) basketball. Between the 2011-2012 and\n2015-2016 seasons, NCAA teams performed better at home compared to on the road\nin nearly all box score statistics across both genders and all three divisions.\nUsing box score data from over 100,000 games spanning the three divisions for\nboth women and men, we examine the factors underlying this discrepancy. The\nprevalence of neutral location games in the NCAA provides an additional angle\nthrough which to examine the gaps in box score statistic performance, which we\nbelieve has been underutilized in existing literature. We also estimate a\nregression model to quantify the home court advantages for box score statistics\nafter controlling for other factors such as number of possessions, and team\nstrength. Additionally, we examine the biases of scorekeepers and referees. We\npresent evidence that scorekeepers tend to have greater home team biases when\nobserving men compared to women, higher divisions compared to lower divisions,\nand stronger teams compared to weaker teams. Finally, we present statistically\nsignificant results indicating referee decisions are impacted by attendance,\nwith larger crowds resulting in greater bias in favor of the home team.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 02:02:30 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 18:57:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["van Bommel", "Matthew", ""], ["Bornn", "Luke", ""], ["Chow-White", "Peter", ""], ["Gao", "Chuancong", ""]]}, {"id": "1909.04926", "submitter": "Robert Cowell", "authors": "Robert G. Cowell", "title": "A sub-critical branching process model for application to analysing Y\n  haplotype DNA mixtures", "comments": "53 pages, 4 figures. Submitted to the Electronic Journal of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of short-tandem-repeat (STR) loci on the Y chromosome presents\nspecial problems in the forensic analysis of DNA mixtures, chiefly but not\nexclusively relating to the linkage of Y-STR loci which precludes the use of\nthe `product rule' for estimating Y-haplotype match probabilities. In recent\npaper, Andersen and Balding(2017) estimated, via a population simulation model,\nthe distribution of the number of haplotypes sharing a common profile over a\nset of Y-STR loci, and argued for its use as an alternative to estimating\nY-haplotype match probabilities.\n  In this paper we present a sub-critical branching process model that\napproximates their population model, and show how to estimate the haplotype\nnumber distribution numerically using multivariate probability generating\nfunctions. It is shown that the approximation provides a good fit to their\nsimulations. The model is extended to propose a new framework for evaluating\nthe weight-of-evidence of Y-STR haplotype mixtures, and it is illustrated with\npublicly available data of a three person DNA mixture.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 08:55:14 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Cowell", "Robert G.", ""]]}, {"id": "1909.04990", "submitter": "Aditya Mishra", "authors": "Aditya Mishra, Christian L. M\u007fuller", "title": "Robust Regression with Compositional Covariates", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological high-throughput data sets, such as targeted amplicon-based\nand metagenomic sequencing data, are compositional in nature. A common\nexploratory data analysis task is to infer statistical associations between the\nhigh-dimensional microbial compositions and habitat- or host-related\ncovariates. We propose a general robust statistical regression framework,\nRobRegCC (Robust Regression with Compositional Covariates), which extends the\nlinear log-contrast model by a mean shift formulation for capturing outliers.\nRobRegCC includes sparsity-promoting convex and non-convex penalties for\nparsimonious model estimation, a data-driven robust initialization procedure,\nand a novel robust cross-validation model selection scheme. We show RobRegCC's\nability to perform simultaneous sparse log-contrast regression and outlier\ndetection over a wide range of simulation settings and provide theoretical\nnon-asymptotic guarantees for the underlying estimators. To demonstrate the\nseamless applicability of the workflow on real data, we consider a gut\nmicrobiome data set from HIV patients and infer robust associations between a\nsparse set of microbial species and host immune response from soluble CD14\nmeasurements. All experiments are fully reproducible and available on GitHub at\nhttps://github.com/amishra-stats/robregcc.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:59:14 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 08:14:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mishra", "Aditya", ""], ["M\u007fuller", "Christian L.", ""]]}, {"id": "1909.05018", "submitter": "Steven Thompson", "authors": "Steve Thompson", "title": "Design-adherent estimators for network surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network surveys of key populations at risk for HIV are an essential part of\nthe effort to understand how the epidemic spreads and how it can be prevented.\nEstimation of population values from the sample data has been probematical,\nhowever, because the link-tracing of the network surveys includes different\npeople in the sample with unequal probabilities, and these inclusion\nprobabilities have to be estimated accurately to avoid large biases in survey\nestimates. A new approach to estimation is introduced here, based on resampling\nthe sample network many times using a design that adheres to main features of\nthe design used in the field. These features include network link tracing,\nbranching, and without-replacement sampling. The frequency that a person is\nincluded in the resamples is used to estimate the inclusion probability for\neach person in the original sample, and these estimates of inclusion\nprobabilities are used in an unequal-probability estimator. In simulations\nusing a population of drug users, sex workers, and their partners for which the\nactual values of population characteristics are known, the design-adherent\nestimation approach increases the accuracy of estimates of population\nquantities, largely by eliminating most of the biases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 21:06:19 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Thompson", "Steve", ""]]}, {"id": "1909.05097", "submitter": "Chieh Wu T", "authors": "Chieh Wu, Jared Miller, Yale Chang, Mario Sznaier, Jennifer Dy", "title": "Spectral Non-Convex Optimization for Dimension Reduction with\n  Hilbert-Schmidt Independence Criterion", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.03093", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hilbert Schmidt Independence Criterion (HSIC) is a kernel dependence\nmeasure that has applications in various aspects of machine learning.\nConveniently, the objectives of different dimensionality reduction applications\nusing HSIC often reduce to the same optimization problem. However, the\nnonconvexity of the objective function arising from non-linear kernels poses a\nserious challenge to optimization efficiency and limits the potential of\nHSIC-based formulations. As a result, only linear kernels have been\ncomputationally tractable in practice. This paper proposes a spectral-based\noptimization algorithm that extends beyond the linear kernel. The algorithm\nidentifies a family of suitable kernels and provides the first and second-order\nlocal guarantees when a fixed point is reached. Furthermore, we propose a\nprincipled initialization strategy, thereby removing the need to repeat the\nalgorithm at random initialization points. Compared to state-of-the-art\noptimization algorithms, our empirical results on real data show a run-time\nimprovement by as much as a factor of $10^5$ while consistently achieving lower\ncost and classification/clustering errors. The implementation source code is\npublicly available on https://github.com/endsley.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 20:41:04 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Wu", "Chieh", ""], ["Miller", "Jared", ""], ["Chang", "Yale", ""], ["Sznaier", "Mario", ""], ["Dy", "Jennifer", ""]]}, {"id": "1909.05142", "submitter": "Majnu John", "authors": "Sujit Vettam, Majnu John", "title": "Regularized deep learning with nonconvex penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization methods are often employed in deep learning neural networks\n(DNNs) to prevent overfitting. For penalty based DNN regularization methods,\nconvex penalties are typically considered because of their optimization\nguarantees. Recent theoretical work have shown that nonconvex penalties that\nsatisfy certain regularity conditions are also guaranteed to perform well with\nstandard optimization algorithms. In this paper, we examine new and currently\nexisting nonconvex penalties for DNN regularization. We provide theoretical\njustifications for the new penalties and also assess the performance of all\npenalties with DNN analyses of seven datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:32:44 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 15:48:42 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 18:24:15 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 19:56:37 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Vettam", "Sujit", ""], ["John", "Majnu", ""]]}, {"id": "1909.05237", "submitter": "Samuele Grillo", "authors": "Davide Beretta, Samuele Grillo, Davide Pigoli, Enea Bionda, Claudio\n  Bossi and Carlo Tornelli", "title": "Functional Principal Component Analysis as a Versatile Technique to\n  Understand and Predict the Electric Consumption Patterns", "comments": "Accepted for publication on Sustainable Energy, Grids and Networks\n  (Elsevier)", "journal-ref": null, "doi": "10.1016/j.segan.2020.100308", "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and predicting the electric consumption patterns in the short-,\nmid- and long-term, at the distribution and transmission level, is a\nfundamental asset for smart grids infrastructure planning, dynamic network\nreconfiguration, dynamic energy pricing and savings, and thus energy\nefficiency. This work introduces the Functional Principal Component Analysis\n(FPCA) as a versatile method to both investigate and predict, at different\nlevel of spatial aggregation, the consumption patterns. The method was applied\nto a unique and sensitive dataset that includes electric consumption and\ncontractual information of Milan metropolitan area. The decomposition of the\nload patterns into principal functions was found to be a powerful method to\nidentify the physical and behavioral causes underlying the daily consumptions,\ngiven knowledge of exogenous variables such as calendar and meteorological\ndata. The effectiveness of long-term predictions based on principal functions\nwas proved on Milan's metropolitan area data and assessed on a\npublicly-available dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:48:29 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 18:50:24 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 09:31:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Beretta", "Davide", ""], ["Grillo", "Samuele", ""], ["Pigoli", "Davide", ""], ["Bionda", "Enea", ""], ["Bossi", "Claudio", ""], ["Tornelli", "Carlo", ""]]}, {"id": "1909.05481", "submitter": "Aurelie Muller-Gueudin", "authors": "B\\'erang\\`ere Bastien, Taha Boukhobza (CRAN), H\\'el\\`ene Dumond\n  (CRAN), Anne G\\'egout-Petit (BIGS, IECL), Aur\\'elie Muller-Gueudin (BIGS,\n  IECL), Charl\\`ene Thi\\'ebaut (CRAN)", "title": "A statistical methodology to select covariates in high-dimensional data\n  under dependence. Application to the classification of genetic profiles in\n  oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology for selecting and ranking covariates associated\nwith a variable of interest in a context of high-dimensional data under\ndependence but few observations. The methodology successively intertwines the\nclustering of covariates, decorrelation of covariates using Factor Latent\nAnalysis, selection using aggregation of adapted methods and finally ranking.\nSimulations study shows the interest of the decorrelation inside the different\nclusters of covariates. We first apply our method to transcriptomic data of 37\npatients with advanced non-small-cell lung cancer who have received\nchemotherapy, to select the transcriptomic covariates that explain the survival\noutcome of the treatment. Secondly, we apply our method to 79 breast tumor\nsamples to define patient profiles for a new metastatic biomarker and\nassociated gene network in order to personalize the treatments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:45:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bastien", "B\u00e9rang\u00e8re", "", "CRAN"], ["Boukhobza", "Taha", "", "CRAN"], ["Dumond", "H\u00e9l\u00e8ne", "", "CRAN"], ["G\u00e9gout-Petit", "Anne", "", "BIGS, IECL"], ["Muller-Gueudin", "Aur\u00e9lie", "", "BIGS,\n  IECL"], ["Thi\u00e9baut", "Charl\u00e8ne", "", "CRAN"]]}, {"id": "1909.05501", "submitter": "G\\'abor Petneh\\'azi", "authors": "G\\'abor Petneh\\'azi and J\\'ozsef G\\'all", "title": "Mortality rate forecasting: can recurrent neural networks beat the\n  Lee-Carter model?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article applies a long short-term memory recurrent neural network to\nmortality rate forecasting. The model can be trained jointly on the mortality\nrate history of different countries, ages, and sexes. The RNN-based method\nseems to outperform the popular Lee-Carter model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:22:56 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 06:55:55 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Petneh\u00e1zi", "G\u00e1bor", ""], ["G\u00e1ll", "J\u00f3zsef", ""]]}, {"id": "1909.05571", "submitter": "Christian R\\\"over", "authors": "M. Zabel, S. Schl\\\"ogl, A. Lubinski, J. H. Svendsen, A. Bauer, E.\n  Arbelo, S. Brusich, D. Conen, I. Cygankiewicz, M. Dommasch, P. Flevari, J.\n  Galuszka, J. Hansen, G. Hasenfu{\\ss}, R. Hatala, H. V. Huikuri, T. Kentt\\\"a,\n  T. Kuczejko, H. Haarmann, M. Harden, S. Iovev, S. K\\\"a\\\"ab, G. Kaliska, A.\n  Katsimardos, J. D. Kasprzak, D. Qavoq, L. L\\\"uthje, M. Malik, T. Novotny, N.\n  Pavlovic, P. Perge, C. R\\\"over, G. Schmidt, T. Shalganov, R. Sritharan, M.\n  Svetlosak, Z. Sallo, J. Szavits-Nossan, V. Traykov, B. Vandenberk, V.\n  Velchec, M. A. Vos, S. N. Willich, T. Friede, R. Willems, B. Merkely, C.\n  Sticherling, the EU-CERT-ICD study investigators", "title": "Present criteria for prophylactic ICD implantation: Insights from the\n  EU-CERT-ICD (Comparative Effectiveness Research to Assess the Use of Primary\n  ProphylacTic Implantable Cardioverter Defibrillators in EUrope) project", "comments": "22 pages, 3 figures", "journal-ref": "Journal of Electrocardiology, 2019", "doi": "10.1016/j.jelectrocard.2019.09.001", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  BACKGROUND. The clinical effectiveness of primary prevention implantable\ncardioverter defibrillator (ICD) therapy is under debate. It is urgently needed\nto better identify patients who benefit from prophylactic ICD therapy. The\nEUropean Comparative Effectiveness Research to Assess the Use of Primary\nProphylacTic Implantable Cardioverter Defibrillators (EU-CERT-ICD) completed in\n2019 will assess this issue.\n  SUMMARY. The EU-CERT-ICD is a prospective investigator-initiated\nnon-randomized, controlled, multicenter observational cohort study done in 44\ncenters across 15 European countries. A total of 2327 patients with heart\nfailure due to ischemic heart disease or dilated cardiomyopathy indicated for\nprimary prophylactic ICD implantation were recruited between 2014 and 2018\n(>1500 patients at first ICD implantation, >750 patients non-randomized non-ICD\ncontrol group). The primary endpoint was all-cause mortality, first appropriate\nshock was co-primary endpoint. At baseline, all patients underwent 12-lead ECG\nand Holter-ECG analysis using multiple advanced methods for risk stratification\nas well as documentation of clinical characteristics and laboratory values. The\nEU-CERT-ICD data will provide much needed information on the survival benefit\nof preventive ICD therapy and expand on previous prospective risk\nstratification studies which showed very good applicability of clinical\nparameters and advanced risk stratifiers in order to define patient subgroups\nwith above or below average ICD benefit.\n  CONCLUSION. The EU-CERT-ICD study will provide new and current data about\neffectiveness of primary prophylactic ICD implantation. The study also aims for\nimproved risk stratification and patient selection using clinical risk markers\nin general, and advanced ECG risk markers in particular.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:20:38 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zabel", "M.", ""], ["Schl\u00f6gl", "S.", ""], ["Lubinski", "A.", ""], ["Svendsen", "J. H.", ""], ["Bauer", "A.", ""], ["Arbelo", "E.", ""], ["Brusich", "S.", ""], ["Conen", "D.", ""], ["Cygankiewicz", "I.", ""], ["Dommasch", "M.", ""], ["Flevari", "P.", ""], ["Galuszka", "J.", ""], ["Hansen", "J.", ""], ["Hasenfu\u00df", "G.", ""], ["Hatala", "R.", ""], ["Huikuri", "H. V.", ""], ["Kentt\u00e4", "T.", ""], ["Kuczejko", "T.", ""], ["Haarmann", "H.", ""], ["Harden", "M.", ""], ["Iovev", "S.", ""], ["K\u00e4\u00e4b", "S.", ""], ["Kaliska", "G.", ""], ["Katsimardos", "A.", ""], ["Kasprzak", "J. D.", ""], ["Qavoq", "D.", ""], ["L\u00fcthje", "L.", ""], ["Malik", "M.", ""], ["Novotny", "T.", ""], ["Pavlovic", "N.", ""], ["Perge", "P.", ""], ["R\u00f6ver", "C.", ""], ["Schmidt", "G.", ""], ["Shalganov", "T.", ""], ["Sritharan", "R.", ""], ["Svetlosak", "M.", ""], ["Sallo", "Z.", ""], ["Szavits-Nossan", "J.", ""], ["Traykov", "V.", ""], ["Vandenberk", "B.", ""], ["Velchec", "V.", ""], ["Vos", "M. A.", ""], ["Willich", "S. N.", ""], ["Friede", "T.", ""], ["Willems", "R.", ""], ["Merkely", "B.", ""], ["Sticherling", "C.", ""], ["investigators", "the EU-CERT-ICD study", ""]]}, {"id": "1909.05575", "submitter": "Mar\\'ia \\'Alvarez Hern\\'andez", "authors": "A. Mart\\'in Andr\\'es and M. \\'Alvarez Hern\\'andez", "title": "Multi-rater delta: extending the delta nominal measure of agreement\n  between two raters to many raters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to measure the degree of agreement among R raters who independently\nclassify n subjects within K nominal categories is frequent in many scientific\nareas. The most popular measures are Cohen's kappa (R = 2), Fleiss' kappa,\nConger's kappa and Hubert's kappa (R $\\geq$ 2) coefficients, which have several\ndefects. In 2004, the delta coefficient was defined for the case of R = 2,\nwhich did not have the defects of Cohen's kappa coefficient. This article\nextends the coefficient delta from R = 2 raters to R $\\geq$ 2. The coefficient\nmulti-rater delta has the same advantages as the coefficient delta with regard\nto the type kappa coefficients: i) it is intuitive and easy to interpret,\nbecause it refers to the proportion of replies that are concordant and non\nrandom; ii) the summands which give its value allow the degree of agreement in\neach category to be measured accurately, with no need to be collapsed; and iii)\nit is not affected by the marginal imbalance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:29:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Andr\u00e9s", "A. Mart\u00edn", ""], ["Hern\u00e1ndez", "M. \u00c1lvarez", ""]]}, {"id": "1909.05661", "submitter": "Sezen Cekic", "authors": "Sezen Cekic, Stephen Aichele, Andreas M. Brandmaier, Ylva K\\\"ohncke\n  and Paolo Ghisletta", "title": "A Tutorial for Joint Modeling of Longitudinal and Time-to-Event Data in\n  R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biostatistics and medical research, longitudinal data are often composed\nof repeated assessments of a variable (e.g., blood pressure or other\nbiomarkers) and dichotomous indicators to mark an event of interest (e.g.,\nrecovery from disease, or death). Consequently, joint modeling of longitudinal\nand time-to-event data has generated much interest in these disciplines over\nthe previous decade. In psychology, too, often we are interested in relating\nindividual trajectories (e.g., cognitive performance or well-being across many\nyears) and discrete events (e.g., death, diagnosis of dementia, or of\ndepression). Yet, joint modeling are rarely applied in psychology and social\nsciences more generally. This tutorial presents an overview and general\nframework for joint modeling of longitudinal and time-to-event data, and fully\nillustrates its application in the context of a behavioral (cognitive aging)\nstudy. We discuss practical topics, such as model selection and comparison for\nboth longitudinal and time-to-event data, choice of joint modeling\nparameterization, and interpretation of model parameters. To do so, we examined\nseven frequently used packages for joint modeling in the R language and\nenvironment. We concluded that of these, JMbayes is especially attractive due\nto its flexibility, its various parameterizations of the association structure,\nand for its powerful and fully Bayesian implementation. We make available the R\nsyntax to apply the JMbayes package within our example.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 13:50:25 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Cekic", "Sezen", ""], ["Aichele", "Stephen", ""], ["Brandmaier", "Andreas M.", ""], ["K\u00f6hncke", "Ylva", ""], ["Ghisletta", "Paolo", ""]]}, {"id": "1909.05892", "submitter": "Sen Na", "authors": "Sen Na, Mladen Kolar, Oluwasanmi Koyejo", "title": "Estimating Differential Latent Variable Graphical Models with\n  Applications to Brain Connectivity", "comments": "60 pages", "journal-ref": "Biometrika 2020", "doi": "10.1093/biomet/asaa066", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential graphical models are designed to represent the difference\nbetween the conditional dependence structures of two groups, thus are of\nparticular interest for scientific investigation. Motivated by modern\napplications, this manuscript considers an extended setting where each group is\ngenerated by a latent variable Gaussian graphical model. Due to the existence\nof latent factors, the differential network is decomposed into sparse and\nlow-rank components, both of which are symmetric indefinite matrices. We\nestimate these two components simultaneously using a two-stage procedure: (i)\nan initialization stage, which computes a simple, consistent estimator, and\n(ii) a convergence stage, implemented using a projected alternating gradient\ndescent algorithm applied to a nonconvex objective, initialized using the\noutput of the first stage. We prove that given the initialization, the\nestimator converges linearly with a nontrivial, minimax optimal statistical\nerror. Experiments on synthetic and real data illustrate that the proposed\nnonconvex procedure outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:12:46 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:58:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Na", "Sen", ""], ["Kolar", "Mladen", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1909.06432", "submitter": "Reagan Mozer", "authors": "Reagan Mozer, Mark E. Glickman", "title": "Bayesian analysis of longitudinal studies with treatment by indication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest in observational studies to measure the causal effect\nof a treatment on time-to-event outcomes. In a medical setting, observational\nstudies commonly involve patients who initiate medication therapy and others\nwho do not, and the goal is to infer the effect of medication therapy on time\nuntil recovery, a pre-defined level of improvement, or some other time-to-event\noutcome. A difficulty with such studies is that the notion of a medication\ninitiation time does not exist in the control group. We propose an approach to\ninfer causal effects of an intervention in longitudinal observational studies\nwhen the time of treatment assignment is only observed for treated units and\nwhere treatment is given by indication. We present a framework for\nconceptualizing an underlying randomized experiment in this setting based on\nseparating the process that governs the time of study arm assignment from the\nmechanism that determines the assignment. Our approach involves inferring the\nmissing times of assignment followed by estimating treatment effects. This\napproach allows us to incorporate uncertainty about the missing times of study\narm assignment, which induces uncertainty in both the selection of the control\ngroup and the measurement of time-to-event outcomes for these controls. We\ndemonstrate our approach to study the effects on mortality of inappropriately\nprescribing phosphodiesterase type 5 inhibitors (PDE5Is), a medication\ncontraindicated for groups 2 and 3 pulmonary hypertension, using administrative\ndata from the Veterans Affairs (VA) health care system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:18:20 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 16:52:24 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mozer", "Reagan", ""], ["Glickman", "Mark E.", ""]]}, {"id": "1909.06610", "submitter": "Steffen Ehrmann", "authors": "Steffen Ehrmann, Ralf Seppelt, Carsten Meyer", "title": "Harmonise and integrate heterogeneous areal data with the R package\n  arealDB", "comments": "14 pages, 3 supplements, 6 figures, R-package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many relevant applications in the environmental and socioeconomic sciences\nuse areal data, such as biodiversity checklists, agricultural statistics, or\nsocioeconomic surveys. For applications that surpass the spatial, temporal or\nthematic scope of any single data source, data must be integrated from several\nheterogeneous sources. Inconsistent concepts, definitions, or messy data tables\nmake this a tedious and error-prone process. To date, a dedicated tool to\naddress these challenges is still lacking. Here, we introduce the R package\narealDB that integrates heterogeneous areal data and associated geometries into\na consistent database, in an easy-to-use workflow. It is useful for harmonising\nlanguage and semantics of variables, relating data to geometries, and\ndocumenting metadata and provenance. We illustrate the functionality by\nintegrating two disparate datasets (Brazil, USA) on the harvested area of\nsoybean. arealDB promises quality-improvements to downstream scientific,\nmonitoring, and management applications but also substantial time-savings to\ndatabase collation efforts.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 15:19:23 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 10:26:19 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 10:40:44 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 15:41:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ehrmann", "Steffen", ""], ["Seppelt", "Ralf", ""], ["Meyer", "Carsten", ""]]}, {"id": "1909.06631", "submitter": "Wei Jiang", "authors": "Wei Jiang, Malgorzata Bogdan, Julie Josse, Blazej Miasojedow, Veronika\n  Rockova, TraumaBase Group", "title": "Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing\n  Values", "comments": "R package https://github.com/wjiang94/ABSLOPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in high-dimensional settings\nwith missing observations among the covariates. To address this relatively\nunderstudied problem, we propose a new synergistic procedure -- adaptive\nBayesian SLOPE -- which effectively combines the SLOPE method (sorted $l_1$\nregularization) together with the Spike-and-Slab LASSO method. We position our\napproach within a Bayesian framework which allows for simultaneous variable\nselection and parameter estimation, despite the missing values. As with the\nSpike-and-Slab LASSO, the coefficients are regarded as arising from a\nhierarchical model consisting of two groups: (1) the spike for the inactive and\n(2) the slab for the active. However, instead of assigning independent spike\npriors for each covariate, here we deploy a joint \"SLOPE\" spike prior which\ntakes into account the ordering of coefficient magnitudes in order to control\nfor false discoveries. Through extensive simulations, we demonstrate\nsatisfactory performance in terms of power, FDR and estimation bias under a\nwide range of scenarios. Finally, we analyze a real dataset consisting of\npatients from Paris hospitals who underwent a severe trauma, where we show\nexcellent performance in predicting platelet levels. Our methodology has been\nimplemented in C++ and wrapped into an R package ABSLOPE for public use.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 17:09:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:12:37 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jiang", "Wei", ""], ["Bogdan", "Malgorzata", ""], ["Josse", "Julie", ""], ["Miasojedow", "Blazej", ""], ["Rockova", "Veronika", ""], ["Group", "TraumaBase", ""]]}, {"id": "1909.06738", "submitter": "Fernan Villa", "authors": "Dayana Jim\\'enez, Paola Guti\\'errez, Yeisson Guti\\'errez, Fern\\'an\n  Villa", "title": "Preliminary study of mortality by cause and sociodemographic\n  characteristics, municipality of San Francisco, Antioquia (Columbia),\n  2001-2010", "comments": "in Spanish", "journal-ref": "Rervista Nacional de Salud P\\'ublica, Universidad de Antioquia,\n  2015", "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: Determining the structure of mortality from causes and\nsociodemographic characteristics, in the municipality of San Francisco,\nAntioquia, 2001-2010. Methodology: Quantitative descriptive study with\nretrospective longitudinal data obtained from secondary source of death events\nthrough databases in electronic media supplied by the DANE. A description of\nthe sociodemographic variables was performed by groups of cause of death, the\nlife table by sex and years of potential life lost (APVP) for each year group\nand cause of death was calculated. Results: External causes and assaults,\nhomicides, as the main cause of death occurs during the decade of study, and\nespecially in men, which had higher mortality rates, more likely to die and\nless life expectancy during the period. On average men and external causes\nshowed a higher number of potential years of life lost for the years 2001-2010\nlife. Conclusions: Men have higher mortality rates over the decade, as external\ncauses, assaults, murders, and a greater proportion of young men. The causes of\ndeath that bring more potential years of life lost are external causes. Life\nexpectancy at birth and throughout the decade is greater for women than it is\ntherefore essential for men, that the municipality analyze the current\nsituation of these causes of death, and to be carried out public policies that\ncontribute to its decline and hence to improve the life expectancy of the\npopulation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 05:14:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Jim\u00e9nez", "Dayana", ""], ["Guti\u00e9rrez", "Paola", ""], ["Guti\u00e9rrez", "Yeisson", ""], ["Villa", "Fern\u00e1n", ""]]}, {"id": "1909.06977", "submitter": "Xing He", "authors": "Xing He, Qian Ai, Robert C. Qiu, Dongxia Zhang", "title": "Preliminary Exploration on Digital Twin for Power Systems: Challenges,\n  Framework, and Applications", "comments": "8 pages. Submitted to IEEE Transactions on Industrial Informaticss", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twin (DT) is one of the most promising enabling technologies for\nrealizing smart grids. Characterized by seamless and active---data-driven,\nreal-time, and closed-loop---integration between digital and physical spaces, a\nDT is much more than a blueprint, simulation tool, or cyber-physical system\n(CPS). Numerous state-of-the-art technologies such as internet of things (IoT),\n5G, big data, and artificial intelligence (AI) serve as a basis for DT. DT for\npower systems aims at situation awareness and virtual test to assist the\ndecision-making on power grid operation and management under normal or urgent\nconditions. This paper, from both science paradigms and engineering practice,\noutlines the backgrounds, challenges, framework, tools, and possible directions\nof DT as a preliminary exploration. To our best knowledge, it is also the first\nexploration on DT in the context of power systems. Starting from the\nfundamental and most frequently used power flow (PF) analysis, some typical\napplication scenarios are presented. Our work is expected to contribute some\nnovel discoveries, as well as some high-dimensional analytics, to the\nengineering community. Besides, the connection of DT with big data analytics\nand AI may has deep impact on data science.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:07:59 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["He", "Xing", ""], ["Ai", "Qian", ""], ["Qiu", "Robert C.", ""], ["Zhang", "Dongxia", ""]]}, {"id": "1909.07060", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, R. Fahrni, M. Klippel, A. Frangi, B. Sudret", "title": "Bayesian calibration and sensitivity analysis of heat transfer models\n  for fire insulation panels", "comments": null, "journal-ref": null, "doi": "10.1016/j.engstruct.2019.110063", "report-no": "RSUQ-2019-002B", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to assess the performance of fire insulation panels is the\ncomponent additive method (CAM). The parameters of the CAM are based on the\ntemperature-dependent thermal material properties of the panels. These material\nproperties can be derived by calibrating finite element heat transfer models\nusing experimentally measured temperature records. In the past, the calibration\nof the material properties was done manually by trial and error approaches,\nwhich was inefficient and prone to error. In this contribution, the calibration\nproblem is reformulated in a probabilistic setting and solved using the\nBayesian model calibration framework. This not only gives a set of best-fit\nparameters but also confidence bounds on the latter. To make this framework\nfeasible, the procedure is accelerated through the use of advanced surrogate\nmodelling techniques: polynomial chaos expansions combined with principal\ncomponent analysis. This surrogate modelling technique additionally allows one\nto conduct a variance-based sensitivity analysis at no additional cost by\ngiving access to the Sobol' indices. The calibration is finally validated by\nusing the calibrated material properties to predict the temperature development\nin different experimental setups.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:41:59 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 08:57:59 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 10:07:43 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Wagner", "P. -R.", ""], ["Fahrni", "R.", ""], ["Klippel", "M.", ""], ["Frangi", "A.", ""], ["Sudret", "B.", ""]]}, {"id": "1909.07167", "submitter": "Kresimir Nincevic Msc", "authors": "Kresimir Nincevic, Ioannis Boumakis, Stefan Meissl, Roman Wan-Wendner", "title": "Consistent time-to-failure tests and analyses of adhesive anchor systems", "comments": "42 pages, 13 figures", "journal-ref": "Appl. Sci. 2020, 10(4), 1527;", "doi": "10.3390/app10041527", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by tunnel accidents in the recent past several investigations into\nthe sustained load behavior of adhesive anchors have been initiated.\nNevertheless, the reliable life-time prediction of bonded anchor systems based\non a relatively short period of testing still represents an unsolved challenge\ndue to the complex non-liner viscoelastic behaviour of concrete and adhesives\nalike. This contribution summarizes the results of a comprehensive experimental\ninvestigation and systematically carried out time-to-failure analysis performed\non bonded anchors under sustained tensile load. Two different adhesive\nmaterials that find widespread application in the building industry were used,\none epoxy and one vinylester based. Performed experiments include full material\ncharacterizations of concrete and the adhesives, bonded anchor pull-out tests\nat different loading rates, and time-to-failure sustained load tests. All\nanchor tests are performed in a confined configuration with close support.\nAfter a thorough review of available experimental data and analysis methods in\nthe literature the experimental data is presented with the main goals to (i)\nderive a set of recommendations for efficient time to failure tests, and (ii)\nto provide guidance for the analysis of load versus time-to-failure test data.\nFinally,a new approach based on a sigmoid function is proposed and compared to\nthe established regression models. The analyses indicate a better agreement\nwith the physics of the problem and, thus, more reliable extrapolations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:29:45 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 13:03:38 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Nincevic", "Kresimir", ""], ["Boumakis", "Ioannis", ""], ["Meissl", "Stefan", ""], ["Wan-Wendner", "Roman", ""]]}, {"id": "1909.07214", "submitter": "Ari Ercole", "authors": "Jacob Deasy, Pietro Li\\`o, Ari Ercole", "title": "Dynamic survival prediction in intensive care units from heterogeneous\n  time series without the need for variable selection or pre-processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a machine learning pipeline and model that uses the entire\nuncurated EHR for prediction of in-hospital mortality at arbitrary time\nintervals, using all available chart, lab and output events, without the need\nfor pre-processing or feature engineering. Data for more than 45,000 American\nICU patients from the MIMIC-III database were used to develop an ICU mortality\nprediction model. All chart, lab and output events were treated by the model in\nthe same manner inspired by Natural Language Processing (NLP). Patient events\nwere discretized by percentile and mapped to learnt embeddings before being\npassed to a Recurrent Neural Network (RNN) to provide early prediction of\nin-patient mortality risk. We compared mortality predictions with the\nSimplified Acute Physiology Score II (SAPS II) and the Oxford Acute Severity of\nIllness Score (OASIS). Data were split into an independent test set (10%) and a\nten-fold cross-validation was carried out during training to avoid overfitting.\n13,233 distinct variables with heterogeneous data types were included without\nmanual selection or pre-processing. Recordings in the first few hours of a\npatient's stay were found to be strongly predictive of mortality, outperforming\nmodels using SAPS II and OASIS scores within just 2 hours and achieving a state\nof the art Area Under the Receiver Operating Characteristic (AUROC) value of\n0.80 (95% CI 0.79-0.80) at 12 hours vs 0.70 and 0.66 for SAPS II and OASIS at\n24 hours respectively. Our model achieves a very strong performance of AUROC\n0.86 (95% CI 0.85-0.86) for in-patient mortality prediction after 48 hours on\nthe MIMIC-III dataset. Predictive performance increases over the first 48 hours\nof the ICU stay, but suffers from diminishing returns, providing rationale for\ntime-limited trials of critical care and suggesting that the timing of decision\nmaking can be optimised and individualised.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:26:07 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 08:01:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Deasy", "Jacob", ""], ["Li\u00f2", "Pietro", ""], ["Ercole", "Ari", ""]]}, {"id": "1909.07276", "submitter": "Stephanus Marnus Stoltz", "authors": "Marnus Stoltz, Boris Bauemer, Remco Bouckaert, Colin Fox, Gordon\n  Hiscott, David Bryant", "title": "Bayesian inference of species trees using diffusion models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a new and computationally efficient Bayesian methodology for\ninferring species trees and demographics from unlinked binary markers.\nLikelihood calculations are carried out using diffusion models of allele\nfrequency dynamics combined with a new algorithm for numerically computing\nlikelihoods of quantitative traits. The diffusion approach allows for analysis\nof datasets containing hundreds or thousands of individuals. The method, which\nwe call \\snapper, has been implemented as part of the Beast2 package. We\nintroduce the models, the efficient algorithms, and report performance of\n\\snapper on simulated data sets and on SNP data from rattlesnakes and\nfreshwater turtles.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:27:29 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 09:50:04 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Stoltz", "Marnus", ""], ["Bauemer", "Boris", ""], ["Bouckaert", "Remco", ""], ["Fox", "Colin", ""], ["Hiscott", "Gordon", ""], ["Bryant", "David", ""]]}, {"id": "1909.07288", "submitter": "Patrice Loisel", "authors": "Patrice Loisel (MISTEA), Guillerme Duvilli\\'e (MAORE), Denis Barbeau,\n  Brigitte Charnomordic (MISTEA)", "title": "EvaSylv: A user-friendly software to evaluate forestry scenarii\n  including natural risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forest management relies on the evaluation of silviculture practices. The\nincrease in natural risk due to climate change makes it necessary to consider\nevaluation criteria that take natural risk into account. Risk integration in\nexisting software requires advanced programming skills.We propose a\nuser-friendly software to simulate even-aged and monospecific forest at the\nstand level, in order to evaluate and optimize forest management. The software\ngives the possibility to run management scenarii with or without considering\nthe impact of natural risk. The control variables are the dates and rates of\nthinning and the cutting age.The risk model is based on a Poisson processus.\nThe Faustmann approach, including tree damage risk, is used to evaluate future\nbenefits, economic or ecosystem services. It relies on the calculation of\nexpected values, for which a dedicated mathematical development has been done.\nThe optimized criteria used to evaluate the various scenarii are the Faustmann\nvalue and the Averaged yield value.We illustrate the approach and the software\non two case studies: economic optimization of a beech stand and carbon\nsequestration optimization of a pine stand.Software interface makes it easy for\nusers to write their own (growth-tree damage-economic) models without advanced\nprogramming skills. The possibility to run management scenarii with/without\nconsidering the impact of natural risk may contribute improving silviculture\nguidelines and adapting them to climate change. We propose future lines of\nresearch and improvement.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:12:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Loisel", "Patrice", "", "MISTEA"], ["Duvilli\u00e9", "Guillerme", "", "MAORE"], ["Barbeau", "Denis", "", "MISTEA"], ["Charnomordic", "Brigitte", "", "MISTEA"]]}, {"id": "1909.07469", "submitter": "Tugba Suzek", "authors": "Abdulahad Bayraktar, Tugba Onal-Suzek, Baris Ethem Suzek, Omur Baysal", "title": "Meta-analysis of Gene Expression in Neurodegenerative Diseases Reveals\n  Patterns in GABA Synthesis and Heat Stress Pathways", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurodegenerative diseases are characterized as the progressive loss of\nneural cells, e.g. neurons, glial cells. Ageing, monogenic variations, viral\ninfections, and many other factors are determined and speculated as causes for\nthem. While many individual genes, such as APP for Alzheimer disease and HTT\nfor Huntington disease, and biological pathways are studied for\nneurodegenerative diseases, system-wide pathogenesis studies are limited. In\nthis study, we carried out a meta-analysis of RNA-Seq studies for three\nneurodegenerative diseases, namely Alzheimer's disease, Parkinson's disease and\nAmyotrophic Lateral Sclerosis (ALS) to minimize the batch effect derived\ndifferences and identify the similarly altered factors among studies. Our main\nassumption is that these three diseases share some pathological pathway\npattern. For this purpose, we downloaded publicly available Alzheimer's disease\n(84 patients + 33 controls = 117 individuals), Parkinson's disease (28 patients\n+ 43 controls = 71 individuals) and ALS (2 studies: 46 patients + 25 control =\n71 individuals) RNA-Seq data from Sequence Read Archive (SRA) database. The\nsignificantly differentially expressed genes common to these studies were first\nidentified and analyzed for the patterns in their pathways and variations. Our\nmeta-analysis revealed the shared nature of differential gene expression and\nmutation load of the cellular heat stress response and GABA synthesis in\nneurodegenerative diseases. The downregulated GABA synthesis-related genes\n(e.g. GAD1 and GAD2) and the upregulated cellular heat stress response-related\ngenes (e.g. DNAJB6 and HSP90AA1), in addition to their expression patterns,\ncontain unique variations in samples from patients with neurodegenerative\ndiseases. The significance of genes and pathways we identified in this study\ncorroborated by the recent literature on neurodegenerative diseases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:31:38 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bayraktar", "Abdulahad", ""], ["Onal-Suzek", "Tugba", ""], ["Suzek", "Baris Ethem", ""], ["Baysal", "Omur", ""]]}, {"id": "1909.07550", "submitter": "Vincent Chin", "authors": "Vincent Chin, Jarod Y. L. Lee, Louise M. Ryan, Robert Kohn, Scott A.\n  Sisson", "title": "Multiclass classification of growth curves using random change points\n  and heterogeneous random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faltering growth among children is a nutritional problem prevalent in low to\nmedium income countries; it is generally defined as a slower rate of growth\ncompared to a reference healthy population of the same age and gender. As\nfaltering is closely associated with reduced physical, intellectual and\neconomic productivity potential, it is important to identify faltered children\nand be able to characterise different growth patterns so that targeted\ntreatments can be designed and administered. We introduce a multiclass\nclassification model for growth trajectory that flexibly extends a current\nclassification approach called the broken stick model, which is a piecewise\nlinear model with breaks at fixed knot locations. Heterogeneity in growth\npatterns among children is captured using mixture distributed random effects,\nwhereby the mixture components determine the classification of children into\nsubgroups. The mixture distribution is modelled using a Dirichlet process\nprior, which avoids the need to choose the \"true\" number of mixture components,\nand allows this to be driven by the complexity of the data. Because children\nhave individual differences in the onset of growth stages, we introduce\nchild-specific random change points. Simulation results show that the random\nchange point model outperforms the broken stick model because it has fewer\nrestrictions on knot locations. We illustrate our model on a longitudinal birth\ncohort from the Healthy Birth, Growth and Development knowledge integration\nproject funded by the Bill and Melinda Gates Foundation. Analysis reveals 9\nsubgroups of children within the population which exhibit varying faltering\ntrends between birth and age one.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:05:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chin", "Vincent", ""], ["Lee", "Jarod Y. L.", ""], ["Ryan", "Louise M.", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1909.07689", "submitter": "Sergio Garrido", "authors": "Sergio Garrido, Stanislav S. Borysov, Francisco C. Pereira, Jeppe Rich", "title": "Prediction of rare feature combinations in population synthesis:\n  Application of deep generative modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population synthesis applications, when considering populations with many\nattributes, a fundamental problem is the estimation of rare combinations of\nfeature attributes. Unsurprisingly, it is notably more difficult to reliably\nrepresentthe sparser regions of such multivariate distributions and in\nparticular combinations of attributes which are absent from the original\nsample. In the literature this is commonly known as sampling zeros for which no\nsystematic solution has been proposed so far. In this paper, two machine\nlearning algorithms, from the family of deep generative models,are proposed for\nthe problem of population synthesis and with particular attention to the\nproblem of sampling zeros. Specifically, we introduce the Wasserstein\nGenerative Adversarial Network (WGAN) and the Variational Autoencoder(VAE), and\nadapt these algorithms for a large-scale population synthesis application. The\nmodels are implemented on a Danish travel survey with a feature-space of more\nthan 60 variables. The models are validated in a cross-validation scheme and a\nset of new metrics for the evaluation of the sampling-zero problem is proposed.\nResults show how these models are able to recover sampling zeros while keeping\nthe estimation of truly impossible combinations, the structural zeros, at a\ncomparatively low level. Particularly, for a low dimensional experiment, the\nVAE, the marginal sampler and the fully random sampler generate 5%, 21% and\n26%, respectively, more structural zeros per sampling zero generated by the\nWGAN, while for a high dimensional case, these figures escalate to 44%, 2217%\nand 170440%, respectively. This research directly supports the development of\nagent-based systems and in particular cases where detailed socio-economic or\ngeographical representations are required.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:58:45 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Garrido", "Sergio", ""], ["Borysov", "Stanislav S.", ""], ["Pereira", "Francisco C.", ""], ["Rich", "Jeppe", ""]]}, {"id": "1909.08024", "submitter": "Robert Krafty", "authors": "Jun Zhang, Greg J Siegle, Wendy D'Andrea, Robert T Krafty", "title": "Interpretable Principal Components Analysis for Multilevel Multivariate\n  Functional Data, with Application to EEG Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies collect functional data from multiple subjects that have both\nmultilevel and multivariate structures. An example of such data comes from\npopular neuroscience experiments where participants' brain activity is recorded\nusing modalities such as EEG and summarized as power within multiple\ntime-varying frequency bands within multiple electrodes, or brain regions.\nSummarizing the joint variation across multiple frequency bands for both\nwhole-brain variability between subjects, as well as location-variation within\nsubjects, can help to explain neural reactions to stimuli. This article\nintroduces a novel approach to conducting interpretable principal components\nanalysis on multilevel multivariate functional data that decomposes total\nvariation into subject-level and replicate-within-subject-level (i.e.\nelectrode-level) variation, and provides interpretable components that can be\nboth sparse among variates (e.g. frequency bands) and have localized support\nover time within each frequency band. The sparsity and localization of\ncomponents is achieved by solving an innovative rank-one based convex\noptimization problem with block Frobenius and matrix $L_1$-norm based\npenalties. The method is used to analyze data from a study to better understand\nreactions to emotional information in individuals with histories of trauma and\nthe symptom of dissociation, revealing new neurophysiological insights into how\nsubject- and electrode-level brain activity are associated with these\nphenomena.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:51:24 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Zhang", "Jun", ""], ["Siegle", "Greg J", ""], ["D'Andrea", "Wendy", ""], ["Krafty", "Robert T", ""]]}, {"id": "1909.08035", "submitter": "Arnab Hazra", "authors": "Arnab Hazra, Abhik Ghosh", "title": "Robust statistical modeling of monthly rainfall: The minimum density\n  power divergence approach", "comments": "29 pages, 8 Tables, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of rainfall is an important challenge in meteorology,\nparticularly from the perspective of rainfed agriculture where a proper\nassessment of the future availability of rainwater is necessary. The\nprobability models mostly used for this purpose are exponential, gamma, Weibull\nand lognormal distributions, where the unknown model parameters are routinely\nestimated using the maximum likelihood estimator (MLE). However, presence of\noutliers or extreme observations is quite common in rainfall data and the MLEs\nbeing highly sensitive to them often leads to spurious inference. In this\npaper, we discuss a robust parameter estimation approach based on the minimum\ndensity power divergence estimators (MDPDEs) which provides a class of\nestimates through a tuning parameter including the MLE as a special case. The\nunderlying tuning parameter controls the trade-offs between efficiency and\nrobustness of the resulting inference; we also discuss a procedure for\ndata-driven optimal selection of this tuning parameter as well as robust\nselection of an appropriate model that provides best fit to some specific\nrainfall data. We fit the above four parametric models to the areally-weighted\nmonthly rainfall data from the 36 meteorological subdivisions of India for the\nyears 1951-2014 and compare the fits based on the MLE and the proposed optimum\nMDPDE; the superior performances of the MDPDE based approach are illustrated\nfor several cases. For all month-subdivision combinations, the best-fit models\nand the estimated median rainfall amounts are provided. Software (written in R)\nfor calculating MDPDEs and their standard errors, optimal tuning parameter\nselection and model selection are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:08:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Hazra", "Arnab", ""], ["Ghosh", "Abhik", ""]]}, {"id": "1909.08066", "submitter": "Yoann Altmann", "authors": "Haonan Zhu and Yoann Altmann and Angela Di Fulvioand Stephen\n  McLaughlin and Sara Pozzi and Alfred Hero", "title": "A Hierarchical Bayesian Approach to Neutron Spectrum Unfolding with\n  Organic Scintillators", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TNS.2019.2941317", "report-no": null, "categories": "physics.ins-det nucl-ex physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical Bayesian model and state-of-art Monte Carlo\nsampling method to solve the unfolding problem, i.e., to estimate the spectrum\nof an unknown neutron source from the data detected by an organic scintillator.\nInferring neutron spectra is important for several applications, including\nnonproliferation and nuclear security, as it allows the discrimination of\nfission sources in special nuclear material (SNM) from other types of neutron\nsources based on the differences of the emitted neutron spectra. Organic\nscintillators interact with neutrons mostly via elastic scattering on hydrogen\nnuclei and therefore partially retain neutron energy information. Consequently,\nthe neutron spectrum can be derived through deconvolution of the measured light\noutput spectrum and the response functions of the scintillator to monoenergetic\nneutrons. The proposed approach is compared to three existing methods using\nsimulated data to enable controlled benchmarks. We consider three sets of\ndetector responses. One set corresponds to a 2.5 MeV monoenergetic neutron\nsource and two sets are associated with (energy-wise) continuous neutron\nsources ($^{252}$Cf and $^{241}$AmBe). Our results show that the proposed\nmethod has similar or better unfolding performance compared to other iterative\nor Tikhonov regularization-based approaches in terms of accuracy and robustness\nagainst limited detection events, while requiring less user supervision. The\nproposed method also provides a posteriori confidence measures, which offers\nadditional information regarding the uncertainty of the measurements and the\nextracted information.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:05:31 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhu", "Haonan", ""], ["Altmann", "Yoann", ""], ["McLaughlin", "Angela Di Fulvioand Stephen", ""], ["Pozzi", "Sara", ""], ["Hero", "Alfred", ""]]}, {"id": "1909.08169", "submitter": "Konrad Sakowski", "authors": "Agata Lonc, Monika J. Piotrowska, Konrad Sakowski", "title": "Analysis of the hospital records from AOK Plus", "comments": "19 pages, 9 figures, 7 tables. arXiv admin note: text overlap with\n  arXiv:1903.04701", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present analysis of anonymised admission/discharge data from insurance\nprovider for Saxony and Thuringia (Germany) for years 2010--2016. Study of such\ndata are necessary to derive a structure of healthcare system transfer network,\nas no patients' transfer data are currently available. Hospital network can be\ndirectly used as a basis for modelling of multidrug-resistant pathogen spread\nallowing to study the effectiveness of disease-control strategies. In this\npaper, the properties of the dataset under consideration are presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 02:06:00 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Lonc", "Agata", ""], ["Piotrowska", "Monika J.", ""], ["Sakowski", "Konrad", ""]]}, {"id": "1909.08299", "submitter": "Konstantin G\\\"orgen", "authors": "Konstantin G\\\"orgen, Melanie Schienle", "title": "How have German University Tuition Fees Affected Enrollment Rates:\n  Robust Model Selection and Design-based Inference in High-Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use official data for all 16 federal German states to study the causal\neffect of a flat 1000 Euro state-dependent university tuition fee on the\nenrollment behavior of students during the years 2006-2014. In particular, we\nshow how the variation in the introduction scheme across states and times can\nbe exploited to identify the federal average causal effect of tuition fees by\ncontrolling for a large amount of potentially influencing attributes for state\nheterogeneity. We suggest a stability post-double selection methodology to\nrobustly determine the causal effect across types in the transparently modeled\nunknown response components. The proposed stability resampling scheme in the\ntwo LASSO selection steps efficiently mitigates the risk of model\nunderspecification and thus biased effects when the tuition fee policy decision\nalso depends on relevant variables for the state enrollment rates. Correct\ninference for the full cross-section state population in the sample requires\nadequate design -- rather than sampling-based standard errors. With the\ndata-driven model selection and explicit control for spatial cross-effects we\ndetect that tuition fees induce substantial migration effects where the\nmobility occurs both from fee but also from non-fee states suggesting also a\ngeneral movement for quality. Overall, we find a significant negative impact of\nup to 4.5 percentage points of fees on student enrollment. This is in contrast\nto plain one-step LASSO or previous empirical studies with full fixed effects\nlinear panel regressions which generally underestimate the size and get an only\ninsignificant effect.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:12:54 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 09:04:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["G\u00f6rgen", "Konstantin", ""], ["Schienle", "Melanie", ""]]}, {"id": "1909.08336", "submitter": "Jonas Crevecoeur", "authors": "Roel Verbelen, Katrien Antonio, Gerda Claeskens, Jonas Crevecoeur", "title": "Modeling the occurrence of events subject to a reporting delay via an EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A delay between the occurrence and the reporting of events often has\npractical implications such as for the amount of capital to hold for insurance\ncompanies, or for taking preventive actions in case of infectious diseases. The\naccurate estimation of the number of incurred but not (yet) reported events\nforms an essential part of properly dealing with this phenomenon. We review the\ncurrent practice for analysing such data and we present a flexible regression\nframework to jointly estimate the occurrence and reporting of events. By\nlinking this setting to an incomplete data problem, estimation is performed via\nan expectation-maximization algorithm. The resulting method is elegant, easy to\nunderstand and implement, and provides refined insights in the nowcasts. The\nproposed methodology is applied to a European general liability portfolio in\ninsurance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:24:10 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 12:45:29 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 09:51:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Verbelen", "Roel", ""], ["Antonio", "Katrien", ""], ["Claeskens", "Gerda", ""], ["Crevecoeur", "Jonas", ""]]}, {"id": "1909.08492", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Vladim\\'ir Hol\\'y", "title": "The Impact of Operating Environment on Efficiency of Public Libraries", "comments": null, "journal-ref": null, "doi": "10.1007/s10100-020-00696-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of technical efficiency is an important tool in management of public\nlibraries. We assess the efficiency of 4660 public libraries established by\nmunicipalities in the Czech Republic in the year 2017. For this purpose, we\nutilize the data envelopment analysis (DEA) based on the Chebyshev distance. We\npay special attention to the operating environment and find that the efficiency\nscores significantly depend on the population of the municipality and distance\nto the municipality with extended powers. To remove the effect of the operating\nenvironment, we perform DEA separately for categories based on the decision\ntree analysis as well as categories designed by an expert.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:10:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hol\u00fd", "Vladim\u00edr", ""]]}, {"id": "1909.08578", "submitter": "Leontine Alkema", "authors": "Emily Peterson, Doris Chou, Ann-Beth Moller, Alison Gemmill, Lale Say,\n  Leontine Alkema", "title": "Estimating maternal mortality using data from national civil\n  registration vital statistics systems: A Bayesian hierarchical bivariate\n  random walk model to estimate sensitivity and specificity of reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Civil registration vital statistics (CRVS) data are used to produce national\nestimates of maternal mortality, but are often subject to substantial reporting\nerrors due to misclassification of maternal deaths. The accuracy of CRVS\nsystems can be assessed by comparing CRVS-based counts of maternal and\nnon-maternal deaths to those obtained from specialized studies, which are\nrigorous assessments of maternal mortality for a given country-period. We\ndeveloped a Bayesian bivariate random walk model to estimate sensitivity and\nspecificity of the reporting on maternal mortality in CRVS data, and associated\nCRVS adjustment factors. The model was fitted to a global data set of CRVS and\nspecialized study data. Validation exercises suggest that the model performs\nwell in terms of predicting CRVS-based proportions of maternal deaths for\ncountry-periods without specialized studies. The new model is used by the UN\nMaternal Mortality Inter-Agency Group to account for misclassification errors\nwhen estimating maternal mortality using CRVS data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:07:19 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Peterson", "Emily", ""], ["Chou", "Doris", ""], ["Moller", "Ann-Beth", ""], ["Gemmill", "Alison", ""], ["Say", "Lale", ""], ["Alkema", "Leontine", ""]]}, {"id": "1909.08579", "submitter": "Zad Rafi", "authors": "Zad Rafi and Sander Greenland", "title": "Semantic and Cognitive Tools to Aid Statistical Science: Replace\n  Confidence and Significance by Compatibility and Surprise", "comments": "22 pages; 5 figures; 2 tables; 94 references; Published at BMC\n  Medical Research Methodology", "journal-ref": "BMC Med Res Methodol 20, 244 (2020)", "doi": "10.1186/s12874-020-01105-9", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often misinterpret and misrepresent statistical outputs. This\nabuse has led to a large literature on modification or replacement of testing\nthresholds and $P$-values with confidence intervals, Bayes factors, and other\ndevices. Because the core problems appear cognitive rather than statistical, we\nreview simple aids to statistical interpretations. These aids emphasize logical\nand information concepts over probability, and thus may be more robust to\ncommon misinterpretations than are traditional descriptions. We use the Shannon\ntransform of the $P$-value $p$, also known as the binary surprisal or $S$-value\n$s=-\\log_{2}(p)$, to measure the information supplied by the testing procedure,\nand to help calibrate intuitions against simple physical experiments like coin\ntossing. We also use tables or graphs of test statistics for alternative\nhypotheses, and interval estimates for different percentile levels, to thwart\nfallacies arising from arbitrary dichotomies. Finally, we reinterpret\n$P$-values and interval estimates in unconditional terms, which describe\ncompatibility of data with the entire set of analysis assumptions. We\nillustrate these methods with a reanalysis of data from an existing\nrecord-based cohort study. In line with other recent recommendations, we advise\nthat teaching materials and research reports discuss $P$-values as measures of\ncompatibility rather than significance, compute $P$-values for alternative\nhypotheses whenever they are computed for null hypotheses, and interpret\ninterval estimates as showing values of high compatibility with data, rather\nthan regions of confidence. Our recommendations emphasize cognitive devices for\ndisplaying the compatibility of the observed data with various hypotheses of\ninterest, rather than focusing on single hypothesis tests or interval\nestimates. We believe these simple reforms are well worth the minor effort they\nrequire.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:09:43 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 02:49:12 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 02:18:45 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 00:27:15 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 01:55:45 GMT"}, {"version": "v6", "created": "Tue, 29 Sep 2020 18:28:15 GMT"}, {"version": "v7", "created": "Thu, 1 Oct 2020 01:43:23 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rafi", "Zad", ""], ["Greenland", "Sander", ""]]}, {"id": "1909.08583", "submitter": "Zad Rafi", "authors": "Sander Greenland and Zad Rafi", "title": "To Aid Scientific Inference, Emphasize Unconditional Descriptions of\n  Statistics", "comments": "11 pages; 1 figure; 47 references with added DOI hyperlinks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have elsewhere reviewed proposals to reform terminology and improve\ninterpretations of conventional statistics by emphasizing logical and\ninformation concepts over probability concepts. We here give detailed reasons\nand methods for reinterpreting statistics (including but not limited to)\nP-values and interval estimates in unconditional terms, which describe\ncompatibility of observations with an entire set of analysis assumptions,\nrather than just a narrow target hypothesis. Such reinterpretations help avoid\noverconfident inferences whenever there is uncertainty about the assumptions\nused to derive and compute the statistical results. Examples of such\nassumptions include not only standard statistical modeling assumptions, but\nalso assumptions about absence of systematic errors, protocol violations, and\ndata corruption. Unconditional descriptions introduce uncertainty about such\nassumptions directly into statistical presentations of results, rather than\nleaving that only to the informal discussion that ensues. We thus view\nunconditional description as a vital component of good statistical training and\npresentation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:12:02 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 02:50:44 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 02:20:13 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 01:49:30 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 01:27:28 GMT"}, {"version": "v6", "created": "Sat, 27 Feb 2021 18:40:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Greenland", "Sander", ""], ["Rafi", "Zad", ""]]}, {"id": "1909.09014", "submitter": "William Briggs", "authors": "William M Briggs and Jaap Hanekamp", "title": "Fixes to the Ryden & McNeil Ammonia Flux Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two simple fixes to the Ryden and McNeil ammonia flux model. These\nare necessary to prevent estimates from becoming unphysical, which very often\nhappens and which has not yet been noted in the literature. The first fix is to\nconstrain the limits of certain of the model's parameters; without this limit,\nestimates from the model are seen to produce absurd values. The second is to\nestimate a point at which additional contributions of atmospheric ammonia are\nnot part of a planned expert but are the result of natural background levels.\nThese two fixes produce results that are everywhere physical. Some experiment\ntypes, such as surface broadcast, are not well cast in the Ryden and McNeil\nscheme, and lead to over-estimates of atmospheric ammonia.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 02:14:18 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Briggs", "William M", ""], ["Hanekamp", "Jaap", ""]]}, {"id": "1909.09138", "submitter": "Jennie Brand", "authors": "Jennie E. Brand, Jiahui Xu, Bernard Koch, and Pablo Geraldo", "title": "Uncovering Sociological Effect Heterogeneity using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals do not respond uniformly to treatments, events, or interventions.\nSociologists routinely partition samples into subgroups to explore how the\neffects of treatments vary by covariates like race, gender, and socioeconomic\nstatus. In so doing, analysts determine the key subpopulations based on\ntheoretical priors. Data-driven discoveries are also routine, yet the analyses\nby which sociologists typically go about them are problematic and seldom move\nus beyond our expectations, and biases, to explore new meaningful subgroups.\nEmerging machine learning methods allow researchers to explore sources of\nvariation that they may not have previously considered, or envisaged. In this\npaper, we use causal trees to recursively partition the sample and uncover\nsources of treatment effect heterogeneity. We use honest estimation, splitting\nthe sample into a training sample to grow the tree and an estimation sample to\nestimate leaf-specific effects. Assessing a central topic in the social\ninequality literature, college effects on wages, we compare what we learn from\nconventional approaches for exploring variation in effects to causal trees.\nGiven our use of observational data, we use leaf-specific matching and\nsensitivity analyses to address confounding and offer interpretations of\neffects based on observed and unobserved heterogeneity. We encourage\nresearchers to follow similar practices in their work on variation in\nsociological effects.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 19:09:17 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Brand", "Jennie E.", ""], ["Xu", "Jiahui", ""], ["Koch", "Bernard", ""], ["Geraldo", "Pablo", ""]]}, {"id": "1909.09261", "submitter": "Tianjian Zhou", "authors": "Tong Li, Tianjian Zhou, Kam-Wah Tsui, Lin Wei, Yuan Ji", "title": "Posterior Contraction Rate of Sparse Latent Feature Models with\n  Application to Proteomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Indian buffet process (IBP) and phylogenetic Indian buffet process (pIBP)\ncan be used as prior models to infer latent features in a data set. The\ntheoretical properties of these models are under-explored, however, especially\nin high dimensional settings. In this paper, we show that under mild sparsity\ncondition, the posterior distribution of the latent feature matrix, generated\nvia IBP or pIBP priors, converges to the true latent feature matrix\nasymptotically. We derive the posterior convergence rate, referred to as the\ncontraction rate. We show that the convergence holds even when the\ndimensionality of the latent feature matrix increases with the sample size,\ntherefore making the posterior inference valid in high dimensional setting. We\ndemonstrate the theoretical results using computer simulation, in which the\nparallel-tempering Markov chain Monte Carlo method is applied to overcome\ncomputational hurdles. The practical utility of the derived properties is\ndemonstrated by inferring the latent features in a reverse phase protein arrays\n(RPPA) dataset under the IBP prior model. Software and dataset reported in the\nmanuscript are provided at http://www.compgenome.org/IBP.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 23:44:27 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Tong", ""], ["Zhou", "Tianjian", ""], ["Tsui", "Kam-Wah", ""], ["Wei", "Lin", ""], ["Ji", "Yuan", ""]]}, {"id": "1909.09293", "submitter": "Xiaoming Li", "authors": "Xiaoming Li, Chun Wang, Xiao Huang", "title": "A Two-Stage Stochastic Programming Model for Car-Sharing Problem using\n  Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Car-sharing problem is a popular research field in sharing economy. In this\npaper, we investigate the car-sharing re-balancing problem under uncertain\ndemands. An innovative framework that integrates a non-parametric approach -\nkernel density estimation (KDE) and a two-stage stochastic programming (SP)\nmodel are proposed. Specifically, the probability distributions are derived\nfrom New York taxi trip data sets by KDE, which is used as the input uncertain\nparameters for SP. Additionally, the car-sharing problem is formulated as a\ntwo-stage SP model which aims to maximize the overall profit. Meanwhile, a\nMonte Carlo method called sample average approximation (SAA) and Benders\ndecomposition algorithm is introduced to solve the large-scale optimization\nmodel. Finally, the experimental validations show that the proposed framework\noutperforms the existing works in terms of outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:05:11 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Xiaoming", ""], ["Wang", "Chun", ""], ["Huang", "Xiao", ""]]}, {"id": "1909.09370", "submitter": "Sothea Has", "authors": "Aur\\'elie Fisher (LPSM UMR 8001), Mathilde Mougeot (CMLA, ENSIIE, LPSM\n  UMR 8001), Sothea Has (LPSM UMR 8001)", "title": "A clusterwise supervised learning procedure based on aggregation of\n  distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many machine learning procedures are available on the shelve and\nmay be used easily to calibrate predictive models on supervised data. However,\nwhen the input data consists of more than one unknown cluster, and when\ndifferent underlying predictive models exist, fitting a model is a more\nchallenging task. We propose, in this paper, a procedure in three steps to\nautomatically solve this problem. The KFC procedure aggregates different models\nadaptively on data. The first step of the procedure aims at catching the\nclustering structure of the input data, which may be characterized by several\nstatistical distributions. It provides several partitions, given the\nassumptions on the distributions. For each partition, the second step fits a\nspecific predictive model based on the data in each cluster. The overall model\nis computed by a consensual aggregation of the models corresponding to the\ndifferent partitions. A comparison of the performances on different simulated\nand real data assesses the excellent performance of our method in a large\nvariety of prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:37:04 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:20:31 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 15:51:30 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Fisher", "Aur\u00e9lie", "", "LPSM UMR 8001"], ["Mougeot", "Mathilde", "", "CMLA, ENSIIE, LPSM\n  UMR 8001"], ["Has", "Sothea", "", "LPSM UMR 8001"]]}, {"id": "1909.09453", "submitter": "Rahul Sucharitha", "authors": "Rahul Srinivas Sucharitha, Seokcheon Lee", "title": "Application of Clustering Analysis for Investigation of Food\n  Accessibility", "comments": "8 pages, 25th International Conference on Production Research\n  Manufacturing Innovation: Cyber Physical Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to food assistance programs such as food pantries and food banks needs\nfocus in order to mitigate food insecurity. Accessibility to the food\nassistance programs is impacted by demographics of the population and geography\nof the location. It hence becomes imperative to define and identify food\nassistance deserts (Under-served areas) within a given region to find out the\nways to improve the accessibility of food. Food banks, the supplier of food to\nthe food agencies serving the people, can manage its resources more efficiently\nby targeting the food assistance deserts and increase the food supply in those\nregions. This paper will examine the characteristics and structure of the food\nassistance network in the region of Ohio by presenting the possible reasons of\nfood insecurity in this region and identify areas wherein food agencies are\nneeded or may not be needed. Gaussian Mixture Model (GMM) clustering technique\nis employed to identify the possible reasons and address this problem of food\naccessibility.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 18:09:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Sucharitha", "Rahul Srinivas", ""], ["Lee", "Seokcheon", ""]]}, {"id": "1909.09467", "submitter": "Satrajit Roychoudhury", "authors": "Ray S. Lin, Ji Lin, Satrajit Roychoudhury, Keaven M. Anderson, Tianle\n  Hu, Bo Huang, Larry F Leon, Jason JZ Liao, Rong Liu, Xiaodong Luo, Pralay\n  Mukhopadhyay, Rui Qin, Kay Tatsuoka, Xuejing Wang, Yang Wang, Jian Zhu,\n  Tai-Tsang Chen, Renee Iacona, Cross-Pharma Non-proportional Hazards Working\n  Group", "title": "Alternative Analysis Methods for Time to Event Endpoints under\n  Non-proportional Hazards: A Comparative Analysis", "comments": null, "journal-ref": "Statistics in Biopharmaceutical Statistics 2020", "doi": "10.1080/19466315.2019.1697738", "report-no": "NPH12", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-rank test is most powerful under proportional hazards (PH). In\npractice, non-PH patterns are often observed in clinical trials, such as in\nimmuno-oncology; therefore, alternative methods are needed to restore the\nefficiency of statistical testing. Three categories of testing methods were\nevaluated, including weighted log-rank tests, Kaplan-Meier curve-based tests\n(including weighted Kaplan-Meier and Restricted Mean Survival Time, RMST), and\ncombination tests (including Breslow test, Lee's combo test, and MaxCombo\ntest). Nine scenarios representing the PH and various non-PH patterns were\nsimulated. The power, type I error, and effect estimates of each method were\ncompared. In general, all tests control type I error well. There is not a\nsingle most powerful test across all scenarios. In the absence of prior\nknowledge regarding the PH or non-PH patterns, the MaxCombo test is relatively\nrobust across patterns. Since the treatment effect changes overtime under\nnon-PH, the overall profile of the treatment effect may not be represented\ncomprehensively based on a single measure. Thus, multiple measures of the\ntreatment effect should be pre-specified as sensitivity analyses to evaluate\nthe totality of the data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:47:02 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Lin", "Ray S.", ""], ["Lin", "Ji", ""], ["Roychoudhury", "Satrajit", ""], ["Anderson", "Keaven M.", ""], ["Hu", "Tianle", ""], ["Huang", "Bo", ""], ["Leon", "Larry F", ""], ["Liao", "Jason JZ", ""], ["Liu", "Rong", ""], ["Luo", "Xiaodong", ""], ["Mukhopadhyay", "Pralay", ""], ["Qin", "Rui", ""], ["Tatsuoka", "Kay", ""], ["Wang", "Xuejing", ""], ["Wang", "Yang", ""], ["Zhu", "Jian", ""], ["Chen", "Tai-Tsang", ""], ["Iacona", "Renee", ""], ["Group", "Cross-Pharma Non-proportional Hazards Working", ""]]}, {"id": "1909.09533", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty, Kwonsang Lee, Rachel R. Kelz, Luke J. Keele", "title": "Biased Encouragements and Heterogeneous Effects in an Instrumental\n  Variable Study of Emergency General Surgical Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the efficacy of surgical versus non-surgical management for\ntwo gastrointestinal conditions, colitis and diverticulitis, using\nobservational data. We deploy an instrumental variable design with surgeons'\ntendencies to operate as an instrument. Assuming instrument validity, we find\nthat non-surgical alternatives can reduce both hospital length of stay and the\nrisk of complications, with estimated effects larger for septic patients than\nfor non-septic patients. The validity of our instrument is plausible but not\nironclad, necessitating a sensitivity analysis. Existing sensitivity analyses\nfor IV designs assume effect homogeneity, unlikely to hold here because of\npatient-specific physiology. We develop a new sensitivity analysis that\naccommodates arbitrary effect heterogeneity and exploits components explainable\nby observed features. We find that the results for non-septic patients prove\nmore robust to hidden bias despite having smaller estimated effects. For\nnon-septic patients, two individuals with identical observed characteristics\nwould have to differ in their odds of assignment to a high tendency to operate\nsurgeon by a factor of 2.34 to overturn our finding of a benefit for\nnon-surgical management in reducing length of stay. For septic patients, this\nvalue is only 1.64. Simulations illustrate that this phenomenon may be\nexplained by differences in within-group heterogeneity.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:38:10 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 03:31:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Fogarty", "Colin B.", ""], ["Lee", "Kwonsang", ""], ["Kelz", "Rachel R.", ""], ["Keele", "Luke J.", ""]]}, {"id": "1909.09545", "submitter": "Jason Hilton", "authors": "Jason Hilton, Erengul Dodd, Jonathan J. Forster, Peter W.F. Smith,\n  Jakub Bijak", "title": "Forecasting Fertility with Parametric Mixture Models", "comments": "26 pages. 5 figures. 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper sets out a forecasting method that employs a mixture of parametric\nfunctions to capture the pattern of fertility with respect to age. The overall\nlevel of cohort fertility is decomposed over the range of fertile ages using a\nmixture of parametric density functions. The level of fertility and the\nparameters describing the shape of the fertility curve are projected foward\nusing time series methods. The model is estimated within a Bayesian framework,\nallowing predictive distributions of future fertility rates to be produced that\nnaturally incorporate both time series and parametric uncertainty. A number of\nchoices are possible for the precise form of the functions used in the\ntwo-component mixtures. The performance of several model variants is tested on\ndata from four countries; England and Wales, the USA, Sweden and France. The\nformer two countries exhibit multi-modality in their fertility rate curves as a\nfunction of age, while the latter two are largely uni-modal. The models are\nestimated using Hamiltonian Monte Carlo and the `stan` software package on data\ncovering the period up to 2006, with the period 2007-2016 held back for\nassessment purposes. Forecasting performance is found to be comparable to other\nmodels identified as producing accurate fertility forecasts in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:58:37 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hilton", "Jason", ""], ["Dodd", "Erengul", ""], ["Forster", "Jonathan J.", ""], ["Smith", "Peter W. F.", ""], ["Bijak", "Jakub", ""]]}, {"id": "1909.09611", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Fabrizia Mealli, Jason D. Sacks, Francesca Dominici", "title": "Causal inference and machine learning approaches for evaluation of the\n  health impacts of large-scale air quality regulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a causal inference approach to estimate the number of adverse\nhealth events prevented by large-scale air quality regulations via changes in\nexposure to multiple pollutants. This approach is motivated by regulations that\nimpact pollution levels in all areas within their purview. We introduce a\ncausal estimand called the Total Events Avoided (TEA) by the regulation,\ndefined as the difference in the expected number of health events under the\nno-regulation pollution exposures and the observed number of health events\nunder the with-regulation pollution exposures. We propose a matching method and\na machine learning method that leverage high-resolution, population-level\npollution and health data to estimate the TEA. Our approach improves upon\ntraditional methods for regulation health impact analyses by clarifying the\ncausal identifying assumptions, utilizing population-level data, minimizing\nparametric assumptions, and considering the impacts of multiple pollutants\nsimultaneously. To reduce model-dependence, the TEA estimate captures health\nimpacts only for units in the data whose anticipated no-regulation features are\nwithin the support of the observed with-regulation data, thereby providing a\nconservative but data-driven assessment to complement traditional parametric\napproaches. We apply these methods to investigate the health impacts of the\n1990 Clean Air Act Amendments in the US Medicare population.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 20:40:52 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Mealli", "Fabrizia", ""], ["Sacks", "Jason D.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1909.10006", "submitter": "Hongwei Wang", "authors": "Hongwei Wang and Hongbin Li and Wei Zhang and Junyi Zuo and Heping\n  Wang and Jun Fang", "title": "Outlier-Detection Based Robust Information Fusion for Networked Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider state estimation for networked systems where measurements from\nsensor nodes are contaminated by outliers. A new hierarchical measurement model\nis formulated for outlier detection by integrating the outlier-free measurement\nmodel with a binary indicator variable. The binary indicator variable, which is\nassigned a beta-Bernoulli prior, is utilized to characterize if the sensor's\nmeasurement is nominal or an outlier. Based on the proposed outlier-detection\nmeasurement model, both centralized and decentralized information fusion\nfilters are developed. Specifically, in the centralized approach, all\nmeasurements are sent to a fusion center where the state and outlier indicators\nare jointly estimated by employing the mean-field variational Bayesian\ninference in an iterative manner. In the decentralized approach, however, every\nnode shares its information, including the prior and likelihood, only with its\nneighbors based on a hybrid consensus strategy. Then each node independently\nperforms the estimation task based on its own and shared information. In\naddition, an approximation distributed solution is proposed to reduce the local\ncomputational complexity and communication overhead. Simulation results reveal\nthat the proposed algorithms are effective in dealing with outliers compared\nwith several recent robust solutions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:55:12 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wang", "Hongwei", ""], ["Li", "Hongbin", ""], ["Zhang", "Wei", ""], ["Zuo", "Junyi", ""], ["Wang", "Heping", ""], ["Fang", "Jun", ""]]}, {"id": "1909.10017", "submitter": "Anita Mariana Bunea", "authors": "Anita M. Bunea, Pietro Manfredi, Pompeo Della Posta, Mariangela\n  Guidolin", "title": "What do adoption patterns of solar panels observed so far tell about\n  governments' incentive? insight from diffusion models", "comments": "23 pages, 6 figures, 1 table, under review by Technological\n  Forecasting and Social Change", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper uses diffusion models to understand the main determinants of\ndiffusion of solar photovoltaic panels (SPP) worldwide, focusing on the role of\npublic incentives. We applied the generalized Bass model (GBM) to adoption data\nof 26 countries between 1992-2016. The SPP market appears as a frail and\ncomplicate one, lacking public media support. Even the major shocks in adoption\ncurves, following state incentive implemented after 2006, failed to go beyond\nshort-term effects and therefore were unable to provide sustained momentum to\nthe market. This suggests that further barriers to adoption should be removed.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 14:38:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bunea", "Anita M.", ""], ["Manfredi", "Pietro", ""], ["Della Posta", "Pompeo", ""], ["Guidolin", "Mariangela", ""]]}, {"id": "1909.10101", "submitter": "Zhigang Li", "authors": "Zhigang Li, Lu Tian, A. James O'Malley, Margaret R. Karagas, Anne G.\n  Hoen, Brock C. Christensen, Juliette C. Madan, Quran Wu, Raad Z. Gharaibeh,\n  Christian Jobin, Hongzhe Li", "title": "IFAA: Robust association identification and Inference For Absolute\n  Abundance in microbiome analyses", "comments": "Corresponding email: zhigang.li@ufl.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target of inference in microbiome analyses is usually relative abundance\n(RA) because RA in a sample (e.g., stool) can be considered as an approximation\nof RA in an entire ecosystem (e.g., gut). However, inference on RA suffers from\nthe fact that RA are calculated by dividing absolute abundances (AA) over the\ncommon denominator (CD), the summation of all AA (i.e., library size). Because\nof that, perturbation in one taxon will result in a change in the CD and thus\ncause false changes in RA of all other taxa, and those false changes could lead\nto false positive/negative findings. We propose a novel analysis approach\n(IFAA) to make robust inference on AA of an ecosystem that can circumvent the\nissues induced by the CD problem and compositional structure of RA. IFAA can\nalso address the confounding effect of library size and handle zero-inflated\ndata structures. IFAA identifies microbial taxa associated with the covariates\nin Phase one and estimates the association parameters by employing an\nindependent reference taxon in Phase two. Two real data applications are\npresented and extensive simulations show that IFAA outperforms other\nestablished existing approaches by a big margin in the presence of confounding\neffect of library size.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 22:30:33 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 22:52:52 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 15:16:48 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Li", "Zhigang", ""], ["Tian", "Lu", ""], ["O'Malley", "A. James", ""], ["Karagas", "Margaret R.", ""], ["Hoen", "Anne G.", ""], ["Christensen", "Brock C.", ""], ["Madan", "Juliette C.", ""], ["Wu", "Quran", ""], ["Gharaibeh", "Raad Z.", ""], ["Jobin", "Christian", ""], ["Li", "Hongzhe", ""]]}, {"id": "1909.10117", "submitter": "Edward D Lee", "authors": "Edward D. Lee, Daniel M. Katz, Michael J. Bommarito II, Paul Ginsparg", "title": "Sensitivity of collective outcomes identifies pivotal components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A social system is susceptible to perturbation when its collective properties\ndepend sensitively on a few pivotal components. Using the information geometry\nof minimal models from statistical physics, we develop an approach to identify\npivotal components to which coarse-grained, or aggregate, properties are\nsensitive. As an example, we introduce our approach on a reduced toy model with\na median voter who always votes in the majority. The sensitivity of\nmajority-minority divisions to changing voter behaviour pinpoints the unique\nrole of the median. More generally, the sensitivity identifies pivotal\ncomponents that precisely determine collective outcomes generated by a complex\nnetwork of interactions. Using perturbations to target pivotal components in\nthe models, we analyse datasets from political voting, finance and Twitter.\nAcross these systems, we find remarkable variety, from systems dominated by a\nmedian-like component to those whose components behave more equally. In the\ncontext of political institutions such as courts or legislatures, our\nmethodology can help describe how changes in voters map to new collective\nvoting outcomes. For economic indices, differing system response reflects\nvarying fiscal conditions across time. Thus, our information-geometric approach\nprovides a principled, quantitative framework that may help assess the\nrobustness of collective outcomes to targeted perturbation and compare social\ninstitutions, or even biological networks, with one another and across time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 01:26:31 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 17:01:44 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 20:10:51 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Lee", "Edward D.", ""], ["Katz", "Daniel M.", ""], ["Bommarito", "Michael J.", "II"], ["Ginsparg", "Paul", ""]]}, {"id": "1909.10285", "submitter": "Abhik Ghosh PhD", "authors": "Amarnath Nandy, Ayanendranath Basu, Abhik Ghosh", "title": "Robust Inference for Skewed data in Health Sciences", "comments": "Pre-print Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health data are often not symmetric to be adequately modeled through the\nusual normal distributions; most of them exhibit skewed patterns. They can\nindeed be modeled better through the larger family of skew-normal distributions\ncovering both skewed and symmetric cases. However, the existing likelihood\nbased inference, that is routinely performed in these cases, is extremely\nnon-robust against data contamination/outliers. Since outliers are not uncommon\nin complex real-life experimental datasets, a robust methodology automatically\ntaking care of the noises in the data would be of great practical value to\nproduce stable and more precise research insights leading to better policy\nformulation. In this paper, we develop a class of robust estimators and testing\nprocedures for the family of skew-normal distributions using the minimum\ndensity power divergence approach with application to health data. In\nparticular, a robust procedure for testing of symmetry is discussed in the\npresence of outliers. Two efficient computational algorithms are discussed.\nBesides deriving the asymptotic and robustness theory for the proposed methods,\ntheir advantages and utilities are illustrated through simulations and a couple\nof real-life applications for health data of athletes from Australian Institute\nof Sports and AIDS clinical trial data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:15:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Nandy", "Amarnath", ""], ["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""]]}, {"id": "1909.10604", "submitter": "Renjie Chen", "authors": "Nalini Ravishanker and Renjie Chen", "title": "Topological Data Analysis (TDA) for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The study of topology is strictly speaking, a topic in pure mathematics.\nHowever in only a few years, Topological Data Analysis (TDA), which refers to\nmethods of utilizing topological features in data (such as connected\ncomponents, tunnels, voids, etc.) has gained considerable momentum. More\nrecently, TDA is being used to understand time series. This article provides a\nreview of TDA for time series, with examples using R functions. Features\nderived from TDA are useful in classification and clustering of time series and\nin detecting breaks in patterns.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 20:18:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ravishanker", "Nalini", ""], ["Chen", "Renjie", ""]]}, {"id": "1909.10631", "submitter": "Michael Lopez", "authors": "Michael J. Lopez", "title": "Bigger data, better questions, and a return to fourth down behavior: an\n  introduction to a special issue on tracking data in the National football\n  League", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most historical National Football League (NFL) analysis, both mainstream and\nacademic, has relied on public, play-level data to generate team and player\ncomparisons. Given the number of oft omitted variables that impact on-field\nresults, such as play call, game situation, and opponent strength, findings\ntend to be more anecdotal than actionable. With the release of player tracking\ndata, however, analysts can better ask and answer questions to isolate skill\nand strategy. In this article, we highlight the limitations of traditional\nanalyses, and use a decades-old punching bag for analysts, fourth-down\nstrategy, as a microcosm for why tracking data is needed. Specifically, we\nassert that, in absence of using the precise yardage needed for a first down,\npast findings supporting an aggressive fourth down strategy may have been\noverstated. Next, we synthesize recent work that comprises this special Journal\nof Quantitative Analysis in Sports issue into player tracking data in football.\nFinally, we conclude with some best practices and limitations regarding usage\nof this data. The release of player tracking data marks a transition for the\nleague and its' analysts, and we hope this issue helps guide innovation in\nfootball analytics for years to come.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 21:47:39 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 01:55:06 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 01:25:44 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Lopez", "Michael J.", ""]]}, {"id": "1909.10635", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Yannick D\\\"uren, Kristoffer H. Hellton and Johannes\n  Lederer", "title": "Tuning parameter calibration for prediction in personalized medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine has become an important part of medicine, for instance\npredicting individual drug responses based on genomic information. However,\nmany current statistical methods are not tailored to this task, because they\noverlook the individual heterogeneity of patients. In this paper, we look at\npersonalized medicine from a linear regression standpoint. We introduce an\nalternative version of the ridge estimator and target individuals by\nestablishing a tuning parameter calibration scheme that minimizes prediction\nerrors of individual patients. In stark contrast, classical schemes such as\ncross-validation minimize prediction errors only on average. We show that our\npipeline is optimal in terms of oracle inequalities, fast, and highly effective\nboth in simulations and on real data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 22:00:14 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 09:39:10 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 10:57:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Huang", "Shih-Ting", ""], ["D\u00fcren", "Yannick", ""], ["Hellton", "Kristoffer H.", ""], ["Lederer", "Johannes", ""]]}, {"id": "1909.10679", "submitter": "Kok Haur Ng", "authors": "C.Y.Tan, Y.B.Koh, K.H.Ng and K.H.Ng", "title": "Structural Change Analysis of Active Cryptocurrency Market", "comments": "18 pages, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Change Analysis of Active Cryptocurrency Market\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 02:02:24 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Tan", "C. Y.", ""], ["Koh", "Y. B.", ""], ["Ng", "K. H.", ""], ["Ng", "K. H.", ""]]}, {"id": "1909.10742", "submitter": "Srikanta Sannigrahi", "authors": "Srikanta Sannigrahi, Suman Chakraborti, Pawan Kumar Joshi, Saskia\n  Keesstra, P.S. Roy, Paul. C. Sutton, Urs Kreuter, Saikat Kumar Paul, Somnath\n  Sen, Sandeep Bhatt, Shahid Rahmat, Shouvik Jha, Qi Zhang, Laishram Kanta\n  Singh", "title": "Effects of green revolution led agricultural expansion on net ecosystem\n  service values in India", "comments": null, "journal-ref": "Journal of Environmental Management, 2020", "doi": "10.1016/j.jenvman.2020.111381", "report-no": "Volume 277, 111381", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ecosystem Services are a bundle of natural processes and functions that are\nessential for human well-being, subsistence, and livelihood. The expansion of\ncultivation and cropland, which is the backbone of the Indian economy, is one\nof the main drivers of rapid Land Use Land Cover changes in India. To assess\nthe impact of the Green Revolution led agrarian expansion on the total\necosystem service values, we first estimated the ESVs from 1985 to 2005 for\neight ecoregions in India using several value transfer approaches. Five\nexplanatory factors such as Total Crop Area, Crop Production, Crop Yield, Net\nIrrigated Area, and Cropping Intensity representing the cropping scenarios in\nthe country were used in constructing local Geographical Weighted Regression\nmodel to explore the cumulative and individual effects on ESVs. A Multi-Layer\nPerceptron based Artificial Neural Network algorithm was employed to estimate\nthe normalized importance of these explanatory factors. During the observation\nperiods, cropland, forestland, and water bodies have contributed the most and\nform a significant proportion of ESVs, followed by grassland, mangrove,\nwetland, and urban builtup. In all three years, among the nine ESs, the highest\nESV accounts for water regulation, followed by soil formation and soilwater\nretention, biodiversity maintenance, waste treatment, climate regulation, and\ngas regulation. Among the five explanatory factors, TCA, NIA, CP showed a\nstrong positive association with ESVs, while the CI exhibited a negative\nassociation. The study reveals a strong association between GR led agricultural\nexpansion and ESVs in India.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 07:30:54 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 01:00:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sannigrahi", "Srikanta", ""], ["Chakraborti", "Suman", ""], ["Joshi", "Pawan Kumar", ""], ["Keesstra", "Saskia", ""], ["Roy", "P. S.", ""], ["Sutton", "Paul. C.", ""], ["Kreuter", "Urs", ""], ["Paul", "Saikat Kumar", ""], ["Sen", "Somnath", ""], ["Bhatt", "Sandeep", ""], ["Rahmat", "Shahid", ""], ["Jha", "Shouvik", ""], ["Zhang", "Qi", ""], ["Singh", "Laishram Kanta", ""]]}, {"id": "1909.10881", "submitter": "Amir Karami", "authors": "Amir Karami", "title": "Application of Fuzzy Clustering for Text Data Dimensionality Reduction", "comments": "arXiv admin note: text overlap with arXiv:1712.05997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large textual corpora are often represented by the document-term frequency\nmatrix whose elements are the frequency of terms; however, this matrix has two\nproblems: sparsity and high dimensionality. Four dimension reduction strategies\nare used to address these problems. Of the four strategies, unsupervised\nfeature transformation (UFT) is a popular and efficient strategy to map the\nterms to a new basis in the document-term frequency matrix. Although several\nUFT-based methods have been developed, fuzzy clustering has not been considered\nfor dimensionality reduction. This research explores fuzzy clustering as a new\nUFT-based approach to create a lower-dimensional representation of documents.\nPerformance of fuzzy clustering with and without using global term weighting\nmethods is shown to exceed principal component analysis and singular value\ndecomposition. This study also explores the effect of applying different\nfuzzifier values on fuzzy clustering for dimensionality reduction purpose.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 03:15:04 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Karami", "Amir", ""]]}, {"id": "1909.11009", "submitter": "Han Lin Shang", "authors": "Fearghal Kearney, Han Lin Shang, Lisa Sheenan", "title": "Implied volatility surface predictability: the case of commodity markets", "comments": "35 pages, 6 figures, 9 tables, to appear in Journal of Banking and\n  Finance", "journal-ref": "Journal of Banking & Finance, 2019, 108, 105657", "doi": "10.1016/j.jbankfin.2019.105657", "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature seek to forecast implied volatility derived from equity,\nindex, foreign exchange, and interest rate options using latent factor and\nparametric frameworks. Motivated by increased public attention borne out of the\nfinancialization of futures markets in the early 2000s, we investigate if these\nextant models can uncover predictable patterns in the implied volatility\nsurfaces of the most actively traded commodity options between 2006 and 2016.\nAdopting a rolling out-of-sample forecasting framework that addresses the\ncommon multiple comparisons problem, we establish that, for energy and precious\nmetals options, explicitly modeling the term structure of implied volatility\nusing the Nelson-Siegel factors produces the most accurate forecasts.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 05:37:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kearney", "Fearghal", ""], ["Shang", "Han Lin", ""], ["Sheenan", "Lisa", ""]]}, {"id": "1909.11016", "submitter": "Chaouki Ben Issaid", "authors": "Chaouki ben Issaid and Mohamed-Slim Alouini", "title": "Efficient Estimation of the Left Tail of Bimodal Distributions with\n  Applications to Underwater Optical Communication Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose efficient importance sampling estimators to\nevaluate the outage probability of maximum ratio combining receivers over\nturbulence-induced fadings in underwater wireless optical channels. We consider\ntwo fading models: exponential-lognormal, and exponential-generalized Gamma.\nThe cross-entropy optimization method is used to determine the optimal biased\ndistribution. We show by simulations that the number of samples required by\nimportance sampling estimator is much less compared to naive Monte Carlo for\nthe same accuracy requirement.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 15:56:19 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Issaid", "Chaouki ben", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1909.11046", "submitter": "Youngjae Min", "authors": "Youngjae Min, Soon-Seo Park, Han-Lim Choi", "title": "Informative Planning of Mobile Sensor Networks in GPS-Denied\n  Environments", "comments": "14 pages, 10 figures, Accepted to 2020 AIAA SciTech: Guidance,\n  Navigation, and Control (GN&C)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem to plan mobile sensor networks for target\nlocalization task in GPS-denied environments. Most researches on mobile sensor\nnetworks assume that the states of the sensing agents are precisely known\nduring their missions, which is not feasible under the absence of external\ninfrastructures such as GPS. Thus, we propose a new algorithm to solve this\nproblem by: (i) estimating the states of the sensing agents in addition to the\ntarget's through the combination of a particle filter (PF) and extended Kalman\nfilters (EKF) and (ii) involving the uncertainty of the states of the sensing\nagents in planning the sensor networks based on the combined filters. This\napproach does not require any additional internal/external sensors nor the\nprior knowledge of the surrounding environments. We demonstrate the limitations\nof prior works in GPS-denied environments and the improvements from the\nproposed algorithm through Monte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:58:01 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Min", "Youngjae", ""], ["Park", "Soon-Seo", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1909.11112", "submitter": "Quntao Zhuang", "authors": "Haowei Shi, Zheshen Zhang, Quntao Zhuang", "title": "Practical route to entanglement-assisted communication over noisy\n  bosonic channels", "comments": "10+6 pages, 13 figures; Close to the published version", "journal-ref": "Phys. Rev. Applied 13, 034029 (2020)", "doi": "10.1103/PhysRevApplied.13.034029", "report-no": null, "categories": "quant-ph physics.app-ph physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entanglement offers substantial advantages in quantum information processing,\nbut loss and noise hinder its applications in practical scenarios. Although it\nhas been well known for decades that the classical communication capacity over\nlossy and noisy bosonic channels can be significantly enhanced by entanglement,\nno practical encoding and decoding schemes are available to realize any\nentanglement-enabled advantage. Here, we report structured encoding and\ndecoding schemes for such an entanglement-assisted communication scenario.\nSpecifically, we show that phase encoding on the entangled two-mode squeezed\nvacuum state saturates the entanglement-assisted classical communication\ncapacity over a very noisy channel and overcomes the fundamental limit of\ncovert communication without entanglement assistance. We then construct\nreceivers for optimum hypothesis testing protocols under discrete phase\nmodulation and for optimum noisy phase estimation protocols under continuous\nphase modulation. Our results pave the way for entanglement-assisted\ncommunication and sensing in the radio-frequency and microwave spectral ranges.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 18:14:58 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 19:08:11 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 22:14:43 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 19:46:04 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Shi", "Haowei", ""], ["Zhang", "Zheshen", ""], ["Zhuang", "Quntao", ""]]}, {"id": "1909.11114", "submitter": "Christian Gary Mena Leco\\~na", "authors": "C. Gary Mena, Arno De Caigny, Kristof Coussement, Koen W. De Bock,\n  Stefan Lessmann", "title": "Churn Prediction with Sequential Data and Deep Neural Networks. A\n  Comparative Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-the-shelf machine learning algorithms for prediction such as regularized\nlogistic regression cannot exploit the information of time-varying features\nwithout previously using an aggregation procedure of such sequential data.\nHowever, recurrent neural networks provide an alternative approach by which\ntime-varying features can be readily used for modeling. This paper assesses the\nperformance of neural networks for churn modeling using recency, frequency, and\nmonetary value data from a financial services provider. Results show that RFM\nvariables in combination with LSTM neural networks have larger top-decile lift\nand expected maximum profit metrics than regularized logistic regression models\nwith commonly-used demographic variables. Moreover, we show that using the\nfitted probabilities from the LSTM as feature in the logistic regression\nincreases the out-of-sample performance of the latter by 25 percent compared to\na model with only static features.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 18:27:14 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Mena", "C. Gary", ""], ["De Caigny", "Arno", ""], ["Coussement", "Kristof", ""], ["De Bock", "Koen W.", ""], ["Lessmann", "Stefan", ""]]}, {"id": "1909.11161", "submitter": "Joshua Keller", "authors": "Joshua P. Keller and Adam A. Szpiro", "title": "Selecting a Scale for Spatial Confounding Adjustment", "comments": "22 pages, 6 figures", "journal-ref": "Journal of the Royal Statistical Society: Series A (2020) 183,\n  Part 3, 1121-1143", "doi": "10.1111/rssa.12556", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured, spatially-structured factors can confound associations between\nspatial environmental exposures and health outcomes. Adding flexible splines to\na regression model is a simple approach for spatial confounding adjustment, but\nthe spline degrees of freedom do not provide an easily interpretable spatial\nscale. We describe a method for quantifying the extent of spatial confounding\nadjustment in terms of the Euclidean distance at which variation is removed. We\ndevelop this approach for confounding adjustment with splines and using Fourier\nand wavelet filtering. We demonstrate differences in the spatial scales these\nbases can represent and provide a comparison of methods for selecting the\namount of confounding adjustment. We find the best performance for selecting\nthe amount of adjustment using an information criterion evaluated on an outcome\nmodel without exposure. We apply this method to spatial adjustment in an\nanalysis of particulate matter and blood pressure in a cohort of United States\nwomen.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 20:29:04 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Keller", "Joshua P.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "1909.11211", "submitter": "I\\~nigo Urteaga", "authors": "Kathy Li, I\\~nigo Urteaga, Chris H. Wiggins, Anna Druet, Amanda Shea,\n  Virginia J. Vitzthum, No\\'emie Elhadad", "title": "Characterizing physiological and symptomatic variation in menstrual\n  cycles using self-tracked mobile health data", "comments": "The Supplementary Information for this work, as well as the code\n  required for data pre-processing and producing results is available in\n  https://github.com/iurteaga/menstrual_cycle_analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The menstrual cycle is a key indicator of overall health for women of\nreproductive age. Previously, menstruation was primarily studied through survey\nresults; however, as menstrual tracking mobile apps become more widely adopted,\nthey provide an increasingly large, content-rich source of menstrual health\nexperiences and behaviors over time. By exploring a database of user-tracked\nobservations from the Clue app by BioWink of over 378,000 users and 4.9 million\nnatural cycles, we show that self-reported menstrual tracker data can reveal\nstatistically significant relationships between per-person cycle length\nvariability and self-reported qualitative symptoms. A concern for self-tracked\ndata is that they reflect not only physiological behaviors, but also the\nengagement dynamics of app users. To mitigate such potential artifacts, we\ndevelop a procedure to exclude cycles lacking user engagement, thereby allowing\nus to better distinguish true menstrual patterns from tracking anomalies. We\nuncover that women located at different ends of the menstrual variability\nspectrum, based on the consistency of their cycle length statistics, exhibit\nstatistically significant differences in their cycle characteristics and\nsymptom tracking patterns. We also find that cycle and period length statistics\nare stationary over the app usage timeline across the variability spectrum. The\nsymptoms that we identify as showing statistically significant association with\ntiming data can be useful to clinicians and users for predicting cycle\nvariability from symptoms or as potential health indicators for conditions like\nendometriosis. Our findings showcase the potential of longitudinal,\nhigh-resolution self-tracked data to improve understanding of menstruation and\nwomen's health as a whole.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 22:21:10 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 23:41:29 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 22:57:02 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Li", "Kathy", ""], ["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""], ["Druet", "Anna", ""], ["Shea", "Amanda", ""], ["Vitzthum", "Virginia J.", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "1909.11484", "submitter": "Federico Amato", "authors": "Federico Amato, Mohamed Laib, Fabian Guignard, Mikhail Kanevski", "title": "Analysis of air pollution time series using complexity-invariant\n  distance and information measures", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.124391", "report-no": null, "categories": "stat.AP nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution is known to be a major threat for human and ecosystem health. A\nproper understanding of the factors generating pollution and of the behavior of\nair pollution in time is crucial to support the development of effective\npolicies aiming at the reduction of pollutant concentration. This paper\nconsiders the hourly time series of three pollutants, namely NO$_2$, O$_3$ and\nPM$_{2.5}$, collected on sixteen measurement stations in Switzerland. The air\npollution patterns due to the location of measurement stations and their\nrelationship with anthropogenic activities, and specifically land use, are\nstudied using two approaches: Fisher-Shannon information plane and\ncomplexity-invariant distance between time series. A clustering analysis is\nused to recognize within the measurements of a same pollutant group of stations\nbehaving in a similar way. The results clearly demonstrate the relationship\nbetween the air pollution probability densities and land use activities.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 08:39:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Amato", "Federico", ""], ["Laib", "Mohamed", ""], ["Guignard", "Fabian", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1909.11560", "submitter": "Peter Neal Dr", "authors": "Jessica Welding and Peter Neal", "title": "Real time analysis of epidemic data", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases have severe health and economic consequences for society.\nIt is important in controlling the spread of an emerging infectious disease to\nbe able to both estimate the parameters of the underlying model and identify\nthose individuals most at risk of infection in a timely manner. This requires\nhaving a mechanism to update inference on the model parameters and the\nprogression of the disease as new data becomes available. However, Markov chain\nMonte Carlo (MCMC), the gold standard for statistical inference for infectious\ndisease models, is not equipped to deal with this important problem. Motivated\nby the need to develop effective statistical tools for emerging diseases and\nusing the 2001 UK Foot-and-Mouth disease outbreak as an exemplar, we introduce\na Sequential Monte Carlo (SMC) algorithm to enable real-time analysis of\nepidemic outbreaks. Naive application of SMC methods leads to significant\nparticle degeneracy which are successfully overcome by particle perturbation\nand incorporating MCMC-within-SMC updates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:43:46 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Welding", "Jessica", ""], ["Neal", "Peter", ""]]}, {"id": "1909.11566", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Gerty J.L.M. Lensvelt-Mulders, Karin Lasthuizen", "title": "A Note on a Simple and Practical Randomized Response Framework for\n  Eliciting Sensitive Dichotomous & Quantitative Information", "comments": "Postprint, 11 pages, 1 figure", "journal-ref": "Sociological Methods & Research, 39 (2010): 283-296", "doi": "10.1177/0049124110378099", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many issues of interest to social scientists and policymakers are of a\nsensitive nature in the sense that they are intrusive, stigmatizing or\nincriminating to the respondent. This results in refusals to cooperate or\nevasive cooperation in studies using self-reports. In a seminal article Warner\nproposed to curb this problem by generating an artificial variability in\nresponses to inoculate the individual meaning of answers to sensitive\nquestions. This procedure was further developed and extended, and came to be\nknown as the randomized response (RR) technique. Here, we propose a unified\ntreatment for eliciting sensitive binary as well as quantitative information\nwith RR based on a model where the inoculating elements are provided for by the\nrandomization device. The procedure is simple and we will argue that its\nimplementation in a computer-assisted setting may have superior practical\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:35:32 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["Lensvelt-Mulders", "Gerty J. L. M.", ""], ["Lasthuizen", "Karin", ""]]}, {"id": "1909.11594", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Jiaxi Ying, Jos'e Vin'icius de M. Cardoso, and Daniel\n  P.Palomar", "title": "Structured Graph Learning Via Laplacian Spectral Constraints", "comments": "12 Pages, Accepted for NIPS 2019. arXiv admin note: substantial text\n  overlap with arXiv:1904.09792", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a graph with a specific structure is essential for interpretability\nand identification of the relationships among data. It is well known that\nstructured graph learning from observed samples is an NP-hard combinatorial\nproblem. In this paper, we first show that for a set of important graph\nfamilies it is possible to convert the structural constraints of structure into\neigenvalue constraints of the graph Laplacian matrix. Then we introduce a\nunified graph learning framework, lying at the integration of the spectral\nproperties of the Laplacian matrix with Gaussian graphical modeling that is\ncapable of learning structures of a large class of graph families. The proposed\nalgorithms are provably convergent and practically amenable for large-scale\nsemi-supervised and unsupervised graph-based learning tasks. Extensive\nnumerical experiments with both synthetic and real data sets demonstrate the\neffectiveness of the proposed methods. An R package containing code for all the\nexperimental results is available at\nhttps://cran.r-project.org/package=spectralGraphTopology.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:25:10 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Kumar", "Sandeep", ""], ["Ying", "Jiaxi", ""], ["Cardoso", "Jos'e Vin'icius de M.", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1909.11711", "submitter": "Qingchun Hou", "authors": "Qingchun Hou, Ning Zhang, Ershun Du, Miao Miao, Fei Peng, Chongqing\n  Kang", "title": "Probabilistic duck curve in high PV penetration power system: Concept,\n  modeling, and empirical analysis in China", "comments": null, "journal-ref": null, "doi": "10.1016/j.apenergy.2019.03.067", "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The high penetration of photovoltaic (PV) is reshaping the electricity\nnet-load curve and has a significant impact on power system operation and\nplanning. The concept of duck curve is widely used to describe the timing\nimbalance between peak demand and PV generation. The traditional duck curve is\ndeterministic and only shows a single extreme or typical scenario during a day.\nThus, it cannot capture both the probability of that scenario and the\nuncertainty of PV generation and loads. These weaknesses limit the application\nof the duck curve on power system planning under high PV penetration. To\naddress this issue, the novel concepts of probabilistic duck curve (PDC) and\nprobabilistic ramp curve (PRC) are proposed to accurately model the uncertainty\nand variability of electricity net load and ramp under high PV penetration. An\nefficient method is presented for modeling PDC and PRC using kernel density\nestimation, copula function, and dependent discrete convolution. Several\nindices are designed to quantify the characteristics of the PDC and PRC. For\nthe application, we demonstrate how the PDC and PRC will benefit flexible\nresource planning. Finally, an empirical study on the Qinghai provincial power\nsystem of China validates the effectiveness of the presented method. The\nresults of PDC and PRC intuitively illustrate that the ramp demand and the\nvalley of net load face considerable uncertainty under high PV penetration. The\nresults of flexible resource planning indicate that retrofitting coal-fired\nunits has remarkable performance on enhancing the power system flexibility in\nQinghai. In average, reducing the minimal output of coal-fired units by 1 MW\nwill increase PV accommodation by over 4 MWh each day.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:52:04 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hou", "Qingchun", ""], ["Zhang", "Ning", ""], ["Du", "Ershun", ""], ["Miao", "Miao", ""], ["Peng", "Fei", ""], ["Kang", "Chongqing", ""]]}, {"id": "1909.11714", "submitter": "Thomas Loredo", "authors": "Gwendolyn Eadie, Thomas J. Loredo, Ashish A. Mahabal, Aneta\n  Siemiginowska, Eric Feigelson, Eric B. Ford, S. G. Djorgovski, Matthew\n  Graham, Zeljko Ivezic, Kirk Borne, Jessi Cisewski-Kehe, J. E. G. Peek, Chad\n  Schafer, Padma A. Yanamandra-Fisher, C. Alex Young", "title": "Realizing the potential of astrostatistics and astroinformatics", "comments": "14 pages, 1 figure; submitted to the Decadal Survey on Astronomy and\n  Astrophysics (Astro2020) on 10 July 2019; see\n  https://sites.nationalacademies.org/DEPS/Astro2020/DEPS_192906", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Astro2020 State of the Profession Consideration White Paper highlights\nthe growth of astrostatistics and astroinformatics in astronomy, identifies key\nissues hampering the maturation of these new subfields, and makes\nrecommendations for structural improvements at different levels that, if acted\nupon, will make significant positive impacts across astronomy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:56:47 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Eadie", "Gwendolyn", ""], ["Loredo", "Thomas J.", ""], ["Mahabal", "Ashish A.", ""], ["Siemiginowska", "Aneta", ""], ["Feigelson", "Eric", ""], ["Ford", "Eric B.", ""], ["Djorgovski", "S. G.", ""], ["Graham", "Matthew", ""], ["Ivezic", "Zeljko", ""], ["Borne", "Kirk", ""], ["Cisewski-Kehe", "Jessi", ""], ["Peek", "J. E. G.", ""], ["Schafer", "Chad", ""], ["Yanamandra-Fisher", "Padma A.", ""], ["Young", "C. Alex", ""]]}, {"id": "1909.11841", "submitter": "Markus Foote", "authors": "Markus D. Foote (1), Pouya Sabouri (2), Amit Sawant (2), and Sarang C.\n  Joshi (1) ((1) Scientific Computing and Imaging Institute, Department of\n  Biomedical Engineering, University of Utah, (2) University of Maryland School\n  of Medicine, Baltimore, Maryland)", "title": "Rank Constrained Diffeomorphic Density Motion Estimation for Respiratory\n  Correlated Computed Tomography", "comments": null, "journal-ref": "In: MFCA 2017. Lecture Notes in Computer Science, vol 10551.\n  Springer, Cham", "doi": "10.1007/978-3-319-67675-3_16", "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation of organs in a sequence of images is important in numerous\nmedical imaging applications. The focus of this paper is the analysis of 4D\nRespiratory Correlated Computed Tomography (RCCT) Imaging. It is hypothesized\nthat the quasi-periodic breathing induced motion of organs in the thorax can be\nrepresented by deformations spanning a very low dimension subspace of the full\ninfinite dimensional space of diffeomorphic transformations. This paper\npresents a novel motion estimation algorithm that includes the constraint for\nlow-rank motion between the different phases of the RCCT images. Low-rank\ndeformation solutions are necessary for the efficient statistical analysis and\nimproved treatment planning and delivery. Although the application focus of\nthis paper is RCCT the algorithm is quite general and applicable to various\nmotion estimation problems in medical imaging.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 01:52:33 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Foote", "Markus D.", ""], ["Sabouri", "Pouya", ""], ["Sawant", "Amit", ""], ["Joshi", "Sarang C.", ""]]}, {"id": "1909.11913", "submitter": "Tong Wu", "authors": "Yue Wang, Tong Wu, Yunlong Wang, Gao Wang", "title": "Enhancing Model Interpretability and Accuracy for Disease Progression\n  Prediction via Phenotype-Based Patient Similarity Learning", "comments": "12 pages, accepted by Pacific Symposium on Biocomputing (PSB) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models have been proposed to extract temporal patterns from longitudinal\nelectronic health records (EHR) for clinical predictive models. However, the\ncommon relations among patients (e.g., receiving the same medical treatments)\nwere rarely considered. In this paper, we propose to learn patient similarity\nfeatures as phenotypes from the aggregated patient-medical service matrix using\nnon-negative matrix factorization. On real-world medical claim data, we show\nthat the learned phenotypes are coherent within each group, and also\nexplanatory and indicative of targeted diseases. We conducted experiments to\npredict the diagnoses for Chronic Lymphocytic Leukemia (CLL) patients. Results\nshow that the phenotype-based similarity features can improve prediction over\nmultiple baselines, including logistic regression, random forest, convolutional\nneural network, and more.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 05:56:33 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wang", "Yue", ""], ["Wu", "Tong", ""], ["Wang", "Yunlong", ""], ["Wang", "Gao", ""]]}, {"id": "1909.12073", "submitter": "Alessandra Pasquini", "authors": "Giovanni Mellace, Alessandra Pasquini", "title": "Identify More, Observe Less: Mediation Analysis Synthetic Control", "comments": "We have benefited from comments by Simone De Angelis and participants\n  at several seminars, workshops, and conferences. Addresses for\n  correspondence: Giovanni Mellace (giome@sam.sdu.dk), and Alessandra Pasquini\n  (alessandra.pasquini@uniroma2.it", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthetic control method (SCM) allows estimation of the causal effect of\nan intervention in settings where panel data on just a few treated units and\ncontrol units are available. We show that the existing SCM as well as its\nextensions can be easily modified to estimate how much of the \"total\" effect\ngoes through observed causal channels. The additional assumptions needed are\narguably very mild in many settings. Furthermore, in an illustrative empirical\napplication we estimate the effects of adopting the euro on labor productivity\nin several countries and show that a reduction in the Economic Complexity Index\nhelped to mitigate the negative short run effects of adopting the new currency\nin some countries and boosted the positive effects in others.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:06:28 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 09:54:27 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Mellace", "Giovanni", ""], ["Pasquini", "Alessandra", ""]]}, {"id": "1909.12429", "submitter": "Suman Majumder", "authors": "Suman Majumder, Yawen Guan, Brian J. Reich, Susan O'Neill and Ana G.\n  Rappold", "title": "Statistical downscaling with spatial misalignment: Application to\n  wildland fire PM$_{2.5}$ concentration forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine particulate matter, PM$_{2.5}$, has been documented to have adverse\nhealth effects and wildland fires are a major contributor to PM$_{2.5}$ air\npollution in the US. Forecasters use numerical models to predict PM$_{2.5}$\nconcentrations to warn the public of impending health risk. Statistical methods\nare needed to calibrate the numerical model forecast using monitor data to\nreduce bias and quantify uncertainty. Typical model calibration techniques do\nnot allow for errors due to misalignment of geographic locations. We propose a\nspatiotemporal downscaling methodology that uses image registration techniques\nto identify the spatial misalignment and accounts for and corrects the bias\nproduced by such warping. Our model is fitted in a Bayesian framework to\nprovide uncertainty quantification of the misalignment and other sources of\nerror. We apply this method to different simulated data sets and show enhanced\nperformance of the method in the presence of spatial misalignment. Finally, we\napply the method to a large fire in Washington state and show that the proposed\nmethod provides more realistic uncertainty quantification than standard\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 23:04:12 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Majumder", "Suman", ""], ["Guan", "Yawen", ""], ["Reich", "Brian J.", ""], ["O'Neill", "Susan", ""], ["Rappold", "Ana G.", ""]]}, {"id": "1909.12502", "submitter": "James Cavenaugh", "authors": "James Stephens Cavenaugh", "title": "Bootstrap Cross-validation Improves Model Selection in Pharmacometrics", "comments": "submitted to Statistics in Biopharmaceutical Research 16 pages + 2\n  tables + 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation assesses the predictive ability of a model, allowing one to\nrank models accordingly. Although the nonparametric bootstrap is almost always\nused to assess the variability of a parameter, it can be used as the basis for\ncross-validation if one keeps track of which items were not selected in a given\nbootstrap iteration. The items which were selected constitute the training data\nand the omitted items constitute the testing data. This bootstrap\ncross-validation (BS-CV) allows model selection to be made on the basis of\npredictive ability by comparing the median values of ensembles of summary\nstatistics of testing data. BS-CV is herein demonstrated using several summary\nstatistics, including a new one termed the simple metric for prediction quality\n(SMPQ), and using the warfarin data included in the Monolix distribution with\n13 pharmacokinetics (PK) models and 12 pharmacodynamics (PD) models. Of note\nthe two best PK models by AIC had the worst predictive ability, underscoring\nthe danger of using single realizations of a random variable (such as AIC) as\nthe basis for model selection. Using these data BS-CV was able to discriminate\nbetween similar indirect response models (inhibition of input versus\nstimulation of output). This could be useful in situations in which the\nmechanism of action is unknown (unlike warfarin).\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 05:40:40 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Cavenaugh", "James Stephens", ""]]}, {"id": "1909.12530", "submitter": "Rui Zhou", "authors": "Rui Zhou, Junyan Liu, Sandeep Kumar, Daniel P. Palomar", "title": "Robust Factor Analysis Parameter Estimation", "comments": "Presented at Eurocast 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robustly estimating the parameters of a\nheavy-tailed multivariate distribution when the covariance matrix is known to\nhave the structure of a low-rank matrix plus a diagonal matrix as considered in\nfactor analysis (FA). By assuming the observed data to follow the multivariate\nStudent's t distribution, we can robustly estimate the parameters via maximum\nlikelihood estimation (MLE). However, the MLE of parameters becomes an\nintractable problem when the multivariate Student's t distribution and the FA\nstructure are both introduced. In this paper, we propose an algorithm based on\nthe generalized expectation maximization (GEM) method to obtain estimators. The\nrobustness of our proposed method is further enhanced to cope with missing\nvalues. Finally, we show the performance of our proposed algorithm using both\nsynthetic data and real financial data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:32:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zhou", "Rui", ""], ["Liu", "Junyan", ""], ["Kumar", "Sandeep", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1909.12551", "submitter": "Pascal Schlosser", "authors": "Pascal Schlosser and Jochen Knaus and Maximilian Schmutz and Konstanze\n  D\\\"ohner and Christoph Plass and Lars Bullinger and Rainer Claus and Harald\n  Binder and Michael L\\\"ubbert and Martin Schumacher", "title": "Netboost: Boosting-supported network analysis improves high-dimensional\n  omics prediction in acute myeloid leukemia and Huntington's disease", "comments": null, "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics\n  (May 2020)", "doi": "10.1109/TCBB.2020.2983010", "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: State-of-the art selection methods fail to identify weak but\ncumulative effects of features found in many high-dimensional omics datasets.\nNevertheless, these features play an important role in certain diseases.\n  Results: We present Netboost, a three-step dimension reduction technique.\nFirst, a boosting-based filter is combined with the topological overlap measure\nto identify the essential edges of the network. Second, sparse hierarchical\nclustering is applied on the selected edges to identify modules and finally\nmodule information is aggregated by the first principal components. The primary\nanalysis is than carried out on these summary measures instead of the original\ndata. We demonstrate the application of the newly developed Netboost in\ncombination with CoxBoost for survival prediction of DNA methylation and gene\nexpression data from 180 acute myeloid leukemia (AML) patients and show, based\non cross-validated prediction error curve estimates, its prediction superiority\nover variable selection on the full dataset as well as over an alternative\nclustering approach. The identified signature related to chromatin modifying\nenzymes was replicated in an independent dataset of AML patients in the phase\nII AMLSG 12-09 study. In a second application we combine Netboost with Random\nForest classification and improve the disease classification error in\nRNA-sequencing data of Huntington's disease mice.\n  Conclusion: Netboost improves definition of predictive variables for survival\nanalysis and classification. It is a freely available Bioconductor R package\nfor dimension reduction and hypothesis generation in high-dimensional omics\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 08:21:35 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Schlosser", "Pascal", ""], ["Knaus", "Jochen", ""], ["Schmutz", "Maximilian", ""], ["D\u00f6hner", "Konstanze", ""], ["Plass", "Christoph", ""], ["Bullinger", "Lars", ""], ["Claus", "Rainer", ""], ["Binder", "Harald", ""], ["L\u00fcbbert", "Michael", ""], ["Schumacher", "Martin", ""]]}, {"id": "1909.12862", "submitter": "Luiz Gomes M.Sc", "authors": "Luiz E. S. Gomes, Tha\\'is C. O. Fonseca, Kelly C. M. Gon\\c{c}alves,\n  Ramiro Ruiz-C\\'ardenas", "title": "Space-time calibration of wind speed forecasts from regional climate\n  models", "comments": "43 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical weather predictions (NWP) are systematically subject to errors due\nto the deterministic solutions used by numerical models to simulate the\natmosphere. Statistical postprocessing techniques are widely used nowadays for\nNWP calibration. However, time-varying bias is usually not accommodated by such\nmodels. Its calibration performance is also sensitive to the temporal window\nused for training. This paper proposes space-time models that extend the main\nstatistical postprocessing approaches to calibrate NWP model outputs.\nTrans-Gaussian random fields are considered to account for meteorological\nvariables with asymmetric behavior. Data augmentation is used to account for\ncensuring in the response variable. The benefits of the proposed extensions are\nillustrated through the calibration of hourly 10 m wind speed forecasts in\nSoutheastern Brazil coming from the Eta model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 18:22:56 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 18:59:30 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 13:28:27 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gomes", "Luiz E. S.", ""], ["Fonseca", "Tha\u00eds C. O.", ""], ["Gon\u00e7alves", "Kelly C. M.", ""], ["Ruiz-C\u00e1rdenas", "Ramiro", ""]]}, {"id": "1909.12881", "submitter": "Linqing Wei", "authors": "Linqing Wei, Lucy Z.Kornblith, Alan Hubbard", "title": "A Data-Adaptive Targeted Learning Approach of Evaluating Viscoelastic\n  Assay Driven Trauma Treatment Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the impact of trauma treatment protocols is complicated by the\nhigh dimensional yet finite sample nature of trauma data collected from\nobservational studies. Viscoelastic assays are highly predictive measures of\nhemostasis. However, the effectiveness of thromboelastography(TEG) based\ntreatment protocols has not been statistically evaluated.To conduct robust and\nreliable estimation with sparse data, we built an estimation \"machine\" for\nestimating causal impacts of candidate variables using the collaborative\ntargeted maximum loss-based estimation(CTMLE) framework.The computational\nefficiency is achieved by using the scalable version of CTMLE such that the\ncovariates are pre-ordered by summary statistics of their importance before\nproceeding to the estimation steps.To extend the application of the estimator\nin practice, we used super learning in combination with CTMLE to flexibly\nchoose the best convex combination of algorithms. By selecting the optimal\ncovariates set in high dimension and reducing constraints in choosing\npre-ordering algorithms, we are able to construct a robust and data-adaptive\nmodel to estimate the parameter of interest.Under this estimation framework,\nCTMLE outperformed the other doubly robust estimators(IPW,AIPW,stabilized\nIPW,TMLE) in the simulation study. CTMLE demonstrated very accurate estimation\nof the target parameter (ATE). Applying CTMLE on the real trauma data, the\ntreatment protocol (using TEG values immediately after injury) showed\nsignificant improvement in trauma patient hemostasis status (control of\nbleeding), and a decrease in mortality rate at 6h compared to standard care.The\nestimation results did not show significant change in mortality rate at 24h\nafter arrival.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 19:21:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wei", "Linqing", ""], ["Kornblith", "Lucy Z.", ""], ["Hubbard", "Alan", ""]]}, {"id": "1909.13031", "submitter": "Sarath Yasodharan", "authors": "Sarath Yasodharan, Patrick Loiseau", "title": "Nonzero-sum Adversarial Hypothesis Testing Games", "comments": "23 pages, 14 figures. Accepted for publication in the 33rd Conference\n  on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonzero-sum hypothesis testing games that arise in the context of\nadversarial classification, in both the Bayesian as well as the Neyman-Pearson\nframeworks. We first show that these games admit mixed strategy Nash\nequilibria, and then we examine some interesting concentration phenomena of\nthese equilibria. Our main results are on the exponential rates of convergence\nof classification errors at equilibrium, which are analogous to the well-known\nChernoff-Stein lemma and Chernoff information that describe the error exponents\nin the classical binary hypothesis testing problem, but with parameters derived\nfrom the adversarial model. The results are validated through numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:46:48 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yasodharan", "Sarath", ""], ["Loiseau", "Patrick", ""]]}, {"id": "1909.13084", "submitter": "Mahdi Abolghasemi", "authors": "Mahdi Abolghasemi, Richard Gerlach, Garth Tarr, Eric Beh", "title": "Demand forecasting in supply chain: The impact of demand volatility in\n  the presence of promotion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for a particular product or service is typically associated with\ndifferent uncertainties that can make them volatile and challenging to predict.\nDemand unpredictability is one of the managers' concerns in the supply chain\nthat can cause large forecasting errors, issues in the upstream supply chain\nand impose unnecessary costs. We investigate 843 real demand time series with\ndifferent values of coefficient of variations (CoV) where promotion causes\nvolatility over the entire demand series. In such a case, forecasting demand\nfor different CoV require different models to capture the underlying behavior\nof demand series and pose significant challenges due to very different and\ndiverse demand behavior. We decompose demand into baseline and promotional\ndemand and propose a hybrid model to forecast demand. Our results indicate that\nour proposed hybrid model generates robust and accurate forecast across series\nwith different levels of volatilities. We stress the necessity of decomposition\nfor volatile demand series. We also model demand series with a number of well\nknown statistical and machine learning (ML) models to investigate their\nperformance empirically. We found that ARIMA with covariate (ARIMAX) works well\nto forecast volatile demand series, but exponential smoothing with covariate\n(ETSX) has a poor performance. Support vector regression (SVR) and dynamic\nlinear regression (DLR) models generate robust forecasts across different\ncategories of demands with different CoV values.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 12:50:07 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Abolghasemi", "Mahdi", ""], ["Gerlach", "Richard", ""], ["Tarr", "Garth", ""], ["Beh", "Eric", ""]]}, {"id": "1909.13118", "submitter": "Ritabrata Dutta", "authors": "Lorenzo Pacchiardi, Pierre Kunzli, Marcel Schoengens, Bastien Chopard,\n  Ritabrata Dutta", "title": "Distance-learning For Approximate Bayesian Computation To Model a\n  Volcanic Eruption", "comments": null, "journal-ref": "Sankhya B (2020)", "doi": "10.1007/s13571-019-00208-8", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) provides us with a way to infer\nparameters of models, for which the likelihood function is not available, from\nan observation. Using ABC, which depends on many simulations from the\nconsidered model, we develop an inferential framework to learn parameters of a\nstochastic numerical simulator of volcanic eruption. Moreover, the model itself\nis parallelized using Message Passing Interface (MPI). Thus, we develop a\nnested-parallelized MPI communicator to handle the expensive numerical model\nwith ABC algorithms. ABC usually relies on summary statistics of the data in\norder to measure the discrepancy model output and observation. However,\ninformative summary statistics cannot be found for the considered model. We\ntherefore develop a technique to learn a distance between model outputs based\non deep metric-learning. We use this framework to learn the plume\ncharacteristics (eg. initial plume velocity) of the volcanic eruption from the\ntephra deposits collected by field-work associated with the 2450 BP Pululagua\n(Ecuador) volcanic eruption.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 16:14:08 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Kunzli", "Pierre", ""], ["Schoengens", "Marcel", ""], ["Chopard", "Bastien", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "1909.13133", "submitter": "Carlo Manzo", "authors": "Tina Kos\\v{u}ta, Marta Cullell-Dalmau, Francesca Cella Zanacchi and\n  Carlo Manzo", "title": "Bayesian analysis of data from segmented super-resolution images for\n  quantifying protein clustering", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": "10.1039/C9CP05616E", "report-no": null, "categories": "physics.bio-ph physics.data-an q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution imaging techniques have largely improved our capabilities to\nvisualize nanometric structures in biological systems. Their application\nfurther enables one to potentially quantitate relevant parameters to determine\nthe molecular organization and stoichiometry in cells. However, the inherently\nstochastic nature of the fluorescence emission and labeling strategies imposes\nthe use of dedicated methods to accurately measure these parameters. Here, we\ndescribe a Bayesian approach to precisely quantitate the relative abundance of\nmolecular oligomers from segmented images. The distribution of proxies for the\nnumber of molecules in a cluster -- such as the number of localizations or the\nfluorescence intensity -- is fitted via a nested sampling algorithm to compare\nmixture models of increasing complexity and determine the optimal number of\nmixture components and their weights. We test the performance of the algorithm\non {\\it in silico} data as a function of the number of data points, threshold,\nand distribution shape. We compare these results to those obtained with other\nstatistical methods, showing the improved performance of our approach. Our\nmethod provides a robust tool for model selection in fitting data extracted\nfrom fluorescence imaging, thus improving the precision of parameter\ndetermination. Importantly, the largest benefit of this method occurs for\nsmall-statistics or incomplete datasets, enabling accurate analysis at the\nsingle image level. We further present the results of its application to\nexperimental data obtained from the super-resolution imaging of dynein in HeLa\ncells, confirming the presence of a mixed population of cytoplasmatic single\nmotors and higher-order structures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 19:12:27 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Kos\u01d4ta", "Tina", ""], ["Cullell-Dalmau", "Marta", ""], ["Zanacchi", "Francesca Cella", ""], ["Manzo", "Carlo", ""]]}, {"id": "1909.13307", "submitter": "Ali Karimnezhad", "authors": "Ali Karimnezhad", "title": "A Simple Yet Efficient Parametric Method of Local False Discovery Rate\n  Estimation Designed for Genome-Wide Association Data Analysis", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association studies, hundreds of thousands of genetic features\n(genes, proteins, etc.) in a given case-control population are tested to verify\nexistence of an association between each genetic marker and a specific disease.\nA popular approach in this regard is to estimate local false discovery rate\n(LFDR), the posterior probability that the null hypothesis is true, given an\nobserved test statistic. However, the existing LFDR estimation methods in the\nliterature are usually complicated. Assuming a chi-square model with one degree\nof freedom, which covers many situations in genome-wide association studies, we\nuse the method of moments and introduce a simple, fast and efficient approach\nfor LFDR estimation. We perform two different simulation strategies and compare\nthe performance of the proposed approach with three popular LFDR estimation\nmethods. We also examine the practical utility of the proposed method by\nanalyzing a comprehensive 1000 genomes-based genome-wide association data\ncontaining approximately 9.4 million single nucleotide polymorphisms, and a\nmicroarray data set consisting of genetic expression levels for 6033 genes for\nprostate cancer patients. The R package implementing the proposed method is\navailable on CRAN <https://cran.r-project.org/web/packages/LFDR.MME>.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 16:02:33 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 13:45:12 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:39:48 GMT"}, {"version": "v4", "created": "Sun, 22 Nov 2020 02:23:19 GMT"}, {"version": "v5", "created": "Fri, 29 Jan 2021 17:27:16 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Karimnezhad", "Ali", ""]]}, {"id": "1909.13361", "submitter": "Christoph Kern", "authors": "Christoph Kern, Bernd Weiss, Jan-Philipp Kolb", "title": "A Longitudinal Framework for Predicting Nonresponse in Panel Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonresponse in panel studies can lead to a substantial loss in data quality\ndue to its potential to introduce bias and distort survey estimates. Recent\nwork investigates the usage of machine learning to predict nonresponse in\nadvance, such that predicted nonresponse propensities can be used to inform the\ndata collection process. However, predicting nonresponse in panel studies\nrequires accounting for the longitudinal data structure in terms of model\nbuilding, tuning, and evaluation. This study proposes a longitudinal framework\nfor predicting nonresponse with machine learning and multiple panel waves and\nillustrates its application. With respect to model building, this approach\nutilizes information from multiple waves by introducing features that aggregate\nprevious (non)response patterns. Concerning model tuning and evaluation,\ntemporal cross-validation is employed by iterating through pairs of panel waves\nsuch that the training and test sets move in time. Implementing this approach\nwith data from a German probability-based mixed-mode panel shows that\naggregating information over multiple panel waves can be used to build\nprediction models with competitive and robust performance over all test waves.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 20:28:45 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 02:41:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kern", "Christoph", ""], ["Weiss", "Bernd", ""], ["Kolb", "Jan-Philipp", ""]]}, {"id": "1909.13381", "submitter": "Enguerrand Horel", "authors": "Enguerrand Horel, Kay Giesecke, Victor Storchan, Naren Chittar", "title": "Explainable Clustering and Application to Wealth Management Compliance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications from the financial industry successfully leverage\nclustering algorithms to reveal meaningful patterns among a vast amount of\nunstructured financial data. However, these algorithms suffer from a lack of\ninterpretability that is required both at a business and regulatory level. In\norder to overcome this issue, we propose a novel two-steps method to explain\nclusters. A classifier is first trained to predict the clusters labels, then\nthe Single Feature Introduction Test (SFIT) method is run on the model to\nidentify the statistically significant features that characterise each cluster.\nWe describe a real wealth management compliance use-case that highlights the\nnecessity of such an interpretable clustering method. We illustrate the\nperformance of our method using simulated data and through an experiment on\nfinancial ratios of U.S. companies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:16:15 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 18:23:58 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 06:16:50 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Horel", "Enguerrand", ""], ["Giesecke", "Kay", ""], ["Storchan", "Victor", ""], ["Chittar", "Naren", ""]]}, {"id": "1909.13766", "submitter": "Dave Osthus", "authors": "Dave Osthus and Kelly R Moran", "title": "Multiscale Influenza Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza forecasting in the United States (US) is complex and challenging\nfor reasons including substantial spatial and temporal variability, nested\ngeographic scales of forecast interest, and heterogeneous surveillance\nparticipation. Here we present a flexible influenza forecasting model called\nDante, a multiscale flu forecasting model that learns rather than prescribes\nspatial, temporal, and surveillance data structure. Forecasts at the Health and\nHuman Services (HHS) regional and national scales are generated as linear\ncombinations of state forecasts with weights proportional to US Census\npopulation estimates, resulting in coherent forecasts across nested geographic\nscales. We retrospectively compare Dante's short-term and seasonal forecasts at\nthe state, regional, and national scales for the 2012 through 2017 flu seasons\nin the US to the Dynamic Bayesian Model (DBM), a leading flu forecasting model.\nDante outperformed DBM for nearly all spatial units, flu seasons, geographic\nscales, and forecasting targets. The improved performance is due to Dante\nmaking forecasts, especially short-term forecasts, more confidently and\naccurately than DBM, suggesting Dante's improved forecast scores will also\ntranslate to more useful forecasts for the public health sector. Dante\nparticipated in the prospective 2018/19 FluSight challenge hosted by the\nCenters for Disease Control and Prevention and placed 1st in both the national\nand regional competition and the state competition. The methodology\nunderpinning Dante can be used in other disease forecasting contexts where\nnested geographic scales of interest exist.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:57:13 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Osthus", "Dave", ""], ["Moran", "Kelly R", ""]]}, {"id": "1909.13773", "submitter": "Gianmarco Alto\\`e", "authors": "Gianmarco Alto\\`e, Giulia Bertoldo, Claudio Zandonella Callegher,\n  Enrico Toffalini, Antonio Calcagn\\`i, Livio Finos and Massimiliano Pastore", "title": "Enhancing statistical inference in psychological research via\n  prospective and retrospective design analysis", "comments": "35 pages, 4 figures, manuscript currently under review", "journal-ref": "Front. Psychol. (2020). 10:2893", "doi": "10.3389/fpsyg.2019.02893", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past two decades, psychological science has experienced an\nunprecedented replicability crisis which uncovered several issues. Among\nothers, statistical inference is too often viewed as an isolated procedure\nlimited to the analysis of data that have already been collected. We build on\nand further develop an idea proposed by Gelman and Carlin (2014) termed\n\"prospective and retrospective design analysis\". Rather than focusing only on\nthe statistical significance of a result and on the classical control of type I\nand type II errors, a comprehensive design analysis involves reasoning about\nwhat can be considered a plausible effect size. Furthermore, it introduces two\nrelevant inferential risks: the exaggeration ratio or Type M error (i.e., the\npredictable average overestimation of an effect that emerges as statistically\nsignificant), and the sign error or Type S error (i.e., the risk that a\nstatistically significant effect is estimated in the wrong direction). Another\nimportant aspect of design analysis is that it can be usefully carried out both\nin the planning phase of a study and for the evaluation of studies that have\nalready been conducted, thus increasing researchers' awareness during all\nphases of a research project. We use a familiar example in psychology where the\nresearcher is interested in analyzing the differences between two independent\ngroups. We examine the case in which the plausible effect size is formalized as\na single value, and propose a method in which uncertainty concerning the\nmagnitude of the effect is formalized via probability distributions. Through\nseveral examples, we show that even though a design analysis requires big\neffort, it has the potential to contribute to planning more robust and\nreplicable studies. Finally, future developments in the Bayesian framework are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 15:14:00 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Alto\u00e8", "Gianmarco", ""], ["Bertoldo", "Giulia", ""], ["Callegher", "Claudio Zandonella", ""], ["Toffalini", "Enrico", ""], ["Calcagn\u00ec", "Antonio", ""], ["Finos", "Livio", ""], ["Pastore", "Massimiliano", ""]]}]