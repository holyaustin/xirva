[{"id": "1809.00052", "submitter": "Niki Gitinabard", "authors": "Niki Gitinabard, Farzaneh Khoshnevisan, Collin F. Lynch, and Elle Yuan\n  Wang", "title": "Your Actions or Your Associates? Predicting Certification and Dropout in\n  MOOCs with Behavioral and Social Features", "comments": "Published at the 11th International Conference on Educational Data\n  Mining (EDM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high level of attrition and low rate of certification in Massive Open\nOnline Courses (MOOCs) has prompted a great deal of research. Prior researchers\nhave focused on predicting dropout based upon behavioral features such as\nstudent confusion, click-stream patterns, and social interactions. However, few\nstudies have focused on combining student logs with forum data.\n  In this work, we use data from two different offerings of the same MOOC. We\nconduct a survival analysis to identify likely dropouts. We then examine two\nclasses of features, social and behavioral, and apply a combination of modeling\nand feature-selection methods to identify the most relevant features to predict\nboth dropout and certification. We examine the utility of three different model\ntypes and we consider the impact of different definitions of dropout on the\npredictors. Finally, we assess the reliability of the models over time by\nevaluating whether or not models from week 1 can predict dropout in week 2, and\nso on. The outcomes of this study will help instructors identify students\nlikely to fail or dropout as soon as the first two weeks and provide them with\nmore support.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 20:39:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gitinabard", "Niki", ""], ["Khoshnevisan", "Farzaneh", ""], ["Lynch", "Collin F.", ""], ["Wang", "Elle Yuan", ""]]}, {"id": "1809.00258", "submitter": "Yogatheesan Varatharajah", "authors": "Yogatheesan Varatharajah, Brent Berry, Sanmi Koyejo, and Ravishankar\n  Iyer", "title": "A Contextual-bandit-based Approach for Informed Decision-making in\n  Clinical Trials", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials involving multiple treatments utilize randomization of the\ntreatment assignments to enable the evaluation of treatment efficacies in an\nunbiased manner. Such evaluation is performed in post hoc studies that usually\nuse supervised-learning methods that rely on large amounts of data collected in\na randomized fashion. That approach often proves to be suboptimal in that some\nparticipants may suffer and even die as a result of having not received the\nmost appropriate treatments during the trial. Reinforcement-learning methods\nimprove the situation by making it possible to learn the treatment efficacies\ndynamically during the course of the trial, and to adapt treatment assignments\naccordingly. Recent efforts using \\textit{multi-arm bandits}, a type of\nreinforcement-learning methods, have focused on maximizing clinical outcomes\nfor a population that was assumed to be homogeneous. However, those approaches\nhave failed to account for the variability among participants that is becoming\nincreasingly evident as a result of recent clinical-trial-based studies. We\npresent a contextual-bandit-based online treatment optimization algorithm that,\nin choosing treatments for new participants in the study, takes into account\nnot only the maximization of the clinical outcomes but also the patient\ncharacteristics. We evaluated our algorithm using a real clinical trial dataset\nfrom the International Stroke Trial. The results of our retrospective analysis\nindicate that the proposed approach performs significantly better than either a\nrandom assignment of treatments (the current gold standard) or a\nmulti-arm-bandit-based approach, providing substantial gains in the percentage\nof participants who are assigned the most suitable treatments. The\ncontextual-bandit and multi-arm bandit approaches provide 72.63% and 64.34%\ngains, respectively, compared to a random assignment.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 22:07:23 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Varatharajah", "Yogatheesan", ""], ["Berry", "Brent", ""], ["Koyejo", "Sanmi", ""], ["Iyer", "Ravishankar", ""]]}, {"id": "1809.00358", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Stephen Allsop, Kay M. Tye, Demba Ba and Vahid Tarokh", "title": "Sequential Detection of Regime Changes in Neural Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting changes in firing patterns in neural data is\nstudied. The problem is formulated as a quickest change detection problem.\nImportant algorithms from the literature are reviewed. A new algorithmic\ntechnique is discussed to detect deviations from learned baseline behavior. The\nalgorithms studied can be applied to both spike and local field potential data.\nThe algorithms are applied to mice spike data to verify the presence of\nbehavioral learning.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 15:31:14 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Banerjee", "Taposh", ""], ["Allsop", "Stephen", ""], ["Tye", "Kay M.", ""], ["Ba", "Demba", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1809.00463", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Shrinkage for Covariance Estimation: Asymptotics, Confidence Intervals,\n  Bounds and Applications in Sensor Monitoring and Finance", "comments": null, "journal-ref": "Statistical Papers, 2018, Vol. 59, 1441-1462", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When shrinking a covariance matrix towards (a multiple) of the identity\nmatrix, the trace of the covariance matrix arises naturally as the optimal\nscaling factor for the identity target. The trace also appears in other\ncontext, for example when measuring the size of a matrix or the amount of\nuncertainty.\n  Of particular interest is the case when the dimension of the covariance\nmatrix is large. Then the problem arises that the sample covariance matrix is\nsingular if the dimension is larger than the sample size. Another issue is that\nusually the estimation has to based on correlated time series data. We study\nthe estimation of the trace functional allowing for a high-dimensional time\nseries model, where the dimension is allowed to grow with the sample size -\nwithout any constraint. Based on a recent result, we investigate a confidence\ninterval for the trace, which also allows us to propose lower and upper bounds\nfor the shrinkage covariance estimator as well as bounds for the variance of\nprojections. In addition, we provide a novel result dealing with shrinkage\ntowards a diagonal target.\n  We investigate the accuracy of the confidence interval by a simulation study,\nwhich indicates good performance, and analyze three stock market data sets to\nillustrate the proposed bounds, where the dimension (number of stocks) ranges\nbetween $32$ and $475$. Especially, we apply the results to portfolio\noptimization and determine bounds for the risk associated to the\nvariance-minimizing portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 06:45:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1809.00480", "submitter": "Yuzhou Li", "authors": "Hongkuan Zhou, Yuzhou Li, and Tao Jiang", "title": "Sea Clutter Distribution Modeling: A Kernel Density Estimation Approach", "comments": "6 pages, 4 figures, 1 table, to appear in Proc. International\n  Conference on Wireless Communications & Signal Processing (WCSP), Hangzhou,\n  China, Oct. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate sea clutter distribution is crucial for decision region\ndetermination when detecting sea-surface floating targets. However, traditional\nparametric models possibly have a considerable gap to the realistic\ndistribution of sea clutters due to the volatile sea states. In this paper, we\ndevelop a kernel density estimation based framework to model the sea clutter\ndistributions without requiring any prior knowledge. In this framework, we\njointly consider two embedded fundamental problems, the selection of a proper\nkernel density function and the determination of its corresponding optimal\nbandwidth. Regarding these two problems, we adopt the Gaussian, Gamma, and\nWeibull distributions as the kernel functions, and derive the closed-form\noptimal bandwidth equations for them. To deal with the highly complicated\nequations for the three kernels, we further design a fast iterative bandwidth\nselection algorithm to solve them. Experimental results show that, compared\nwith existing methods, our proposed approach can significantly decrease the\nerror incurred by sea clutter modeling (about two orders of magnitude\nreduction) and improve the target detection probability (up to $36\\%$ in low\nfalse alarm rate cases).\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 07:52:56 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhou", "Hongkuan", ""], ["Li", "Yuzhou", ""], ["Jiang", "Tao", ""]]}, {"id": "1809.00511", "submitter": "Mohamed Laib", "authors": "Mohamed Laib and Fabian Guignard and Mikhail Kanevski and Luciano\n  Telesca", "title": "Community detection analysis in wind speed-monitoring systems using\n  mutual information-based complex network", "comments": null, "journal-ref": null, "doi": "10.1063/1.5054724", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mutual information-based weighted network representation of a wide wind\nspeed monitoring system in Switzerland was analysed in order to detect\ncommunities. Two communities have been revealed, corresponding to two clusters\nof sensors situated respectively on the Alps and on the Jura-Plateau that\ndefine the two major climatic zones of Switzerland. The silhouette measure is\nused to evaluate the obtained communities and confirm the membership of each\nsensor to its cluster.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 09:12:13 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Laib", "Mohamed", ""], ["Guignard", "Fabian", ""], ["Kanevski", "Mikhail", ""], ["Telesca", "Luciano", ""]]}, {"id": "1809.00544", "submitter": "Oliver Stoner", "authors": "Oliver Stoner and Theo Economou and Gabriela Drummond", "title": "A Hierarchical Framework for Correcting Under-Reporting in Count Data", "comments": "34 pages, 14 figures, Journal of the American Statistical Association", "journal-ref": null, "doi": "10.1080/01621459.2019.1573732", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis poses a global health risk and Brazil is among the top twenty\ncountries by absolute mortality. However, this epidemiological burden is masked\nby under-reporting, which impairs planning for effective intervention. We\npresent a comprehensive investigation and application of a Bayesian\nhierarchical approach to modelling and correcting under-reporting in\ntuberculosis counts, a general problem arising in observational count data. The\nframework is applicable to fully under-reported data, relying only on an\ninformative prior distribution for the mean reporting rate to supplement the\npartial information in the data. Covariates are used to inform both the true\ncount generating process and the under-reporting mechanism, while also allowing\nfor complex spatio-temporal structures. We present several sensitivity analyses\nbased on simulation experiments to aid the elicitation of the prior\ndistribution for the mean reporting rate and decisions relating to the\ninclusion of covariates. Both prior and posterior predictive model checking are\npresented, as well as a critical evaluation of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 10:46:45 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Stoner", "Oliver", ""], ["Economou", "Theo", ""], ["Drummond", "Gabriela", ""]]}, {"id": "1809.00734", "submitter": "Ivana Malenica", "authors": "Mark J. van der Laan and Ivana Malenica", "title": "Robust Estimation of Data-Dependent Causal Effects based on Observing a\n  Single Time-Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the case that one observes a single time-series, where at each time\nt one observes a data record O(t) involving treatment nodes A(t), possible\ncovariates L(t) and an outcome node Y(t). The data record at time t carries\ninformation for an (potentially causal) effect of the treatment A(t) on the\noutcome Y(t), in the context defined by a fixed dimensional summary measure\nCo(t). We are concerned with defining causal effects that can be consistently\nestimated, with valid inference, for sequentially randomized experiments\nwithout further assumptions. More generally, we consider the case when the\n(possibly causal) effects can be estimated in a double robust manner, analogue\nto double robust estimation of effects in the i.i.d. causal inference\nliterature. We propose a general class of averages of conditional\n(context-specific) causal parameters that can be estimated in a double robust\nmanner, therefore fully utilizing the sequential randomization. We propose a\ntargeted maximum likelihood estimator (TMLE) of these causal parameters, and\npresent a general theorem establishing the asymptotic consistency and normality\nof the TMLE. We extend our general framework to a number of typically studied\ncausal target parameters, including a sequentially adaptive design within a\nsingle unit that learns the optimal treatment rule for the unit over time. Our\nwork opens up robust statistical inference for causal questions based on\nobserving a single time-series on a particular unit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 22:02:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Malenica", "Ivana", ""]]}, {"id": "1809.00964", "submitter": "Dorje C. Brody Professor", "authors": "Dorje C. Brody and David M. Meier", "title": "How to model fake news", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT cs.SI econ.GN math.IT math.PR q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past three years it has become evident that fake news is a danger to\ndemocracy. However, until now there has been no clear understanding of how to\ndefine fake news, much less how to model it. This paper addresses both these\nissues. A definition of fake news is given, and two approaches for the\nmodelling of fake news and its impact in elections and referendums are\nintroduced. The first approach, based on the idea of a representative voter, is\nshown to be suitable to obtain a qualitative understanding of phenomena\nassociated with fake news at a macroscopic level. The second approach, based on\nthe idea of an election microstructure, describes the collective behaviour of\nthe electorate by modelling the preferences of individual voters. It is shown\nthrough a simulation study that the mere knowledge that pieces of fake news may\nbe in circulation goes a long way towards mitigating the impact of fake news.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 13:50:49 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 10:39:34 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Brody", "Dorje C.", ""], ["Meier", "David M.", ""]]}, {"id": "1809.01094", "submitter": "Stephen Ellison", "authors": "Stephen L.R.Ellison", "title": "An outlier-resistant indicator of anomalies among inter-laboratory\n  comparison data with associated uncertainty", "comments": null, "journal-ref": null, "doi": "10.1088/1681-7575/aae610", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust pairwise statistic, the pairwise median scaled difference (MSD),\nis proposed for the detection of anomalous location/uncertainty pairs in\nheteroscedastic interlaboratory study data with associated uncertainties. The\ndistribution for the IID case is presented and approximate critical values for\nroutine use are provided. The determination of observation-specific quantiles\nand p-values for heteroscedastic data, using parametric bootstrapping, is\ndemonstrated by example. It is shown that the statistic has good power for\ndetecting anomalies compared to a previous pairwise statistic, and offers much\ngreater resistance to multiple outlying values.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:01:43 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Ellison", "Stephen L. R.", ""]]}, {"id": "1809.01722", "submitter": "Hau-tieng Wu", "authors": "Yu-Ting Lin, Yu-Lun Lo, Chen-Yun Lin, Hau-Tieng Wu, Martin G. Frasch", "title": "Unexpected sawtooth artifact in beat-to-beat pulse transit time measured\n  from patient monitor data", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0221319", "report-no": null, "categories": "q-bio.QM cs.LG eess.SP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object: It is increasingly popular to collect as much data as possible in the\nhospital setting from clinical monitors for research purposes. However, in this\nsetup the data calibration issue is often not discussed and, rather, implicitly\nassumed, while the clinical monitors might not be designed for the data\nanalysis purpose. We hypothesize that this calibration issue for a secondary\nanalysis may become an important source of artifacts in patient monitor data.\nWe test an off-the-shelf integrated photoplethysmography (PPG) and\nelectrocardiogram (ECG) monitoring device for its ability to yield a reliable\npulse transit time (PTT) signal. Approach: This is a retrospective clinical\nstudy using two databases: one containing 35 subjects who underwent\nlaparoscopic cholecystectomy, another containing 22 subjects who underwent\nspontaneous breathing test in the intensive care unit. All data sets include\nrecordings of PPG and ECG using a commonly deployed patient monitor. We\ncalculated the PTT signal offline. Main Results: We report a novel constant\noscillatory pattern in the PTT signal and identify this pattern as a sawtooth\nartifact. We apply an approach based on the de-shape method to visualize,\nquantify and validate this sawtooth artifact. Significance: The PPG and ECG\nsignals not designed for the PTT evaluation may contain unwanted artifacts. The\nPTT signal should be calibrated before analysis to avoid erroneous\ninterpretation of its physiological meaning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:35:09 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 14:41:54 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Yu-Ting", ""], ["Lo", "Yu-Lun", ""], ["Lin", "Chen-Yun", ""], ["Wu", "Hau-Tieng", ""], ["Frasch", "Martin G.", ""]]}, {"id": "1809.01740", "submitter": "Matthew Engelhard", "authors": "Matthew Engelhard, Hongteng Xu, Lawrence Carin, Jason A Oliver,\n  Matthew Hallyburton, F Joseph McClernon", "title": "Predicting Smoking Events with a Time-Varying Semi-Parametric Hawkes\n  Process Model", "comments": "Presented at Machine Learning for Healthcare 2018, Stanford, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health risks from cigarette smoking -- the leading cause of preventable death\nin the United States -- can be substantially reduced by quitting. Although most\nsmokers are motivated to quit, the majority of quit attempts fail. A number of\nstudies have explored the role of self-reported symptoms, physiologic\nmeasurements, and environmental context on smoking risk, but less work has\nfocused on the temporal dynamics of smoking events, including daily patterns\nand related nicotine effects. In this work, we examine these dynamics and\nimprove risk prediction by modeling smoking as a self-triggering process, in\nwhich previous smoking events modify current risk. Specifically, we fit smoking\nevents self-reported by 42 smokers to a time-varying semi-parametric Hawkes\nprocess (TV-SPHP) developed for this purpose. Results show that the TV-SPHP\nachieves superior prediction performance compared to related and existing\nmodels, with the incorporation of time-varying predictors having greatest\nbenefit over longer prediction windows. Moreover, the impact function\nillustrates previously unknown temporal dynamics of smoking, with possible\nconnections to nicotine metabolism to be explored in future work through a\nrandomized study design. By more effectively predicting smoking events and\nexploring a self-triggering component of smoking risk, this work supports\ndevelopment of novel or improved cessation interventions that aim to reduce\ndeath from smoking.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 21:37:27 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Engelhard", "Matthew", ""], ["Xu", "Hongteng", ""], ["Carin", "Lawrence", ""], ["Oliver", "Jason A", ""], ["Hallyburton", "Matthew", ""], ["McClernon", "F Joseph", ""]]}, {"id": "1809.01832", "submitter": "Pratheepa Jeganathan", "authors": "Pratheepa Jeganathan, Benjamin J. Callahan, Diana M. Proctor, David A.\n  Relman, Susan P. Holmes", "title": "The Block Bootstrap Method for Longitudinal Microbiome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial ecology serves as a foundation for a wide range of scientific and\nbiomedical studies. Rapidly-evolving high-throughput sequencing technology\nenables the comprehensive search for microbial biomarkers using longitudinal\nexperiments. Such experiments consist of repeated biological observations from\neach subject over time and are essential in accounting for the high\nbetween-subject and within-subject variability.\n  Unfortunately, many of the statistical tests based on parametric models rely\non correctly specifying temporal dependence structure which is unavailable in\nmost microbiome data.\n  In this paper, we propose an extension of the nonparametric bootstrap method\nthat enables inference on these types longitudinal data. The proposed moving\nblock bootstrap (MBB) method accounts for within-subject dependency by using\noverlapping blocks of repeated observations within each subject to draw valid\ninferences based on approximately pivotal statistics. Our simulation studies\nshow an increase in power compared to merge-by-subject (MBS) strategies. We\nalso show that compared to tests that presume independent samples (PIS), our\nproposed method reduces false microbial biomarker discovery rates.\n  In this paper, we illustrated the MBB method using three different pregnancy\ndata and an oral microbiome data. We provide an open-source R package\nhttps://github.com/PratheepaJ/bootLong to make our method accessible and the\nstudy in this paper reproducible.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:42:21 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 18:51:50 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jeganathan", "Pratheepa", ""], ["Callahan", "Benjamin J.", ""], ["Proctor", "Diana M.", ""], ["Relman", "David A.", ""], ["Holmes", "Susan P.", ""]]}, {"id": "1809.02408", "submitter": "Laura Balzer PhD", "authors": "Hachem Saddiki and Laura B. Balzer", "title": "A Primer on Causality in Data Science", "comments": "26 pages (with references); 4 figures", "journal-ref": "Journal de la Societe Francaise de Statistique, 161, 2020, 67-90", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many questions in Data Science are fundamentally causal in that our objective\nis to learn the effect of some exposure, randomized or not, on an outcome\ninterest. Even studies that are seemingly non-causal, such as those with the\ngoal of prediction or prevalence estimation, have causal elements, including\ndifferential censoring or measurement. As a result, we, as Data Scientists,\nneed to consider the underlying causal mechanisms that gave rise to the data,\nrather than simply the pattern or association observed in those data. In this\nwork, we review the 'Causal Roadmap' of Petersen and van der Laan (2014) to\nprovide an introduction to some key concepts in causal inference. Similar to\nother causal frameworks, the steps of the Roadmap include clearly stating the\nscientific question, defining of the causal model, translating the scientific\nquestion into a causal parameter, assessing the assumptions needed to express\nthe causal parameter as a statistical estimand, implementation of statistical\nestimators including parametric and semi-parametric methods, and interpretation\nof our findings. We believe that using such a framework in Data Science will\nhelp to ensure that our statistical analyses are guided by the scientific\nquestion driving our research, while avoiding over-interpreting our results. We\nfocus on the effect of an exposure occurring at a single time point and\nhighlight the use of targeted maximum likelihood estimation (TMLE) with Super\nLearner.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 11:26:51 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 04:38:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Saddiki", "Hachem", ""], ["Balzer", "Laura B.", ""]]}, {"id": "1809.02512", "submitter": "Guilherme Gomes", "authors": "Guilherme Gomes and Vinayak Rao and Jennifer Neville", "title": "Multi-level hypothesis testing for populations of heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider hypothesis testing and anomaly detection on\ndatasets where each observation is a weighted network. Examples of such data\ninclude brain connectivity networks from fMRI flow data, or word co-occurrence\ncounts for populations of individuals. Current approaches to hypothesis testing\nfor weighted networks typically requires thresholding the edge-weights, to\ntransform the data to binary networks. This results in a loss of information,\nand outcomes are sensitivity to choice of threshold levels. Our work avoids\nthis, and we consider weighted-graph observations in two situations, 1) where\neach graph belongs to one of two populations, and 2) where entities belong to\none of two populations, with each entity possessing multiple graphs (indexed\ne.g. by time). Specifically, we propose a hierarchical Bayesian hypothesis\ntesting framework that models each population with a mixture of latent space\nmodels for weighted networks, and then tests populations of networks for\ndifferences in distribution over components. Our framework is capable of\npopulation-level, entity-specific, as well as edge-specific hypothesis testing.\nWe apply it to synthetic data and three real-world datasets: two social media\ndatasets involving word co-occurrences from discussions on Twitter of the\npolitical unrest in Brazil, and on Instagram concerning Attention Deficit\nHyperactivity Disorder (ADHD) and depression drugs, and one medical dataset\ninvolving fMRI brain-scans of human subjects. The results show that our\nproposed method has lower Type I error and higher statistical power compared to\nalternatives that need to threshold the edge weights. Moreover, they show our\nproposed method is better suited to deal with highly heterogeneous datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:44:11 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Gomes", "Guilherme", ""], ["Rao", "Vinayak", ""], ["Neville", "Jennifer", ""]]}, {"id": "1809.02699", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Hongbin Li, Junyi Zuo, Wei Zhang and Heping Wang", "title": "Maximum Correntropy Derivative-Free Robust Kalman Filter and Smoother", "comments": null, "journal-ref": "IEEE Access 6 (2018): 70794-70807", "doi": "10.1109/ACCESS.2018.2880618", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust estimation involving filtering and\nsmoothing for nonlinear state space models which are disturbed by heavy-tailed\nimpulsive noises. To deal with heavy-tailed noises and improve the robustness\nof the traditional nonlinear Gaussian Kalman filter and smoother, we propose in\nthis work a general framework of robust filtering and smoothing, which adopts a\nnew maximum correntropy criterion to replace the minimum mean square error for\nstate estimation. To facilitate understanding, we present our robust framework\nin conjunction with the cubature Kalman filter and smoother. A half-quadratic\noptimization method is utilized to solve the formulated robust estimation\nproblems, which leads to a new maximum correntropy derivative-free robust\nKalman filter and smoother. Simulation results show that the proposed methods\nachieve a substantial performance improvement over the conventional and\nexisting robust ones with slight computational time increase.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 22:19:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Hongwei", ""], ["Li", "Hongbin", ""], ["Zuo", "Junyi", ""], ["Zhang", "Wei", ""], ["Wang", "Heping", ""]]}, {"id": "1809.02911", "submitter": "Mansur Arief", "authors": "Zhiyuan Huang, Mansur Arief, Henry Lam and Ding Zhao", "title": "Synthesis of Different Autonomous Vehicles Test Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the most prevalent way to evaluate an autonomous vehicle is to\ndirectly test it on the public road. However, because of recent accidents\ncaused by autonomous vehicles, it becomes controversial about whether on-road\ntests should be the best approach. Alternatively, people use test tracks or\nsimulation to assess the safety of autonomous vehicles. These approaches are\ntime-efficient and less costly, however, their credibility varies. In this\npaper, we propose to use a co-Kriging model to synthesize the results from\ndifferent evaluation approaches, which allows us to fully utilize the\ninformation and provides an accurate, affordable, and safe way to assess a\ndesign of an autonomous vehicle.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 01:57:37 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Huang", "Zhiyuan", ""], ["Arief", "Mansur", ""], ["Lam", "Henry", ""], ["Zhao", "Ding", ""]]}, {"id": "1809.02935", "submitter": "Eleni Elia", "authors": "Eleni G. Elia, Nicolas St\\\"adler, Oriana Ciani, Rod S. Taylor, Sylwia\n  Bujkiewicz", "title": "Combining tumour response and progression free survival as surrogate\n  endpoints for overall survival in advanced colorectal cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progression free survival (PFS) and tumour response (TR) have been\ninvestigated as surrogate endpoints for overall survival (OS) in advanced\ncolorectal cancer (aCRC), however their validity has been shown to be\nsuboptimal. In recent years, meta-analytic methods allowing for use of multiple\nsurrogate endpoints jointly have been proposed. The aim of this research was to\nassess if PFS and TR used jointly as surrogate endpoints to OS improve their\npredictive value. Data were obtained from a systematic review of randomised\ncontrolled trials investigating effectiveness of different pharmacological\ntherapies in aCRC: systemic chemotherapies, anti-epidermal growth factor\nreceptor therapies, anti-angiogenic agents, other multi-targeted antifolate\ntreatments and intra-hepatic arterial chemotherapy. Multivariate meta-analysis\nwas used to model the association patterns between treatment effects on the\nsurrogate endpoints (PFS, TR) and the final outcome (OS). Analysis of 33 trials\nwhich reported treatment effects on all three outcomes showed reasonably strong\nassociation between treatment effects on PFS and OS. A weak surrogate\nrelationship was noted between the treatment effects on TR and OS. Modelling\nthe two surrogate endpoints, TR and PFS, jointly as predictors of OS gave no\nmarked improvement in neither surrogacy patterns nor the precision of predicted\ntreatment effect in the cross-validation procedure. When investigating\nsubgroups of therapy, only small improvement in precision of predicted\ntreatment effects on the final outcome in studies investigating anti-angiogenic\ntherapy was noted. Overall, the simultaneous modelling of two surrogate\nendpoints did not lead to improvement in association between treatment effects\non surrogate and final endpoints in aCRC.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 07:12:00 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Elia", "Eleni G.", ""], ["St\u00e4dler", "Nicolas", ""], ["Ciani", "Oriana", ""], ["Taylor", "Rod S.", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "1809.02959", "submitter": "Mahdi Teimouri Yanesari", "authors": "Mahdi Teimouri", "title": "MPS: An R package for modelling new families of distributions", "comments": "35 pages; 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an \\verb|R| package, called \\verb|MPS|, for computing the\nprobability density function, computing the cumulative distribution function,\ncomputing the quantile function, simulating random variables, and estimating\nthe parameters of 24 new shifted families of distributions. By considering an\nextra shift (location) parameter for each family more flexibility yields. Under\nsome situations, since the maximum likelihood estimators may fail to exist, we\nadopt the well-known maximum product spacings approach to estimate the\nparameters of shifted 24 new families of distributions. The performance of the\n\\verb|MPS| package for computing the cdf, pdf, and simulating random samples\nwill be checked by examples. The performance of the maximum product spacings\napproach is demonstrated by executing \\verb|MPS| package for three sets of real\ndata. As it will be shown, for the first set, the maximum likelihood estimators\nbreak down but \\verb|MPS| package find them. For the second set, adding the\nlocation parameter leads to acceptance the model while absence of the location\nparameter makes the model quite inappropriate. For the third set, presence of\nthe location parameter yields a better fit.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 11:51:55 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Teimouri", "Mahdi", ""]]}, {"id": "1809.02977", "submitter": "Alessandro Casa", "authors": "Alessandro Casa and Giovanna Menardi", "title": "Nonparametric semisupervised classification for signal detection in high\n  energy physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-independent searches in particle physics aim at completing our\nknowledge of the universe by looking for new possible particles not predicted\nby the current theories. Such particles, referred to as signal, are expected to\nbehave as a deviation from the background, representing the known physics.\nInformation available on the background can be incorporated in the search, in\norder to identify potential anomalies. From a statistical perspective, the\nproblem is recasted to a peculiar classification one where only partial\ninformation is accessible. Therefore a semisupervised approach shall be\nadopted, either by strengthening or by relaxing assumptions underlying\nclustering or classification methods respectively. In this work, following the\nfirst route, we semisupervise nonparametric clustering in order to identify a\npossible signal. The main contribution consists in tuning a nonparametric\nestimate of the density underlying the experimental data with the aid of the\navailable information on the physical theory. As a side contribution, a\nvariable selection procedure is presented. The whole procedure is tested on a\ndataset mimicking proton-proton collisions performed within a particle\naccelerator. While finding motivation in the field of particle physics, the\napproach is applicable to various science domains, where similar problems of\nanomaly detection arise.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 14:10:17 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 07:07:20 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Casa", "Alessandro", ""], ["Menardi", "Giovanna", ""]]}, {"id": "1809.03127", "submitter": "Philipp Wittenberg", "authors": "Tahir Mahmood, Philipp Wittenberg, Inez Maria Zwetsloot, Hailiang\n  Wang, Kwok Leung Tsui", "title": "Monitoring data quality for telehealth systems in the presence of\n  missing data", "comments": "13 pages, 5 figures", "journal-ref": "Int. J. Med. Inform. 126 (2019) 156-163", "doi": "10.1016/j.ijmedinf.2019.03.011", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: All-in-one station-based health monitoring devices are\nimplemented in elder homes in Hong Kong to support the monitoring of vital\nsigns of the elderly. During a pilot study, it was discovered that the systolic\nblood pressure was incorrectly measured during multiple weeks. A real-time\nsolution was needed to identify future data quality issues as soon as possible.\n  Methods: Control charts are an effective tool for real-time monitoring and\nsignaling issues (changes) in data. In this study, as in other healthcare\napplications, many observations are missing. Few methods are available for\nmonitoring data with missing observations. A data quality monitoring method is\ndeveloped to signal issues with the accuracy of the collected data quickly.\nThis method has the ability to deal with missing observations. A Hotelling's\nT-squared control chart is selected as the basis for our proposed method.\n  Findings: The proposed method is retrospectively validated on a case study\nwith a known measurement error in the systolic blood pressure measurements. The\nmethod is able to adequately detect this data quality problem. The proposed\nmethod was integrated into a personalized telehealth monitoring system and\nprospectively implemented in a second case study. It was found that the\nproposed scheme supports the control of data quality.\n  Conclusions: Data quality is an important issue and control charts are useful\nfor real-time monitoring of data quality. However, these charts must be\nadjusted to account for missing data that often occur in healthcare context.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 04:28:42 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:14:43 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Mahmood", "Tahir", ""], ["Wittenberg", "Philipp", ""], ["Zwetsloot", "Inez Maria", ""], ["Wang", "Hailiang", ""], ["Tsui", "Kwok Leung", ""]]}, {"id": "1809.03279", "submitter": "Odile Sauzet", "authors": "Odile Sauzet, Jannik Rehse, Janne Helene Breiding", "title": "DistdichoR a R Package for the distributional dichotomisation of\n  continuous outcomes", "comments": "20 Pages, two Figures, Documentation for R package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functions included in the R Packet distdichoR for the\nimplementation of the distributional method for the dichotomisation of\ncontinuous outcomes. While recalling the principle of the method for the\ndistributions for which the method has already been developed, we add a new\ndistribution - the gamma distribution - to the range of distribution available.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:53:39 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 10:32:45 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Sauzet", "Odile", ""], ["Rehse", "Jannik", ""], ["Breiding", "Janne Helene", ""]]}, {"id": "1809.03326", "submitter": "Gwang-Il Ri", "authors": "Gwang-Il Ri, Mun-Chol Kim, Su-Rim Ji", "title": "A Stable Minutia Descriptor based on Gabor Wavelet and Linear\n  Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The minutia descriptor which describes characteristics of minutia, plays a\nmajor role in fingerprint recognition. Typically, fingerprint recognition\nsystems employ minutia descriptors to find potential correspondence between\nminutiae, and they use similarity between two minutia descriptors to calculate\noverall similarity between two fingerprint images. A good minutia descriptor\ncan improve recognition accuracy of fingerprint recognition system and largely\nreduce comparing time. A good minutia descriptor should have high ability to\ndistinguish between different minutiae and at the same time should be robust in\ndifficult conditions including poor quality image and small size image. It also\nshould be effective in computational cost of similarity among descriptors. In\nthis paper, a robust minutia descriptor is constructed using Gabor wavelet and\nlinear discriminant analysis. This minutia descriptor has high distinguishing\nability, stability and simple comparing method. Experimental results on FVC2004\nand FVC2006 databases show that the proposed minutia descriptor is very\neffective in fingerprint recognition.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 10:46:19 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ri", "Gwang-Il", ""], ["Kim", "Mun-Chol", ""], ["Ji", "Su-Rim", ""]]}, {"id": "1809.03395", "submitter": "Fuad Noman", "authors": "Fuad Noman, Sh-Hussain Salleh, Chee-Ming Ting, S. Balqis Samdin,\n  Hernando Ombao, Hadri Hussain", "title": "A Markov-Switching Model Approach to Heart Sound Segmentation and\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2019.2925036", "report-no": null, "categories": "eess.SP cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This paper considers challenges in developing algorithms for\naccurate segmentation and classification of heart sound (HS) signals. Methods:\nWe propose an approach based on Markov switching autoregressive model (MSAR) to\nsegmenting the HS into four fundamental components each with distinct\nsecond-order structure. The identified boundaries are then utilized for\nautomated classification of pathological HS using the continuous density hidden\nMarkov model (CD-HMM). The MSAR formulated in a state-space form is able to\ncapture simultaneously both the continuous hidden dynamics in HS, and the\nregime switching in the dynamics using a discrete Markov chain. This overcomes\nthe limitation of HMM which uses a single-layer of discrete states. We\nintroduce three schemes for model estimation: (1.) switching Kalman filter\n(SKF); (2.) refined SKF; (3.) fusion of SKF and the duration-dependent Viterbi\nalgorithm (SKF-Viterbi). Results: The proposed methods are evaluated on\nPhysionet/CinC Challenge 2016 database. The SKF-Viterbi significantly\noutperforms SKF by improvement of segmentation accuracy from 71% to 84.2%. The\nuse of CD-HMM as a classifier and Mel-frequency cepstral coefficients (MFCCs)\nas features can characterize not only the normal and abnormal morphologies of\nHS signals but also morphologies considered as unclassifiable (denoted as\nX-Factor). It gives classification rates with best gross F1 score of 90.19\n(without X-Factor) and 82.7 (with X-Factor) for abnormal beats. Conclusion: The\nproposed MSAR approach for automatic localization and detection of pathological\nHS shows a noticeable performance on large HS dataset. Significance: It has\npotential applications in heart monitoring systems to assist cardiologists for\npre-screening of heart pathologies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:28:34 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Noman", "Fuad", ""], ["Salleh", "Sh-Hussain", ""], ["Ting", "Chee-Ming", ""], ["Samdin", "S. Balqis", ""], ["Ombao", "Hernando", ""], ["Hussain", "Hadri", ""]]}, {"id": "1809.03561", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic\n  Load Forecasting", "comments": "accepted for International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1400-1408", "doi": "10.1016/j.ijforecast.2018.07.004", "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple quantile regression-based forecasting method that was\napplied in a probabilistic load forecasting framework of the Global Energy\nForecasting Competition 2017 (GEFCom2017). The hourly load data is log\ntransformed and split into a long-term trend component and a remainder term.\nThe key forecasting element is the quantile regression approach for the\nremainder term that takes into account weekly and annual seasonalities such as\ntheir interactions. Temperature information is only used to stabilize the\nforecast of the long-term trend component. Public holidays information is\nignored. Still, the forecasting method placed second in the open data track and\nfourth in the definite data track with our forecasting method, which is\nremarkable given simplicity of the model. The method also outperforms the\nVanilla benchmark consistently.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:31:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1809.03574", "submitter": "Bismark Singh", "authors": "Bismark Singh and David Pozo", "title": "A Guide to Solar Power Forecasting using ARMA Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple and succinct methodology to develop hourly\nauto-regressive moving average (ARMA) models to forecast power output from a\nphotovoltaic solar generator. We illustrate how to build an ARMA model, to use\nstatistical tests to validate it, and construct hourly samples. The resulting\nmodel inherits nice properties for embedding it into more sophisticated\noperation and planning models, while at the same time showing relatively good\naccuracy. Additionally, it represents a good forecasting tool for sample\ngeneration for stochastic energy optimization models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 20:08:34 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Singh", "Bismark", ""], ["Pozo", "David", ""]]}, {"id": "1809.03593", "submitter": "Sarah Elizabeth Heaps", "authors": "Sarah E. Heaps, Malcolm Farrow and Kevin J. Wilson", "title": "Identifying the effect of public holidays on daily demand for gas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce operational costs, gas distribution networks require accurate\nforecasts of the demand for gas. Amongst domestic and commercial customers,\ndemand relates primarily to the weather and patterns of life and work. Public\nholidays have a pronounced effect which often spreads into neighbouring days.\nWe call this spread the \"proximity effect\". Traditionally, the days over which\nthe proximity effect is felt are pre-specified in fixed windows around each\nholiday, allowing no uncertainty in their identification. We are motivated by\nan application to modelling daily gas demand in two large British regions. We\nintroduce a novel model which does not fix the days on which the proximity\neffect is felt. Our approach uses a four-state, non-homogeneous hidden Markov\nmodel, with cyclic dynamics, where the classification of days as public\nholidays is observed, but the assignment of days as \"pre-holiday\",\n\"post-holiday\" or \"normal\" is unknown. The number of days to the preceding and\nsucceeding holidays guide transitions between states. We apply Bayesian\ninference and illustrate the benefit of our modelling approach. A preliminary\nversion of the model is now being used by one of the UK's regional distribution\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 20:55:35 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 10:48:42 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Heaps", "Sarah E.", ""], ["Farrow", "Malcolm", ""], ["Wilson", "Kevin J.", ""]]}, {"id": "1809.03735", "submitter": "Sebastian Meyer", "authors": "Leonhard Held and Sebastian Meyer", "title": "Forecasting Based on Surveillance Data", "comments": "This is an author-created preprint of a book chapter to appear in the\n  Handbook of Infectious Disease Data Analysis edited by Leonhard Held, Niel\n  Hens, Philip D O'Neill and Jacco Wallinga, Chapman and Hall/CRC, 2019. 19\n  pages, including 9 figures and 4 tables; supplementary R package\n  'HIDDA.forecasting' available https://HIDDA.github.io/forecasting/", "journal-ref": "Handbook of Infectious Disease Data Analysis; Chapman & Hall/CRC,\n  2019; Chapter 25", "doi": "10.1201/9781315222912-25", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the future course of epidemics has always been one of the main\ngoals of epidemic modelling. This chapter reviews statistical methods to\nquantify the accuracy of epidemic forecasts. We distinguish point and\nprobabilistic forecasts and describe different methods to evaluate and compare\nthe predictive performance across models. Two case studies demonstrate how to\napply the different techniques to uni- and multivariate forecasts. We focus on\nforecasting count time series from routine public health surveillance: weekly\ncounts of influenza-like illness in Switzerland, and age-stratified counts of\nnorovirus gastroenteritis in Berlin, Germany. Data and code for all analyses\nare available in a supplementary R package.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:27:42 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Held", "Leonhard", ""], ["Meyer", "Sebastian", ""]]}, {"id": "1809.03741", "submitter": "Baldur Magnusson", "authors": "Baldur P. Magnusson, Heinz Schmidli, Nicolas Rouyrre, Daniel O.\n  Scharfstein", "title": "Bayesian inference for a principal stratum estimand to assess the\n  treatment effect in a subgroup characterized by post-randomization events", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment effect in a specific subgroup is often of interest in\nrandomized clinical trials. When the subgroup is characterized by the absence\nof certain post-randomization events, a naive analysis on the subset of\npatients without these events may be misleading. The principal stratification\nframework allows one to define an appropriate causal estimand in such settings.\nStatistical inference for the principal stratum estimand hinges on\nscientifically justified assumptions, which can be included with Bayesian\nmethods through prior distributions. Our motivating example is a large\nrandomized placebo-controlled trial of siponimod in patients with secondary\nprogressive multiple sclerosis. The primary objective of this trial was to\ndemonstrate the efficacy of siponimod relative to placebo in delaying\ndisability progression for the whole study population. However, the treatment\neffect in the subgroup of patients who would not relapse during the trial is\nrelevant from both a scientific and regulatory perspective. Assessing this\nsubgroup treatment effect is challenging as there is strong evidence that\nsiponimod reduces relapses. Aligned with the draft regulatory guidance ICH\nE9(R1), we describe in detail the scientific question of interest, the\nprincipal stratum estimand, the corresponding analysis method for binary\nendpoints and sensitivity analyses.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:37:42 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Magnusson", "Baldur P.", ""], ["Schmidli", "Heinz", ""], ["Rouyrre", "Nicolas", ""], ["Scharfstein", "Daniel O.", ""]]}, {"id": "1809.03878", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Hyekyoung Lee, Andrey Gritsenko, Alex DiChristofano,\n  Dustin Pluta, Hernando Ombao, Victor Solo", "title": "Topological Brain Network Distances", "comments": "arXiv admin note: text overlap with arXiv:1701.04171", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing brain network distances are often based on matrix norms. The\nelement-wise differences in the existing matrix norms may fail to capture\nunderlying topological differences. Further, matrix norms are sensitive to\noutliers. A major disadvantage to element-wise distance calculations is that it\ncould be severely affected even by a small number of extreme edge weights. Thus\nit is necessary to develop network distances that recognize topology. In this\npaper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and\nKolmogorov-Smirnov (KS) distances that are adapted for brain networks, and\ncompare them against matrix-norm based network distances. Bottleneck and\nGH-distances are often used in persistent homology. However, they were rarely\nutilized to measure similarity between brain networks. KS-distance is recently\nintroduced to measure the similarity between networks across different\nfiltration values. The performance analysis was conducted using the random\nnetwork simulations with the ground truths. Using a twin imaging study, which\nprovides biological ground truth, we demonstrate that the KS distance has the\nability to determine heritability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 10:47:58 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Chung", "Moo K.", ""], ["Lee", "Hyekyoung", ""], ["Gritsenko", "Andrey", ""], ["DiChristofano", "Alex", ""], ["Pluta", "Dustin", ""], ["Ombao", "Hernando", ""], ["Solo", "Victor", ""]]}, {"id": "1809.03897", "submitter": "Divine Wanduku (Dr. )", "authors": "Divine Wanduku", "title": "A comparative stochastic and deterministic study of a class of epidemic\n  dynamic models for malaria: exploring the impacts of noise on eradication and\n  persistence of disease", "comments": "arXiv admin note: substantial text overlap with arXiv:1808.09842,\n  arXiv:1809.03866", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A comparative stochastic and deterministic study of a family of SEIRS\nepidemic dynamic models for malaria is presented. The family type is determined\nby the qualitative behavior of the nonlinear incidence rates of the disease.\nFurthermore, the malaria models exhibit three random delays:- two of the delays\nrepresent the incubation periods of the disease inside the vector and human\nhosts, whereas the third delay is the period of effective natural immunity\nagainst the disease. The stochastic malaria models are improved by including\nthe random environmental fluctuations in the disease transmission and natural\ndeath rates of humans. Insights about the effects of the delays and the noises\non the malaria dynamics are gained via comparative analyses of the family of\nstochastic and deterministic models, and further critical examination of the\nsignificance of the intensities of the white noises in the system on (1) the\nexistence and stability of the equilibria, and also on (2) the eradication and\npersistence of malaria in the human population. The basic reproduction numbers\nand other threshold values for malaria in the stochastic and deterministic\nsettings are determined and compared for the cases of constant or random delays\nin the system. Numerical simulation results are presented.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:19:17 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wanduku", "Divine", ""]]}, {"id": "1809.03910", "submitter": "Madeline Ausdemore", "authors": "Madeline Ausdemore, Jessie H. Hendricks, and Cedric Neumann", "title": "Review of several false positive error rate estimates for latent\n  fingerprint examination proposed based on the 2014 Miami Dade Police\n  Department study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several studies have been conducted to estimate the\nfalse positive error rate (FPR) associated with latent fingerprint examination.\nThe so-called Black-box study by Ulery et al. is regularly used to support the\nclaim that the FPR in fingerprint examination is reasonably low (0.1%). The\nUlery et al.'s estimate of the FPR is supported by the results of the extensive\nstudy of the overall fingerprint examination process by Langenburg. In 2014,\nthe Miami Dade Police Department (MDPD) Forensic Services Bureau conducted\nresearch to study the false positive error rate associated with latent\nfingerprint examination. They report that approximately 3.0% of latent\nfingerprint examinations result in a false positive conclusion. Their estimate\nof the FPR becomes as high as 4.2% when inconclusive decisions are excluded\nfrom the calculation. In their 2016 report, the President's Council of Advisors\non Science and Technology (PCAST) proposes that the MDPD FPR estimate be used\nto inform jurors that errors occur at a detectable rate in fingerprint\nexamination; more specifically, they declare that false positives may occur as\noften as 1 in 18 cases. The large discrepancy between the FPR estimates\nreported by Ulery et al. and Langenburg on the one hand, and the MDPD on the\nother hand, causes a great deal of controversy. In this paper, we review the\nMDPD study and the various error rate calculations that have been proposed to\ninterpret its data. To assess the appropriateness of the different proposed\nestimates, we develop a model that re-creates the MDPD study. This model allows\nus to estimate the expected number of false positive conclusions that should be\nobtained with any proposed FPR and compare this number to the actual number of\nerroneous identifications observed by MDPD.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:09:21 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 16:10:12 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Ausdemore", "Madeline", ""], ["Hendricks", "Jessie H.", ""], ["Neumann", "Cedric", ""]]}, {"id": "1809.03935", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Kengo Nagashima and Toshi A. Furukawa", "title": "Permutation inference methods for multivariate meta-analysis", "comments": "20 pages, 2 figures, 2 table", "journal-ref": "Biometrics 2020;76(1):337-347", "doi": "10.1111/biom.13134", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate meta-analysis is gaining prominence in evidence synthesis\nresearch because it enables simultaneous synthesis of multiple correlated\noutcome data, and random-effects models have generally been used for addressing\nbetween-studies heterogeneities. However, coverage probabilities of confidence\nregions or intervals for standard inference methods for random-effects models\n(e.g., restricted maximum likelihood estimation) cannot retain their nominal\nconfidence levels in general, especially when the number of synthesized studies\nis small because their validities depend on large sample approximations. In\nthis article, we provide permutation-based inference methods that enable exact\njoint inferences for average outcome measures without large sample\napproximations. We also provide accurate marginal inference methods under\ngeneral settings of multivariate meta-analyses. We propose effective approaches\nfor permutation inferences using optimal weighting based on the efficient score\nstatistic. The effectiveness of the proposed methods is illustrated via\napplications to bivariate meta-analyses of diagnostic accuracy studies for\nairway eosinophilia in asthma and a network meta-analysis for antihypertensive\ndrugs on incident diabetes, as well as through simulation experiments. In\nnumerical evaluations performed via simulations, our methods generally provided\naccurate confidence regions or intervals under a broad range of settings,\nwhereas the current standard inference methods exhibited serious undercoverage\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:43:49 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 13:29:55 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 07:59:27 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 15:56:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Noma", "Hisashi", ""], ["Nagashima", "Kengo", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1809.03964", "submitter": "Hao Wang", "authors": "Hao Wang, Bojin Zhuang and Yang Chen, Ni Li, Dongxia Wei", "title": "Deep Inferential Spatial-Temporal Network for Forecasting Air Pollution\n  Concentrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution poses a serious threat to human health as well as economic\ndevelopment around the world. To meet the increasing demand for accurate\npredictions for air pollutions, we proposed a Deep Inferential Spatial-Temporal\nNetwork to deal with the complicated non-linear spatial and temporal\ncorrelations. We forecast three air pollutants (i.e., PM2.5, PM10 and O3) of\nmonitoring stations over the next 48 hours, using a hybrid deep learning model\nconsists of inferential predictor (inference for regions without air pollution\nreadings), spatial predictor (capturing spatial correlations using CNN) and\ntemporal predictor (capturing temporal relationship using sequence-to-sequence\nmodel with simplified attention mechanism). Our proposed model considers\nhistorical air pollution records and historical meteorological data. We\nevaluate our model on a large-scale dataset containing air pollution records of\n35 monitoring stations and grid meteorological data in Beijing, China. Our\nmodel outperforms other state-of-art methods in terms of SMAPE and RMSE.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:15:04 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wang", "Hao", ""], ["Zhuang", "Bojin", ""], ["Chen", "Yang", ""], ["Li", "Ni", ""], ["Wei", "Dongxia", ""]]}, {"id": "1809.04000", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Stephan Hemri and Mehrez El Ayari", "title": "Statistical post-processing of hydrological forecasts using Bayesian\n  model averaging", "comments": "19 pages, 6 figures", "journal-ref": "Water Resources Research 55 (2019), no. 5, 3997-4013", "doi": "10.1029/2018WR024028", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable probabilistic forecasts of hydrological quantities like\nrunoff or water level are beneficial to various areas of society. Probabilistic\nstate-of-the-art hydrological ensemble prediction models are usually driven\nwith meteorological ensemble forecasts. Hence, biases and dispersion errors of\nthe meteorological forecasts cascade down to the hydrological predictions and\nadd to the errors of the hydrological models. The systematic parts of these\nerrors can be reduced by applying statistical post-processing. For a sound\nestimation of predictive uncertainty and an optimal correction of systematic\nerrors, statistical post-processing methods should be tailored to the\nparticular forecast variable at hand. Former studies have shown that it can\nmake sense to treat hydrological quantities as bounded variables. In this\npaper, a doubly truncated Bayesian model averaging (BMA) method, which allows\nfor flexible post-processing of (multi-model) ensemble forecasts of water\nlevel, is introduced. A case study based on water level for a gauge of river\nRhine, reveals a good predictive skill of doubly truncated BMA compared both to\nthe raw ensemble and the reference ensemble model output statistics approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:06:16 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Hemri", "Stephan", ""], ["Ayari", "Mehrez El", ""]]}, {"id": "1809.04035", "submitter": "Jaehyuk Choi", "authors": "Jaehyuk Choi, Chenru Liu, Byoung Ki Seo", "title": "Hyperbolic normal stochastic volatility model", "comments": "26 pages, 4 figures, 5 tables", "journal-ref": "Journal of Futures Markets, 39(2):186-204, 2019", "doi": "10.1002/fut.21967", "report-no": null, "categories": "q-fin.MF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For option pricing models and heavy-tailed distributions, this study proposes\na continuous-time stochastic volatility model based on an arithmetic Brownian\nmotion: a one-parameter extension of the normal stochastic alpha-beta-rho\n(SABR) model. Using two generalized Bougerol's identities in the literature,\nthe study shows that our model has a closed-form Monte-Carlo simulation scheme\nand that the transition probability for one special case follows Johnson's\n$S_U$ distribution---a popular heavy-tailed distribution originally proposed\nwithout stochastic process. It is argued that the $S_U$ distribution serves as\nan analytically superior alternative to the normal SABR model because the two\ndistributions are empirically similar.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:04:57 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Choi", "Jaehyuk", ""], ["Liu", "Chenru", ""], ["Seo", "Byoung Ki", ""]]}, {"id": "1809.04042", "submitter": "S\\'andor Baran", "authors": "Mailiu D\\'iaz, Orietta Nicolis, Julio C\\'esar Mar\\'in and S\\'andor\n  Baran", "title": "Statistical post-processing of ensemble forecasts of temperature in\n  Santiago de Chile", "comments": "19 pages, 7 figures, 4 tables", "journal-ref": "Meteorological Applications 27 (2020), no. 1, e1818", "doi": "10.1002/met.1818", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently all major meteorological centres generate ensemble forecasts using\ntheir operational ensemble prediction systems; however, it is a general problem\nthat the spread of the ensemble is too small, resulting in underdispersive\nforecasts, leading to a lack of calibration. In order to correct this problem,\ndifferent statistical calibration techniques have been developed in the last\ntwo decades. In the present work different post-processing techniques are\ntested for calibrating 9 member ensemble forecast of temperature for Santiago\nde Chile, obtained by the Weather Research and Forecasting (WRF) model using\ndifferent planetary boundary layer and land surface model parametrization. In\nparticular, the ensemble model output statistics (EMOS) and Bayesian model\naveraging techniques are implemented and since the observations are\ncharacterized by large altitude differences, the estimation of model parameters\nis adapted to the actual conditions at hand. Compared to the raw ensemble all\ntested post-processing approaches significantly improve the calibration of\nprobabilistic and the accuracy of point forecasts. The EMOS method using\nparameter estimation based on expert clustering of stations (according to their\naltitudes) shows the best forecast skill.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:18:14 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["D\u00edaz", "Mailiu", ""], ["Nicolis", "Orietta", ""], ["Mar\u00edn", "Julio C\u00e9sar", ""], ["Baran", "S\u00e1ndor", ""]]}, {"id": "1809.04131", "submitter": "Felipe Elorrieta", "authors": "Susana Eyheramendy, Felipe Elorrieta, Wilfredo Palma", "title": "An irregular discrete time series model to identify residuals with\n  autocorrelation in astronomical light curves", "comments": "14 pages, 7 figures, 6 tables; Monthly Notices of the Royal\n  Astronomical Society (MNRAS), in press", "journal-ref": null, "doi": "10.1093/mnras/sty2487", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series observations are ubiquitous in astronomy, and are generated to\ndistinguish between different types of supernovae, to detect and characterize\nextrasolar planets and to classify variable stars. These time series are\nusually modeled using a parametric and/or physical model that assumes\nindependent and homoscedastic errors, but in many cases these assumptions are\nnot accurate and there remains a temporal dependency structure on the errors.\nThis can occur, for example, when the proposed model cannot explain all the\nvariability of the data or when the parameters of the model are not properly\nestimated. In this work we define an autoregressive model for irregular\ndiscrete-time series, based on the discrete time representation of the\ncontinuous autoregressive model of order 1. We show that the model is ergodic\nand stationary. We further propose a maximum likelihood estimation procedure\nand assess the finite sample performance by Monte Carlo simulations. We\nimplement the model on real and simulated data from Gaussian as well as other\ndistributions, showing that the model can flexibly adapt to different data\ndistributions. We apply the irregular autoregressive model to the residuals of\na transit of an extrasolar planet to illustrate errors that remain with\ntemporal structure. We also apply this model to residuals of an harmonic fit of\nlight-curves from variable stars to illustrate how the model can be used to\ndetect incorrect parameter estimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:57:18 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Eyheramendy", "Susana", ""], ["Elorrieta", "Felipe", ""], ["Palma", "Wilfredo", ""]]}, {"id": "1809.04141", "submitter": "Benjamin Campbell", "authors": "Benjamin Campbell, Skyler Cranmer, Bruce Desmarais", "title": "Triangulating War: Network Structure and the Democratic Peace", "comments": "38 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decades of research has found that democratic dyads rarely exhibit violent\ntendencies, making the democratic peace arguably the principal finding of Peace\nScience. However, the democratic peace rests upon a dyadic understanding of\nconflict. Conflict rarely reflects a purely dyadic phenomena---even if a\nconflict is not multi-party, multiple states may be engaged in distinct\ndisputes with the same enemy. We postulate a network theory of conflict that\ntreats the democratic peace as a function of the competing interests of\nmixed-regime dyads and the strategic inefficiencies of fighting with enemies'\nenemies. Specifically, we find that a state's decision to engage in conflict\nwith a target state is conditioned by the other states in which the target\nstate is in conflict. When accounting for this network effect, we are unable to\nfind support for the democratic peace. This suggests that the major finding of\nthree decades worth of conflict research is spurious.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:25:01 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Campbell", "Benjamin", ""], ["Cranmer", "Skyler", ""], ["Desmarais", "Bruce", ""]]}, {"id": "1809.04235", "submitter": "Kellie Ottoboni", "authors": "Kellie Ottoboni, Philip B. Stark, Mark Lindeman and Neal McBurnett", "title": "Risk-Limiting Audits by Stratified Union-Intersection Tests of Elections\n  (SUITE)", "comments": "This article draws heavily from arXiv:1803.00698", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Risk-limiting audits (RLAs) offer a statistical guarantee: if a full manual\ntally of the paper ballots would show that the reported election outcome is\nwrong, an RLA has a known minimum chance of leading to a full manual tally.\nRLAs generally rely on random samples. Stratified sampling--partitioning the\npopulation of ballots into disjoint strata and sampling independently from the\nstrata--may simplify logistics or increase efficiency compared to simpler\nsampling designs, but makes risk calculations harder. We present SUITE, a new\nmethod for conducting RLAs using stratified samples. SUITE considers all\npossible partitions of outcome-changing error across strata. For each\npartition, it combines P-values from stratum-level tests into a combined\nP-value; there is no restriction on the tests used in different strata. SUITE\nmaximizes the combined P-value over all partitions of outcome-changing error.\nThe audit can stop if that maximum is less than the risk limit. Voting systems\nin some Colorado counties (comprising 98.2% of voters) allow auditors to check\nhow the system interpreted each ballot, which allows ballot-level comparison\nRLAs. Other counties use ballot polling, which is less efficient. Extant\napproaches to conducting an RLA of a statewide contest would require major\nchanges to Colorado's procedures and software, or would sacrifice the\nefficiency of ballot-level comparison. SUITE does not. It divides ballots into\ntwo strata: those cast in counties that can conduct ballot-level comparisons,\nand the rest. Stratum-level P-values are found by methods derived here. The\nresulting audit is substantially more efficient than statewide ballot polling.\nSUITE is useful in any state with a mix of voting systems or that uses\nstratified sampling for other reasons. We provide an open-source reference\nimplementation and exemplar calculations in Jupyter notebooks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 02:56:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Stark", "Philip B.", ""], ["Lindeman", "Mark", ""], ["McBurnett", "Neal", ""]]}, {"id": "1809.04264", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra and Maxim Finkelstein", "title": "Comparing Lifetimes of Coherent Systems with Dependent Components\n  Operating in Random Environments", "comments": "NO", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an impact of a random environment on lifetimes of coherent systems\nwith dependent components. There are two combined sources of this dependence.\nOne results from the dependence of the components of the coherent system\noperating in a deterministic environment and the other is due to dependence of\ncomponents of the system sharing the same random environment. We provide\ndifferent sets of sufficient conditions for the corresponding stochastic\ncomparisons and consider various scenarios, namely, (i) two different coherent\nsystems operate under the same random environment; (ii) two coherent systems\noperate under two different random environments; (iii) one of the coherent\nsystems operates under a random environment, whereas the other under a\ndeterministic one. Some examples are given to illustrate the proposed\nreasoning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 05:55:12 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 11:00:30 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Finkelstein", "Maxim", ""]]}, {"id": "1809.04389", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang", "title": "Spatio-Temporal Data Fusion for Massive Sea Surface Temperature Data\n  from MODIS and AMSR-E Instruments", "comments": "Accepted in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing data have been widely used to study various geophysical\nprocesses. With the advances in remote-sensing technology, massive amount of\nremote sensing data are collected in space over time. Different satellite\ninstruments typically have different footprints, measurement-error\ncharacteristics, and data coverages. To combine datasets from different\nsatellite instruments, we propose a dynamic fused Gaussian process (DFGP) model\nthat enables fast statistical inference such as filtering and smoothing for\nmassive spatio-temporal datasets in a data-fusion context. Based upon a\nspatio-temporal-random-effects model, the DFGP methodology represents the\nunderlying true process with two components: a linear combination of a small\nnumber of basis functions and random coefficients with a general covariance\nmatrix, together with a linear combination of a large number of basis functions\nand Markov random coefficients. To model the underlying geophysical process at\ndifferent spatial resolutions, we rely on the change-of-support property, which\nalso allows efficient computations in the DFGP model. To estimate model\nparameters, we devise a computationally efficient stochastic\nexpectation-maximization (SEM) algorithm to ensure its scalability for massive\ndatasets. The DFGP model is applied to a total of 3.7 million sea surface\ntemperature datasets in the tropical Pacific Ocean for a one-week time period\nin 2010 from MODIS and AMSR-E instruments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 12:54:06 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 17:01:34 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""]]}, {"id": "1809.04407", "submitter": "Burak K\\\"ursad G\\\"unhan", "authors": "Burak K\\\"ursad G\\\"unhan (1), Christian R\\\"over (1), Tim Friede (1)\n  ((1) Department of Medical Statistics, University Medical Center G\\\"ottingen)", "title": "Meta-analysis of few studies involving rare events", "comments": null, "journal-ref": "Research Synthesis Methods 2020; 11: 74-90", "doi": "10.1002/jrsm.1370", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analyses of clinical trials targeting rare events face particular\nchallenges when the data lack adequate numbers of events for all treatment\narms. Especially when the number of studies is low, standard meta-analysis\nmethods can lead to serious distortions because of such data sparsity. To\novercome this, we suggest the use of weakly informative priors (WIP) for the\ntreatment effect parameter of a Bayesian meta-analysis model, which may also be\nseen as a form of penalization. As a data model, we use a binomial-normal\nhierarchical model (BNHM) which does not require continuity corrections in case\nof zero counts in one or both arms. We suggest a normal prior for the log odds\nratio with mean 0 and standard deviation 2.82, which is motivated (1) as a\nsymmetric prior centred around unity and constraining the odds ratio to within\na range from 1/250 to 250 with 95 % probability, and (2) as consistent with\nempirically observed effect estimates from a set of $\\mbox{$37\\,773$}$\nmeta-analyses from the Cochrane Database of Systematic Reviews. In a simulation\nstudy with rare events and few studies, our BNHM with a WIP outperformed a\nBayesian method without a WIP and a maximum likelihood estimator in terms of\nsmaller bias and shorter interval estimates with similar coverage. Furthermore,\nthe methods are illustrated by a systematic review in immunosuppression of rare\nsafety events following paediatric transplantation. A publicly available\n$\\textbf{R}$ package, $\\texttt{MetaStan}$, is developed to automate the\n$\\textbf{Stan}$ implementation of meta-analysis models using WIPs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:18:25 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["G\u00fcnhan", "Burak K\u00fcrsad", "", "Department of Medical Statistics, University Medical Center G\u00f6ttingen"], ["R\u00f6ver", "Christian", "", "Department of Medical Statistics, University Medical Center G\u00f6ttingen"], ["Friede", "Tim", "", "Department of Medical Statistics, University Medical Center G\u00f6ttingen"]]}, {"id": "1809.04587", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Stefano Marano", "title": "Distributed Chernoff Test: Optimal decision systems over networks", "comments": "A part of this work has been accepted in ISIT 2018 and CDC 2018;\n  Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.MA math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \"active\" decision making over sensor networks where the sensors'\nsequential probing actions are actively chosen by continuously learning from\npast observations. We consider two network settings: with and without central\ncoordination. In the first case, the network nodes interact with each other\nthrough a central entity, which plays the role of a fusion center. In the\nsecond case, the network nodes interact in a fully distributed fashion. In both\nof these scenarios, we propose sequential and adaptive hypothesis tests\nextending the classic Chernoff test. We compare the performance of the proposed\ntests to the optimal sequential test. In the presence of a fusion center, our\ntest achieves the same asymptotic optimality of the Chernoff test, minimizing\nthe risk, expressed by the expected cost required to reach a decision plus the\nexpected cost of making a wrong decision, when the observation cost per unit\ntime tends to zero. The test is also asymptotically optimal in the higher\nmoments of the time required to reach a decision. Additionally, the test is\nparsimonious in terms of communications, and the expected number of channel\nuses per network node tends to a small constant. In the distributed setup, our\ntest achieves the same asymptotic optimality of Chernoff's test, up to a\nmultiplicative constant in terms of both risk and the higher moments of the\ndecision time. Additionally, the test is parsimonious in terms of\ncommunications in comparison to state-of-the-art schemes proposed in the\nliterature. The analysis of these tests is also extended to account for message\nquantization and communication over channels with random erasures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:51:30 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 14:31:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Marano", "Stefano", ""]]}, {"id": "1809.04684", "submitter": "Jiahao Chen", "authors": "Jiahao Chen", "title": "Fair lending needs explainable models for responsible recommendation", "comments": "4 pages, position paper accepted for FATREC 2018 conference at ACM\n  RecSys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The financial services industry has unique explainability and fairness\nchallenges arising from compliance and ethical considerations in credit\ndecisioning. These challenges complicate the use of model machine learning and\nartificial intelligence methods in business decision processes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:29:20 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Chen", "Jiahao", ""]]}, {"id": "1809.04885", "submitter": "Anneke Weide", "authors": "Anneke Cleopatra Weide and Andr\\'e Beauducel", "title": "Varimax rotation based on gradient projection needs between 10 and more\n  than 500 random start loading matrices for optimal performance", "comments": "19 pages, 8 figures, 2 tables, 4 figures in the Supplement", "journal-ref": null, "doi": "10.3389/fpsyg.2019.00645", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient projection rotation (GPR) is a promising method to rotate factor or\ncomponent loadings by different criteria. Since the conditions for optimal\nperformance of GPR-Varimax are widely unknown, this simulation study\ninvestigates GPR towards the Varimax criterion in principal component analysis.\nThe conditions of the simulation study comprise two sample sizes (n = 100, n =\n300), with orthogonal simple structure population models based on four numbers\nof components (3, 6, 9, 12), with- and without Kaiser-normalization, and six\nnumbers of random start loading matrices for GPR-Varimax rotation (1, 10, 50,\n100, 500, 1,000). GPR-Varimax rotation always performed better when at least 10\nrandom matrices were used for start loadings instead of the identity matrix.\nGPR-Varimax worked better for a small number of components, larger (n = 300) as\ncompared to smaller (n = 100) samples, and when loadings were Kaiser-normalized\nbefore rotation. To ensure optimal (stationary) performance of GPR-Varimax in\nrecovering orthogonal simple structure, we recommend using at least 10\niterations of start loading matrices for the rotation of up to three components\nand 50 iterations for up to six components. For up to nine components, rotation\nshould be based on a sample size of at least 300 cases, Kaiser-normalization,\nand more than 50 different start loading matrices. For more than nine\ncomponents, GPR-Varimax rotation should be based on at least 300 cases,\nKaiser-normalization, and at least 500 different start loading matrices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 10:59:10 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Weide", "Anneke Cleopatra", ""], ["Beauducel", "Andr\u00e9", ""]]}, {"id": "1809.04933", "submitter": "Alejandro Baldominos", "authors": "Alejandro Baldominos, Iv\\'an Blanco, Antonio Jos\\'e Moreno, Rub\\'en\n  Iturrarte, \\'Oscar Bern\\'ardez and Carlos Afonso", "title": "Identifying Real Estate Opportunities using Machine Learning", "comments": "24 pages, 13 figures, 5 tables", "journal-ref": "Baldominos, A.; Blanco, I.; Moreno, A.J.; Iturrarte, R.;\n  Bern\\'ardez, \\'O.; Afonso, C. Identifying Real Estate Opportunities Using\n  Machine Learning. Appl. Sci. 2018, 8, 2321", "doi": "10.3390/app8112321", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The real estate market is exposed to many fluctuations in prices because of\nexisting correlations with many variables, some of which cannot be controlled\nor might even be unknown. Housing prices can increase rapidly (or in some\ncases, also drop very fast), yet the numerous listings available online where\nhouses are sold or rented are not likely to be updated that often. In some\ncases, individuals interested in selling a house (or apartment) might include\nit in some online listing, and forget about updating the price. In other cases,\nsome individuals might be interested in deliberately setting a price below the\nmarket price in order to sell the home faster, for various reasons. In this\npaper, we aim at developing a machine learning application that identifies\nopportunities in the real estate market in real time, i.e., houses that are\nlisted with a price substantially below the market price. This program can be\nuseful for investors interested in the housing market. We have focused in a use\ncase considering real estate assets located in the Salamanca district in Madrid\n(Spain) and listed in the most relevant Spanish online site for home sales and\nrentals. The application is formally implemented as a regression problem that\ntries to estimate the market price of a house given features retrieved from\npublic online listings. For building this application, we have performed a\nfeature engineering stage in order to discover relevant features that allows\nfor attaining a high predictive performance. Several machine learning\nalgorithms have been tested, including regression trees, k-nearest neighbors,\nsupport vector machines and neural networks, identifying advantages and\nhandicaps of each of them.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 13:19:23 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 11:53:32 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Baldominos", "Alejandro", ""], ["Blanco", "Iv\u00e1n", ""], ["Moreno", "Antonio Jos\u00e9", ""], ["Iturrarte", "Rub\u00e9n", ""], ["Bern\u00e1rdez", "\u00d3scar", ""], ["Afonso", "Carlos", ""]]}, {"id": "1809.05075", "submitter": "Samuel W.K. Wong", "authors": "Katherine C. Kempfert, Samuel W.K. Wong", "title": "Where Does Haydn End and Mozart Begin? Composer Classification of String\n  Quartets", "comments": "30 pages with 11 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For centuries, the history and music of Joseph Franz Haydn and Wolfgang\nAmadeus Mozart have been compared by scholars. Recently, the growing field of\nmusic information retrieval (MIR) has offered quantitative analyses to\ncomplement traditional qualitative analyses of these composers. In this MIR\nstudy, we classify the composer of Haydn and Mozart string quartets based on\nthe content of their scores. Our contribution is an interpretable statistical\nand machine learning approach that provides high classification accuracies and\nmusical relevance. We develop novel global features that are automatically\ncomputed from symbolic data and informed by musicological Haydn-Mozart\ncomparative studies, particularly relating to the sonata form. Several of these\nproposed features are found to be important for distinguishing between Haydn\nand Mozart string quartets. Our Bayesian logistic regression model attains\nleave-one-out classification accuracies over 84%, higher than prior works and\nproviding interpretations that could aid in assessing musicological claims.\nOverall, our work can help expand the longstanding dialogue surrounding Haydn\nand Mozart and exemplify the benefit of interpretable machine learning in MIR,\nwith potential applications to music generation and classification of other\nclassical composers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:31:05 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:50:54 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 20:27:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kempfert", "Katherine C.", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1809.05173", "submitter": "Jan Van Haaren", "authors": "Bart Aalbers, Jan Van Haaren", "title": "Distinguishing Between Roles of Football Players in Play-by-play Match\n  Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, the player recruitment process in professional\nfootball has evolved into a multi-billion industry and has thus become of vital\nimportance. To gain insights into the general level of their candidate\nreinforcements, many professional football clubs have access to extensive video\nfootage and advanced statistics. However, the question whether a given player\nwould fit the team's playing style often still remains unanswered. In this\npaper, we aim to bridge that gap by proposing a set of 21 player roles and\nintroducing a method for automatically identifying the most applicable roles\nfor each player from play-by-play event data collected during matches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:43:40 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Aalbers", "Bart", ""], ["Van Haaren", "Jan", ""]]}, {"id": "1809.05651", "submitter": "Jongbin Jung", "authors": "Jongbin Jung and Sam Corbett-Davies and Ravi Shroff and Sharad Goel", "title": "Omitted and Included Variable Bias in Tests for Disparate Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policymakers often seek to gauge discrimination against groups defined by\nrace, gender, and other protected attributes. One popular strategy is to\nestimate disparities after controlling for observed covariates, typically with\na regression model. This approach, however, suffers from two statistical\nchallenges. First, omitted-variable bias can skew results if the model does not\ncontrol for all relevant factors; second, and conversely, included-variable\nbias can skew results if the set of controls includes irrelevant factors. Here\nwe introduce a simple three-step strategy---which we call risk-adjusted\nregression---that addresses both concerns in settings where decision makers\nhave clearly measurable objectives. In the first step, we use all available\ncovariates to estimate the utility of possible decisions. In the second step,\nwe measure disparities after controlling for these utility estimates alone,\nmitigating the problem of included-variable bias. Finally, in the third step,\nwe examine the sensitivity of results to unmeasured confounding, addressing\nconcerns about omitted-variable bias. We demonstrate this method on a detailed\ndataset of 2.2 million police stops of pedestrians in New York City, and show\nthat traditional statistical tests of discrimination can yield misleading\nresults. We conclude by discussing implications of our statistical approach for\nquestions of law and policy.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 05:12:49 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 23:37:51 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 05:01:04 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Jung", "Jongbin", ""], ["Corbett-Davies", "Sam", ""], ["Shroff", "Ravi", ""], ["Goel", "Sharad", ""]]}, {"id": "1809.05760", "submitter": "Haroldo Ribeiro", "authors": "Higor Y. D. Sigaki, Matjaz Perc, Haroldo V. Ribeiro", "title": "History of art paintings through the lens of entropy and complexity", "comments": "10 two-column pages, 5 figures; accepted for publication in PNAS\n  [supplementary information available at\n  http://www.pnas.org/highwire/filestream/824089/field_highwire_adjunct_files/0/pnas.1800083115.sapp.pdf]", "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 115, E8585-E8594 (2018)", "doi": "10.1073/pnas.1800083115", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Art is the ultimate expression of human creativity that is deeply influenced\nby the philosophy and culture of the corresponding historical epoch. The\nquantitative analysis of art is therefore essential for better understanding\nhuman cultural evolution. Here we present a large-scale quantitative analysis\nof almost 140 thousand paintings, spanning nearly a millennium of art history.\nBased on the local spatial patterns in the images of these paintings, we\nestimate the permutation entropy and the statistical complexity of each\npainting. These measures map the degree of visual order of artworks into a\nscale of order-disorder and simplicity-complexity that locally reflects\nqualitative categories proposed by art historians. The dynamical behavior of\nthese measures reveals a clear temporal evolution of art, marked by transitions\nthat agree with the main historical periods of art. Our research shows that\ndifferent artistic styles have a distinct average degree of entropy and\ncomplexity, thus allowing a hierarchical organization and clustering of styles\naccording to these metrics. We have further verified that the identified groups\ncorrespond well with the textual content used to qualitatively describe the\nstyles, and that the employed complexity-entropy measures can be used for an\neffective classification of artworks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 19:21:14 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Sigaki", "Higor Y. D.", ""], ["Perc", "Matjaz", ""], ["Ribeiro", "Haroldo V.", ""]]}, {"id": "1809.06023", "submitter": "Mohammad Javad Khojasteh", "authors": "Mohammad Javad Khojasteh, Anatoly Khina, Massimo Franceschetti, and\n  Tara Javidi", "title": "Learning-based attacks in cyber-physical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CR cs.IT cs.LG cs.SY math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of learning-based attacks in a simple abstraction of\ncyber-physical systems---the case of a discrete-time, linear, time-invariant\nplant that may be subject to an attack that overrides the sensor readings and\nthe controller actions. The attacker attempts to learn the dynamics of the\nplant and subsequently override the controller's actuation signal, to destroy\nthe plant without being detected. The attacker can feed fictitious sensor\nreadings to the controller using its estimate of the plant dynamics and mimic\nthe legitimate plant operation. The controller, on the other hand, is\nconstantly on the lookout for an attack; once the controller detects an attack,\nit immediately shuts the plant off. In the case of scalar plants, we derive an\nupper bound on the attacker's deception probability for any measurable control\npolicy when the attacker uses an arbitrary learning algorithm to estimate the\nsystem dynamics. We then derive lower bounds for the attacker's deception\nprobability for both scalar and vector plants by assuming a specific\nauthentication test that inspects the empirical variance of the system\ndisturbance. We also show how the controller can improve the security of the\nsystem by superimposing a carefully crafted privacy-enhancing signal on top of\nthe \"nominal control policy.\" Finally, for nonlinear scalar dynamics that\nbelong to the Reproducing Kernel Hilbert Space (RKHS), we investigate the\nperformance of attacks based on nonlinear Gaussian-processes (GP) learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 05:22:38 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 01:36:07 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 22:45:47 GMT"}, {"version": "v4", "created": "Sun, 16 Feb 2020 02:23:01 GMT"}, {"version": "v5", "created": "Thu, 5 Mar 2020 23:23:17 GMT"}, {"version": "v6", "created": "Mon, 9 Mar 2020 04:48:15 GMT"}, {"version": "v7", "created": "Sat, 27 Jun 2020 07:34:11 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Khojasteh", "Mohammad Javad", ""], ["Khina", "Anatoly", ""], ["Franceschetti", "Massimo", ""], ["Javidi", "Tara", ""]]}, {"id": "1809.06052", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Arjun K Gupta and Debasis Kundu", "title": "Parameter Estimation of absolute continuous four parameter Geometric\n  Marshall-Olkin bivariate Pareto Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate a four parameter absolute continuous Geometric\nMarshall-Olkin bivariate Pareto distribution and study its parameter estimation\nthrough EM algorithm and also explore the bayesian analysis through slice cum\nGibbs sampler approach. Numerical results are shown to verify the performance\nof the algorithms. We illustrate the procedures through a real life data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:42:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Gupta", "Arjun K", ""], ["Kundu", "Debasis", ""]]}, {"id": "1809.06329", "submitter": "Atin Angrish", "authors": "Atin Angrish, Benjamin Craver, Binil Starly", "title": "\"FabSearch\" : A 3D CAD Model Based Search Engine for Sourcing\n  Manufacturing Services", "comments": "Submitted for journal considerations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present \"FabSearch\", a prototype search engine for sourcing\nmanufacturer service providers, by making use of the product manufacturing\ninformation contained within a 3D digital file of a product. FabSearch is\ndesigned to take in a query 3D model, such as the .STEP file of a part model\nwhich then produces a ranked list of job shop service providers who are best\nsuited to fabricate the part. Service providers may have potentially built\nhundreds to thousands of parts with associated part 3D models over time.\nFabSearch assumes that these service providers have shared shape signatures of\nthe part models built previously to enable the algorithm to most effectively\nrank the service providers who have the most experience to build the query part\nmodel. FabSearch has two important features that helps it produce relevant\nresults. First, it makes use of the shape characteristics of the 3D part by\ncalculating the Spherical Harmonics signature of the part to calculate the most\nsimilar shapes built previously be job shop service providers. Second,\nFabSearch utilizes meta-data about each part, such as material specification,\ntolerance requirements to help improve the search results based on the specific\nquery model requirements. The algorithm is tested against a repository\ncontaining more than 2000 models distributed across various job shop service\nproviders. For the first time, we show the potential for utilizing the rich\ninformation contained within a 3D part model to automate the sourcing and\neventual selection of manufacturing service providers.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 17:01:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Angrish", "Atin", ""], ["Craver", "Benjamin", ""], ["Starly", "Binil", ""]]}, {"id": "1809.06405", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey and Sanku Dey", "title": "Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:48:30 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""]]}, {"id": "1809.06418", "submitter": "Yili Hong", "authors": "Yimeng Xie and Li Xu and Jie Li and Xinwei Deng and Yili Hong and\n  Korine Kolivras and David N. Gaines", "title": "Spatial Variable Selection and An Application to Virginia Lyme Disease\n  Emergence", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lyme disease is an infectious disease that is caused by a bacterium called\nBorrelia burgdorferi sensu stricto. In the United States, Lyme disease is one\nof the most common infectious diseases. The major endemic areas of the disease\nare New England, Mid-Atlantic, East-North Central, South Atlantic, and West\nNorth-Central. Virginia is on the front-line of the disease's diffusion from\nthe northeast to the south. One of the research objectives for the infectious\ndisease community is to identify environmental and economic variables that are\nassociated with the emergence of Lyme disease. In this paper, we use a spatial\nPoisson regression model to link the spatial disease counts and environmental\nand economic variables, and develop a spatial variable selection procedure to\neffectively identify important factors by using an adaptive elastic net\npenalty. The proposed methods can automatically select important covariates,\nwhile adjusting for possible spatial correlations of disease counts. The\nperformance of the proposed method is studied and compared with existing\nmethods via a comprehensive simulation study. We apply the developed variable\nselection methods to the Virginia Lyme disease data and identify important\nvariables that are new to the literature. Supplementary materials for this\npaper are available online.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 19:57:37 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Xie", "Yimeng", ""], ["Xu", "Li", ""], ["Li", "Jie", ""], ["Deng", "Xinwei", ""], ["Hong", "Yili", ""], ["Kolivras", "Korine", ""], ["Gaines", "David N.", ""]]}, {"id": "1809.06636", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer and Marta Pittavino", "title": "Comparison between Suitable Priors for Additive Bayesian Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive Bayesian networks are types of graphical models that extend the\nusual Bayesian generalized linear model to multiple dependent variables through\nthe factorisation of the joint probability distribution of the underlying\nvariables. When fitting an ABN model, the choice of the prior of the parameters\nis of crucial importance. If an inadequate prior - like a too weakly\ninformative one - is used, data separation and data sparsity lead to issues in\nthe model selection process. In this work a simulation study between two weakly\nand a strongly informative priors is presented. As weakly informative prior we\nuse a zero mean Gaussian prior with a large variance, currently implemented in\nthe R-package abn. The second prior belongs to the Student's t-distribution,\nspecifically designed for logistic regressions and, finally, the strongly\ninformative prior is again Gaussian with mean equal to true parameter value and\na small variance. We compare the impact of these priors on the accuracy of the\nlearned additive Bayesian network in function of different parameters. We\ncreate a simulation study to illustrate Lindley's paradox based on the prior\nchoice. We then conclude by highlighting the good performance of the\ninformative Student's t-prior and the limited impact of the Lindley's paradox.\nFinally, suggestions for further developments are provided.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 10:53:51 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""], ["Pittavino", "Marta", ""]]}, {"id": "1809.06852", "submitter": "Sneha Jadhav", "authors": "Sneha Jadhav, Chenjin Ma, Yefei Jiang, Ben-Chang Shia, Shuangge Ma", "title": "Pan-disease clustering analysis of the trend of period prevalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For all diseases, prevalence has been carefully studied. In the \"classic\"\nparadigm, the prevalence of different diseases has usually been studied\nseparately. Accumulating evidences have shown that diseases can be\n\"correlated\". The joint analysis of prevalence of multiple diseases can provide\nimportant insights beyond individual-disease analysis, however, has not been\nwell conducted. In this study, we take advantage of the uniquely valuable\nTaiwan National Health Insurance Research Database (NHIRD), and conduct a\npan-disease analysis of period prevalence trend. The goal is to identify\nclusters within which diseases share similar period prevalence trends. For this\npurpose, a novel penalization pursuit approach is developed, which has an\nintuitive formulation and satisfactory properties. In data analysis, the period\nprevalence values are computed using records on close to 1 million subjects and\n14 years of observation. For 405 diseases, 35 nontrivial clusters (with sizes\nlarger than one) and 27 trivial clusters (with sizes one) are identified. The\nresults differ significantly from those of the alternatives. A closer\nexamination suggests that the clustering results have sound interpretations.\nThis study is the first to conduct a pan-disease clustering analysis of\nprevalence trend using the uniquely valuable NHIRD data and can have important\nvalue in multiple aspects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 01:45:56 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Jadhav", "Sneha", ""], ["Ma", "Chenjin", ""], ["Jiang", "Yefei", ""], ["Shia", "Ben-Chang", ""], ["Ma", "Shuangge", ""]]}, {"id": "1809.06899", "submitter": "Ru Zhang", "authors": "Ru Zhang and Cheng-Ta Yang and Janne V. Kujala", "title": "Testing Selective Influence Directly Using Trackball Movement Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems factorial technology (SFT; Townsend & Nozawa, 1995) is regarded as a\nuseful tool to diagnose if features (or dimensions) of the investigated\nstimulus are processed in a parallel or serial fashion. In order to use SFT,\none has to assume the speed to process each feature is influenced by that\nfeature only, termed as selective influence (Sternberg, 1969). This assumption\nis usually untestable as the processing time for a stimulus feature is not\nobservable. Stochastic dominance is traditionally used as an indirect evidence\nfor selective influence (e.g., Townsend & Fifi\\'c, 2004). However, one should\nkeep in mind that selective influence may be violated even when stochastic\ndominance holds. The current study proposes a trackball movement paradigm for a\ndirect test of selective influence. The participants were shown a reference\nstimulus and a test stimulus simultaneously on a computer screen. They were\nasked to use the trackball to adjust the test stimulus until it appeared to\nmatch the position or shape of the reference stimulus. We recorded the reaction\ntime, the parameters defined the reference stimulus (denoted as \\alpha and\n\\beta ), and the parameters defined the test stimulus (denoted as A and B). We\ntested selective influence of \\alpha and \\beta on the amount of time to adjust\nA and B through testing selective influence of \\alpha and \\beta on the values\nof A and B using the linear feasibility test (Dzhafarov & Kujala, 2010). We\nfound that when the test was passed and stochastic dominance held, the inferred\narchitecture was as expected, which was further confirmed by the trajectory of\nA and B observed in each trial. However, with stochastic dominance only SFT can\nsuggest a prohibited architecture. Our results indicate the proposed method is\nmore reliable for testing selective influence on the processing speed than\nexamining stochastic dominance only.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 19:27:30 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zhang", "Ru", ""], ["Yang", "Cheng-Ta", ""], ["Kujala", "Janne V.", ""]]}, {"id": "1809.07109", "submitter": "Young-Jin Park", "authors": "Young-Jin Park, and Han-Lim Choi", "title": "InfoSSM: Interpretable Unsupervised Learning of Nonparametric\n  State-Space Model for Multi-modal Dynamics", "comments": "Submitted to AIAA Intelligent Systems Student Paper Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of system identification is to learn about underlying physics\ndynamics behind the time-series data. To model the probabilistic and\nnonparametric dynamics model, Gaussian process (GP) have been widely used; GP\ncan estimate the uncertainty of prediction and avoid over-fitting. Traditional\nGPSSMs, however, are based on Gaussian transition model, thus often have\ndifficulty in describing a more complex transition model, e.g. aircraft\nmotions. To resolve the challenge, this paper proposes a framework using\nmultiple GP transition models which is capable of describing multi-modal\ndynamics. Furthermore, we extend the model to the information-theoretic\nframework, the so-called InfoSSM, by introducing a mutual information\nregularizer helping the model to learn interpretable and distinguishable\nmultiple dynamics models. Two illustrative numerical experiments in simple\nDubins vehicle and high-fidelity flight simulator are presented to demonstrate\nthe performance and interpretability of the proposed model. Finally, this paper\nintroduces a framework using InfoSSM with Bayesian filtering for air traffic\ncontrol tracking.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:16:00 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 10:05:23 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Park", "Young-Jin", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1809.07114", "submitter": "Siva Rajesh Kasa", "authors": "Malay Bhattacharyya and Siva Rajesh Kasa", "title": "A Test for detecting Structural Breakdowns in Markets using Eigenvalue\n  Decompositions", "comments": "15 pages, 9th International Conference of the Financial Engineering\n  and Banking Society (FEBS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Correlations among stock returns during volatile markets differ substantially\ncompared to those from quieter markets. During times of financial crisis, it\nhas been observed that traditional dependency in global markets breaks down.\nHowever, such an upheaval in dependency structure happens over a span of\nseveral months, with the breakdown coinciding with a major bankruptcy or\nsovereign default. Even though risk managers generally agree that identifying\nthese periods of breakdown is important, there are few statistical methods to\ntest for significant breakdowns. The purpose of this paper is to propose a\nsimple test to detect such structural changes in global markets. This test\nrelies on the assumption that asset price follows a Geometric Brownian Motion.\nWe test for a breakdown in correlation structure using eigenvalue\ndecomposition. We derive the asymptotic distribution under the null hypothesis\nand apply the test to stock returns. We compute the power of our test and\ncompare it with the power of other known tests. Our test is able to accurately\nidentify the times of structural breakdown in real-world stock returns. Overall\nwe argue, despite the parsimony and simplicity in the assumption of Geometric\nBrownian Motion, our test can perform well to identify the breakdown in\ndependency of global markets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:26:12 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 15:58:04 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bhattacharyya", "Malay", ""], ["Kasa", "Siva Rajesh", ""]]}, {"id": "1809.07203", "submitter": "Junyan Liu", "authors": "Junyan Liu, Sandeep Kumar, and Daniel P. Palomar", "title": "Parameter Estimation of Heavy-Tailed AR Model with Missing Data via\n  Stochastic EM", "comments": "This is a companion document to a paper that is accepted to IEEE\n  Transaction on Signal Processing 2019, complemented with the supplementary\n  material", "journal-ref": null, "doi": "10.1109/TSP.2019.2899816", "report-no": null, "categories": "stat.AP eess.SP math.OC q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The autoregressive (AR) model is a widely used model to understand time\nseries data. Traditionally, the innovation noise of the AR is modeled as\nGaussian. However, many time series applications, for example, financial time\nseries data, are non-Gaussian, therefore, the AR model with more general\nheavy-tailed innovations is preferred. Another issue that frequently occurs in\ntime series is missing values, due to system data record failure or unexpected\ndata loss. Although there are numerous works about Gaussian AR time series with\nmissing values, as far as we know, there does not exist any work addressing the\nissue of missing data for the heavy-tailed AR model. In this paper, we consider\nthis issue for the first time, and propose an efficient framework for parameter\nestimation from incomplete heavy-tailed time series based on a stochastic\napproximation expectation maximization (SAEM) coupled with a Markov Chain Monte\nCarlo (MCMC) procedure. The proposed algorithm is computationally cheap and\neasy to implement. The convergence of the proposed algorithm to a stationary\npoint of the observed data likelihood is rigorously proved. Extensive\nsimulations and real datasets analyses demonstrate the efficacy of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:09:52 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 08:58:22 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Liu", "Junyan", ""], ["Kumar", "Sandeep", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1809.07221", "submitter": "Hazel Murray", "authors": "Hazel Murray and David Malone", "title": "Exploring the Impact of Password Dataset Distribution on Guessing", "comments": null, "journal-ref": "2018 16th Annual Conference on Privacy, Security and Trust (PST)", "doi": "10.1109/PST.2018.8514194", "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leaks from password datasets are a regular occurrence. An organization may\ndefend a leak with reassurances that just a small subset of passwords were\ntaken. In this paper we show that the leak of a relatively small number of\ntext-based passwords from an organizations' stored dataset can lead to a\nfurther large collection of users being compromised. Taking a sample of\npasswords from a given dataset of passwords we exploit the knowledge we gain of\nthe distribution to guess other samples from the same dataset. We show\ntheoretically and empirically that the distribution of passwords in the sample\nfollows the same distribution as the passwords in the whole dataset. We propose\na function that measures the ability of one distribution to estimate another.\nLeveraging this we show that a sample of passwords leaked from a given dataset,\nwill compromise the remaining passwords in that dataset better than a sample\nleaked from another source.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:35:57 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Murray", "Hazel", ""], ["Malone", "David", ""]]}, {"id": "1809.07232", "submitter": "Thomas M\\\"obius", "authors": "Thomas Wilhelm Dieter M\\\"obius", "title": "Modelling the data and not the images in FMRI", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to the analysis of functional magnetic resonance\nimaging (FMRI) data applies various preprocessing steps to the original FMRI.\nThese preprocessings lead to a general underestimation of residual variance in\nthe downstream analysis. This negatively impacts the type I error of\nstatistical tests and increases the risk for reporting false positive results.\nA genuine approach to the statistical analysis of FMRI data of brain scans is\nderived from first principles that is deeply rooted in statistical test theory.\nThe method combines all preprocessing steps of the standard approach into one\nsingle modelling step, enabling valid statistical tests to be constructed. On\npopulation level, BOLD effects are modelled by random effects meta regression\nmodels. This acknowledges that subjects are random entities, and it\nacknowledges that the accuracy of the BOLD signal is estimated with various\ncertainty in an FMRI. The concept of a reference scalar field is introduced\nthat enables individual effect sizes to be related to each other with respect\nto a common unit. In particular, multicentre studies will gain interpretability\nand power by its use.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:04:11 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["M\u00f6bius", "Thomas Wilhelm Dieter", ""]]}, {"id": "1809.07292", "submitter": "David Robertson", "authors": "David S. Robertson and James M.S. Wason", "title": "Online control of the false discovery rate in biomedical research", "comments": "48 pages, 18 figures; Updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biomedical research frequently involves testing multiple related\nhypotheses, while maintaining control over a suitable error rate. In many\napplications the false discovery rate (FDR), which is the expected proportion\nof false positives among the rejected hypotheses, has become the standard error\ncriterion. Procedures that control the FDR, such as the well-known\nBenjamini-Hochberg procedure, assume that all p-values are available to be\ntested at a single time point. However, this ignores the sequential nature of\nmany biomedical experiments, where a sequence of hypotheses is tested without\nhaving access to future p-values or even the number of hypotheses. Recently,\nthe first procedures that control the FDR in this online manner have been\nproposed by Javanmard and Montanari (Ann. Stat. 2018), and built upon by Ramdas\net al. (NIPS 2017, ICML 2018). In this paper, we compare and contrast these\nproposed procedures, with a particular focus on the setting where the p-values\nare dependent. We also propose a simple modification of the procedures for when\nthere is an upper bound on the number of hypotheses to be tested. Using\ncomprehensive simulation scenarios and case studies, we provide recommendations\nfor which procedures to use in practice for online FDR control.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:37:46 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:08:01 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Robertson", "David S.", ""], ["Wason", "James M. S.", ""]]}, {"id": "1809.07359", "submitter": "Yong Luo", "authors": "Yong Luo", "title": "Parameter Recovery with Marginal Maximum Likelihood and Markov Chain\n  Monte Carlo Estimation for the Generalized Partial Credit Model", "comments": "36 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized partial credit model (GPCM) is a popular polytomous IRT model\nthat has been widely used in large-scale educational surveys and health care\nservices. Same as other IRT models, GPCM can be estimated via marginal maximum\nlikelihood estimation (MMLE) and Markov chain Monte Carlo (MCMC) methods. While\nstudies comparing MMLE and MCMC as estimation methods for other polytomous IRT\nmodels such as the nominal response model and the graded response model exist\nin literature, no studies have compared how well MMLE and MCMC recover the\nmodel parameters of GPCM. In the current study, a comprehensive simulation\nstudy was conducted to compare parameter recovery of GPCM via MMLE and MCMC.\nThe manipulated factors included latent distribution, sample size, and test\nlength, and parameter recovery was evaluated with bias and root mean square\nerror. It was found that there were no statistically significant differences in\nrecovery of the item location and ability parameters between MMLE and MCMC; for\nthe item discrimination parameter, MCMC had less bias in parameter recovery\nthan MMLE under both normal and uniform latent distributions, and MMLE\noutperformed MCMC with less bias in parameter recovery under skewed latent\ndistributions. A real dataset from a high-stakes test was used to demonstrate\nthe estimation of GPCM with MMLE and MCMC.\n  Keywords: polytomous IRT, GPCM, MCMC, MMLE, parameter recovery.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 18:26:29 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Luo", "Yong", ""]]}, {"id": "1809.07394", "submitter": "Jessica Hwang", "authors": "Jessica Hwang and Paulo Orenstein and Judah Cohen and Karl Pfeiffer\n  and Lester Mackey", "title": "Improving Subseasonal Forecasting in the Western U.S. with Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water managers in the western United States (U.S.) rely on longterm forecasts\nof temperature and precipitation to prepare for droughts and other wet weather\nextremes. To improve the accuracy of these longterm forecasts, the U.S. Bureau\nof Reclamation and the National Oceanic and Atmospheric Administration (NOAA)\nlaunched the Subseasonal Climate Forecast Rodeo, a year-long real-time\nforecasting challenge in which participants aimed to skillfully predict\ntemperature and precipitation in the western U.S. two to four weeks and four to\nsix weeks in advance. Here we present and evaluate our machine learning\napproach to the Rodeo and release our SubseasonalRodeo dataset, collected to\ntrain and evaluate our forecasting system.\n  Our system is an ensemble of two regression models. The first integrates the\ndiverse collection of meteorological measurements and dynamic model forecasts\nin the SubseasonalRodeo dataset and prunes irrelevant predictors using a\ncustomized multitask model selection procedure. The second uses only historical\nmeasurements of the target variable (temperature or precipitation) and\nintroduces multitask nearest neighbor features into a weighted local linear\nregression. Each model alone is significantly more accurate than the debiased\noperational U.S. Climate Forecasting System (CFSv2), and our ensemble skill\nexceeds that of the top Rodeo competitor for each target variable and forecast\nhorizon. Moreover, over 2011-2018, an ensemble of our regression models and\ndebiased CFSv2 improves debiased CFSv2 skill by 40-50% for temperature and\n129-169% for precipitation. We hope that both our dataset and our methods will\nhelp to advance the state of the art in subseasonal forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:08:26 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 02:42:49 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 10:16:23 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Hwang", "Jessica", ""], ["Orenstein", "Paulo", ""], ["Cohen", "Judah", ""], ["Pfeiffer", "Karl", ""], ["Mackey", "Lester", ""]]}, {"id": "1809.07401", "submitter": "David Dias", "authors": "Helder Rojas, David Dias", "title": "Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in\n  Stress Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in evaluating the resilience of financial\nportfolios under extreme economic conditions. Therefore, we use empirical\nmeasures to characterize the transmission process of macroeconomic shocks to\nrisk parameters. We propose the use of an extensive family of models, called\nGeneral Transfer Function Models, which condense well the characteristics of\nthe transmission described by the impact measures. The procedure for estimating\nthe parameters of these models is described employing the Bayesian approach and\nusing the prior information provided by the impact measures. In addition, we\nillustrate the use of the estimated models from the credit risk data of a\nportfolio.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:36:02 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 10:45:09 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 18:51:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Rojas", "Helder", ""], ["Dias", "David", ""]]}, {"id": "1809.07419", "submitter": "Peng Ding", "authors": "Jason Wu and Peng Ding", "title": "Randomization Tests for Weak Null Hypotheses in Randomized Experiments", "comments": null, "journal-ref": "Journal of the American Statistical Association 2020", "doi": "10.1080/01621459.2020.1750415", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher randomization test (FRT) is appropriate for any test statistic,\nunder a sharp null hypothesis that can recover all missing potential outcomes.\nHowever, it is often sought after to test a weak null hypothesis that the\ntreatment does not affect the units on average. To use the FRT for a weak null\nhypothesis, we must address two issues. First, we need to impute the missing\npotential outcomes although the weak null hypothesis cannot determine all of\nthem. Second, we need to choose a proper test statistic. For a general weak\nnull hypothesis, we propose an approach to imputing missing potential outcomes\nunder a compatible sharp null hypothesis. Building on this imputation scheme,\nwe advocate a studentized statistic. The resulting FRT has multiple desirable\nfeatures. First, it is model-free. Second, it is finite-sample exact under the\nsharp null hypothesis that we use to impute the potential outcomes. Third, it\nconservatively controls large-sample type I errors under the weak null\nhypothesis of interest. Therefore, our FRT is agnostic to the treatment effect\nheterogeneity. We establish a unified theory for general factorial experiments.\nWe also extend it to stratified and clustered experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:18:32 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 02:18:34 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 08:01:18 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 21:25:52 GMT"}, {"version": "v5", "created": "Thu, 5 Nov 2020 21:14:06 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Wu", "Jason", ""], ["Ding", "Peng", ""]]}, {"id": "1809.07659", "submitter": "Marius \\\"Otting", "authors": "Christian Deutscher, Marius \\\"Otting, Roland Langrock, Sebastian\n  Gehrmann, Sandra Schneemann, Hendrik Scholten", "title": "Very Highly Skilled Individuals Do Not Choke Under Pressure: Evidence\n  from Professional Darts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and predicting how individuals perform in high-pressure\nsituations is of importance in designing and managing workplaces, but also in\nother areas of society such as disaster management or professional sports. For\nsimple effort tasks, an increase in the pressure experienced by an individual,\ne.g. due to incentive schemes in a workplace, will increase the effort put into\nthe task and hence in most cases also the performance. For the more complex and\nusually harder to capture case of skill tasks, there exists a substantial body\nof literature that fairly consistently reports a choking phenomenon under\npressure. However, we argue that many of the corresponding studies have crucial\nlimitations, such as neglected interaction effects or insufficient numbers of\nobservations to allow within-individual analysis. Here, we investigate\nperformance under pressure in professional darts as a near-ideal setting with\nno direct interaction between players and a high number of observations per\nsubject. We analyze almost one year of tournament data covering 23,192 dart\nthrows, hence a data set that is very much larger than those used in most\nprevious studies. Contrary to what would be expected given the evidence in\nfavor of a choking phenomenon, we find strong evidence for an overall improved\nperformance under pressure, for nearly all 83 players in the sample. These\nresults could have important consequences for our understanding of how highly\nskilled individuals deal with high-pressure situations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 14:54:59 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Deutscher", "Christian", ""], ["\u00d6tting", "Marius", ""], ["Langrock", "Roland", ""], ["Gehrmann", "Sebastian", ""], ["Schneemann", "Sandra", ""], ["Scholten", "Hendrik", ""]]}, {"id": "1809.07748", "submitter": "Shing Chan", "authors": "Shing Chan and Ahmed H. Elsheikh", "title": "Exemplar-based synthesis of geology using kernel discrepancies and\n  generative neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for synthesis of geological images based on an\nexemplar image. We synthesize new realizations such that the discrepancy in the\npatch distribution between the realizations and the exemplar image is\nminimized. Such discrepancy is quantified using a kernel method for two-sample\ntest called maximum mean discrepancy. To enable fast synthesis, we train a\ngenerative neural network in an offline phase to sample realizations\nefficiently during deployment, while also providing a parametrization of the\nsynthesis process. We assess the framework on a classical binary image\nrepresenting channelized subsurface reservoirs, finding that the method\nreproduces the visual patterns and spatial statistics (image histogram and\ntwo-point probability functions) of the exemplar image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 17:33:20 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 09:31:45 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1809.07822", "submitter": "Belinda Spratt", "authors": "Belinda Spratt, Erhan Kozan and Michael Sinnott", "title": "Analysis of uncertainty in the surgical department: durations, requests,\n  and cancellations", "comments": null, "journal-ref": null, "doi": "10.1071/AH18082", "report-no": null, "categories": "stat.AP physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BACKGROUND: Analytical techniques are being implemented with increasing\nfrequency to improve the management of surgical departments and to ensure that\ndecisions are well-informed. Often these analytical techniques rely on the\nvalidity of underlying statistical assumptions, including those around choice\nof distribution when modelling uncertainty. OBJECTIVE: The objective of the\nresearch is to determine a set of suitable statistical distributions and\nprovide recommendations to assist hospital planning staff, based on three full\nyears of historical data. METHODS: Statistical analysis has been performed to\ndetermine the most appropriate distributions and models in a variety of\nsurgical contexts. Data from 2013 to 2015 was collected from the surgical\ndepartment at a large Australian public hospital. RESULTS: A lognormal\ndistribution approximation of the total duration of surgeries in an operating\nroom is appropriate when considering probability of overtime. Surgical requests\ncan be modelled as a Poisson process with rate dependent on urgency and day of\nthe week. It is found that individual cancellations can be modelled as\nBernoulli trials, with the probability of patient, staff, and resource based\ncancellations provided herein. CONCLUSIONS: The analysis presented here can be\nused to ensure that assumptions surrounding planning and scheduling in the\nsurgical department are valid. Understanding the stochasticity in the surgical\ndepartment may result in the implementation of more realistic decision models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 06:20:04 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Spratt", "Belinda", ""], ["Kozan", "Erhan", ""], ["Sinnott", "Michael", ""]]}, {"id": "1809.07879", "submitter": "Andreea Gornea", "authors": "Andreea Ioana Gornea, Alexandru Calin, Paul Daniel Dumitru, Dan Alin\n  Nedelcu, Radu Stefan Stoica", "title": "Statistical analysis of astro-geodetic data through principal component\n  analysis, linear modelling and bootstrap based inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates the application of statistical based methodology for\nthe analysis of the vertical deviation angle. The studied data set contains\nastro-geodetic observations. The Principal Component Analysis and the Multiple\nLinear Regression models are embedded within a bootstrap procedure, in order to\novercome the difficulties related to data correlation, while taking advantage\nof all the information provided. The methodology is applied on real data. The\nobtained results indicate that the pressure, the temperature and the humidity\nare variables that may influence the measure of the vertical deviation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 22:04:46 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Gornea", "Andreea Ioana", ""], ["Calin", "Alexandru", ""], ["Dumitru", "Paul Daniel", ""], ["Nedelcu", "Dan Alin", ""], ["Stoica", "Radu Stefan", ""]]}, {"id": "1809.07946", "submitter": "Takeshi Ise", "authors": "Takeshi Ise and Yurika Oba", "title": "Estimating mesoscale linkage between land-surface conditions and marine\n  productions in Japan: a study using a sparse high-dimensional model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several scientific studies concerning the interconnectedness\nbetween land-surface conditions (e.g., vegetation, land use, and socioeconomic\nactivities) and marine ecosystems (e.g., biodiversity, primary production, and\nseafood production). This idea of the land-sea connectivity sounded reasonable\nfor many scientists because there is an obvious connection by rivers, however,\nquantitative estimation of this relationship has been thought to be difficult\ndue to the size of the target areas, the numbers of possible variables, and the\namount of noises in this complex system. In this study, we applied a sparse\nhigh-dimensional modeling to overcome these difficulties and found several\nsignificant land-sea linkages. In this modeling, the key is the penalization in\nthe number of independent variables, and thus a limited number of significant\nindependent variables are chosen in an objective and quantitative manner. We\nselected 448 independent variables (geological, biological, and social) and 68\ndependent variables (marine products) from a governmental database. Then we\nsummarized the data according to the political boundaries of 47 prefectures.\nThe sparse high-dimensional model we constructed successfully highlighted\nseveral significant variables to estimate the amount of marine products by\nprefecture. For example, we found that coastal, especially sessile marine\nproducts such as seaweed and shellfish had more explanatory variables than\nopen-water marine products such as tuna and sailfishes. In addition, salmon\nproduction had a strong connection to the mesoscale (prefecture-level)\nland-surface conditions possibly due to their interconnected life cycle between\nfreshwater rivers and the sea. We believe that the sparse modeling is an\neffective statistical tool to explain relationships in a complex system such as\nland-sea connections.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 05:14:22 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Ise", "Takeshi", ""], ["Oba", "Yurika", ""]]}, {"id": "1809.08024", "submitter": "Harry Gray", "authors": "Harry Gray, Gwena\\\"el G.R. Leday, Catalina A. Vallejos, and Sylvia\n  Richardson", "title": "Shrinkage estimation of large covariance matrices using multiple\n  shrinkage targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear shrinkage estimators of a covariance matrix --- defined by a weighted\naverage of the sample covariance matrix and a pre-specified shrinkage target\nmatrix --- are popular when analysing high-throughput molecular data. However,\ntheir performance strongly relies on an appropriate choice of target matrix.\nThis paper introduces a more flexible class of linear shrinkage estimators that\ncan accommodate multiple shrinkage target matrices, directly accounting for the\nuncertainty regarding the target choice. This is done within a conjugate\nBayesian framework, which is computationally efficient. Using both simulated\nand real data, we show that the proposed estimator is less sensitive to target\nmisspecification and can outperform state-of-the-art (nonparametric)\nsingle-target linear shrinkage estimators. Using protein expression data from\nThe Cancer Proteome Atlas we illustrate how multiple sources of prior\ninformation (obtained from more than 30 different cancer types) can be\nincorporated into the proposed multi-target linear shrinkage estimator. In\nparticular, it is shown that the target-specific weights can provide insights\ninto the differences and similarities between cancer types. Software for the\nmethod is freely available as an R-package at http://github.com/HGray384/TAS.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:34:31 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Gray", "Harry", ""], ["Leday", "Gwena\u00ebl G. R.", ""], ["Vallejos", "Catalina A.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1809.08060", "submitter": "Mikko Pakkanen", "authors": "Maxime Morariu-Patrichi and Mikko S. Pakkanen", "title": "State-dependent Hawkes processes and their application to limit order\n  book modelling", "comments": "30 pages, 13 figures, v2: minor improvements and corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical aspects of state-dependent Hawkes processes, which are\nan extension of Hawkes processes where a self- and cross-exciting counting\nprocess and a state process are fully coupled, interacting with each other. The\nexcitation kernel of the counting process depends on the state process that,\nreciprocally, switches state when there is an event in the counting process. We\nfirst establish the existence and uniqueness of state-dependent Hawkes\nprocesses and explain how they can be simulated. Then we develop maximum\nlikelihood estimation methodology for parametric specifications of the process.\nWe apply state-dependent Hawkes processes to high-frequency limit order book\ndata, allowing us to build a novel model that captures the feedback loop\nbetween the order flow and the shape of the limit order book. We estimate two\nspecifications of the model, using the bid-ask spread and the queue imbalance\nas state variables, and find that excitation effects in the order flow are\nstrongly state-dependent. Additionally, we find that the endogeneity of the\norder flow, measured by the magnitude of excitation, is also state-dependent,\nbeing more pronounced in disequilibrium states of the limit order book.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 12:29:38 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:42:42 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Morariu-Patrichi", "Maxime", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "1809.08143", "submitter": "Kenia Parra", "authors": "Rosana de Fatima Martinhao, Kenia Naara Parra, Daniela Maria Lemos\n  Barbato and Ana Claudia Kasseboehmer", "title": "Psychometric properties of an instrument to investigate the motivation\n  of visitors to a science museum: The combination of methods", "comments": "26 pages, 4 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visit to a science museum may be manifested through complex and dynamic\nmotivations which, according to the literature, are under-investigated in a\nBrazilian context. In the present study, an instrument originally developed by\nDelgado in 2008 (http://hdl.handle.net/10773/1623) has been modified and\napplied to 202 visitors up to 15 years to the Science Museum \"Professor Mario\nTolentino\" in Sao Carlos, Brazil, in order to investigate motivation for\nvisiting the institute. Combined application of Exploratory Factor Analysis and\nthe Information Bottleneck method revealed that 17 out of the 20 initial items\nin the questionnaire aligned with three dimensions of motivation. The main\nmotivation was learning desire, while entertainment and interaction motivations\nwere significantly less important. The study provided relevant evidence\nregarding the motivations of visitors, and this information will be valuable in\nimproving the activities of the museum. The implications of our findings for\nfuture research are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:25:59 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 19:17:56 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Martinhao", "Rosana de Fatima", ""], ["Parra", "Kenia Naara", ""], ["Barbato", "Daniela Maria Lemos", ""], ["Kasseboehmer", "Ana Claudia", ""]]}, {"id": "1809.08427", "submitter": "Lewis Mitchell", "authors": "Jonathan Tuke, Andrew Nguyen, Mehwish Nasim, Drew Mellor, Asanga\n  Wickramasinghe, Nigel Bean, Lewis Mitchell", "title": "Pachinko Prediction: A Bayesian method for event prediction from social\n  media data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of large open data sources with machine learning approaches\npresents a potentially powerful way to predict events such as protest or social\nunrest. However, accounting for uncertainty in such models, particularly when\nusing diverse, unstructured datasets such as social media, is essential to\nguarantee the appropriate use of such methods. Here we develop a Bayesian\nmethod for predicting social unrest events in Australia using social media\ndata. This method uses machine learning methods to classify individual postings\nto social media as being relevant, and an empirical Bayesian approach to\ncalculate posterior event probabilities. We use the method to predict events in\nAustralian cities over a period in 2017/18.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 11:40:22 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Tuke", "Jonathan", ""], ["Nguyen", "Andrew", ""], ["Nasim", "Mehwish", ""], ["Mellor", "Drew", ""], ["Wickramasinghe", "Asanga", ""], ["Bean", "Nigel", ""], ["Mitchell", "Lewis", ""]]}, {"id": "1809.08449", "submitter": "Erik van Zwet", "authors": "Erik van Zwet", "title": "A default prior for regression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the sample size is not too small, M-estimators of regression\ncoefficients are approximately normal and unbiased. This leads to the familiar\nfrequentist inference in terms of normality-based confidence intervals and\np-values. From a Bayesian perspective, use of the (improper) uniform prior\nyields matching results in the sense that posterior quantiles agree with\none-sided confidence bounds. For this, and various other reasons, the uniform\nprior is often considered objective or non-informative. In spite of this, we\nargue that the uniform prior is not suitable as a default prior for inference\nabout a regression coefficient in the context of the bio-medical and social\nsciences. We propose that a more suitable default choice is the normal\ndistribution with mean zero and standard deviation equal to the standard error\nof the M-estimator. We base this recommendation on two arguments. First, we\nshow that this prior is non-informative for inference about the sign of the\nregression coefficient. Secondly, we show that this prior agrees well with a\nmeta-analysis of 50 articles from the MEDLINE database.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 15:54:44 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 13:07:36 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["van Zwet", "Erik", ""]]}, {"id": "1809.08771", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Trevor Hastie", "title": "Longitudinal data analysis using matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of defect\nof vision, or assessment of gait in patients with neurological disorders. Since\nthe data collection is often costly and inconvenient, estimation of progression\nfrom sparse observations is of great interest for practitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both random and fixed effect.\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions and require\nvery careful implementation.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields point\nestimates of progression curves by iterative application of the SVD. Our\nframework covers multivariate longitudinal data, regression and can be easily\nextended to other settings.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables discovering that subtypes of Cerebral Palsy exhibit\ndifferent progression trends.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 06:18:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hastie", "Trevor", ""]]}, {"id": "1809.08785", "submitter": "Charles Fontaine PhD", "authors": "Charles Fontaine and Ron D. Frostig and Hernando Ombao", "title": "Modeling non-linear spectral domain dependence using copulas with\n  applications to rat local field potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper intends to develop tools for characterizing non-linear spectral\ndependence between spontaneous brain signals. We use parametric copula models\n(both bivariate and vine models) applied on the magnitude of Fourier\ncoefficients rather than using coherence. The motivation behind this work is an\nexperiment on rats that studied the impact of stroke on the connectivity\nstructure (dependence) between local field potentials recorded at various\nchannels. We address the following major questions. First, we ask whether one\ncan detect any changepoint in the regime of a brain channel for a given\nfrequency band based on a difference between the cumulative distribution\nfunctions modeled for each epoch (small window of time). Our proposed approach\nis an iterative algorithm which compares each successive bivariate copulas on\nall the epochs range, using a bivariate Kolmogorov-Smirnov statistic. Second,\nwe ask whether stroke can alter the dependence structure of brain signals; and\nexamine whether changes in dependence are present only in some channels or\ngeneralized across channels. These questions are addressed by comparing\nVine-copulas models fitted for each epoch. We provide the necessary framework\nand show the effectiveness of our methods through the results for the local\nfield potential data analysis of a rat.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 07:43:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fontaine", "Charles", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "1809.08801", "submitter": "Safa Medin", "authors": "Safa C. Medin, John Murray-Bruce, David Casta\\~n\\'on, Vivek K Goyal", "title": "Beyond Binomial and Negative Binomial: Adaptation in Bernoulli Parameter\n  Estimation", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the parameter of a Bernoulli process arises in many applications,\nincluding photon-efficient active imaging where each illumination period is\nregarded as a single Bernoulli trial. Motivated by acquisition efficiency when\nmultiple Bernoulli processes are of interest, we formulate the allocation of\ntrials under a constraint on the mean as an optimal resource allocation\nproblem. An oracle-aided trial allocation demonstrates that there can be a\nsignificant advantage from varying the allocation for different processes and\ninspires a simple trial allocation gain quantity. Motivated by realizing this\ngain without an oracle, we present a trellis-based framework for representing\nand optimizing stopping rules. Considering the convenient case of Beta priors,\nthree implementable stopping rules with similar performances are explored, and\nthe simplest of these is shown to asymptotically achieve the oracle-aided trial\nallocation. These approaches are further extended to estimating functions of a\nBernoulli parameter. In simulations inspired by realistic active imaging\nscenarios, we demonstrate significant mean-squared error improvements: up to\n4.36 dB for the estimation of p and up to 1.80 dB for the estimation of log p.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 08:38:34 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 19:08:11 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Medin", "Safa C.", ""], ["Murray-Bruce", "John", ""], ["Casta\u00f1\u00f3n", "David", ""], ["Goyal", "Vivek K", ""]]}, {"id": "1809.08871", "submitter": "Edouard Fournier", "authors": "Edouard Fournier (ENAC), St\\'ephane Grihon, Thierry Klein (IMT)", "title": "Semi Parametric Estimations of rotating and scaling parameters for\n  aeronautic loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform registration of noisy curves. We provide an\nappropriate model in estimating the rotation and scaling parameters to adjust a\nset of curves through a M-estimation procedure. We prove the consistency and\nthe asymptotic normality of our estimators. Numerical simulation and a real\nlife aeronautic example are given to illustrate our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 12:26:15 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:00:42 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Fournier", "Edouard", "", "ENAC"], ["Grihon", "St\u00e9phane", "", "IMT"], ["Klein", "Thierry", "", "IMT"]]}, {"id": "1809.08988", "submitter": "Yang Ni", "authors": "Yang Ni, Peter Mueller, Yuan Ji", "title": "Bayesian Double Feature Allocation for Phenotyping with Electronic\n  Health Records", "comments": "32 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a categorical matrix factorization method to infer latent diseases\nfrom electronic health records (EHR) data in an unsupervised manner. A latent\ndisease is defined as an unknown biological aberration that causes a set of\ncommon symptoms for a group of patients. The proposed approach is based on a\nnovel double feature allocation model which simultaneously allocates features\nto the rows and the columns of a categorical matrix. Using a Bayesian approach,\navailable prior information on known diseases greatly improves identifiability\nand interpretability of latent diseases. This includes known diagnoses for\npatients and known association of diseases with symptoms. We validate the\nproposed approach by simulation studies including mis-specified models and\ncomparison with sparse latent factor models. In the application to Chinese EHR\ndata, we find interesting results, some of which agree with related clinical\nand medical knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 19:29:49 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 02:16:32 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Ni", "Yang", ""], ["Mueller", "Peter", ""], ["Ji", "Yuan", ""]]}, {"id": "1809.09215", "submitter": "Tavpritesh Sethi", "authors": "Tavpritesh Sethi, Anant Mittal, Shubham Maheshwari, Samarth Chugh", "title": "Learning to Address Health Inequality in the United States with a\n  Bayesian Decision Network", "comments": "8 pages, 4 figures, 1 table (excluding the supplementary material),\n  accepted for publication in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Life-expectancy is a complex outcome driven by genetic, socio-demographic,\nenvironmental and geographic factors. Increasing socio-economic and health\ndisparities in the United States are propagating the longevity-gap, making it a\ncause for concern. Earlier studies have probed individual factors but an\nintegrated picture to reveal quantifiable actions has been missing. There is a\ngrowing concern about a further widening of healthcare inequality caused by\nArtificial Intelligence (AI) due to differential access to AI-driven services.\nHence, it is imperative to explore and exploit the potential of AI for\nilluminating biases and enabling transparent policy decisions for positive\nsocial and health impact. In this work, we reveal actionable interventions for\ndecreasing the longevity-gap in the United States by analyzing a County-level\ndata resource containing healthcare, socio-economic, behavioral, education and\ndemographic features. We learn an ensemble-averaged structure, draw inferences\nusing the joint probability distribution and extend it to a Bayesian Decision\nNetwork for identifying policy actions. We draw quantitative estimates for the\nimpact of diversity, preventive-care quality and stable-families within the\nunified framework of our decision network. Finally, we make this analysis and\ndashboard available as an interactive web-application for enabling users and\npolicy-makers to validate our reported findings and to explore the impact of\nones beyond reported in this work.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 00:24:06 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 04:20:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Sethi", "Tavpritesh", ""], ["Mittal", "Anant", ""], ["Maheshwari", "Shubham", ""], ["Chugh", "Samarth", ""]]}, {"id": "1809.09590", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Fabrizia Mealli, Corwin M. Zigler, Francesca\n  Dominici, Jason H. Wasfy, Christine Choirat", "title": "Causal Impact of the Hospital Readmissions Reduction Program on Hospital\n  Readmissions and Mortality", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects of the Hospital Readmissions Reduction Program\n(HRRP), part of the Affordable Care Act, has been very controversial.\nAssociational studies have demonstrated decreases in hospital readmissions,\nconsistent with the intent of the program, although analyses with different\ndata sources and methods have differed in estimating effects on patient\nmortality. To address these issues, we define the estimands of interest in the\ncontext of potential outcomes, we formalize a Bayesian structural time-series\nmodel for causal inference, and discuss the necessary assumptions for\nestimation of effects using observed data. The method is used to estimate the\neffect of the passage of HRRP on both the 30-day readmissions and 30-day\nmortality. We show that for acute myocardial infarction and congestive heart\nfailure, HRRP caused reduction in readmissions while it had no statistically\nsignificant effect on mortality. However, for pneumonia, HRRP had no\nstatistically significant effect on readmissions but caused an increase in\nmortality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:48:06 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Mealli", "Fabrizia", ""], ["Zigler", "Corwin M.", ""], ["Dominici", "Francesca", ""], ["Wasfy", "Jason H.", ""], ["Choirat", "Christine", ""]]}, {"id": "1809.09620", "submitter": "Shujaat Khan Engr", "authors": "Shujaat Khan, Imran Naseem, Roberto Togneri, and Mohammed Bennamoun", "title": "RAFP-Pred: Robust Prediction of Antifreeze Proteins using Localized\n  Analysis of n-Peptide Compositions", "comments": "7 pages, 2 figures", "journal-ref": "\"RAFP-Pred: Robust Prediction of Antifreeze Proteins Using\n  Localized Analysis of n-Peptide Compositions,\" in IEEE/ACM Transactions on\n  Computational Biology and Bioinformatics, vol. 15, no. 1, pp. 244-250, 1\n  Jan.-Feb. 2018", "doi": "10.1109/TCBB.2016.2617337", "report-no": null, "categories": "q-bio.BM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)\nto counter the otherwise lethal intracellular formation of ice. Structures and\nsequences of various AFPs exhibit a high degree of heterogeneity, consequently\nthe prediction of the AFPs is considered to be a challenging task. In this\nresearch, we propose to handle this arduous manifold learning task using the\nnotion of localized processing. In particular an AFP sequence is segmented into\ntwo sub-segments each of which is analyzed for amino acid and di-peptide\ncompositions. We propose to use only the most significant features using the\nconcept of information gain (IG) followed by a random forest classification\napproach. The proposed RAFP-Pred achieved an excellent performance on a number\nof standard datasets. We report a high Youden's index\n(sensitivity+specificity-1) value of 0.75 on the standard independent test data\nset outperforming the AFP-PseAAC, AFP\\_PSSM, AFP-Pred and iAFP by a margin of\n0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB\ndataset is found to be 83.19\\% which is substantially superior to the 57.18\\%\nreported for the iAFP method.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:51:29 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Khan", "Shujaat", ""], ["Naseem", "Imran", ""], ["Togneri", "Roberto", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1809.09729", "submitter": "Matthew Nunes", "authors": "Rebecca E. Wilson, Idris A. Eckley, Matthew A. Nunes and Timothy Park", "title": "Dynamic detection of anomalous regions within distributed acoustic\n  sensing data streams using locally stationary wavelet time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed acoustic sensing technology is increasingly being used to support\nproduction and well management within the oil and gas sector, for example to\nimprove flow monitoring and production profiling. This sensing technology is\ncapable of recording substantial data volumes at multiple depths within an oil\nwell, giving unprecedented insights into production behaviour. However the\ntechnology is also prone to recording periods of anomalous behaviour, where the\nsame physical features are concurrently observed at multiple depths. Such\nfeatures are called `stripes' and are undesirable, detrimentally affecting well\nperformance modelling. This paper focuses on the important challenge of\ndeveloping a principled approach to identifying such anomalous periods within\ndistributed acoustic signals. We extend recent work on classifying locally\nstationary wavelet time series to an online setting and, in so doing, introduce\na computationally-efficient online procedure capable of accurately identifying\nanomalous regions within multivariate time series.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 21:17:33 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Wilson", "Rebecca E.", ""], ["Eckley", "Idris A.", ""], ["Nunes", "Matthew A.", ""], ["Park", "Timothy", ""]]}, {"id": "1809.09813", "submitter": "Rabindra Lamsal", "authors": "Rabindra Lamsal, Ayesha Choudhary", "title": "Predicting Outcome of Indian Premier League (IPL) Matches Using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cricket, especially the Twenty20 format, has maximum uncertainty, where a\nsingle over can completely change the momentum of the game. With millions of\npeople following the Indian Premier League (IPL), developing a model for\npredicting the outcome of its matches is a real-world problem. A cricket match\ndepends upon various factors, and in this work, the factors which significantly\ninfluence the outcome of a Twenty20 cricket match are identified. Each player's\nperformance in the field is considered to find out the overall weight (relative\nstrength) of the teams. A multivariate regression based solution is proposed to\ncalculate points for each player in the league and the overall weight of a team\nis computed based on the past performance of the players who have appeared most\nfor the team. Finally, a dataset is modeled based on the identified seven\nfactors which influence the outcome of an IPL match. Six machine learning\nmodels were trained and used for predicting the outcome of each 2018 IPL match,\n15 minutes before the gameplay, immediately after the toss. Three of the\ntrained models were seen to be correctly predicting more than 40 matches, with\nMultilayer Perceptron outperforming all other models with an impressive\naccuracy of 71.66%.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 05:26:14 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 05:04:01 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 07:09:17 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 05:22:58 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2020 17:14:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lamsal", "Rabindra", ""], ["Choudhary", "Ayesha", ""]]}, {"id": "1809.09881", "submitter": "Almond St\\\"ocker", "authors": "Almond St\\\"ocker and Sarah Brockhaus and Sophia Schaffer and Benedikt\n  von Bronk and Madeleine Opitz and Sonja Greven", "title": "Boosting Functional Response Models for Location, Scale and Shape with\n  an Application to Bacterial Competition", "comments": "bootstrap confidence interval type uncertainty bounds added; minor\n  changes in formulations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Generalized Additive Models for Location, Scale, and Shape (GAMLSS)\nto regression with functional response. This allows us to simultaneously model\npoint-wise mean curves, variances and other distributional parameters of the\nresponse in dependence of various scalar and functional covariate effects. In\naddition, the scope of distributions is extended beyond exponential families.\nThe model is fitted via gradient boosting, which offers inherent model\nselection and is shown to be suitable for both complex model structures and\nhighly auto-correlated response curves. This enables us to analyze bacterial\ngrowth in \\textit{Escherichia coli} in a complex interaction scenario,\nfruitfully extending usual growth models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:56:22 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 16:54:35 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["St\u00f6cker", "Almond", ""], ["Brockhaus", "Sarah", ""], ["Schaffer", "Sophia", ""], ["von Bronk", "Benedikt", ""], ["Opitz", "Madeleine", ""], ["Greven", "Sonja", ""]]}, {"id": "1809.10074", "submitter": "Jingchen Hu", "authors": "Jingchen Hu and Terrance D. Savitsky", "title": "Bayesian Data Synthesis and Disclosure Risk Quantification: An\n  Application to the Consumer Expenditure Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The release of synthetic data generated from a model estimated on the data\nhelps statistical agencies disseminate respondent-level data with high utility\nand privacy protection. Motivated by the challenge of disseminating sensitive\nvariables containing geographic information in the Consumer Expenditure Surveys\n(CE) at the U.S. Bureau of Labor Statistics, we propose two non-parametric\nBayesian models as data synthesizers for the county identifier of each data\nrecord: a Bayesian latent class model and a Bayesian areal model. Both data\nsynthesizers use Dirichlet Process priors to cluster observations of similar\ncharacteristics and allow borrowing information across observations. We develop\ninnovative disclosure risks measures to quantify inherent risks in the\nconfidential CE data and how those data risks are ameliorated by our proposed\nsynthesizers. By creating a lower bound and an upper bound of disclosure risks\nunder a minimum and a maximum disclosure risks scenarios respectively, our\nproposed inherent risks measures provide a range of acceptable disclosure risks\nfor evaluating risks level in the synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:47:07 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 15:34:08 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1809.10329", "submitter": "Mauricio Tec", "authors": "Natalia Zuniga-Garcia, Mauricio Tec, James G. Scott, Natalia\n  Ruiz-Juri, Randy B. Machemehl", "title": "Evaluation of Ride-Sourcing Search Frictions and Driver Productivity: A\n  Spatial Denoising Approach", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of measuring spatial and temporal variation\nin driver productivity on ride-sourcing trips. This variation is especially\nimportant from a driver's perspective: if a platform's drivers experience\nsystematic disparities in earnings because of variation in their riders'\ndestinations, they may perceive the pricing model as inequitable. This\nperception can exacerbate search frictions if it leads drivers to avoid\nlocations where they believe they may be assigned \"unlucky\" fares. To\ncharacterize any such systematic disparities in productivity, we develop an\nanalytic framework with three key components. First, we propose a productivity\nmetric that looks two consecutive trips ahead, thus capturing the effect on\nexpected earnings of market conditions at drivers' drop-off locations. Second,\nwe develop a natural experiment by analyzing trips with a common origin but\nvarying destinations, thus isolating purely spatial effects on productivity.\nThird, we apply a spatial denoising method that allows us to work with raw\nspatial information exhibiting high levels of noise and sparsity, without\nhaving to aggregate data into large, low-resolution spatial zones. By applying\nour framework to data on more than 1.4 million rides in Austin, Texas, we find\nsignificant spatial variation in ride-sourcing driver productivity and search\nfrictions. Drivers at the same location experienced disparities in productivity\nafter being dispatched on trips with different destinations, with origin-based\nsurge pricing increasing these earnings disparities. Our results show that trip\ndistance is the dominant factor in driver productivity: short trips yielded\nlower productivity, even when ending in areas with high demand. These findings\nsuggest that new pricing strategies are required to minimize random disparities\nin driver earnings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:25:25 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 21:02:07 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 14:10:18 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 16:49:04 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zuniga-Garcia", "Natalia", ""], ["Tec", "Mauricio", ""], ["Scott", "James G.", ""], ["Ruiz-Juri", "Natalia", ""], ["Machemehl", "Randy B.", ""]]}, {"id": "1809.10632", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Rapha\\\"el Nedellec, Yannig Goude, Simon N. Wood", "title": "Scalable visualisation methods for modern Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades the growth of computational resources has made it\npossible to handle Generalized Additive Models (GAMs) that formerly were too\ncostly for serious applications. However, the growth in model complexity has\nnot been matched by improved visualisations for model development and results\npresentation. Motivated by an industrial application in electricity load\nforecasting, we identify the areas where the lack of modern visualisation tools\nfor GAMs is particularly severe, and we address the shortcomings of existing\nmethods by proposing a set of visual tools that a) are fast enough for\ninteractive use, b) exploit the additive structure of GAMs, c) scale to large\ndata sets and d) can be used in conjunction with a wide range of response\ndistributions. All the new visual methods proposed in this work are implemented\nby the mgcViz R package, which can be found on the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:51:33 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:57:27 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Nedellec", "Rapha\u00ebl", ""], ["Goude", "Yannig", ""], ["Wood", "Simon N.", ""]]}, {"id": "1809.10834", "submitter": "Xiqun Chen", "authors": "Jielun Liu, Ke Han, Xiqun (Michael) Chen, Ghim Ping Ong", "title": "Spatial-Temporal Inference of Urban Traffic Emissions Based on Taxi\n  Trajectories and Multi-Source Urban Data", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle trajectory data collected via GPS-enabled devices have played\nincreasingly important roles in estimating network-wide traffic, given their\nbroad spatial-temporal coverage and representativeness of traffic dynamics.\nThis paper exploits taxi GPS data, license plate recognition (LPR) data and\ngeographical information for reconstructing the spatial and temporal patterns\nof urban traffic emissions. Vehicle emission factor models are employed to\nestimate emissions based on taxi trajectories. The estimated emissions are then\nmapped to spatial grids of urban areas to account for spatial heterogeneity. To\nextrapolate emissions from the taxi fleet to the whole vehicle population, we\nuse Gaussian process regression models supported by geographical features to\nestimate the spatially heterogeneous traffic volume and fleet composition.\nUnlike previous studies, this paper utilizes the taxi GPS data and LPR data to\ndisaggregate vehicle and emission characteristics through space and time in a\nlarge-scale urban network. The results of a case study in Hangzhou, China,\nreveal high-resolution spatiotemporal patterns of traffic flows and emissions\nand identify emission hotspots at different locations. This study provides an\naccessible means of inferring the environmental impact of urban traffic with\nmulti-source data that are now widely available in urban areas.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 03:02:31 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Liu", "Jielun", "", "Michael"], ["Han", "Ke", "", "Michael"], ["Xiqun", "", "", "Michael"], ["Chen", "", ""], ["Ong", "Ghim Ping", ""]]}, {"id": "1809.10838", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Model confidence sets and forecast combination: An application to\n  age-specific mortality", "comments": "32 pages, 7 tables, To appear in Genus - Journal of Population\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model averaging combines forecasts obtained from a range of models, and it\noften produces more accurate forecasts than a forecast from a single model. The\ncrucial part of forecast accuracy improvement in using the model averaging lies\nin the determination of optimal weights from a finite sample. If the weights\nare selected sub-optimally, this can affect the accuracy of the model-averaged\nforecasts. Instead of choosing the optimal weights, we consider trimming a set\nof models before equally averaging forecasts from the selected superior models.\nMotivated by Hansen, Lunde and Nason (2011), we apply and evaluate the model\nconfidence set procedure when combining mortality forecasts. The proposed model\naveraging procedure is motivated by Samuels and Sekkel (2017) based on the\nconcept of model confidence sets as proposed by Hansen et al. (2011) that\nincorporates the statistical significance of the forecasting performance. As\nthe model confidence level increases, the set of superior models generally\ndecreases. The proposed model averaging procedure is demonstrated via national\nand sub-national Japanese mortality for retirement ages between 60 and 100+.\nIllustrated by national and sub-national Japanese mortality for ages between 60\nand 100+, the proposed model-average procedure gives the smallest interval\nforecast errors, especially for males. We find that robust out-of-sample point\nand interval forecasts may be obtained from the trimming method. By robust, we\nmean robustness against model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 03:24:31 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "1809.10882", "submitter": "Xin Ma", "authors": "Wenqing Wu, Xin Ma, Bo Zeng, Yong Wang, Wei Cai", "title": "Application of the novel fractional grey model FAGMO(1,1,k) to predict\n  China's nuclear energy consumption", "comments": null, "journal-ref": "Energy, 2018, vol. 165, pp. 223-234", "doi": "10.1016/j.energy.2018.09.155", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the energy structure of China is shifting towards cleaner and\nlower amounts of carbon fuel, driven by environmental needs and technological\nadvances. Nuclear energy, which is one of the major low-carbon resources, plays\na key role in China's clean energy development. To formulate appropriate energy\npolicies, it is necessary to conduct reliable forecasts. This paper discusses\nthe nuclear energy consumption of China by means of a novel fractional grey\nmodel FAGMO(1,1,k). The fractional accumulated generating matrix is introduced\nto analyse the fractional grey model properties. Thereafter, the modelling\nprocedures of the FAGMO(1,1,k) are presented in detail, along with the\ntransforms of its optimal parameters. A stochastic testing scheme is provided\nto validate the accuracy and properties of the optimal parameters of the\nFAGMO(1,1,k). Finally, this model is used to forecast China's nuclear energy\nconsumption and the results demonstrate that the FAGMO(1,1,k) model provides\naccurate prediction, outperforming other grey models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 07:04:52 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Wu", "Wenqing", ""], ["Ma", "Xin", ""], ["Zeng", "Bo", ""], ["Wang", "Yong", ""], ["Cai", "Wei", ""]]}, {"id": "1809.10925", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy, Carsten Schuett, Elisabeth M. Werner", "title": "Data depth and floating body", "comments": null, "journal-ref": null, "doi": "10.1214/19-SS123", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little known relations of the renown concept of the halfspace depth for\nmultivariate data with notions from convex and affine geometry are discussed.\nHalfspace depth may be regarded as a measure of symmetry for random vectors. As\nsuch, the depth stands as a generalization of a measure of symmetry for convex\nsets, well studied in geometry. Under a mild assumption, the upper level sets\nof the halfspace depth coincide with the convex floating bodies used in the\ndefinition of the affine surface area for convex bodies in Euclidean spaces.\nThese connections enable us to partially resolve some persistent open problems\nregarding theoretical properties of the depth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:11:55 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Nagy", "Stanislav", ""], ["Schuett", "Carsten", ""], ["Werner", "Elisabeth M.", ""]]}, {"id": "1809.11098", "submitter": "Ixavier Higgins", "authors": "Ixavier A Higgins, Ying Guo, Suprateek Kundu, Ki Sueng Choi, Helen\n  Mayberg", "title": "A Differential Degree Test for Comparing Brain Networks", "comments": "35 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph theory has become a popular method for characterizing brain\nfunctional organization. One important goal in graph theoretical analysis of\nbrain networks is to identify network differences across disease types or\nconditions. Typical approaches include massive univariate testing of each edge\nor comparisons of local and/or global network metrics to identify deviations in\ntopological organization. Some limitations of these methods include low\nstatistical power due to the large number of comparisons and difficulty\nattributing overall differences in networks to local variations in brain\nfunction. We propose a novel differential degree test (DDT) to identify brain\nregions incident to a large number of differentially weighted edges across two\npopulations. The proposed test could help detect key brain locations involved\nin diseases by demonstrating significantly altered neural connections. We\nachieve this by generating an appropriate set of null networks which are\nmatched on the first and second moments of the observed difference network\nusing the Hirschberger-Qi-Steuer (HQS) algorithm. This formulation permits\nseparation of the network's true topology from the nuisance topology which is\ninduced by the correlation measure and may drive inter-regional connectivity in\nways unrelated to the brain function. Simulations indicate that the proposed\napproach routinely outperforms competing methods in detecting differentially\nconnected regions of interest. Furthermore, we propose a data-adaptive\nthreshold selection procedure which is able to detect differentially weighted\nedges and is shown to outperform competing methods that perform edge-wise\ncomparisons controlling for the error rate. An application of our method to a\nmajor depressive disorder dataset leads to the identification of brain regions\nin the default mode network commonly implicated in this ruminative disorder.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:43:49 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Higgins", "Ixavier A", ""], ["Guo", "Ying", ""], ["Kundu", "Suprateek", ""], ["Choi", "Ki Sueng", ""], ["Mayberg", "Helen", ""]]}]