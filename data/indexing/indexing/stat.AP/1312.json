[{"id": "1312.0084", "submitter": "Alberto Baccini", "authors": "Alberto Baccini, Lucio Barabesi, Martina Cioni, Caterina Pisani", "title": "Crossing the hurdle: the determinants of individual scientific\n  performance", "comments": "Revised version accepted for publication by Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-014-1395-3", "report-no": null, "categories": "physics.soc-ph cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An original cross sectional dataset referring to a medium sized Italian\nuniversity is implemented in order to analyze the determinants of scientific\nresearch production at individual level. The dataset includes 942 permanent\nresearchers of various scientific sectors for a three year time span (2008 -\n2010). Three different indicators - based on the number of publications or\ncitations - are considered as response variables. The corresponding\ndistributions are highly skewed and display an excess of zero - valued\nobservations. In this setting, the goodness of fit of several Poisson mixture\nregression models are explored by assuming an extensive set of explanatory\nvariables. As to the personal observable characteristics of the researchers,\nthe results emphasize the age effect and the gender productivity gap, as\npreviously documented by existing studies. Analogously, the analysis confirm\nthat productivity is strongly affected by the publication and citation\npractices adopted in different scientific disciplines. The empirical evidence\non the connection between teaching and research activities suggests that no\nunivocal substitution or complementarity thesis can be claimed: a major\nteaching load does not affect the odds to be a non-active researcher and does\nnot significantly reduce the number of publications for active researchers. In\naddition, new evidence emerges on the effect of researchers administrative\ntasks, which seem to be negatively related with researcher's productivity, and\non the composition of departments. Researchers' productivity is apparently\nenhanced by operating in department filled with more administrative and\ntechnical staff, and it is not significantly affected by the composition of the\ndepartment in terms of senior or junior researchers.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 10:20:15 GMT"}, {"version": "v2", "created": "Sun, 27 Jul 2014 14:30:20 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Baccini", "Alberto", ""], ["Barabesi", "Lucio", ""], ["Cioni", "Martina", ""], ["Pisani", "Caterina", ""]]}, {"id": "1312.0365", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "The Law of Total Odds", "comments": "12 pages, 1 figure, new references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The law of total probability may be deployed in binary classification\nexercises to estimate the unconditional class probabilities if the class\nproportions in the training set are not representative of the population class\nproportions. We argue that this is not a conceptually sound approach and\nsuggest an alternative based on the new law of total odds. We quantify the bias\nof the total probability estimator of the unconditional class probabilities and\nshow that the total odds estimator is unbiased. The sample version of the total\nodds estimator is shown to coincide with a maximum-likelihood estimator known\nfrom the literature. The law of total odds can also be used for transforming\nthe conditional class probabilities if independent estimates of the\nunconditional class probabilities of the population are available.\n  Keywords: Total probability, likelihood ratio, Bayes' formula, binary\nclassification, relative odds, unbiased estimator, supervised learning, dataset\nshift.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 07:54:15 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 19:10:11 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2013 19:01:11 GMT"}, {"version": "v4", "created": "Sat, 18 Jan 2014 22:36:12 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2014 17:54:41 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1312.0390", "submitter": "Wanlu Deng", "authors": "Wanlu Deng, Zhi Geng, Hongzhe Li", "title": "Learning local directed acyclic graphs based on multivariate time series\n  data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS635 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1663-1683", "doi": "10.1214/13-AOAS635", "report-no": "IMS-AOAS-AOAS635", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series (MTS) data such as time course gene expression data\nin genomics are often collected to study the dynamic nature of the systems.\nThese data provide important information about the causal dependency among a\nset of random variables. In this paper, we introduce a computationally\nefficient algorithm to learn directed acyclic graphs (DAGs) based on MTS data,\nfocusing on learning the local structure of a given target variable. Our\nalgorithm is based on learning all parents (P), all children (C) and some\ndescendants (D) (PCD) iteratively, utilizing the time order of the variables to\norient the edges. This time series PCD-PCD algorithm (tsPCD-PCD) extends the\nprevious PCD-PCD algorithm to dependent observations and utilizes composite\nlikelihood ratio tests (CLRTs) for testing the conditional independence. We\npresent the asymptotic distribution of the CLRT statistic and show that the\ntsPCD-PCD is guaranteed to recover the true DAG structure when the faithfulness\ncondition holds and the tests correctly reject the null hypotheses. Simulation\nstudies show that the CLRTs are valid and perform well even when the sample\nsizes are small. In addition, the tsPCD-PCD algorithm outperforms the PCD-PCD\nalgorithm in recovering the local graph structures. We illustrate the algorithm\nby analyzing a time course gene expression data related to mouse T-cell\nactivation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 09:52:50 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Deng", "Wanlu", ""], ["Geng", "Zhi", ""], ["Li", "Hongzhe", ""]]}, {"id": "1312.0401", "submitter": "Alireza Daneshkhah Alireza Daneshkhah", "authors": "Fatemeh Shahsanaei and Alireza Daneshkhah", "title": "Estimation of Stress-Strength model in the Generalized Linear Failure\n  Rate Distribution", "comments": "31 pages, 2 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation of $R=P [Y < X ]$, also so-called the\nstress-strength model, when both $X$ and $Y$ are two independent random\nvariables with the generalized linear failure rate distributions, under\ndifferent assumptions about their parameters. We address the maximum likelihood\nestimator (MLE) of $R$ and the associated asymptotic confidence interval. In\naddition, we compute the MLE and the corresponding Bootstrap confidence\ninterval when the sample sizes are small. The Bayes estimates of $R$ and the\nassociated credible intervals are also investigated. An extensive computer\nsimulation is implemented to compare the performances of the proposed\nestimators. Eventually, we briefly study the estimation of this model when the\ndata obtained from both distributions are progressively type-II censored. We\npresent the MLE and the corresponding confidence interval under three different\nprogressive censoring schemes. We also analysis a set of real data for\nillustrative purpose.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 10:27:34 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Shahsanaei", "Fatemeh", ""], ["Daneshkhah", "Alireza", ""]]}, {"id": "1312.0506", "submitter": "Marie Kratz", "authors": "Marc Busse, Michel Dacorogna, Marie Kratz", "title": "The impact of systemic risk on the diversification benefits of a risk\n  portfolio", "comments": "17 pages, 5 tableaux", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk diversification is the basis of insurance and investment. It is thus\ncrucial to study the effects that could limit it. One of them is the existence\nof systemic risk that affects all the policies at the same time. We introduce\nhere a probabilistic approach to examine the consequences of its presence on\nthe risk loading of the premium of a portfolio of insurance policies. This\napproach could be easily generalized for investment risk. We see that, even\nwith a small probability of occurrence, systemic risk can reduce dramatically\nthe diversification benefits. It is clearly revealed via a non-diversifiable\nterm that appears in the analytical expression of the variance of our models.\nWe propose two ways of introducing it and discuss their advantages and\nlimitations. By using both VaR and TVaR to compute the loading, we see that\nonly the latter captures the full effect of systemic risk when its probability\nto occur is low\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:30:09 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Busse", "Marc", ""], ["Dacorogna", "Michel", ""], ["Kratz", "Marie", ""]]}, {"id": "1312.0516", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick", "title": "Grid Topology Identification using Electricity Prices", "comments": "PES General Meeting 2014 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of recovering the topology of a grid using solely publicly\navailable market data is explored here. In contemporary whole-sale electricity\nmarkets, real-time prices are typically determined by solving the\nnetwork-constrained economic dispatch problem. Under a linear DC model,\nlocational marginal prices (LMPs) correspond to the Lagrange multipliers of the\nlinear program involved. The interesting observation here is that the matrix of\nspatiotemporally varying LMPs exhibits the following property: Once\npremultiplied by the weighted grid Laplacian, it yields a low-rank and sparse\nmatrix. Leveraging this rich structure, a regularized maximum likelihood\nestimator (MLE) is developed to recover the grid Laplacian from the LMPs. The\nconvex optimization problem formulated includes low rank- and\nsparsity-promoting regularizers, and it is solved using a scalable algorithm.\nNumerical tests on prices generated for the IEEE 14-bus benchmark provide\nencouraging topology recovery results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:58:10 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 00:35:43 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""], ["Baldick", "Ross", ""]]}, {"id": "1312.0538", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Francesco Giordano", "title": "Nonparametric estimation of the dynamic range of music signals", "comments": null, "journal-ref": "2017, Australian & New Zealand Journal of Statistics, Vol. 59(4),\n  pp. 389-412", "doi": "10.1111/anzs.12217", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic range is an important parameter which measures the spread of\nsound power, and for music signals it is a measure of recording quality. There\nare various descriptive measures of sound power, none of which has strong\nstatistical foundations. We start from a nonparametric model for sound waves\nwhere an additive stochastic term has the role to catch transient energy. This\ncomponent is recovered by a simple rate-optimal kernel estimator that requires\na single data-driven tuning. The distribution of its variance is approximated\nby a consistent random subsampling method that is able to cope with the massive\nsize of the typical dataset. Based on the latter, we propose a statistic, and\nan estimation method that is able to represent the dynamic range concept\nconsistently. The behavior of the statistic is assessed based on a large\nnumerical experiment where we simulate dynamic compression on a selection of\nreal music signals. Application of the method to real data also shows how the\nproposed method can predict subjective experts' opinions about the hifi quality\nof a recording.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 18:24:28 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 15:37:58 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 17:10:17 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 09:57:57 GMT"}, {"version": "v5", "created": "Sat, 4 Apr 2015 12:47:31 GMT"}, {"version": "v6", "created": "Wed, 2 Sep 2015 08:22:14 GMT"}, {"version": "v7", "created": "Thu, 6 Oct 2016 15:48:58 GMT"}, {"version": "v8", "created": "Wed, 14 Feb 2018 07:53:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Coretto", "Pietro", ""], ["Giordano", "Francesco", ""]]}, {"id": "1312.0594", "submitter": "Marcos Capistran  Dr", "authors": "Yendry N. Arguedas-Flatts, Marcos A. Capistr\\'an, J. Andr\\'es\n  Christen, Daniel E. Noyola", "title": "An analysis of the interaction between influenza and respiratory\n  syncytial virus based on acute respiratory infection records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the hypothesis that both influenza and respiratory syncytial virus\n(RSV) are the two leading causes of acute respiratory infections (ARI), in this\npaper we have used a standard two-pathogen epidemic model as a regressor to\nexplain, on a yearly basis, high season ARI data in terms of the contact rates\nand initial conditions of the mathematical model. The rationale is that ARI\nhigh season is a transient regime of a noisy system, e.g., the system is driven\naway from equilibrium every year by fluctuations in variables such as humidity,\ntemperature, viral mutations and human behavior. Using the value of the\nreplacement number as a phenotypic trait associated to fitness, we provide\nevidence that influenza and RSV coexists throughout the ARI high season through\nsuperinfection.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 00:13:36 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Arguedas-Flatts", "Yendry N.", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Christen", "J. Andr\u00e9s", ""], ["Noyola", "Daniel E.", ""]]}, {"id": "1312.0919", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer, Tom M. Elliott", "title": "Hierarchical Reverberation Mapping", "comments": "Accepted for publication in MNRAS Letters. 5 pages, 5 figures. Source\n  code for this paper is available at https://github.com/eggplantbren/RMHB", "journal-ref": null, "doi": "10.1093/mnrasl/slt174", "report-no": null, "categories": "astro-ph.IM astro-ph.CO physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverberation mapping (RM) is an important technique in studies of active\ngalactic nuclei (AGN). The key idea of RM is to measure the time lag $\\tau$\nbetween variations in the continuum emission from the accretion disc and\nsubsequent response of the broad line region (BLR). The measurement of $\\tau$\nis typically used to estimate the physical size of the BLR and is combined with\nother measurements to estimate the black hole mass $M_{\\rm BH}$. A major\ndifficulty with RM campaigns is the large amount of data needed to measure\n$\\tau$. Recently, Fine et al (2012) introduced a new approach to RM where the\nBLR light curve is sparsely sampled, but this is counteracted by observing a\nlarge sample of AGN, rather than a single system. The results are combined to\ninfer properties of the sample of AGN. In this letter we implement this method\nusing a hierarchical Bayesian model and contrast this with the results from the\nprevious stacked cross-correlation technique. We find that our inferences are\nmore precise and allow for more straightforward interpretation than the stacked\ncross-correlation results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 20:23:31 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Elliott", "Tom M.", ""]]}, {"id": "1312.1088", "submitter": "Rajesh  Singh", "authors": "Sachin Malik, Jayant Singh and Rajesh Singh", "title": "A family of estimators for estimating the population mean in simple\n  random sampling under measurement errors", "comments": "9 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we have suggested an improved estimator for estimating the\npopulation mean in simple random sampling using auxiliary information under the\npresence of measurement errors. The mean square error (MSE) of the proposed\nestimator has been derived under large sample approximation. Besides,\nconsidering the minimum case of the MSE equation, the efficient conditions\nbetween the proposed and existing estimators are obtained. These theoretical\nfindings are supported by a numerical example.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 10:00:45 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Malik", "Sachin", ""], ["Singh", "Jayant", ""], ["Singh", "Rajesh", ""]]}, {"id": "1312.1268", "submitter": "Peter Aronow", "authors": "Peter M. Aronow, Alexander Coppock, Forrest W. Crawford, Donald P.\n  Green", "title": "Combining List Experiment and Direct Question Estimates of Sensitive\n  Behavior Prevalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey respondents may give untruthful answers to sensitive questions when\nasked directly. In recent years, researchers have turned to the list experiment\n(also known as the item count technique) to overcome this difficulty. While\nlist experiments may be less prone to bias than direct questioning, list\nexperiments are also more susceptible to sampling variability. We show that\nresearchers do not have to abandon direct questioning altogether in order to\ngain the advantages of list experimentation. We develop a nonparametric\nestimator of the prevalence of sensitive behaviors that combines list\nexperimentation and direct questioning. We prove that this estimator is\nasymptotically more efficient than the standard difference-in-means estimator,\nand we provide a basis for inference using Wald-type confidence intervals.\nAdditionally, leveraging information from the direct questioning, we derive two\nnonparametric placebo tests of the identifying assumptions for the list\nexperiment. We demonstrate the effectiveness of our combined estimator and\nplacebo tests with an original survey experiment.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 18:21:48 GMT"}, {"version": "v2", "created": "Sun, 1 Jun 2014 23:29:17 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Aronow", "Peter M.", ""], ["Coppock", "Alexander", ""], ["Crawford", "Forrest W.", ""], ["Green", "Donald P.", ""]]}, {"id": "1312.1548", "submitter": "Thomas Rusch", "authors": "Thomas Rusch, Paul Hofmarcher, Reinhold Hatzinger, Kurt Hornik", "title": "Model trees with topic model preprocessing: An approach for data\n  journalism illustrated with the WikiLeaks Afghanistan war logs", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS618 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 613-639", "doi": "10.1214/12-AOAS618", "report-no": "IMS-AOAS-AOAS618", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WikiLeaks Afghanistan war logs contain nearly $77,000$ reports of\nincidents in the US-led Afghanistan war, covering the period from January 2004\nto December 2009. The recent growth of data on complex social systems and the\npotential to derive stories from them has shifted the focus of journalistic and\nscientific attention increasingly toward data-driven journalism and\ncomputational social science. In this paper we advocate the usage of modern\nstatistical methods for problems of data journalism and beyond, which may help\njournalistic and scientific work and lead to additional insight. Using the\nWikiLeaks Afghanistan war logs for illustration, we present an approach that\nbuilds intelligible statistical models for interpretable segments in the data,\nin this case to explore the fatality rates associated with different\ncircumstances in the Afghanistan war. Our approach combines preprocessing by\nLatent Dirichlet Allocation (LDA) with model trees. LDA is used to process the\nnatural language information contained in each report summary by estimating\nlatent topics and assigning each report to one of them. Together with other\nvariables these topic assignments serve as splitting variables for finding\nsegments in the data to which local statistical models for the reported number\nof fatalities are fitted. Segmentation and fitting is carried out with\nrecursive partitioning of negative binomial distributions. We identify segments\nwith different fatality rates that correspond to a small number of topics and\nother variables as well as their interactions. Furthermore, we carve out the\nsimilarities between segments and connect them to stories that have been\ncovered in the media. This gives an unprecedented description of the war in\nAfghanistan and serves as an example of how data journalism, computational\nsocial science and other areas with interest in database data can benefit from\nmodern statistical techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 13:56:41 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Rusch", "Thomas", ""], ["Hofmarcher", "Paul", ""], ["Hatzinger", "Reinhold", ""], ["Hornik", "Kurt", ""]]}, {"id": "1312.1560", "submitter": "Bledar A. Konomi", "authors": "Bledar A. Konomi, Soma S. Dhavala, Jianhua Z. Huang, Subrata Kundu,\n  David Huitink, Hong Liang, Yu Ding, Bani K. Mallick", "title": "Bayesian object classification of gold nanoparticles", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS616 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 640-668", "doi": "10.1214/12-AOAS616", "report-no": "IMS-AOAS-AOAS616", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The properties of materials synthesized with nanoparticles (NPs) are highly\ncorrelated to the sizes and shapes of the nanoparticles. The transmission\nelectron microscopy (TEM) imaging technique can be used to measure the\nmorphological characteristics of NPs, which can be simple circles or more\ncomplex irregular polygons with varying degrees of scales and sizes. A major\ndifficulty in analyzing the TEM images is the overlapping of objects, having\ndifferent morphological properties with no specific information about the\nnumber of objects present. Furthermore, the objects lying along the boundary\nrender automated image analysis much more difficult. To overcome these\nchallenges, we propose a Bayesian method based on the marked-point process\nrepresentation of the objects. We derive models, both for the marks which\nparameterize the morphological aspects and the points which determine the\nlocation of the objects. The proposed model is an automatic image segmentation\nand classification procedure, which simultaneously detects the boundaries and\nclassifies the NPs into one of the predetermined shape families. We execute the\ninference by sampling the posterior distribution using Markov chain Monte Carlo\n(MCMC) since the posterior is doubly intractable. We apply our novel method to\nseveral TEM imaging samples of gold NPs, producing the needed statistical\ncharacterization of their morphology.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 14:26:29 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Konomi", "Bledar A.", ""], ["Dhavala", "Soma S.", ""], ["Huang", "Jianhua Z.", ""], ["Kundu", "Subrata", ""], ["Huitink", "David", ""], ["Liang", "Hong", ""], ["Ding", "Yu", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1312.1653", "submitter": "Emmanuel Lesigne", "authors": "Julie Oger (LMPT), Emmanuel Lesigne (LMPT), Philippe Leduc", "title": "A Random Field Model and its Application in Industrial Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In competitive industries, a reliable yield forecasting is a prime factor to\naccurately determine the production costs and therefore ensure profitability.\nIndeed, quantifying the risks long before the effective manufacturing process\nenables fact-based decision-making. From the development stage, improvement\nefforts can be early identified and prioritized. In order to measure the impact\nof industrial process fluctuations on the product performances, the\nconstruction of a failure risk probability estimator is presented in this\narticle. The complex relationship between the process technology and the\nproduct design (non linearities, multi-modal features...) is handled via random\nprocess regression. A random field encodes, for each product configuration, the\navailable information regarding the risk of non-compliance. After a brief\npresentation of the Gaussian model approach, we describe a Bayesian reasoning\navoiding a priori choices of location and scale parameters. The Gaussian\nmixture prior, conditioned by measured (or calculated) data, yields a posterior\ncharacterized by a multivariate Student distribution. The probabilistic nature\nof the model is then operated to derive a failure risk probability, defined as\na random variable. To do this, our approach is to consider as random all\nunknown, inaccessible or fluctuating data. In order to propagate uncertainties,\na fuzzy set approach provides an appropriate framework for the implementation\nof a Bayesian model mimicking expert elicitation. The underlying leitmotiv is\nto insert minimal a priori information in the failure risk model. The relevancy\nof this concept is illustrated with theoretical examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 19:43:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Oger", "Julie", "", "LMPT"], ["Lesigne", "Emmanuel", "", "LMPT"], ["Leduc", "Philippe", ""]]}, {"id": "1312.1670", "submitter": "Kristian Lum", "authors": "Kristian Lum, Samarth Swarup, Stephen Eubank, James Hawdon", "title": "An agent-based epidemiological model of incarceration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build an agent-based model of incarceration based on the SIS model of\ninfectious disease propagation. Our central hypothesis is that the observed\nracial disparities in incarceration rates between Black and White Americans can\nbe explained as the result of differential sentencing between the two\ndemographic groups. We demonstrate that if incarceration can be spread through\na social influence network, then even relatively small differences in\nsentencing can result in the large disparities in incarceration rates.\nControlling for effects of transmissibility, susceptibility, and influence\nnetwork structure, our model reproduces the observed large disparities in\nincarceration rates given the differences in sentence lengths for White and\nBlack drug offenders in the United States without extensive parameter tuning.\nWe further establish the suitability of the SIS model as applied to\nincarceration, as the observed structural patterns of recidivism are an\nemergent property of the model. In fact, our model shows a remarkably close\ncorrespondence with California incarceration data, without requiring any\nparameter tuning. This work advances efforts to combine the theories and\nmethods of epidemiology and criminology.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 20:12:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Lum", "Kristian", ""], ["Swarup", "Samarth", ""], ["Eubank", "Stephen", ""], ["Hawdon", "James", ""]]}, {"id": "1312.1794", "submitter": "Cristiano Varin", "authors": "Cristiano Varin, Manuela Cattelan, David Firth", "title": "Statistical Modelling of Citation Exchange Between Statistics Journals", "comments": "To be published with discussion on Journal of the Royal Statistical\n  Society Series A", "journal-ref": "Journal of the Royal Statistical Society Series A Volume 179,\n  Issue 1 Pages 1 - 318, January 2016", "doi": "10.1111/rssa.12124", "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rankings of scholarly journals based on citation data are often met with\nskepticism by the scientific community. Part of the skepticism is due to\ndisparity between the common perception of journals' prestige and their ranking\nbased on citation counts. A more serious concern is the inappropriate use of\njournal rankings to evaluate the scientific influence of authors. This paper\nfocuses on analysis of the table of cross-citations among a selection of\nStatistics journals. Data are collected from the Web of Science database\npublished by Thomson Reuters. Our results suggest that modelling the exchange\nof citations between journals is useful to highlight the most prestigious\njournals, but also that journal citation data are characterized by considerable\nheterogeneity, which needs to be properly summarized. Inferential conclusions\nrequire care in order to avoid potential over-interpretation of insignificant\ndifferences between journal ratings. Comparison with published ratings of\ninstitutions from the UK's Research Assessment Exercise shows strong\ncorrelation at aggregate level between assessed research quality and journal\ncitation `export scores' within the discipline of Statistics.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 07:58:12 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 14:39:03 GMT"}, {"version": "v3", "created": "Sat, 29 Nov 2014 10:16:08 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 20:27:30 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Varin", "Cristiano", ""], ["Cattelan", "Manuela", ""], ["Firth", "David", ""]]}, {"id": "1312.1795", "submitter": "Gwena\\\"{e}l G. R. Leday", "authors": "Gwena\\\"el G. R. Leday, Aad W. van der Vaart, Wessel N. van Wieringen,\n  Mark A. van de Wiel", "title": "Modeling association between DNA copy number and gene expression with\n  constrained piecewise linear regression splines", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS605 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 823-845", "doi": "10.1214/12-AOAS605", "report-no": "IMS-AOAS-AOAS605", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA copy number and mRNA expression are widely used data types in cancer\nstudies, which combined provide more insight than separately. Whereas in\nexisting literature the form of the relationship between these two types of\nmarkers is fixed a priori, in this paper we model their association. We employ\npiecewise linear regression splines (PLRS), which combine good interpretation\nwith sufficient flexibility to identify any plausible type of relationship. The\nspecification of the model leads to estimation and model selection in a\nconstrained, nonstandard setting. We provide methodology for testing the effect\nof DNA on mRNA and choosing the appropriate model. Furthermore, we present a\nnovel approach to obtain reliable confidence bands for constrained PLRS, which\nincorporates model uncertainty. The procedures are applied to colorectal and\nbreast cancer data. Common assumptions are found to be potentially misleading\nfor biologically relevant genes. More flexible models may bring more insight in\nthe interaction between the two markers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 08:12:38 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Leday", "Gwena\u00ebl G. R.", ""], ["van der Vaart", "Aad W.", ""], ["van Wieringen", "Wessel N.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1312.1797", "submitter": "Joseph B. Kadane", "authors": "Joseph B. Kadane, Ferdinand L. N{\\ae}shagen", "title": "The number of killings in southern rural Norway, 1300-1569", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS612 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 846-859", "doi": "10.1214/12-AOAS612", "report-no": "IMS-AOAS-AOAS612", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dual systems estimates are employed to study the number of killings in\nsouthern rural Norway in a period of slightly over 250 years. The first system\nis a set of five letters sent to each killer as part of the legal process. The\nsecond system is the mention of killings from all other contemporary sources.\nThe posterior distributions derived suggest fewer such killings than rough\ndemographic estimates.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 08:24:16 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kadane", "Joseph B.", ""], ["N\u00e6shagen", "Ferdinand L.", ""]]}, {"id": "1312.1801", "submitter": "Travis L. Gaydos", "authors": "Travis L. Gaydos, Nancy E. Heckman, Mark Kirkpatrick, J. R.\n  Stinchcombe, Johanna Schmitt, Joel Kingsolver, J. S. Marron", "title": "Visualizing genetic constraints", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS603 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 860-882", "doi": "10.1214/12-AOAS603", "report-no": "IMS-AOAS-AOAS603", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Components Analysis (PCA) is a common way to study the sources of\nvariation in a high-dimensional data set. Typically, the leading principal\ncomponents are used to understand the variation in the data or to reduce the\ndimension of the data for subsequent analysis. The remaining principal\ncomponents are ignored since they explain little of the variation in the data.\nHowever, evolutionary biologists gain important insights from these low\nvariation directions. Specifically, they are interested in directions of low\ngenetic variability that are biologically interpretable. These directions are\ncalled genetic constraints and indicate directions in which a trait cannot\nevolve through selection. Here, we propose studying the subspace spanned by low\nvariance principal components by determining vectors in this subspace that are\nsimplest. Our method and accompanying graphical displays enhance the\nbiologist's ability to visualize the subspace and identify interpretable\ndirections of low genetic variability that align with simple directions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 08:38:00 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Gaydos", "Travis L.", ""], ["Heckman", "Nancy E.", ""], ["Kirkpatrick", "Mark", ""], ["Stinchcombe", "J. R.", ""], ["Schmitt", "Johanna", ""], ["Kingsolver", "Joel", ""], ["Marron", "J. S.", ""]]}, {"id": "1312.1809", "submitter": "Jie Ding", "authors": "Jie Ding, Lorenzo Trippa, Xiaogang Zhong, Giovanni Parmigiani", "title": "Hierarchical Bayesian analysis of somatic mutation data in cancer", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS604 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 883-903", "doi": "10.1214/12-AOAS604", "report-no": "IMS-AOAS-AOAS604", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying genes underlying cancer development is critical to cancer biology\nand has important implications across prevention, diagnosis and treatment.\nCancer sequencing studies aim at discovering genes with high frequencies of\nsomatic mutations in specific types of cancer, as these genes are potential\ndriving factors (drivers) for cancer development. We introduce a hierarchical\nBayesian methodology to estimate gene-specific mutation rates and driver\nprobabilities from somatic mutation data and to shed light on the overall\nproportion of drivers among sequenced genes. Our methodology applies to\ndifferent experimental designs used in practice, including one-stage, two-stage\nand candidate gene designs. Also, sample sizes are typically small relative to\nthe rarity of individual mutations. Via a shrinkage method borrowing strength\nfrom the whole genome in assessing individual genes, we reinforce inference and\naddress the selection effects induced by multistage designs. Our simulation\nstudies show that the posterior driver probabilities provide a nearly unbiased\nfalse discovery rate estimate. We apply our methods to pancreatic and breast\ncancer data, contrast our results to previous estimates and provide estimated\nproportions of drivers for these two types of cancer.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 09:16:20 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Ding", "Jie", ""], ["Trippa", "Lorenzo", ""], ["Zhong", "Xiaogang", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "1312.1816", "submitter": "Brian Reich", "authors": "Brian Reich, Daniel Cooley, Kristen Foley, Sergey Napelenok, Benjamin\n  Shaby", "title": "Extreme value analysis for evaluating ozone control strategies", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS628 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 739-762", "doi": "10.1214/13-AOAS628", "report-no": "IMS-AOAS-AOAS628", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropospheric ozone is one of six criteria pollutants regulated by the US EPA,\nand has been linked to respiratory and cardiovascular endpoints and adverse\neffects on vegetation and ecosystems. Regional photochemical models have been\ndeveloped to study the impacts of emission reductions on ozone levels. The\nstandard approach is to run the deterministic model under new emission levels\nand attribute the change in ozone concentration to the emission control\nstrategy. However, running the deterministic model requires substantial\ncomputing time, and this approach does not provide a measure of uncertainty for\nthe change in ozone levels. Recently, a reduced form model (RFM) has been\nproposed to approximate the complex model as a simple function of a few\nrelevant inputs. In this paper, we develop a new statistical approach to make\nfull use of the RFM to study the effects of various control strategies on the\nprobability and magnitude of extreme ozone events. We fuse the model output\nwith monitoring data to calibrate the RFM by modeling the conditional\ndistribution of monitoring data given the RFM using a combination of flexible\nsemiparametric quantile regression for the center of the distribution where\ndata are abundant and a parametric extreme value distribution for the tail\nwhere data are sparse. Selected parameters in the conditional distribution are\nallowed to vary by the RFM value and the spatial location. Also, due to the\nsimplicity of the RFM, we are able to embed the RFM in our Bayesian\nhierarchical framework to obtain a full posterior for the model input\nparameters, and propagate this uncertainty to the estimation of the effects of\nthe control strategies. We use the new framework to evaluate three potential\ncontrol strategies, and find that reducing mobile-source emissions has a larger\nimpact than reducing point-source emissions or a combination of several\nemission sources.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 09:57:41 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Reich", "Brian", ""], ["Cooley", "Daniel", ""], ["Foley", "Kristen", ""], ["Napelenok", "Sergey", ""], ["Shaby", "Benjamin", ""]]}, {"id": "1312.1818", "submitter": "Vinicius Diniz Mayrink", "authors": "Vinicius Diniz Mayrink, Joseph Edward Lucas", "title": "Sparse latent factor models with interactions: Analysis of gene\n  expression data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS607 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 799-822", "doi": "10.1214/12-AOAS607", "report-no": "IMS-AOAS-AOAS607", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse latent multi-factor models have been used in many exploratory and\npredictive problems with high-dimensional multivariate observations. Because of\nconcerns with identifiability, the latent factors are almost always assumed to\nbe linearly related to measured feature variables. Here we explore the analysis\nof multi-factor models with different structures of interactions between latent\nfactors, including multiplicative effects as well as a more general framework\nfor nonlinear interactions introduced via the Gaussian Process. We utilize\nsparsity priors to test whether the factors and interaction terms have\nsignificant effect. The performance of the models is evaluated through\nsimulated and real data applications in genomics. Variation in the number of\ncopies of regions of the genome is a well-known and important feature of most\ncancers. We examine interactions between factors directly associated with\ndifferent chromosomal regions detected with copy number alteration in breast\ncancer data. In this context, significant interaction effects for specific\ngenes suggest synergies between duplications and deletions in different regions\nof the chromosome.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 10:00:37 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Mayrink", "Vinicius Diniz", ""], ["Lucas", "Joseph Edward", ""]]}, {"id": "1312.1840", "submitter": "Kanti V. Mardia", "authors": "Kanti V. Mardia, Christopher J. Fallaize, Stuart Barber, Richard M.\n  Jackson, Douglas L. Theobald", "title": "Bayesian alignment of similarity shapes", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS615 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 989-1009", "doi": "10.1214/12-AOAS615", "report-no": "IMS-AOAS-AOAS615", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian model for the alignment of two point configurations\nunder the full similarity transformations of rotation, translation and scaling.\nOther work in this area has concentrated on rigid body transformations, where\nscale information is preserved, motivated by problems involving molecular data;\nthis is known as form analysis. We concentrate on a Bayesian formulation for\nstatistical shape analysis. We generalize the model introduced by Green and\nMardia [Biometrika 93 (2006) 235-254] for the pairwise alignment of two\nunlabeled configurations to full similarity transformations by introducing a\nscaling factor to the model. The generalization is not straightforward, since\nthe model needs to be reformulated to give good performance when scaling is\nincluded. We illustrate our method on the alignment of rat growth profiles and\na novel application to the alignment of protein domains. Here, scaling is\napplied to secondary structure elements when comparing protein folds;\nadditionally, we find that one global scaling factor is not in general\nsufficient to model these data and, hence, we develop a model in which multiple\nscale factors can be included to handle different scalings of shape components.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 12:20:34 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Mardia", "Kanti V.", ""], ["Fallaize", "Christopher J.", ""], ["Barber", "Stuart", ""], ["Jackson", "Richard M.", ""], ["Theobald", "Douglas L.", ""]]}, {"id": "1312.1856", "submitter": "Terrance D. Savitsky", "authors": "Terrance D. Savitsky, Susan M. Paddock", "title": "Bayesian nonparametric hierarchical modeling for multiple membership\n  data in grouped attendance interventions", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS620 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1074-1094", "doi": "10.1214/12-AOAS620", "report-no": "IMS-AOAS-AOAS620", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a dependent Dirichlet process (DDP) model for repeated measures\nmultiple membership (MM) data. This data structure arises in studies under\nwhich an intervention is delivered to each client through a sequence of\nelements which overlap with those of other clients on different occasions. Our\ninterest concentrates on study designs for which the overlaps of sequences\noccur for clients who receive an intervention in a shared or grouped fashion\nwhose memberships may change over multiple treatment events. Our motivating\napplication focuses on evaluation of the effectiveness of a group therapy\nintervention with treatment delivered through a sequence of cognitive\nbehavioral therapy session blocks, called modules. An open-enrollment protocol\npermits entry of clients at the beginning of any new module in a manner that\nmay produce unique MM sequences across clients. We begin with a model that\ncomposes an addition of client and multiple membership module random effect\nterms, which are assumed independent. Our MM DDP model relaxes the assumption\nof conditionally independent client and module random effects by specifying a\ncollection of random distributions for the client effect parameters that are\nindexed by the unique set of module attendances. We demonstrate how this\nconstruction facilitates examining heterogeneity in the relative effectiveness\nof group therapy modules over repeated measurement occasions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 13:34:04 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Paddock", "Susan M.", ""]]}, {"id": "1312.1873", "submitter": "Bradford S. Westgate", "authors": "Bradford S. Westgate, Dawn B. Woodard, David S. Matteson, Shane G.\n  Henderson", "title": "Travel time estimation for ambulances using Bayesian data augmentation", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS626 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1139-1161", "doi": "10.1214/13-AOAS626", "report-no": "IMS-AOAS-AOAS626", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian model for estimating the distribution of ambulance\ntravel times on each road segment in a city, using Global Positioning System\n(GPS) data. Due to sparseness and error in the GPS data, the exact ambulance\npaths and travel times on each road segment are unknown. We simultaneously\nestimate the paths, travel times, and parameters of each road segment travel\ntime distribution using Bayesian data augmentation. To draw ambulance path\nsamples, we use a novel reversible jump Metropolis-Hastings step. We also\nintroduce two simpler estimation methods based on GPS speed data. We compare\nthese methods to a recently published travel time estimation method, using\nsimulated data and data from Toronto EMS. In both cases, out-of-sample point\nand interval estimates of ambulance trip times from the Bayesian method\noutperform estimates from the alternative methods. We also construct\nprobability-of-coverage maps for ambulances. The Bayesian method gives more\nrealistic maps than the recently published method. Finally, path estimates from\nthe Bayesian method interpolate well between sparsely recorded GPS readings and\nare robust to GPS location errors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 14:33:25 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Westgate", "Bradford S.", ""], ["Woodard", "Dawn B.", ""], ["Matteson", "David S.", ""], ["Henderson", "Shane G.", ""]]}, {"id": "1312.1876", "submitter": "Vince Grolmusz", "authors": "Balazs Szalkai, Vince K. Grolmusz, Vince I. Grolmusz, Coalition\n  Against Major Diseases", "title": "Identifying Combinatorial Biomarkers by Association Rule Mining in the\n  CAMD Alzheimer's Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The concept of combinatorial biomarkers was conceived around\n2010: it was noticed that simple biomarkers are often inadequate for\nrecognizing and characterizing complex diseases.\n  Methods: Here we present an algorithmic search method for complex biomarkers\nwhich may predict or indicate Alzheimer's disease (AD) and other kinds of\ndementia. We applied data mining techniques that are capable to uncover\nimplication-like logical schemes with detailed quality scoring. Our program\nSCARF is capable of finding multi-factor relevant association rules\nautomatically. The new SCARF program was applied for the Tucson, Arizona based\nCritical Path Institute's CAMD database, containing laboratory and cognitive\ntest data for more than 6000 patients from the placebo arm of clinical trials\nof large pharmaceutical companies, and consequently, the data is much more\nreliable than numerous other databases for dementia.\n  Results: The results suggest connections between liver enzyme-, B12 vitamin-,\nsodium- and cholesterol levels and dementia, and also some hematologic\nparameter-levels and dementia.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 14:55:53 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Szalkai", "Balazs", ""], ["Grolmusz", "Vince K.", ""], ["Grolmusz", "Vince I.", ""], ["Diseases", "Coalition Against Major", ""]]}, {"id": "1312.2041", "submitter": "John Storey", "authors": "Wei Hao, Minsun Song, and John D. Storey", "title": "Probabilistic models of genetic variation in structured populations\n  applied to global human studies", "comments": "Wei Hao and Minsun Song contributed equally to this work", "journal-ref": null, "doi": "10.1093/bioinformatics/btv641", "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern population genetics studies typically involve genome-wide genotyping\nof individuals from a diverse network of ancestries. An important, unsolved\nproblem is how to formulate and estimate probabilistic models of observed\ngenotypes that allow for complex population structure. We formulate two general\nprobabilistic models, and we propose computationally efficient algorithms to\nestimate them. First, we show how principal component analysis (PCA) can be\nutilized to estimate a general model that includes the well-known\nPritchard-Stephens-Donnelly mixed-membership model as a special case. Noting\nsome drawbacks of this approach, we introduce a new \"logistic factor analysis\"\n(LFA) framework that seeks to directly model the logit transformation of\nprobabilities underlying observed genotypes in terms of latent variables that\ncapture population structure. We demonstrate these advances on data from the\nhuman genome diversity panel and 1000 genomes project, where we are able to\nidentify SNPs that are highly differentiated with respect to structure while\nmaking minimal modeling assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 00:14:17 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 03:41:05 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Hao", "Wei", ""], ["Song", "Minsun", ""], ["Storey", "John D.", ""]]}, {"id": "1312.2129", "submitter": "Cindie Andrieu", "authors": "Cindie Andrieu (IFSTTAR/COSYS/LIVIC), Guillaume Saint Pierre\n  (IFSTTAR/LIVIC), Xavier Bressaud (IMT)", "title": "A moving fixed-interval filter/smoother for estimation of vehicle\n  position using odometer and map-matched GPS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents some optimal real-time and post-processing estimators of\nvehicle position using odometer and map-matched GPS measurements. These\nestimators were based on a simple statistical error model of the odometer and\nthe GPS which makes the model generalizable to other applications. Firstly, an\nasymptotically minimum variance unbiased estimator and two optimal moving fixed\ninterval filters which are more flexibles are exposed. Then, the\npost-processing case leads to the construction of two moving fixed interval\nsmoothers. These estimators are tested and compared with the classical Kalman\nfilter with simulated and real data, and the results show a good accuracy of\neach of them.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 19:13:31 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Andrieu", "Cindie", "", "IFSTTAR/COSYS/LIVIC"], ["Pierre", "Guillaume Saint", "", "IFSTTAR/LIVIC"], ["Bressaud", "Xavier", "", "IMT"]]}, {"id": "1312.2252", "submitter": "Cindie Andrieu", "authors": "Cindie Andrieu (IFSTTAR/COSYS/LIVIC, IMT), Guillaume Saint Pierre\n  (IFSTTAR/LIVIC), Xavier Bressaud (IMT)", "title": "A functional analysis of speed profiles: smoothing using derivative\n  information, curve registration, and functional boxplot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a functional analysis of a set of individual\nspace-speed profiles corresponding to speed as function of the distance\ntraveled by the vehicle from an initial point. This functional analysis begins\nwith a functional modeling of space-speed profiles and the study of\nmathematical properties of these functions. Then, in a first step, a smoothing\nprocedure based on spline smoothing is developed in order to convert the raw\ndata into functional objets and to filter out the measurement noise as\nefficiently as possible. It is shown that this smoothing step leads to a\ncomplex nonparametric regression problem that needs to take into account two\nconstraints: the use of the derivative information, and a monotonicity\nconstraint. The performance of the proposed two-step estimator (smooth, and\nthen monotonize) is illustrated on simulation studies and a real data example.\nIn a second step, we use a curve registration method based on landmarks\nalignment in order to construct an average speed profile representative of a\nset of individual speed profiles. Finally, the variability of such a set is\nexplored by the use of functional boxplots.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 19:51:16 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 08:46:09 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Andrieu", "Cindie", "", "IFSTTAR/COSYS/LIVIC, IMT"], ["Pierre", "Guillaume Saint", "", "IFSTTAR/LIVIC"], ["Bressaud", "Xavier", "", "IMT"]]}, {"id": "1312.2363", "submitter": "Makram Talih", "authors": "Makram Talih", "title": "A reference-invariant health disparity index based on R\\'{e}nyi\n  divergence", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS621 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1217-1243", "doi": "10.1214/12-AOAS621", "report-no": "IMS-AOAS-AOAS621", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of four overarching goals of Healthy People 2020 (HP2020) is to achieve\nhealth equity, eliminate disparities, and improve the health of all groups. In\nhealth disparity indices (HDIs) such as the mean log deviation (MLD) and Theil\nindex (TI), disparities are relative to the population average, whereas in the\nindex of disparity (IDisp) the reference is the group with the least adverse\nhealth outcome. Although the latter may be preferable, identification of a\nreference group can be affected by statistical reliability. To address this\nissue, we propose a new HDI, the R\\'{e}nyi index (RI), which is\nreference-invariant. When standardized, the RI extends the Atkinson index,\nwhere a disparity aversion parameter can incorporate societal values associated\nwith health equity. In addition, both the MLD and TI are limiting cases of the\nRI. Also, a symmetrized R\\'{e}nyi index (SRI) can be constructed, resulting in\na symmetric measure in the two distributions whose relative entropy is being\nevaluated. We discuss alternative symmetric and reference-invariant HDIs\nderived from the generalized entropy (GE) class and the Bregman divergence, and\nargue that the SRI is more robust than its GE-based counterpart to small\nchanges in the distribution of the adverse health outcome. We evaluate the\ndesign-based standard errors and bootstrapped sampling distributions for the\nSRI, and illustrate the proposed methodology using data from the National\nHealth and Nutrition Examination Survey (NHANES) on the 2001-04 prevalence of\nmoderate or severe periodontitis among adults aged 45-74, which track Oral\nHealth objective OH-5 in HP2020. Such data, which use a binary individual-level\noutcome variable, are typical of HP2020 data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 10:08:09 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Talih", "Makram", ""]]}, {"id": "1312.2364", "submitter": "Heng Lian", "authors": "Yuao Hu, Ye Tian, Heng Lian", "title": "Letter to the Editor", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS640 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1244-1246", "doi": "10.1214/13-AOAS640", "report-no": "IMS-AOAS-AOAS640", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper by Alfons, Croux and Gelper (2013), Sparse least trimmed squares\nregression for analyzing high-dimensional large data sets, considered a\ncombination of least trimmed squares (LTS) and lasso penalty for robust and\nsparse high-dimensional regression. In a recent paper [She and Owen (2011)], a\nmethod for outlier detection based on a sparsity penalty on the mean shift\nparameter was proposed (designated by \"SO\" in the following). This work is\nmentioned in Alfons et al. as being an \"entirely different approach.\" Certainly\nthe problem studied by Alfons et al. is novel and interesting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 10:13:37 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Hu", "Yuao", ""], ["Tian", "Ye", ""], ["Lian", "Heng", ""]]}, {"id": "1312.2393", "submitter": "Isobel Claire Gormley Dr.", "authors": "Gift Nyamundanda, Isobel Claire Gormley and Lorraine Brennan", "title": "A dynamic probabilistic principal components model for the analysis of\n  longitudinal metabolomic data", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a longitudinal metabolomics study, multiple metabolites are measured from\nseveral observations at many time points. Interest lies in reducing the\ndimensionality of such data and in highlighting influential metabolites which\nchange over time. A dynamic probabilistic principal components analysis (DPPCA)\nmodel is proposed to achieve dimension reduction while appropriately modelling\nthe correlation due to repeated measurements. This is achieved by assuming an\nautoregressive model for some of the model parameters. Linear mixed models are\nsubsequently used to identify influential metabolites which change over time.\nThe proposed model is used to analyse data from a longitudinal metabolomics\nanimal study.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 11:38:43 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Nyamundanda", "Gift", ""], ["Gormley", "Isobel Claire", ""], ["Brennan", "Lorraine", ""]]}, {"id": "1312.2404", "submitter": "Isobel Claire Gormley Dr.", "authors": "Gift Nyamundanda, Isobel Claire Gormley, Yue Fan, William M Gallagher\n  and Lorraine Brennan", "title": "MetSizeR: selecting the optimal sample size for metabolomic studies\n  using an analysis based approach", "comments": "15 pages, 3 figures", "journal-ref": "BMC Bioinformatics (2013) 14:338", "doi": "10.1186/1471-2105-14-338", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Determining sample sizes for metabolomic experiments is important\nbut due to the complexity of these experiments, there are currently no standard\nmethods for sample size estimation in metabolomics. Since pilot studies are\nrarely done in metabolomics, currently existing sample size estimation\napproaches which rely on pilot data can not be applied.\n  Results: In this article, an analysis based approach called MetSizeR is\ndeveloped to estimate sample size for metabolomic experiments even when\nexperimental pilot data are not available. The key motivation for MetSizeR is\nthat it considers the type of analysis the researcher intends to use for data\nanalysis when estimating sample size. MetSizeR uses information about the data\nanalysis technique and prior expert knowledge of the metabolomic experiment to\nsimulate pilot data from a statistical model. Permutation based techniques are\nthen applied to the simulated pilot data to estimate the required sample size.\n  Conclusions: The MetSizeR methodology, and a publicly available software\npackage which implements the approach, are illustrated through real metabolomic\napplications. Sample size estimates, informed by the intended statistical\nanalysis technique, and the associated uncertainty are provided.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 12:21:25 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Nyamundanda", "Gift", ""], ["Gormley", "Isobel Claire", ""], ["Fan", "Yue", ""], ["Gallagher", "William M", ""], ["Brennan", "Lorraine", ""]]}, {"id": "1312.2413", "submitter": "Paulo Justiniano Ribeiro Jr", "authors": "Wagner H. Bonat, Paulo J. Ribeiro Jr., Walmes Marque Zeviani", "title": "Likelihood analysis for a class of beta mixed models", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2014.947248", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression models are a suitable choice for continuous response\nvariables on the unity interval. Random effects add further flexibility to the\nmodels and accommodate data structures such as hierarchical, repeated measures\nand longitudinal, which typically induce extra variability and/or dependence.\nClosed expressions cannot be obtained for parameter estimation and numerical\nmethods are required and possibly combined with sampling algorithms. We focus\non likelihood inference and related algorithms for the analysis of beta mixed\nmodels motivated by two real problems with grouped data structures. The first\nis a study on a life quality index of industry workers with data collected\naccording to an hierarchical sampling scheme. The second is a study with a\nnested and longitudinal data structure assessing the impact of hydroelectric\npower plants upon measures of water quality indexes up, downstream and at the\nreservoirs of the dammed rivers. Relevant scientific hypothesis are\ninvestigated by comparing alternative models. The analysis uses different\nalgorithms including data-cloning, an alternative to numerical approximations\nwhich also assess identifiability. Confidence intervals based on profiled\nlikelihoods are compared to those obtained by asymptotic quadratic\napproximations, showing relevant differences for parameters related to the\nrandom effects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 12:46:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 21:08:25 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner H.", ""], ["Ribeiro", "Paulo J.", "Jr."], ["Zeviani", "Walmes Marque", ""]]}, {"id": "1312.2423", "submitter": "Paulo Justiniano Ribeiro Jr", "authors": "Walmes Marques Zeviani, Paulo Justiniano Ribeiro Jr., Wagner Hugo\n  Bonat, Silvia Emiko Shimakura, Joel Augusti Muniz", "title": "The Gamma-count distribution in the analysis of experimental\n  underdispersed data", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2014.922168", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event counts are response variables with non-negative integer values\nrepresenting the number of times that an event occurs within a fixed domain\nsuch as a time interval, a geographical area or a cell of a contingency table.\nAnalysis of counts by Gaussian regression models ignores the discreteness,\nasymmetry and heterocedasticity and is inefficient, providing unrealistic\nstandard errors or possibily negative predictions of the expected number of\nevents. The Poisson regression is the standard model for count data with\nunderlying assumptions on the generating process which may be implausible in\nmany applications. Statisticians have long recognized the limitation of\nimposing equidispersion under the Poisson regression model. A typical situation\nis when the conditional variance exceeds the conditional mean, in which case\nmodels allowing for overdispersion are routinely used. Less reported is the\ncase of underdispersion with fewer modelling alternatives and assessments\navailable in the literature. One of such alternatives, the Gamma-count model,\nis adopted here in the analysis of an agronomic experiment designed to\ninvestigate the effect of levels of defoliation on different phenological\nstates upon the number of cotton bolls. Results show improvements over the\nPoisson model and the semiparametric quasi-Poisson model in capturing the\nobserved variability in the data. Estimating rather than assuming the\nunderlying variance process lead to important insights into the process.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 13:32:05 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zeviani", "Walmes Marques", ""], ["Ribeiro", "Paulo Justiniano", "Jr."], ["Bonat", "Wagner Hugo", ""], ["Shimakura", "Silvia Emiko", ""], ["Muniz", "Joel Augusti", ""]]}, {"id": "1312.2565", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), St\\'ephan Cl\\'emen\\c{c}on (LTCI)", "title": "An SIR Graph Growth Model for the Epidemics of Communicable Diseases", "comments": "A few minor corrections and tidy of references. arXiv admin note:\n  text overlap with arXiv:0810.0896 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is the main purpose of this paper to introduce a graph-valued stochastic\nprocess in order to model the spread of a communicable infectious disease. The\nmajor novelty of the SIR model we promote lies in the fact that the social\nnetwork on which the epidemics is taking place is not specified in advance but\nevolves through time, accounting for the temporal evolution of the interactions\ninvolving infective individuals. Without assuming the existence of a fixed\nunderlying network model, the stochastic process introduced describes, in a\nflexible and realistic manner, epidemic spread in non-uniformly mixing and\npossibly heterogeneous populations. It is shown how to fit such a\n(parametrised) model by means of Approximate Bayesian Computation methods based\non graph-valued statistics. The concepts and statistical methods described in\nthis paper are finally applied to a real epidemic dataset, related to the\nspread of HIV in Cuba in presence of a contact tracing system, which permits\none to reconstruct partly the evolution of the graph of sexual partners\ndiagnosed HIV positive between 1986 and 2006.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 20:19:36 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 14:27:44 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1312.2638", "submitter": "C. E. Priebe", "authors": "D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe", "title": "Vertex nomination schemes for membership prediction", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS834 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1510-1532", "doi": "10.1214/15-AOAS834", "report-no": "IMS-AOAS-AOAS834", "categories": "stat.ML math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a graph is realized from a stochastic block model where one of\nthe blocks is of interest, but many or all of the vertices' block labels are\nunobserved. The task is to order the vertices with unobserved block labels into\na ``nomination list'' such that, with high probability, vertices from the\ninteresting block are concentrated near the list's beginning. We propose\nseveral vertex nomination schemes. Our basic - but principled - setting and\ndevelopment yields a best nomination scheme (which is a Bayes-Optimal\nanalogue), and also a likelihood maximization nomination scheme that is\npractical to implement when there are a thousand vertices, and which is\nempirically near-optimal when the number of vertices is small enough to allow\ncomparison to the best nomination scheme. We then illustrate the robustness of\nthe likelihood maximization nomination scheme to the modeling challenges\ninherent in real data, using examples which include a social network involving\nhuman trafficking, the Enron Graph, a worm brain connectome and a political\nblog network.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 01:04:59 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 15:09:43 GMT"}, {"version": "v3", "created": "Thu, 7 Aug 2014 14:27:12 GMT"}, {"version": "v4", "created": "Fri, 17 Apr 2015 14:09:53 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2015 11:58:17 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Fishkind", "D. E.", ""], ["Lyzinski", "V.", ""], ["Pao", "H.", ""], ["Chen", "L.", ""], ["Priebe", "C. E.", ""]]}, {"id": "1312.2686", "submitter": "Ran Zhang", "authors": "Ran Zhang, Claudia Czado, Karin Sigloch", "title": "A Bayesian linear model for the high-dimensional inverse problem of\n  seismic tomography", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS623 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1111-1138", "doi": "10.1214/12-AOAS623", "report-no": "IMS-AOAS-AOAS623", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a linear Bayesian model to seismic tomography, a high-dimensional\ninverse problem in geophysics. The objective is to estimate the\nthree-dimensional structure of the earth's interior from data measured at its\nsurface. Since this typically involves estimating thousands of unknowns or\nmore, it has always been treated as a linear(ized) optimization problem. Here\nwe present a Bayesian hierarchical model to estimate the joint distribution of\nearth structural and earthquake source parameters. An ellipsoidal spatial prior\nallows to accommodate the layered nature of the earth's mantle. With our\nefficient algorithm we can sample the posterior distributions for large-scale\nlinear inverse problems and provide precise uncertainty quantification in terms\nof parameter distributions and credible intervals given the data. We apply the\nmethod to a full-fledged tomography problem, an inversion for upper-mantle\nstructure under western North America that involves more than 11,000\nparameters. In studies on simulated and real data, we show that our approach\nretrieves the major structures of the earth's interior as well as classical\nleast-squares minimization, while additionally providing uncertainty\nassessments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 06:52:36 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Zhang", "Ran", ""], ["Czado", "Claudia", ""], ["Sigloch", "Karin", ""]]}, {"id": "1312.2687", "submitter": "Michael L. Stein", "authors": "Michael L. Stein, Jie Chen, Mihai Anitescu", "title": "Stochastic approximation of score functions for Gaussian processes", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS627 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1162-1191", "doi": "10.1214/13-AOAS627", "report-no": "IMS-AOAS-AOAS627", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the statistical properties of a recently introduced unbiased\nstochastic approximation to the score equations for maximum likelihood\ncalculation for Gaussian processes. Under certain conditions, including bounded\ncondition number of the covariance matrix, the approach achieves $O(n)$ storage\nand nearly $O(n)$ computational effort per optimization step, where $n$ is the\nnumber of data sites. Here, we prove that if the condition number of the\ncovariance matrix is bounded, then the approximate score equations are nearly\noptimal in a well-defined sense. Therefore, not only is the approximation\nefficient to compute, but it also has comparable statistical properties to the\nexact maximum likelihood estimates. We discuss a modification of the stochastic\napproximation in which design elements of the stochastic terms mimic patterns\nfrom a $2^n$ factorial design. We prove these designs are always at least as\ngood as the unstructured design, and we demonstrate through simulation that\nthey can produce a substantial improvement over random designs. Our findings\nare validated by numerical experiments on simulated data sets of up to 1\nmillion observations. We apply the approach to fit a space-time model to over\n80,000 observations of total column ozone contained in the latitude band\n$40^{\\circ}$-$50^{\\circ}$N during April 2012.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 06:56:33 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Stein", "Michael L.", ""], ["Chen", "Jie", ""], ["Anitescu", "Mihai", ""]]}, {"id": "1312.2800", "submitter": "Florence Forbes", "authors": "Florence Forbes, Myriam Charras-Garrido, Lamiae Azizi, Senan Doyle,\n  David Abrial", "title": "Spatial risk mapping for rare disease with hidden Markov fields and\n  variational EM", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS629 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1192-1216", "doi": "10.1214/13-AOAS629", "report-no": "IMS-AOAS-AOAS629", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current risk mapping models for pooled data focus on the estimated risk for\neach geographical unit. A risk classification, that is, grouping of\ngeographical units with similar risk, is then necessary to easily draw\ninterpretable maps, with clearly delimited zones in which protection measures\ncan be applied. As an illustration, we focus on the Bovine Spongiform\nEncephalopathy (BSE) disease that threatened the bovine production in Europe\nand generated drastic cow culling. This example features typical animal disease\nrisk analysis issues with very low risk values, small numbers of observed cases\nand population sizes that increase the difficulty of an automatic\nclassification. We propose to handle this task in a spatial clustering\nframework using a nonstandard discrete hidden Markov model prior designed to\nfavor a smooth risk variation. The model parameters are estimated using an EM\nalgorithm and a mean field approximation for which we develop a new\ninitialization strategy appropriate for spatial Poisson mixtures. Using both\nsimulated and our BSE data, we show that our strategy performs well in dealing\nwith low population sizes and accurately determines high risk regions, both in\nterms of localization and risk level estimation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 13:59:41 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Forbes", "Florence", ""], ["Charras-Garrido", "Myriam", ""], ["Azizi", "Lamiae", ""], ["Doyle", "Senan", ""], ["Abrial", "David", ""]]}, {"id": "1312.2923", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Eric Danioux", "title": "Lagrangian Time Series Models for Ocean Surface Drifter Trajectories", "comments": "21 pages, 10 figures", "journal-ref": "Journal of the Royal Statistical Society (Series C, Applied\n  Statistics), 65(1), 29-50, 2016", "doi": "10.1111/rssc.12112", "report-no": null, "categories": "stat.AP physics.ao-ph physics.flu-dyn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes stochastic models for the analysis of ocean surface\ntrajectories obtained from freely-drifting satellite-tracked instruments. The\nproposed time series models are used to summarise large multivariate datasets\nand infer important physical parameters of inertial oscillations and other\nocean processes. Nonstationary time series methods are employed to account for\nthe spatiotemporal variability of each trajectory. Because the datasets are\nlarge, we construct computationally efficient methods through the use of\nfrequency-domain modelling and estimation, with the data expressed as\ncomplex-valued time series. We detail how practical issues related to sampling\nand model misspecification may be addressed using semi-parametric techniques\nfor time series, and we demonstrate the effectiveness of our stochastic models\nthrough application to both real-world data and to numerical model output.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 19:34:43 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 22:44:56 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2015 00:05:09 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Danioux", "Eric", ""]]}, {"id": "1312.3077", "submitter": "Lutz Bornmann Dr.", "authors": "Marcin Kozak, Lutz Bornmann, Loet Leydesdorff", "title": "How have the Eastern European countries of the former Warsaw Pact\n  developed since 1990? A bibliometric study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Did the demise of the Soviet Union in 1991 influence the scientific\nperformance of the researchers in Eastern European countries? Did this\nhistorical event affect international collaboration by researchers from the\nEastern European countries with those of Western countries? Did it also change\ninternational collaboration among researchers from the Eastern European\ncountries? Trying to answer these questions, this study aims to shed light on\ninternational collaboration by researchers from the Eastern European countries\n(Russia, Ukraine, Belarus, Moldova, Bulgaria, the Czech Republic, Hungary,\nPoland, Romania and Slovakia). The number of publications and normalized\ncitation impact values are compared for these countries based on InCites\n(Thomson Reuters), from 1981 up to 2011. The international collaboration by\nresearchers affiliated to institutions in Eastern European countries at the\ntime points of 1990, 2000 and 2011 was studied with the help of Pajek and\nVOSviewer software, based on data from the Science Citation Index (Thomson\nReuters). Our results show that the breakdown of the communist regime did not\nlead, on average, to a huge improvement in the publication performance of the\nEastern European countries and that the increase in international co-authorship\nrelations by the researchers affiliated to institutions in these countries was\nsmaller than expected. Most of the Eastern European countries are still subject\nto changes and are still awaiting their boost in scientific development.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 08:43:14 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Kozak", "Marcin", ""], ["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1312.3382", "submitter": "Xiaobei Zhou", "authors": "Xiaobei Zhou and Helen Lindsay and Mark D. Robinson", "title": "Robustly detecting differential expression in RNA sequencing data using\n  observation weights", "comments": "18 pages, 6 figures (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for comparing gene expression levels between (replicated)\nconditions of RNA sequencing data relies on counting reads that map to features\nof interest. Within such count-based methods, many flexible and advanced\nstatistical approaches now exist and offer the ability to adjust for covariates\n(e.g., batch effects). Often, these methods include some sort of (sharing of\ninformation) across features to improve inferences in small samples. It is\nimportant to achieve an appropriate tradeoff between statistical power and\nprotection against outliers. Here, we study the robustness of existing\napproaches for count-based differential expression analysis and propose a new\nstrategy based on observation weights that can be used within existing\nframeworks. The results suggest that outliers can have a global effect on\ndifferential analyses. We demonstrate the effectiveness of our new approach\nwith real data and simulated data that reflects properties of real datasets\n(e.g., dispersion-mean trend) and develop an extensible framework for\ncomprehensive testing of current and future methods. In addition, we explore\nthe origin of such outliers, in some cases highlighting additional biological\nor technical factors within the experiment. Further details can be downloaded\nfrom the project website:\nhttp://imlspenticton.uzh.ch/robinson_lab/edgeR_robust/\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 02:01:15 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 16:15:39 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Zhou", "Xiaobei", ""], ["Lindsay", "Helen", ""], ["Robinson", "Mark D.", ""]]}, {"id": "1312.3398", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow, Cyrus Samii, Valentina A. Assenova", "title": "Cluster-Robust Variance Estimation for Dyadic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic data are common in the social sciences, although inference for such\nsettings involves accounting for a complex clustering structure. Many analyses\nin the social sciences fail to account for the fact that multiple dyads share a\nmember, and that errors are thus likely correlated across these dyads. We\npropose a nonparametric sandwich-type robust variance estimator for linear\nregression to account for such clustering in dyadic data. We enumerate\nconditions for estimator consistency. We also extend our results to repeated\nand weighted observations, including directed dyads and longitudinal data, and\nprovide an implementation for generalized linear models such as logistic\nregression. We examine empirical performance with simulations and applications\nto international relations and speed dating.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 03:48:34 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 13:10:39 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 00:34:02 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 23:03:57 GMT"}, {"version": "v5", "created": "Sat, 23 May 2015 01:56:00 GMT"}, {"version": "v6", "created": "Fri, 12 Jun 2015 18:21:56 GMT"}, {"version": "v7", "created": "Mon, 6 Jul 2015 19:45:45 GMT"}, {"version": "v8", "created": "Wed, 22 Jul 2015 21:18:35 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Assenova", "Valentina A.", ""]]}, {"id": "1312.3589", "submitter": "Florian Knorr", "authors": "Florian Knorr and Thomas Zaksek and Johannes Br\\\"ugmann and Michael\n  Schreckenberg", "title": "Statistical Analysis of High-Flow Traffic States", "comments": "6 pages, 4 figures, presented at \"Traffic and Granular Flow 2013\"\n  conference", "journal-ref": null, "doi": "10.1007/978-3-319-10629-8_62", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between the fundamental observables of traffic flow (i.e.,\nvehicle density, flow rate, and average velocity) is of great importance for\nthe study of traffic phenomena. Probably the most common source of such data\nare inductive loop detectors, which count the number of passing vehicles and\nmeasure their speed. We will present an analysis of detector data collected by\nmore than 3000 loop detectors during the past three years on the motorway\nnetwork of the state of North Rhine-Westphalia. Besides presenting some general\naspects of traffic flow, our analysis focuses on the characteristics of\nso-called high-flow states, i.e. traffic states where the flow rate exceeds 50\nvehicles per minute and lane (3000 veh/h/lane). We investigate the duration,\nfrequency and other statistics of such states, the viability of the data and we\nstudy the conditions under which they occur. The factors that influence the\nexistence of high-flow states in traffic are, for instance, the fraction of\nslow vehicles (namely trucks), the motorway's general topology (e.g. number of\nlanes), the hour of the day and day of the week. This information is directly\naccessible from the detector data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 18:57:42 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Knorr", "Florian", ""], ["Zaksek", "Thomas", ""], ["Br\u00fcgmann", "Johannes", ""], ["Schreckenberg", "Michael", ""]]}, {"id": "1312.3763", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Andr\\'as Hor\\'anyi and D\\'ora Nemoda", "title": "Comparison of BMA and EMOS statistical calibration methods for\n  temperature and wind speed ensemble weather prediction", "comments": null, "journal-ref": "Id\\H{o}j\\'ar\\'as 118 (2014), no. 3, 217-241", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the weather can be described by deterministic numerical\nweather forecasting models. Multiple runs of these models with different\ninitial conditions and/or model physics result in forecast ensembles which are\nused for estimating the distribution of future atmospheric variables. However,\nthese ensembles are usually under-dispersive and uncalibrated, so\npost-processing is required.\n  In the present work we compare different versions of Bayesian Model Averaging\n(BMA) and Ensemble Model Output Statistics (EMOS) post-processing methods in\norder to calibrate 2m temperature and 10m wind speed forecasts of the\noperational ALADIN Limited Area Model Ensemble Prediction System of the\nHungarian Meteorological Service. We show that compared to the raw ensemble\nboth post-processing methods improve the calibration of probabilistic and\naccuracy of point forecasts and that the best BMA method slightly outperforms\nthe EMOS technique.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 10:38:52 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Hor\u00e1nyi", "Andr\u00e1s", ""], ["Nemoda", "D\u00f3ra", ""]]}, {"id": "1312.4323", "submitter": "Stefan Siegert", "authors": "Stefan Siegert, Jochen Broecker, Holger Kantz", "title": "Skill of data based predictions versus dynamical models -- case study on\n  extreme temperature anomalies", "comments": "32 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare probabilistic predictions of extreme temperature anomalies issued\nby two different forecast schemes. One is a dynamical physical weather model,\nthe other a simple data model. We recall the concept of skill scores in order\nto assess the performance of these two different predictors. Although the\nresult confirms the expectation that the (computationally expensive) weather\nmodel outperforms the simple data model, the performance of the latter is\nsurprisingly good. More specifically, for some parameter range, it is even\nbetter than the uncalibrated weather model. Since probabilistic predictions are\nnot easily interpreted by the end user, we convert them into deterministic\nyes/no statements and measure the performance of these by ROC statistics.\nScored in this way, conclusions about model performance partly change, which\nillustrates that predictive power depends on how it is quantified.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 11:33:39 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Siegert", "Stefan", ""], ["Broecker", "Jochen", ""], ["Kantz", "Holger", ""]]}, {"id": "1312.4383", "submitter": "Faustino Prieto", "authors": "Faustino Prieto, Emilio G\\'omez-D\\'eniz, Jos\\'e Mar\\'ia Sarabia", "title": "Modelling Road Accident Blackspots Data with the Discrete Generalized\n  Pareto distribution", "comments": "This is a preprint (20 pages, 7 tables, 2 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study shows how road traffic networks events, in particular road\naccidents on blackspots, can be modelled with simple probabilistic\ndistributions. We considered the number of accidents and the number of deaths\non Spanish blackspots in the period 2003-2007, from Spanish General Directorate\nof Traffic (DGT). We modelled those datasets, respectively, with the discrete\ngeneralized Pareto distribution (a discrete parametric model with three\nparameters) and with the discrete Lomax distribution (a discrete parametric\nmodel with two parameters, and particular case of the previous model). For\nthat, we analyzed the basic properties of both parametric models: cumulative\ndistribution, survival, probability mass, quantile and hazard functions,\ngenesis and rth-order moments; applied two estimation methods of their\nparameters: the $\\mu$ and ($\\mu+1$) frequency method and the maximum likelihood\nmethod; and used two goodness-of-fit tests: Chi-square test and discrete\nKolmogorov-Smirnov test based on bootstrap resampling. We found that those\nprobabilistic models can be useful to describe the road accident blackspots\ndatasets analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 14:50:33 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Prieto", "Faustino", ""], ["G\u00f3mez-D\u00e9niz", "Emilio", ""], ["Sarabia", "Jos\u00e9 Mar\u00eda", ""]]}, {"id": "1312.4472", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Laura Hoyden, Sonja Kuhnt, Kirsten Schorning", "title": "Optimal designs for multi-response generalized linear models with\n  applications in thermal spraying", "comments": "Keywords and Phrases: Generalized linear models, day effects, optimal\n  designs, thermal spraying", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing experiments for investigating particle\nin-flight properties in thermal spraying. Observations are available on an\nextensive design for an initial day and thereafter in limited number for any\nparticular day. Generalized linear models including additional day effects are\nused for analyzing the process, where the models vary with respect to different\nresponses. We construct robust D-optimal designs to collect additional data on\nany current day, which are efficient for the estimation of the parameters in\nall models under consideration. These designs improve a reference fractional\nfactorial design substantially. We also investigate designs, which maximize the\npower of the test for an additional day effect. The results are used to design\nadditional experiments of the thermal spraying process and a comparison of the\nstatistical analysis based on a reference design as well as on a selected\nD-optimal design is performed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 19:25:07 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Dette", "Holger", ""], ["Hoyden", "Laura", ""], ["Kuhnt", "Sonja", ""], ["Schorning", "Kirsten", ""]]}, {"id": "1312.4594", "submitter": "Mark Wheldon", "authors": "Mark C. Wheldon, Adrian E. Raftery, Samuel J. Clark, Patrick Gerland", "title": "Bayesian Reconstruction of Two-Sex Populations by Age: Estimating Sex\n  Ratios at Birth and Sex Ratios of Mortality", "comments": "43 pages, 12 tables, 15 figures. Since last version: Text of main\n  article unchanged: - Added citation to auxiliary source files. - Added\n  institution report number", "journal-ref": "J R Stat Soc A., 2015, 178(4) 977--1007", "doi": "10.1111/rssa.12104", "report-no": "Univ. of Wash. CSSS WP 138", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original version of Bayesian reconstruction, a method for estimating\nage-specific fertility, mortality, migration and population counts of the\nrecent past with uncertainty, produced estimates for female-only populations.\nHere we show how two-sex populations can be similarly reconstructed and\nprobabilistic estimates of various sex ratio quantities obtained. We\ndemonstrate the method by reconstructing the populations of India from 1971 to\n2001, Thailand from 1960 to 2000, and Laos from 1985 to 2005. We found evidence\nthat in India, sex ratio at birth exceeded its conventional upper limit of\n1.06, and, further, increased over the period of study, with posterior\nprobability above 0.9. In addition, almost uniquely, we found evidence that\nlife expectancy at birth was lower for females than for males in India\n(posterior probability for 1971--1976 0.79), although there was strong evidence\nfor a narrowing of the gap through to 2001. In both Thailand and Laos, we found\nstrong evidence for the more usual result that life expectancy at birth was\ngreater for females and, in Thailand, that the difference increased over the\nperiod of study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 00:02:15 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 02:02:56 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Wheldon", "Mark C.", ""], ["Raftery", "Adrian E.", ""], ["Clark", "Samuel J.", ""], ["Gerland", "Patrick", ""]]}, {"id": "1312.4802", "submitter": "Niraj Singh", "authors": "Niraj Kumar Singh, Soubhik Chakraborty, and Dheeresh Kumar Mallick", "title": "A Statistical Peek into Average Case Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper gives a statistical adventure towards exploring the average\ncase complexity behavior of computer algorithms. Rather than following the\ntraditional count based analytical (pen and paper) approach, we instead talk in\nterms of the weight based analysis that permits mixing of distinct operations\ninto a conceptual bound called the statistical bound and its empirical\nestimate, the so called \"empirical O\". Based on careful analysis of the results\nobtained, we have introduced two new conjectures in the domain of algorithmic\nanalysis. The analytical way of average case analysis falls flat when it comes\nto a data model for which the expectation does not exist (e.g. Cauchy\ndistribution for continuous input data and certain discrete distribution inputs\nas those studied in the paper). The empirical side of our approach, with a\nthrust in computer experiments and applied statistics in its paradigm, lends a\nhelping hand by complimenting and supplementing its theoretical counterpart.\nComputer science is or at least has aspects of an experimental science as well,\nand hence hopefully, our statistical findings will be equally recognized among\ntheoretical scientists as well.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 14:36:36 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Singh", "Niraj Kumar", ""], ["Chakraborty", "Soubhik", ""], ["Mallick", "Dheeresh Kumar", ""]]}, {"id": "1312.5124", "submitter": "Paul Fogel", "authors": "Paul Fogel", "title": "Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the\n  Score Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Negative Matrix Factorization, NMF, attempts to find a number of\narchetypal response profiles, or parts, such that any sample profile in the\ndataset can be approximated by a close profile among these archetypes or a\nlinear combination of these profiles. The non-negativity constraint is imposed\nwhile estimating archetypal profiles, due to the non-negative nature of the\nobserved signal. Apart from non negativity, a volume constraint can be applied\non the Score matrix W to enhance the ability of learning parts of NMF. In this\nreport, we describe a very simple algorithm, which in effect achieves volume\nminimization, although indirectly.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 13:13:39 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fogel", "Paul", ""]]}, {"id": "1312.5207", "submitter": "Massimiliano Tamborrino", "authors": "M. Tamborrino, and S. Ditlevsen, and P Lansky", "title": "Parameter inference from hitting times for perturbed Brownian motion", "comments": "17 pages, 6 figure", "journal-ref": "Lifetime Data Anal., 21:331--352, 2015", "doi": "10.1007/s10985-014-9307-7", "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A latent internal process describes the state of some system, e.g. the social\ntension in a political conflict, the strength of an industrial component or the\nhealth status of a person. When this process reaches a predefined threshold,\nthe process terminates and an observable event occurs, e.g. the political\nconflict finishes, the industrial component breaks down or the person has a\nheart attack. Imagine an intervention, e.g., a political decision, maintenance\nof a component or a medical treatment, is initiated to the process before the\nevent occurs. How can we evaluate whether the intervention had an effect?\n  To answer this question we describe the effect of the intervention through\nparameter changes of the law governing the internal process. Then, the time\ninterval between the start of the process and the final event is divided into\ntwo subintervals: the time from the start to the instant of intervention,\ndenoted by $S$, and the time between the intervention and the threshold\ncrossing, denoted by $R$. The first question studied here is: What is the joint\ndistribution of $(S,R)$? The theoretical expression is provided and serves as a\nbasis to answer the main question: Can we estimate the parameters of the model\nfrom observations of $S$ and $R$ and compare them statistically? Maximum\nlikelihood estimators are illustrated on simulated data under the assumption\nthat the process before and after the intervention is described by the same\ntype of model, i.e. a Brownian motion, but with different parameters.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:28:42 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Tamborrino", "M.", ""], ["Ditlevsen", "S.", ""], ["Lansky", "P", ""]]}, {"id": "1312.5224", "submitter": "Niels Bache", "authors": "Niels Bache", "title": "The passive and active periods for the intermittent use of an active\n  sensor to detect an evasive target", "comments": "Key words: Active sensor. Sonar. Radar. Lidar. Evasive target.\n  Detection. Missed opportunities. Detection width. Forestalling. Counter\n  detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Your task is to detect a submarine with your active sonar. The submarine can\nhear your active sonar before you can detect him. If the submarine is fast\nenough he can evade you before you can detect him. How do you then detect him?\nIf you are using your active sonar continuously you will not detect him.\nLikewise, if you are not using your sonar at all. In between those two extremes\nthere is an optimum. We will find that optimum. Or said more precisely and\ngeneral: In the same two dimensional region two platforms are present. One\nplatform, the searcher, equipped with one active sensor (sonar, radar, lidar\netc.), is trying to detect the other platform, the target, by means of its\nactive sensor. The target tries to avoid detection using only a passive sensor\nto detect the searcher. The target can detect the active sensor before the\nsearcher can detect the target (forestalling). The active sensor is therefore\nused intermittently to surprise the target. The aim of this study is to\nquantify the passive period of the active sensor by minimizing missed detection\nopportunities. The active period is subsequently found by maximizing the\naverage detection width of the searcher sensor over time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 17:03:54 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Bache", "Niels", ""]]}, {"id": "1312.5370", "submitter": "Yubin Park", "authors": "Yubin Park and Joydeep Ghosh", "title": "Perturbed Gibbs Samplers for Synthetic Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a categorical data synthesizer with a quantifiable disclosure\nrisk. Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensional\ncategorical data that are often intractable to represent as contingency tables.\nThe algorithm extends a multiple imputation strategy for fully synthetic data\nby utilizing feature hashing and non-parametric distribution approximations.\nCalifornia Patient Discharge data are used to demonstrate statistical\nproperties of the proposed synthesizing methodology. Marginal and conditional\ndistributions, as well as the coefficients of regression models built on the\nsynthesized data are compared to those obtained from the original data.\nIntruder scenarios are simulated to evaluate disclosure risks of the\nsynthesized data from multiple angles. Limitations and extensions of the\nproposed algorithm are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 23:16:52 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Park", "Yubin", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1312.5391", "submitter": "Guofeng Cao", "authors": "Guofeng Cao and Phaedon Kyriakidis and Michael Goodchild", "title": "On Spatial Transition Probabilities as Continuity Measures in\n  Categorical Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of spatial transition probabilities, or equivalently, transiogram\nmodels have been recently proposed as spatial continuity measures in\ncategorical fields. In this paper, properties of transiogram models are\nexamined analytically, and three important findings are reported. Firstly,\nconnections between the behaviors of auto-transiogram models near the origin\nand the spatial distribution of the corresponding category are carefully\ninvestigated. Secondly, it is demonstrated that for the indicators of excursion\nsets of Gaussian random fields, most of the commonly used basic mathematical\nforms of covariogram models are not eligible for transiograms in most cases; an\nexception is the exponential distance-decay function and models that are\nconstructed from it. Finally, a kernel regression method is proposed for\nefficient, non-parametric joint modeling of auto- and cross-transiograms, which\nis particularly useful for situations where the number of categories is large.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 01:51:11 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 04:29:06 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Cao", "Guofeng", ""], ["Kyriakidis", "Phaedon", ""], ["Goodchild", "Michael", ""]]}, {"id": "1312.5496", "submitter": "Carles Breto (Martinez)", "authors": "Carles Bret\\'o", "title": "On idiosyncratic stochasticity of financial leverage effects", "comments": "8 pages, 2 figures", "journal-ref": "Statistics & Probability Letters 91 (2014) 20-26", "doi": "10.1016/j.spl.2014.04.003", "report-no": null, "categories": "q-fin.GN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model leverage as stochastic but independent of return shocks and of\nvolatility and perform likelihood-based inference via the recently developed\niterated filtering algorithm using S&P500 data, contributing new evidence to\nthe still slim empirical support for random leverage variation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 12:01:18 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Bret\u00f3", "Carles", ""]]}, {"id": "1312.5734", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer and Richard G. Baraniuk", "title": "Time-varying Learning and Content Analytics via Sparse Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SPARFA-Trace, a new machine learning-based framework for\ntime-varying learning and content analytics for education applications. We\ndevelop a novel message passing-based, blind, approximate Kalman filter for\nsparse factor analysis (SPARFA), that jointly (i) traces learner concept\nknowledge over time, (ii) analyzes learner concept knowledge state transitions\n(induced by interacting with learning resources, such as textbook sections,\nlecture videos, etc, or the forgetting effect), and (iii) estimates the content\norganization and intrinsic difficulty of the assessment questions. These\nquantities are estimated solely from binary-valued (correct/incorrect) graded\nlearner response data and a summary of the specific actions each learner\nperforms (e.g., answering a question or studying a learning resource) at each\ntime instance. Experimental results on two online course datasets demonstrate\nthat SPARFA-Trace is capable of tracing each learner's concept knowledge\nevolution over time, as well as analyzing the quality and content organization\nof learning resources, the question-concept associations, and the question\nintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable\nor better performance in predicting unobserved learner responses than existing\ncollaborative filtering and knowledge tracing approaches for personalized\neducation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 20:44:44 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1312.5859", "submitter": "Jiaping Wang", "authors": "Jiaping Wang, Hongtu Zhu, Jianqing Fan, Kelly Giovanello, Weili Lin", "title": "Multiscale adaptive smoothing models for the hemodynamic response\n  function in fMRI", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS609 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 904-935", "doi": "10.1214/12-AOAS609", "report-no": "IMS-AOAS-AOAS609", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the event-related functional magnetic resonance imaging (fMRI) data\nanalysis, there is an extensive interest in accurately and robustly estimating\nthe hemodynamic response function (HRF) and its associated statistics (e.g.,\nthe magnitude and duration of the activation). Most methods to date are\ndeveloped in the time domain and they have utilized almost exclusively the\ntemporal information of fMRI data without accounting for the spatial\ninformation. The aim of this paper is to develop a multiscale adaptive\nsmoothing model (MASM) in the frequency domain by integrating the spatial and\nfrequency information to adaptively and accurately estimate HRFs pertaining to\neach stimulus sequence across all voxels in a three-dimensional (3D) volume. We\nuse two sets of simulation studies and a real data set to examine the finite\nsample performance of MASM in estimating HRFs. Our real and simulated data\nanalyses confirm that MASM outperforms several other state-of-the-art methods,\nsuch as the smooth finite impulse response (sFIR) model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 09:12:03 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Wang", "Jiaping", ""], ["Zhu", "Hongtu", ""], ["Fan", "Jianqing", ""], ["Giovanello", "Kelly", ""], ["Lin", "Weili", ""]]}, {"id": "1312.6220", "submitter": "Anthony Cousien", "authors": "Anthony Cousien (IAME), Viet Chi Tran (LPP), Sylvie Deuffic-Burban\n  (IAME), Marie Jauffret-Roustide (CERMES3, INVS), Jean-Stephane Dhersin\n  (LAGA), Yazdan Yazdanpanah (IAME)", "title": "Dynamic modelling of hepatitis C virus transmission among people who\n  inject drugs: a methodological review", "comments": null, "journal-ref": "Journal of Viral Hepatitis, Wiley-Blackwell, 2015, 22 (3),\n  pp.213-29.\n  \\&lt;http://onlinelibrary.wiley.com/doi/10.1111/jvh.12337/abstract\\&gt;.\n  \\&lt;10.1111/jvh.12337\\&gt;", "doi": "10.1111/jvh.12337", "report-no": null, "categories": "stat.AP math.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equipment sharing among people who inject drugs (PWID) is a key risk factor\nin infection by hepatitis C virus (HCV). Both the effectiveness and\ncost-effectiveness of interventions aimed at reducing HCV transmission in this\npopulation (such as opioid substitution therapy, needle exchange programs or\nimproved treatment) are difficult to evaluate using field surveys. Ethical\nissues and complicated access to the PWID population make it difficult to\ngather epidemiological data. In this context, mathematical modelling of HCV\ntransmission is a useful alternative for comparing the cost and effectiveness\nof various interventions. Several models have been developed in the past few\nyears. They are often based on strong hypotheses concerning the population\nstructure. This review presents compartmental and individual-based models in\norder to underline their strengths and limits in the context of HCV infection\namong PWID. The final section discusses the main results of the papers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 08:10:32 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 20:02:34 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Cousien", "Anthony", "", "IAME"], ["Tran", "Viet Chi", "", "LPP"], ["Deuffic-Burban", "Sylvie", "", "IAME"], ["Jauffret-Roustide", "Marie", "", "CERMES3, INVS"], ["Dhersin", "Jean-Stephane", "", "LAGA"], ["Yazdanpanah", "Yazdan", "", "IAME"]]}, {"id": "1312.6403", "submitter": "Richard D. Gill", "authors": "Richard D. Gill", "title": "The triangle wave versus the cosine: How classical systems can optimally\n  approximate EPR-B correlations", "comments": "This version, arXiv:1312.6403v.6, as accepted by \"Entropy\" 27\n  February 2020", "journal-ref": null, "doi": "10.3390/e22030287", "report-no": null, "categories": "quant-ph math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The famous singlet correlations of a composite quantum system consisting of\ntwo spatially separated components exhibit notable features of two kinds. The\nfirst kind consists of striking certainty relations: perfect correlation and\nperfect anti-correlation in certain settings. The second kind consists of a\nnumber of symmetries, in particular, invariance under rotation, as well as\ninvariance under exchange of components, parity, or chirality. In this note, I\ninvestigate the class of correlation functions that can be generated by\nclassical composite physical systems when we restrict attention to systems\nwhich reproduce the certainty relations exactly, and for which the rotational\ninvariance of the correlation function is the manifestation of rotational\ninvariance of the underlying classical physics. I call such correlation\nfunctions classical EPR-B correlations. It turns out that the other three\n(binary) symmetries can then be obtained \"for free\": they are exhibited by the\ncorrelation function, and can be imposed on the underlying physics by adding an\nunderlying randomisation level. We end up with a simple probabilistic\ndescription of all possible classical EPR-B correlations in terms of a\n\"spinning coloured disk\" model, and a research programme: describe these\nfunctions in a concise analytic way. We survey open problems, and we show that\nthe widespread idea that \"quantum correlations are more extreme than classical\nphysics allows\" is at best highly inaccurate, through giving a concrete example\nof a classical correlation which satisfies all the symmetries and all the\ncertainty relations and which exceeds the quantum correlations over a whole\nrange of settings\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 16:51:33 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 06:52:53 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 16:32:05 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 06:47:11 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 23:43:21 GMT"}, {"version": "v6", "created": "Thu, 27 Feb 2020 08:21:57 GMT"}, {"version": "v7", "created": "Fri, 6 Mar 2020 18:35:49 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Gill", "Richard D.", ""]]}, {"id": "1312.6443", "submitter": "Victor M. Yakovenko", "authors": "Scott Lawrence and Qin Liu and Victor M. Yakovenko", "title": "Global inequality in energy consumption from 1980 to 2010", "comments": "15 pages, 14 figures, 2 movies; v.2 correction to the inset in the\n  movie Lorenz-Energy.mov, no changes in the paper", "journal-ref": "Entropy 15, 5565-5579 (2013)", "doi": "10.3390/e15125565", "report-no": null, "categories": "physics.data-an q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the global probability distribution of energy consumption per capita\naround the world using data from the U.S. Energy Information Administration\n(EIA) for 1980-2010. We find that the Lorenz curves have moved up during this\ntime period, and the Gini coefficient G has decreased from 0.66 in 1980 to 0.55\nin 2010, indicating a decrease in inequality. The global probability\ndistribution of energy consumption per capita in 2010 is close to the\nexponential distribution with G=0.5. We attribute this result to the\nglobalization of the world economy, which mixes the world and brings it closer\nto the state of maximal entropy. We argue that global energy production is a\nlimited resource that is partitioned among the world population. The most\nprobable partition is the one that maximizes entropy, thus resulting in the\nexponential distribution function. A consequence of the latter is the law of\n1/3: the top 1/3 of the world population consumes 2/3 of produced energy. We\nalso find similar results for the global probability distribution of CO2\nemissions per capita.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 23:57:22 GMT"}, {"version": "v2", "created": "Sat, 8 Mar 2014 22:39:56 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Lawrence", "Scott", ""], ["Liu", "Qin", ""], ["Yakovenko", "Victor M.", ""]]}, {"id": "1312.6481", "submitter": "S. W. Taylor", "authors": "S. W. Taylor, Douglas G. Woolford, C. B. Dean, David L. Martell", "title": "Wildfire Prediction to Inform Fire Management: Statistical Science\n  Challenges", "comments": "Published in at http://dx.doi.org/10.1214/13-STS451 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 586-615", "doi": "10.1214/13-STS451", "report-no": "IMS-STS-STS451", "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfire is an important system process of the earth that occurs across a\nwide range of spatial and temporal scales. A variety of methods have been used\nto predict wildfire phenomena during the past century to better our\nunderstanding of fire processes and to inform fire and land management\ndecision-making. Statistical methods have an important role in wildfire\nprediction due to the inherent stochastic nature of fire phenomena at all\nscales. Predictive models have exploited several sources of data describing\nfire phenomena. Experimental data are scarce; observational data are dominated\nby statistics compiled by government fire management agencies, primarily for\nadministrative purposes and increasingly from remote sensing observations.\nFires are rare events at many scales. The data describing fire phenomena can be\nzero-heavy and nonstationary over both space and time. Users of fire modeling\nmethodologies are mainly fire management agencies often working under great\ntime constraints, thus, complex models have to be efficiently estimated. We\nfocus on providing an understanding of some of the information needed for fire\nmanagement decision-making and of the challenges involved in predicting fire\noccurrence, growth and frequency at regional, national and global scales.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 08:20:07 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Taylor", "S. W.", ""], ["Woolford", "Douglas G.", ""], ["Dean", "C. B.", ""], ["Martell", "David L.", ""]]}, {"id": "1312.6761", "submitter": "Niamh Cahill", "authors": "Niamh Cahill, Andrew C. Kemp, Benjamin P. Horton, Andrew C. Parnell", "title": "Modeling sea-level change using errors-in-variables integrated Gaussian\n  processes", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS824 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 547-571", "doi": "10.1214/15-AOAS824", "report-no": "IMS-AOAS-AOAS824", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform Bayesian inference on historical and late Holocene (last 2000\nyears) rates of sea-level change. The input data to our model are tide-gauge\nmeasurements and proxy reconstructions from cores of coastal sediment. These\ndata are complicated by multiple sources of uncertainty, some of which arise as\npart of the data collection exercise. Notably, the proxy reconstructions\ninclude temporal uncertainty from dating of the sediment core using techniques\nsuch as radiocarbon. The model we propose places a Gaussian process prior on\nthe rate of sea-level change, which is then integrated and set in an\nerrors-in-variables framework to take account of age uncertainty. The resulting\nmodel captures the continuous and dynamic evolution of sea-level change with\nfull consideration of all sources of uncertainty. We demonstrate the\nperformance of our model using two real (and previously published) example data\nsets. The global tide-gauge data set indicates that sea-level rise increased\nfrom a rate with a posterior mean of 1.13 mm$/$yr in 1880 AD (0.89 to 1.28\nmm$/$yr 95% credible interval for the posterior mean) to a posterior mean rate\nof 1.92 mm$/$yr in 2009 AD (1.84 to 2.03 mm$/$yr 95% credible interval for the\nposterior mean). The proxy reconstruction from North Carolina (USA) after\ncorrection for land-level change shows the 2000 AD rate of rise to have a\nposterior mean of 2.44 mm$/$yr (1.91 to 3.01 mm$/$yr 95% credible interval).\nThis is unprecedented in at least the last 2000 years.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 05:25:05 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 16:03:59 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 14:21:29 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2015 21:00:05 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2015 12:49:28 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Cahill", "Niamh", ""], ["Kemp", "Andrew C.", ""], ["Horton", "Benjamin P.", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1312.6896", "submitter": "Renan Cortes Ms", "authors": "Renan Xavier Cortes, Thiago Guerrera Martins, Marcos Oliveira Prates,\n  Br\\'aulio Figueiredo Alves da Silva", "title": "Inference on Dynamic Models for non-Gaussian Random Fields using INLA: A\n  Homicide Rate Analysis of Brazilian Cities", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust time series analysis is an important subject in statistical modeling.\nModels based on Gaussian distribution are sensitive to outliers, which may\nimply in a significant degradation in estimation performance as well as in\nprediction accuracy. State-space models, also referred as Dynamic Models, is a\nvery useful way to describe the evolution of a time series variable through a\nstructured latent evolution system. Integrated Nested Laplace Approximation\n(INLA) is a recent approach proposed to perform fast Bayesian inference in\nLatent Gaussian Models which naturally comprises Dynamic Models. We present how\nto perform fast and accurate non-Gaussian dynamic modeling with INLA and show\nhow these models can provide a more robust time series analysis when compared\nwith standard dynamic models based on Gaussian distributions. We formalize the\nframework used to fit complex non-Gaussian space-state models using the R\npackage INLA and illustrate our approach in both a simulation study and on the\nbrazilian homicide rate dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 21:58:02 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 13:54:20 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Cortes", "Renan Xavier", ""], ["Martins", "Thiago Guerrera", ""], ["Prates", "Marcos Oliveira", ""], ["da Silva", "Br\u00e1ulio Figueiredo Alves", ""]]}, {"id": "1312.6954", "submitter": "Michal Brzezinski", "authors": "Michal Brzezinski", "title": "Empirical modeling of the impact factor distribution", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of impact factors has been modeled in the recent informetric\nliterature using two-exponent law proposed by Mansilla et al. (2007). This\npaper shows that two distributions widely-used in economics, namely the Dagum\nand Singh-Maddala models, possess several advantages over the two-exponent\nmodel. Compared to the latter, the former give as good as or slightly better\nfit to data on impact factors in eight important scientific fields. In contrast\nto the two-exponent model, both proposed distributions have closed-from\nprobability density functions and cumulative distribution functions, which\nfacilitates fitting these distributions to data and deriving their statistical\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 11:06:47 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Brzezinski", "Michal", ""]]}, {"id": "1312.7003", "submitter": "Faicel Chamroukhi", "authors": "Ra\\\"issa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso,\n  Patrice Aknin, Daniel Hissel", "title": "Supervised learning of a regression model based on latent process.\n  Application to the estimation of fuel cell life time", "comments": "In Proceeding of the 8th IEEE International Conference on Machine\n  Learning and Applications (IEEE ICMLA'09), pages 632-637, 2009, Miami Beach,\n  FL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a pattern recognition approach aiming to estimate fuel\ncell duration time from electrochemical impedance spectroscopy measurements. It\nconsists in first extracting features from both real and imaginary parts of the\nimpedance spectrum. A parametric model is considered in the case of the real\npart, whereas regression model with latent variables is used in the latter\ncase. Then, a linear regression model using different subsets of extracted\nfeatures is used fo r the estimation of fuel cell time duration. The\nperformances of the proposed approach are evaluated on experimental data set to\nshow its feasibility. This could lead to interesting perspectives for\npredictive maintenance policy of fuel cell.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:55:59 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Onanena", "Ra\u00efssa", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Candusso", "Denis", ""], ["Aknin", "Patrice", ""], ["Hissel", "Daniel", ""]]}, {"id": "1312.7132", "submitter": "Enkelejd Hashorva", "authors": "Krzysztof D\\c{e}bicki, Julia Farkas, Enkelejd Hashorva", "title": "Random Scaling of Gumbel Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the product of two positive independent risks $Y_1$\nand $Y_2$. If $Y_1$ is bounded and $Y_2$ has distribution in the Gumbel\nmax-domain of attraction with some auxiliary function which is regularly\nvarying at infinity, then we show that $Y_1Y_2$ has also distribution in the\nGumbel max-domain of attraction. Additionally, if both $Y_1,Y_2$ have\nlog-Weibullian or Weibullian tail behavior, we show that $Y_1Y_2$ has\nlog-Weibullian or Weibullian asymptotic tail behavior, respectively. We present\ntwo applications of our results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 17:19:24 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 17:37:02 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["D\u0229bicki", "Krzysztof", ""], ["Farkas", "Julia", ""], ["Hashorva", "Enkelejd", ""]]}, {"id": "1312.7158", "submitter": "Benjamin Baumer", "authors": "Benjamin S. Baumer, Shane T. Jensen, and Gregory J. Matthews", "title": "openWAR: An Open Source System for Evaluating Overall Player Performance\n  in Major League Baseball", "comments": "27 pages including supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within baseball analytics, there is substantial interest in comprehensive\nstatistics intended to capture overall player performance. One such measure is\nWins Above Replacement (WAR), which aggregates the contributions of a player in\neach facet of the game: hitting, pitching, baserunning, and fielding. However,\ncurrent versions of WAR depend upon proprietary data, ad hoc methodology, and\nopaque calculations. We propose a competitive aggregate measure, openWAR, that\nis based upon public data and methodology with greater rigor and transparency.\nWe discuss a principled standard for the nebulous concept of a \"replacement\"\nplayer. Finally, we use simulation-based techniques to provide interval\nestimates for our openWAR measure.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 22:54:45 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 20:27:32 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2015 17:01:32 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Baumer", "Benjamin S.", ""], ["Jensen", "Shane T.", ""], ["Matthews", "Gregory J.", ""]]}, {"id": "1312.7614", "submitter": "Denis Chetverikov", "authors": "Victor Chernozhukov, Denis Chetverikov, and Kengo Kato", "title": "Inference on causal and structural parameters using many moment\n  inequalities", "comments": "This paper was previously circulated under the title \"Testing many\n  moment inequalities\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of testing many moment inequalities where\nthe number of moment inequalities, denoted by $p$, is possibly much larger than\nthe sample size $n$. There is a variety of economic applications where solving\nthis problem allows to carry out inference on causal and structural parameters,\na notable example is the market structure model of Ciliberto and Tamer (2009)\nwhere $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter\nthe market. We consider the test statistic given by the maximum of $p$\nStudentized (or $t$-type) inequality-specific statistics, and analyze various\nways to compute critical values for the test statistic. Specifically, we\nconsider critical values based upon (i) the union bound combined with a\nmoderate deviation inequality for self-normalized sums, (ii) the multiplier and\nempirical bootstraps, and (iii) two-step and three-step variants of (i) and\n(ii) by incorporating the selection of uninformative inequalities that are far\nfrom being binding and a novel selection of weakly informative inequalities\nthat are potentially binding but do not provide first order information. We\nprove validity of these methods, showing that under mild conditions, they lead\nto tests with the error in size decreasing polynomially in $n$ while allowing\nfor $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$\nfor some $c > 0$. Importantly, all these results hold without any restriction\non the correlation structure between $p$ Studentized statistics, and also hold\nuniformly with respect to suitably large classes of underlying distributions.\nMoreover, in the online supplement, we show validity of a test based on the\nblock multiplier bootstrap in the case of dependent data under some general\nmixing conditions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 02:46:52 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 15:53:25 GMT"}, {"version": "v3", "created": "Sun, 22 Jun 2014 22:09:27 GMT"}, {"version": "v4", "created": "Fri, 21 Nov 2014 05:33:02 GMT"}, {"version": "v5", "created": "Sun, 30 Sep 2018 17:22:06 GMT"}, {"version": "v6", "created": "Thu, 18 Oct 2018 18:06:19 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Kato", "Kengo", ""]]}, {"id": "1312.7714", "submitter": "Christopher King", "authors": "C. Ryan King, Paul J. Rathouz, Dan L. Nicolae", "title": "Prediction and replication from case-control sequencing studies using\n  custom genotyping and additional sequencing", "comments": "Essentially final draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two results about using allele-count (AC) burdens of rare SNPs\ndiscovered in a case-control sequencing study for prediction or validation in\nan external prospective study. When genotyping only the SNPs polymorphic in the\nsequence data, the phenotype to AC correlation tends to be larger in the\nreplication data than the primary study. Conversely, if the replication sample\nis sequenced, ACs of SNPs which are novel in the replication tend to have much\nsmaller or opposite signed associations. We explain this by first deriving the\nAC-phenotype association implied by a model of diverse SNP effects, and second\naccounting for the shifted distribution of SNP effects when using a\ncase-control study as a filter for SNP inclusion. In rare diseases, the case\npopulation is depleted of protective SNPs and enriched for deleterious SNPs,\ncreating the above difference in AC associations. This phenomenon is most\nrelevant in re-sequencing for risk prediction in rare diseases with\nheterogeneous rare mutations because it applies to SNPs with MAF near 1 out of\nthe case-control sample size and is exaggerated when SNP log-odds ratios come\nfrom a heavy-tailed distribution. It also suggests a ``winner's curse'' in\nwhich most risk increasing SNPs at a particular MAF are quickly discovered and\nfuture sequencing finds more protective or irrelevant SNPs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 13:40:42 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 18:26:44 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 01:15:42 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["King", "C. Ryan", ""], ["Rathouz", "Paul J.", ""], ["Nicolae", "Dan L.", ""]]}, {"id": "1312.7734", "submitter": "Suleiman Khan", "authors": "Suleiman A Khan, Seppo Virtanen, Olli P Kallioniemi, Krister\n  Wennerberg, Antti Poso and Samuel Kaski", "title": "Identification of structural features in chemicals associated with\n  cancer drug response: A systematic data-driven analysis", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Analysis of relationships of drug structure to biological\nresponse is key to understanding off-target and unexpected drug effects, and\nfor developing hypotheses on how to tailor drug thera-pies. New methods are\nrequired for integrated analyses of a large number of chemical features of\ndrugs against the corresponding genome-wide responses of multiple cell models.\nResults: In this paper, we present the first comprehensive multi-set analysis\non how the chemical structure of drugs impacts on ge-nome-wide gene expression\nacross several cancer cell lines (CMap database). The task is formulated as\nsearching for drug response components across multiple cancers to reveal shared\neffects of drugs and the chemical features that may be responsible. The\ncom-ponents can be computed with an extension of a very recent ap-proach called\nGroup Factor Analysis (GFA). We identify 11 compo-nents that link the\nstructural descriptors of drugs with specific gene expression responses\nobserved in the three cell lines, and identify structural groups that may be\nresponsible for the responses. Our method quantitatively outperforms the\nlimited earlier studies on CMap and identifies both the previously reported\nassociations and several interesting novel findings, by taking into account\nmultiple cell lines and advanced 3D structural descriptors. The novel\nobservations include: previously unknown similarities in the effects induced by\n15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3D\ndescriptors of the drugs; and the induction by simvastatin of leukemia-specific\nanti-inflammatory response, resem-bling the effects of corticosteroids.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 15:10:52 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 18:47:45 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Khan", "Suleiman A", ""], ["Virtanen", "Seppo", ""], ["Kallioniemi", "Olli P", ""], ["Wennerberg", "Krister", ""], ["Poso", "Antti", ""], ["Kaski", "Samuel", ""]]}, {"id": "1312.7827", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Surface response analysis and determination of confidence regions for\n  atmospheric CO2: a global warming study for U.S.A. data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the atmospheric CO2 measurements taken in Hawaii between 1959\nand 2008, a quadratic model with interactions was fitted, using 5 attributable\nvariables. Surface response analysis returned the eigenvalues and eigenvectors\nat the critical point, which turns out to be of mixed type, with two positive\neigenvalues, one null, and the rest negative. From these data, it is derived\nthat the confidence regions in two variables are of various types (elliptic,\nhyperbolic, and degenerate). Based on these results we indicate how to\ndetermine two-dimensional confidence regions for statistically-significant\nvariables which are relevant contributors to the atmospheric CO2 emissions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 19:09:52 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}, {"id": "1312.7867", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Contributors of carbon dioxide in the atmosphere in Europe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carbon dioxide, along with atmospheric temperature are interacting to cause\nwhat we have defined as global warming. In the present study we develop a\nstatistical model using real data to identify the attributable variables (risk\nfactors) that cause the CO2 emissions in the atmosphere in Europe. Some\nscientists believe that there are more than nineteen attributable variables\nthat cause the CO2 in our atmosphere. However, our study has identified only\nthree individual risk factors and five interactions among the attributable\nvariables that cause almost all the CO2 emissions in the atmosphere in Europe.\nWe rank the risk factors and interactions according to the amount of CO2 they\ngenerate. In addition, we compare the present findings of the European data\nwith a similar study for the Continental United States [1, 2]. For example, in\nthe US, liquid fuels ranks number one, while in Europe is gas fuels. In fact,\nliquid fuels in Europe is the least contributable variable of CO2 in the\natmosphere, and gas fuels ranks seventh.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:51:39 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}]