[{"id": "1505.00055", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin and V.P. Roychowdhury", "title": "Chess players' fame versus their merit", "comments": "To appear in Applied Economics Letters", "journal-ref": "Applied Economics Letters 22(18):1499-1504, 2015", "doi": "10.1080/13504851.2015.1042135", "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a pool of international chess title holders born between 1901\nand 1943. Using Elo ratings we compute for every player his expected score in a\ngame with a randomly selected player from the pool. We use this figure as\nplayer's merit. We measure players' fame as the number of Google hits. The\ncorrelation between fame and merit is 0.38. At the same time the correlation\nbetween the logarithm of fame and merit is 0.61. This suggests that fame grows\nexponentially with merit.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 23:10:24 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Simkin", "M. V.", ""], ["Roychowdhury", "V. P.", ""]]}, {"id": "1505.00115", "submitter": "Alberto Baccini", "authors": "Alberto Baccini, Giuseppe De Nicolao", "title": "Do they agree? Bibliometric evaluation vs informed peer review in the\n  Italian research assessment exercise", "comments": "in Scientometrics, 2016", "journal-ref": null, "doi": "10.1007/s11192-016-1929-y", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the Italian research assessment exercise, the national agency ANVUR\nperformed an experiment to assess agreement between grades attributed to\njournal articles by informed peer review (IR) and by bibliometrics. A sample of\narticles was evaluated by using both methods and agreement was analyzed by\nweighted Cohen's kappas. ANVUR presented results as indicating an overall\n'good' or 'more than adequate' agreement. This paper re-examines the experiment\nresults according to the available statistical guidelines for interpreting\nkappa values, by showing that the degree of agreement, always in the range\n0.09-0.42 has to be interpreted, for all research fields, as unacceptable, poor\nor, in a few cases, as, at most, fair. The only notable exception, confirmed\nalso by a statistical meta-analysis, was a moderate agreement for economics and\nstatistics (Area 13) and its sub-fields. We show that the experiment protocol\nadopted in Area 13 was substantially modified with respect to all the other\nresearch fields, to the point that results for economics and statistics have to\nbe considered as fatally flawed. The evidence of a poor agreement supports the\nconclusion that IR and bibliometrics do not produce similar results, and that\nthe adoption of both methods in the Italian research assessment possibly\nintroduced systematic and unknown biases in its final results. The conclusion\nreached by ANVUR must be reversed: the available evidence does not justify at\nall the joint use of IR and bibliometrics within the same research assessment\nexercise.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 08:17:01 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 13:51:55 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Baccini", "Alberto", ""], ["De Nicolao", "Giuseppe", ""]]}, {"id": "1505.00275", "submitter": "Igor Volobouev", "authors": "D.P. Amali Dassanayake, Igor Volobouev, A. Alexandre Trindade", "title": "Local Orthogonal Polynomial Expansion for Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Local Orthogonal Polynomial Expansion (LOrPE) of the empirical density\nfunction is proposed as a novel method to estimate the underlying density. The\nestimate is constructed by matching localized expectation values of orthogonal\npolynomials to the values observed in the sample. LOrPE is related to several\nexisting methods, and generalizes straightforwardly to multivariate settings.\nBy manner of construction, it is similar to Local Likelihood Density Estimation\n(LLDE). In the limit of small bandwidths, LOrPE functions as Kernel Density\nEstimation (KDE) with high-order (effective) kernels inherently free of\nboundary bias, a natural consequence of kernel reshaping to accommodate\nendpoints. Faster asymptotic convergence rates follow. In the limit of large\nbandwidths, LOrPE is equivalent to Orthogonal Series Density Estimation (OSDE)\nwith Legendre polynomials. We compare the performance of LOrPE to KDE, LLDE,\nand OSDE, in a number of simulation studies. In terms of mean integrated\nsquared error, the results suggest that with a proper balance of the two tuning\nparameters, bandwidth and degree, LOrPE generally outperforms these competitors\nwhen estimating densities with sharply truncated supports.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:34:30 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Dassanayake", "D. P. Amali", ""], ["Volobouev", "Igor", ""], ["Trindade", "A. Alexandre", ""]]}, {"id": "1505.00456", "submitter": "Dan McQuillan", "authors": "Peter MacDonald, Dan McQuillan, Ian McQuillan", "title": "A Remark on Baserunning risk: Waiting Can Cost You the Game", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the value of a baserunner at first base waiting to see if a ball\nin play falls in for a hit, before running. When a ball is hit in the air, the\nbaserunner will usually wait, to gather additional information as to whether a\nball will fall for a hit before deciding to run aggressively. This additional\ninformation guarantees that there will not be a double play and an \"unnecessary\nout\". However, waiting could potentially cost the runner the opportunity to\nreach third base, or even scoring on the play if the ball falls for a hit. This\nin turn affects the probability of scoring at least one run henceforth in the\ninning. We create a new statistic, the baserunning risk threshold (BRT), which\nmeasures the minimum probability with which the baserunner should be sure that\na ball in play will fall in for a hit, before running without waiting to see if\nthe ball will be caught, with the goal of scoring at least one run in the\ninning. We measure a 0-out and a 1-out version of BRT, both in aggregate, and\nalso in high leverage situations, where scoring one run is particularly\nimportant. We show a drop in BRT for pitchers who pitch in more high leverage\ninnings, and a very low BRT on average for \"elite closers\". It follows that\nbaserunners should be frequently running without waiting, and getting thrown\nout in double plays regularly to maximize their chances of scoring at least one\nrun.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 17:44:16 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["MacDonald", "Peter", ""], ["McQuillan", "Dan", ""], ["McQuillan", "Ian", ""]]}, {"id": "1505.00562", "submitter": "RadhaKrishna Ganti", "authors": "Radha Krishna Ganti, Andrew Thangaraj and Arijit Mondal", "title": "Approximation of Capacity for ISI Channels with One-bit Output\n  Quantization", "comments": "Will be presented at ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent high bandwidth communication systems, Inter-Symbol\nInterference (ISI) channels with 1-bit quantized output are considered under an\naverage-power-constrained continuous input. While the exact capacity is\ndifficult to characterize, an approximation that matches with the exact channel\noutput up to a probability of error is provided. The approximation does not\nhave additive noise, but constrains the channel output (without noise) to be\nabove a threshold in absolute value. The capacity under the approximation is\ncomputed using methods involving standard Gibbs distributions. Markovian\nachievable schemes approaching the approximate capacity are provided. The\nmethods used over the approximate ISI channel result in ideas for practical\ncoding schemes for ISI channels with 1-bit output quantization.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 09:08:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Ganti", "Radha Krishna", ""], ["Thangaraj", "Andrew", ""], ["Mondal", "Arijit", ""]]}, {"id": "1505.00829", "submitter": "Steven Pav", "authors": "Steven E. Pav", "title": "Inference on the Sharpe ratio via the upsilon distribution", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upsilon distribution, the sum of independent chi random variates and a\nnormal, is introduced. As a special case, the upsilon distribution includes\nLecoutre's lambda-prime distribution. The upsilon distribution finds\napplication in Frequentist inference on the Sharpe ratio, including hypothesis\ntests on independent samples, confidence intervals, and prediction intervals,\nas well as their Bayesian counterparts. These tests are extended to the case of\nfactor models of returns.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 22:14:34 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 20:39:15 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Pav", "Steven E.", ""]]}, {"id": "1505.00864", "submitter": "Shihao Yang", "authors": "Shihao Yang, Mauricio Santillana, and S. C. Kou", "title": "Accurate estimation of influenza epidemics using Google search data via\n  ARGO", "comments": "23 pages, 2 figures, Proceedings of the National Academy of Sciences\n  (2015)", "journal-ref": null, "doi": "10.1073/pnas.1515373112", "report-no": null, "categories": "stat.AP cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time tracking of influenza outbreaks helps public health\nofficials make timely and meaningful decisions that could save lives. We\npropose an influenza tracking model, ARGO (AutoRegression with GOogle search\ndata), that uses publicly available online search data. In addition to having a\nrigorous statistical foundation, ARGO outperforms all previously available\nGoogle-search-based tracking models, including the latest version of Google Flu\nTrends, even though it uses only low-quality search data as input from publicly\navailable Google Trends and Google Correlate websites. ARGO not only\nincorporates the seasonality in influenza epidemics but also captures changes\nin people's online search behavior over time. ARGO is also flexible,\nself-correcting, robust, and scalable, making it a potentially powerful tool\nthat can be used for real-time tracking of other social events at multiple\ntemporal and spatial resolutions.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:10:18 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 19:33:43 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Yang", "Shihao", ""], ["Santillana", "Mauricio", ""], ["Kou", "S. C.", ""]]}, {"id": "1505.01147", "submitter": "Franz J. Kir\\'aly", "authors": "Duncan A.J. Blythe and Franz J. Kir\\'aly", "title": "Prediction and Quantification of Individual Athletic Performance", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0157257", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide scientific foundations for athletic performance prediction on an\nindividual level, exposing the phenomenology of individual athletic running\nperformance in the form of a low-rank model dominated by an individual power\nlaw. We present, evaluate, and compare a selection of methods for prediction of\nindividual running performance, including our own, \\emph{local matrix\ncompletion} (LMC), which we show to perform best. We also show that many\ndocumented phenomena in quantitative sports science, such as the form of\nscoring tables, the success of existing prediction methods including Riegel's\nformula, the Purdy points scheme, the power law for world records performances\nand the broken power law for world record speeds may be explained on the basis\nof our findings in a unified way.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 19:59:29 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 19:07:32 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Blythe", "Duncan A. J.", ""], ["Kir\u00e1ly", "Franz J.", ""]]}, {"id": "1505.01164", "submitter": "Emily Fox", "authors": "You Ren, Emily B. Fox, and Andrew Bruce", "title": "Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by\n  Bayesian Dynamical Modeling of Multiple Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how housing values evolve over time is important to policy\nmakers, consumers and real estate professionals. Existing methods for\nconstructing housing indices are computed at a coarse spatial granularity, such\nas metropolitan regions, which can mask or distort price dynamics apparent in\nlocal markets, such as neighborhoods and census tracts. A challenge in moving\nto estimates at, for example, the census tract level is the sparsity of\nspatiotemporally localized house sales observations. Our work aims at\naddressing this challenge by leveraging observations from multiple census\ntracts discovered to have correlated valuation dynamics. Our proposed Bayesian\nnonparametric approach builds on the framework of latent factor models to\nenable a flexible, data-driven method for inferring the clustering of\ncorrelated census tracts. We explore methods for scalability and\nparallelizability of computations, yielding a housing valuation index at the\nlevel of census tract rather than zip code, and on a monthly basis rather than\nquarterly. Our analysis is provided on a large Seattle metropolitan housing\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:01:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Ren", "You", ""], ["Fox", "Emily B.", ""], ["Bruce", "Andrew", ""]]}, {"id": "1505.01187", "submitter": "Changshuai Wei", "authors": "Changshuai Wei, and Qing Lu", "title": "GWGGI: software for genome-wide gene-gene interaction analysis", "comments": null, "journal-ref": "BMC Genetics 2014, 15:101", "doi": "10.1186/s12863-014-0101-z", "report-no": null, "categories": "q-bio.QM cs.DS q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: While the importance of gene-gene interactions in human diseases\nhas been well recognized, identifying them has been a great challenge,\nespecially through association studies with millions of genetic markers and\nthousands of individuals. Computationally efficient and powerful tools are in\ngreat need for the identification of new gene-gene interactions in\nhigh-dimensional association studies. Result: We develop C++ software for\ngenome-wide gene-gene interaction analyses (GWGGI). GWGGI utilizes tree-based\nalgorithms to search a large number of genetic markers for a disease-associated\njoint association with the consideration of high-order interactions, and then\nuses non-parametric statistics to test the joint association. The package\nincludes two functions, likelihood ratio Mann-whitney (LRMW) and Tree\nAssembling Mann-whitney (TAMW).We optimize the data storage and computational\nefficiency of the software, making it feasible to run the genome-wide analysis\non a personal computer. The use of GWGGI was demonstrated by using two real\ndata-sets with nearly 500 k genetic markers. Conclusion: Through the empirical\nstudy, we demonstrated that the genome-wide gene-gene interaction analysis\nusing GWGGI could be accomplished within a reasonable time on a personal\ncomputer (i.e., ~3.5 hours for LRMW and ~10 hours for TAMW). We also showed\nthat LRMW was suitable to detect interaction among a small number of genetic\nvariants with moderate-to-strong marginal effect, while TAMW was useful to\ndetect interaction among a larger number of low-marginal-effect genetic\nvariants.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 21:11:22 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Wei", "Changshuai", ""], ["Lu", "Qing", ""]]}, {"id": "1505.01316", "submitter": "Andrew Lover", "authors": "Andrew A. Lover", "title": "Short Report: Study variability in recent human challenge experiments\n  with Plasmodium falciparum sporozoites (PfSPZ Challenge)", "comments": "8 pages with 1 figure, 3 tables, 2 appendices; submitted manuscript", "journal-ref": null, "doi": "10.4269/ajtmh.15-0327.", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been renewed interest in the use of sporozoite-based approaches for\nmalaria vaccination and controlled human infections, and several sets of human\nchallenge studies have recently completed. A study undertaken in Tanzania and\npublished in 2014 found dose-dependence between 10,000 and 25,000 sporozoite\ndoses, as well as divergent times-to-parasitemia relative to earlier studies in\nEuropean volunteers. However, this analysis shows that these conclusions are\nbased upon suboptimal analytical methods; with more optimal analysis, there is\nno evidence for dose-dependence within this dose range; and more importantly,\nno evidence for differences in event times between Dutch and Tanzanian study\nsites. While these finding do not impact the reported safety and tolerability\nof PfSPZ, they highlight critical issues that should be comprehensively\nconsidered in future challenge studies.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:47:45 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Lover", "Andrew A.", ""]]}, {"id": "1505.01547", "submitter": "Gordon J Ross", "authors": "Gordon J Ross and Tim Jones", "title": "Understanding the Heavy Tailed Dynamics in Human Behavior", "comments": "9 pages in Physical Review E, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent availability of electronic datasets containing large volumes of\ncommunication data has made it possible to study human behavior on a larger\nscale than ever before. From this, it has been discovered that across a diverse\nrange of data sets, the inter-event times between consecutive communication\nevents obey heavy tailed power law dynamics. Explaining this has proved\ncontroversial, and two distinct hypotheses have emerged. The first holds that\nthese power laws are fundamental, and arise from the mechanisms such as\npriority queuing that humans use to schedule tasks. The second holds that they\nare a statistical artifact which only occur in aggregated data when features\nsuch as circadian rhythms and burstiness are ignored. We use a large social\nmedia data set to test these hypotheses, and find that although models that\nincorporate circadian rhythms and burstiness do explain part of the observed\nheavy tails, there is residual unexplained heavy tail behavior which suggests a\nmore fundamental cause. Based on this, we develop a new quantitative model of\nhuman behavior which improves on existing approaches, and gives insight into\nthe mechanisms underlying human interactions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 00:12:24 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ross", "Gordon J", ""], ["Jones", "Tim", ""]]}, {"id": "1505.01668", "submitter": "Mark Ryan Leonard", "authors": "Mark R. Leonard and Abdelhak M. Zoubir", "title": "Multi-Target Tracking in Distributed Sensor Networks using Particle PHD\n  Filters", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target tracking is an important problem in civilian and military\napplications. This paper investigates multi-target tracking in distributed\nsensor networks. Data association, which arises particularly in multi-object\nscenarios, can be tackled by various solutions. We consider sequential Monte\nCarlo implementations of the Probability Hypothesis Density (PHD) filter based\non random finite sets. This approach circumvents the data association issue by\njointly estimating all targets in the region of interest. To this end, we\ndevelop the Diffusion Particle PHD Filter (D-PPHDF) as well as a centralized\nversion, called the Multi-Sensor Particle PHD Filter (MS-PPHDF). Their\nperformance is evaluated in terms of the Optimal Subpattern Assignment (OSPA)\nmetric, benchmarked against a distributed extension of the Posterior\nCram\\'er-Rao Lower Bound (PCRLB), and compared to the performance of an\nexisting distributed PHD Particle Filter. Furthermore, the robustness of the\nproposed tracking algorithms against outliers and their performance with\nrespect to different amounts of clutter is investigated.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 11:31:12 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 10:47:17 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 10:33:43 GMT"}, {"version": "v4", "created": "Mon, 9 Oct 2017 09:13:02 GMT"}, {"version": "v5", "created": "Sun, 2 Dec 2018 14:30:12 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Leonard", "Mark R.", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1505.02072", "submitter": "Andrew Lover", "authors": "Andrew A. Lover", "title": "Epidemiology of Latency and Relapse in Plasmodium vivax Malaria", "comments": "PhD thesis (2015); 128 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a major contributor to health burdens throughout the regions where\nit is endemic. Historically, it was believed that there was limited morbidity\nand essentially no mortality associated with Plasmodium vivax; however,\nevidence from diverse settings now suggests that infections with P. vivax can\nbe both severe and fatal. This awareness has highlighted a critical gap: the\nvast majority of research has been directed towards P. falciparum, leading to a\ndecades-long neglect of epidemiological and clinical studies of P. vivax. There\nexists a large body of historical data on human experimental infections with P.\nvivax; these studies in controlled settings provided a wealth of wide-ranging\nstatements based on expert opinion, which form the basis for much of what is\ncurrently known about P. vivax. In this thesis, portions of this evidence-base\nhave been re-examined using modern epidemiological analyses with two aims: to\ncritically examine this accumulated knowledge base, and to inform current\nresearch agendas towards global malaria elimination for all species of\nPlasmodium. Chapter 2 examines geographic variation in the epidemiology of P.\nvivax, especially the timing of incubation periods and of relapses, by origin\nof the parasites. Chapter 3 re-assesses the impact of sporozoite dosage upon\nincubation and pre-patent periods; Chapter 4 provides well-defined mathematical\ndistributions for incubation and relapses periods in experimental infections,\nand explores their epidemiological impacts using simple transmission models.\nChapter 5 examines the epidemiology of mixed-strain P. vivax infections and\ncompares these results with studies in murine models and general ecological\ntheory; and Chapter 6 clarifies the origin of the Madagascar strain of P.\nvivax.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:57:46 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Lover", "Andrew A.", ""]]}, {"id": "1505.02350", "submitter": "Andrea Saltelli", "authors": "Sergei Kucherenko, Daniel Albrecht, Andrea Saltelli", "title": "Exploring multi-dimensional spaces: a Comparison of Latin Hypercube and\n  Quasi Monte Carlo Sampling Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three sampling methods are compared for efficiency on a number of test\nproblems of various complexity for which analytic quadratures are available.\nThe methods compared are Monte Carlo with pseudo-random numbers, Latin\nHypercube Sampling, and Quasi Monte Carlo with sampling based on Sobol\nsequences. Generally results show superior performance of the Quasi Monte Carlo\napproach based on Sobol sequences in line with theoretical predictions. Latin\nHypercube Sampling can be more efficient than both Monte Carlo method and Quasi\nMonte Carlo method but the latter inequality holds for a reduced set of\nfunction typology and at small number of sampled points. In conclusion Quasi\nMonte Carlo method would appear the safest bet when integrating functions of\nunknown typology.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 07:16:41 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kucherenko", "Sergei", ""], ["Albrecht", "Daniel", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1505.02589", "submitter": "Garritt L. Page", "authors": "Garritt L. Page, Fernando A. Quintana", "title": "Predictions Based on the Clustering of Heterogeneous Functions via Shape\n  and Subject-Specific Covariates", "comments": "Published at http://dx.doi.org/10.1214/14-BA919 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 379-410", "doi": "10.1214/14-BA919", "report-no": "VTeX-BA-BA919", "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a study of players employed by teams who are members of the\nNational Basketball Association where units of observation are functional\ncurves that are realizations of production measurements taken through the\ncourse of one's career. The observed functional output displays large amounts\nof between player heterogeneity in the sense that some individuals produce\ncurves that are fairly smooth while others are (much) more erratic. We argue\nthat this variability in curve shape is a feature that can be exploited to\nguide decision making, learn about processes under study and improve\nprediction. In this paper we develop a methodology that takes advantage of this\nfeature when clustering functional curves. Individual curves are flexibly\nmodeled using Bayesian penalized B-splines while a hierarchical structure\nallows the clustering to be guided by the smoothness of individual curves. In a\nsense, the hierarchical structure balances the desire to fit individual curves\nwell while still producing meaningful clusters that are used to guide\nprediction. We seamlessly incorporate available covariate information to guide\nthe clustering of curves non-parametrically through the use of a product\npartition model prior for a random partition of individuals. Clustering based\non curve smoothness and subject-specific covariate information is particularly\nimportant in carrying out the two types of predictions that are of interest,\nthose that complete a partially observed curve from an active player, and those\nthat predict the entire career curve for a player yet to play in the National\nBasketball Association.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:47:08 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "1505.02590", "submitter": "Scott H. Holan", "authors": "Guohui Wu, Scott H. Holan, Charles H. Nilon, Christopher K. Wikle", "title": "Bayesian binomial mixture models for estimating abundance in ecological\n  monitoring studies", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS801 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 1-26", "doi": "10.1214/14-AOAS801", "report-no": "IMS-AOAS-AOAS801", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of species abundance has become a vital component of many\necological monitoring studies. The primary objective of these studies is to\nunderstand how specific species are distributed across the study domain, as\nwell as quantification of the sampling efficiency for detecting these species.\nTo achieve these goals, preselected locations are sampled during scheduled\nvisits, in which the number of species observed at each location is recorded.\nThis results in spatially referenced replicated count data that are often\nunbalanced in structure and exhibit overdispersion. Motivated by the Baltimore\nEcosystem Study, we propose Bayesian hierarchical binomial mixture models,\nincluding Binomial Conway-Maxwell Poisson (Bin-CMP) mixture models, that\nformally account for varying levels of spatial dispersion. Our proposed models\nalso allow for variable selection of model covariates and grouping of\ndispersion parameters through the implementation of reversible jump Markov\nchain Monte Carlo methodology. Finally, using demographic covariates from the\nAmerican Community Survey, we demonstrate the effectiveness of our approach\nthrough estimation of abundance for the American Robin (Turdus migratorius) in\nthe Baltimore Ecosystem Study.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:48:06 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Wu", "Guohui", ""], ["Holan", "Scott H.", ""], ["Nilon", "Charles H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1505.02886", "submitter": "Timothy Hanson", "authors": "Haiming Zhou, Timothy Hanson, Alejandro Jara, Jiajia Zhang", "title": "Modeling county level breast cancer survival data using a\n  covariate-adjusted frailty proportional hazards model", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS793 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 43-68", "doi": "10.1214/14-AOAS793", "report-no": "IMS-AOAS-AOAS793", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the factors that explain differences in survival times is an\nimportant issue for establishing policies to improve national health systems.\nMotivated by breast cancer data arising from the Surveillance Epidemiology and\nEnd Results program, we propose a covariate-adjusted proportional hazards\nfrailty model for the analysis of clustered right-censored data. Rather than\nincorporating exchangeable frailties in the linear predictor of commonly-used\nsurvival models, we allow the frailty distribution to flexibly change with both\ncontinuous and categorical cluster-level covariates and model them using a\ndependent Bayesian nonparametric model. The resulting process is flexible and\neasy to fit using an existing R package. The application of the model to our\nmotivating example showed that, contrary to intuition, those diagnosed during a\nperiod of time in the 1990s in more rural and less affluent Iowan counties\nsurvived breast cancer better. Additional analyses showed the opposite trend\nfor earlier time windows. We conjecture that this anomaly has to be due to\nincreased hormone replacement therapy treatments prescribed to more urban and\naffluent subpopulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 06:54:38 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Zhou", "Haiming", ""], ["Hanson", "Timothy", ""], ["Jara", "Alejandro", ""], ["Zhang", "Jiajia", ""]]}, {"id": "1505.03366", "submitter": "Mohammed Sedki", "authors": "Matthieu Marbac and Pascale Tubert-Bitter and Mohammed Sedki", "title": "Bayesian model selection in logistic regression for the detection of\n  adverse drug reactions", "comments": "7 pages, 3 figures, submitted to Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Spontaneous adverse event reports have a high potential for\ndetecting adverse drug reactions. However, due to their dimension, exploring\nsuch databases requires statistical methods. In this context,\ndisproportionality measures are used. However, by projecting the data onto\ncontingency tables, these methods become sensitive to the problem of\nco-prescriptions and masking effects. Recently, logistic regressions have been\nused with a Lasso type penalty to perform the detection of associations between\ndrugs and adverse events. However, the choice of the penalty value is open to\ncriticism while it strongly influences the results. Results: In this paper, we\npropose to use a logistic regression whose sparsity is viewed as a model\nselection challenge. Since the model space is huge, a Metropolis-Hastings\nalgorithm carries out the model selection by maximizing the BIC criterion.\nThus, we avoid the calibration of penalty or threshold. During our application\non the French pharmacovigilance database, the proposed method is compared to\nwell established approaches on a reference data set, and obtains better rates\nof positive and negative controls. However, many signals are not detected by\nthe proposed method. So, we conclude that this method should be used in\nparallel to existing measures in pharmacovigilance.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 13:07:38 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 08:39:39 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Marbac", "Matthieu", ""], ["Tubert-Bitter", "Pascale", ""], ["Sedki", "Mohammed", ""]]}, {"id": "1505.03506", "submitter": "Konstantin Zuev M", "authors": "Konstantin Zuev", "title": "Subset Simulation Method for Rare Event Estimation: An Introduction", "comments": "14 page, 12 figures, MATLAB code in Zuev K.: Subset Simulation Method\n  for Rare Event Estimation: An Introduction. In: Beer M. et al (Ed.)\n  Encyclopedia of Earthquake Engineering: SpringerReference\n  (www.springerreference.com), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a detailed introductory description of Subset Simulation,\nan advanced stochastic simulation method for estimation of small probabilities\nof rare failure events. A simple and intuitive derivation of the method is\ngiven along with the discussion on its implementation. The method is\nillustrated with several easy-to-understand examples. For demonstration\npurposes, the MATLAB code for the considered examples is provided. The reader\nis assumed to be familiar only with elementary probability theory and\nstatistics.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 19:41:04 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Zuev", "Konstantin", ""]]}, {"id": "1505.03609", "submitter": "Qingzhao Zhang PhD", "authors": "Hao Chai, Qingzhao Zhang, Yu Jiang, Guohua Wang, Sanguo Zhang and\n  Shuangge Ma", "title": "A Robust Approach for Identifying Gene-Environment Interactions for\n  Prognosis", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many complex diseases, prognosis is of essential importance. It has been\nshown that, beyond the main effects of genetic (G) and environmental (E) risk\nfactors, the gene-environment (G$\\times$E) interactions also play a critical\nrole. In practice, the prognosis outcome data can be contaminated, and most of\nthe existing methods are not robust to data contamination. In the literature,\nit has been shown that even a single contaminated observation can lead to\nseverely biased model estimation. In this study, we describe prognosis using an\naccelerated failure time (AFT) model. An exponential squared loss is proposed\nto accommodate possible data contamination. A penalization approach is adopted\nfor regularized estimation and marker selection. The proposed method is\nrealized using an effective coordinate descent (CD) and minorization\nmaximization (MM) algorithm. Simulation shows that without contamination, the\nproposed method has performance comparable to or better than the unrobust\nalternative. With contamination, it outperforms the unrobust alternative and,\nunder certain scenarios, can be superior to the robust method based on quantile\nregression. The proposed method is applied to the analysis of TCGA (The Cancer\nGenome Atlas) lung cancer data. It identifies interactions different from those\nusing the alternatives. The identified marker have important implications and\nsatisfactory stability.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 03:37:58 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Chai", "Hao", ""], ["Zhang", "Qingzhao", ""], ["Jiang", "Yu", ""], ["Wang", "Guohua", ""], ["Zhang", "Sanguo", ""], ["Ma", "Shuangge", ""]]}, {"id": "1505.03789", "submitter": "Olivier Besson", "authors": "Yuri Abramovich, Olivier Besson, Ben Johnson", "title": "Bounds for maximum likelihood regular and non-regular DoA estimation in\n  $K$-distributed noise", "comments": "25 pages, 8 figures, submitted to IEEE Transactions Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2460218", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the direction of arrival of a signal\nembedded in $K$-distributed noise, when secondary data which contains noise\nonly are assumed to be available. Based upon a recent formula of the Fisher\ninformation matrix (FIM) for complex elliptically distributed data, we provide\na simple expression of the FIM with the two data sets framework. In the\nspecific case of $K$-distributed noise, we show that, under certain conditions,\nthe FIM for the deterministic part of the model can be unbounded, while the FIM\nfor the covariance part of the model is always bounded. In the general case of\nelliptical distributions, we provide a sufficient condition for unboundedness\nof the FIM. Accurate approximations of the FIM for $K$-distributed noise are\nalso derived when it is bounded. Additionally, the maximum likelihood estimator\nof the signal DoA and an approximated version are derived, assuming known\ncovariance matrix: the latter is then estimated from secondary data using a\nconventional regularization technique. When the FIM is unbounded, an analysis\nof the estimators reveals a rate of convergence much faster than the usual\n$T^{-1}$. Simulations illustrate the different behaviors of the estimators,\ndepending on the FIM being bounded or not.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 16:33:51 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Abramovich", "Yuri", ""], ["Besson", "Olivier", ""], ["Johnson", "Ben", ""]]}, {"id": "1505.04008", "submitter": "Agnieszka Prochenka", "authors": "Aleksandra Maj-Ka\\'nska, Piotr Pokarowski and Agnieszka Prochenka", "title": "Delete or merge regressors for linear model selection", "comments": null, "journal-ref": null, "doi": "10.1214/15-EJS1050", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of linear model selection in the presence of both\ncontinuous and categorical predictors. Feasible models consist of subsets of\nnumerical variables and partitions of levels of factors. A new algorithm called\ndelete or merge regressors (DMR) is presented which is a stepwise backward\nprocedure involving ranking the predictors according to squared t-statistics\nand choosing the final model minimizing BIC. In the article we prove\nconsistency of DMR when the number of predictors tends to infinity with the\nsample size and describe a simulation study using a pertaining R package. The\nresults indicate significant advantage in time complexity and selection\naccuracy of our algorithm over Lasso-based methods described in the literature.\nMoreover, a version of DMR for generalized linear models is proposed.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 10:11:39 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Maj-Ka\u0144ska", "Aleksandra", ""], ["Pokarowski", "Piotr", ""], ["Prochenka", "Agnieszka", ""]]}, {"id": "1505.04066", "submitter": "Jacek Urbanek PhD", "authors": "Jacek K. Urbanek, Vadim Zipunnikov, Tamara Harris, William Fadel,\n  Nancy Glynn, Annemarie Koster, Paolo Caserotti, Ciprian Crainiceanu, Jaroslaw\n  Harezlak", "title": "Prediction of sustained harmonic walking in the free-living environment\n  using raw accelerometry data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Using raw, sub-second level, accelerometry data, we propose and\nvalidate a method for identifying and characterizing walking in the free-living\nenvironment. We focus on the sustained harmonic walking (SHW), which we define\nas walking for at least 10 seconds with low variability of step frequency.\nApproach. We utilize the harmonic nature of SHW and quantify local periodicity\nof the tri-axial raw accelerometry data. We also estimate fundamental frequency\nof observed signals and link it to the instantaneous walking (step-to-step)\nfrequency (IWF). Next, we report total time spent in SHW, number and durations\nof SHW bouts, time of the day when SHW occurred and IWF for 49 healthy, elderly\nindividuals. Main results. Sensitivity of the proposed classification method\nwas found to be 97%, while specificity ranged between 87% and 97% and\nprediction accuracy between 94% and 97%. We report total time in SHW between\n140 and 10 minutes-per-day distributed between 340 and 50 bouts. We estimate\nthe average IWF to be 1.7 steps-per-second. Significance. We propose a simple\napproach for detection of SHW and estimation of IWF, based on Fourier\ndecomposition. The resulting approach is fast and allows processing of a\nweek-long raw accelerometry data (approx. 150 million measurements) in\nrelatively short time (~half an hour) on a common laptop computer (2.8 GHz\nIntel Core i7, 16 GB DDR3 RAM).\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 13:55:12 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:03:00 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Urbanek", "Jacek K.", ""], ["Zipunnikov", "Vadim", ""], ["Harris", "Tamara", ""], ["Fadel", "William", ""], ["Glynn", "Nancy", ""], ["Koster", "Annemarie", ""], ["Caserotti", "Paolo", ""], ["Crainiceanu", "Ciprian", ""], ["Harezlak", "Jaroslaw", ""]]}, {"id": "1505.04303", "submitter": "Jie Sun", "authors": "Erik M. Bollt, Jie Sun", "title": "Editorial Comment on the Special Issue of \"Information in Dynamical\n  Systems and Complex Systems\"", "comments": null, "journal-ref": "Entropy 16, 4992-5001 (2014)", "doi": "10.3390/e16094992", "report-no": null, "categories": "nlin.CD math-ph math.DS math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This special issue collects contributions from the participants of the\n\"Information in Dynamical Systems and Complex Systems\" workshop, which cover a\nwide range of important problems and new approaches that lie in the\nintersection of information theory and dynamical systems. The contributions\ninclude theoretical characterization and understanding of the different types\nof information flow and causality in general stochastic processes, inference\nand identification of coupling structure and parameters of system dynamics,\nrigorous coarse-grain modeling of network dynamical systems, and exact\nstatistical testing of fundamental information-theoretic quantities such as the\nmutual information. The collective efforts reported herein reflect a modern\nperspective of the intimate connection between dynamical systems and\ninformation flow, leading to the promise of better understanding and modeling\nof natural complex systems and better/optimal design of engineering systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 17:58:36 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Bollt", "Erik M.", ""], ["Sun", "Jie", ""]]}, {"id": "1505.04305", "submitter": "Jie Sun", "authors": "Bing Wang, Jie Sun, Adilson E. Motter", "title": "Detecting structural breaks in seasonal time series by regularized\n  optimization", "comments": "Safety, Reliability, Risk and Life-Cycle Performance of Structures\n  and Infrastructures (Edited by George Deodatis, Bruce R. Ellingwood and Dan\n  M. Frangopol), CRC Press 2014, Pages 3621-3628", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world systems are often complex, dynamic, and nonlinear. Understanding\nthe dynamics of a system from its observed time series is key to the prediction\nand control of the system's behavior. While most existing techniques tacitly\nassume some form of stationarity or continuity, abrupt changes, which are often\ndue to external disturbances or sudden changes in the intrinsic dynamics, are\ncommon in time series. Structural breaks, which are time points at which the\nstatistical patterns of a time series change, pose considerable challenges to\ndata analysis. Without identification of such break points, the same dynamic\nrule would be applied to the whole period of observation, whereas false\nidentification of structural breaks may lead to overfitting. In this paper, we\ncast the problem of decomposing a time series into its trend and seasonal\ncomponents as an optimization problem. This problem is ill-posed due to the\narbitrariness in the number of parameters. To overcome this difficulty, we\npropose the addition of a penalty function (i.e., a regularization term) that\naccounts for the number of parameters. Our approach simultaneously identifies\nseasonality and trend without the need of iterations, and allows the reliable\ndetection of structural breaks. The method is applied to recorded data on fish\npopulations and sea surface temperature, where it detects structural breaks\nthat would have been neglected otherwise. This suggests that our method can\nlead to a general approach for the monitoring, prediction, and prevention of\nstructural changes in real systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 18:10:54 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Wang", "Bing", ""], ["Sun", "Jie", ""], ["Motter", "Adilson E.", ""]]}, {"id": "1505.04319", "submitter": "Alexandra Schmidt", "authors": "Alexandra M. Schmidt, Marco A. Rodr\\'iguez and Estelina S. Capistrano", "title": "Population counts along elliptical habitat contours: hierarchical\n  modelling using Poisson-lognormal mixtures with nonstationary spatial\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecologists often interpret variation in the spatial distribution of\npopulations in terms of responses to environmental features, but disentangling\nthe effects of individual variables can be difficult if latent effects and\nspatial and temporal correlations are not accounted for properly. Here, we use\nhierarchical models based on a Poisson log-normal mixture to understand the\nspatial variation in relative abundance (counts per standardized unit of\neffort) of yellow perch, Perca flavescens, the most abundant fish species in\nLake Saint Pierre, Quebec, Canada. The mixture incorporates spatially varying\nenvironmental covariates that represent local habitat characteristics, and\nrandom temporal and spatial effects that capture the effects of unobserved\necological processes. The sampling design covers the margins but not the\ncentral region of the lake. We fit spatial generalized linear mixed models\nbased on three different prior covariance structures for the local latent\neffects: a single Gaussian process (GP) over the lake, a GP over a circle, and\nindependent GP for each shore. The models allow for independence, isotropy, or\nnonstationary spatial effects. Nonstationarity is dealt with using two\ndifferent approaches, geometric anisotropy, and the inclusion of covariates in\nthe correlation structure of the latent spatial process. The proposed\napproaches for specification of spatial domain and choice of Gaussian process\npriors may prove useful in other applications that involve spatial correlation\nalong an irregular contour or in discontinous spatial domains.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 20:00:34 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Schmidt", "Alexandra M.", ""], ["Rodr\u00edguez", "Marco A.", ""], ["Capistrano", "Estelina S.", ""]]}, {"id": "1505.04629", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Peng Ding, Tirthankar Dasgupta", "title": "Construction of alternative hypotheses for randomization tests with\n  ordinal outcomes", "comments": null, "journal-ref": "Stat. Prob. Lett., 107:348-355 (2015)", "doi": "10.1016/j.spl.2015.09.013", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ordinal outcomes, we construct sequences of alternative hypotheses in\nincreasing departures from the sharp null hypothesis of zero treatment effect\non each experimental unit, to help assess the powers of randomization tests in\nrandomized treatment-control experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:24:15 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 17:54:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lu", "Jiannan", ""], ["Ding", "Peng", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1505.04697", "submitter": "Adam Sales", "authors": "Adam C Sales, Ben B Hansen, Brian Rowan", "title": "Rebar: Reinforcing a Matching Estimator with Predictions from\n  High-Dimensional Covariates", "comments": "Published in Journal of Educational and Behavioral Statistics\n  (Currently 12/6/17 \"Online First\")", "journal-ref": "Journal of Educational and Behavioral Statistics, 43(1), 3-31", "doi": "10.3102/1076998617731518", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In causal matching designs, some control subjects are often left unmatched,\nand some covariates are often left unmodeled. This article introduces \"rebar,\"\na method using high-dimensional modeling to incorporate these commonly\ndiscarded data without sacrificing the integrity of the matching design. After\nconstructing a match, a researcher uses the unmatched control subjects--the\nremnant--to fit a machine learning model predicting control potential outcomes\nas a function of the full covariate matrix. The resulting predictions in the\nmatched set are used to adjust the causal estimate to reduce confounding bias.\nWe present theoretical results to justify the method's bias-reducing properties\nas well as a simulation study that demonstrates them. Additionally, we\nillustrate the method in an evaluation of a school-level comprehensive\neducational reform program in Arizona.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 16:00:25 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 19:51:20 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 16:04:31 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Sales", "Adam C", ""], ["Hansen", "Ben B", ""], ["Rowan", "Brian", ""]]}, {"id": "1505.04722", "submitter": "Nassim Nicholas Taleb", "authors": "Pasquale Cirillo and Nassim Nicholas Taleb", "title": "On the statistical properties and tail risk of violent conflicts", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications 429,\n  252-260, 2016", "doi": "10.1016/j.physa.2016.01.050", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine statistical pictures of violent conflicts over the last 2000\nyears, finding techniques for dealing with incompleteness and unreliability of\nhistorical data. We introduce a novel approach to apply extreme value theory to\nfat-tailed variables that have a remote, but nonetheless finite upper bound, by\ndefining a corresponding unbounded dual distribution (given that potential war\ncasualties are bounded by the world population). We apply methods from extreme\nvalue theory on the dual distribution and derive its tail properties. The dual\nmethod allows us to calculate the real mean of war casualties, which proves to\nbe considerably larger than the sample mean, meaning severe underestimation of\nthe tail risks of conflicts from naive observation. We analyze the robustness\nof our results to errors in historical reports, taking into account the\nunreliability of accounts by historians and absence of critical data. We study\ninter-arrival times between tail events and find that no particular trend can\nbe asserted. All the statistical pictures obtained are at variance with the\nprevailing claims about \"long peace\", namely that violence has been declining\nover time.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 17:05:51 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 18:07:29 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Cirillo", "Pasquale", ""], ["Taleb", "Nassim Nicholas", ""]]}, {"id": "1505.04768", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela, Victor M. Panaretos", "title": "Statistical unfolding of elementary particle spectra: Empirical Bayes\n  estimation and bias-corrected uncertainty quantification", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS857 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note:\n  substantial text overlap with arXiv:1401.8274", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1671-1705", "doi": "10.1214/15-AOAS857", "report-no": "IMS-AOAS-AOAS857", "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high energy physics unfolding problem where the goal is to\nestimate the spectrum of elementary particles given observations distorted by\nthe limited resolution of a particle detector. This important statistical\ninverse problem arising in data analysis at the Large Hadron Collider at CERN\nconsists in estimating the intensity function of an indirectly observed Poisson\npoint process. Unfolding typically proceeds in two steps: one first produces a\nregularized point estimate of the unknown intensity and then uses the\nvariability of this estimator to form frequentist confidence intervals that\nquantify the uncertainty of the solution. In this paper, we propose forming the\npoint estimate using empirical Bayes estimation which enables a data-driven\nchoice of the regularization strength through marginal maximum likelihood\nestimation. Observing that neither Bayesian credible intervals nor standard\nbootstrap confidence intervals succeed in achieving good frequentist coverage\nin this problem due to the inherent bias of the regularized point estimate, we\nintroduce an iteratively bias-corrected bootstrap technique for constructing\nimproved confidence intervals. We show using simulations that this enables us\nto achieve nearly nominal frequentist coverage with only a modest increase in\ninterval length. The proposed methodology is applied to unfolding the $Z$ boson\ninvariant mass spectrum as measured in the CMS experiment at the Large Hadron\nCollider.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:26:46 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 13:01:49 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 13:25:17 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Kuusela", "Mikael", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1505.04827", "submitter": "Roland Langrock", "authors": "Ruth King and Roland Langrock", "title": "Semi-Markov Arnason-Schwarz models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-state capture-recapture-recovery data where observed\nindividuals are recorded in a set of possible discrete states. Traditionally,\nthe Arnason-Schwarz model has been fitted to such data where the state process\nis modeled as a first-order Markov chain, though second-order models have also\nbeen proposed and fitted to data. However, low-order Markov models may not\naccurately represent the underlying biology. For example, specifying a\n(time-independent) first-order Markov process assumes that the dwell time in\neach state (i.e., the duration of a stay in a given state) has a geometric\ndistribution, and hence that the modal dwell time is one. Specifying\ntime-dependent or higher-order processes provides additional flexibility, but\nat the expense of a potentially significant number of additional model\nparameters. We extend the Arnason-Schwarz model by specifying a semi-Markov\nmodel for the state process, where the dwell-time distribution is specified\nmore generally, using for example a shifted Poisson or negative binomial\ndistribution. A state expansion technique is applied in order to represent the\nresulting semi-Markov Arnason-Schwarz model in terms of a simpler and\ncomputationally tractable hidden Markov model. Semi-Markov Arnason-Schwarz\nmodels come with only a very modest increase in the number of parameters, yet\npermit a significantly more flexible state process. Model selection can be\nperformed using standard procedures, and in particular via the use of\ninformation criteria. The semi-Markov approach allows for important biological\ninference to be drawn on the underlying state process, for example on the times\nspent in the different states. The feasibility of the approach is demonstrated\nin a simulation study, before being applied to real data corresponding to house\nfinches where the states correspond to the presence or absence of\nconjunctivitis.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 21:55:27 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["King", "Ruth", ""], ["Langrock", "Roland", ""]]}, {"id": "1505.04883", "submitter": "Chandler Zuo", "authors": "Chandler Zuo, Kailei Chen, Kyle Hewitt, Emery Bresnick, Sunduz Keles", "title": "A Hierarchical Framework for State Space Matrix Inference and Clustering", "comments": "65 pages, 27 figures", "journal-ref": "Annals of Applied Statistics. Volume 10, Number 3 (2016),\n  1348-1372", "doi": "10.1214/16-AOAS938", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a large number of genomic and epigenomic studies have been\nfocusing on the integrative analysis of multiple experimental datasets measured\nover a large number of observational units. The objectives of such studies\ninclude not only inferring a hidden state of activity for each unit over\nindividual experiments, but also detecting highly associated clusters of units\nbased on their inferred states. In this paper, we develop the MBASIC (Matrix\nBased Analysis for State-space Inference and Clustering) framework. MBASIC\nconsists of two parts: state-space mapping and state-space clustering. In\nstate-space mapping, it maps observations onto a finite state-space,\nrepresenting the activation states of units across conditions. In state-space\nclustering, MBASIC incorporates a finite mixture model to cluster the units\nbased on their inferred state-space profiles across all conditions. Both the\nstate-space mapping and clustering can be simultaneously estimated through an\nExpectation-Maximization algorithm. MBASIC flexibly adapts to a large number of\nparametric distributions for the observed data, as well as the heterogeneity in\nreplicate experiments. In our data-driven simulation studies, MBASIC showed\nsignificant accuracy in recovering both the underlying state-space variables\nand clustering structures. We applied MBASIC to two genome research problems\nusing large numbers of datasets from the ENCODE project. In both studies,\nMBASIC showed higher levels of raw data fidelity than analyzing these data with\na two-step approach using ENCODE results on transcription factor occupancy\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 06:21:37 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 17:01:05 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zuo", "Chandler", ""], ["Chen", "Kailei", ""], ["Hewitt", "Kyle", ""], ["Bresnick", "Emery", ""], ["Keles", "Sunduz", ""]]}, {"id": "1505.05184", "submitter": "Tsvetan Asamov", "authors": "Christina M. Young, Mingyu Li, Yada Zhu, Minge Xie, Elsayed A.\n  Elsayed, and Tsvetan Asamov", "title": "Multi-Objective Optimization of a Port-of-Entry Inspection Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the port-of-entry containers are inspected through a specific sequence of\nsensor stations to detect the presence of nuclear materials, biological and\nchemical agents, and other illegal cargo. The inspection policy, which includes\nthe sequence in which sensors are applied and the threshold levels used at the\ninspection stations, affects the probability of misclassifying a container as\nwell as the cost and time spent in inspection. In this paper we consider a\nsystem operating with a Boolean decision function combining station results and\npresent a multi-objective optimization approach to determine the optimal sensor\narrangement and threshold levels while considering cost and time. The total\ncost includes cost incurred by misclassification errors and the total expected\ncost of inspection, while the time represents the total expected time a\ncontainer spends in the inspection system. An example which applies the\napproach in a theoretical inspection system is presented.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 20:52:54 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Young", "Christina M.", ""], ["Li", "Mingyu", ""], ["Zhu", "Yada", ""], ["Xie", "Minge", ""], ["Elsayed", "Elsayed A.", ""], ["Asamov", "Tsvetan", ""]]}, {"id": "1505.05482", "submitter": "Michelle Miranda", "authors": "Michelle F. Miranda, Hongtu Zhu and Joseph G. Ibrahim", "title": "TPRM: Tensor partition regression models with applications in imaging\n  biomarker detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging studies have collected high dimensional imaging data to\nidentify imaging biomarkers for diagnosis, screening, and prognosis, among many\nothers. These imaging data are often represented in the form of a\nmulti-dimensional array, called a tensor. The aim of this paper is to develop a\ntensor partition regression modeling (TPRM) framework to establish a\nrelationship between low-dimensional clinical outcomes (e.g., diagnosis) and\nhigh dimensional tensor covariates. Our TPRM is a hierarchical model and\nefficiently integrates four components: (i) a partition model, (ii) a canonical\npolyadic decomposition model, (iii) a principal components model, and (iv) a\ngeneralized linear model with a sparse inducing normal mixture prior. This\nframework not only reduces ultra-high dimensionality to a manageable level,\nresulting in efficient estimation, but also optimizes prediction accuracy in\nthe search for informative sub-tensors. Posterior computation proceeds via an\nefficient Markov chain Monte Carlo algorithm. Simulation shows that TPRM\noutperforms several other competing methods. We apply TPRM to predict disease\nstatus (Alzheimer versus control) by using structural magnetic resonance\nimaging data obtained from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) study.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 18:33:58 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 17:05:02 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Miranda", "Michelle F.", ""], ["Zhu", "Hongtu", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "1505.05557", "submitter": "Jim Albert", "authors": "Jim Albert", "title": "Improved Component Predictions of Batting Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard measures of batting performance such as a batting average and an\non-base percentage can be decomposed into component rates such as strikeout\nrates and home run rates. The likelihood of hitting data for a group of players\ncan be expressed as a product of likelihoods of the component probabilities and\nthis motivates the use of random effects models to estimate the groups of\ncomponent rates. This methodology leads to accurate estimates at hitting\nprobabilities and good predictions of performance for following seasons. This\napproach is also illustrated for on-base probabilities and FIP abilities of\npitchers.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 23:34:43 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:02:14 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Albert", "Jim", ""]]}, {"id": "1505.05639", "submitter": "Elena Harpa", "authors": "Elena Lucia Harpa, Liviu Marian, Sorina Moica, Iulia Elena Apavaloaie", "title": "Analysis of the most important variables which determine innovation\n  among rural entrepreneurs", "comments": "4th RMEE Conference The Management Between Profit and Social\n  Responsibility, UT Cluj, 18th-20th September 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present research aims to highlight the main factors influencing the\ndevelopment of entrepreneurial innovation in a rural environment and to perform\nan empirical study with the purpose of assessing the main problems in rural\ndevelopment. The research performed is mostly of a quantitative nature, being\nbased on the use of the questionnaire as research tool, although some of the\nquestions were raised in order to collect respondents impressions and opinions\nwhich would form the object fo qualitative research. The research outlines the\nfact that in the rural entrepreneurship innovation is performed with minimal\ninvestment in new technologies and depends on the entrepreneur's involvement in\nInnovation Systems Network. It was also noted that most of the entrepreneurs in\nrural environment are non-innovators. The research question started to assess\nthe key elements which identify the role of innovation among entrepreneurs in\nrural areas. Based on these facts, we determined the variables that make\nentrepreneurial innovation in rural areas, followed by the analysis of the most\nsignificant variable rural entrepreneurs in the Mures county. This result can\nsupport the creation of the future model of innovation in rural\nentrepreneurship. There are relatively few studies addressing the problem of\nresearch regarding innovation in rural areas. The emphasis is on national and\nregional studies, without differentiating between the rural and urban areas.\nThus, the purpose of the analysis is to add a descriptive background related to\nthe innovation at a micro-economic level in the rural areas\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 08:05:54 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Harpa", "Elena Lucia", ""], ["Marian", "Liviu", ""], ["Moica", "Sorina", ""], ["Apavaloaie", "Iulia Elena", ""]]}, {"id": "1505.05644", "submitter": "Aristides Moustakas", "authors": "Marianna Louca, Ioannis N. Vogiatzakis, and Aristides Moustakas", "title": "Modelling the combined effects of land use and climatic changes:\n  coupling bioclimatic modelling with markov-chain cellular automata in a case\n  study in Cyprus", "comments": "to appear (in press in Ecological Informatics (2015))", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CY cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two endemic plant species in the Mediterranean island of Cyprus, Crocus\ncyprius and Ophrys kotschyi, were used as a case study. We have coupled climate\nchange scenarios, and land use change models with species distribution models.\nFuture land use scenarios were modelled by initially calculating the rate of\ncurrent land use changes between two time snapshots (2000 and 2006) on the\nisland, and based on these transition probabilities markov-chain cellular\nautomata were used to generate future land use changes for 2050. Climate change\nscenarios A1B, A2, B1 and B2A were derived from the IPCC reports. Species\nclimatic preferences were derived from their current distributions using\nclassification trees while habitats preferences were derived from the Red Data\nBook of the Flora of Cyprus. A bioclimatic model for Crocus cyprius was built\nusing mean temperature of wettest quarter, max temperature of warmest month and\nprecipitation seasonality, while for Ophrys kotchyi the bioclimatic model was\nbuilt using precipitation of wettest month, mean temperature of warmest\nquarter, isothermality, precipitation of coldest quarter, and annual\nprecipitation. Sequentially, simulation scenarios were performed regarding\nfuture species distributions by accounting climate alone and both climate and\nland use changes. The distribution of the two species resulting from the\nbioclimatic models was then filtered by future land use changes, providing the\nspecies projected potential distribution. The species projected potential\ndistribution varies depending on the type and scenario used, but many of both\nspecies current sites/locations are projected to be outside their future\npotential distribution. Our results demonstrate the importance of including\nboth land use and climatic changes in predictive species modeling.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 08:25:16 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Louca", "Marianna", ""], ["Vogiatzakis", "Ioannis N.", ""], ["Moustakas", "Aristides", ""]]}, {"id": "1505.05668", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Locally Adaptive Dynamic Networks", "comments": null, "journal-ref": "Annals of Applied Statistics (2016). 10, 2203-2232", "doi": "10.1214/16-AOAS971", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our focus is on realistically modeling and forecasting dynamic networks of\nface-to-face contacts among individuals. Important aspects of such data that\nlead to problems with current methods include the tendency of the contacts to\nmove between periods of slow and rapid changes, and the dynamic heterogeneity\nin the actors' connectivity behaviors. Motivated by this application, we\ndevelop a novel method for Locally Adaptive DYnamic (LADY) network inference.\nThe proposed model relies on a dynamic latent space representation in which\neach actor's position evolves in time via stochastic differential equations.\nUsing a state space representation for these stochastic processes and\nP\\'olya-gamma data augmentation, we develop an efficient MCMC algorithm for\nposterior inference along with tractable procedures for online updating and\nforecasting of future networks. We evaluate performance in simulation studies,\nand consider an application to face-to-face contacts among individuals in a\nprimary school.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:27:53 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 09:22:49 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 14:16:21 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1505.05705", "submitter": "Etienne Birmele", "authors": "Thomas Picchetti (MAP5), Julien Chiquet (LaMME), Mohamed Elati (ISSB),\n  Pierre Neuvial (LaMME), R\\'emy Nicolle (ISSB), Etienne Birmel\\'e (MAP5)", "title": "A model for gene deregulation detection using expression data", "comments": null, "journal-ref": null, "doi": "10.1186/1752-0509-9-S6-S6", "report-no": "MAP5 2015-17", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tumoral cells, gene regulation mechanisms are severely altered, and these\nmodifications in the regulations may be characteristic of different subtypes of\ncancer. However, these alterations do not necessarily induce differential\nexpressions between the subtypes. To answer this question, we propose a\nstatistical methodology to identify the misregulated genes given a reference\nnetwork and gene expression data. Our model is based on a regulatory process in\nwhich all genes are allowed to be deregulated. We derive an EM algorithm where\nthe hidden variables correspond to the status (under/over/normally expressed)\nof the genes and where the E-step is solved thanks to a message passing\nalgorithm. Our procedure provides posterior probabilities of deregulation in a\ngiven sample for each gene. We assess the performance of our method by\nnumerical experiments on simulations and on a bladder cancer data set.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 12:52:06 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 15:14:02 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Picchetti", "Thomas", "", "MAP5"], ["Chiquet", "Julien", "", "LaMME"], ["Elati", "Mohamed", "", "ISSB"], ["Neuvial", "Pierre", "", "LaMME"], ["Nicolle", "R\u00e9my", "", "ISSB"], ["Birmel\u00e9", "Etienne", "", "MAP5"]]}, {"id": "1505.05816", "submitter": "Peter Ralph", "authors": "Peter L. Ralph", "title": "An empirical approach to demographic inference with genomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference with population genetic data usually treats the population pedigree\nas a nuisance parameter, the unobserved product of a past history of random\nmating. However, the history of genetic relationships in a given population is\na fixed, unobserved object, and so an alternative approach is to treat this\nnetwork of relationships as a complex object we wish to learn about, by\nobserving how genomes have been noisily passed down through it. This paper\nexplores this point of view, showing how to translate questions about\npopulation genetic data into calculations with a Poisson process of mutations\non all ancestral genomes. This method is applied to give a robust\ninterpretation to the $f_4$ statistic used to identify admixture, and to design\na new statistic that measures covariances in mean times to most recent common\nancestor between two pairs of sequences. The method more generally interprets\npopulation genetic statistics in terms of sums of specific functions over\nancestral genomes, thereby providing concrete, broadly interpretable\ninterpretations for these statistics. This provides a method for describing\ndemographic history without simplified demographic models. More generally, it\nbrings into focus the population pedigree, which is averaged over in\nmodel-based demographic inference.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 18:13:54 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 23:12:10 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 17:05:36 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ralph", "Peter L.", ""]]}, {"id": "1505.05917", "submitter": "Shang Li", "authors": "Shang Li and Xiaoou Li and Xiaodong Wang and Jingchen Liu", "title": "Decentralized Sequential Composite Hypothesis Test Based on One-Bit\n  Communication", "comments": "39 pages", "journal-ref": "IEEE Transactions on Information Theory, vol. 63, no. 6, pp. 3405\n  - 3424, June 2017", "doi": "10.1109/TIT.2017.2693156", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the sequential composite hypothesis test with multiple\nsensors. The sensors observe random samples in parallel and communicate with a\nfusion center, who makes the global decision based on the sensor inputs. On one\nhand, in the centralized scenario, where local samples are precisely\ntransmitted to the fusion center, the generalized sequential likelihood ratio\ntest (GSPRT) is shown to be asymptotically optimal in terms of the expected\nsample size as error rates tend to zero. On the other hand, for systems with\nlimited power and bandwidth resources, decentralized solutions that only send a\nsummary of local samples (we particularly focus on a one-bit communication\nprotocol) to the fusion center is of great importance. To this end, we first\nconsider a decentralized scheme where sensors send their one-bit quantized\nstatistics every fixed period of time to the fusion center. We show that such a\nuniform sampling and quantization scheme is strictly suboptimal and its\nsuboptimality can be quantified by the KL divergence of the distributions of\nthe quantized statistics under both hypotheses. We then propose a decentralized\nGSPRT based on level-triggered sampling. That is, each sensor runs its own\nGSPRT repeatedly and reports its local decision to the fusion center\nasynchronously. We show that this scheme is asymptotically optimal as the local\nthresholds and global thresholds grow large at different rates. Lastly, two\nparticular models and their associated applications are studied to compare the\ncentralized and decentralized approaches. Numerical results are provided to\ndemonstrate that the proposed level-triggered sampling based decentralized\nscheme aligns closely with the centralized scheme with substantially lower\ncommunication overhead, and significantly outperforms the uniform sampling and\nquantization based decentralized scheme.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 22:18:46 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Li", "Shang", ""], ["Li", "Xiaoou", ""], ["Wang", "Xiaodong", ""], ["Liu", "Jingchen", ""]]}, {"id": "1505.06129", "submitter": "Magalie Fromont", "authors": "M\\'elisande Albert and Yann Bouret and Magalie Fromont and Patricia\n  Reynaud-Bouret", "title": "A Distribution Free Unitary Events Method based on Delayed Coincidence\n  Count", "comments": "45 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate several distribution free dependence detection procedures,\nmainly based on bootstrap principles and their approximation properties. Thanks\nto this study, we introduce a new distribution free Unitary Events (UE) method,\nnamed Permutation UE, which consists in a multiple testing procedure based on\npermutation and delayed coincidence count. Each involved single test of this\nprocedure achieves the prescribed level, so that the corresponding multiple\ntesting procedure controls the False Discovery Rate (FDR), and this with as few\nassumptions as possible on the underneath distribution. Some simulations show\nthat this method outperforms the trial-shuffling and the MTGAUE method in terms\nof single levels and FDR, for a comparable amount of false negatives.\nApplication on real data is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 15:57:26 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Albert", "M\u00e9lisande", ""], ["Bouret", "Yann", ""], ["Fromont", "Magalie", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "1505.06188", "submitter": "Daisuke Murakami", "authors": "Yoshiki Yamagata, Daisuke Murakami, Gareth W. Peters, Tomoko Matsui", "title": "A spatiotemporal analysis of participatory sensing data \"tweets\" and\n  extreme climate events toward real-time urban risk management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time urban climate monitoring provides useful information that can be\nutilized to help monitor and adapt to extreme events, including urban\nheatwaves. Typical approaches to the monitoring of climate data include weather\nstation monitoring and remote sensing. However, climate monitoring stations are\nvery often distributed spatially in a sparse manner, and consequently, this has\na significant impact on the ability to reveal exposure risks due to extreme\nclimates at an intra-urban scale. Additionally, traditional remote sensing data\nsources are typically not received and analyzed in real-time which is often\nrequired for adaptive urban management of climate extremes, such as sudden\nheatwaves. Fortunately, recent social media, such as Twitter, furnishes\nreal-time and high-resolution spatial information that might be useful for\nclimate condition estimation. The objective of this study is utilizing\ngeo-tagged tweets (participatory sensing data) for urban temperature analysis.\nWe first detect tweets relating hotness (hot-tweets). Then, we study\nrelationships between monitored temperatures and hot-tweets via a statistical\nmodel framework based on copula modelling methods. We demonstrate that there\nare strong relationships between \"hot-tweets\" and temperatures recorded at an\nintra-urban scale. Subsequently, we then investigate the application of\n\"hot-tweets\" informing spatio-temporal Gaussian process interpolation of\ntemperatures as an application example of \"hot-tweets\". We utilize a\ncombination of spatially sparse weather monitoring sensor data and spatially\nand temporally dense lower quality twitter data. Here, a spatial best linear\nunbiased estimation technique is applied. The result suggests that tweets\nprovide some useful auxiliary information for urban climate assessment. Lastly,\neffectiveness of tweets toward a real-time urban risk management is discussed\nbased on the results.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 19:26:48 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 05:03:33 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Yamagata", "Yoshiki", ""], ["Murakami", "Daisuke", ""], ["Peters", "Gareth W.", ""], ["Matsui", "Tomoko", ""]]}, {"id": "1505.06275", "submitter": "Curtis Storlie", "authors": "Curtis B Storlie, Brian J Reich, William N Rust, Lawrence O Ticknor,\n  Amanda M Bonnie, Andrew J Montoya, Sarah E Michalak", "title": "Spatiotemporal Modeling of Node Temperatures in Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Los Alamos National Laboratory (LANL) is home to many large supercomputing\nclusters. These clusters require an enormous amount of power (~500-2000 kW\neach), and most of this energy is converted into heat. Thus, cooling the\ncomponents of the supercomputer becomes a critical and expensive endeavor.\nRecently a project was initiated to optimize the cooling system used to cool\none of the rooms housing three of these large clusters and develop a general\ngood-practice procedure for reducing cooling costs and monitoring other machine\nrooms. This work focuses on the statistical approach used to quantify the\neffect that several cooling changes to the room had on the temperatures of the\nindividual nodes of the computers. The largest cluster in the room has 1600\nnodes that run a variety of jobs during general use. Since extremes\ntemperatures are important, a Normal distribution plus generalized Pareto\ndistribution for the upper tail is used to model the marginal distribution,\nalong with a Gaussian process copula to account for spatio-temporal dependence.\nA Gaussian Markov random field (GMRF) model is used to model the spatial and/or\ntemporal effects on the node temperatures as the cooling changes take place.\nThis model is then used to assess the condition of the node temperatures after\neach change to the room. The analysis approach was used to uncover the cause of\na problematic episode of overheating nodes on one of the supercomputing\nclusters. The next step is to also use the model to estimate the trend in node\ntemperatures due to an increase in supply air temperature and ultimately decide\nwhen any further temperature increases would become unsafe. This same process\ncan be applied to reduce the cooling expenses for other data centers as well.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 04:36:36 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 03:49:07 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 15:14:27 GMT"}, {"version": "v4", "created": "Thu, 26 May 2016 02:21:43 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Storlie", "Curtis B", ""], ["Reich", "Brian J", ""], ["Rust", "William N", ""], ["Ticknor", "Lawrence O", ""], ["Bonnie", "Amanda M", ""], ["Montoya", "Andrew J", ""], ["Michalak", "Sarah E", ""]]}, {"id": "1505.06416", "submitter": "Mosayeb Rastgou", "authors": "Mosayeb Rastgou", "title": "Robust multiuser detection in impulsive channels based on M-estimation\n  using a new penalty function", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of multiuser detection in non-Gaussian\nchannels. We propose a new penalty function for robust multiuser detection. The\nproposed detector outperforms other suboptimal detectors in non-Gaussian\nenvironment. Analytical and simulation result shows the performance of the\nproposed detector compare to other detectors.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 08:07:54 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rastgou", "Mosayeb", ""]]}, {"id": "1505.06658", "submitter": "Yasunori Aoki", "authors": "Yasunori Aoki, Monika Sundqvist, Andrew C. Hooker and Peter Gennemark", "title": "PopED lite: an optimal design software for preclinical pharmacokinetic\n  and pharmacodynamic studies", "comments": "Submitted to Computer Methods and Programs in Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal experimental design approaches are seldom used in pre-clinical drug\ndiscovery. Main reasons for this lack of use are that available software tools\nrequire relatively high insight in optimal design theory, and that the\ndesign-execution cycle of in vivo experiments is short, making time-consuming\noptimizations infeasible. We present the publicly available software PopED lite\nin order to increase the use of optimal design in pre-clinical drug discovery.\nPopED lite is designed to be simple, fast and intuitive. Simple, to give many\nusers access to basic optimal design calculations. Fast, to fit the short\ndesign-execution cycle and allow interactive experimental design (test one\ndesign, discuss proposed design, test another design, etc). Intuitive, so that\nthe input to and output from the software can easily be understood by users\nwithout knowledge of the theory of optimal design. In this way, PopED lite is\nhighly useful in practice and complements existing tools. Key functionality of\nPopED lite is demonstrated by three case studies from real drug discovery\nprojects.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 15:07:44 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Aoki", "Yasunori", ""], ["Sundqvist", "Monika", ""], ["Hooker", "Andrew C.", ""], ["Gennemark", "Peter", ""]]}, {"id": "1505.06891", "submitter": "Emanuele  Giorgi", "authors": "Peter J. Diggle and Emanuele Giorgi", "title": "Model-Based Geostatistics for Prevalence Mapping in Low-Resource\n  Settings", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low-resource settings, prevalence mapping relies on empirical prevalence\ndata from a finite, often spatially sparse, set of surveys of communities\nwithin the region of interest, possibly supplemented by remotely sensed images\nthat can act as proxies for environmental risk factors. A standard\ngeostatistical model for data of this kind is a generalized linear mixed model\nwith binomial error distribution, logistic link and a combination of\nexplanatory variables and a Gaussian spatial stochastic process in the linear\npredictor. In this paper, we first review statistical methods and software\nassociated with this standard model, then consider several methodological\nextensions whose development has been motivated by the requirements of specific\napplications. These include: methods for combining randomised survey data with\ndata from non-randomised, and therefore potentially biased, surveys;\nspatio-temporal extensions; spatially structured zero-inflation. Throughout, we\nillustrate the methods with disease mapping applications that have arisen\nthrough our involvement with a range of African public health programmes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 10:34:11 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Diggle", "Peter J.", ""], ["Giorgi", "Emanuele", ""]]}, {"id": "1505.07027", "submitter": "Dr. Wolfgang A. Rolke", "authors": "Wolfgang A. Rolke", "title": "A Comparison of Limit Setting Methods for the On-Off Problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.nima.2015.10.028", "report-no": null, "categories": "physics.data-an hep-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the frequentist properties of confidence intervals for the On-Off\nproblem. The methods include all those in common use today. We derive explicit\nformulas for the limits and calculate the true coverage and the expected\nlengths of these methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 16:00:59 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 15:51:03 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 13:39:32 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Rolke", "Wolfgang A.", ""]]}, {"id": "1505.07281", "submitter": "Jean-Michel Becu", "authors": "Jean-Michel B\\'ecu (Heudiasyc), Yves Grandvalet (Heudiasyc),\n  Christophe Ambroise (LaMME), Cyril Dalmasso (LaMME)", "title": "Beyond Support in Two-Stage Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous variable selection methods rely on a two-stage procedure, where a\nsparsity-inducing penalty is used in the first stage to predict the support,\nwhich is then conveyed to the second stage for estimation or inference\npurposes. In this framework, the first stage screens variables to find a set of\npossibly relevant variables and the second stage operates on this set of\ncandidate variables, to improve estimation accuracy or to assess the\nuncertainty associated to the selection of variables. We advocate that more\ninformation can be conveyed from the first stage to the second one: we use the\nmagnitude of the coefficients estimated in the first stage to define an\nadaptive penalty that is applied at the second stage. We give two examples of\nprocedures that can benefit from the proposed transfer of information, in\nestimation and inference problems respectively. Extensive simulations\ndemonstrate that this transfer is particularly efficient when each stage\noperates on distinct subsamples. This separation plays a crucial role for the\ncomputation of calibrated p-values, allowing to control the False Discovery\nRate. In this setup, the proposed transfer results in sensitivity gains ranging\nfrom 50% to 100% compared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 12:14:20 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["B\u00e9cu", "Jean-Michel", "", "Heudiasyc"], ["Grandvalet", "Yves", "", "Heudiasyc"], ["Ambroise", "Christophe", "", "LaMME"], ["Dalmasso", "Cyril", "", "LaMME"]]}, {"id": "1505.07484", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Fitting a distribution to Value-at-Risk and Expected Shortfall, with an\n  application to covered bonds", "comments": "27 pages, 2 figures, 3 tables", "journal-ref": "Journal of Credit Risk 12(2), 1-34, 2016", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covered bonds are a specific example of senior secured debt. If the issuer of\nthe bonds defaults the proceeds of the assets in the cover pool are used for\ntheir debt service. If in this situation the cover pool proceeds do not suffice\nfor the debt service, the creditors of the bonds have recourse to the issuer's\nassets and their claims are pari passu with the claims of the creditors of\nsenior unsecured debt. Historically, covered bonds have been very safe\ninvestments. During their more than two hundred years of existence, investors\nnever suffered losses due to missed payments from covered bonds. From a risk\nmanagement perspective, therefore modelling covered bonds losses is mainly of\ninterest for estimating the impact that the asset encumbrance by the cover pool\nhas on the loss characteristics of the issuer's senior unsecured debt. We\nexplore one-period structural modelling approaches for covered bonds and senior\nunsecured debt losses with one and two asset value variables respectively.\nObviously, two-assets models with separate values of the cover pool and the\nissuer's remaining portfolio allow for more realistic modelling. However, we\ndemonstrate that exact calibration of such models may be impossible. We also\ninvestigate a one-asset model in which the riskiness of the cover pool is\nreflected by a risk-based adjustment of the encumbrance ratio of the issuer's\nassets.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 20:17:06 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 19:26:27 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1505.07526", "submitter": "Shou-Wen Wang", "authors": "Shou-Wen Wang, Yueheng Lan and Lei-Han Tang", "title": "Energy dissipation in an adaptive molecular circuit", "comments": null, "journal-ref": "J. Stat. Mech. Theor. Exp. 2015, P07025 (2015)", "doi": "10.1088/1742-5468/2015/00/P07025", "report-no": null, "categories": "q-bio.SC cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to monitor nutrient and other environmental conditions with high\nsensitivity is crucial for cell growth and survival. Sensory adaptation allows\na cell to recover its sensitivity after a transient response to a shift in the\nstrength of extracellular stimulus. The working principles of adaptation have\nbeen established previously based on rate equations which do not consider\nfluctuations in a thermal environment. Recently, G. Lan et al. (Nature Phys.,\n8:422-8, 2012) performed a detailed analysis of a stochastic model for the E.\ncoli sensory network. They showed that accurate adaptation is possible only\nwhen the system operates in a nonequilibrium steady-state (NESS). They further\nproposed an energy-speed-accuracy (ESA) trade-off relation. We present here\nanalytic results on the NESS of the model through a mapping to a\none-dimensional birth-death process. An exact expression for the entropy\nproduction rate is also derived. Based on these results, we are able to discuss\nthe ESA relation in a more general setting. Our study suggests that the\nadaptation error can be reduced exponentially as the methylation range\nincreases. Finally, we show that a nonequilibrium phase transition exists in\nthe infinite methylation range limit, despite the fact that the model contains\nonly two discrete variables.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 01:42:34 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Wang", "Shou-Wen", ""], ["Lan", "Yueheng", ""], ["Tang", "Lei-Han", ""]]}, {"id": "1505.07649", "submitter": "Lucas Theis", "authors": "Lucas Theis and Matthew D. Hoffman", "title": "A trust-region method for stochastic variational inference with\n  applications to streaming data", "comments": "in Proceedings of the 32nd International Conference on Machine\n  Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference allows for fast posterior inference in\ncomplex Bayesian models. However, the algorithm is prone to local optima which\ncan make the quality of the posterior approximation sensitive to the choice of\nhyperparameters and initialization. We address this problem by replacing the\nnatural gradient step of stochastic varitional inference with a trust-region\nupdate. We show that this leads to generally better results and reduced\nsensitivity to hyperparameters. We also describe a new strategy for variational\ninference on streaming data and show that here our trust-region method is\ncrucial for getting good performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 11:25:55 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Theis", "Lucas", ""], ["Hoffman", "Matthew D.", ""]]}, {"id": "1505.07661", "submitter": "\\c{S}eyda Ertekin", "authors": "\\c{S}eyda Ertekin, Cynthia Rudin, Tyler H. McCormick", "title": "Reactive point processes: A new approach to predicting power failures in\n  underground electrical systems", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS789 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 122-144", "doi": "10.1214/14-AOAS789", "report-no": "IMS-AOAS-AOAS789", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive point processes (RPPs) are a new statistical model designed for\npredicting discrete events in time based on past history. RPPs were developed\nto handle an important problem within the domain of electrical grid\nreliability: short-term prediction of electrical grid failures (\"manhole\nevents\"), including outages, fires, explosions and smoking manholes, which can\ncause threats to public safety and reliability of electrical service in cities.\nRPPs incorporate self-exciting, self-regulating and saturating components. The\nself-excitement occurs as a result of a past event, which causes a temporary\nrise in vulner ability to future events. The self-regulation occurs as a result\nof an external inspection which temporarily lowers vulnerability to future\nevents. RPPs can saturate when too many events or inspections occur close\ntogether, which ensures that the probability of an event stays within a\nrealistic range. Two of the operational challenges for power companies are (i)\nmaking continuous-time failure predictions, and (ii) cost/benefit analysis for\ndecision making and proactive maintenance. RPPs are naturally suited for\nhandling both of these challenges. We use the model to predict power-grid\nfailures in Manhattan over a short-term horizon, and to provide a cost/benefit\nanalysis of different proactive maintenance programs.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 12:00:16 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Ertekin", "\u015eeyda", ""], ["Rudin", "Cynthia", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1505.07684", "submitter": "Spencer Wheatley Mr.", "authors": "Spencer Wheatley, Thomas Maillart, and Didier Sornette", "title": "The Extreme Risk of Personal Data Breaches & The Erosion of Privacy", "comments": "16 pages, 3 sets of figures, and 4 tables", "journal-ref": null, "doi": "10.1140/epjb/e2015-60754-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal data breaches from organisations, enabling mass identity fraud,\nconstitute an \\emph{extreme risk}. This risk worsens daily as an ever-growing\namount of personal data are stored by organisations and on-line, and the attack\nsurface surrounding this data becomes larger and harder to secure. Further,\nbreached information is distributed and accumulates in the hands of cyber\ncriminals, thus driving a cumulative erosion of privacy. Statistical modeling\nof breach data from 2000 through 2015 provides insights into this risk: A\ncurrent maximum breach size of about 200 million is detected, and is expected\nto grow by fifty percent over the next five years. The breach sizes are found\nto be well modeled by an \\emph{extremely heavy tailed} truncated Pareto\ndistribution, with tail exponent parameter decreasing linearly from 0.57 in\n2007 to 0.37 in 2015. With this current model, given a breach contains above\nfifty thousand items, there is a ten percent probability of exceeding ten\nmillion. A size effect is unearthed where both the frequency and severity of\nbreaches scale with organisation size like $s^{0.6}$. Projections indicate that\nthe total amount of breached information is expected to double from two to four\nbillion items within the next five years, eclipsing the population of users of\nthe Internet. This massive and uncontrolled dissemination of personal\nidentities raises fundamental concerns about privacy.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 13:25:00 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 12:51:40 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Wheatley", "Spencer", ""], ["Maillart", "Thomas", ""], ["Sornette", "Didier", ""]]}, {"id": "1505.07752", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm and Julian Pan", "title": "The Impact of Estimation: A New Method for Clustering and Trajectory\n  Estimation in Patient Flow Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately forecast and control inpatient census, and thereby\nworkloads, is a critical and longstanding problem in hospital management.\nMajority of current literature focuses on optimal scheduling of inpatients, but\nlargely ignores the process of accurate estimation of the trajectory of\npatients throughout the treatment and recovery process. The result is that\ncurrent scheduling models are optimizing based on inaccurate input data. We\ndeveloped a Clustering and Scheduling Integrated (CSI) approach to capture\npatient flows through a network of hospital services. CSI functions by\nclustering patients into groups based on similarity of trajectory using a novel\nSemi-Markov model (SMM)-based clustering scheme proposed in this paper, as\nopposed to clustering by admit type or condition as in previous literature. The\nmethodology is validated by simulation and then applied to real patient data\nfrom a partner hospital where we see it outperforms current methods. Further,\nwe demonstrate that extant optimization methods achieve significantly better\nresults on key hospital performance measures under CSI, compared with\ntraditional estimation approaches, increasing elective admissions by 97% and\nutilization by 22% compared to 30% and 8% using traditional estimation\ntechniques. From a theoretical standpoint, the SMM-clustering is a novel\napproach applicable to any temporal-spatial stochastic data that is prevalent\nin many industries and application areas.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 16:47:57 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 14:13:03 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2015 19:16:52 GMT"}, {"version": "v4", "created": "Tue, 12 Jul 2016 16:36:24 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 03:59:24 GMT"}, {"version": "v6", "created": "Wed, 11 Jan 2017 01:17:26 GMT"}, {"version": "v7", "created": "Mon, 30 Jan 2017 02:26:23 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ranjan", "Chitta", ""], ["Paynabar", "Kamran", ""], ["Helm", "Jonathan E.", ""], ["Pan", "Julian", ""]]}, {"id": "1505.08000", "submitter": "Christoph Degen", "authors": "Roy Streit, Christoph Degen, Wolfgang Koch", "title": "The Pointillist Family of Multitarget Tracking Filters", "comments": "21 pages, 1 table, 2 figures, submitted to IEEE T-AES 201500380 on\n  29.05.2015, changes to previous version: added Section X, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of pointillist multitarget tracking filters is defined to be the\nclass of filters that is characterized by a joint target-measurement finite\npoint process. The probability generating functional (PGFL) of the joint\nprocess is derived directly from the probabilistic structure of the tracking\nproblem. PGFLs exemplify the analytic combinatoric method applied to the\nmeasurement to target assignment problems that are fundamental to the tracking\nproblem. It is shown that multi-hypothesis tracking (MHT), joint probabilistic\ndata association (JPDA), and many other now-classic tracking filters can be\nderived via PGFLs, and thus are members of the family of pointillist filters.\nWhen one or more of the target processes are dimensionally compatible, targets\ncan be superposed. It is shown that the classic MHT filter for superposed\ntargets is closely related to the multi-Bernoulli filter. A technique is\npresented for deriving the functional derivatives by ordinary differentiation,\nand both exact and approximate methods for evaluating the ordinary derivatives\nare discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 11:44:37 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 13:30:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Streit", "Roy", ""], ["Degen", "Christoph", ""], ["Koch", "Wolfgang", ""]]}, {"id": "1505.08171", "submitter": "Jack O'Brien", "authors": "John D. O'Brien, Zamin Iqbal, Lucas Amenga-Etego", "title": "An integrative statistical model for inferring strain admixture within\n  clinical Plasmodium falciparum isolates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the arrival of genetic typing methods in the late 1960's, researchers\nhave puzzled at the clinical consequence of observed strain mixtures within\nclinical isolates of Plasmodium falciparum. We present a new statistical model\nthat infers the number of strains present and the amount of admixture with the\nlocal population (panmixia) using whole-genome sequence data. The model\nprovides a rigorous statistical approach to inferring these quantities as well\nas the proportions of the strains within each sample. Applied to 168 samples of\nwhole-genome sequence data from northern Ghana, the model provides\nsignificantly improvement fit over models implementing simpler approaches to\nmixture for a large majority (129/168) of samples. We discuss the possible uses\nof this model as a window into within-host selection for clinical and\nepidemiological studies and outline possible means for experimental validation.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 19:58:10 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["O'Brien", "John D.", ""], ["Iqbal", "Zamin", ""], ["Amenga-Etego", "Lucas", ""]]}]