[{"id": "1905.00095", "submitter": "The Tien Mai", "authors": "The Tien Mai, Leiv R{\\o}nneberg, Zhi Zhao, Manuela Zucknick, Jukka\n  Corander", "title": "Composite local low-rank structure in learning drug sensitivity", "comments": null, "journal-ref": "CIBB 2019,http://www.cibb2019.it/", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The molecular characterization of tumor samples by multiple omics data sets\nof different types or modalities (e.g. gene expression, mutation, CpG\nmethylation) has become an invaluable source of information for assessing the\nexpected performance of individual drugs and their combinations. Merging the\nrelevant information from the omics data modalities provides the statistical\nbasis for determining suitable therapies for specific cancer patients.\nDifferent data modalities may each have their specific structures that need to\nbe taken into account during inference. In this paper, we assume that each\nomics data modality has a low-rank structure with only few relevant features\nthat affect the prediction and we propose to use a composite local nuclear norm\npenalization for learning drug sensitivity. Numerical results show that the\ncomposite low-rank structure can improve the prediction performance compared to\nusing a global low-rank approach or elastic net regression.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 20:28:13 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 12:25:16 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Mai", "The Tien", ""], ["R\u00f8nneberg", "Leiv", ""], ["Zhao", "Zhi", ""], ["Zucknick", "Manuela", ""], ["Corander", "Jukka", ""]]}, {"id": "1905.00353", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene, Cesar Cristancho, Mariana Ospina, Domingo Morales", "title": "Prevalence of international migration: an alternative for small area\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an alternative procedure for estimating the prevalence\nof international migration at the municipal level in Colombia. The new\nmethodology uses the empirical best linear unbiased predictor based on a\nFay-Herriot model with target and auxiliary variables available from census\nstudies and from the Demographic and Health Survey. The proposed alternative\nproduces prevalence estimates which are consistent with sample sizes and\ndemographic dynamics in Colombia. Additionally, the estimated coefficients of\nvariation are lower than 20% for municipalities and large\ndemographically-relevant capital cities and therefore estimates may be\nconsidered as reliable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:15:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fuquene", "Jairo", ""], ["Cristancho", "Cesar", ""], ["Ospina", "Mariana", ""], ["Morales", "Domingo", ""]]}, {"id": "1905.00377", "submitter": "Siddharth Arora Dr.", "authors": "Siddharth Arora, Ladan Baghai-Ravary, Athanasios Tsanas", "title": "Developing a large scale population screening tool for the assessment of\n  Parkinson's disease using telephone-quality voice", "comments": "43 pages, 5 figures, 6 tables", "journal-ref": null, "doi": "10.1121/1.5100272", "report-no": null, "categories": "stat.AP cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated that analysis of laboratory-quality voice\nrecordings can be used to accurately differentiate people diagnosed with\nParkinson's disease (PD) from healthy controls (HC). These findings could help\nfacilitate the development of remote screening and monitoring tools for PD. In\nthis study, we analyzed 2759 telephone-quality voice recordings from 1483 PD\nand 15321 recordings from 8300 HC participants. To account for variations in\nphonetic backgrounds, we acquired data from seven countries. We developed a\nstatistical framework for analyzing voice, whereby we computed 307 dysphonia\nmeasures that quantify different properties of voice impairment, such as,\nbreathiness, roughness, monopitch, hoarse voice quality, and exaggerated vocal\ntremor. We used feature selection algorithms to identify robust parsimonious\nfeature subsets, which were used in combination with a Random Forests (RF)\nclassifier to accurately distinguish PD from HC. The best 10-fold\ncross-validation performance was obtained using Gram-Schmidt Orthogonalization\n(GSO) and RF, leading to mean sensitivity of 64.90% (standard deviation, SD\n2.90%) and mean specificity of 67.96% (SD 2.90%). This large-scale study is a\nstep forward towards assessing the development of a reliable, cost-effective\nand practical clinical decision support tool for screening the population at\nlarge for PD using telephone-quality voice.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:55:15 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Arora", "Siddharth", ""], ["Baghai-Ravary", "Ladan", ""], ["Tsanas", "Athanasios", ""]]}, {"id": "1905.00393", "submitter": "Phuong T. Vu", "authors": "Phuong T. Vu, Timothy V. Larson, Adam A. Szpiro", "title": "Probabilistic Predictive Principal Component Analysis for\n  Spatially-Misaligned and High-Dimensional Air Pollution Data with Missing\n  Observations", "comments": "36 pages, 8 figures, 5 tables. v2 is a pre peer-reviewed version that\n  was submitted to Environmetrics. A final version with minor revisions was\n  accepted for publication by Environmetrics on Oct 30, 2019, and will be\n  linked to this version once published", "journal-ref": "Environmetrics 2020, Vol. 31, No. 4, e2614", "doi": "10.1002/env.2614", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate predictions of pollutant concentrations at new locations are often\nof interest in air pollution studies on fine particulate matters (PM$_{2.5}$),\nin which data is usually not measured at all study locations. PM$_{2.5}$ is\nalso a mixture of many different chemical components. Principal component\nanalysis (PCA) can be incorporated to obtain lower-dimensional representative\nscores of such multi-pollutant data. Spatial prediction can then be used to\nestimate these scores at new locations. Recently developed predictive PCA\nmodifies the traditional PCA algorithm to obtain scores with spatial structures\nthat can be well predicted at unmeasured locations. However, these approaches\nrequire complete data, whereas multi-pollutant data tends to have complex\nmissing patterns in practice. We propose probabilistic versions of predictive\nPCA which allow for flexible model-based imputation that can account for\nspatial information and subsequently improve the overall predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:29:50 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 02:09:07 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Vu", "Phuong T.", ""], ["Larson", "Timothy V.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "1905.00592", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski, Vitaly Belik", "title": "Modeling the ASF (African Swine Fever) spread till summer 2017 and risk\n  assessment for Poland", "comments": "conference proceedings", "journal-ref": "Katedra Matematyki Uniwersytetu Przyrodniczego we Wroclawiu,XLVII\n  SEMINARIUM ZASTOSOWAN MATEMATYKI, Kobyla Gora 11 wrzesnia 2017", "doi": null, "report-no": null, "categories": "q-bio.PE nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  African Swine Fever (ASF) is viral infection which causes acute disease in\ndomestic pigs and wild boar. Although the virus does not cause disease in\nhumans, the impact it has on the economy, especially through trade and farming,\nis substantial. Recent rapid propagation of the (ASF) from East to West of\nEurope encouraged us to prepare risk assessment for Poland. The early growth\nestimation can be easily done by matching incidence trajectory to the\nexponential function, resulting in the approximation of the force of infection.\nWith these calculations the basic reproduction rate of the epidemic, the\neffective outbreaks detection and elimination times could be estimated. In\nregression mode, 380 Polish counties (poviats) have been analysed, where 18\n(located in Northeast Poland) have been affected (until August 2017) for\nspatial propagation (risk assessment for future). Mathematical model has been\napplied by taking into account: swine amount significance, disease vectors\n(wild boards) significance. We use pseudogravitational models of short and\nlongrange interactions referring to the socio-migratory behavior of wild boars\nand the pork production chain significance. Spatial modeling in a certain range\nof parameters proves the existence of a natural protective barrier within\nboarders of the Congress Poland. The spread of the disease to the Greater\nPoland should result in the accelerated outbreak of ASF production chain. In\nthe preliminary setup, we perform regression analysis, network outbreak\ninvestigation, early epidemic growth estimation and simulate landscape-based\npropagation.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 07:03:30 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Jarynowski", "Andrzej", ""], ["Belik", "Vitaly", ""]]}, {"id": "1905.00635", "submitter": "Martina Patone", "authors": "Martina Patone, Li-Chun Zhang", "title": "On two existing approaches to statistical analysis of social media data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using social media data for statistical analysis of general population faces\ncommonly two basic obstacles: firstly, social media data are collected for\ndifferent objects than the population units of interest; secondly, the relevant\nmeasures are typically not available directly but need to be extracted by\nalgorithms or machine learning techniques. In this paper we examine and\nsummarise two existing approaches to statistical analysis based on social media\ndata, which can be discerned in the literature. In the first approach, analysis\nis applied to the social media data that are organised around the objects\ndirectly observed in the data; in the second one, a different analysis is\napplied to a constructed pseudo survey dataset, aimed to transform the observed\nsocial media data to a set of units from the target population. We elaborate\nsystematically the relevant data quality frameworks, exemplify their\napplications, and highlight some typical challenges associated with social\nmedia data.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:23:18 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Patone", "Martina", ""], ["Zhang", "Li-Chun", ""]]}, {"id": "1905.00672", "submitter": "Jithin Sreedharan", "authors": "Krzysztof Turowski and Jithin K. Sreedharan and Wojciech Szpankowski", "title": "Temporal Ordered Clustering in Dynamic Networks: Unsupervised and\n  Semi-supervised Learning Algorithms", "comments": "14 pages, 9 figures, and 3 tables. This version is submitted to a\n  journal. A shorter version of this work is published in the proceedings of\n  IEEE International Symposium on Information Theory (ISIT), 2020. The first\n  two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In temporal ordered clustering, given a single snapshot of a dynamic network\nin which nodes arrive at distinct time instants, we aim at partitioning its\nnodes into $K$ ordered clusters $\\mathcal{C}_1 \\prec \\cdots \\prec\n\\mathcal{C}_K$ such that for $i<j$, nodes in cluster $\\mathcal{C}_i$ arrived\nbefore nodes in cluster $\\mathcal{C}_j$, with $K$ being a data-driven parameter\nand not known upfront. Such a problem is of considerable significance in many\napplications ranging from tracking the expansion of fake news to mapping the\nspread of information. We first formulate our problem for a general dynamic\ngraph, and propose an integer programming framework that finds the optimal\nclustering, represented as a strict partial order set, achieving the best\nprecision (i.e., fraction of successfully ordered node pairs) for a fixed\ndensity (i.e., fraction of comparable node pairs). We then develop a sequential\nimportance procedure and design unsupervised and semi-supervised algorithms to\nfind temporal ordered clusters that efficiently approximate the optimal\nsolution. To illustrate the techniques, we apply our methods to the vertex\ncopying (duplication-divergence) model which exhibits some edge-case challenges\nin inferring the clusters as compared to other network models. Finally, we\nvalidate the performance of the proposed algorithms on synthetic and real-world\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 11:36:11 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 23:37:42 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 19:36:16 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 18:29:54 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Turowski", "Krzysztof", ""], ["Sreedharan", "Jithin K.", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "1905.00676", "submitter": "Catherine Cliquet", "authors": "Etienne Rivot (ESE), Maxime Olmos (ESE), G\\'erald Chaput, Etienne\n  Pr\\'evost (ECOBIOP)", "title": "A hierarchical life cycle model for Atlantic salmon stock assessment at\n  the North Atlantic basin scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an integrated hierarchical Bayesian life cycle model that\nsimultaneously estimates the abundance of post-smolts at sea, post-smolt\nsurvival rates, and proportions maturing as 1SW, for all SU in Northern Europe,\nSouthern Europe and North America. The model is an age- and stage-based life\ncycle model that considers 1SW and 2SW life history strategies and harmonizes\nthe life history dynamics among SU in North America and Europe. The new\nframework brought a major contribution to improve the scientific basis for\nAtlantic salmon stock assessment. It is a benchmark for the assessment and\nforecast models currently used by ICES for Atlantic salmon stock assessment in\nthe North Atlantic. ...\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 11:43:10 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Rivot", "Etienne", "", "ESE"], ["Olmos", "Maxime", "", "ESE"], ["Chaput", "G\u00e9rald", "", "ECOBIOP"], ["Pr\u00e9vost", "Etienne", "", "ECOBIOP"]]}, {"id": "1905.00685", "submitter": "Ioannis Boumakis", "authors": "Ioannis Boumakis, Kre\\v{s}imir Nin\\v{c}evi\\'c, Jan Vorel, Roman\n  Wan-Wendner", "title": "Creep rate based time to failure prediction of adhesive anchor systems\n  under sustained load", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution studies a well-known failure criterion and its application\nto the life-time prediction of adhesive anchor systems under sustained load.\nThe Monkman-Grant relation, which has previously been applied to a wide range\nof materials, is now applied to adhesive anchors installed in concrete. It\npostulates a linear relationship between the logarithm of stable creep rate and\ntime to failure. In this paper the criterion is evaluated first on a large\nexperimental campaign on one concrete involving two chemically different\nadhesives and then by several experimental data sets reported in literature. In\nall cases the data is well represented and highly accurate predictions are\nobtained. The second part of the paper focuses on the relationship between\nstable creep rate and relative load level of the remote constant stress based\non the Norton-Bailey and the Prandtl-Garofalo creep laws. The latter was found\nto perform better on fitting the experimental data. Finally, the combination of\nthe Monkman-Grant criterion and the aforementioned creep laws allows the\nprediction of stress versus time to failure curves including uncertainty\nbounds, that are in very good agreement with all experimental data sets, making\nit an interesting alternative to existing test methods for adhesive anchor\nsystems under sustained loads.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 12:00:23 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Boumakis", "Ioannis", ""], ["Nin\u010devi\u0107", "Kre\u0161imir", ""], ["Vorel", "Jan", ""], ["Wan-Wendner", "Roman", ""]]}, {"id": "1905.00699", "submitter": "Naoki Masuda Dr.", "authors": "Makoto Okada, Kenji Yamanishi, Naoki Masuda", "title": "Long-tailed distributions of inter-event times as mixtures of\n  exponential distributions", "comments": "2 figures, 4 tables, SI and code are available here:\n  https://github.com/naokimas/exp_mixture_model", "journal-ref": "Royal Society Open Science, 7, 191643 (2020)", "doi": "10.1098/rsos.191643", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-event times of various human behavior are apparently non-Poissonian and\nobey long-tailed distributions as opposed to exponential distributions, which\ncorrespond to Poisson processes. It has been suggested that human individuals\nmay switch between different states in each of which they are regarded to\ngenerate events obeying a Poisson process. If this is the case, inter-event\ntimes should approximately obey a mixture of exponential distributions with\ndifferent parameter values. In the present study, we introduce the minimum\ndescription length principle to compare mixtures of exponential distributions\nwith different numbers of components (i.e., constituent exponential\ndistributions). Because these distributions violate the identifiability\nproperty, one is mathematically not allowed to apply the Akaike or Bayes\ninformation criteria to their maximum likelihood estimator to carry out model\nselection. We overcome this theoretical barrier by applying a minimum\ndescription principle to joint likelihoods of the data and latent variables. We\nshow that mixtures of exponential distributions with a few components are\nselected as opposed to more complex mixtures in various data sets and that the\nfitting accuracy is comparable to that of state-of-the-art algorithms to fit\npower-law distributions to data. Our results lend support to Poissonian\nexplanations of apparently non-Poissonian human behavior.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:03:19 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:34:33 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Okada", "Makoto", ""], ["Yamanishi", "Kenji", ""], ["Masuda", "Naoki", ""]]}, {"id": "1905.00803", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri and Mark S. Handcock", "title": "A Conditional Empirical Likelihood Based Method for Model Parameter\n  Estimation from Complex survey Datasets", "comments": null, "journal-ref": "Statistics and Applications, Volume 16, No. 1, 2018 (New Series),\n  pp 245-268", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an empirical likelihood framework for inference for a statistical\nmodel based on an informative sampling design. Covariate information is\nincorporated both through the weights and the estimating equations. The\nestimator is based on conditional weights. We show that under usual conditions,\nwith population size increasing unbounded, the estimates are strongly\nconsistent, asymptotically unbiased and normally distributed. Our framework\nprovides additional justification for inverse probability weighted score\nestimators in terms of conditional empirical likelihood. In doing so, it\nbridges the gap between design-based and model-based modes of inference in\nsurvey sampling settings. We illustrate these ideas with an application to an\nelectoral survey.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:24:36 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1905.00816", "submitter": "Ozgur Asar", "authors": "Ozgur Asar, Marie-Cecile Fournier, Etienne Dantan", "title": "Dynamic predictions of kidney graft survival in the presence of\n  longitudinal outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic predictions of survival outcomes are of great interest to physicians\nand patients, since such predictions are useful elements of clinical\ndecision-making. Joint modelling of longitudinal and survival data has been\nincreasingly used to obtain dynamic predictions. A common assumption of joint\nmodelling is that random-effects and error terms in the longitudinal sub-model\nare Gaussian. However, this assumption may be too restrictive, e.g. in the\npresence of outliers as commonly encountered in many real-life applications. A\nnatural extension is to robustify the joint models by assuming more flexible\ndistributions than Gaussian for the random-effects and/or error terms. Previous\nresearch reported improved performance of robust joint models compared to the\nGaussian version in terms of parameter estimation, but dynamic prediction\naccuracy obtained from such approach has not been yet evaluated. In this study,\nwe define a general robust joint model with t-distributed random-effects and\nerror terms under a Bayesian paradigm. Dynamic predictions of graft failure\nwere obtained for kidney transplant recipients from the French transplant\ncohort, DIVAT. Calibration and discrimination performances of Gaussian and\nrobust joint models were compared for a validation sample. Dynamic predictions\nfor two individuals are presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:43:31 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 19:03:36 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Asar", "Ozgur", ""], ["Fournier", "Marie-Cecile", ""], ["Dantan", "Etienne", ""]]}, {"id": "1905.00822", "submitter": "Daniel Daly-Grafstein", "authors": "Luke Bornn, Daniel Daly-Grafstein", "title": "Using In-Game Shot Trajectories to Better Understand Defensive Impact in\n  the NBA", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3-point shooting in the NBA continues to increase, the importance of\nperimeter defense has never been greater. Perimeter defenders are often\nevaluated by their ability to tightly contest shots, but how exactly does\ncontesting a jump shot cause a decrease in expected shooting percentage, and\ncan we use this insight to better assess perimeter defender ability? In this\npaper we analyze over 50,000 shot trajectories from the NBA to explain why, in\nterms of impact on shot trajectories, shooters tend to miss more when tightly\ncontested. We present a variety of results derived from this shot trajectory\ndata. Additionally, pairing trajectory data with features such as defender\nheight, distance, and contest angle, we are able to evaluate not just perimeter\ndefenders, but also shooters' resilience to defensive pressure. Utilizing shot\ntrajectories and corresponding modeled shot-make probabilities, we are able to\ncreate perimeter defensive metrics that are more accurate and less variable\nthan traditional metrics like opponent field goal percentage.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:54:16 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Bornn", "Luke", ""], ["Daly-Grafstein", "Daniel", ""]]}, {"id": "1905.01106", "submitter": "Ozgur Asar", "authors": "\\\"Ozg\\\"ur Asar", "title": "Bayesian analysis of Turkish Income and Living Conditions data, using\n  clustered longitudinal ordinal modelling with Bridge distributed\n  random-effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the panel surveys, called Statistics on Income and\nLiving Conditions (SILC), conducted annually on (randomly selected)\ncountry-representative households to monitor EU 2020 aims on poverty reduction.\nWe particularly consider the surveys conducted in Turkey, within the scope of\nintegration to the EU, between 2010 and 2013. Our main interests are on health\naspects of economic and living conditions. The outcome is {\\it self-reported\nhealth} that is clustered longitudinal ordinal, since repeated measures of it\nare nested within individuals and individuals are nested within families.\nEconomic and living conditions were measured through a number of individual-\nand family-level explanatory variables. The questions of interest are on the\nmarginal relationships between the outcome and covariates that are addressed\nusing a polytomous logistic regression with Bridge distributed random-effects.\nThis choice of distribution allows one to {\\it directly} obtain marginal\ninferences in the presence of random-effects. Widely used Normal distribution\nis also considered as the random-effects distribution. Samples from the joint\nposterior density of parameters and random-effects are drawn using Markov Chain\nMonte Carlo. Interesting findings from public health point of view are that\ndifferences were found between sub-groups of employment status, income level\nand panel year in terms of odds of reporting better health.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 10:33:35 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 09:30:14 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Asar", "\u00d6zg\u00fcr", ""]]}, {"id": "1905.01218", "submitter": "Jeffrey Doser", "authors": "Jeffrey W. Doser, Kristina M. Hannam, Andrew O. Finley", "title": "Characterizing functional relationships between anthropogenic and\n  biological sounds: A western New York state soundscape case study", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roads are a widespread feature of landscapes worldwide, and road traffic\nsound potentially makes nearby habitat unsuitable for acoustically\ncommunicating organisms. It is important to understand the influence of roads\nat the soundscape level to mitigate negative impacts of road sound on\nindividual species as well as subsequent effects on the surrounding landscape.\nWe seek to characterize the relationship between anthropogenic and biological\nsounds in western New York and assess the extent to which available traffic\ndata explains variability in anthropogenic noise. Recordings were obtained in\nthe spring of 2016 at 18 sites throughout western New York. We used the Welch\nPower Spectral Density (PSD) at low frequencies (0.5-2 kHz) to represent\nanthropogenic noise and PSD values at higher frequencies (2-11 kHz) to\nrepresent biological sound. Relationships were modeled using a novel two-stage\nhierarchical Bayesian model utilizing beta regression and basis splines. Model\nresults and map predictions illustrate that anthropogenic noise and biological\nsound have an inverse relationship, and anthropogenic noise is greatest in\nclose proximity to high traffic volume roads. The predictions have large\nuncertainty, resulting from the temporal coarseness of public road data used as\na proxy for traffic sound. Results suggest that finer temporal resolution\ntraffic sound data, such as crowd-sourced time-indexed traffic data from\ngeographic positioning systems, might better account for observed temporal\nchanges in the soundscape. The use of such data, in combination with the\nproposed modeling framework, could have important implications for the\ndevelopment of sound management policies.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 15:14:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 22:27:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Doser", "Jeffrey W.", ""], ["Hannam", "Kristina M.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "1905.01241", "submitter": "Daniel Williamson", "authors": "Daniel B. Williamson and Philip G. Sansom", "title": "How are emergent constraints quantifying uncertainty and what do they\n  leave behind?", "comments": null, "journal-ref": null, "doi": "10.1175/BAMS-D-19-0131.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of emergent constraints to quantify uncertainty for key policy\nrelevant quantities such as Equilibrium Climate Sensitivity (ECS) has become\nincreasingly widespread in recent years. Many researchers, however, claim that\nemergent constraints are inappropriate or even under-report uncertainty. In\nthis paper we contribute to this discussion by examining the emergent\nconstraints methodology in terms of its underpinning statistical assumptions.\nWe argue that the existing frameworks are based on indefensible assumptions,\nthen show how weakening them leads to a more transparent Bayesian framework\nwherein hitherto ignored sources of uncertainty, such as how reality might\ndiffer from models, can be quantified. We present a guided framework for the\nquantification of additional uncertainties that is linked to the confidence we\ncan have in the underpinning physical arguments for using linear constraints.\nWe provide a software tool for implementing our general framework for emergent\nconstraints and use it to illustrate the framework on a number of recent\nemergent constraints for ECS. We find that the robustness of any constraint to\nadditional uncertainties depends strongly on the confidence we can have in the\nunderpinning physics, allowing a future framing of the debate over the validity\nof a particular constraint around the underlying physical arguments, rather\nthan statistical assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 15:59:53 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Williamson", "Daniel B.", ""], ["Sansom", "Philip G.", ""]]}, {"id": "1905.01243", "submitter": "Elena Kulinskaya", "authors": "Ilyas Bakbergenuly, David C. Hoaglin, and Elena Kulinskaya", "title": "Simulation study of estimating between-study variance and overall effect\n  in meta-analyses of log-response-ratio for lognormal data", "comments": "17 pages and full simulation results, comprising 160 figures, each\n  presenting 12 combinations of sample sizes and numbers of studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for random-effects meta-analysis require an estimate of the\nbetween-study variance, $\\tau^2$. The performance of estimators of $\\tau^2$\n(measured by bias and coverage) affects their usefulness in assessing\nheterogeneity of study-level effects, and also the performance of related\nestimators of the overall effect. For the effect measure log-response-ratio\n(LRR, also known as the logarithm of the ratio of means, RoM), we review four\npoint estimators of $\\tau^2$ (the popular methods of DerSimonian-Laird (DL),\nrestricted maximum likelihood, and Mandel and Paule (MP), and the less-familiar\nmethod of Jackson), four interval estimators for $\\tau^2$ (profile likelihood,\nQ-profile, Biggerstaff and Jackson, and Jackson), five point estimators of the\noverall effect (the four related to the point estimators of $\\tau^2$ and an\nestimator whose weights use only study-level sample sizes), and seven interval\nestimators for the overall effect (four based on the point estimators for\n$\\tau^2$, the Hartung-Knapp-Sidik-Jonkman (HKSJ) interval, a modification of\nHKSJ that uses the MP estimator of $\\tau^2$ instead of the DL estimator, and an\ninterval based on the sample-size-weighted estimator). We obtain empirical\nevidence from extensive simulations of data from lognormal distributions.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:01:54 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 19:33:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "1905.01770", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko, Dmitry Logashenko, Raul Tempone, Gabriel Wittum,\n  David Keyes", "title": "Propagation of Uncertainties in Density-Driven Flow", "comments": "21 page, 9 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate modeling of contamination in subsurface flow and water aquifers is\ncrucial for agriculture and environmental protection. Here, we demonstrate a\nparallel method to quantify the propagation of the uncertainty in the dispersal\nof pollution in subsurface flow. Specifically, we consider the density-driven\nflow and estimate how uncertainty from permeability and porosity propagates to\nthe solution. We take an Elder-like problem as a numerical benchmark and we use\nrandom fields to model the limited knowledge on the porosity and permeability.\nWe construct a low-cost generalized polynomial chaos expansion (gPC) surrogate\nmodel, where the gPC coefficients are computed by projection on sparse and full\ntensor grids. We parallelize both the numerical solver for the deterministic\nproblem based on the multigrid method, and the quadrature over the parametric\nspace\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 00:04:27 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Litvinenko", "Alexander", ""], ["Logashenko", "Dmitry", ""], ["Tempone", "Raul", ""], ["Wittum", "Gabriel", ""], ["Keyes", "David", ""]]}, {"id": "1905.01838", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Felix M. Kluxen", "title": "Robust multiple comparisons against a control group with application in\n  toxicology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dunnett procedure compares several treatment or dose groups with a\ncontrol group, while controlling the familywise error rate. When deviations\nfrom the normal distribution and heterogeneous variances occur, the nominal\n$\\alpha$ level may be violated, and power may be reduced. Various robust\nmodifications are discussed, whereby the novel most likely transformation\n(MLT)-Dunnett version is recommended as almost always appropriate by means of a\nsimulation study. The MLT-Dunnett is especially useful because it can jointly\nand comparably analyse differently scaled endpoints. Furthermore, a related\nmultiple endpoints test is proposed using the odds ratio as a common effect\nsize. With the statistical software R, the method is readily applicable using\nthe CRAN libraries \\verb|multcomp| and \\verb|mlt|, real data can be easily\nanalyzed.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 06:35:21 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Kluxen", "Felix M.", ""]]}, {"id": "1905.02015", "submitter": "Claus Vogl", "authors": "Claus Vogl and Lynette Caitlin Mikula", "title": "Maximum likelihood (ML) estimators for scaled mutation parameters with a\n  strand symmetric mutation model in equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the multiallelic parent-independent mutation-drift model, the\nequilibrium proportions of alleles are known to be Dirichlet distributed. A\nspecial case is the biallelic model, in which the proportions are beta\ndistributed. A sample taken from these models is then Dirichlet-multinomially\nor beta-binomially distributed, respectively. Maximum likelihood (ML)\nestimators for the mutation parameters of the biallelic parent-independent\nmutation model are available via an expectation maximization algorithm.\nAssuming small scaled mutation rates, the distribution of a sample of size $M$\ncan be expanded in a Taylor series of first order. Then the ML estimators for\nthe two parameters in the biallelic model can be expressed using the site\nfrequency spectrum. In this article, we go beyond parent-independent mutation\nand analyse a strand-symmetric mutation model with six scaled mutation\nparameters that deviates from parent independent mutation and, generally, from\ndetailed balance. We derive ML estimators for these six parameters assuming\nmutation-drift equilibrium and small scaled mutation rates. This is the first\ntime that ML estimators are provided for a mutation model more complex than\nparent-independent mutation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:05:47 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Vogl", "Claus", ""], ["Mikula", "Lynette Caitlin", ""]]}, {"id": "1905.02061", "submitter": "Xin Shi", "authors": "Xin Shi, Robert Qiu", "title": "Estimation of high-dimensional factor models and its application in\n  power data analysis", "comments": "10 pages, submitted to IEEE Trans. Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dealing with high-dimensional data, factor models are often used for\nreducing dimensions and extracting relevant information. The spectrum of\ncovariance matrices from power data exhibits two aspects: 1) bulk, which arises\nfrom random noise or fluctuations and 2) spikes, which represents factors\ncaused by anomaly events. In this paper, we propose a new approach to the\nestimation of high-dimensional factor models, minimizing the distance between\nthe empirical spectral density (ESD) of covariance matrices of the residuals of\npower data that are obtained by subtracting principal components and the\nlimiting spectral density (LSD) from a multiplicative covariance structure\nmodel. The free probability theory (FPT) is used to derive the spectral density\nof the multiplicative covariance model, which efficiently solves the\ncomputational difficulties. The proposed approach connects the estimation of\nthe number of factors to the LSD of covariance matrices of the residuals, which\nprovides estimators of the number of factors and the correlation structure\ninformation in the residuals. Considering a lot of measurement noise is\ncontained in the power data and the correlation structure is complex for the\nresiduals, the approach prefers approaching the ESD of covariance matrices of\nthe residuals through a multiplicative covariance model, which avoids making\ncrude assumptions or simplifications on the complex structure of the data.\nTheoretical studies show the proposed approach is robust against noise and\nsensitive to the presence of weak factors. The synthetic data from IEEE 118-bus\npower system is used to validate the effectiveness of the approach.\nFurthermore, the application to the analysis of the real-world online\nmonitoring data in a power grid shows that the estimators in the approach can\nbe used to indicate the system behavior.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:33:17 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 04:12:46 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shi", "Xin", ""], ["Qiu", "Robert", ""]]}, {"id": "1905.02062", "submitter": "Qi Long", "authors": "Pallavi Mishra-Kalyani and Brent A. Johnson and Jonathan D. Glass and\n  Qi Long", "title": "Estimating the effect of PEG in ALS patients using observational data\n  subject to censoring by death and missing outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though they may offer valuable patient and disease information that is\nimpossible to study in a randomized trial, clinical disease registries also\nrequire special care and attention in causal inference. Registry data may be\nincomplete, inconsistent, and subject to confounding. In this paper we aim to\naddress several analytical issues in estimating treatment effects that plague\nclinical registries such as the Emory amyotrophic lateral sclerosis (ALS)\nClinic Registry. When attempting to assess the effect of a surgical insertion\nof a percutaneous endoscopic gastrostomy (PEG) tube on body mass index (BMI)\nusing the data from the ALS Clinic Registry, one must combat issues of\nconfounding, censoring by death, and missing outcome data that have not been\naddressed in previous studies of PEG. We propose a causal inference framework\nfor estimating the survivor average causal effect (SACE) of PEG, which\nincorporates a model for generalized propensity scores to correct for\nconfounding by pre-treatment variables, a model for principal stratification to\naccount for censoring by death, and a model for the missing data mechanism.\nApplying the proposed framework to the ALS Clinic Registry Data, our analysis\nshows that PEG has a positive SACE on BMI at month 18 post-baseline; our\nresults likely offer more definitive answers regarding the effect of PEG than\nprevious studies of PEG.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:33:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mishra-Kalyani", "Pallavi", ""], ["Johnson", "Brent A.", ""], ["Glass", "Jonathan D.", ""], ["Long", "Qi", ""]]}, {"id": "1905.02068", "submitter": "Quentin Frederik Gronau", "authors": "Quentin F. Gronau, K. N. Akash Raj, and Eric-Jan Wagenmakers", "title": "Informed Bayesian Inference for the A/B Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Booming in business and a staple analysis in medical trials, the A/B test\nassesses the effect of an intervention or treatment by comparing its success\nrate with that of a control condition. Across many practical applications, it\nis desirable that (1) evidence can be obtained in favor of the null hypothesis\nthat the treatment is ineffective; (2) evidence can be monitored as the data\naccumulate; (3) expert prior knowledge can be taken into account. Most existing\napproaches do not fulfill these desiderata. Here we describe a Bayesian A/B\nprocedure based on Kass and Vaidyanathan (1992) that allows one to monitor the\nevidence for the hypotheses that the treatment has either a positive effect, a\nnegative effect, or, crucially, no effect. Furthermore, this approach enables\none to incorporate expert knowledge about the relative prior plausibility of\nthe rival hypotheses and about the expected size of the effect, given that it\nis non-zero. To facilitate the wider adoption of this Bayesian procedure we\ndeveloped the abtest package in R. We illustrate the package options and the\nassociated statistical results with a fictitious business example and a real\ndata medical example.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:43:37 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 12:41:05 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 14:13:12 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 10:36:46 GMT"}, {"version": "v5", "created": "Sun, 16 Aug 2020 13:32:56 GMT"}, {"version": "v6", "created": "Fri, 13 Nov 2020 16:15:58 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Gronau", "Quentin F.", ""], ["Raj", "K. N. Akash", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1905.02090", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski", "title": "Collapse of cooperation and corruption in a mathematical model within\n  game theory including Moldovan case study (Homo Sociologicus vs Homo\n  Economicus)", "comments": "conference proceedings", "journal-ref": "PROBLEMELE VIABILITATII ECONOMICO MANAGERIALE PENTRU ASIGURAREA\n  SECURITATII ECONOMICE 2015", "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple model from game theory, which can imitate a mechanism of corruption\nand cooperation patterns, is proposed. The settings are divided into two\nstudies with examples related to safety of Moldovan economy. In Homo Economicus\nworld, players seem to act in rational way and decisions are driven by payouts.\nIn Homo Sociologicus world decisions are described by players acquiring\nreputation and evolving altruism, which in turn determine their choice of\nstrategy (evolution of cooperation). I shall discuss both modeling approaches\nof Prisoner Dilemma, to understand collapse of cooperation and increase of\ncorruption in postcommunistic countries as Republic of Moldova. Possible more\nefficient for society outcomes (fairness equilibrium) are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 15:31:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Jarynowski", "Andrzej", ""]]}, {"id": "1905.02257", "submitter": "Shu Wang", "authors": "Shu Wang, Jonathan G. Yabes and Chung-Chou H. Chang", "title": "Hybrid Density- and Partition-based Clustering Algorithm for Data with\n  Mixed-type Variables", "comments": null, "journal-ref": "Journal of Data Science 19(2021)15-36", "doi": "10.6339/21-JDS996", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an essential technique for discovering patterns in data. The\nsteady increase in amount and complexity of data over the years led to\nimprovements and development of new clustering algorithms. However, algorithms\nthat can cluster data with mixed variable types (continuous and categorical)\nremain limited, despite the abundance of data with mixed types particularly in\nthe medical field. Among existing methods for mixed data, some posit\nunverifiable distributional assumptions or that the contributions of different\nvariable types are not well balanced.\n  We propose a two-step hybrid density- and partition-based algorithm (HyDaP)\nthat can detect clusters after variables selection. The first step involves\nboth density-based and partition-based algorithms to identify the data\nstructure formed by continuous variables and recognize the important variables\nfor clustering; the second step involves partition-based algorithm together\nwith a novel dissimilarity measure we designed for mixed data to obtain\nclustering results. Simulations across various scenarios and data structures\nwere conducted to examine the performance of the HyDaP algorithm compared to\ncommonly used methods. We also applied the HyDaP algorithm on electronic health\nrecords to identify sepsis phenotypes.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 20:34:35 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Shu", ""], ["Yabes", "Jonathan G.", ""], ["Chang", "Chung-Chou H.", ""]]}, {"id": "1905.02406", "submitter": "Francesca Fortunato", "authors": "Laura Anderlucci, Francesca Fortunato, Angela Montanari", "title": "One-class classification with application to forensic analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of broken glass is forensically important to reconstruct the\nevents of a criminal act. In particular, the comparison between the glass\nfragments found on a suspect (recovered cases) and those collected on the crime\nscene (control cases) may help the police to correctly identify the\noffender(s). The forensic issue can be framed as a one-class classification\nproblem. One-class classification is a recently emerging and special\nclassification task, where only one class is fully known (the so-called target\nclass), while information on the others is completely missing. We propose to\nconsider classic Gini's transvariation probability as a measure of typicality,\ni.e. a measure of resemblance between an observation and a set of well-known\nobjects (the control cases). The aim of the proposed Transvariation-based\nOne-Class Classifier (TOCC) is to identify the best boundary around the target\nclass, that is, to recognise as many target objects as possible while rejecting\nall those deviating from this class.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:41:37 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Anderlucci", "Laura", ""], ["Fortunato", "Francesca", ""], ["Montanari", "Angela", ""]]}, {"id": "1905.02452", "submitter": "Rapha\\\"elle Momal", "authors": "Rapha\\\"elle Momal, St\\'ephane Robin, Christophe Ambroise", "title": "Tree-based Inference of Species Interaction Network from Abundance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of ecological systems mainly relies on the interactions between\nthe species it involves. We consider the problem of inferring the species\ninteraction network from abundance data. To be relevant, any network inference\nmethodology needs to handle count data and to account for possible\nenvironmental effects. It also needs to distinguish between direct interactions\nand indirect associations and graphical models provide a convenient framework\nfor this purpose. We introduce a generic statistical model for network\ninference based on abundance data. The model includes fixed effects to account\nfor environmental covariates and sampling efforts, and correlated random\neffects to encode species interactions. The inferred network is obtained by\naveraging over all possible tree-shaped (and therefore sparse) networks, in a\ncomputationally efficient manner. An output of the procedure is the probability\nfor each edge to be part of the underlying network. A simulation study shows\nthat the proposed methodology compares well with state-of-the-art approaches,\neven when the underlying graph strongly differs from a tree. The analysis of\ntwo datasets highlights the influence of covariates on the inferred network.\nAccounting for covariates is critical to avoid spurious edges. The proposed\napproach could be extended to perform network comparison or to look for missing\nspecies.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 10:27:53 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 14:00:54 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Momal", "Rapha\u00eblle", ""], ["Robin", "St\u00e9phane", ""], ["Ambroise", "Christophe", ""]]}, {"id": "1905.02659", "submitter": "Isabella Gollini", "authors": "Isabella Gollini", "title": "A mixture model approach for clustering bipartite networks", "comments": "To appear in \"Challenges in Social Network Research\" Volume in the\n  Lecture Notes in Social Networks (LNSN - Series of Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter investigates the latent structure of bipartite networks via a\nmodel-based clustering approach which is able to capture both latent groups of\nsending nodes and latent variability of the propensity of sending nodes to\ncreate links with receiving nodes within each group. This modelling approach is\nvery flexible and can be estimated by using fast inferential approaches such as\nvariational inference. We apply this model to the analysis of a terrorist\nnetwork in order to identify the main latent groups of terrorists and their\nlatent trait scores based on their attendance to some events.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:05:20 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:46:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Gollini", "Isabella", ""]]}, {"id": "1905.02721", "submitter": "Jin Xu", "authors": "Jin Xu and Xu He and Xiaojun Duan and Zhengming Wang", "title": "Sliced Latin hypercube designs with arbitrary run sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latin hypercube designs achieve optimal univariate stratifications and are\nuseful for computer experiments. Sliced Latin hypercube designs are Latin\nhypercube designs that can be partitioned into smaller Latin hypercube designs.\nIn this work, we give, to the best of our knowledge, the first construction of\nsliced Latin hypercube designs that allow arbitrarily chosen run sizes for the\nslices. We also provide an algorithm to reduce correlations of our proposed\ndesigns.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:05:48 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Xu", "Jin", ""], ["He", "Xu", ""], ["Duan", "Xiaojun", ""], ["Wang", "Zhengming", ""]]}, {"id": "1905.02944", "submitter": "Yoann Altmann", "authors": "Yoann Altmann, Stephen McLaughlin, Michael E. Davies", "title": "Fast online 3D reconstruction of dynamic scenes from individual\n  single-photon detection events", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2952008", "report-no": null, "categories": "eess.IV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for online 3D reconstruction of\ndynamic scenes using individual times of arrival (ToA) of photons recorded by\nsingle-photon detector arrays. One of the main challenges in 3D imaging using\nsingle-photon Lidar is the integration time required to build ToA histograms\nand reconstruct reliable 3D profiles in the presence of non-negligible ambient\nillumination. This long integration time also prevents the analysis of rapid\ndynamic scenes using existing techniques. We propose a new method which does\nnot rely on the construction of ToA histograms but allows, for the first time,\nindividual detection events to be processed online, in a parallel manner in\ndifferent pixels, while accounting for the intrinsic spatiotemporal structure\nof dynamic scenes. Adopting a Bayesian approach, a Bayesian model is\nconstructed to capture the dynamics of the 3D profile and an approximate\ninference scheme based on assumed density filtering is proposed, yielding a\nfast and robust reconstruction algorithm able to process efficiently thousands\nto millions of frames, as usually recorded using single-photon detectors. The\nperformance of the proposed method, able to process hundreds of frames per\nsecond, is assessed using a series of experiments conducted with static and\ndynamic 3D scenes and the results obtained pave the way to a new family of\nreal-time 3D reconstruction solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 07:41:33 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Altmann", "Yoann", ""], ["McLaughlin", "Stephen", ""], ["Davies", "Michael E.", ""]]}, {"id": "1905.03092", "submitter": "Chaitanya K. Joshi", "authors": "Kuhu Joshi, Chaitanya K. Joshi", "title": "Working women and caste in India: A study of social disadvantage using\n  feature attribution", "comments": "Presented at the ICLR AI for Social Good Workshop 2019; Updated with\n  Addendum (Jan 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Women belonging to the socially disadvantaged caste-groups in India have\nhistorically been engaged in labour-intensive, blue-collar work. We study\nwhether there has been any change in the ability to predict a woman's\nwork-status and work-type based on her caste by interpreting machine learning\nmodels using feature attribution. We find that caste is now a less important\ndeterminant of work for the younger generation of women compared to the older\ngeneration. Moreover, younger women from disadvantaged castes are now more\nlikely to be working in white-collar jobs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 07:15:33 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 12:54:19 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Joshi", "Kuhu", ""], ["Joshi", "Chaitanya K.", ""]]}, {"id": "1905.03138", "submitter": "Ehsan Ebrahimzadeh", "authors": "Ehsan Ebrahimzadeh, Maggie Engler, David Tse, Razvan Cristescu, Aslan\n  Tchamkerten", "title": "Somatic mutations render human exome and pathogen DNA more similar", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0197949", "report-no": null, "categories": "q-bio.GN cs.IT math.IT math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immunotherapy has recently shown important clinical successes in a\nsubstantial number of oncology indications. Additionally, the tumor somatic\nmutation load has been shown to associate with response to these therapeutic\nagents, and specific mutational signatures are hypothesized to improve this\nassociation, including signatures related to pathogen insults. We sought to\nstudy in silico the validity of these observations and how they relate to each\nother. We first addressed whether somatic mutations typically involved in\ncancer may increase, in a statistically meaningful manner, the similarity\nbetween common pathogens and the human exome. Our study shows that common\nmutagenic processes increase, in the upper range of biologically plausible\nfrequencies, the similarity between cancer exomes and pathogen DNA at a scale\nof 12-16 nucleotide sequences and established that this increased similarity is\ndue to the specific mutation distribution of the considered mutagenic\nprocesses. Next, we studied the impact of mutation rate and showed that\nincreasing mutation rate generally results in an increased similarity between\nthe cancer exome and pathogen DNA, at a scale of 4-5 amino acids. Finally, we\ninvestigated whether the considered mutational processes result in amino-acid\nchanges with functional relevance that are more likely to be immunogenic. We\nshowed that functional tolerance to mutagenic processes across species\ngenerally suggests more resilience to mutagenic processes that are due to\nexposure to elements of nature than to mutagenic processes that are due to\nexposure to cancer-causing artificial substances. These results support the\nidea that recognition of pathogen sequences as well as differential functional\ntolerance to mutagenic processes may play an important role in the immune\nrecognition process involved in tumor infiltration by lymphocytes.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:52:34 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Ebrahimzadeh", "Ehsan", ""], ["Engler", "Maggie", ""], ["Tse", "David", ""], ["Cristescu", "Razvan", ""], ["Tchamkerten", "Aslan", ""]]}, {"id": "1905.03325", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Fair tournament design: A flaw of the UEFA Euro 2020 qualification", "comments": "21 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrity of a sport can be seriously undermined if its rules punish\nwinning as this creates incentives for strategic manipulation. Therefore, a\nsports tournament can be called unfair if the overall win probabilities are not\nordered according to the teams' ranking based on their past performances. We\npresent how statistical methods can contribute to choosing a tournament format\nthat is in line with the above axiom. In particular, the qualification for the\n2020 UEFA European Championship is shown to violate this requirement: being a\ntop team in the lowest-ranked League D of the 2018/19 UEFA Nations League\nsubstantially increases the probability of qualifying compared to being a\nbottom team in the higher-ranked League C. The unfairness can be remarkably\nreduced or even eliminated with slightly changing the path formation policy of\nthe UEFA Euro 2020 qualifying play-offs. The misaligned design has severely\npunished a team for winning a match years before. Since the deficiency is an\ninherent feature of the qualifying process, the Union of European Football\nAssociations (UEFA) should reconsider the format of future tournaments to\neliminate the unfair advantage enjoyed by certain teams.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 20:43:00 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 13:22:16 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 16:00:10 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2020 18:53:29 GMT"}, {"version": "v5", "created": "Mon, 7 Sep 2020 15:57:38 GMT"}, {"version": "v6", "created": "Fri, 12 Feb 2021 12:35:19 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1905.03372", "submitter": "Manuel Herrera", "authors": "Carlo Giudicianni, Manuel Herrera, Armando di Nardo, Kemi Adeyeye", "title": "Automatic multiscale approach for water networks partitioning into\n  dynamic district metered areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel methodology to automatically split a water\ndistribution system (WDS) into self-adapting district metered areas (DMAs) of\ndifferent size. Complex networks theory is used to propose a novel multiscale\nnetwork layout made by landmark and key nodes for the water supply plus the\nhyper-links representing the connection between them. The proposed multiscale\nlayout partitioning was tested on a real medium-size water distribution\nnetwork. This is shown to naturally support further DMA aggregation /\ndisaggregation operations with the direct benefit of providing a better dynamic\nsystem control and superior efficient water management than static DMA\nconfigurations, particularly in the case of abnormal functioning conditions.\nThe proposed tool gives the possibility to automatically define a dynamic\npartitioning of WDSs according to spatial and temporal water demand\nvariability, ensuring an efficient, sustainable and low-cost management of the\nsystem whilst simultaneously preserving the hydraulic performance of the WDS.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 22:14:09 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Giudicianni", "Carlo", ""], ["Herrera", "Manuel", ""], ["di Nardo", "Armando", ""], ["Adeyeye", "Kemi", ""]]}, {"id": "1905.03467", "submitter": "Maia Lesosky", "authors": "Maia Lesosky, Tracy Glass, Brian Rambau, Nei-Yuan Hsiao, Elaine J\n  Abrams, Landon Myer", "title": "Bias in the estimation of cumulative viremia in cohort studies of\n  HIV-infected individuals", "comments": "16 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The use of cumulative measures of exposure to raised HIV viral load\n(viremia copy-years) is an increasingly common in HIV prevention and treatment\nepidemiology due to the high biological plausibility. We sought to estimate the\nmagnitude and direction of bias in a cumulative measure of viremia caused by\ndifferent frequency of sampling and duration of follow-up.\n  Methods: We simulated longitudinal viral load measures and reanalysed cohort\nstudy datasets with longitudinal viral load measurements under different\nsampling strategies to estimate cumulative viremia.\n  Results: In both simulated and observed data, estimates of cumulative viremia\nby the trapezoidal rule show systematic upward bias when there are fewer\nsampling time points and/or increased duration between sampling time points,\ncompared to estimation of full time series. Absolute values of cumulative\nviremia vary appreciably by the patterns of viral load over time, even after\nadjustment for total duration of follow up.\n  Conclusions: Sampling bias due to differential frequency of sampling appears\nextensive and of meaningful magnitude in measures of cumulative viremia.\nCumulative measures of viremia should be used only in studies with sufficient\nfrequency of viral load measures and always as relative measures.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 07:17:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Lesosky", "Maia", ""], ["Glass", "Tracy", ""], ["Rambau", "Brian", ""], ["Hsiao", "Nei-Yuan", ""], ["Abrams", "Elaine J", ""], ["Myer", "Landon", ""]]}, {"id": "1905.03530", "submitter": "Maria Michela Dickson", "authors": "Maria Michela Dickson, Giuseppe Espa, Lorenzo Fattorini", "title": "Double-calibration estimators accounting for under-coverage and\n  nonresponse in socio-economic surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under-coverage and nonresponse problems are jointly present in most\nsocio-economic surveys. The purpose of this paper is to propose a completely\ndesign-based estimation strategy that accounts for both problems without\nresorting to models but simply performing a two-step calibration. The first\ncalibration exploits a set of auxiliary variables only available for the units\nin the sampled population to account for nonresponse. The second calibration\nexploits a different set of auxiliary variables available for the whole\npopulation, to account for under-coverage. The two calibrations are then\nunified in a double-calibration estimator. Mean and variance of the estimator\nare derived up to the first order of approximation. Conditions ensuring\napproximate unbiasedness are derived and discussed. The strategy is empirically\nchecked by a simulation study performed on a set of artificial populations. A\ncase study is lead on Danish data coming from the European Union Statistics on\nIncome and Living Conditions survey. The strategy proposed is flexible and\nsuitable in most situations in which both under-coverage and nonresponse are\npresent.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 11:10:29 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Fattorini", "Lorenzo", ""]]}, {"id": "1905.03594", "submitter": "Booma Sowkarthiga Balasubramani", "authors": "Booma Sowkarthiga Balasubramani, Marco Nanni, Shin Imai, Isabel F.\n  Cruz", "title": "The Identification and Analysis of Indicators for Predicting Malarial\n  Incidence in Zimbabwe", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With over 50% of the country's population at risk of contracting malaria\ndespite the introduction of several measures to combat the disease, Zimbabwe is\none of the eight countries in the Malaria Elimination 8 platform of the\nSouthern African Development Community. Various indicators, including\ntemperature, population distribution, land cover, and access to hospitals\naffect the incidence and spread of this disease. In this paper, we consider\ndifferent such indicators and present our analysis of their interaction (e.g.,\nhow the Plasmodium falciparum Parasite Rate (PfPR) affects the sickle cell\ntrait) and their effect on malaria incidence in Zimbabwe. We also discuss the\nresults of our preliminary experiments on predictive analytics of malaria\nincidence based on the indicators we have considered.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 20:17:01 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Balasubramani", "Booma Sowkarthiga", ""], ["Nanni", "Marco", ""], ["Imai", "Shin", ""], ["Cruz", "Isabel F.", ""]]}, {"id": "1905.03611", "submitter": "Yang Qin", "authors": "Yang Qin, Rojiemiahd Edjoc, Nathaniel D Osgood", "title": "Effect of E-cigarette Use and Social Network on Smoking Behavior Change:\n  An agent-based model of E-cigarette and Cigarette Interaction", "comments": "10 pages, SBP-BRiMS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a general reduction in smoking in many areas of the developed world,\nit remains one of the biggest public health threats. As an alternative to\ntobacco, the use of electronic cigarettes (ECig) has been increased\ndramatically over the last decade. ECig use is hypothesized to impact smoking\nbehavior through several pathways, not only as a means of quitting cigarettes\nand lowering risk of relapse, but also as both an alternative nicotine delivery\ndevice to cigarettes, as a visible use of nicotine that can lead to imitative\nbehavior in the form of smoking, and as a gateway nicotine delivery technology\nthat can build high levels of nicotine tolerance and pave the way for\ninitiation of smoking. Evidence regarding the effect of ECig use on smoking\nbehavior change remains inconclusive. To address these challenges, we built an\nagent-based model (ABM) of smoking and ECig use to examine the effects of ECig\nuse on smoking behavior change. The impact of social network (SN) on the\ninitiation of smoking and ECig use were also explored. Findings from the\nsimulation suggest that the use of ECig generates substantially lower\nprevalence of current smoker (PCS), which demonstrates the potential for\nreducing smoking and lowering the risk of relapse. The effects of\nproximity-based influences within SN increases the prevalence of current ECig\nuser (PCEU). The model also suggests the importance of improved understanding\nof drivers in cessation and relapse in ECig use, in light of findings that such\naspects of behavior change may notably influence smoking behavior change and\nburden.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 01:27:37 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Qin", "Yang", ""], ["Edjoc", "Rojiemiahd", ""], ["Osgood", "Nathaniel D", ""]]}, {"id": "1905.03628", "submitter": "Lorenz Gilch", "authors": "Lorenz A. Gilch", "title": "Prediction Model for the Africa Cup of Nations 2019 via Nested Poisson\n  Regression", "comments": "14 pages, 3 figures, 15 tables. arXiv admin note: substantial text\n  overlap with arXiv:1806.01930", "journal-ref": "http://dx.doi.org/10.16929/ajas/2019.599.233", "doi": "10.16929/ajas/2019.599.233", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the forecast of the Africa Cup of Nations 2019\nfootball tournament. It is based on a Poisson regression model that includes\nthe Elo points of the participating teams as covariates and incorporates\ndifferences of team-specific skills. The proposed model allows predictions in\nterms of probabilities in order to quantify the chances for each team to reach\na certain stage of the tournament. Monte Carlo simulations are used to estimate\nthe outcome of each single match of the tournament and hence to simulate the\nwhole tournament itself. The model is fitted on all football games on neutral\nground of the participating teams since 2010.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 13:10:40 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Gilch", "Lorenz A.", ""]]}, {"id": "1905.03657", "submitter": "Daniel Eck", "authors": "Daniel J. Eck and Forrest W. Crawford", "title": "Efficient and minimal length parametric conformal prediction regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction methods construct prediction regions for iid data that\nare valid in finite samples. We provide two parametric conformal prediction\nregions that are applicable for a wide class of continuous statistical models.\nThis class of statistical models includes generalized linear models (GLMs) with\ncontinuous outcomes. Our parametric conformal prediction regions possesses\nfinite sample validity, even when the model is misspecified, and are\nasymptotically of minimal length when the model is correctly specified. The\nfirst parametric conformal prediction region is constructed through binning of\nthe predictor space, guarantees finite-sample local validity and is\nasymptotically minimal at the $\\sqrt{\\log(n)/n}$ rate when the dimension $d$ of\nthe predictor space is one or two, and converges at the\n$O\\{(\\log(n)/n)^{1/d}\\}$ rate when $d > 2$. The second parametric conformal\nprediction region is constructed by transforming the outcome variable to a\ncommon distribution via the probability integral transform, guarantees\nfinite-sample marginal validity, and is asymptotically minimal at the\n$\\sqrt{\\log(n)/n}$ rate. We develop a novel concentration inequality for\nmaximum likelihood estimation that induces these convergence rates. We analyze\nprediction region coverage properties, large-sample efficiency, and robustness\nproperties of four methods for constructing conformal prediction intervals for\nGLMs: fully nonparametric kernel-based conformal, residual based conformal,\nnormalized residual based conformal, and parametric conformal which uses the\nassumed GLM density as a conformity measure. Extensive simulations compare\nthese approaches to standard asymptotic prediction regions. The utility of the\nparametric conformal prediction region is demonstrated in an application to\ninterval prediction of glycosylated hemoglobin levels, a blood measurement used\nto diagnose diabetes.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:31:29 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 15:52:10 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 18:08:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Eck", "Daniel J.", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1905.03680", "submitter": "Shu Wang", "authors": "Shu Wang, Jonathan G. Yabes and Chung-Chou H. Chang", "title": "A Bayesian Finite Mixture Model with Variable Selection for Data with\n  Mixed-type Variables", "comments": "34 pages, 12 table and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture model is an important branch of clustering methods and can be\napplied on data sets with mixed types of variables. However, challenges exist\nin its applications. First, it typically relies on the EM algorithm which could\nbe sensitive to the choice of initial values. Second, biomarkers subject to\nlimits of detection (LOD) are common to encounter in clinical data, which\nbrings censored variables into finite mixture model. Additionally, researchers\nare recently getting more interest in variable importance due to the increasing\nnumber of variables that become available for clustering.\n  To address these challenges, we propose a Bayesian finite mixture model to\nsimultaneously conduct variable selection, account for biomarker LOD and obtain\nclustering results. We took a Bayesian approach to obtain parameter estimates\nand the cluster membership to bypass the limitation of the EM algorithm. To\naccount for LOD, we added one more step in Gibbs sampling to iteratively fill\nin biomarker values below or above LODs. In addition, we put a spike-and-slab\ntype of prior on each variable to obtain variable importance. Simulations\nacross various scenarios were conducted to examine the performance of this\nmethod. Real data application on electronic health records was also conducted.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:13:13 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Shu", ""], ["Yabes", "Jonathan G.", ""], ["Chang", "Chung-Chou H.", ""]]}, {"id": "1905.03900", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Dynamic principal component regression: Application to age-specific\n  mortality forecasting", "comments": "27 pages, 10 figures, to appear at ASTIN Bulletin", "journal-ref": "ASTIN Bulletin: The Journal of the IAA (2019)", "doi": "10.1017/asb.2019.20", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In areas of application, including actuarial science and demography, it is\nincreasingly common to consider a time series of curves; an example of this is\nage-specific mortality rates observed over a period of years. Given that age\ncan be treated as a discrete or continuous variable, a dimension reduction\ntechnique, such as principal component analysis, is often implemented. However,\nin the presence of moderate to strong temporal dependence, static principal\ncomponent analysis commonly used for analyzing independent and identically\ndistributed data may not be adequate. As an alternative, we consider a\n\\textit{dynamic} principal component approach to model temporal dependence in a\ntime series of curves. Inspired by Brillinger's (1974) theory of dynamic\nprincipal components, we introduce a dynamic principal component analysis,\nwhich is based on eigen-decomposition of estimated long-run covariance. Through\na series of empirical applications, we demonstrate the potential improvement of\none-year-ahead point and interval forecast accuracies that the dynamic\nprincipal component regression entails when compared with the static\ncounterpart.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 00:41:27 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1905.03981", "submitter": "Christian Bartels", "authors": "Christian Bartels, Johanna Mielke and Ekkehard Glimm", "title": "Confidence intervals with maximal average power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a frequentist testing procedure that maintains a defined coverage\nand is optimal in the sense that it gives maximal power to detect deviations\nfrom a null hypothesis when the alternative to the null hypothesis is sampled\nfrom a pre-specified distribution (the prior distribution). Selecting a prior\ndistribution allows to tune the decision rule. This leads to an increased\npower, if the true data generating distribution happens to be compatible with\nthe prior. It comes at the cost of losing power, if the data generating\ndistribution or the observed data are incompatible with the prior. We\nillustrate the proposed approach for a binomial experiment, which is\nsufficiently simple such that the decision sets can be illustrated in figures,\nwhich should facilitate an intuitive understanding. The potential beyond the\nsimple example will be discussed: the approach is generic in that the test is\ndefined based on the likelihood function and the prior only. It is\ncomparatively simple to implement and efficient to execute, since it does not\nrely on Minimax optimization. Conceptually it is interesting to note that for\nconstructing the testing procedure the Bayesian posterior probability\ndistribution is used.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 07:31:28 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 07:38:55 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 10:11:30 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bartels", "Christian", ""], ["Mielke", "Johanna", ""], ["Glimm", "Ekkehard", ""]]}, {"id": "1905.04022", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (CNRM), Anne-Laure Foug\\`eres (PSPM), Philippe\n  Naveau (LSCE), Rapha\\\"el de Fondeville (EPFL)", "title": "Extreme events evaluation using CRPS distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of ensemble forecasts for extreme events remains a challenging\nquestion. The general public as well as the media naturely pay particular\nattention on extreme events and conclude about the global predictive\nperformance of ensembles, which are often unskillful when they are needed.\nAshing classical verification tools to focus on such events can lead to\nunexpected behaviors. To square up these effects, thresholded and weighted\nscoring rules have been developed. Most of them use derivations of the\nContinuous Ranked Probability Score (CRPS). However, some properties of the\nCRPS for extreme events generate undesirable effects on the quality of\nverification. Using theoretical arguments and simulation examples, we\nillustrate some pitfalls of conventional verification tools and propose a\ndifferent direction to assess ensemble forecasts using extreme value theory,\nconsidering proper scores as random variables.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:15:38 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM"], ["Foug\u00e8res", "Anne-Laure", "", "PSPM"], ["Naveau", "Philippe", "", "LSCE"], ["de Fondeville", "Rapha\u00ebl", "", "EPFL"]]}, {"id": "1905.04028", "submitter": "Debopam Bhattacharya", "authors": "Debopam Bhattacharya, Pascaline Dupas, Shin Kanaya", "title": "Demand and Welfare Analysis in Discrete Choice Models with Social\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life settings of consumer-choice involve social interactions,\ncausing targeted policies to have spillover-effects. This paper develops novel\nempirical tools for analyzing demand and welfare-effects of\npolicy-interventions in binary choice settings with social interactions.\nExamples include subsidies for health-product adoption and vouchers for\nattending a high-achieving school. We establish the connection between\neconometrics of large games and Brock-Durlauf-type interaction models, under\nboth I.I.D. and spatially correlated unobservables. We develop new convergence\nresults for associated beliefs and estimates of preference-parameters under\nincreasing-domain spatial asymptotics. Next, we show that even with fully\nparametric specifications and unique equilibrium, choice data, that are\nsufficient for counterfactual demand-prediction under interactions, are\ninsufficient for welfare-calculations. This is because distinct underlying\nmechanisms producing the same interaction coefficient can imply different\nwelfare-effects and deadweight-loss from a policy-intervention. Standard\nindex-restrictions imply distribution-free bounds on welfare. We illustrate our\nresults using experimental data on mosquito-net adoption in rural Kenya.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:32:07 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bhattacharya", "Debopam", ""], ["Dupas", "Pascaline", ""], ["Kanaya", "Shin", ""]]}, {"id": "1905.04201", "submitter": "Edwin Wintermute V", "authors": "Edwin H. Wintermute, Matthieu Cisel, Ariel B. Lindner", "title": "A survival model for course-course interactions in a Massive Open Online\n  Course platform", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Course (MOOC) platforms incorporate large course catalogs\nfrom which individual students may register multiple courses. We performed a\nnetwork-based analysis of student achievement, considering how course-course\ninteractions may positively or negatively affect student success. Our dataset\nincluded 378,000 users and 1,000,000 unique registration events in France\nUniversite Numerique (FUN), a national MOOC platform. We adapt reliability\ntheory to model certificate completion rates with a Weibull survival function,\nfollowing the intuition that students \"survive\" in a course for a certain time\nbefore stochastically dropping out. Course-course interactions are found to be\nwell described by a single parameter for user engagement that can be estimated\nfrom a user's registration profile. User engagement, in turn, correlates with\ncertificate rates in all courses regardless of specific content. The\nreliability approach is shown to capture several certificate rate patterns that\nare overlooked by conventional regression models. User engagement emerges as a\nnatural metric for tracking student progress across demographics and over time.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 14:53:31 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Wintermute", "Edwin H.", ""], ["Cisel", "Matthieu", ""], ["Lindner", "Ariel B.", ""]]}, {"id": "1905.04362", "submitter": "Nuno Martins", "authors": "Varun Jog and Richard J. La and Michael Lin and Nuno C. Martins", "title": "Channels, Remote Estimation and Queueing Systems With A\n  Utilization-Dependent Component: A Unifying Survey Of Recent Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we survey the main models, techniques, concepts, and results\ncentered on the design and performance evaluation of engineered systems that\nrely on a utilization-dependent component (UDC) whose operation may depend on\nits usage history or assigned workload. Specifically, we report on research\nthemes concentrating on the characterization of the capacity of channels and\nthe design with performance guarantees of remote estimation and queueing\nsystems. Causes for the dependency of a UDC on past utilization include the use\nof replenishable energy sources to power the transmission of information among\nthe sub-components of a networked system, and the assistance of a human\noperator for servicing a queue. Our analysis unveils the similarity of the UDC\nmodels typically adopted in each of the research themes, and it reveals the\ndifferences in the objectives and technical approaches employed. We also\nidentify new challenges and future research directions inspired by the\ncross-pollination among the central concepts, techniques, and problem\nformulations of the research themes discussed.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 19:54:54 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 21:12:35 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Jog", "Varun", ""], ["La", "Richard J.", ""], ["Lin", "Michael", ""], ["Martins", "Nuno C.", ""]]}, {"id": "1905.04389", "submitter": "Deborah Kunkel", "authors": "Deborah Kunkel and Mario Peruggia", "title": "Statistical inference with anchored Bayesian mixture of regressions\n  models: A case study analysis of allometric data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study in which we use a mixture of regressions model to\nimprove on an ill-fitting simple linear regression model relating log brain\nmass to log body mass for 100 placental mammalian species. The slope of this\nregression model is of particular scientific interest because it corresponds to\na constant that governs a hypothesized allometric power law relating brain mass\nto body mass. A specific line of investigation is to determine whether the\nregression parameters vary across subgroups of related species.\n  We model these data using an anchored Bayesian mixture of regressions model,\nwhich modifies the standard Bayesian Gaussian mixture by pre-assigning small\nsubsets of observations to given mixture components with probability one. These\nobservations (called anchor points) break the relabeling invariance typical of\nexchangeable model specifications (the so-called label-switching problem). A\ncareful choice of which observations to pre-classify to which mixture\ncomponents is key to the specification of a well-fitting anchor model.\n  In the article we compare three strategies for the selection of anchor\npoints. The first assumes that the underlying mixture of regressions model\nholds and assigns anchor points to different components to maximize the\ninformation about their labeling. The second makes no assumption about the\nrelationship between x and y and instead identifies anchor points using a\nbivariate Gaussian mixture model. The third strategy begins with the assumption\nthat there is only one mixture regression component and identifies anchor\npoints that are representative of a clustering structure based on case-deletion\nimportance sampling weights. We compare the performance of the three strategies\non the allometric data set and use auxiliary taxonomic information about the\nspecies to evaluate the model-based classifications estimated from these\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:06:56 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kunkel", "Deborah", ""], ["Peruggia", "Mario", ""]]}, {"id": "1905.04396", "submitter": "Leying Guan", "authors": "Leying Guan, Rob Tibshirani", "title": "Prediction and outlier detection in classification problems", "comments": "22 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-class classification problem when the training data and\nthe out-of-sample test data may have different distributions and propose a\nmethod called BCOPS (balanced and conformal optimized prediction sets). BCOPS\nconstructs a prediction set $C(x)$ as a subset of class labels, possibly empty.\nIt tries to optimize the out-of-sample performance, aiming to include the\ncorrect class as often as possible, but also detecting outliers $x$, for which\nthe method returns no prediction (corresponding to $C(x)$ equal to the empty\nset). The proposed method combines supervised-learning algorithms with the\nmethod of conformal prediction to minimize a misclassification loss averaged\nover the out-of-sample distribution. The constructed prediction sets have a\nfinite-sample coverage guarantee without distributional assumptions.\n  We also propose a method to estimate the outlier detection rate of a given\nmethod. We prove asymptotic consistency and optimality of our proposals under\nsuitable assumptions and illustrate our methods on real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:56:39 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 05:00:56 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 16:23:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Guan", "Leying", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1905.04444", "submitter": "Andrey Sarantsev Mr", "authors": "Andrey Sarantsev", "title": "Partisan Lean of States: Electoral College and Popular Vote", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare federal election results for each state versus the USA in every\nsecond year from 1992 to 2018, to model partisan lean of each state and its\ndependence on the nationwide popular vote. For each state, we model both its\ncurrent partisan lean and its rate of change, as well as sensitivity of state\nresults with respect to the nationwide popular vote, using Bayesian linear\nregression. We apply this to simulate the Electoral College outcome in 2020,\ngiven even (equal) nationwide popular vote, as well as 2016, 2008, and 2004\nnationwide popular vote. We backtest 2012 and 2016 elections given actual\npopular vote. Taking equal popular vote for two major parties, we prove that\nthe Electoral College is biased towards Republicans.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 04:13:54 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 00:52:02 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 23:06:31 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 00:12:10 GMT"}, {"version": "v5", "created": "Mon, 7 Oct 2019 22:25:15 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sarantsev", "Andrey", ""]]}, {"id": "1905.04488", "submitter": "Marco Helbich", "authors": "Ruoyu Wang, Marco Helbich, Yao Yao, Jinbao Zhang, Penghua Liu, Yuan\n  Yuana, Ye Liu", "title": "Urban greenery and mental wellbeing in adults: Cross-sectional mediation\n  analyses on multiple pathways across different greenery measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple mechanisms have been proposed to explain how greenery enhances their\nmental wellbeing. Mediation studies, however, focus on a limited number of\nmechanisms and rely on remotely sensed greenery measures, which do not\naccurately capture how neighborhood greenery is perceived on the ground. To\nexamine: 1) how streetscape and remote sensing-based greenery affect people's\nmental wellbeing in Guangzhou, China; 2) whether and, if so, to what extent the\nassociations are mediated by physical activity, stress, air quality and noise,\nand social cohesion; and 3) whether differences in the mediation across the\nstreetscape greenery and NDVI exposure metrics occurred. Mental wellbeing was\nquantified by the WHO-5 wellbeing index. Greenery measures were extracted at\nthe neighborhood level: 1) streetscape greenery from street view data via a\nconvolutional neural network, and 2) the NDVI remote sensing images. Single and\nmultiple mediation analyses with multilevel regressions were conducted.\nStreetscape and NDVI greenery were weakly and positively, but not\nsignificantly, correlated. Our regression results revealed that streetscape\ngreenery and NDVI were, individually and jointly, positively associated with\nmental wellbeing. Significant partial mediators for the streetscape greenery\nwere physical activity, stress, air quality and noise, and social cohesion;\ntogether, they explained 62% of the association. For NDVI, only physical\nactivity and social cohesion were significant partial mediators, accounting for\n22% of the association. Mental health and wellbeing and both streetscape and\nsatellite-derived greenery seem to be both directly correlated and indirectly\nmediated. Our findings signify that both greenery measures capture different\naspects of natural environments and may contribute to people's wellbeing by\nmeans of different mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 09:59:51 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Ruoyu", ""], ["Helbich", "Marco", ""], ["Yao", "Yao", ""], ["Zhang", "Jinbao", ""], ["Liu", "Penghua", ""], ["Yuana", "Yuan", ""], ["Liu", "Ye", ""]]}, {"id": "1905.04606", "submitter": "Inder Tecuapetla-G\\'omez", "authors": "Inder Tecuapetla-G\\'omez", "title": "Time delay estimation in satellite imagery time series of precipitation\n  and NDVI: Pearson's cross correlation revisited", "comments": "5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to describe more accurately the time relationships between daily\nsatellite imagery time series of precipitation and NDVI we propose an estimator\nwhich takes into account the sparsity naturally observed in precipitation. We\nconducted a series of simulation studies and show that the proposed estimator's\nvariance is smaller than the canonical's (Pearson-based), in particular, when\nthe signal-to-noise ratio is rather low. Also, the proposed estimator's\nvariance was found smaller than the canonical's one when we applied them to\nstacks of images (2002-2016) taken on some ecological regions of Mexico.\nComputations for this paper are based on functions implemented in our new R\npackage geoTS.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 23:04:40 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Tecuapetla-G\u00f3mez", "Inder", ""]]}, {"id": "1905.04667", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Functional Correlations in the Pursuit of Performance Assessment of\n  Classifiers", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical classification and machine learning, as well as in social and\nother sciences, a number of measures of association have been proposed for\nassessing and comparing individual classifiers, raters, as well as their\ngroups. In this paper, we introduce, justify, and explore several new measures\nof association, which we call CO-, ANTI- and COANTI-correlation coefficients,\nthat we demonstrate to be powerful tools for classifying confusion matrices. We\nillustrate the performance of these new coefficients using a number of\nexamples, from which we also conclude that the coefficients are new objects in\nthe sense that they differ from those already in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 08:43:06 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:18:24 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 08:18:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1905.04758", "submitter": "Arrigo Coen", "authors": "Arrigo Coen", "title": "The compound product distribution; a solution to the distributional\n  equation X=AX+1", "comments": "10 pages, 5 figures, one appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solution of $ X=AX+1 $ is analyzed for a discrete variable $ A $ with $\n\\mathbb{P}\\left[A=0\\right]>0 $. Accordingly, a fast algorithm is presented to\ncalculate the obtained heavy tail density. To exemplify, the compound product\ndistribution is studied in detail for some particular families of\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 18:08:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Coen", "Arrigo", ""]]}, {"id": "1905.05056", "submitter": "Raphael Huser", "authors": "Yan Gong and Rapha\\\"el Huser", "title": "Asymmetric tail dependence modeling, with application to cryptocurrency\n  market data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of Bitcoin in 2008, cryptocurrencies have played an\nincreasing role in the world of e-commerce, but the recent turbulence in the\ncryptocurrency market in 2018 has raised some concerns about their stability\nand associated risks. For investors, it is crucial to uncover the dependence\nrelationships between cryptocurrencies for a more resilient portfolio\ndiversification. Moreover, the stochastic behavior in both tails is important,\nas long positions are sensitive to a decrease in prices (lower tail), while\nshort positions are sensitive to an increase in prices (upper tail). In order\nto assess both risk types, we develop in this paper a flexible copula model\nwhich is able to distinctively capture asymptotic dependence or independence in\nits lower and upper tails simultaneously. Our proposed model is parsimonious\nand smoothly bridges (in each tail) both extremal dependence classes in the\ninterior of the parameter space. Inference is performed using a full or\ncensored likelihood approach, and we investigate by simulation the estimators'\nefficiency under three different censoring schemes which reduce the impact of\nnon-extreme observations. We also develop a local likelihood approach to\ncapture the temporal dynamics of extremal dependence among two leading\ncryptocurrencies. We here apply our model to historical closing prices of five\nleading cryotocurrencies, which share most of the cryptocurrency market\ncapitalizations. The results show that our proposed copula model outperforms\nalternative copula models and that the lower tail dependence level between most\npairs of leading cryptocurrencies -- and in particular Bitcoin and Ethereum --\nhas become stronger over time, smoothly transitioning from an asymptotic\nindependence regime to an asymptotic dependence regime in recent years, whilst\nthe upper tail has been relatively more stable overall at a weaker dependence\nlevel.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:31:01 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 10:37:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gong", "Yan", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1905.05074", "submitter": "Wenqian Wang", "authors": "Wenqian Wang, Beth Andrews", "title": "Partially Specified Space Time Autoregressive Model with Artificial\n  Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.07822", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The space time autoregressive model has been widely applied in science, in\nareas such as economics, public finance, political science, agricultural\neconomics, environmental studies and transportation analyses. The classical\nspace time autoregressive model is a linear model for describing spatial\ncorrelation. In this work, we expand the classical model to include related\nexogenous variables, possibly non-Gaussian, high volatility errors, and a\nnonlinear neural network component. The nonlinear neural network component\nallows for more model flexibility, the ability to learn and model nonlinear and\ncomplex relationships. We use a maximum likelihood approach for model parameter\nestimation. We establish consistency and asymptotic normality for these\nestimators under some standard conditions on the space time model and neural\nnetwork component. We investigate the quality of the asymptotic approximations\nfor finite samples by means of numerical simulation studies. For illustration,\nwe include a real world application.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:05:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Wenqian", ""], ["Andrews", "Beth", ""]]}, {"id": "1905.05145", "submitter": "Arrigo Coen", "authors": "Arrigo Coen, Luis Guti\\'errez and Rams\\'es H. Mena", "title": "Modeling failures times with dependent renewal type models via\n  exchangeability", "comments": "15 pages and 5 figures", "journal-ref": null, "doi": "10.1080/02331888.2019.1618858", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure times of a machinery cannot always be assumed independent and\nidentically distributed, e.g. if after reparations the machinery is not\nrestored to a same-as-new condition. Framed within the renewal processes\napproach, a generalization that considers exchangeable inter-arrival times is\npresented. The resulting model provides a more realistic approach to capture\nthe dependence among events occurring at random times, while retaining much of\nthe tractability of the classical renewal process. Extensions of some classical\nresults and special cases of renewal functions are analyzed, in particular the\none corresponding to an exchangeable sequence driven by a Dirichlet process.\nThe proposal is tested through an estimation procedure using simulated data\nsets and with an application to the reliability of hydraulic subsystems in\nload-haul-dump machines.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 17:01:52 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Coen", "Arrigo", ""], ["Guti\u00e9rrez", "Luis", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "1905.05242", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Xinyi Lu, Perry J. Williams, and Mevin B. Hooten", "title": "Hierarchical approaches for flexible and interpretable binary regression\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary regression models are ubiquitous in virtually every scientific field.\nFrequently, traditional generalized linear models fail to capture the\nvariability in the probability surface that gives rise to the binary\nobservations and novel methodology is required. This has generated a\nsubstantial literature comprised of binary regression models motivated by\nvarious applications. We describe a novel organization of generalizations to\ntraditional binary regression methods based on the familiar three-part\nstructure of generalized linear models (random component, systematic component,\nlink function). This new perspective facilitates both the comparison of\nexisting approaches, and the development of novel, flexible models with\ninterpretable parameters that capture application-specific data generating\nmechanisms. We use our proposed organizational structure to discuss some\nconcerns with certain existing models for binary data based on quantile\nregression. We then use the framework to develop several new binary regression\nmodels tailored to occupancy data for European red squirrels (Sciurus\nvulgaris).\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 18:56:36 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Scharf", "Henry R.", ""], ["Lu", "Xinyi", ""], ["Williams", "Perry J.", ""], ["Hooten", "Mevin B.", ""]]}, {"id": "1905.05337", "submitter": "Brendan McVeigh", "authors": "Brendan S. McVeigh, Bradley T. Spahn, and Jared S. Murray", "title": "Scaling Bayesian Probabilistic Record Linkage with Post-Hoc Blocking: An\n  Application to the California Great Registers", "comments": "42 pages with appendices, 7 figures, 20 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic record linkage (PRL) is the process of determining which\nrecords in two databases correspond to the same underlying entity in the\nabsence of a unique identifier. Bayesian solutions to this problem provide a\npowerful mechanism for propagating uncertainty due to uncertain links between\nrecords (via the posterior distribution). However, computational considerations\nseverely limit the practical applicability of existing Bayesian approaches. We\npropose a new computational approach, providing both a fast algorithm for\nderiving point estimates of the linkage structure that properly account for\none-to-one matching and a restricted MCMC algorithm that samples from an\napproximate posterior distribution. Our advances make it possible to perform\nBayesian PRL for larger problems, and to assess the sensitivity of results to\nvarying prior specifications. We demonstrate the methods on a subset of an\nOCR'd dataset, the California Great Registers, a collection of 57 million voter\nregistrations from 1900 to 1968 that comprise the only panel data set of party\nregistration collected before the advent of scientific surveys.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 01:20:36 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 00:39:15 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["McVeigh", "Brendan S.", ""], ["Spahn", "Bradley T.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.05345", "submitter": "Jan N. Fuhg", "authors": "Jan N. Fuhg", "title": "Adaptive surrogate models for parametric studies", "comments": "225 pages, Master's thesis, Leibniz University of Hannover, Germany\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational effort for the evaluation of numerical simulations based on\ne.g. the finite-element method is high. Metamodels can be utilized to create a\nlow-cost alternative. However the number of required samples for the creation\nof a sufficient metamodel should be kept low, which can be achieved by using\nadaptive sampling techniques. In this Master thesis adaptive sampling\ntechniques are investigated for their use in creating metamodels with the\nKriging technique, which interpolates values by a Gaussian process governed by\nprior covariances. The Kriging framework with extension to multifidelity\nproblems is presented and utilized to compare adaptive sampling techniques\nfound in the literature for benchmark problems as well as applications for\ncontact mechanics. This thesis offers the first comprehensive comparison of a\nlarge spectrum of adaptive techniques for the Kriging framework. Furthermore a\nmultitude of adaptive techniques is introduced to multifidelity Kriging as well\nas well as to a Kriging model with reduced hyperparameter dimension called\npartial least squares Kriging. In addition, an innovative adaptive scheme for\nbinary classification is presented and tested for identifying chaotic motion of\na Duffing's type oscillator.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 11:08:50 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Fuhg", "Jan N.", ""]]}, {"id": "1905.05389", "submitter": "Michael Lingzhi Li", "authors": "Kosuke Imai, Michael Lingzhi Li", "title": "Experimental Evaluation of Individualized Treatment Rules", "comments": "Accepted at JASA", "journal-ref": null, "doi": "10.1080/01621459.2021.1923511", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of individual-level data has led to numerous\napplications of individualized (or personalized) treatment rules (ITRs). Policy\nmakers often wish to empirically evaluate ITRs and compare their relative\nperformance before implementing them in a target population. We propose a new\nevaluation metric, the population average prescriptive effect (PAPE). The PAPE\ncompares the performance of ITR with that of non-individualized treatment rule,\nwhich randomly treats the same proportion of units. Averaging the PAPE over a\nrange of budget constraints yields our second evaluation metric, the area under\nthe prescriptive effect curve (AUPEC). The AUPEC represents an overall\nperformance measure for evaluation, like the area under the receiver and\noperating characteristic curve (AUROC) does for classification, and is a\ngeneralization of the QINI coefficient utilized in uplift modeling. We use\nNeyman's repeated sampling framework to estimate the PAPE and AUPEC and derive\ntheir exact finite-sample variances based on random sampling of units and\nrandom assignment of treatment. We extend our methodology to a common setting,\nin which the same experimental data is used to both estimate and evaluate ITRs.\nIn this case, our variance calculation incorporates the additional uncertainty\ndue to random splits of data used for cross-validation. The proposed evaluation\nmetrics can be estimated without requiring modeling assumptions, asymptotic\napproximation, or resampling methods. As a result, it is applicable to any ITR\nincluding those based on complex machine learning algorithms. The open-source\nsoftware package is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:40:26 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:06:52 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 06:09:48 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 02:01:16 GMT"}, {"version": "v5", "created": "Mon, 25 Jan 2021 19:50:07 GMT"}, {"version": "v6", "created": "Wed, 5 May 2021 17:18:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Imai", "Kosuke", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "1905.05394", "submitter": "Mingyuan Zhou", "authors": "Chaojie Wang, Bo Chen, Sucheng Xiao, Mingyuan Zhou", "title": "Convolutional Poisson Gamma Belief Network", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text analysis, one often resorts to a lossy representation that either\ncompletely ignores word order or embeds each word as a low-dimensional dense\nfeature vector. In this paper, we propose convolutional Poisson factor analysis\n(CPFA) that directly operates on a lossless representation that processes the\nwords in each document as a sequence of high-dimensional one-hot vectors. To\nboost its performance, we further propose the convolutional Poisson gamma\nbelief network (CPGBN) that couples CPFA with the gamma belief network via a\nnovel probabilistic pooling layer. CPFA forms words into phrases and captures\nvery specific phrase-level topics, and CPGBN further builds a hierarchy of\nincreasingly more general phrase-level topics. For efficient inference, we\ndevelop both a Gibbs sampler and a Weibull distribution based convolutional\nvariational auto-encoder. Experimental results demonstrate that CPGBN can\nextract high-quality text latent representations that capture the word order\ninformation, and hence can be leveraged as a building block to enrich a wide\nvariety of existing latent variable models that ignore word order.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 05:08:47 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Wang", "Chaojie", ""], ["Chen", "Bo", ""], ["Xiao", "Sucheng", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.05540", "submitter": "Donya Rahmani", "authors": "Donya Rahmani, Damien Fay and Jacek Brodzki", "title": "A self-organising eigenspace map for time series clustering", "comments": "16 pages-27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel time series clustering method, the\nself-organising eigenspace map (SOEM), based on a generalisation of the\nwell-known self-organising feature map (SOFM). The SOEM operates on the\neigenspaces of the embedded covariance structures of time series which are\nrelated directly to modes in those time series. Approximate joint\ndiagonalisation acts as a pseudo-metric across these spaces allowing us to\ngeneralise the SOFM to a neural network with matrix input. The technique is\nempirically validated against three sets of experiments; univariate and\nmultivariate time series clustering, and application to (clustered)\nmulti-variate time series forecasting. Results indicate that the technique\nperforms a valid topologically ordered clustering of the time series. The\nclustering is superior in comparison to standard benchmarks when the data is\nnon-aligned, gives the best clustering stage for when used in forecasting, and\ncan be used with partial/non-overlapping time series, multivariate clustering\nand produces a topological representation of the time series objects.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 12:09:04 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Rahmani", "Donya", ""], ["Fay", "Damien", ""], ["Brodzki", "Jacek", ""]]}, {"id": "1905.05598", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento, Vera Costa", "title": "Confirmatory Factor Analysis -- A Case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confirmatory Factor Analysis (CFA) is a particular form of factor analysis,\nmost commonly used in social research. In confirmatory factor analysis, the\nresearcher first develops a hypothesis about what factors they believe are\nunderlying the used measures and may impose constraints on the model based on\nthese a priori hypotheses. For example, if two factors are accounting for the\ncovariance in the measures, and these factors are unrelated to one another, we\ncan create a model where the correlation between factor X and factor Y is set\nto zero. Measures could then be obtained to assess how well the fitted model\ncaptured the covariance between all the items or measures in the model. Thus,\nif the results of statistical tests of the model fit indicate a poor fit, the\nmodel will be rejected. If the fit is weak, it may be due to a variety of\nreasons. We propose to introduce state of the art techniques to do CFA in R\nlanguage. Then, we propose to do some examples of CFA with R and some datasets,\nrevealing several scenarios where CFA is relevant.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 19:27:22 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Costa", "Vera", ""]]}, {"id": "1905.05760", "submitter": "Marie B\\\"ohnstedt", "authors": "Marie B\\\"ohnstedt, Hein Putter, Nadine Ouellette, Gerda Claeskens and\n  Jutta Gampe", "title": "Shifting attention to old age: Detecting mortality deceleration using\n  focused model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decrease in the increase in death rates at old ages is a phenomenon that\nhas repeatedly been discussed in demographic research. While mortality\ndeceleration can be explained in the gamma-Gompertz model as an effect of\nselection in heterogeneous populations, this phenomenon can be difficult to\nassess statistically because it relates to the tail of the distribution of the\nages at death. By using a focused information criterion (FIC) for model\nselection, we can directly target model performance at those advanced ages. The\ngamma-Gompertz model is reduced to the competing Gompertz model without\nmortality deceleration if the variance parameter lies on the boundary of the\nparameter space. We develop a new version of the FIC that is adapted to this\nnon-standard condition. In a simulation study, the new FIC is shown to\noutperform other methods in detecting mortality deceleration. The application\nof the FIC to extinct French-Canadian birth cohorts demonstrates that focused\nmodel selection can be used to rebut previous assertions about mortality\ndeceleration.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 12:55:13 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["B\u00f6hnstedt", "Marie", ""], ["Putter", "Hein", ""], ["Ouellette", "Nadine", ""], ["Claeskens", "Gerda", ""], ["Gampe", "Jutta", ""]]}, {"id": "1905.05803", "submitter": "Renee Obringer", "authors": "Renee Obringer and Roshanak Nateghi", "title": "Multivariate Modeling for Sustainable and Resilient Infrastructure\n  Systems and Communities", "comments": "Proceedings of 2019 Institute of Industrial and Systems Engineers\n  annual conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sustainability and resilience of urban systems are multifaceted concepts,\nrequiring information about multiple system attributes to adequately evaluate\nand characterize. However, despite the scientific consensus on the multivariate\nnature of these concepts, many of the existing techniques to model urban\nsustainability and resilience are unidimensional in nature, focusing on a\ncharacterizing a single element of highly interconnected urban systems. We\nchampion a paradigm shift in modeling urban sustainability and resilience,\nusing an integrated approach to simultaneously estimate multiple interconnected\n(correlated) system attributes of sustainability and resilience as a function\nof key environmental factors. We present a novel case study and review a few\nrecent studies to illustrate the applicability and benefits of the multivariate\napproach to modeling urban sustainability and resilience. Our proposed\nframework can be utilized by infrastructure managers, urban planners, and\nresearchers to conceptualize and assess urban sustainability and resilience\nmore holistically, and to better understand the key factors in advancing the\nsustainability and resilience of infrastructure systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 19:20:37 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Obringer", "Renee", ""], ["Nateghi", "Roshanak", ""]]}, {"id": "1905.05820", "submitter": "Yujia Chen", "authors": "Yujia Chen, Yang Lou, Kun Wang, Matthew A. Kupinski, Mark A. Anastasio", "title": "Reconstruction-Aware Imaging System Ranking by use of a Sparsity-Driven\n  Numerical Observer Enabled by Variational Bayesian Inference", "comments": "IEEE transactions on medical imaging (2018)", "journal-ref": null, "doi": "10.1109/TMI.2018.2880870", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely accepted that optimization of imaging system performance should\nbe guided by task-based measures of image quality (IQ). It has been advocated\nthat imaging hardware or data-acquisition designs should be optimized by use of\nan ideal observer (IO) that exploits full statistical knowledge of the\nmeasurement noise and class of objects to be imaged, without consideration of\nthe reconstruction method. In practice, accurate and tractable models of the\ncomplete object statistics are often difficult to determine. Moreover, in\nimaging systems that employ compressive sensing concepts, imaging hardware and\nsparse image reconstruction are innately coupled technologies. In this work, a\nsparsity-driven observer (SDO) that can be employed to optimize hardware by use\nof a stochastic object model describing object sparsity is described and\ninvestigated. The SDO and sparse reconstruction method can therefore be\n\"matched\" in the sense that they both utilize the same statistical information\nregarding the class of objects to be imaged. To efficiently compute the SDO\ntest statistic, computational tools developed recently for variational Bayesian\ninference with sparse linear models are adopted. The use of the SDO to rank\ndata-acquisition designs in a stylized example as motivated by magnetic\nresonance imaging (MRI) is demonstrated. This study reveals that the SDO can\nproduce rankings that are consistent with visual assessments of the\nreconstructed images but different from those produced by use of the\ntraditionally employed Hotelling observer (HO).\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:12:49 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Chen", "Yujia", ""], ["Lou", "Yang", ""], ["Wang", "Kun", ""], ["Kupinski", "Matthew A.", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "1905.05830", "submitter": "Yejin Kim", "authors": "Xiaoqian Jiang, Samden Lhatoo, Guo-Qiang Zhang, Luyao Chen, Yejin Kim", "title": "Combining Representation Learning with Tensor Factorization for Risk\n  Factor Analysis - an application to Epilepsy and Alzheimer's disease", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbi.2020.103462", "report-no": null, "categories": "stat.AP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies consider Alzheimer's disease (AD) a comorbidity of epilepsy,\nbut also recognize epilepsy to occur more frequently in patients with AD than\nthose without. The goal of this paper is to understand the relationship between\nepilepsy and AD by studying causal relations among subgroups of epilepsy\npatients. We develop an approach combining representation learning with tensor\nfactorization to provide an in-depth analysis of the risk factors among\nepilepsy patients for AD. An epilepsy-AD cohort of ~600,000 patients were\nextracted from Cerner Health Facts data (50M patients). Our experimental\nresults not only suggested a causal relationship between epilepsy and later\nonset of AD ( p = 1.92e-51), but also identified five epilepsy subgroups with\ndistinct phenotypic patterns leading to AD. While such findings are\npreliminary, the proposed method combining representation learning with tensor\nfactorization seems to be an effective approach for risk factor analysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:28:06 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Jiang", "Xiaoqian", ""], ["Lhatoo", "Samden", ""], ["Zhang", "Guo-Qiang", ""], ["Chen", "Luyao", ""], ["Kim", "Yejin", ""]]}, {"id": "1905.05835", "submitter": "Pablo Moriano", "authors": "Pablo Moriano, Raquel Hill, L. Jean Camp", "title": "Using Bursty Announcements for Detecting BGP Routing Anomalies", "comments": "16 pages, 13 figures, 4 table", "journal-ref": "Comput. Netw. vol. 188, pp. 107835, 2021", "doi": "10.1016/j.comnet.2021.107835", "report-no": null, "categories": "cs.NI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the robust structure of the Internet, it is still susceptible to\ndisruptive routing updates that prevent network traffic from reaching its\ndestination. Our research shows that BGP announcements that are associated with\ndisruptive updates tend to occur in groups of relatively high frequency,\nfollowed by periods of infrequent activity. We hypothesize that we may use\nthese bursty characteristics to detect anomalous routing incidents. In this\nwork, we use manually verified ground truth metadata and volume of\nannouncements as a baseline measure, and propose a burstiness measure that\ndetects prior anomalous incidents with high recall and better precision than\nthe volume baseline. We quantify the burstiness of inter-arrival times around\nthe date and times of four large-scale incidents: the Indosat hijacking event\nin April 2014, the Telecom Malaysia leak in June 2015, the Bharti Airtel Ltd.\nhijack in November 2015, and the MainOne leak in November 2018; and three\nsmaller scale incidents that led to traffic interception: the Belarusian\ntraffic direction in February 2013, the Icelandic traffic direction in July\n2013, and the Russian telecom that hijacked financial services in April 2017.\nOur method leverages the burstiness of disruptive update messages to detect\nthese incidents. We describe limitations, open challenges, and how this method\ncan be used for routing anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:44:11 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 22:30:59 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Moriano", "Pablo", ""], ["Hill", "Raquel", ""], ["Camp", "L. Jean", ""]]}, {"id": "1905.05938", "submitter": "Rohan Wickramasuriya", "authors": "Rohan Wickramasuriya and Dean Marchiori", "title": "Automated detection of business-relevant outliers in e-commerce\n  conversion rate", "comments": "21 pages, 14 figures, 5 tables Amendment to equation 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate how modern outlier detection methods perform in identifying\noutliers in e-commerce conversion rate data. Based on the limitations\nidentified, we then present a novel method to detect outliers in e-commerce\nconversion rate. This unsupervised method is made more business relevant by\nletting it automatically adjust the sensitivity based on the activity observed\non the e-commerce platform. We call this outlier detection method the fluid\nIQR. Using real e-commerce conversion data acquired from a known store, we\ncompare the performance of the existing and the new outlier detection methods.\nFluid IQR method outperforms the existing outlier detection methods by a large\nmargin when it comes to business-relevance. Furthermore, the fluids IQR method\nis the most robust outlier detection method in the presence of clusters of\nextreme outliers or level shifts. Future research will evaluate how the fluid\nIQR method perform in diverse e-business settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 04:09:03 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 04:59:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wickramasuriya", "Rohan", ""], ["Marchiori", "Dean", ""]]}, {"id": "1905.06004", "submitter": "Qin Wang", "authors": "Qin Wang, Gabriel Michau, Olga Fink", "title": "Domain Adaptive Transfer Learning for Fault Diagnosis", "comments": "Presented at 2019 Prognostics and System Health Management Conference\n  (PHM 2019) in Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to digitization of industrial assets in fleets, the ambitious goal of\ntransferring fault diagnosis models fromone machine to the other has raised\ngreat interest. Solving these domain adaptive transfer learning tasks has the\npotential to save large efforts on manually labeling data and modifying models\nfor new machines in the same fleet. Although data-driven methods have shown\ngreat potential in fault diagnosis applications, their ability to generalize on\nnew machines and new working conditions are limited because of their tendency\nto overfit to the training set in reality. One promising solution to this\nproblem is to use domain adaptation techniques. It aims to improve model\nperformance on the target new machine. Inspired by its successful\nimplementation in computer vision, we introduced Domain-Adversarial Neural\nNetworks (DANN) to our context, along with two other popular methods existing\nin previous fault diagnosis research. We then carefully justify the\napplicability of these methods in realistic fault diagnosis settings, and offer\na unified experimental protocol for a fair comparison between domain adaptation\nmethods for fault diagnosis problems.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 07:48:21 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wang", "Qin", ""], ["Michau", "Gabriel", ""], ["Fink", "Olga", ""]]}, {"id": "1905.06225", "submitter": "Hamid R Noori", "authors": "Farzad Fathizadeh, Ekaterina Mitricheva, Rui Kimura, Nikos Logothetis,\n  Hamid Reza Noori", "title": "Signal detection in extracellular neural ensemble recordings using\n  higher criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information processing in the brain is conducted by a concerted action of\nmultiple neural populations. Gaining insights in the organization and dynamics\nof such populations can best be studied with broadband intracranial recordings\nof so-called extracellular field potential, reflecting neuronal spiking as well\nas mesoscopic activities, such as waves, oscillations, intrinsic large\ndeflections, and multiunit spiking activity. Such signals are critical for our\nunderstanding of how neuronal ensembles encode sensory information and how such\ninformation is integrated in the large networks underlying cognition. The\naforementioned principles are now well accepted, yet the efficacy of extracting\ninformation out of the complex neural data, and their employment for improving\nour understanding of neural networks, critically depends on the mathematical\nprocessing steps ranging from simple detection of action potentials in noisy\ntraces - to fitting advanced mathematical models to distinct patterns of the\nneural signal potentially underlying intra-processing of information, e.g.\ninterneuronal interactions. Here, we present a robust strategy for detecting\nsignals in broadband and noisy time series such as spikes, sharp waves and\nmulti-unit activity data that is solely based on the intrinsic statistical\ndistribution of the recorded data. By using so-called higher criticism - a\nsecond-level significance testing procedure comparing the fraction of observed\nsignificances to an expected fraction under the global null - we are able to\ndetect small signals in correlated noisy time-series without prior filtering,\ndenoising or data regression. Results demonstrate the efficiency and\nreliability of the method and versatility over a wide range of experimental\nconditions and suggest the appropriateness of higher criticism to characterize\nneuronal dynamics without prior manipulation of the data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:58:03 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Fathizadeh", "Farzad", ""], ["Mitricheva", "Ekaterina", ""], ["Kimura", "Rui", ""], ["Logothetis", "Nikos", ""], ["Noori", "Hamid Reza", ""]]}, {"id": "1905.06306", "submitter": "Sumanta Kumar Das Dr", "authors": "Sumanta Kumar Das and Randhir Singh", "title": "A multiple-frame approach of crop yield estimation from satellite\n  remotely sensed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have recently explored the information from the\nsatellite-remotely sensed data (SRSD) for estimating the crop production\nstatistics. The value of this information depends on the aerial and spatial\nresolutions of SRSD. The SRSD with fine spatial resolution is costly and the\naerial coverage is less. Use of multiple frames of SRSD in the estimation\nprocess of crop production can increase the precision. We propose an estimator\nfor the average yield of wheat for the state of Haryana, India. This estimator\nuses the information from the Wide Field Sensor (WiFS) and the Linear Imaging\nSelf Scanner (LISS-III) data from the Indian Remote Sensing satellite (IRS-1D)\nand the crop cutting experiment data collected by probability sampling design\nfrom a list frame of villages. We find that the relative efficiencies of the\nmultiple-frame estimators are high in comparison to the single frame\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 11:57:48 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Das", "Sumanta Kumar", ""], ["Singh", "Randhir", ""]]}, {"id": "1905.06310", "submitter": "Vinny Davies", "authors": "Vinny Davies, Umberto No\\`e, Alan Lazarus, Hao Gao, Benn Macdonald,\n  Colin Berry, Xiaoyu Luo, Dirk Husmeier", "title": "Fast Parameter Inference in a Biomechanical Model of the Left Ventricle\n  using Statistical Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in biomechanical studies of personalised human left\nventricular (LV) modelling is estimating the material properties and\nbiophysical parameters from in-vivo clinical measurements in a time frame\nsuitable for use within a clinic. Understanding these properties can provide\ninsight into heart function or dysfunction and help inform personalised\nmedicine. However, finding a solution to the differential equations which\nmathematically describe the kinematics and dynamics of the myocardium through\nnumerical integration can be computationally expensive. To circumvent this\nissue, we use the concept of emulation to infer the myocardial properties of a\nhealthy volunteer in a viable clinical time frame using in-vivo magnetic\nresonance image (MRI) data. Emulation methods avoid computationally expensive\nsimulations from the LV model by replacing the biomechanical model, which is\ndefined in terms of explicit partial differential equations, with a surrogate\nmodel inferred from simulations generated before the arrival of a patient,\nvastly improving computational efficiency at the clinic. We compare and\ncontrast two emulation strategies: (i) emulation of the computational model\noutputs and (ii) emulation of the loss between the observed patient data and\nthe computational model outputs. These strategies are tested with two different\ninterpolation methods, as well as two different loss functions...\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:10:44 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Davies", "Vinny", ""], ["No\u00e8", "Umberto", ""], ["Lazarus", "Alan", ""], ["Gao", "Hao", ""], ["Macdonald", "Benn", ""], ["Berry", "Colin", ""], ["Luo", "Xiaoyu", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1905.06411", "submitter": "Arrigo Coen", "authors": "Arrigo Coen and Beatriz God\\'inez-Chaparro", "title": "Compound Dirichlet Processes", "comments": "11 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compound Poisson process and the Dirichlet process are the pillar\nstructures of Renewal theory and Bayesian nonparametric theory, respectively.\nBoth processes have many useful extensions to fulfill the practitioners needs\nto model the particularities of data structures. Accordingly, in this\ncontribution, we joined their primal ideas to construct the compound Dirichlet\nprocess and the compound Dirichlet process mixture. As a consequence, these new\nprocesses had a fruitful structure to model the time occurrence among events,\nwith also a flexible structure on the arrival variables. These models have a\ndirect Bayesian interpretation of their posterior estimators and are easy to\nimplement. We obtain expressions of the posterior distribution, nonconditional\ndistribution and expected values. In particular to find these formulas we\nanalyze sums of random variables with Dirichlet process priors. We assessed our\napproach by applying our model on a real data example of a contagious zoonotic\ndisease.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:54:42 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Coen", "Arrigo", ""], ["God\u00ednez-Chaparro", "Beatriz", ""]]}, {"id": "1905.06463", "submitter": "Alimire Nabijiang", "authors": "Alimire Nabijiang, Supratik Mukhopadhyay, Yimin Zhu, Ravindra\n  Gudishala, Sanaz Saeidi, Qun Liu", "title": "Why do you take that route?", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to determine whether a particular context factor\namong the variables that a researcher is interested in causally affects the\nroute choice behavior of drivers. To our knowledge, there is limited literature\nthat consider the effects of various factors on route choice based on causal\ninference.Yet, collecting data sets that are sensitive to the aforementioned\nfactors are challenging and the existing approaches usually take into account\nonly the general factors motivating drivers route choice behavior. To fill\nthese gaps, we carried out a study using Immersive Virtual Environment (IVE)\ntools to elicit drivers' route choice behavioral data, covering drivers'\nnetwork familiarity, educationlevel, financial concern, etc, apart from\nconventional measurement variables. Having context-aware, high-fidelity\nproperties, IVE data affords the opportunity to incorporate the impacts of\nhuman related factors into the route choice causal analysis and advance a more\ncustomizable research tool for investigating causal factors on path selection\nin network routing. This causal analysis provides quantitative evidence to\nsupport drivers' diversion decision.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 22:57:51 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Nabijiang", "Alimire", ""], ["Mukhopadhyay", "Supratik", ""], ["Zhu", "Yimin", ""], ["Gudishala", "Ravindra", ""], ["Saeidi", "Sanaz", ""], ["Liu", "Qun", ""]]}, {"id": "1905.06467", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Christian Bressen Pipper (Section of\n  Biostatistics, Department of Public Health, University of Copenhagen)", "title": "Moment-based Estimation of Mixtures of Regression Models", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models provide a flexible modeling framework\nfor many phenomena. Using moment-based estimation of the regression parameters,\nwe develop unbiased estimators with a minimum of assumptions on the mixture\ncomponents. In particular, only the average regression model for one of the\ncomponents in the mixture model is needed and no requirements on the\ndistributions. The consistency and asymptotic distribution of the estimators is\nderived and the proposed method is validated through a series of simulation\nstudies and is shown to be highly accurate. We illustrate the use of the\nmoment-based mixture of regression models with an application to wine quality\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 23:11:48 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"], ["Pipper", "Christian Bressen", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"]]}, {"id": "1905.06873", "submitter": "Beno\\^it Choffin", "authors": "Beno\\^it Choffin, Fabrice Popineau, Yolaine Bourda and Jill-J\\^enn Vie", "title": "DAS3H: Modeling Student Learning and Forgetting for Optimally Scheduling\n  Distributed Practice of Skills", "comments": "10 pages, 1 figure, 6 tables, to appear at the 12th International\n  Conference on Educational Data Mining (EDM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spaced repetition is among the most studied learning strategies in the\ncognitive science literature. It consists in temporally distributing exposure\nto an information so as to improve long-term memorization. Providing students\nwith an adaptive and personalized distributed practice schedule would benefit\nmore than just a generic scheduler. However, the applicability of such adaptive\nschedulers seems to be limited to pure memorization, e.g. flashcards or foreign\nlanguage learning. In this article, we first frame the research problem of\noptimizing an adaptive and personalized spaced repetition scheduler when\nmemorization concerns the application of underlying multiple skills. To this\nend, we choose to rely on a student model for inferring knowledge state and\nmemory dynamics on any skill or combination of skills. We argue that no\nknowledge tracing model takes both memory decay and multiple skill tagging into\naccount for predicting student performance. As a consequence, we propose a new\nstudent learning and forgetting model suited to our research problem: DAS3H\nbuilds on the additive factor models and includes a representation of the\ntemporal distribution of past practice on the skills involved by an item. In\nparticular, DAS3H allows the learning and forgetting curves to differ from one\nskill to another. Finally, we provide empirical evidence on three real-world\neducational datasets that DAS3H outperforms other state-of-the-art EDM models.\nThese results suggest that incorporating both item-skill relationships and\nforgetting effect improves over student models that consider one or the other.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 16:41:03 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Choffin", "Beno\u00eet", ""], ["Popineau", "Fabrice", ""], ["Bourda", "Yolaine", ""], ["Vie", "Jill-J\u00eann", ""]]}, {"id": "1905.06978", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis", "title": "Randomized Algorithms for Data-Driven Stabilization of Stochastic Linear\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven control strategies for dynamical systems with unknown parameters\nare popular in theory and applications. An essential problem is to prevent\nstochastic linear systems becoming destabilized, due to the uncertainty of the\ndecision-maker about the dynamical parameter. Two randomized algorithms are\nproposed for this problem, but the performance is not sufficiently\ninvestigated. Further, the effect of key parameters of the algorithms such as\nthe magnitude and the frequency of applying the randomizations is not currently\navailable. This work studies the stabilization speed and the failure\nprobability of data-driven procedures. We provide numerical analyses for the\nperformance of two methods: stochastic feedback, and stochastic parameter. The\npresented results imply that as long as the number of statistically independent\nrandomizations is not too small, fast stabilization is guaranteed.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 18:13:07 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1905.07172", "submitter": "Sara Wade", "authors": "Sara Wade, Raffaella Piccarreta, Andrea Cremaschi, Isadora\n  Antoniano-Villalobos", "title": "Colombian Women's Life Patterns: A Multivariate Density Regression\n  Approach", "comments": "to appear in Bayesian analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Women in Colombia face difficulties related to the patriarchal traits of\ntheir societies and well-known conflict afflicting the country since 1948. In\nthis critical context, our aim is to study the relationship between baseline\nsocio-demographic factors and variables associated to fertility, partnership\npatterns, and work activity. To best exploit the explanatory structure, we\npropose a Bayesian multivariate density regression model, which can accommodate\nmixed responses with censored, constrained, and binary traits. The flexible\nnature of the models allows for nonlinear regression functions and non-standard\nfeatures in the errors, such as asymmetry or multi-modality. The model has\ninterpretable covariate-dependent weights constructed through normalization,\nallowing for combinations of categorical and continuous covariates.\nComputational difficulties for inference are overcome through an adaptive\ntruncation algorithm combining adaptive Metropolis-Hastings and sequential\nMonte Carlo to create a sequence of automatically truncated posterior mixtures.\nFor our study on Colombian women's life patterns, a variety of quantities are\nvisualised and described, and in particular, our findings highlight the\ndetrimental impact of family violence on women's choices and behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 09:03:59 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 11:12:23 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 15:21:14 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 09:18:16 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wade", "Sara", ""], ["Piccarreta", "Raffaella", ""], ["Cremaschi", "Andrea", ""], ["Antoniano-Villalobos", "Isadora", ""]]}, {"id": "1905.07194", "submitter": "Anastasios Papanikos Tasos", "authors": "Tasos Papanikos, John Thompson, Keith Abrams, Nicolas Staedler, Oriana\n  Ciani, Rod Taylor and Sylwia Bujkiewicz", "title": "A Bayesian hierarchical meta-analytic method for modelling surrogate\n  relationships that vary across treatment classes using aggregate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate endpoints play an important role in drug development when they can\nbe used to measure treatment effect early compared to the final clinical\noutcome and to predict clinical benefit or harm. Such endpoints are assessed\nfor their predictive value of clinical benefit by investigating the surrogate\nrelationship between treatment effects on the surrogate and final outcomes\nusing meta-analytic methods. When surrogate relationships vary across treatment\nclasses, such validation may fail due to limited data within each treatment\nclass. In this paper, two alternative Bayesian meta-analytic methods are\nintroduced which allow for borrowing of information from other treatment\nclasses when exploring the surrogacy in a particular class. The first approach\nextends a standard model for the evaluation of surrogate endpoints to a\nhierarchical meta-analysis model assuming full exchangeability of surrogate\nrelationships across all the treatment classes, thus facilitating borrowing of\ninformation across the classes. The second method is able to relax this\nassumption by allowing for partial exchangeability of surrogate relationships\nacross treatment classes to avoid excessive borrowing of information from\ndistinctly different classes. We carried out a simulation study to assess the\nproposed methods in nine data scenarios and compared them with subgroup\nanalysis using the standard model within each treatment class. We also applied\nthe methods to an illustrative example in colorectal cancer which led to\nobtaining the parameters describing the surrogate relationships with higher\nprecision.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 10:58:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 11:22:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Papanikos", "Tasos", ""], ["Thompson", "John", ""], ["Abrams", "Keith", ""], ["Staedler", "Nicolas", ""], ["Ciani", "Oriana", ""], ["Taylor", "Rod", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "1905.07456", "submitter": "Xiao Wu", "authors": "Xiao Wu, Yi Xu, Bradley P. Carlin", "title": "Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In developing products for rare diseases, statistical challenges arise due to\nthe limited number of patients available for participation in drug trials and\nother clinical research. Bayesian adaptive clinical trial designs offer the\npossibility of increased statistical efficiency, reduced development cost and\nethical hazard prevention via their incorporation of evidence from external\nsources (historical data, expert opinions, and real-world evidence), and\nflexibility in the specification of interim looks. In this paper, we propose a\nnovel Bayesian adaptive commensurate design that borrows adaptively from\nhistorical information and also uses a particular payoff function to optimize\nthe timing of the study's interim analysis. The trial payoff is a function of\nhow many samples can be saved via early stopping and the probability of making\ncorrect early decisions for either futility or efficacy. We calibrate our\nBayesian algorithm to have acceptable long-run frequentist properties (Type I\nerror and power) via simulation at the design stage. We illustrate our approach\nusing a pediatric trial design setting testing the effect of a new drug for a\nrare genetic disease. The optimIA R package available at\nhttps://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use\nimplementation of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:05:29 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 04:23:06 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wu", "Xiao", ""], ["Xu", "Yi", ""], ["Carlin", "Bradley P.", ""]]}, {"id": "1905.07502", "submitter": "Benjamin Risk", "authors": "Benjamin B. Risk and Hongtu Zhu", "title": "ACE of Space: Estimating Genetic Components of High-Dimensional Imaging\n  Data", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxz022", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of great interest to quantify the contributions of genetic variation to\nbrain structure and function, which are usually measured by high-dimensional\nimaging data (e.g., magnetic resonance imaging). In addition to the variance,\nthe covariance patterns in the genetic effects of a functional phenotype are of\nbiological importance, and covariance patterns have been linked to psychiatric\ndisorders. The aim of this paper is to develop a scalable method to estimate\nheritability and the non-stationary covariance components in high-dimensional\nimaging data from twin studies. Our motivating example is from the Human\nConnectome Project (HCP). Several major big-data challenges arise from\nestimating the genetic and environmental covariance functions of functional\nphenotypes extracted from imaging data, such as cortical thickness with 60,000\nvertices. Notably, truncating to positive eigenvalues and their eigenfunctions\nfrom unconstrained estimators can result in large bias. This motivated our\ndevelopment of a novel estimator ensuring positive semidefiniteness. Simulation\nstudies demonstrate large improvements over existing approaches, both with\nrespect to heritability estimates and covariance estimation. We applied the\nproposed method to cortical thickness data from the HCP. Our analysis suggests\nfine-scale differences in covariance patterns, identifying locations in which\ngenetic control is correlated with large areas of the brain and locations where\nit is highly localized.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 23:44:38 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Risk", "Benjamin B.", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1905.07755", "submitter": "Mason A. Porter", "authors": "Joseph H. Tien, Marisa C. Eisenberg, Sarah T. Cherng, Mason A. Porter", "title": "Online reactions to the 2017 'Unite the Right' rally in Charlottesville:\n  Measuring polarization in Twitter networks using media followership", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI nlin.AO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Twitter conversation following the August 2017 `Unite the Right'\nrally in Charlottesville, Virginia, using tools from network analysis and data\nscience. We use media followership on Twitter and principal component analysis\n(PCA) to compute a `Left'/`Right' media score on a one-dimensional axis to\ncharacterize nodes. We then use these scores, in concert with retweet\nrelationships, to examine the structure of a retweet network of approximately\n300,000 accounts that communicated with the #Charlottesville hashtag. The\nretweet network is sharply polarized, with an assortativity coefficient of 0.8\nwith respect to the sign of the media PCA score. Community detection using two\napproaches, a Louvain method and InfoMap, yields largely homogeneous\ncommunities in terms of Left/Right node composition. When comparing tweet\ncontent, we find that tweets about `Trump' were widespread in both the Left and\nRight, though the accompanying language was unsurprisingly different. Nodes\nwith large degrees in communities on the Left include accounts that are\nassociated with disparate areas, including activism, business, arts and\nentertainment, media, and politics. Support of Donald Trump was a common thread\namong the Right communities, connecting communities with accounts that\nreference white-supremacist hate symbols, communities with influential\npersonalities in the alt-right, and the largest Right community (which includes\nthe Twitter account FoxNews).\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 15:14:19 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 02:35:40 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Tien", "Joseph H.", ""], ["Eisenberg", "Marisa C.", ""], ["Cherng", "Sarah T.", ""], ["Porter", "Mason A.", ""]]}, {"id": "1905.07764", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh, Sebastien J-P.A. Haneuse, James M. Robins, Sarah E.\n  Robertson, Ashley L. Buchanan, Elisabeth A. Stuart, Miguel A. Hern\\'an", "title": "Study designs for extending causal inferences from a randomized trial to\n  a target population", "comments": "first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine study designs for extending (generalizing or transporting) causal\ninferences from a randomized trial to a target population. Specifically, we\nconsider nested trial designs, where randomized individuals are nested within a\nsample from the target population, and non-nested trial designs, including\ncomposite dataset designs, where a randomized trial is combined with a\nseparately obtained sample of non-randomized individuals from the target\npopulation. We show that the causal quantities that can be identified in each\nstudy design depend on what is known about the probability of sampling\nnon-randomized individuals. For each study design, we examine identification of\npotential outcome means via the g-formula and inverse probability weighting.\nLast, we explore the implications of the sampling properties underlying the\ndesigns for the identification and estimation of the probability of trial\nparticipation.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:15:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Robins", "James M.", ""], ["Robertson", "Sarah E.", ""], ["Buchanan", "Ashley L.", ""], ["Stuart", "Elisabeth A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1905.07771", "submitter": "Martina Han\\v{c}ov\\'a", "authors": "Martina Han\\v{c}ov\\'a, Gabriela Voz\\'arikov\\'a, Andrej Gajdo\\v{s},\n  Jozef Han\\v{c}", "title": "Estimating variances in time series linear regression models using\n  empirical BLUPs and convex optimization", "comments": "29 pages, 1 figure, 5 tables", "journal-ref": "Statistical Papers 2020", "doi": "10.1007/s00362-020-01165-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage estimation method of variance components in time\nseries models known as FDSLRMs, whose observations can be described by a linear\nmixed model (LMM). We based estimating variances, fundamental quantities in a\ntime series forecasting approach called kriging, on the empirical (plug-in)\nbest linear unbiased predictions of unobservable random components in FDSLRM.\n  The method, providing invariant non-negative quadratic estimators, can be\nused for any absolutely continuous probability distribution of time series\ndata. As a result of applying the convex optimization and the LMM methodology,\nwe resolved two problems $-$ theoretical existence and equivalence between\nleast squares estimators, non-negative (M)DOOLSE, and maximum likelihood\nestimators, (RE)MLE, as possible starting points of our method and a practical\nlack of computational implementation for FDSLRM. As for computing (RE)MLE in\nthe case of $ n $ observed time series values, we also discovered a new\nalgorithm of order $\\mathcal{O}(n)$, which at the default precision is $10^7$\ntimes more accurate and $n^2$ times faster than the best current Python(or\nR)-based computational packages, namely CVXPY, CVXR, nlme, sommer and mixed.\n  We illustrate our results on three real data sets $-$ electricity\nconsumption, tourism and cyber security $-$ which are easily available,\nreproducible, sharable and modifiable in the form of interactive Jupyter\nnotebooks.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:46:55 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Han\u010dov\u00e1", "Martina", ""], ["Voz\u00e1rikov\u00e1", "Gabriela", ""], ["Gajdo\u0161", "Andrej", ""], ["Han\u010d", "Jozef", ""]]}, {"id": "1905.07776", "submitter": "Sagar Kumar Tamang", "authors": "Sagar K. Tamang, Ardeshir M. Ebtehaj, Andreas F. Prein and Andrew J.\n  Heymsfield", "title": "On Changes of Global Wet-bulb Temperature and Snowfall Regimes", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To properly interpret the observed shrinkage of the Earth's cryosphere it is\nimportant to understand global changes of snowfall dominant regimes. To\ndocument these changes, three different reanalysis products of wet-bulb\ntemperature together with observationally-based data sets are processed from\n1979 to 2017. It is found that over the Northern Hemisphere (NH), the annual\nmean wet-bulb temperature has increased at a rate of 0.34$^\\circ$C per decade\n(pd) over land and 0.35$^\\circ$C pd over ocean, resulting in a reduction of the\nannual mean potential areas of snowfall dominant regimes by 0.52/0.34 million\nkm$^2$pd over land/ocean. However, the changes in the Southern Hemisphere (SH)\nare less conclusive and more uncertain. Among the K$\\\"o$ppen-Geiger climate\nclasses, the highest warming trend is observed over the NH polar climate\nregimes. Over studied mountain regions, the Alps are warming at a faster rate\ncompared to the Rockies, Andes and High Mountain Asia (HMA). Due to such\nwarming, potential snowfall areas over the Alps is reducing at 3.64% pd\nfollowed by Rockies at 2.81 and HMA at 1.85% pd. On average, these mountain\nranges have lost 0.02 million km$^2$pd of potential snowfall areas. The NH\npotential snowfall areas is retracting towards the North pole over the Central\nAsia and Europe at a rate of 0.45 and 0.7 degree pd. Furthermore, terrestrial\nregions over the NH including the Great Plains in the United States, Canadian\nprovinces around the Hudson Bay, Central Siberian and Tibetan Plateaus, are\nlosing as much as 4% of the solid proportion of the annual precipitation amount\npd.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 17:09:45 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Tamang", "Sagar K.", ""], ["Ebtehaj", "Ardeshir M.", ""], ["Prein", "Andreas F.", ""], ["Heymsfield", "Andrew J.", ""]]}, {"id": "1905.07859", "submitter": "Evgeny Burnaev", "authors": "Oleg Sudakov and Dmitri Koroteev and Boris Belozerov and Evgeny\n  Burnaev", "title": "Artificial Neural Network Surrogate Modeling of Oil Reservoir: a Case\n  Study", "comments": "10 pages, 5 figures", "journal-ref": "16th International Symposium on Neural Networks, ISNN 2019", "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a data-driven model, introducing recent advances in machine\nlearning to reservoir simulation. We use a conventional reservoir modeling tool\nto generate training set and a special ensemble of artificial neural networks\n(ANNs) to build a predictive model. The ANN-based model allows to reproduce the\ntime dependence of fluids and pressure distribution within the computational\ncells of the reservoir model. We compare the performance of the ANN-based model\nwith conventional reservoir modeling and illustrate that ANN-based model (1) is\nable to capture all the output parameters of the conventional model with very\nhigh accuracy and (2) demonstrate much higher computational performance. We\nfinally elaborate on further options for research and developments within the\narea of reservoir modeling.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 03:34:47 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sudakov", "Oleg", ""], ["Koroteev", "Dmitri", ""], ["Belozerov", "Boris", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1905.07886", "submitter": "Christopher Kath", "authors": "Christopher Kath and Florian Ziel", "title": "Conformal Prediction Interval Estimations with an Application to\n  Day-Ahead and Intraday Power Markets", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2020.09.006", "report-no": null, "categories": "econ.EM q-fin.PM q-fin.TR stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We discuss a concept denoted as Conformal Prediction (CP) in this paper.\nWhile initially stemming from the world of machine learning, it was never\napplied or analyzed in the context of short-term electricity price forecasting.\nTherefore, we elaborate the aspects that render Conformal Prediction worthwhile\nto know and explain why its simple yet very efficient idea has worked in other\nfields of application and why its characteristics are promising for short-term\npower applications as well. We compare its performance with different\nstate-of-the-art electricity price forecasting models such as quantile\nregression averaging (QRA) in an empirical out-of-sample study for three\nshort-term electricity time series. We combine Conformal Prediction with\nvarious underlying point forecast models to demonstrate its versatility and\nbehavior under changing conditions. Our findings suggest that Conformal\nPrediction yields sharp and reliable prediction intervals in short-term power\nmarkets. We further inspect the effect each of Conformal Prediction's model\ncomponents has and provide a path-based guideline on how to find the best CP\nmodel for each market.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 05:46:03 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 16:40:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kath", "Christopher", ""], ["Ziel", "Florian", ""]]}, {"id": "1905.07902", "submitter": "Evgeny Burnaev", "authors": "Rodrigo Rivera-Castro and Ivan Nazarov and Yuke Xiang and Alexander\n  Pletneev and Ivan Maksimov and Evgeny Burnaev", "title": "Demand forecasting techniques for build-to-order lean manufacturing\n  supply chains", "comments": "10 pages, 2 figures", "journal-ref": "16th International Symposium on Neural Networks, ISNN 2019", "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Build-to-order (BTO) supply chains have become common-place in industries\nsuch as electronics, automotive and fashion. They enable building products\nbased on individual requirements with a short lead time and minimum inventory\nand production costs. Due to their nature, they differ significantly from\ntraditional supply chains. However, there have not been studies dedicated to\ndemand forecasting methods for this type of setting. This work makes two\ncontributions. First, it presents a new and unique data set from a manufacturer\nin the BTO sector. Second, it proposes a novel data transformation technique\nfor demand forecasting of BTO products. Results from thirteen forecasting\nmethods show that the approach compares well to the state-of-the-art while\nbeing easy to implement and to explain to decision-makers.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 06:33:53 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Rivera-Castro", "Rodrigo", ""], ["Nazarov", "Ivan", ""], ["Xiang", "Yuke", ""], ["Pletneev", "Alexander", ""], ["Maksimov", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1905.07907", "submitter": "Christopher Albert", "authors": "Christopher Albert", "title": "Physics-informed transfer path analysis with parameter estimation using\n  Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.class-ph physics.comp-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gaussian processes regression is applied to augment experimental data of\ntransfer-path analysis (TPA) by known information about the underlying physical\nproperties of the system under investigation. The approach can be used as an\nalternative to model updating and is also applicable if no detailed simulation\nmodel of the system exists. For vibro-acoustic systems at least three features\nare known. Firstly, observable quantities fulfill a wave equation or a\nHelmholtz-like equation in the frequency domain. Secondly, the relation between\npressure/stress and displacement/velocity/acceleration are known via\nconstitutive relations involving mass density and elastic constants of the\nmaterial. The latter also determine the propagation speed of waves. Thirdly,\nthe geometry of the system is often known up to a certain accuracy. Here it is\ndemonstrated that taking into account this information can potentially enhance\nTPA results and quantify their uncertainties at the same time. In particular\nthis is the case for noisy measurement data and if material parameters and\nsource distributions are (partly) unknown. Due to the probabilistic nature of\nthe procedure unknown parameters can be estimated, making the method also\napplicable to material characterization as an inverse problem.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 06:45:43 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Albert", "Christopher", ""]]}, {"id": "1905.07912", "submitter": "Veronique Maume-Deschamps", "authors": "Abdul-Fattah Abu-Awwad (ICJ, PSPM), V\\'eronique Maume-Deschamps (ICJ,\n  PSPM), Pierre Ribereau (PSPM, ICJ)", "title": "Semiparametric estimation for space-time max-stable processes: F\n  -madogram-based estimation approach", "comments": "arXiv admin note: text overlap with arXiv:1507.07750 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes have been expanded to quantify extremal dependence in\nspatio-temporal data. Due to the interaction between space and time,\nspatio-temporal data are often complex to analyze. So, characterizing these\ndependencies is one of the crucial challenges in this field of statistics. This\npaper suggests a semiparametric inference methodology based on the\nspatio-temporal F-madogram for estimating the parameters of a space-time\nmax-stable process using gridded data. The performance of the method is\ninvestigated through various simulation studies. Finally, we apply our\ninferential procedure to quantify the extremal behavior of radar rainfall data\nin a region in the State of Florida.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:04:38 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Abu-Awwad", "Abdul-Fattah", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ,\n  PSPM"], ["Ribereau", "Pierre", "", "PSPM, ICJ"]]}, {"id": "1905.08022", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Andreas Wieser", "title": "An iterative scheme for feature based positioning using a weighted\n  dissimilarity measure", "comments": "18 pages, 9 figures, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an iterative scheme for feature-based positioning using a new\nweighted dissimilarity measure with the goal of reducing the impact of large\nerrors among the measured or modeled features. The weights are computed from\nthe location-dependent standard deviations of the features and stored as part\nof the reference fingerprint map (RFM). Spatial filtering and kernel smoothing\nof the kinematically collected raw data allow efficiently estimating the\nstandard deviations during RFM generation. In the positioning stage, the\nweights control the contribution of each feature to the dissimilarity measure,\nwhich in turn quantifies the difference between the set of online measured\nfeatures and the fingerprints stored in the RFM. Features with little\nvariability contribute more to the estimated position than features with high\nvariability. Iterations are necessary because the variability depends on the\nlocation, and the location is initially unknown when estimating the position.\nUsing real WiFi signal strength data from extended test measurements with\nground truth in an office building, we show that the standard deviations of\nthese features vary considerably within the region of interest and are neither\nsimple functions of the signal strength nor of the distances from the\ncorresponding access points. This is the motivation to include the empirical\nstandard deviations in the RFM. We then analyze the deviations of the estimated\npositions with and without the location-dependent weighting. In the present\nexample the maximum radial positioning error from ground truth are reduced by\n40% comparing to kNN without the weighted dissimilarity measure.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 12:12:38 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 14:56:24 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1905.08038", "submitter": "Dan Lin", "authors": "Jiajing Wu, Dan Lin, Zibin Zheng, Qi Yuan", "title": "T-EDGE: Temporal WEighted MultiDiGraph Embedding for Ethereum\n  Transaction Network Analysis", "comments": "12 pages", "journal-ref": "Front. Phys. 8:204 (2020)", "doi": "10.3389/fphy.2020.00204", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph embedding techniques have been widely used in the analysis of\nvarious networks, but most of the existing embedding methods omit the network\ndynamics and the multiplicity of edges, so it is difficult to accurately\ndescribe the detailed characteristics of the transaction networks. Ethereum is\na blockchain-based platform supporting smart contracts. The open nature of\nblockchain makes the transaction data on Ethereum completely public, and also\nbrings unprecedented opportunities for the transaction network analysis. By\ntaking the realistic rules and features of transaction networks into\nconsideration, we first model the Ethereum transaction network as a Temporal\nWeighted Multidigraph (TWMDG), where each node is a unique Ethereum account and\neach edge represents a transaction weighted by amount and assigned with\ntimestamp. Then we define the problem of Temporal Weighted Multidigraph\nEmbedding (T-EDGE) by incorporating both temporal and weighted information of\nthe edges, the purpose being to capture more comprehensive properties of\ndynamic transaction networks. To evaluate the effectiveness of the proposed\nembedding method, we conduct experiments of node classification on real-world\ntransaction data collected from Ethereum. Experimental results demonstrate that\nT-EDGE outperforms baseline embedding methods, indicating that time-dependent\nwalks and multiplicity characteristic of edges are informative and essential\nfor time-sensitive transaction networks.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 13:59:34 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:09:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wu", "Jiajing", ""], ["Lin", "Dan", ""], ["Zheng", "Zibin", ""], ["Yuan", "Qi", ""]]}, {"id": "1905.08122", "submitter": "Konul Mustafayeva", "authors": "Konul Mustafayeva and Weining Wang", "title": "Non-Parametric Estimation of Spot Covariance Matrix with High-Frequency\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating spot covariance is an important issue to study, especially with\nthe increasing availability of high-frequency financial data. We study the\nestimation of spot covariance using a kernel method for high-frequency data. In\nparticular, we consider first the kernel weighted version of realized\ncovariance estimator for the price process governed by a continuous\nmultivariate semimartingale. Next, we extend it to the threshold kernel\nestimator of the spot covariances when the underlying price process is a\ndiscontinuous multivariate semimartingale with finite activity jumps. We derive\nthe asymptotic distribution of the estimators for both fixed and shrinking\nbandwidth. The estimator in a setting with jumps has the same rate of\nconvergence as the estimator for diffusion processes without jumps. A\nsimulation study examines the finite sample properties of the estimators. In\naddition, we study an application of the estimator in the context of covariance\nforecasting. We discover that the forecasting model with our estimator\noutperforms a benchmark model in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 14:01:22 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Mustafayeva", "Konul", ""], ["Wang", "Weining", ""]]}, {"id": "1905.08414", "submitter": "Vadim Sokolov", "authors": "Vadim Sokolov and Michael Polson", "title": "Strategic Bayesian Asset Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic asset allocation requires an investor to select stocks from a given\nbasket of assets. The perspective of our investor is to maximize risk-adjusted\nalpha returns relative to a benchmark index. Historical returns are used to\nprovide inputs into an optimization algorithm. Our approach uses Bayesian\nregularization to not only provide stock selection but also optimal sequential\nportfolio weights. By incorporating investor preferences with a number of\ndifferent regularization penalties we extend the approaches of Black (1992) and\nPuelz (2015). We tailor standard sparse MCMC algorithms to calculate portfolio\nweights and perform selection. We illustrate our methodology on stock selection\nfrom the SP100 stock index and from the top fifty holdings of two hedge funds\nRenaissance Technologies and Viking Global. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:02:48 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 00:12:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sokolov", "Vadim", ""], ["Polson", "Michael", ""]]}, {"id": "1905.08450", "submitter": "Edward Wu", "authors": "Edward Wu and Johann A. Gagnon-Bartsch", "title": "The P-LOOP Estimator: Covariate Adjustment for Paired Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In paired experiments, participants are grouped into pairs with similar\ncharacteristics, and one observation from each pair is randomly assigned to\ntreatment. Because of both the pairing and the randomization, the treatment and\ncontrol groups should be well balanced; however, there may still be small\nchance imbalances. It may be possible to improve the precision of the treatment\neffect estimate by adjusting for these imbalances. Building on related work for\ncompletely randomized experiments, we propose the P-LOOP (paired leave-one-out\npotential outcomes) estimator for paired experiments. We leave out each pair\nand then impute its potential outcomes using any prediction algorithm. The\nimputation method is flexible; for example, we could use lasso or random\nforests. While similar methods exist for completely randomized experiments,\ncovariate adjustment methods in paired experiments are relatively understudied.\nA unique trade-off exists for paired experiments, where it can be unclear\nwhether to factor in pair assignments when making adjustments. We address this\nissue in the P-LOOP estimator by automatically deciding whether to account for\nthe pairing when imputing the potential outcomes. By addressing this trade-off,\nthe method has the potential to improve precision over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 05:48:08 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wu", "Edward", ""], ["Gagnon-Bartsch", "Johann A.", ""]]}, {"id": "1905.08659", "submitter": "Kevin Wilson Dr", "authors": "Kevin James Wilson and Malcolm Farrow (School of Mathematics,\n  Statistics & Physics, Newcastle University, UK)", "title": "Assurance for sample size determination in reliability demonstration\n  testing", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manufacturers are required to demonstrate products meet reliability targets.\nA typical way to achieve this is with reliability demonstration tests (RDTs),\nin which a number of products are put on test and the test is passed if a\ntarget reliability is achieved. There are various methods for determining the\nsample size for RDTs, typically based on the power of a hypothesis test\nfollowing the RDT or risk criteria. Bayesian risk criteria approaches can\nconflate the choice of sample size and the analysis to be undertaken once the\ntest has been conducted and rely on the specification of somewhat artificial\nacceptable and rejectable reliability levels. In this paper we offer an\nalternative approach to sample size determination based on the idea of\nassurance. This approach chooses the sample size to answer provide a certain\nprobability that the RDT will result in a successful outcome. It separates the\ndesign and analysis of the RDT, allowing different priors for each. We develop\nthe assurance approach for sample size calculations in RDTs for binomial and\nWeibull likelihoods and propose appropriate prior distributions for the design\nand analysis of the test. In each case, we illustrate the approach with an\nexample based on real data.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:08:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wilson", "Kevin James", "", "School of Mathematics,\n  Statistics & Physics, Newcastle University, UK"], ["Farrow", "Malcolm", "", "School of Mathematics,\n  Statistics & Physics, Newcastle University, UK"]]}, {"id": "1905.08726", "submitter": "Jessica Silva Lomba", "authors": "Jessica Silva Lomba and Maria Isabel Fraga Alves", "title": "L-moments for automatic threshold selection in extreme value analysis", "comments": null, "journal-ref": "L-moments for automatic threshold selection in extreme value\n  analysis. Stoch Environ Res Risk Assess 34, 465-491 (2020)", "doi": "10.1007/s00477-020-01789-x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In extreme value analysis, sensitivity of inference to the definition of\nextreme event is a paramount issue. Under the peaks-over-threshold (POT)\napproach, this translates directly into the need of fitting a Generalized\nPareto distribution to observations above a suitable level that balances bias\nversus variance of estimates. Selection methodologies established in the\nliterature face recurrent challenges such as an inherent subjectivity or high\ncomputational intensity. We suggest a truly automated method for threshold\ndetection, aiming at time efficiency and elimination of subjective judgment.\nBased on the well-established theory of L-moments, this versatile data-driven\ntechnique can handle batch processing of large collections of extremes data,\nwhile also presenting good performance on small samples.\n  The technique's performance is evaluated in a large simulation study and\nillustrated with significant wave height data sets from the literature. We find\nthat it compares favorably to other state-of-the-art methods regarding the\nchoice of threshold, associated parameter estimation and the ultimate goal of\ncomputationally efficient return level estimation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:12:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lomba", "Jessica Silva", ""], ["Alves", "Maria Isabel Fraga", ""]]}, {"id": "1905.08736", "submitter": "Ting-Shuo Yo", "authors": "Shih-Hao Su and Jung-Lien Chu and Ting-Shuo Yo and Lee-Yaw Lin", "title": "Identification of synoptic weather types over Taiwan area with multiple\n  classifiers", "comments": "journal article, open access", "journal-ref": "Atmos Sci Lett.2018;e861", "doi": "10.1002/asl.861", "report-no": null, "categories": "physics.ao-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel machine learning approach was used to classify three\ntypes of synoptic weather events in Taiwan area from 2001 to 2010. We used\nreanalysis data with three machine learning algorithms to recognize weather\nsystems and evaluated their performance. Overall, the classifiers successfully\nidentified 52-83% of weather events (hit rate), which is higher than the\nperformance of traditional objective methods. The results showed that the\nmachine learning approach gave low false alarm rate in general, while the\nsupport vector machine (SVM) with more principal components of reanalysis data\nhad higher hit rate on all tested weather events. The sensitivity tests of grid\ndata resolution indicated that the differences between the high- and\nlow-resolution datasets are limited, which implied that the proposed method can\nachieve reasonable performance in weather forecasting with minimal resources.\nBy identifying daily weather systems in historical reanalysis data, this method\ncan be used to study long-term weather changes, to monitor climatological-scale\nvariations, and to provide a better estimate of climate projections.\nFurthermore, this method can also serve as an alternative to model output\nstatistics and potentially be used for synoptic weather forecasting.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:29:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Su", "Shih-Hao", ""], ["Chu", "Jung-Lien", ""], ["Yo", "Ting-Shuo", ""], ["Lin", "Lee-Yaw", ""]]}, {"id": "1905.08840", "submitter": "Paul Sharkey", "authors": "Paul Sharkey, Jonathan A. Tawn, Simon J. Brown", "title": "A stochastic model for the lifecycle and track of extreme extratropical\n  cyclones in the North Atlantic", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extratropical cyclones are large-scale weather systems which are often the\nsource of extreme weather events in Northern Europe, often leading to mass\ninfrastructural damage and casualties. Such systems create a local vorticity\nmaxima which tracks across the Atlantic Ocean and from which can be determined\na climatology for the region. While there have been considerable advances in\ndeveloping algorithms for extracting the track and evolution of cyclones from\nreanalysis datasets, the data record is relatively short. This justifies the\nneed for a statistical model to represent the more extreme characteristics of\nthese weather systems, specifically their intensity and the spatial variability\nin their tracks. This paper presents a novel simulation-based approach to\nmodelling the lifecycle of extratropical cyclones in terms of both their tracks\nand vorticity, incorporating various aspects of cyclone evolution and movement.\nBy drawing on methods from extreme value analysis, we can simulate more extreme\nstorms than those observed, representing a useful tool for practitioners\nconcerned with risk assessment with regard to these weather systems.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 19:18:43 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Sharkey", "Paul", ""], ["Tawn", "Jonathan A.", ""], ["Brown", "Simon J.", ""]]}, {"id": "1905.08869", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi, Hassan Yazdani, Azadeh Vosoughi, Nazanin Rahnavard", "title": "Source Localization and Tracking for Dynamic Radio Cartography using\n  Directional Antennas", "comments": "SECON 2019 workshop on Edge Computing for Cyber Physical Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilization of directional antennas is a promising solution for efficient\nspectrum sensing and accurate source localization and tracking. Spectrum\nsensors equipped with directional antennas should constantly scan the space in\norder to track emitting sources and discover new activities in the area of\ninterest. In this paper, we propose a new formulation that unifies\nreceived-signal-strength (RSS) and direction of arrival (DoA) in a compressive\nsensing (CS) framework. The underlying CS measurement matrix is a function of\nbeamforming vectors of sensors and is referred to as the propagation matrix.\nComparing to the omni-directional antenna case, our employed propagation matrix\nprovides more incoherent projections, an essential factor in the compressive\nsensing theory. Based on the new formulation, we optimize the antenna beams,\nenhance spectrum sensing efficiency, track active primary users accurately and\nmonitor spectrum activities in an area of interest. In many practical scenarios\nthere is no fusion center to integrate received data from spectrum sensors. We\npropose the distributed version of our algorithm for such cases. Experimental\nresults show a significant improvement in source localization accuracy,\ncompared with the scenario when sensors are equipped with omni-directional\nantennas. Applicability of the proposed framework for dynamic radio cartography\nis shown. Moreover, comparing the estimated dynamic RF map over time with the\nground truth demonstrates the effectiveness of our proposed method for accurate\nsignal estimation and recovery.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:59:08 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Joneidi", "Mohsen", ""], ["Yazdani", "Hassan", ""], ["Vosoughi", "Azadeh", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "1905.08870", "submitter": "Claude Kl\\\"ockl", "authors": "Claude Kl\\\"ockl, Katharina Gruber, Peter Regner, Sebastian Wehrle,\n  Johannes Schmidt", "title": "The perils of automated fitting of datasets: the case of a wind turbine\n  cost model", "comments": "Comments welcome! All employed R-scripts and Python scripts can be\n  found on our repository: DOI: 10.5281/zenodo.3066230", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rinne et al. conduct an interesting analysis of the impact of wind turbine\ntechnology and land-use on wind power potentials, which allows profound\ninsights into each factors contribution to overall potentials. The paper\npresents a detailed model of site-specific wind turbine investment cost (i.e.\nroad- and grid access costs) complemented by a model used to estimate\nsite-independent costs. We believe that propose a cutting edge model of\nsite-specific investment costs. However, the site-independent cost model is\nflawed in our opinion. This flaw most likely does not impact the results\npresented in the paper, although we expect a considerable generalization error.\nThus the application of the wind turbine cost model in other contexts may lead\nto unreasonable results. More generally, the derivation of the wind turbine\ncost model serves as an example of how applications of automated regression\nanalysis can go wrong.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:01:15 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kl\u00f6ckl", "Claude", ""], ["Gruber", "Katharina", ""], ["Regner", "Peter", ""], ["Wehrle", "Sebastian", ""], ["Schmidt", "Johannes", ""]]}, {"id": "1905.09059", "submitter": "Kitsuchart Pasupa", "authors": "Wanthanee Rathasamuth, Kitsuchart Pasupa, Sissades Tongsima", "title": "Selection of a Minimal Number of Significant Porcine SNPs by an\n  Information Gain and Genetic Algorithm Hybrid Model", "comments": "16 pages, 9 figures, preprint submitted to Malaysian Journal of\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A panel of large number of common Single Nucleotide Polymorphisms (SNPs)\ndistributed across an entire porcine genome has been widely used to represent\ngenetic variability of pig. With the advent of SNP-array technology, a\ngenome-wide genetic profile of a specimen can be easily observed. Among the\nlarge number of such variations, there exist a much smaller subset of the SNP\npanel that could equally be used to correctly identify the corresponding breed.\nThis work presents a SNP selection heuristic that can still be used effectively\nin the breed classification process. The proposed feature selection was done by\nthe approach of combining a filter method and a wrapper method--information\ngain method and genetic algorithm--plus a feature frequency selection step,\nwhile classification was done by support vector machine. The approach was able\nto reduce the number of significant SNPs to 0.86 % of the total number of SNPs\nin a swine dataset and provided a high classification accuracy of 94.80 %.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:40:40 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Rathasamuth", "Wanthanee", ""], ["Pasupa", "Kitsuchart", ""], ["Tongsima", "Sissades", ""]]}, {"id": "1905.09252", "submitter": "Jason Wang", "authors": "Jason (Xiao) Wang and Pauline Burke", "title": "Measuring Average Treatment Effect from Heavy-tailed Data", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed metrics are common and often critical to product evaluation in\nthe online world. While we may have samples large enough for Central Limit\nTheorem to kick in, experimentation is challenging due to the wide confidence\ninterval of estimation. We demonstrate the pressure by running A/A simulations\nwith customer spending data from a large-scale Ecommerce site. Solutions are\nthen explored. On one front we address the heavy tail directly and highlight\nthe often ignored nuances of winsorization. In particular, the legitimacy of\nfalse positive rate could be at risk. We are further inspired by the idea of\nrobust statistics and introduce Huber regression as a better way to measure\ntreatment effect. On another front covariates from pre-experiment period are\nexploited. Although they are independent to assignment and potentially explain\nthe variation of response well, concerns are that models are learned against\nprediction error rather than the bias of parameter. We find the framework of\northogonal learning useful, matching not raw observations but residuals from\ntwo predictions, one towards the response and the other towards the assignment.\nRobust regression is readily integrated, together with cross-fitting. The final\ndesign is proven highly effective in driving down variance at the same time\ncontrolling bias. It is empowering our daily practice and hopefully can also\nbenefit other applications in the industry.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:15:19 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Jason", "", "", "Xiao"], ["Wang", "", ""], ["Burke", "Pauline", ""]]}, {"id": "1905.09376", "submitter": "Georgy Meshcheryakov", "authors": "Meshcheryakov Georgy, Igolkina Anna", "title": "semopy: A Python package for Structural Equation Modeling", "comments": null, "journal-ref": "Structural Equation Modeling: A Multidisciplinary Journal, 27:6,\n  952-963 (2020)", "doi": "10.1080/10705511.2019.1704289", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation modelling (SEM) is a multivariate statistical technique\nfor estimating complex relationships between observed and latent variables.\nAlthough numerous SEM packages exist, each of them has limitations. Some\npackages are not free or open-source; the most popular package not having this\ndisadvantage is $\\textbf{lavaan}$, but it is written in R language, which is\nbehind current mainstream tendencies that make it harder to be incorporated\ninto developmental pipelines (i.e. bioinformatical ones). Thus we developed the\nPython package $\\textbf{semopy}$ to satisfy those criteria. The paper provides\ndetailed examples of package usage and explains it's inner clockworks.\nMoreover, we developed the unique generator of SEM models to extensively test\nSEM packages and demonstrated that $\\textbf{semopy}$ significantly outperforms\n$\\textbf{lavaan}$ in execution time and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:58:24 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 19:34:45 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Georgy", "Meshcheryakov", ""], ["Anna", "Igolkina", ""]]}, {"id": "1905.09403", "submitter": "Amir Rafe", "authors": "Mohammad Ali Arman, Amir Rafe, Tobias Kretz", "title": "Applied hybrid binary mixed logit to investigate pedestrian crossing\n  safety at midblock and unsignalized intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian's crossing from unsignalized locations at intersections or\nmidblock locations is a risky decision that could lead to fatal accidents.\nDespite making a decision to accept a safe gap to cross the street is a\npersonal choice, studying this phenomena and its affecting variables are\ncrucial for accident analysis and traffic safety investigations. In this paper\nwe used video taped data to study pedestrian gap acceptance behavior at an\nunsignalized intersection and a midblock facility in Tehran, Iran. Multi way\nANOVA test indicates simultaneous effect of gender and child accompaniment have\nthe highest effect on size of accepted gap and waiting time. In addition,\nnumber of rejected gap is mostly affected by the child accompaniment status.\nInvestigation about critical gap shows in general, critical gap was bigger at\nunsignalized intersection and for women. We defined a latent variable named\ncaution behavior based on some observable indicators and using structural\nequation modeling it was estimated and used as an input in a binary mixed logit\nmodel. This variable identified important in gap acceptance decision. Despite\nboth structural equation and logit models are used in previous gap acceptance\nstudies, but according to our best knowledge this is the first use of the\nhybrid mixed logit modeling approach in this field and we developed a new\nmethodology to combine psychological and behavioral aspects of pedestrians' gap\nacceptance studies. Modeling approach shows pedestrian decision regarding\nacceptance or rejection of a gap to be highly influenced by the size of current\ngap, caution behavior and waiting time.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 23:17:20 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Arman", "Mohammad Ali", ""], ["Rafe", "Amir", ""], ["Kretz", "Tobias", ""]]}, {"id": "1905.09405", "submitter": "Jennifer Starling", "authors": "Jennifer E. Starling, Jared S. Murray, Patricia A. Lohr, Abigail R.A.\n  Aiken, Carlos M. Carvalho, James G. Scott", "title": "Targeted Smooth Bayesian Causal Forests: An analysis of heterogeneous\n  treatment effects for simultaneous versus interval medical abortion regimens\n  over gestation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric\nBayesian approach for estimating heterogeneous treatment effects which vary\nsmoothly over a single covariate in the observational data setting. The tsBCF\nmethod induces smoothness by parameterizing terminal tree nodes with smooth\nfunctions, and allows for separate regularization of treatment effects versus\nprognostic effect of control covariates. Smoothing parameters for prognostic\nand treatment effects can be chosen to reflect prior knowledge or tuned in a\ndata-dependent way.\n  We use tsBCF to analyze a new clinical protocol for early medical abortion.\nOur aim is to assess relative effectiveness of simultaneous versus interval\nadministration of mifepristone and misoprostol over the first nine weeks of\ngestation. The model reflects our expectation that the relative effectiveness\nvaries smoothly over gestation, but not necessarily over other covariates. We\ndemonstrate the performance of the tsBCF method on benchmarking experiments.\nSoftware for tsBCF is available at https://github.com/jestarling/tsbcf/.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 23:35:43 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 21:59:46 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Starling", "Jennifer E.", ""], ["Murray", "Jared S.", ""], ["Lohr", "Patricia A.", ""], ["Aiken", "Abigail R. A.", ""], ["Carvalho", "Carlos M.", ""], ["Scott", "James G.", ""]]}, {"id": "1905.09551", "submitter": "Antonio Possolo", "authors": "Christos Merkatas, Blaza Toman, Antonio Possolo, Stephan Schlamminger", "title": "Shades of Dark Uncertainty and Consensus Value for the Newtonian\n  Constant of Gravitation", "comments": null, "journal-ref": null, "doi": "10.1088/1681-7575/ab3365", "report-no": null, "categories": "physics.data-an physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Newtonian constant of gravitation, $G$, stands out in the landscape of\nthe most common fundamental constants owing to its surprisingly large relative\nuncertainty, which is attributable mostly to the dispersion of the values\nmeasured for it in different experiments.\n  This study focuses on a set of measurements of $G$ that are mutually\ninconsistent, in the sense that the dispersion of the measured values is\nsignificantly larger than what their reported uncertainties suggest that it\nshould be. Furthermore, there is a loosely defined group of measured values\nthat lie fairly close to a consensus value that may be derived from all the\nmeasurement results, and then there are one or more groups with measured values\nfarther away from the consensus value, some higher, others lower.\n  This same general pattern is often observed in many interlaboratory studies\nand meta-analyses. In the conventional treatments of such data, the mutual\ninconsistency is addressed by inflating the reported uncertainties, either\nmultiplicatively, or by the addition of random effects, both reflecting the\npresence of dark uncertainty. The former approach is often used by CODATA and\nby the Particle Data Group, and the latter is common in medical meta-analysis\nand in metrology.\n  We propose a new procedure for consensus building that models the results\nusing latent clusters with different shades of dark uncertainty, which assigns\na customized amount of dark uncertainty to each measured value, as a mixture of\nthose shades, and does so taking into account both the placement of the\nmeasured values relative to the consensus value, and the reported\nuncertainties. We demonstrate this procedure by deriving a new estimate for\n$G$, as a consensus value $G = 6.67408 \\times 10^{-11} \\,\\text{m}^{-3} \\,\n\\text{kg}^{-1} \\, \\text{s}^{-2}$, with $u(G) = 0.00024 \\times 10^{-11}\n\\,\\text{m}^{-3} \\, \\text{kg}^{-1} \\, \\text{s}^{-2}$.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 09:29:17 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Merkatas", "Christos", ""], ["Toman", "Blaza", ""], ["Possolo", "Antonio", ""], ["Schlamminger", "Stephan", ""]]}, {"id": "1905.09633", "submitter": "Min Shu", "authors": "Min Shu, Wei Zhu", "title": "Diagnosis and Prediction of the 2015 Chinese Stock Market Bubble", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we perform a novel analysis of the 2015 financial bubble in\nthe Chinese stock market by calibrating the Log Periodic Power Law Singularity\n(LPPLS) model to two important Chinese stock indices, SSEC and SZSC, from early\n2014 to June 2015. The back tests of the 2015 Chinese stock market bubbles\nindicates that the LPPLS model can readily detect the bubble behavior of the\nfaster-than-exponential increase corrected by the accelerating\nlogarithm-periodic oscillations in the 2015 Chinese Stock market. The existence\nof log-periodicity is detected by applying the Lomb spectral analysis on the\ndetrended residuals. The Ornstein-Uhlenbeck property and the stationarity of\nthe LPPLS fitting residuals are confirmed by the two Unit-root tests\n(Philips-Perron test and Dickery-Fuller test). According to our analysis, the\nactual critical day t_c can be well predicted by the LPPLS model as far back as\ntwo months before the actual bubble crash. Compared to the traditional\noptimization method used in the LPPLS model, we find the covariance matrix\nadaptation evolution strategy (CMA-ES) to have a significantly lower\ncomputation cost, and thus recommend this as a better alternative algorithm for\nLPPLS model fit. Furthermore, in the LPPLS fitting with expanding windows, the\ngap (tc -t2) shows a significant decrease when the end day t2 approaches the\nactual bubble crash time. The change rate of the gap (tc-t2) may be used as an\nadditional indicator besides the key indicator tc to improve the prediction of\nbubble burst.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:09:50 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:12:03 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Shu", "Min", ""], ["Zhu", "Wei", ""]]}, {"id": "1905.09640", "submitter": "Min Shu", "authors": "Min Shu, Wei Zhu", "title": "Detection of Chinese Stock Market Bubbles with LPPLS Confidence\n  Indicator", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.124892", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an advance bubble detection methodology based on the Log Periodic\nPower Law Singularity (LPPLS) confidence indicator for the early causal\nidentification of positive and negative bubbles in the Chinese stock market\nusing the daily data on the Shanghai Shenzhen CSI 300 stock market index from\nJanuary 2002 through April 2018. We account for the damping condition of LPPLS\nmodel in the search space and implement the stricter filter conditions for the\nqualification of the valid LPPLS fits by taking account of the maximum relative\nerror, performing the Lomb log-periodic test of the detrended residual, and\nunit-root tests of the logarithmic residual based on both the Phillips-Perron\ntest and Dickey-Fuller test to improve the performance of LPPLS confidence\nindicator. Our analysis shows that the LPPLS detection strategy diagnoses the\npositive bubbles and negative bubbles corresponding to well-known historical\nevents, implying the detection strategy based on the LPPLS confidence indicator\nhas an outstanding performance to identify the bubbles in advance. We find that\nthe probability density distribution of the estimated beginning time of bubbles\nappears to be skewed and the mass of the distribution is concentrated on the\narea where the price starts to have an obvious super-exponentially growth. This\nstudy is the first work in the literature that identifies the existence of\nbubbles in the Chinese stock market using the daily data of CSI 300 index with\nthe advance bubble detection methodology of LPPLS confidence indicator. We have\nshown that it is possible to detect the potential positive and negative bubbles\nand crashes ahead of time, which in turn limits the bubble sizes and eventually\nminimizes the damages from the bubble crash.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:19:10 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:18:34 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Shu", "Min", ""], ["Zhu", "Wei", ""]]}, {"id": "1905.09647", "submitter": "Min Shu", "authors": "Min Shu, Wei Zhu", "title": "Real-time Prediction of Bitcoin Bubble Crashes", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.124477", "report-no": null, "categories": "q-fin.ST q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, Bitcoin as an emerging asset class has gained widespread\npublic attention because of their extraordinary returns in phases of extreme\nprice growth and their unpredictable massive crashes. We apply the log-periodic\npower law singularity (LPPLS) confidence indicator as a diagnostic tool for\nidentifying bubbles using the daily data on Bitcoin price in the past two\nyears. We find that the LPPLS confidence indicator based on the daily Bitcoin\nprice data fails to provide effective warnings for detecting the bubbles when\nthe Bitcoin price suffers from a large fluctuation in a short time, especially\nfor positive bubbles. In order to diagnose the existence of bubbles and\naccurately predict the bubble crashes in the cryptocurrency market, this study\nproposes an adaptive multilevel time series detection methodology based on the\nLPPLS model and finer (than daily) timescale for the Bitcoin price data. We\nadopt two levels of time series, 1 hour and 30 minutes, to demonstrate the\nadaptive multilevel time series detection methodology. The results show that\nthe LPPLS confidence indicator based on this new method is an outstanding\ninstrument to effectively detect the bubbles and accurately forecast the bubble\ncrashes, even if a bubble exists in a short time. In addition, we discover that\nthe short-term LPPLS confidence indicator highly sensitive to the extreme\nfluctuations of Bitcoin price can provide some useful insights into the bubble\nstatus on a shorter time scale - on a day to week scale, and the long-term\nLPPLS confidence indicator has a stable performance in terms of effectively\nmonitoring the bubble status on a longer time scale - on a week to month scale.\nThe adaptive multilevel time series detection methodology can provide real-time\ndetection of bubbles and advanced forecast of crashes to warn of the imminent\nrisk.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:38:36 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:21:53 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Shu", "Min", ""], ["Zhu", "Wei", ""]]}, {"id": "1905.09693", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Andrew Gelman and Matthijs V\\'ak\\'ar", "title": "Slamming the sham: A Bayesian model for adaptive adjustment with noisy\n  control data", "comments": "20 pages; to appear in Statistics in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not always clear how to adjust for control data in causal inference,\nbalancing the goals of reducing bias and variance. We show how, in a setting\nwith repeated experiments, Bayesian hierarchical modeling yields an adaptive\nprocedure that uses the data to determine how much adjustment to perform. The\nresult is a novel analysis with increased statistical efficiency compared to\nthe default analysis based on difference estimates. We demonstrate this\nprocedure on two real examples, as well as on a series of simulated datasets.\nWe show that the increased efficiency can have real-world consequences in terms\nof the conclusions that can be drawn from the experiments. We also discuss the\nrelevance of this work to causal inference and statistical design and analysis\nmore generally.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:14:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 15:36:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gelman", "Andrew", ""], ["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "1905.09722", "submitter": "Zhantao Lin", "authors": "Zhantao Lin, Nancy Flournoy, William F. Rosenberger", "title": "Random Norming Aids Analysis of Non-linear Regression Models with\n  Sequential Informative Dose Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage adaptive optimal design is an attractive option for increasing\nthe efficiency of clinical trials. In these designs, based on interim data, the\nlocally optimal dose is chosen for further exploration, which induces\ndependencies between data from the two stages. When the maximum likelihood\nestimator (MLE) is used under nonlinear regression models with independent\nnormal errors in a pilot study where the first stage sample size is fixed, and\nthe second stage sample size is large, the Fisher information fails to\nnormalize the estimator adequately asymptotically, because of dependencies. In\nthis situation, we present three alternative random information measures and\nshow that they provide better normalization of the MLE asymptotically. The\nperformance of random information measures is investigated in simulation\nstudies, and the results suggest that the observed information performs best\nwhen the sample size is small.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:30:16 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Lin", "Zhantao", ""], ["Flournoy", "Nancy", ""], ["Rosenberger", "William F.", ""]]}, {"id": "1905.09993", "submitter": "Dingjue Ji", "authors": "Dingjue Ji, Junwei Lu, Yiliang Zhang, Hongyu Zhao, Siyuan Gao", "title": "Inference of Dynamic Graph Changes for Functional Connectome", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and\n  Statistics, 26-28 August 2020, Online, PMLR 108:3230-3240", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity is an effective measure for the brain's\nresponses to continuous stimuli. We propose an inferential method to detect the\ndynamic changes of brain networks based on time-varying graphical models.\nWhereas most existing methods focus on testing the existence of change points,\nthe dynamics in the brain network offer more signals in many neuroscience\nstudies. We propose a novel method to conduct hypothesis testing on changes in\ndynamic brain networks. We introduce a bootstrap statistic to approximate the\nsupreme of the high-dimensional empirical processes over dynamically changing\nedges. Our simulations show that this framework can capture the change points\nwith changed connectivity. Finally, we apply our method to a brain imaging\ndataset under a natural audio-video stimulus and illustrate that we are able to\ndetect temporal changes in brain networks. The functions of the identified\nregions are consistent with specific emotional annotations, which are closely\nassociated with changes inferred by our method.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:44:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 22:07:08 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ji", "Dingjue", ""], ["Lu", "Junwei", ""], ["Zhang", "Yiliang", ""], ["Zhao", "Hongyu", ""], ["Gao", "Siyuan", ""]]}, {"id": "1905.10172", "submitter": "Dr. Alexander Paraskevov", "authors": "A.V. Paraskevov, A.S. Minkin", "title": "Damped oscillations of the probability of random events followed by\n  absolute refractory period", "comments": "additional section has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.PR math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many events are followed by an absolute refractory state, when for some time\nafter the event a repetition of a similar event is impossible. If uniform\nevents, each of which is followed by the same period of absolute\nrefractoriness, occur randomly, as in the Bernoulli scheme, then the event\nprobability as a function of time can exhibit damped transient oscillations\ncaused by a specific initial condition. Here we give an exact analytical\ndescription of the oscillations, with a focus on application within\nneuroscience. The resulting formulas stand out for their relative simplicity,\nenabling analytical calculation of the damping coefficients for the second and\nthird peaks of the event probability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 12:02:17 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 15:05:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Paraskevov", "A. V.", ""], ["Minkin", "A. S.", ""]]}, {"id": "1905.10176", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Victor Lei, Miruna Oprescu, Maggie Hei, Keith\n  Battocchi, Greg Lewis", "title": "Machine Learning Estimation of Heterogeneous Treatment Effects with\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of heterogeneous treatment effects with arbitrary\nmachine learning methods in the presence of unobserved confounders with the aid\nof a valid instrument. Such settings arise in A/B tests with an intent-to-treat\nstructure, where the experimenter randomizes over which user will receive a\nrecommendation to take an action, and we are interested in the effect of the\ndownstream action. We develop a statistical learning approach to the estimation\nof heterogeneous effects, reducing the problem to the minimization of an\nappropriate loss function that depends on a set of auxiliary models (each\ncorresponding to a separate prediction task). The reduction enables the use of\nall recent algorithmic advances (e.g. neural nets, forests). We show that the\nestimated effect model is robust to estimation errors in the auxiliary models,\nby showing that the loss satisfies a Neyman orthogonality criterion. Our\napproach can be used to estimate projections of the true effect model on\nsimpler hypothesis spaces. When these spaces are parametric, then the parameter\nestimates are asymptotically normal, which enables construction of confidence\nsets. We applied our method to estimate the effect of membership on downstream\nwebpage engagement on TripAdvisor, using as an instrument an intent-to-treat\nA/B test among 4 million TripAdvisor users, where some users received an easier\nmembership sign-up process. We also validate our method on synthetic data and\non public datasets for the effects of schooling on income.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 12:14:08 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 01:05:23 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 01:57:04 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Lei", "Victor", ""], ["Oprescu", "Miruna", ""], ["Hei", "Maggie", ""], ["Battocchi", "Keith", ""], ["Lewis", "Greg", ""]]}, {"id": "1905.10297", "submitter": "Lin Wang", "authors": "Fang Wang, Lin Wang and Yuming Chen", "title": "A DFA-based bivariate regression model for estimating the dependence of\n  PM2.5 among neighbouring cities", "comments": "This is a pre-print of an article published in Scientific Reports.\n  The final authenticated version is available online at:\n  https://www.nature.com/articles/s41598-018-25822-w", "journal-ref": "Scientific Reports, 8(2018): 7475", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the basis of detrended fluctuation analysis (DFA), we propose a new\nbivariate linear regression model. This new model provides estimators of\nmulti-scale regression coefficients to measure the dependence between variables\nand corresponding variables of interest with multi-scales. Numerical tests are\nperformed to illustrate that the proposed DFA-based regression estimators are\ncapable of accurately depicting the dependence between the variables of\ninterest and can be used to identify different dependence at different time\nscales. We apply this model to analyze the PM2.5 series of three adjacent\ncities (Beijing, Tianjin, and Baoding) in Northern China. The estimated\nregression coefficients confirmed the dependence of PM2.5 among the three\ncities and illustrated that each city has different influence on the others at\ndifferent seasons and at different time scales. Two statistics based on the\nscale-dependent $t$-statistic and the partial detrended cross-correlation\ncoefficient are used to demonstrate the significance of the dependence. Three\nnew scale-dependent evaluation indices show that the new DFA-based bivariate\nregression model can provide rich information on studied variables.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 13:56:27 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wang", "Fang", ""], ["Wang", "Lin", ""], ["Chen", "Yuming", ""]]}, {"id": "1905.10309", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, Yiqing Zhao, Terry M. Therneau, Elizabeth J. Atkinson,\n  Ahmad P. Tafti, Nan Zhang, Shreyasee Amin, Andrew H. Limper, Hongfang Liu", "title": "Unsupervised Machine Learning for the Discovery of Latent Disease\n  Clusters and Patient Subgroups Using Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become ubiquitous and a key technology on mining\nelectronic health records (EHRs) for facilitating clinical research and\npractice. Unsupervised machine learning, as opposed to supervised learning, has\nshown promise in identifying novel patterns and relations from EHRs without\nusing human created labels. In this paper, we investigate the application of\nunsupervised machine learning models in discovering latent disease clusters and\npatient subgroups based on EHRs. We utilized Latent Dirichlet Allocation (LDA),\na generative probabilistic model, and proposed a novel model named Poisson\nDirichlet Model (PDM), which extends the LDA approach using a Poisson\ndistribution to model patients' disease diagnoses and to alleviate age and sex\nfactors by considering both observed and expected observations. In the\nempirical experiments, we evaluated LDA and PDM on three patient cohorts with\nEHR data retrieved from the Rochester Epidemiology Project (REP), for the\ndiscovery of latent disease clusters and patient subgroups. We compared the\neffectiveness of LDA and PDM in identifying latent disease clusters through the\nvisualization of disease representations learned by two approaches. We also\ntested the performance of LDA and PDM in differentiating patient subgroups\nthrough survival analysis, as well as statistical analysis. The experimental\nresults show that the proposed PDM could effectively identify distinguished\ndisease clusters by alleviating the impact of age and sex, and that LDA could\nstratify patients into more differentiable subgroups than PDM in terms of\np-values. However, the subgroups discovered by PDM might imply the underlying\npatterns of diseases of greater interest in epidemiology research due to the\nalleviation of age and sex. Both unsupervised machine learning approaches could\nbe leveraged to discover patient subgroups using EHRs but with different foci.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:07:22 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wang", "Yanshan", ""], ["Zhao", "Yiqing", ""], ["Therneau", "Terry M.", ""], ["Atkinson", "Elizabeth J.", ""], ["Tafti", "Ahmad P.", ""], ["Zhang", "Nan", ""], ["Amin", "Shreyasee", ""], ["Limper", "Andrew H.", ""], ["Liu", "Hongfang", ""]]}, {"id": "1905.10341", "submitter": "Lauren Kennedy", "authors": "Lauren Kennedy and Daniel Simpson and Andrew Gelman", "title": "The experiment is just as important as the likelihood in understanding\n  the prior: A cautionary note on robust cognitive modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive modelling shares many features with statistical modelling, making\nit seem trivial to borrow from the practices of robust Bayesian statistics to\nprotect the practice of robust cognitive modelling. We take one aspect of\nstatistical workflow-prior predictive checks-and explore how they might be\napplied to a cognitive modelling task. We find that it is not only the\nlikelihood that is needed to interpret the priors, we also need to incorporate\nexperiment information as well. This suggests that while cognitive modelling\nmight borrow from statistical practices, especially workflow, care must be made\nto make the adaptions necessary.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:13:52 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 13:01:04 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Kennedy", "Lauren", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "1905.10351", "submitter": "Cornelius Fritz", "authors": "Cornelius Fritz, Michael Lebacher, G\\\"oran Kauermann", "title": "Tempus Volat, Hora Fugit -- A Survey of Tie-Oriented Dynamic Network\n  Models in Discrete and Continuous Time", "comments": null, "journal-ref": null, "doi": "10.1111/stan.12198", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the growing number of available tools for modeling dynamic networks,\nthe choice of a suitable model becomes central. The goal of this survey is to\nprovide an overview of tie-oriented dynamic network models. The survey is\nfocused on introducing binary network models with their corresponding\nassumptions, advantages, and shortfalls. The models are divided according to\ngenerating processes, operating in discrete and continuous time. First, we\nintroduce the Temporal Exponential Random Graph Model (TERGM) and the Separable\nTERGM (STERGM), both being time-discrete models. These models are then\ncontrasted with continuous process models, focusing on the Relational Event\nModel (REM). We additionally show how the REM can handle time-clustered\nobservations, i.e., continuous time data observed at discrete time points.\nBesides the discussion of theoretical properties and fitting procedures, we\nspecifically focus on the application of the models on two networks that\nrepresent international arms transfers and email exchange. The data allow to\ndemonstrate the applicability and interpretation of the network models.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:47:59 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 08:26:50 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 15:36:05 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Fritz", "Cornelius", ""], ["Lebacher", "Michael", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1905.10493", "submitter": "Zhenyu Zhao", "authors": "Zhenyu Zhao, Mandie Liu, Anirban Deb", "title": "Safely and Quickly Deploying New Features with a Staged Rollout\n  Framework Using Sequential Test and Adaptive Experimental Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the rapid development cycle for Internet products (websites and mobile\napps), new features are developed and rolled out to users constantly. Features\nwith code defects or design flaws can cause outages and significant degradation\nof user experience. The traditional method of code review and change management\ncan be time-consuming and error-prone. In order to make the feature rollout\nprocess safe and fast, this paper proposes a methodology for rolling out\nfeatures in an automated way using an adaptive experimental design. Under this\nframework, a feature is gradually ramped up from a small proportion of users to\na larger population based on real-time evaluation of the performance of\nimportant metrics. If there are any regression detected during the ramp-up\nstep, the ramp-up process stops and the feature developer is alerted. There are\ntwo main algorithm components powering this framework: 1) a continuous\nmonitoring algorithm - using a variant of the sequential probability ratio test\n(SPRT) to monitor the feature performance metrics and alert feature developers\nwhen a metric degradation is detected, 2) an automated ramp-up algorithm -\ndeciding when and how to ramp up to the next stage with larger sample size.\nThis paper presents one monitoring algorithm and three ramping up algorithms\nincluding time-based, power-based, and risk-based (a Bayesian approach)\nschedules. These algorithms are evaluated and compared on both simulated data\nand real data. There are three benefits provided by this framework for feature\nrollout: 1) for defective features, it can detect the regression early and\nreduce negative effect, 2) for healthy features, it rolls out the feature\nquickly, 3) it reduces the need for manual intervention via the automation of\nthe feature rollout process.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 01:22:03 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhao", "Zhenyu", ""], ["Liu", "Mandie", ""], ["Deb", "Anirban", ""]]}, {"id": "1905.10550", "submitter": "Evgeny Burnaev", "authors": "Marina Pominova and Anna Kuzina and Ekaterina Kondrateva and Svetlana\n  Sushchinskaya and Maxim Sharaev and Evgeny Burnaev and and Vyacheslav Yarkin", "title": "Ensemble of 3D CNN regressors with data fusion for fluid intelligence\n  prediction", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": "ABCD Neurocognitive Prediction Challenge, Springer LNCS, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim at predicting children's fluid intelligence scores based\non structural T1-weighted MR images from the largest long-term study of brain\ndevelopment and child health. The target variable was regressed on a data\ncollection site, socio-demographic variables and brain volume, thus being\nindependent to the potentially informative factors, which are not directly\nrelated to the brain functioning. We investigate both feature extraction and\ndeep learning approaches as well as different deep CNN architectures and their\nensembles. We propose an advanced architecture of VoxCNNs ensemble, which yield\nMSE (92.838) on blind test.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:54:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Pominova", "Marina", ""], ["Kuzina", "Anna", ""], ["Kondrateva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Sharaev", "Maxim", ""], ["Burnaev", "Evgeny", ""], ["Yarkin", "and Vyacheslav", ""]]}, {"id": "1905.10684", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and James M. Robins and Sebastien J-P.A. Haneuse and\n  Iman Saeed and Sarah E. Robertson and Elisabeth A. Stuart and Miguel A.\n  Hern\\'an", "title": "Sensitivity analysis using bias functions for studies extending\n  inferences from a randomized trial to a target population", "comments": "first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending (generalizing or transporting) causal inferences from a randomized\ntrial to a target population requires ``generalizability'' or\n``transportability'' assumptions, which state that randomized and\nnon-randomized individuals are exchangeable conditional on baseline covariates.\nThese assumptions are made on the basis of background knowledge, which is often\nuncertain or controversial, and need to be subjected to sensitivity analysis.\nWe present simple methods for sensitivity analyses that do not require detailed\nbackground knowledge about specific unknown or unmeasured determinants of the\noutcome or modifiers of the treatment effect. Instead, our methods directly\nparameterize violations of the assumptions using bias functions. We show how\nthe methods can be applied to non-nested trial designs, where the trial data\nare combined with a separately obtained sample of non-randomized individuals,\nas well as to nested trial designs, where a clinical trial is embedded within a\ncohort sampled from the target population. We illustrate the methods using data\nfrom a clinical trial comparing treatments for chronic hepatitis C infection.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 21:58:52 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robins", "James M.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Saeed", "Iman", ""], ["Robertson", "Sarah E.", ""], ["Stuart", "Elisabeth A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1905.10805", "submitter": "Evgeny Burnaev", "authors": "P. Proskura and A. Zaytsev and I. Braslavsky and E. Egorov and E.\n  Burnaev", "title": "Usage of multiple RTL features for Earthquake prediction", "comments": "13 pages, 3 figures, 3 tables", "journal-ref": "Proceedings of the International Conference on Computational\n  Science and Applications (ICCSA-2019), 2019", "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.SP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a classification model that predicts if an earthquake with the\nmagnitude above a threshold will take place at a given location in a time range\n30-180 days from a given moment of time. A common approach is to use expert\nforecasts based on features like Region-Time-Length (RTL) characteristics. The\nproposed approach uses machine learning on top of multiple RTL features to take\ninto account effects at various scales and to improve prediction accuracy. For\nhistorical data about Japan earthquakes 1992-2005 and predictions at locations\ngiven in this database the best model has precision up to ~ 0.95 and recall up\nto ~ 0.98.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 13:37:16 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Proskura", "P.", ""], ["Zaytsev", "A.", ""], ["Braslavsky", "I.", ""], ["Egorov", "E.", ""], ["Burnaev", "E.", ""]]}, {"id": "1905.10806", "submitter": "Domenico Di Gangi", "authors": "Domenico Di Gangi, Giacomo Bormetti, Fabrizio Lillo", "title": "Score-Driven Exponential Random Graphs: A New Class of Time-Varying\n  Parameter Models for Dynamical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the evidence that real-world networks evolve in time and may\nexhibit non-stationary features, we propose an extension of the Exponential\nRandom Graph Models (ERGMs) accommodating the time variation of network\nparameters. Within the ERGM framework, a network realization is sampled from a\nstatic probability distribution defined parametrically in terms of network\nstatistics. Inspired by the fast growing literature on Dynamic Conditional\nScore-driven models, in our approach, each parameter evolves according to an\nupdating rule driven by the score of the conditional distribution. We\ndemonstrate the flexibility of the score-driven ERGMs, both as data generating\nprocesses and as filters, and we prove the advantages of the dynamic version\nwith respect to the static one. Our method captures dynamical network\ndependencies, that emerge from the data, and allows for a test discriminating\nbetween static or time-varying parameters. Finally, we corroborate our findings\nwith the application to networks from real financial and political systems\nexhibiting non stationary dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 13:48:43 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Di Gangi", "Domenico", ""], ["Bormetti", "Giacomo", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1905.10808", "submitter": "Matteo Sordello", "authors": "Matteo Sordello and Dylan S. Small", "title": "A Test for Differential Ascertainment in Case-Control Studies with\n  Application to Child Maltreatment", "comments": "25 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to test for the presence of differential ascertainment in\ncase-control studies, when data are collected by multiple sources. We show\nthat, when differential ascertainment is present, the use of only the observed\ncases leads to severe bias in the computation of the odds ratio. We can\nalleviate the effect of such bias using the estimates that our method of\ntesting for differential ascertainment naturally provides. We apply it to a\ndataset obtained from the National Violent Death Reporting System, with the\ngoal of checking for the presence of differential ascertainment by race in the\ncount of deaths caused by child maltreatment.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 14:41:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 15:13:12 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 21:53:17 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sordello", "Matteo", ""], ["Small", "Dylan S.", ""]]}, {"id": "1905.10831", "submitter": "Neil Oxtoby", "authors": "Agoston Mihalik, Mikael Brudfors, Maria Robu, Fabio S. Ferreira,\n  Hongxiang Lin, Anita Rau, Tong Wu, Stefano B. Blumberg, Baris Kanber, Maira\n  Tariq, Maria Del Mar Estarellas Garcia, Cemre Zor, Daniil I. Nikitichev,\n  Janaina Mourao-Miranda, Neil P. Oxtoby", "title": "ABCD Neurocognitive Prediction Challenge 2019: Predicting individual\n  fluid intelligence scores from structural MRI using probabilistic\n  segmentation and kernel ridge regression", "comments": "Winning entry in the ABCD Neurocognitive Prediction Challenge at\n  MICCAI 2019. 7 pages plus references, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied several regression and deep learning methods to predict fluid\nintelligence scores from T1-weighted MRI scans as part of the ABCD\nNeurocognitive Prediction Challenge (ABCD-NP-Challenge) 2019. We used voxel\nintensities and probabilistic tissue-type labels derived from these as features\nto train the models. The best predictive performance (lowest mean-squared\nerror) came from Kernel Ridge Regression (KRR; $\\lambda=10$), which produced a\nmean-squared error of 69.7204 on the validation set and 92.1298 on the test\nset. This placed our group in the fifth position on the validation leader board\nand first place on the final (test) leader board.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:31:00 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Mihalik", "Agoston", ""], ["Brudfors", "Mikael", ""], ["Robu", "Maria", ""], ["Ferreira", "Fabio S.", ""], ["Lin", "Hongxiang", ""], ["Rau", "Anita", ""], ["Wu", "Tong", ""], ["Blumberg", "Stefano B.", ""], ["Kanber", "Baris", ""], ["Tariq", "Maira", ""], ["Garcia", "Maria Del Mar Estarellas", ""], ["Zor", "Cemre", ""], ["Nikitichev", "Daniil I.", ""], ["Mourao-Miranda", "Janaina", ""], ["Oxtoby", "Neil P.", ""]]}, {"id": "1905.10834", "submitter": "Neil Oxtoby", "authors": "Neil P. Oxtoby, Fabio S. Ferreira, Agoston Mihalik, Tong Wu, Mikael\n  Brudfors, Hongxiang Lin, Anita Rau, Stefano B. Blumberg, Maria Robu, Cemre\n  Zor, Maira Tariq, Maria Del Mar Estarellas Garcia, Baris Kanber, Daniil I.\n  Nikitichev, Janaina Mourao-Miranda", "title": "ABCD Neurocognitive Prediction Challenge 2019: Predicting individual\n  residual fluid intelligence scores from cortical grey matter morphology", "comments": "8 pages plus references, 3 figures, 2 tables. Submission to the ABCD\n  Neurocognitive Prediction Challenge at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predicted residual fluid intelligence scores from T1-weighted MRI data\navailable as part of the ABCD NP Challenge 2019, using morphological similarity\nof grey-matter regions across the cortex. Individual structural covariance\nnetworks (SCN) were abstracted into graph-theory metrics averaged over nodes\nacross the brain and in data-driven communities/modules. Metrics included\ndegree, path length, clustering coefficient, centrality, rich club coefficient,\nand small-worldness. These features derived from the training set were used to\nbuild various regression models for predicting residual fluid intelligence\nscores, with performance evaluated both using cross-validation within the\ntraining set and using the held-out validation set. Our predictions on the test\nset were generated with a support vector regression model trained on the\ntraining set. We found minimal improvement over predicting a zero residual\nfluid intelligence score across the sample population, implying that structural\ncovariance networks calculated from T1-weighted MR imaging data provide little\ninformation about residual fluid intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:38:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Oxtoby", "Neil P.", ""], ["Ferreira", "Fabio S.", ""], ["Mihalik", "Agoston", ""], ["Wu", "Tong", ""], ["Brudfors", "Mikael", ""], ["Lin", "Hongxiang", ""], ["Rau", "Anita", ""], ["Blumberg", "Stefano B.", ""], ["Robu", "Maria", ""], ["Zor", "Cemre", ""], ["Tariq", "Maira", ""], ["Garcia", "Maria Del Mar Estarellas", ""], ["Kanber", "Baris", ""], ["Nikitichev", "Daniil I.", ""], ["Mourao-Miranda", "Janaina", ""]]}, {"id": "1905.10856", "submitter": "Marta Regis", "authors": "M. Regis, L.M. Eerik\\\"ainen, R. Haakma, E.R. van den Heuvel, and P.\n  Serra", "title": "Robust probabilistic modeling of photoplethysmography signals with\n  application to the classification of premature beats", "comments": "24 pages, 43 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a robust approach to model photoplethysmography\n(PPG) signals. After decomposing the signal into two components, we focus the\nanalysis on the pulsatile part, related to cardiac information. The goal is to\nenable a deeper understanding of the information contained in the pulse shape,\ntogether with that derived from the rhythm. Our approach combines functional\ndata analysis with a state space representation and guarantees fitting\nrobustness and flexibility on stationary signals, without imposing a priori\ninformation on the waveform and heart rhythm. With a Bayesian approach, we\nlearn the distribution of the parameters, used for understanding and monitoring\nPPG signals. The model can be used for data compression, for inferring medical\nparameters and to understand condition-related waveform characteristics. In\nparticular, we detail a procedure for the detection of premature contractions\nbased on the residuals of the fit. This method can handle both atrial and\nventricular premature contractions, and classify the type by only using\ninformation from the model fit.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 18:52:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Regis", "M.", ""], ["Eerik\u00e4inen", "L. M.", ""], ["Haakma", "R.", ""], ["Heuvel", "E. R. van den", ""], ["Serra", "P.", ""]]}, {"id": "1905.11010", "submitter": "Adam Farooq", "authors": "Adam Farooq, Yordan P. Raykov, Luc Evers, Max A. Little", "title": "Adaptive probabilistic principal component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the linear Gaussian latent variable model as a starting point we relax\nsome of the constraints it imposes by deriving a nonparametric latent feature\nGaussian variable model. This model introduces additional discrete latent\nvariables to the original structure. The Bayesian nonparametric nature of this\nnew model allows it to adapt complexity as more data is observed and project\neach data point onto a varying number of subspaces. The linear relationship\nbetween the continuous latent and observed variables make the proposed model\nstraightforward to interpret, resembling a locally adaptive probabilistic PCA\n(A-PPCA). We propose two alternative Gibbs sampling procedures for inference in\nthe new model and demonstrate its applicability on sensor data for passive\nhealth monitoring.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 07:18:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Farooq", "Adam", ""], ["Raykov", "Yordan P.", ""], ["Evers", "Luc", ""], ["Little", "Max A.", ""]]}, {"id": "1905.11096", "submitter": "Juli\\'an Urbano", "authors": "Juli\\'an Urbano, Harlley Lima, Alan Hanjalic", "title": "Statistical Significance Testing in Information Retrieval: An Empirical\n  Analysis of Type I, Type II and Type III Errors", "comments": "10 pages, 6 figures, SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331259", "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance testing is widely accepted as a means to assess how\nwell a difference in effectiveness reflects an actual difference between\nsystems, as opposed to random noise because of the selection of topics.\nAccording to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is\nthe most popular choice among IR researchers. However, previous work has\nsuggested computer intensive tests like the bootstrap or the permutation test,\nbased mainly on theoretical arguments. On empirical grounds, others have\nsuggested non-parametric alternatives such as the Wilcoxon test. Indeed, the\nquestion of which tests we should use has accompanied IR and related fields for\ndecades now. Previous theoretical studies on this matter were limited in that\nwe know that test assumptions are not met in IR experiments, and empirical\nstudies were limited in that we do not have the necessary control over the null\nhypotheses to compute actual Type I and Type II error rates under realistic\nconditions. Therefore, not only is it unclear which test to use, but also how\nmuch trust we should put in them. In contrast to past studies, in this paper we\nemploy a recent simulation methodology from TREC data to go around these\nlimitations. Our study comprises over 500 million p-values computed for a range\nof tests, systems, effectiveness measures, topic set sizes and effect sizes,\nand for both the 2-tail and 1-tail cases. Having such a large supply of IR\nevaluation data with full knowledge of the null hypotheses, we are finally in a\nposition to evaluate how well statistical significance tests really behave with\nIR data, and make sound recommendations for practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:02:29 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 22:18:34 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Urbano", "Juli\u00e1n", ""], ["Lima", "Harlley", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1905.11148", "submitter": "Etienne Boursier", "authors": "Etienne Boursier and Vianney Perchet", "title": "Utility/Privacy Trade-off through the lens of Optimal Transport", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic information is valuable either by remaining private (for instance\nif it is sensitive) or, on the other hand, by being used publicly to increase\nsome utility. These two objectives are antagonistic and leaking this\ninformation might be more rewarding than concealing it. Unlike classical\nsolutions that focus on the first point, we consider instead agents that\noptimize a natural trade-off between both objectives. We formalize this as an\noptimization problem where the objective mapping is regularized by the amount\nof information revealed to the adversary (measured as a divergence between the\nprior and posterior on the private knowledge). Quite surprisingly, when\ncombined with the entropic regularization, the Sinkhorn loss naturally emerges\nin the optimization objective, making it efficiently solvable. We apply these\ntechniques to preserve some privacy in online repeated auctions.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:46:42 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 07:48:30 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 10:11:54 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Boursier", "Etienne", ""], ["Perchet", "Vianney", ""]]}, {"id": "1905.11586", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev", "title": "Rare Failure Prediction via Event Matching for Aerospace Applications", "comments": "7 pages, 8 figures, 1 table", "journal-ref": "3rd International Conference on Circuits, System and Simulation\n  (ICCSS 2019), 2019", "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a problem of failure prediction in the context of\npredictive maintenance applications. We present a new approach for rare\nfailures prediction, based on a general methodology, which takes into account\npeculiar properties of technical systems. We illustrate the applicability of\nthe method on the real-world test cases from aircraft operations.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 03:03:18 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Burnaev", "Evgeny", ""]]}, {"id": "1905.11779", "submitter": "Andr\\'e Mas", "authors": "Vianney Bruned, Alice Cleynen, Andr\\'e Mas and Sylvain Wlodarczyck", "title": "Evaluation of mineralogy per geological layers by Approximate Bayesian\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology to perform mineralogic inversion from wellbore\nlogs based on a Bayesian linear regression model. Our method essentially relies\non three steps. The first step makes use of Approximate Bayesian Computation\n(ABC) and selects from the Bayesian generator a set of candidates-volumes\ncorresponding closely to the wellbore data responses. The second step gathers\nthese candidates through a density-based clustering algorithm. A mineral\nscenario is assigned to each cluster through direct mineralogical inversion,\nand we provide a confidence estimate for each lithological hypothesis. The\nadvantage of this approach is to explore all possible mineralogy hypotheses\nthat match the wellbore data. This pipeline is tested on both synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:48:51 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Bruned", "Vianney", ""], ["Cleynen", "Alice", ""], ["Mas", "Andr\u00e9", ""], ["Wlodarczyck", "Sylvain", ""]]}, {"id": "1905.11824", "submitter": "Soham Deshmukh", "authors": "Soham Deshmukh, Rahul Rade, Dr. Faruk Kazi", "title": "Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber threat intelligence is one of the emerging areas of focus in\ninformation security. Much of the recent work has focused on rule-based methods\nand detection of network attacks using Intrusion Detection algorithms. In this\npaper we propose a framework for inspecting and modelling the behavioural\naspect of an attacker to obtain better insight predictive power on his future\nactions. For modelling we propose a novel semi-supervised algorithm called\nFusion Hidden Markov Model (FHMM) which is more robust to noise, requires\ncomparatively less training time, and utilizes the benefits of ensemble\nlearning to better model temporal relationships in data. This paper evaluates\nthe performances of FHMM and compares it with both traditional algorithms like\nMarkov Chain, Hidden Markov Model (HMM) and recently developed Deep Recurrent\nNeural Network (Deep RNN) architectures. We conduct the experiments on dataset\nconsisting of real data attacks on a Cowrie honeypot system. FHMM provides\naccuracy comparable to deep RNN architectures at significant lower training\ntime. Given these experimental results, we recommend using FHMM for modelling\ndiscrete temporal data for significantly faster training and better performance\nthan existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:56:44 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 19:56:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Deshmukh", "Soham", ""], ["Rade", "Rahul", ""], ["Kazi", "Dr. Faruk", ""]]}, {"id": "1905.11875", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Dani\\\"el Lakens", "title": "Can we disregard the whole model? Omnibus non-inferiority testing for\n  $R^{2}$ in multivariable linear regression and $\\hat{\\eta}^{2}$ in ANOVA", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining a lack of association between an outcome variable and a number of\ndifferent explanatory variables is frequently necessary in order to disregard a\nproposed model (i.e., to confirm the lack of an association between an outcome\nand predictors). Despite this, the literature rarely offers information about,\nor technical recommendations concerning, the appropriate statistical\nmethodology to be used to accomplish this task. This paper introduces\nnon-inferiority tests for ANOVA and linear regression analyses, that correspond\nto the standard widely used $F$-test for $\\hat{\\eta}^2$ and $R^{2}$,\nrespectively. A simulation study is conducted to examine the type I error rates\nand statistical power of the tests, and a comparison is made with an\nalternative Bayesian testing approach. The results indicate that the proposed\nnon-inferiority test is a potentially useful tool for 'testing the null.'\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:16:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:15:05 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Campbell", "Harlan", ""], ["Lakens", "Dani\u00ebl", ""]]}, {"id": "1905.12013", "submitter": "Anna Heath", "authors": "Anna Heath, Natalia R. Kunst, Christopher Jackson, Mark Strong,\n  Fernando Alarid-Escudero, Jeremy D. Goldhaber-Fiebert, Gianluca Baio, Nicolas\n  A. Menzies, Hawre Jalal (on behalf of the Collaborative Network for Value of\n  Information (ConVOI))", "title": "Calculating the Expected Value of Sample Information in Practice:\n  Considerations from Three Case Studies", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investing efficiently in future research to improve policy decisions is an\nimportant goal. Expected Value of Sample Information (EVSI) can be used to\nselect the specific design and sample size of a proposed study by assessing the\nbenefit of a range of different studies. Estimating EVSI with the standard\nnested Monte Carlo algorithm has a notoriously high computational burden,\nespecially when using a complex decision model or when optimizing over study\nsample sizes and designs. Therefore, a number of more efficient EVSI\napproximation methods have been developed. However, these approximation methods\nhave not been compared and therefore their relative advantages and\ndisadvantages are not clear. A consortium of EVSI researchers, including the\ndevelopers of several approximation methods, compared four EVSI methods using\nthree previously published health economic models. The examples were chosen to\nrepresent a range of real-world contexts, including situations with multiple\nstudy outcomes, missing data, and data from an observational rather than a\nrandomized study. The computational speed and accuracy of each method were\ncompared, and the relative advantages and implementation challenges of the\nmethods were highlighted. In each example, the approximation methods took\nminutes or hours to achieve reasonably accurate EVSI estimates, whereas the\ntraditional Monte Carlo method took weeks. Specific methods are particularly\nsuited to problems where we wish to compare multiple proposed sample sizes,\nwhen the proposed sample size is large, or when the health economic model is\ncomputationally expensive. All the evaluated methods gave estimates similar to\nthose given by traditional Monte Carlo, suggesting that EVSI can now be\nefficiently computed with confidence in realistic examples.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:18:49 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Heath", "Anna", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Kunst", "Natalia R.", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Jackson", "Christopher", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Strong", "Mark", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Alarid-Escudero", "Fernando", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Goldhaber-Fiebert", "Jeremy D.", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Baio", "Gianluca", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Menzies", "Nicolas A.", "", "on behalf of the Collaborative Network for Value of\n  Information"], ["Jalal", "Hawre", "", "on behalf of the Collaborative Network for Value of\n  Information"]]}, {"id": "1905.12382", "submitter": "Amir Hossein Ansari", "authors": "Amir Hossein Ansari, Perumpillichira Joseph Cherian, Alexander\n  Caicedo, Anneleen Dereymaeker, Katrien Jansen, Leen De Wispelaere, Charlotte\n  Dielman, Jan Vervisch, Paul Govaert, Maarten De Vos, Gunnar Naulaers, Sabine\n  Van Huffel", "title": "NeoGuard: a public, online learning platform for neonatal seizures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seizures occur in the neonatal period more frequently than other periods of\nlife and usually denote the presence of serious brain dysfunction. The gold\nstandard for detecting seizures is based on visual inspection of continuous\nelectroencephalogram (cEEG) complemented by video analysis, performed by an\nexpert clinical neurophysiologist. Previous studies have reported varying\ndegree of agreement between expert EEG readers, with kappa coefficients ranging\nfrom 0.4 to 0.85, calling into question the validity of visual scoring. This\nvariability in visual scoring of neonatal seizures may be due to factors such\nas reader expertise and the nature of expressed patterns. One of the possible\nreasons for low inter-rater agreement is the absence of any benchmark for the\nEEG readers to be able to compare their opinions. One way to develop this is to\nuse a shared multi-center neonatal seizure database and use the inputs from\nmultiple experts. This will also improve the teaching of trainees, and help to\navoid potential bias from a single expert's opinion. In this paper, we\nintroduce and explain the NeoGuard public learning platform that can be used by\ntrainees, tutors, and expert EEG readers who are interested to test their\nknowledge and learn from neonatal EEG-polygraphic segments scored by several\nexpert EEG readers. For this platform, 1919 clinically relevant segments,\ntotaling 280h, recorded from 71 term neonates in two centers, including a wide\nvariety of seizures and artifacts were used. These segments were scored by 4\nEEG readers from three different centers. Users of this platform can score an\narbitrary number of segments and then test their scoring with the experts'\nopinions. The kappa and joint probability of agreement, is then shown as\ninter-rater agreement metrics between the user and each of the experts. The\nplatform is publicly available at the NeoGuard website (www.neoguard.net).\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:45:38 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ansari", "Amir Hossein", ""], ["Cherian", "Perumpillichira Joseph", ""], ["Caicedo", "Alexander", ""], ["Dereymaeker", "Anneleen", ""], ["Jansen", "Katrien", ""], ["De Wispelaere", "Leen", ""], ["Dielman", "Charlotte", ""], ["Vervisch", "Jan", ""], ["Govaert", "Paul", ""], ["De Vos", "Maarten", ""], ["Naulaers", "Gunnar", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1905.12684", "submitter": "Geoffrey Peterson", "authors": "Geoffrey Colin Lee Peterson and Joseph Guinness and Adam Terando and\n  Brian J. Reich", "title": "Mean-dependent nonstationary spatial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonstationarity is a major challenge in analyzing spatial data. For example,\ndaily precipitation measurements may have increased variability and decreased\nspatial smoothness in areas with high mean rainfall. Common nonstationary\ncovariance models introduce parameters specific to each location, giving a\nhighly-parameterized model which is difficult to fit. We develop a\nnonstationary spatial model that uses the mean to determine the covariance in a\nregion, resulting in a far simpler, albeit more specialized, model. We explore\ninferential and predictive properties of the model under various simulated data\nsituations. We show that this model in certain circumstances improves\npredictions compared to a standard stationary spatial model. We further propose\na computationally efficient approximation that has comparable predictive\naccuracy. We also develop a test for nonstationary data and show it reliably\nidentifies nonstationarity. We apply these methods to daily precipitation in\nPuerto Rico.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:16:44 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Peterson", "Geoffrey Colin Lee", ""], ["Guinness", "Joseph", ""], ["Terando", "Adam", ""], ["Reich", "Brian J.", ""]]}, {"id": "1905.12915", "submitter": "Deniz Sargun", "authors": "Deniz Sargun and C. Emre Koksal", "title": "Separating an Outlier from a Change", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the change detection problem with an unknown post-change\ndistribution. Under this constraint, the unknown change in the distribution of\nobservations may occur in many ways without much structure on the observations,\nwhereas, before the change point, a false alarm (outlier) is highly structured,\nfollowing a particular sample path. We first characterize these likely events\nfor the deviation and propose a method to test the empirical distribution,\nrelative to the most likely way for it to occur as an outlier. We benchmark our\nmethod with finite moving average (FMA) and generalized likelihood ratio tests\n(GLRT) under 4 different performance criteria including the run time time\ncomplexity. Finally, we apply our method on economic market indicators and\nclimate data. Our method successfully captures the regime shifts during times\nof historical significance for the markets and identifies the current climate\nchange phenomenon to be a highly likely regime shift rather than a random\nevent.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 08:50:20 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 21:06:26 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 15:39:59 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 23:21:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Sargun", "Deniz", ""], ["Koksal", "C. Emre", ""]]}, {"id": "1905.13408", "submitter": "Zhenwei Luo", "authors": "Zhenwei Luo", "title": "Improving the resolution of Cryo-EM single particle analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We presented a new 3D refinement method for Cryo-EM single particle analysis\nwhich can improve the resolution of final electron density map in this paper.\nWe proposed to enforce both sparsity and smoothness to improve the regularity\nof electron density map in the refinement process. To achieve this goal, we\ndesigned a novel type of real space penalty function and incorporated it into\nthe refinement process. We bridged the backprojection step with local kernel\nregression, thus enabling us to embed the 3D model in reproducing kernel\nHilbert space using specific kernels. We also proposed a first order method to\nsolve the resulting optimization problem and implemented it efficiently with\nCUDA. We compared the performance of our new method with respect to the\ntraditional method on real datasets using a set of widely used metrics for\nCryo-EM model validation. We demonstrated that our method outperforms the\ntraditional method in terms of those metrics. The implementation of our method\ncan be found at https://github.com/alncat/cryoem.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 04:15:04 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 02:49:06 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 19:44:24 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 03:12:48 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 17:56:12 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Luo", "Zhenwei", ""]]}, {"id": "1905.13550", "submitter": "Pei Du", "authors": "Pei Du, Jianzhou Wang, Yan Hao, Tong Niu, Wendong Yang", "title": "A novel hybrid model based on multi-objective Harris hawks optimization\n  algorithm for daily PM2.5 and PM10 forecasting", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High levels of air pollution may seriously affect people's living environment\nand even endanger their lives. In order to reduce air pollution concentrations,\nand warn the public before the occurrence of hazardous air pollutants, it is\nurgent to design an accurate and reliable air pollutant forecasting model.\nHowever, most previous research have many deficiencies, such as ignoring the\nimportance of predictive stability, and poor initial parameters and so on,\nwhich have significantly effect on the performance of air pollution prediction.\nTherefore, to address these issues, a novel hybrid model is proposed in this\nstudy. Specifically, a powerful data preprocessing techniques is applied to\ndecompose the original time series into different modes from low- frequency to\nhigh- frequency. Next, a new multi-objective algorithm called MOHHO is first\ndeveloped in this study, which are introduced to tune the parameters of ELM\nmodel with high forecasting accuracy and stability for air pollution series\nprediction, simultaneously. And the optimized ELM model is used to perform the\ntime series prediction. Finally, a scientific and robust evaluation system\nincluding several error criteria, benchmark models, and several experiments\nusing six air pollutant concentrations time series from three cities in China\nis designed to perform a compressive assessment for the presented hybrid\nforecasting model. Experimental results indicate that the proposed hybrid model\ncan guarantee a more stable and higher predictive performance compared to\nothers, whose superior prediction ability may help to develop effective plans\nfor air pollutant emissions and prevent health problems caused by air\npollution.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:33:59 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Du", "Pei", ""], ["Wang", "Jianzhou", ""], ["Hao", "Yan", ""], ["Niu", "Tong", ""], ["Yang", "Wendong", ""]]}, {"id": "1905.13598", "submitter": "Ayokunle Damilola Familua Dr", "authors": "Ayokunle Damilola Familua", "title": "A Block Diagonal Markov Model for Indoor Software-Defined Power Line\n  Communication", "comments": "Conference Paper with 9 pages, 6 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Semi-Hidden Markov Model (SHMM) for bursty error channels is defined by a\nstate transition probability matrix $A$, a prior probability vector $\\Pi$, and\nthe state dependent output symbol error probability matrix $B$. Several\nprocesses are utilized for estimating $A$, $\\Pi$ and $B$ from a given\nempirically obtained or simulated error sequence. However, despite placing some\nrestrictions on the underlying Markov model structure, we still have a\ncomputationally intensive estimation procedure, especially given a large error\nsequence containing long burst of identical symbols. Thus, in this paper, we\nutilize under some moderate assumptions, a Markov model with random state\ntransition matrix $A$ equivalent to a unique Block Diagonal Markov model with\nstate transition matrix $\\Lambda$ to model an indoor software-defined power\nline communication system. A computationally efficient modified Baum-Welch\nalgorithm for estimation of $\\Lambda$ given an experimentally obtained error\nsequence from the indoor PLC channel is utilized. Resulting Equivalent Block\nDiagonal Markov models assist designers to accelerate and facilitate the\nprocedure of novel PLC systems design and evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:35:05 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Familua", "Ayokunle Damilola", ""]]}, {"id": "1905.13668", "submitter": "Jens Schreiber", "authors": "Jens Schreiber, Artjom Buschin, Bernhard Sick", "title": "Influences in Forecast Errors for Wind and Photovoltaic Power: A Study\n  on Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing importance of forecasts of renewable energy, current\nplanning studies only address a general estimate of the forecast quality to be\nexpected and selected forecast horizons.\n  However, these estimates allow only a limited and highly uncertain use in the\nplanning of electric power distribution. More reliable planning processes\nrequire considerably more information about future forecast quality.\n  In this article, we present an in-depth analysis and comparison of\ninfluencing factors regarding uncertainty in wind and photovoltaic power\nforecasts, based on four different machine learning (ML) models.\n  In our analysis, we found substantial differences in uncertainty depending on\nML models, data coverage, and seasonal patterns that have to be considered in\nfuture planning studies.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 15:14:29 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Schreiber", "Jens", ""], ["Buschin", "Artjom", ""], ["Sick", "Bernhard", ""]]}]