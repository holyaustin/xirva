[{"id": "2010.00042", "submitter": "Kerem Can Tezcan", "authors": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "title": "Sampling possible reconstructions of undersampled acquisitions in MR\n  imaging", "comments": "Main article and appendix together. GIFs can be found on\n  https://polybox.ethz.ch/index.php/s/3DPrRoYQnyzANAF", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undersampling the k-space during MR acquisitions saves time, however results\nin an ill-posed inversion problem, leading to an infinite set of images as\npossible solutions. Traditionally, this is tackled as a reconstruction problem\nby searching for a single \"best\" image out of this solution set according to\nsome chosen regularization or prior. This approach, however, misses the\npossibility of other solutions and hence ignores the uncertainty in the\ninversion process. In this paper, we propose a method that instead returns\nmultiple images which are possible under the acquisition model and the chosen\nprior. To this end, we introduce a low dimensional latent space and model the\nposterior distribution of the latent vectors given the acquisition data in\nk-space, from which we can sample in the latent space and obtain the\ncorresponding images. We use a variational autoencoder for the latent model and\nthe Metropolis adjusted Langevin algorithm for the sampling. This approach\nallows us to obtain multiple possible images and capture the uncertainty in the\ninversion process under the used prior. We evaluate our method on images from\nthe Human Connectome Project dataset as well as in-house measured multi-coil\nimages and compare to two different methods. The results indicate that the\nproposed method is capable of producing images that match the ground truth in\nregions where acquired k-space data is informative and construct different\npossible reconstructions, which show realistic structural variations, in\nregions where acquired k-space data is not informative.\n  Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,\ninverse problems, sampling, MCMC, deep learning, unsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:20:06 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tezcan", "Kerem C.", ""], ["Baumgartner", "Christian F.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2010.00050", "submitter": "Ian Dryden", "authors": "Katie E. Severn, Ian L. Dryden and Simon P. Preston", "title": "Non-parametric regression for networks", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are becoming increasingly available, and so there is a need to\ndevelop suitable methodology for statistical analysis. Networks can be\nrepresented as graph Laplacian matrices, which are a type of manifold-valued\ndata. Our main objective is to estimate a regression curve from a sample of\ngraph Laplacian matrices conditional on a set of Euclidean covariates, for\nexample in dynamic networks where the covariate is time. We develop an adapted\nNadaraya-Watson estimator which has uniform weak consistency for estimation\nusing Euclidean and power Euclidean metrics. We apply the methodology to the\nEnron email corpus to model smooth trends in monthly networks and highlight\nanomalous networks. Another motivating application is given in corpus\nlinguistics, which explores trends in an author's writing style over time based\non word co-occurrence networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:30:50 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Severn", "Katie E.", ""], ["Dryden", "Ian L.", ""], ["Preston", "Simon P.", ""]]}, {"id": "2010.00212", "submitter": "Jean-Bernard Chatelain", "authors": "Jean Bernard Chatelain, Kirsten Ralf", "title": "How Macroeconomists Lost Control of Stabilization Policy: Towards Dark\n  Ages", "comments": null, "journal-ref": "The European Journal of the History of Economic Thought (2020)\n  27(6)", "doi": "10.1080/09672567.2020.1817119", "report-no": null, "categories": "q-fin.GN math.HO math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a study of the history of the transplant of mathematical tools\nusing negative feedback for macroeconomic stabilization policy from 1948 to\n1975 and the subsequent break of the use of control for stabilization policy\nwhich occurred from 1975 to 1993. New-classical macroeconomists selected a\nsubset of the tools of control that favored their support of rules against\ndiscretionary stabilization policy. The Lucas critique and Kydland and\nPrescott's time-inconsistency were over-statements that led to the \"dark ages\"\nof the prevalence of the stabilization-policy-ineffectiveness idea. These\nover-statements were later revised following the success of the Taylor rule.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 06:28:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Chatelain", "Jean Bernard", ""], ["Ralf", "Kirsten", ""]]}, {"id": "2010.00271", "submitter": "Felix Laumann", "authors": "Felix Laumann and Julius von K\\\"ugelgen and Mauricio Barahona", "title": "Kernel Two-Sample and Independence Tests for Non-Stationary Random\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sample and independence tests with the kernel-based MMD and HSIC have\nshown remarkable results on i.i.d. data and stationary random processes.\nHowever, these statistics are not directly applicable to non-stationary random\nprocesses, a prevalent form of data in many scientific disciplines. In this\nwork, we extend the application of MMD and HSIC to non-stationary settings by\nassuming access to independent realisations of the underlying random process.\nThese realisations - in the form of non-stationary time-series measured on the\nsame temporal grid - can then be viewed as i.i.d. samples from a multivariate\nprobability distribution, to which MMD and HSIC can be applied. We further show\nhow to choose suitable kernels over these high-dimensional spaces by maximising\nthe estimated test power with respect to the kernel hyper-parameters. In\nexperiments on synthetic data, we demonstrate superior performance of our\nproposed approaches in terms of test power when compared to current\nstate-of-the-art functional or multivariate two-sample and independence tests.\nFinally, we employ our methods on a real socio-economic dataset as an example\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:29:51 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:21:56 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 09:32:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Laumann", "Felix", ""], ["von K\u00fcgelgen", "Julius", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2010.00300", "submitter": "Stefan T. Radev", "authors": "Stefan T. Radev, Frederik Graw, Simiao Chen, Nico T. Mutters, Vanessa\n  M. Eichel, Till B\\\"arnighausen and Ullrich K\\\"othe", "title": "Model-based Bayesian inference of disease outbreak dynamics with\n  invertible neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mathematical models in epidemiology strive to describe the dynamics and\nimportant characteristics of infectious diseases. Apart from their scientific\nmerit, these models are often used to inform political decisions and\ninterventional measures during an ongoing outbreak. Since high-fidelity models\nare often quite complex and analytically intractable, their applicability to\nreal data depends on powerful estimation algorithms. Moreover, uncertainty\nquantification in such models is far from trivial, and different types of\nuncertainty are often confounded. With this work, we introduce a novel coupling\nbetween epidemiological models and specialized neural network architectures.\nThis coupling results in a powerful Bayesian inference framework capable of\nprincipled uncertainty quantification and efficient amortized inference once\nthe networks have been trained on simulations from an arbitrarily complex\nmodel. We illustrate the utility of our framework by applying it to real\nCovid-19 cases from entire Germany and German federal states.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:01:49 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:54:06 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 12:32:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Radev", "Stefan T.", ""], ["Graw", "Frederik", ""], ["Chen", "Simiao", ""], ["Mutters", "Nico T.", ""], ["Eichel", "Vanessa M.", ""], ["B\u00e4rnighausen", "Till", ""], ["K\u00f6the", "Ullrich", ""]]}, {"id": "2010.00435", "submitter": "Dong Quan Nguyen", "authors": "Dong Quan Ngoc Nguyen, Lin Xing, and Lizhen Lin", "title": "Community detection, pattern recognition, and hypergraph-based learning:\n  approaches using metric geometry and persistent homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph data appear and are hidden in many places in the modern age. They\nare data structure that can be used to model many real data examples since\ntheir structures contain information about higher order relations among data\npoints. One of the main contributions of our paper is to introduce a new\ntopological structure to hypergraph data which bears a resemblance to a usual\nmetric space structure. Using this new topological space structure of\nhypergraph data, we propose several approaches to study community detection\nproblem, detecting persistent features arising from homological structure of\nhypergraph data. Also based on the topological space structure of hypergraph\ndata introduced in our paper, we introduce a modified nearest neighbors methods\nwhich is a generalization of the classical nearest neighbors methods from\nmachine learning. Our modified nearest neighbors methods have an advantage of\nbeing very flexible and applicable even for discrete structures as in\nhypergraphs. We then apply our modified nearest neighbors methods to study sign\nprediction problem in hypegraph data constructed using our method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 21:20:12 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Nguyen", "Dong Quan Ngoc", ""], ["Xing", "Lin", ""], ["Lin", "Lizhen", ""]]}, {"id": "2010.00534", "submitter": "Christophe Folly", "authors": "Christophe L. Folly and Garyfallos Konstantinoudis and Antonella\n  Mazzei-Abba and Christian Kreis and Benno Bucher and Reinhard Furrer and Ben\n  D. Spycher", "title": "Bayesian spatial modelling of terrestrial radiation in Switzerland", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The geographic variation of terrestrial radiation can be exploited in\nepidemiological studies of the health effects of protracted low-dose exposure.\nVarious methods have been applied to derive maps of this variation. We aimed to\nconstruct a map of terrestrial radiation for Switzerland. We used airborne\n$\\gamma$-spectrometry measurements to model the ambient dose rates from\nterrestrial radiation through a Bayesian mixed-effects model and conducted\ninference using Integrated Nested Laplace Approximation (INLA). We predicted\nhigher levels of ambient dose rates in the alpine regions and Ticino compared\nwith the western and northern parts of Switzerland. We provide a map that can\nbe used for exposure assessment in epidemiological studies and as a baseline\nmap for assessing potential contamination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:35:15 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Folly", "Christophe L.", ""], ["Konstantinoudis", "Garyfallos", ""], ["Mazzei-Abba", "Antonella", ""], ["Kreis", "Christian", ""], ["Bucher", "Benno", ""], ["Furrer", "Reinhard", ""], ["Spycher", "Ben D.", ""]]}, {"id": "2010.00692", "submitter": "Tao Liu", "authors": "Tao Liu, Joseph W. Hogan, Lisa Wang, Shangxuan Zhang, Rami Kantor", "title": "Optimal Allocation of Gold Standard Testing under Constrained\n  Availability: Application to Assessment of HIV Treatment Failure", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2013.810149", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Health Organization (WHO) guidelines for monitoring the\neffectiveness of HIV treatment in resource-limited settings (RLS) are mostly\nbased on clinical and immunological markers (e.g., CD4 cell counts). Recent\nresearch indicates that the guidelines are inadequate and can result in high\nerror rates. Viral load (VL) is considered the \"gold standard\", yet its\nwidespread use is limited by cost and infrastructure. In this paper, we propose\na diagnostic algorithm that uses information from routinely-collected clinical\nand immunological markers to guide a selective use of VL testing for diagnosing\nHIV treatment failure, under the assumption that VL testing is available only\nat a certain portion of patient visits. Our algorithm identifies the patient\nsubpopulation, such that the use of limited VL testing on them minimizes a\npre-defined risk (e.g., misdiagnosis error rate). Diagnostic properties of our\nproposal algorithm are assessed by simulations. For illustration, data from the\nMiriam Hospital Immunology Clinic (RI, USA) are analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:31:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Liu", "Tao", ""], ["Hogan", "Joseph W.", ""], ["Wang", "Lisa", ""], ["Zhang", "Shangxuan", ""], ["Kantor", "Rami", ""]]}, {"id": "2010.00708", "submitter": "Fatemeh Ghanami", "authors": "Fatemeh Ghanami, Ghosheh Abed Hodtani, Branka Vucetic, Mahyar\n  Shirvanimoghaddam", "title": "Performance Analysis and Optimization of NOMA with HARQ for Short Packet\n  Communications in Massive IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the massive non-orthogonal multiple access (NOMA)\nwith hybrid automatic repeat request (HARQ) for short packet communications. To\nreduce the latency, each user can perform one re-transmission provided that the\nprevious packet was not decoded successfully. The system performance is\nevaluated for both coordinated and uncoordinated transmissions. We first\ndevelop a Markov model (MM) to analyze the system dynamics and characterize the\npacket error rate (PER) and throughput of each user in the coordinated\nscenario. The power levels are then optimized for two scenarios, including the\npower constrained and reliability constrained scenarios. A simple yet efficient\ndynamic cell planning is also designed for the uncoordinated scenario.\nNumerical results show that both coordinated and uncoordinated NOMA-HARQ with a\nlimited number of retransmissions can achieve the desired level of reliability\nwith the guaranteed latency using a proper power control strategy. Results also\nshow that NOMA-HARQ achieves a higher throughput compared to the orthogonal\nmultiple access scheme with HARQ under the same average received power\nconstraint at the base station.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 22:17:25 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ghanami", "Fatemeh", ""], ["Hodtani", "Ghosheh Abed", ""], ["Vucetic", "Branka", ""], ["Shirvanimoghaddam", "Mahyar", ""]]}, {"id": "2010.00736", "submitter": "Fei Lu", "authors": "Fei Lu", "title": "Data-driven model reduction for stochastic Burgers equations", "comments": "22 pages, 7 figures", "journal-ref": "Entropy 2020, 22(12), 1360", "doi": "10.3390/e22121360", "report-no": null, "categories": "math.NA cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of efficient parametric closure models for 1D stochastic\nBurgers equations. Casting it as statistical learning of the flow map, we\nderive the parametric form by representing the unresolved high wavenumber\nFourier modes as functionals of the resolved variables' trajectory. The reduced\nmodels are nonlinear autoregression (NAR) time series models, with coefficients\nestimated from data by least squares. The NAR models can accurately reproduce\nthe energy spectrum, the invariant densities, and the autocorrelations. Taking\nadvantage of the simplicity of the NAR models, we investigate maximal and\noptimal space-time reduction. Reduction in space dimension is unlimited, and\nNAR models with two Fourier modes can perform well. The NAR model's stability\nlimits time reduction, with a maximal time step smaller than that of the K-mode\nGalerkin system. We report a potential criterion for optimal space-time\nreduction: the NAR models achieve minimal relative error in the energy spectrum\nat the time step where the K-mode Galerkin system's mean CFL number agrees with\nthe full model's.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 00:53:51 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 05:40:48 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Lu", "Fei", ""]]}, {"id": "2010.00781", "submitter": "Chi-Kuang Yeh", "authors": "Chi-Kuang Yeh, Gregory Rice, Joel A. Dubin", "title": "Evaluating real-time probabilistic forecasts with application to\n  National Basketball Association outcome prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the goal of evaluating real-time forecasts of home team win\nprobabilities in the National Basketball Association, we develop new tools for\nmeasuring the quality of continuously updated probabilistic forecasts. This\nincludes introducing calibration surface plots, and simple graphical summaries\nof them, to evaluate at a glance whether a given continuously updated\nprobability forecasting method is well-calibrated, as well as developing\nstatistical tests and graphical tools to evaluate the skill, or relative\nperformance, of two competing continuously updated forecasting methods. These\ntools are studied by means of a Monte Carlo simulation study of simulated\nbasketball games, and demonstrated in an application to evaluate the\ncontinuously updated forecasts published by the United States-based\nmultinational sports network ESPN on its principle webpage {\\tt espn.com}. This\napplication lends statistical evidence that the forecasts published there are\nwell-calibrated, and exhibit improved skill over several na\\\"ive models, but do\nnot demonstrate significantly improved skill over simple logistic regression\nmodels based solely on a measurement of each teams' relative strength, and the\nevolving score difference throughout the game.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:16:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yeh", "Chi-Kuang", ""], ["Rice", "Gregory", ""], ["Dubin", "Joel A.", ""]]}, {"id": "2010.00794", "submitter": "Sayani Gupta", "authors": "Sayani Gupta, Rob J Hyndman, Dianne Cook and Antony Unwin", "title": "Visualizing probability distributions across bivariate cyclic temporal\n  granularities", "comments": "32 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconstructing a time index into time granularities can assist in exploration\nand automated analysis of large temporal data sets. This paper describes\nclasses of time deconstructions using linear and cyclic time granularities.\nLinear granularities respect the linear progression of time such as hours,\ndays, weeks and months. Cyclic granularities can be circular such as\nhour-of-the-day, quasi-circular such as day-of-the-month, and aperiodic such as\npublic holidays. The hierarchical structure of granularities creates a nested\nordering: hour-of-the-day and second-of-the-minute are single-order-up.\nHour-of-the-week is multiple-order-up, because it passes over day-of-the-week.\nMethods are provided for creating all possible granularities for a time index.\nA recommendation algorithm provides an indication whether a pair of\ngranularities can be meaningfully examined together (a \"harmony\"), or when they\ncannot (a \"clash\").\n  Time granularities can be used to create data visualizations to explore for\nperiodicities, associations and anomalies. The granularities form categorical\nvariables (ordered or unordered) which induce groupings of the observations.\nAssuming a numeric response variable, the resulting graphics are then displays\nof distributions compared across combinations of categorical variables.\n  The methods implemented in the open source R package `gravitas` are\nconsistent with a tidy workflow, with probability distributions examined using\nthe range of graphics available in `ggplot2`.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:47:43 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gupta", "Sayani", ""], ["Hyndman", "Rob J", ""], ["Cook", "Dianne", ""], ["Unwin", "Antony", ""]]}, {"id": "2010.00896", "submitter": "Sebastien Coube", "authors": "S\\'ebastien Coube and Beno\\^it Liquet", "title": "Improving performances of MCMC for Nearest Neighbor Gaussian Process\n  models with full data augmentation", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though Nearest Neighbor Gaussian Processes (NNGP) alleviate considerably\nMCMC implementation of Bayesian space-time models, they do not solve the\nconvergence problems caused by high model dimension. Frugal alternatives such\nas response or collapsed algorithms are an answer.gree Our approach is to keep\nfull data augmentation but to try and make it more efficient. We present two\nstrategies to do so. The first scheme is to pay a particular attention to the\nseemingly trivial fixed effects of the model. We show empirically that\nre-centering the latent field on the intercept critically improves chain\nbehavior. We extend this approach to other fixed effects that may interfere\nwith a coherent spatial field. We propose a simple method that requires no\ntuning while remaining affordable thanks to NNGP's sparsity. The second scheme\naccelerates the sampling of the random field using Chromatic samplers. This\nmethod makes long sequential simulation boil down to group-parallelized or\ngroup-vectorized sampling. The attractive possibility to parallelize NNGP\nlikelihood can therefore be carried over to field sampling. We present a R\nimplementation of our methods for Gaussian fields in the public repository\nhttps://github.com/SebastienCoube/Improving_NNGP_full_augmentation . An\nextensive vignette is provided. We run our implementation on two synthetic toy\nexamples along with the state of the art package spNNGP. Finally, we apply our\nmethod on a real data set of lead contamination in the United States of America\nmainland.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:51:08 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 10:42:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Coube", "S\u00e9bastien", ""], ["Liquet", "Beno\u00eet", ""]]}, {"id": "2010.00994", "submitter": "Dong Quan Nguyen", "authors": "Dong Quan Ngoc Nguyen and Lin Xing", "title": "A local geometry of hyperedges in hypergraphs, and its applications to\n  social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world datasets arising from social networks, there are hidden\nhigher order relations among data points which cannot be captured using graph\nmodeling. It is natural to use a more general notion of hypergraphs to model\nsuch social networks. In this paper, we introduce a new local geometry of\nhyperdges in hypergraphs which allows to capture higher order relations among\ndata points. Furthermore based on this new geometry, we also introduce new\nmethodology--the nearest neighbors method in hypergraphs--for analyzing\ndatasets arising from sociology.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 21:20:36 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Nguyen", "Dong Quan Ngoc", ""], ["Xing", "Lin", ""]]}, {"id": "2010.01101", "submitter": "Aria Khademi", "authors": "Christopher Seto and Aria Khademi and Corina Graif and Vasant G.\n  Honavar", "title": "Commuting Network Spillovers and COVID-19 Deaths Across US Counties", "comments": "Accepted for Presentation at The Population Association of America\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explored how population mobility flows form commuting networks\nacross US counties and influence the spread of COVID-19. We utilized 3-level\nmixed effects negative binomial regression models to estimate the impact of\nnetwork COVID-19 exposure on county confirmed cases and deaths over time. We\nalso conducted weighting-based analyses to estimate the causal effect of\nnetwork exposure. Results showed that commuting networks matter for COVID-19\ndeaths and cases, net of spatial proximity, socioeconomic, and demographic\nfactors. Different local racial and ethnic concentrations are also associated\nwith unequal outcomes. These findings suggest that commuting is an important\ncausal mechanism in the spread of COVID-19 and highlight the significance of\ninterconnected of communities. The results suggest that local level mitigation\nand prevention efforts are more effective when complemented by similar efforts\nin the network of connected places. Implications for research on inequality in\nhealth and flexible work arrangements are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:57:34 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 19:52:35 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Seto", "Christopher", ""], ["Khademi", "Aria", ""], ["Graif", "Corina", ""], ["Honavar", "Vasant G.", ""]]}, {"id": "2010.01297", "submitter": "Kim Phuc Tran", "authors": "K.D. Tran, Q.U.A Khaliq, A. A. Nadi, H Tran, and K.P. Tran", "title": "One-sided Shewhart control charts for monitoring the ratio of two normal\n  variables in Short Production Runs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the ratio of two normal random variables plays an important role\nin several manufacturing environments. For short production runs, however, the\ncontrol charts assumed infinite processes cannot function effectively to detect\nanomalies. In this paper, we tackle this problem by proposing two one-sided\nShewhart-type charts to monitor the ratio of two normal random variables for an\ninfinite horizon production. The statistical performance of the proposed charts\nis investigated using the truncated average run length as a performance measure\nin short production runs. In order to help the quality practitioner to\nimplement these control charts, we have provided ready-to-use tables of the\ncontrol limit parameters. An illustrative example from the food industry is\ngiven for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 07:38:21 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tran", "K. D.", ""], ["Khaliq", "Q. U. A", ""], ["Nadi", "A. A.", ""], ["Tran", "H", ""], ["Tran", "K. P.", ""]]}, {"id": "2010.01396", "submitter": "Joshua Chang", "authors": "Joshua C. Chang and Julia Porcino and Elizabeth K. Rasch and Larry\n  Tang", "title": "Regularized Bayesian calibration and scoring of the WD-FAB IRT model\n  improves predictive performance over maximum marginal likelihood", "comments": "Submitted QOL", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) is the statistical paradigm underlying a dominant\nfamily of generative probabilistic models for test responses, used to quantify\ntraits in individuals relative to target populations. The graded response model\n(GRM) is a particular IRT model that is used for ordered polytomous test\nresponses. Both the development and the application of the GRM and other IRT\nmodels require statistical decisions. For formulating these models\n(calibration), one needs to decide on methodologies for item selection,\ninference, and regularization. For applying these models (test scoring), one\nneeds to make similar decisions, often prioritizing computational tractability\nand/or interpretability. In many applications, such as in the Work Disability\nFunctional Assessment Battery (WD-FAB), tractability implies approximating an\nindividual's score distribution using estimates of mean and variance, and\nobtaining that score conditional on only point estimates of the calibrated\nmodel. In this manuscript, we evaluate the calibration and scoring of models\nunder this common use-case using Bayesian cross-validation. Applied to the\nWD-FAB responses collected for the National Institutes of Health, we assess the\npredictive power of implementations of the GRM based on their ability to yield,\non validation sets of respondents, estimates of latent ability with uncertainty\nthat are most predictive of patterns of item responses. IRT models in-general\nhave the concrete interpretation of latent abilities, combining with item\nparameters, to produce predictions of response patterns. Our main finding is\nthat regularized Bayesian calibration of the GRM outperforms the prior-free\nempirical Bayesian procedure of maximum marginal likelihood. We also motivate\nthe use of compactly supported priors in test scoring.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:25:12 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chang", "Joshua C.", ""], ["Porcino", "Julia", ""], ["Rasch", "Elizabeth K.", ""], ["Tang", "Larry", ""]]}, {"id": "2010.01413", "submitter": "Soheil Shirvani", "authors": "Soheil Shirvani, Anita Ghandehari, Hadi Moradi", "title": "Correlation between Air and Urban Travelling with New Confirmed Cases of\n  COVID-19 A Case Study", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 which has spread in Iran from February 19, 2020, has infected\n202,584 people and killed 9,507 people until June 20, 2020. The immediate\nsuggested solution to prevent the spread of this virus was to avoid traveling\naround. In this study, the correlation between traveling between cities with\nnew confirmed cases of COVID-19 in Iran is demonstrated. The data, used in the\nstudy, consisted of the daily inter-state traffic, air traffic data, and daily\nnew COVID-19 confirmed cases. The data is used to train a regression model and\nvoting was used to show the highest correlation between travels made between\ncities and new cases of COVID-19. Although the available data was very coarse\nand there was no detail of inner-city commute, an accuracy of 81% was achieved\nshowing a positive correlation between the number of inter-state travels and\nthe new cases of COVID-19. Consequently, the result suggests that one of the\nbest ways to avoid the spread of the virus is limiting or eliminating traveling\naround.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 19:17:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shirvani", "Soheil", ""], ["Ghandehari", "Anita", ""], ["Moradi", "Hadi", ""]]}, {"id": "2010.01550", "submitter": "Ali Caner T\\\"urkmen", "authors": "Ali Caner Turkmen, Tim Januschowski, Yuyang Wang and Ali Taylan Cemgil", "title": "Intermittent Demand Forecasting with Renewal Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermittency is a common and challenging problem in demand forecasting. We\nintroduce a new, unified framework for building intermittent demand forecasting\nmodels, which incorporates and allows to generalize existing methods in several\ndirections. Our framework is based on extensions of well-established\nmodel-based methods to discrete-time renewal processes, which can\nparsimoniously account for patterns such as aging, clustering and\nquasi-periodicity in demand arrivals. The connection to discrete-time renewal\nprocesses allows not only for a principled extension of Croston-type models,\nbut also for an natural inclusion of neural network based models---by replacing\nexponential smoothing with a recurrent neural network. We also demonstrate that\nmodeling continuous-time demand arrivals, i.e., with a temporal point process,\nis possible via a trivial extension of our framework. This leads to more\nflexible modeling in scenarios where data of individual purchase orders are\ndirectly available with granular timestamps. Complementing this theoretical\nadvancement, we demonstrate the efficacy of our framework for forecasting\npractice via an extensive empirical study on standard intermittent demand data\nsets, in which we report predictive accuracy in a variety of scenarios that\ncompares favorably to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:32:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Turkmen", "Ali Caner", ""], ["Januschowski", "Tim", ""], ["Wang", "Yuyang", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "2010.01568", "submitter": "Jens Braband", "authors": "Jens Braband and Hendrik Sch\\\"abe", "title": "Statistical Assessment of Safety Levels of Railway Operators", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the European Union Agency for Railways (ERA) has received a mandate\nfor 'the development of common safety methods for assessing the safety level\nand the safety performance of railway operators at national and Union level'.\nCurrently, several methods are under development. It is of interest how a\npossible candidate would behave and what would be the advantages and\ndisadvantages of a particular method. In this paper, we study a version of the\nprocedure. On the one hand side we analyze it based on the theory of\nmathematical statistics. As a result, we present a statistically efficient\nmethod the rate-ratio test based on a quantity that has smaller variance than\nthe quantity handled by the ERA. Then, we support the theoretical results with\nthe help of a simple simulation study in order to estimate failure\nprobabilities of the first and second kinds. We construct such alternative\ndistributions which the decision procedure cannot distinguish. We will show\nthat the use of procedures that are optimal in the sense of mathematical\nstatistics combined with the use of a characteristics that has small spread,\nhere the number of accidents, is advantageous.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:51:59 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Braband", "Jens", ""], ["Sch\u00e4be", "Hendrik", ""]]}, {"id": "2010.01844", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, Michael Stanley Smith, David J. Nott", "title": "Deep Distributional Time Series Models and the Probabilistic Forecasting\n  of Intraday Electricity Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) with rich feature vectors of past values can\nprovide accurate point forecasts for series that exhibit complex serial\ndependence. We propose two approaches to constructing deep time series\nprobabilistic models based on a variant of RNN called an echo state network\n(ESN). The first is where the output layer of the ESN has stochastic\ndisturbances and a shrinkage prior for additional regularization. The second\napproach employs the implicit copula of an ESN with Gaussian disturbances,\nwhich is a deep copula process on the feature space. Combining this copula with\na non-parametrically estimated marginal distribution produces a deep\ndistributional time series model. The resulting probabilistic forecasts are\ndeep functions of the feature vector and also marginally calibrated. In both\napproaches, Bayesian Markov chain Monte Carlo methods are used to estimate the\nmodels and compute forecasts. The proposed models are suitable for the complex\ntask of forecasting intraday electricity prices. Using data from the Australian\nNational Electricity Market, we show that our deep time series models provide\naccurate short term probabilistic price forecasts, with the copula model\ndominating. Moreover, the models provide a flexible framework for incorporating\nprobabilistic forecasts of electricity demand as additional features, which\nincreases upper tail forecast accuracy from the copula model significantly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:02:29 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 11:17:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Klein", "Nadja", ""], ["Smith", "Michael Stanley", ""], ["Nott", "David J.", ""]]}, {"id": "2010.01927", "submitter": "Konstantinos Fokianos", "authors": "Sergios Agapiou, Andreas Anastasiou, Anastassia Baxevani, Tasos\n  Christofides, Elisavet Constantinou, Georgios Hadjigeorgiou, Christos\n  Nicolaides, Georgios Nikolopoulos, and Konstantinos Fokianos", "title": "Modeling of Covid-19 Pandemic in Cyprus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Republic of Cyprus is a small island in the southeast of Europe and\nmember of the European Union. The first wave of COVID-19 in Cyprus started in\nearly March, 2020 (imported cases) and peaked in late March-early April. The\nhealth authorities responded rapidly and rigorously to the COVID-19 pandemic by\nscaling-up testing, increasing efforts to trace and isolate contacts of cases,\nand implementing measures such as closures of educational institutions, and\ntravel and movement restrictions. The pandemic was also a unique opportunity\nthat brought together experts from various disciplines including\nepidemiologists, clinicians, mathematicians, and statisticians. The aim of this\npaper is to present the efforts of this new, multidisciplinary research team in\nmodelling the COVID-19 pandemic in the Republic of Cyprus.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:33:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Agapiou", "Sergios", ""], ["Anastasiou", "Andreas", ""], ["Baxevani", "Anastassia", ""], ["Christofides", "Tasos", ""], ["Constantinou", "Elisavet", ""], ["Hadjigeorgiou", "Georgios", ""], ["Nicolaides", "Christos", ""], ["Nikolopoulos", "Georgios", ""], ["Fokianos", "Konstantinos", ""]]}, {"id": "2010.02071", "submitter": "Zheng Chen", "authors": "Jingjing Lyu, Yawen Hou and Zheng Chen", "title": "The use of restricted mean time lost under competing risks data", "comments": null, "journal-ref": "BMC Medical Research Methodology 2020. 20:197", "doi": "10.1186/s12874-020-01040-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Under competing risks, the commonly used sub-distribution hazard\nratio (SHR) is not easy to interpret clinically and is valid only under the\nproportional sub-distribution hazard (SDH) assumption. This paper introduces an\nalternative statistical measure: the restricted mean time lost (RMTL). Methods:\nFirst, the definition and estimation methods of the measures are introduced.\nSecond, based on the differences in RMTLs, a basic difference test (Diff) and a\nsupremum difference test (sDiff) are constructed. Then, the corresponding\nsample size estimation method is proposed. The statistical properties of the\nmethods and the estimated sample size are evaluated using Monte Carlo\nsimulations, and these methods are also applied to two real examples. Results:\nThe simulation results show that sDiff performs well and has relatively high\ntest efficiency in most situations. Regarding sample size calculation, sDiff\nexhibits good performance in various situations. The methods are illustrated\nusing two examples. Conclusions: RMTL can meaningfully summarize treatment\neffects for clinical decision making, which can then be reported with the SDH\nratio for competing risks data. The proposed sDiff test and the two calculated\nsample size formulas have wide applicability and can be considered in real data\nanalysis and trial design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:08:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lyu", "Jingjing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2010.02252", "submitter": "Bahman Rostami-Tabar", "authors": "Bahman Rostami-Tabar and Juan F. Rendon-Sanchez", "title": "Forecasting COVID-19 daily cases using phone call data", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to forecast COVID-19 related variables continues to be pressing as\nthe epidemic unfolds. Different efforts have been made, with compartmental\nmodels in epidemiology and statistical models such as AutoRegressive Integrated\nMoving Average (ARIMA), Exponential Smoothing (ETS) or computing intelligence\nmodels. These efforts have proved useful in some instances by allowing decision\nmakers to distinguish different scenarios during the emergency, but their\naccuracy has been disappointing, forecasts ignore uncertainties and less\nattention is given to local areas. In this study, we propose a simple Multiple\nLinear Regression model, optimised to use call data to forecast the number of\ndaily confirmed cases. Moreover, we produce a probabilistic forecast that\nallows decision makers to better deal with risk. Our proposed approach\noutperforms ARIMA, ETS and a regression model without call data, evaluated by\nthree point forecast error metrics, one prediction interval and two\nprobabilistic forecast accuracy measures. The simplicity, interpretability and\nreliability of the model, obtained in a careful forecasting exercise, is a\nmeaningful contribution to decision makers at local level who acutely need to\norganise resources in already strained health services. We hope that this model\nwould serve as a building block of other forecasting efforts that on the one\nhand would help front-line personal and decision makers at local level, and on\nthe other would facilitate the communication with other modelling efforts being\nmade at the national level to improve the way we tackle this pandemic and other\nsimilar future challenges.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:07:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Rostami-Tabar", "Bahman", ""], ["Rendon-Sanchez", "Juan F.", ""]]}, {"id": "2010.02332", "submitter": "Steven Winter", "authors": "Steven Winter, Zhengwu Zhang, and David Dunson", "title": "Multi-scale graph principal component analysis for connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain connectomics, the cortical surface is parcellated into different\nregions of interest (ROIs) prior to statistical analysis. The brain connectome\nfor each individual can then be represented as a graph, with the nodes\ncorresponding to ROIs and edges to connections between ROIs. Such a graph can\nbe summarized as an adjacency matrix, with each cell containing the strength of\nconnection between a pair of ROIs. These matrices are symmetric with the\ndiagonal elements corresponding to self-connections typically excluded. A major\ndisadvantage of such representations of the connectome is their sensitivity to\nthe chosen ROIs, including critically the number of ROIs and hence the scale of\nthe graph. As the scale becomes finer and more ROIs are used, graphs become\nincreasingly sparse. Clearly, the results of downstream statistical analyses\ncan be highly dependent on the chosen parcellation. To solve this problem, we\npropose a multi-scale graph factorization, which links together scale-specific\nfactorizations through a common set of individual-specific scores. These scores\nsummarize an individual's brain structure combining information across\nmeasurement scales. We obtain a simple and efficient algorithm for\nimplementation, and illustrate substantial advantages over single scale\napproaches in simulations and analyses of the Human Connectome Project dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:58:43 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Winter", "Steven", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David", ""]]}, {"id": "2010.02896", "submitter": "Laura Forastiere", "authors": "Eric M. Feltham, Laura Forastiere, Marcus Alexander, Nicholas A.\n  Christakis", "title": "No increase in COVID-19 mortality after the 2020 primary elections in\n  the USA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examined the impact of voting on the spread of COVID-19 after the US\nprimary elections held from March 17 to July 11, 2020 (1574 counties across 34\nstates). We estimated the average effect of treatment on the treated (ATT)\nusing a non-parametric, generalized difference-in-difference estimator with a\nmatching procedure for panel data. Separately, we estimated the time-varying\nreproduction number $R_t$ using a semi-mechanistic Bayesian hierarchical model\nat the state level. We found no evidence of a spike in COVID-19 deaths in the\nperiod immediately following the primaries. It is possible that elections can\nbe held safely, without necessarily contributing to spreading the epidemic.\nAppropriate precautionary measures that enforce physical distancing and\nmask-wearing are likely needed during general elections, with larger turnout or\ncolder weather.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:27:48 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 07:59:00 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Feltham", "Eric M.", ""], ["Forastiere", "Laura", ""], ["Alexander", "Marcus", ""], ["Christakis", "Nicholas A.", ""]]}, {"id": "2010.02935", "submitter": "Amir Shahmoradi", "authors": "Christopher Bryant, Joshua Alexander Osborne, Amir Shahmoradi", "title": "How unbiased statistical methods lead to biased scientific discoveries:\n  A case study of the Efron-Petrosian statistic applied to the\n  luminosity-redshift evolution of Gamma-Ray Bursts", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/stab1098", "report-no": null, "categories": "astro-ph.HE astro-ph.IM physics.ins-det stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods are frequently built upon assumptions that limit their\napplicability to certain problems and conditions. Failure to recognize these\nlimitations can lead to conclusions that may be inaccurate or biased. An\nexample of such methods is the non-parametric Efron-Petrosian test statistic\nused in the studies of truncated data. We argue and show how the inappropriate\nuse of this statistical method can lead to biased conclusions when the\nassumptions under which the method is valid do not hold. We do so by\nreinvestigating the evidence recently provided by multiple independent reports\non the evolution of the luminosity/energetics distribution of cosmological\nLong-duration Gamma-Ray Bursts (LGRBs) with redshift. We show that the effects\nof detection threshold has been likely significantly underestimated in the\nmajority of previous studies. This underestimation of detection threshold leads\nto severely-incomplete LGRB samples that exhibit strong apparent\nluminosity-redshift or energetics-redshift correlations. We further confirm our\nfindings by performing extensive Monte Carlo simulations of the cosmic rates\nand the luminosity/energy distributions of LGRBs and their detection process.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:00:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Bryant", "Christopher", ""], ["Osborne", "Joshua Alexander", ""], ["Shahmoradi", "Amir", ""]]}, {"id": "2010.03114", "submitter": "Enrique Saldarriaga", "authors": "Enrique M. Saldarriaga", "title": "HIV-prevalence mapping using Small Area Estimation in Kenya, Tanzania,\n  and Mozambique at the first sub-national level", "comments": "Pages: 10; Abstract: 221 words; Text: 2,076 words; Tables & Figures:\n  5; References: 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local estimates of HIV-prevalence provide information that can be used to\ntarget interventions and consequently increase the efficiency of the resources.\nThis closer-to-optimal allocation can lead to better health outcomes, including\nthe control of the disease spread, and for more people. Producing reliable\nestimates at smaller geographical levels can be challenging and careful\nconsideration of the nature of the data and the epidemiologic rational is\nneeded. In this paper, we use the DHS data phase V to estimate HIV prevalence\nat the first-subnational level in Kenya, Tanzania, and Mozambique. We fit the\ndata to a spatial random effect intrinsic conditional autoregressive (ICAR)\nmodel to smooth the outcome. We also use a sampling specification from a\nmultistage cluster design. We found that Nyanza (P=14.2%) and Nairobi (P=7.8%)\nin Kenya, Iringa (P=16.2%) and Dar es Salaam (P=10.1%) in Tanzania, and Gaza\n(P=13.7%) and Maputo City (P=12.7%) in Mozambique are the regions with the\nhighest prevalence of HIV, within country. Our results are based on\nstatistically rigorous methods that allowed us to obtain an accurate visual\nrepresentation of the HIV prevalence in the subset of African countries we\nchose. These results can help in identification and targeting of high-prevalent\nregions to increase the supply of healthcare services to reduce the spread of\nthe disease and increase the health quality of people living with HIV.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:22:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Saldarriaga", "Enrique M.", ""]]}, {"id": "2010.03408", "submitter": "Ivan Makhotin", "authors": "Ivan Makhotin, Denis Orlov, Dmitry Koroteev, Evgeny Burnaev, Aram\n  Karapetyan, Dmitry Antonenko", "title": "Machine learning for recovery factor estimation of an oil reservoir: a\n  tool for de-risking at a hydrocarbon asset evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well known oil recovery factor estimation techniques such as analogy,\nvolumetric calculations, material balance, decline curve analysis, hydrodynamic\nsimulations have certain limitations. Those techniques are time-consuming,\nrequire specific data and expert knowledge. Besides, though uncertainty\nestimation is highly desirable for this problem, the methods above do not\ninclude this by default. In this work, we present a data-driven technique for\noil recovery factor estimation using reservoir parameters and representative\nstatistics. We apply advanced machine learning methods to historical worldwide\noilfields datasets (more than 2000 oil reservoirs). The data-driven model might\nbe used as a general tool for rapid and completely objective estimation of the\noil recovery factor. In addition, it includes the ability to work with partial\ninput data and to estimate the prediction interval of the oil recovery factor.\nWe perform the evaluation in terms of accuracy and prediction intervals\ncoverage for several tree-based machine learning techniques in application to\nthe following two cases: (1) using parameters only related to geometry,\ngeology, transport, storage and fluid properties, (2) using an extended set of\nparameters including development and production data. For both cases model\nproved itself to be robust and reliable. We conclude that the proposed\ndata-driven approach overcomes several limitations of the traditional methods\nand is suitable for rapid, reliable and objective estimation of oil recovery\nfactor for hydrocarbon reservoir.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:33:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:15:34 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 16:46:00 GMT"}, {"version": "v4", "created": "Wed, 21 Apr 2021 15:15:43 GMT"}, {"version": "v5", "created": "Thu, 29 Apr 2021 09:31:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Makhotin", "Ivan", ""], ["Orlov", "Denis", ""], ["Koroteev", "Dmitry", ""], ["Burnaev", "Evgeny", ""], ["Karapetyan", "Aram", ""], ["Antonenko", "Dmitry", ""]]}, {"id": "2010.03551", "submitter": "Zhengfan Wang", "authors": "Zhengfan Wang, Miranda J. Fix, Lucia Hug, Anu Mishra, Danzhen You,\n  Hannah Blencowe, Jon Wakefield, Leontine Alkema", "title": "Estimating the Stillbirth Rate for 195 Countries Using A Bayesian Sparse\n  Regression Model with Temporal Smoothing", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of stillbirth rates globally is complicated because of the paucity\nof reliable data from countries where most stillbirths occur. We compiled data\nand developed a Bayesian hierarchical temporal sparse regression model for\nestimating stillbirth rates for all countries from 2000 to 2019. The model\ncombines covariates with a temporal smoothing process so that estimates are\ndata-driven in country-periods with high-quality data and deter-mined by\ncovariates for country-periods with limited or no data. Horseshoepriors are\nused to encourage sparseness. The model adjusts observations with alternative\nstillbirth definitions and accounts for bias in observations that are subject\nto non-sampling errors. In-sample goodness of fit and out-of-sample validation\nresults suggest that the model is reasonably well calibrated. The model is used\nby the UN Inter-agency Group for Child Mortality Estimation to monitor the\nstillbirth rate for all countries.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:53:10 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Zhengfan", ""], ["Fix", "Miranda J.", ""], ["Hug", "Lucia", ""], ["Mishra", "Anu", ""], ["You", "Danzhen", ""], ["Blencowe", "Hannah", ""], ["Wakefield", "Jon", ""], ["Alkema", "Leontine", ""]]}, {"id": "2010.03561", "submitter": "Ushnish Sengupta", "authors": "Ushnish Sengupta, Matt Amos, J. Scott Hosking, Carl Edward Rasmussen,\n  Matthew Juniper, Paul J. Young", "title": "Ensembling geophysical models with Bayesian Neural Networks", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of geophysical models improve projection accuracy and express\nuncertainties. We develop a novel data-driven ensembling strategy for combining\ngeophysical models using Bayesian Neural Networks, which infers\nspatiotemporally varying model weights and bias while accounting for\nheteroscedastic uncertainties in the observations. This produces more accurate\nand uncertainty-aware projections without sacrificing interpretability. Applied\nto the prediction of total column ozone from an ensemble of 15\nchemistry-climate models, we find that the Bayesian neural network ensemble\n(BayNNE) outperforms existing ensembling methods, achieving a 49.4% reduction\nin RMSE for temporal extrapolation, and a 67.4% reduction in RMSE for polar\ndata voids, compared to a weighted mean. Uncertainty is also\nwell-characterized, with 90.6% of the data points in our extrapolation\nvalidation dataset lying within 2 standard deviations and 98.5% within 3\nstandard deviations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:32:32 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Sengupta", "Ushnish", ""], ["Amos", "Matt", ""], ["Hosking", "J. Scott", ""], ["Rasmussen", "Carl Edward", ""], ["Juniper", "Matthew", ""], ["Young", "Paul J.", ""]]}, {"id": "2010.03700", "submitter": "Xiucai Ding", "authors": "Xiucai Ding, Dengdeng Yu, Zhengwu Zhang and Dehan Kong", "title": "Multivariate functional responses low rank regression with an\n  application to brain imaging data", "comments": "Canadian Journal of Statistics(accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multivariate functional responses low rank regression model with\npossible high dimensional functional responses and scalar covariates. By\nexpanding the slope functions on a set of sieve basis, we reconstruct the basis\ncoefficients as a matrix. To estimate these coefficients, we propose an\nefficient procedure using nuclear norm regularization. We also derive error\nbounds for our estimates and evaluate our method using simulations. We further\napply our method to the Human Connectome Project neuroimaging data to predict\ncortical surface motor task-evoked functional magnetic resonance imaging\nsignals using various clinical covariates to illustrate the usefulness of our\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 00:20:44 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ding", "Xiucai", ""], ["Yu", "Dengdeng", ""], ["Zhang", "Zhengwu", ""], ["Kong", "Dehan", ""]]}, {"id": "2010.03821", "submitter": "Xiaona Xia", "authors": "Xiaona Xia", "title": "Clustering Analysis of Interactive Learning Activities Based on Improved\n  BIRCH Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group tendency is a research branch of computer assisted learning. The\nconstruction of good learning behavior is of great significance to learners'\nlearning process and learning effect, and is the key basis of data-driven\neducation decision-making. Clustering analysis is an effective method for the\nstudy of group tendency. Therefore, it is necessary to obtain the online\nlearning behavior big data set of multi period and multi course, and describe\nthe learning behavior as multi-dimensional learning interaction activities.\nFirst of all, on the basis of data initialization and standardization, we\nlocate the classification conditions of data, realize the differentiation and\nintegration of learning behavior, and form multiple subsets of data to be\nclustered; secondly, according to the topological relevance and dependence\nbetween learning interaction activities, we design an improved algorithm of\nBIRCH clustering based on random walking strategy, which realizes the retrieval\nevaluation and data of key learning interaction activities; Thirdly, through\nthe calculation and comparison of several performance indexes, the improved\nalgorithm has obvious advantages in learning interactive activity clustering,\nand the clustering process and results are feasible and reliable. The\nconclusion of this study can be used for reference and can be popularized. It\nhas practical significance for the research of education big data and the\npractical application of learning analytics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:46:46 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Xia", "Xiaona", ""]]}, {"id": "2010.03828", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez and Mar\\'ia Durb\\'an and Paul\n  H.C. Eilers and Dae-Jin Lee and Francisco Gonzalez", "title": "Multidimensional Adaptive Penalised Splines with Application to Neurons'\n  Activity Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-spline models have achieved great popularity both in statistical and in\napplied research. A possible drawback of P-spline is that they assume a smooth\ntransition of the covariate effect across its whole domain. In some practical\napplications, however, it is desirable and needed to adapt smoothness locally\nto the data, and adaptive P-splines have been suggested. Yet, the extra\nflexibility afforded by adaptive P-spline models is obtained at the cost of a\nhigh computational burden, especially in a multidimensional setting.\nFurthermore, to the best of our knowledge, the literature lacks proposals for\nadaptive P-splines in more than two dimensions. Motivated by the need for\nanalysing data derived from experiments conducted to study neurons' activity in\nthe visual cortex, this work presents a novel locally adaptive anisotropic\nP-spline model in two (e.g., space) and three (space and time) dimensions.\nEstimation is based on the recently proposed SOP (Separation of Overlapping\nPrecision matrices) method, which provides the speed we look for. The practical\nperformance of the proposal is evaluated through simulations, and comparisons\nwith alternative methods are reported. In addition to the spatio-temporal\nanalysis of the data that motivated this work, we also discuss an application\nin two dimensions on the absenteeism of workers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:05:23 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:03:46 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durb\u00e1n", "Mar\u00eda", ""], ["Eilers", "Paul H. C.", ""], ["Lee", "Dae-Jin", ""], ["Gonzalez", "Francisco", ""]]}, {"id": "2010.03985", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Christopher K. Wikle", "title": "A higher-order singular value decomposition tensor emulator for\n  spatio-temporal simulators", "comments": null, "journal-ref": "Journal of Agricultural, Biological, and Environmental Statistics\n  2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce methodology to construct an emulator for environmental and\necological spatio-temporal processes that uses the higher order singular value\ndecomposition (HOSVD) as an extension of singular value decomposition (SVD)\napproaches to emulation. Some important advantages of the method are that it\nallows for the use of a combination of supervised learning methods (e.g.,\nrandom forests and Gaussian process regression) and also allows for the\nprediction of process values at spatial locations and time points that were not\nused in the training sample. The method is demonstrated with two applications:\nthe first is a periodic solution to a shallow ice approximation partial\ndifferential equation from glaciology, and second is an agent-based model of\ncollective animal movement. In both cases, we demonstrate the value of\ncombining different machine learning models for accurate emulation. In\naddition, in the agent-based model case we demonstrate the ability of the\ntensor emulator to successfully capture individual behavior in space and time.\nWe demonstrate via a real data example the ability to perform Bayesian\ninference in order to learn parameters governing collective animal behavior.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:31:54 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:31:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gopalan", "Giri", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "2010.04051", "submitter": "Niccol\\`o Dalmasso", "authors": "Niccol\\`o Dalmasso, Galen Vincent, Dorit Hammerling, Ann B. Lee", "title": "HECT: High-Dimensional Ensemble Consistency Testing for Climate Models", "comments": "Accepted at the Tackling Climate Change with Machine Learning\n  workshop at NeurIPS 2020, 6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate models play a crucial role in understanding the effect of\nenvironmental and man-made changes on climate to help mitigate climate risks\nand inform governmental decisions. Large global climate models such as the\nCommunity Earth System Model (CESM), developed by the National Center for\nAtmospheric Research, are very complex with millions of lines of code\ndescribing interactions of the atmosphere, land, oceans, and ice, among other\ncomponents. As development of the CESM is constantly ongoing, simulation\noutputs need to be continuously controlled for quality. To be able to\ndistinguish a \"climate-changing\" modification of the code base from a true\nclimate-changing physical process or intervention, there needs to be a\nprincipled way of assessing statistical reproducibility that can handle both\nspatial and temporal high-dimensional simulation outputs. Our proposed work\nuses probabilistic classifiers like tree-based algorithms and deep neural\nnetworks to perform a statistically rigorous goodness-of-fit test of\nhigh-dimensional spatio-temporal data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:16:16 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 22:48:17 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dalmasso", "Niccol\u00f2", ""], ["Vincent", "Galen", ""], ["Hammerling", "Dorit", ""], ["Lee", "Ann B.", ""]]}, {"id": "2010.04190", "submitter": "Amir Shahmoradi", "authors": "Shashank Kumbhare, Amir Shahmoradi", "title": "MatDRAM: A pure-MATLAB Delayed-Rejection Adaptive Metropolis-Hastings\n  Markov Chain Monte Carlo Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM cs.CE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms are widely used for stochastic\noptimization, sampling, and integration of mathematical objective functions, in\nparticular, in the context of Bayesian inverse problems and parameter\nestimation. For decades, the algorithm of choice in MCMC simulations has been\nthe Metropolis-Hastings (MH) algorithm. An advancement over the traditional\nMH-MCMC sampler is the Delayed-Rejection Adaptive Metropolis (DRAM). In this\npaper, we present MatDRAM, a stochastic optimization, sampling, and Monte Carlo\nintegration toolbox in MATLAB which implements a variant of the DRAM algorithm\nfor exploring the mathematical objective functions of arbitrary-dimensions, in\nparticular, the posterior distributions of Bayesian models in data science,\nMachine Learning, and scientific inference. The design goals of MatDRAM include\nnearly-full automation of MCMC simulations, user-friendliness,\nfully-deterministic reproducibility, and the restart functionality of\nsimulations. We also discuss the implementation details of a technique to\nautomatically monitor and ensure the diminishing adaptation of the proposal\ndistribution of the DRAM algorithm and a method of efficiently storing the\nresulting simulated Markov chains. The MatDRAM library is open-source,\nMIT-licensed, and permanently located and maintained as part of the ParaMonte\nlibrary at https://github.com/cdslaborg/paramonte.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:09:09 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kumbhare", "Shashank", ""], ["Shahmoradi", "Amir", ""]]}, {"id": "2010.04253", "submitter": "Nathan Wikle", "authors": "Nathan B. Wikle, Ephraim M. Hanks, Lucas R.F. Henneman, and Corwin M.\n  Zigler", "title": "A Mechanistic Model of Annual Sulfate Concentrations in the United\n  States", "comments": "21 pages, 5 figures. For associated code and supplementary material,\n  see https://github.com/nbwikle/mechanisticSO4-supp_material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mechanistic model to analyze the impact of sulfur dioxide\nemissions from coal-fired power plants on average sulfate concentrations in the\ncentral United States. A multivariate Ornstein-Uhlenbeck (OU) process is used\nto approximate the dynamics of the underlying space-time chemical transport\nprocess, and its distributional properties are leveraged to specify novel\nprobability models for spatial data (i.e., spatially-referenced data with no\ntemporal replication) that are viewed as either a snapshot or a time-averaged\nobservation of the OU process. Air pollution transport dynamics determine the\nmean and covariance structure of our atmospheric sulfate model, allowing us to\ninfer which process dynamics are driving observed air pollution concentrations.\nWe use these inferred dynamics to assess the regulatory impact of flue-gas\ndesulfurization (FGD) technologies on human exposure to sulfate aerosols.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:49:10 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wikle", "Nathan B.", ""], ["Hanks", "Ephraim M.", ""], ["Henneman", "Lucas R. F.", ""], ["Zigler", "Corwin M.", ""]]}, {"id": "2010.04326", "submitter": "Richmond Addo Danquah", "authors": "Richmond Addo Danquah", "title": "Handling Imbalanced Data: A Case Study for Binary Class Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For several years till date, the major issues in terms of solving for\nclassification problems are the issues of Imbalanced data. Because majority of\nthe machine learning algorithms by default assumes all data are balanced, the\nalgorithms do not take into consideration the distribution of the data sample\nclass. The results tend to be unsatisfactory and skewed towards the majority\nsample class distribution. This implies that the consequences as a result of\nusing a model built using an Imbalanced data without handling for the Imbalance\nin the data could be misleading both in practice and theory. Most researchers\nhave focused on the application of Synthetic Minority Oversampling Technique\n(SMOTE) and Adaptive Synthetic (ADASYN) Sampling Approach in handling data\nImbalance independently in their works and have failed to better explain the\nalgorithms behind these techniques with computed examples. This paper focuses\non both synthetic oversampling techniques and manually computes synthetic data\npoints to enhance easy comprehension of the algorithms. We analyze the\napplication of these synthetic oversampling techniques on binary classification\nproblems with different Imbalanced ratios and sample sizes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:04:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Danquah", "Richmond Addo", ""]]}, {"id": "2010.04485", "submitter": "Daniel Nevo", "authors": "Daniel Nevo and Malka Gorfine", "title": "Causal inference for semi-competing risks data", "comments": "35 pages, 3 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging challenge for time-to-event data is studying semi-competing\nrisks, namely when two event times are of interest: a non-terminal event time\n(e.g. age at disease diagnosis), and a terminal event time (e.g. age at death).\nThe non-terminal event is observed only if it precedes the terminal event,\nwhich may occur before or after the non-terminal event. Studying treatment or\nintervention effects on the dual event times is complicated because for some\nunits, the non-terminal event may occur under one treatment value but not under\nthe other. Until recently, existing approaches (e.g., the survivor average\ncausal effect) generally disregarded the time-to-event nature of both outcomes.\nMore recent research focused on principal strata effects within time-varying\npopulations under Bayesian approaches. In this paper, we propose alternative\nnon time-varying estimands, based on a single stratification of the population.\nWe present a novel assumption utilizing the time-to-event nature of the data,\nwhich is weaker than the often-invoked monotonicity assumption. We derive\nresults on partial identifiability, suggest a sensitivity analysis approach,\nand give conditions under which full identification is possible. Finally, we\npresent non-parametric and semi-parametric estimation methods for\nright-censored data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:19:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Nevo", "Daniel", ""], ["Gorfine", "Malka", ""]]}, {"id": "2010.04580", "submitter": "Kevin Schultz", "authors": "Kevin Schultz, Gregory Quiroz, Paraj Titum, B. D. Clader", "title": "SchWARMA: A model-based approach for time-correlated noise in quantum\n  circuits", "comments": "6 pages + supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal noise correlations are ubiquitous in quantum systems, yet often\nneglected in the analysis of quantum circuits due to the complexity required to\naccurately characterize and model them. Autoregressive moving average (ARMA)\nmodels are a well-known technique from time series analysis that model time\ncorrelations in data. By identifying the space of completely positive trace\nreserving (CPTP) quantum operations with a particular matrix manifold, we\ngeneralize ARMA models to the space of CPTP maps to parameterize and simulate\ntemporally correlated noise in quantum circuits. This approach, denoted\nSchr\\\"odinger Wave ARMA (SchWARMA), provides a natural path for generalization\nof classic techniques from signal processing, control theory, and system\nidentification for which ARMA models and linear systems are essential. This\nenables the broad theory of classical signal processing to be applied to\nquantum system simulation, characterization, and noise mitigation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:53:46 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Schultz", "Kevin", ""], ["Quiroz", "Gregory", ""], ["Titum", "Paraj", ""], ["Clader", "B. D.", ""]]}, {"id": "2010.04651", "submitter": "Lorenzo Tomaselli", "authors": "Lorenzo Tomaselli, Coty Jen, Ann B. Lee", "title": "Wildfire Smoke and Air Quality: How Machine Learning Can Guide Forest\n  Management", "comments": "Spotlight talk at the Tackling Climate Change with Machine Learning\n  workshop at NeurIPS 2020 (Proposals Track), 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prescribed burns are currently the most effective method of reducing the risk\nof widespread wildfires, but a largely missing component in forest management\nis knowing which fuels one can safely burn to minimize exposure to toxic smoke.\nHere we show how machine learning, such as spectral clustering and manifold\nlearning, can provide interpretable representations and powerful tools for\ndifferentiating between smoke types, hence providing forest managers with vital\ninformation on effective strategies to reduce climate-induced wildfires while\nminimizing production of harmful smoke.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:49:38 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 02:23:52 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Tomaselli", "Lorenzo", ""], ["Jen", "Coty", ""], ["Lee", "Ann B.", ""]]}, {"id": "2010.04715", "submitter": "Eric Zelikman", "authors": "Eric Zelikman, Sharon Zhou, Jeremy Irvin, Cooper Raterink, Hao Sheng,\n  Anand Avati, Jack Kelly, Ram Rajagopal, Andrew Y. Ng, David Gagne", "title": "Short-Term Solar Irradiance Forecasting Using Calibrated Probabilistic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing probabilistic solar forecasting methods is essential to supporting\nthe integration of solar energy into the electricity grid. In this work, we\ndevelop a variety of state-of-the-art probabilistic models for forecasting\nsolar irradiance. We investigate the use of post-hoc calibration techniques for\nensuring well-calibrated probabilistic predictions. We train and evaluate the\nmodels using public data from seven stations in the SURFRAD network, and\ndemonstrate that the best model, NGBoost, achieves higher performance at an\nintra-hourly resolution than the best benchmark solar irradiance forecasting\nmodel across all stations. Further, we show that NGBoost with CRUDE post-hoc\ncalibration achieves comparable performance to a numerical weather prediction\nmodel on hourly-resolution forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:57:59 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 16:03:02 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zelikman", "Eric", ""], ["Zhou", "Sharon", ""], ["Irvin", "Jeremy", ""], ["Raterink", "Cooper", ""], ["Sheng", "Hao", ""], ["Avati", "Anand", ""], ["Kelly", "Jack", ""], ["Rajagopal", "Ram", ""], ["Ng", "Andrew Y.", ""], ["Gagne", "David", ""]]}, {"id": "2010.04719", "submitter": "Behram Wali", "authors": "Behram Wali, Asad Khattak, Thomas Karnowski", "title": "The relationship between driving volatility in time to collision and\n  crash injury severity in a naturalistic driving environment", "comments": null, "journal-ref": null, "doi": "10.1016/j.amar.2020.100136", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key indicator of unsafe driving, driving volatility characterizes the\nvariations in microscopic driving decisions. This study characterizes\nvolatility in longitudinal and lateral driving decisions and examines the links\nbetween driving volatility in time to collision and crash injury severity. By\nusing a unique real-world naturalistic driving database from the 2nd Strategic\nHighway Research Program (SHRP), a test set of 671 crash events featuring\naround 0.2 million temporal samples of real world driving are analyzed. Based\non different driving performance measures, 16 different volatility indices are\ncreated. To explore the relationships between crash-injury severity outcomes\nand driving volatility, the volatility indices are then linked with individual\ncrash events including information on crash severity, drivers' pre crash\nmaneuvers and behaviors, secondary tasks and durations, and other factors. As\ndriving volatility prior to crash involvement can have different components, an\nindepth analysis is conducted using the aggregate as well as segmented (based\non time to collision) real world driving data. To account for the issues of\nobserved and unobserved heterogeneity, fixed and random parameter logit models\nwith heterogeneity in parameter means and variances are estimated. The\nempirical results offer important insights regarding how driving volatility in\ntime to collision relates to crash severity outcomes. Overall, statistically\nsignificant positive correlations are found between the aggregate (as well as\nsegmented) volatility measures and crash severity outcomes. The findings\nsuggest that greater driving volatility (both in longitudinal and lateral\ndirection) in time to collision increases the likelihood of police reportable\nor most severe crash events... ...\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:38:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad", ""], ["Karnowski", "Thomas", ""]]}, {"id": "2010.04769", "submitter": "Birgir Hrafnkelsson", "authors": "Birgir Hrafnkelsson and Helgi Sigurdarson and Sigurdur M. Gardarsson", "title": "Generalization of the power-law rating curve using hydrodynamic theory\n  and Bayesian hierarchical modeling", "comments": "Main paper 25 pages, 2 tables, 6 figures. Supplementary material 5\n  pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power-law rating curve has been used extensively in hydraulic practice\nand hydrology. It is given by $Q(h)=a(h-c)^b$, where $Q$ is discharge, $h$ is\nwater elevation, $a$, $b$ and $c$ are unknown parameters. A novel extension of\nthe power-law rating curve, referred to as the generalized power-law rating\ncurve, is proposed. It is constructed by linking the physics of open channel\nflow to a model of the form $Q(h)=a(h-c)^{f(h)}$. The function $f(h)$ is\nreferred to as the power-law exponent and it depends on the water elevation.\nThe proposed model and the power-law model are fitted within the framework of\nBayesian hierarchical models. By exploring the properties of the proposed\nrating curve and its power-law exponent, we find that cross sectional shapes\nthat are likely to be found in nature are such that the power-law exponent\n$f(h)$ will usually be in the interval $[1.0,2.67]$. This fact is utilized for\nthe construction of prior densities for the model parameters. An efficient\nMarkov chain Monte Carlo sampling scheme, that utilizes the lognormal\ndistributional assumption at the data level and Gaussian assumption at the\nlatent level, is proposed for the two models. The two statistical models were\napplied to four datasets. In the case of three datasets the generalized\npower-law rating curve gave a better fit than the power-law rating curve while\nin the fourth case the two models fitted equally well and the generalized\npower-law rating curve mimicked the power-law rating curve.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:11:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hrafnkelsson", "Birgir", ""], ["Sigurdarson", "Helgi", ""], ["Gardarsson", "Sigurdur M.", ""]]}, {"id": "2010.04771", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Off the Grid... and Back Again? The Recent Evolution of American Street\n  Network Planning and Design", "comments": null, "journal-ref": null, "doi": "10.1080/01944363.2020.1819382", "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This morphological study identifies and measures recent nationwide trends in\nAmerican street network design. Historically, orthogonal street grids provided\nthe interconnectivity and density that researchers identify as important\nfactors for reducing vehicular travel and emissions and increasing road safety\nand physical activity. During the 20th century, griddedness declined in\nplanning practice alongside declines in urban form compactness, density, and\nconnectivity as urbanization sprawled around automobile dependence. But less is\nknown about comprehensive empirical trends across US neighborhoods, especially\nin recent years. This study uses public and open data to examine tract-level\nstreet networks across the entire US. It develops theoretical and measurement\nframeworks for a quality of street networks defined here as griddedness. It\nmeasures how griddedness, orientation order, straightness, 4-way intersections,\nand intersection density declined from 1940 through the 1990s while dead-ends\nand block lengths increased. However, since 2000, these trends have rebounded,\nshifting back toward historical design patterns. Yet, despite this rebound,\nwhen controlling for topography and built environment factors all decades\npost-1939 are associated with lower griddedness than pre-1940. Higher\ngriddedness is associated with less car ownership - which itself has a\nwell-established relationship with vehicle kilometers traveled and greenhouse\ngas emissions - while controlling for density, home and household size, income,\njobs proximity, street network grain, and local topography. Interconnected\ngrid-like street networks offer practitioners an important tool for curbing car\ndependence and emissions. Once established, street patterns determine urban\nspatial structure for centuries, so proactive planning is essential.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:27:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "2010.04775", "submitter": "Zhen Liu", "authors": "Zhen Liu, Xiaoqian Sun, Leping Liu, Yu-Bo Wang", "title": "Bayesian Poisson Log-normal Model with Regularized Time Structure for\n  Mortality Projection of Multi-population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvement of mortality projection is a pivotal topic in the diverse\nbranches related to insurance, demography, and public policy. Motivated by the\nthread of Lee-Carter related models, we propose a Bayesian model to estimate\nand predict mortality rates for multi-population. This new model features in\ninformation borrowing among populations and properly reflecting variations of\ndata. It also provides a solution to a long-time overlooked problem: model\nselection for dependence structures of population-specific time parameters. By\nintroducing a Dirac spike function, simultaneous model selection and estimation\nfor population-specific time effects can be achieved without much extra\ncomputation cost. We use the Japanese mortality data from Human Mortality\nDatabase to illustrate the desirable properties of our model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:33:06 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Zhen", ""], ["Sun", "Xiaoqian", ""], ["Liu", "Leping", ""], ["Wang", "Yu-Bo", ""]]}, {"id": "2010.04789", "submitter": "Sanjib Sharma", "authors": "Sanjib Sharma, Ganesh Raj Ghimire, Rocky Talchabhadel, Jeeban Panthi,\n  Benjamin Seiyon Lee, Fengyun Sun, Rupesh Baniya, and Tirtha Raj Adhikari", "title": "Bayesian Characterization of Uncertainties Surrounding Fluvial Flood\n  Hazard Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluvial floods drive severe risk to riverine communities. There is a strong\nevidence of increasing flood hazards in many regions around the world. The\nchoice of methods and assumptions used in flood hazard estimates can impact the\ndesign of risk management strategies. In this study, we characterize the\nexpected flood hazards conditioned on the uncertain model structures, model\nparameters and prior distributions of the parameters. We construct a Bayesian\nframework for river stage return level estimation using a nonstationary\nstatistical model that relies exclusively on Indian Ocean Dipole Index. We show\nthat ignoring model structural and parametric uncertainties can lead to biased\nestimation of expected flood hazards. We find that the considered model\nparametric uncertainty is more influential than model structures and model\npriors. Our results highlight the importance of incorporating uncertainty in\nriver stage estimates, and are of practical use for informing water\ninfrastructure designs in a changing climate.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:26:31 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 18:43:48 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sharma", "Sanjib", ""], ["Ghimire", "Ganesh Raj", ""], ["Talchabhadel", "Rocky", ""], ["Panthi", "Jeeban", ""], ["Lee", "Benjamin Seiyon", ""], ["Sun", "Fengyun", ""], ["Baniya", "Rupesh", ""], ["Adhikari", "Tirtha Raj", ""]]}, {"id": "2010.04827", "submitter": "Jiahao Chen", "authors": "Eren Kurshan and Hongda Shen and Jiahao Chen", "title": "Towards Self-Regulating AI: Challenges and Opportunities of AI Model\n  Governance in Financial Services", "comments": "8 pages, 7 figures", "journal-ref": "Proceedings of the 1st International Conference on AI in Finance\n  (ICAIF '20), October 15-16, 2020, New York", "doi": "10.1145/3383455.3422564", "report-no": null, "categories": "cs.LG cs.AI q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems have found a wide range of application areas in financial\nservices. Their involvement in broader and increasingly critical decisions has\nescalated the need for compliance and effective model governance. Current\ngovernance practices have evolved from more traditional financial applications\nand modeling frameworks. They often struggle with the fundamental differences\nin AI characteristics such as uncertainty in the assumptions, and the lack of\nexplicit programming. AI model governance frequently involves complex review\nflows and relies heavily on manual steps. As a result, it faces serious\nchallenges in effectiveness, cost, complexity, and speed. Furthermore, the\nunprecedented rate of growth in the AI model complexity raises questions on the\nsustainability of the current practices. This paper focuses on the challenges\nof AI model governance in the financial services industry. As a part of the\noutlook, we present a system-level framework towards increased self-regulation\nfor robustness and compliance. This approach aims to enable potential solution\nopportunities through increased automation and the integration of monitoring,\nmanagement, and mitigation capabilities. The proposed framework also provides\nmodel governance and risk management improved capabilities to manage model risk\nduring deployment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:12:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kurshan", "Eren", ""], ["Shen", "Hongda", ""], ["Chen", "Jiahao", ""]]}, {"id": "2010.04886", "submitter": "Sanjib Sharma", "authors": "Sanjib Sharma, Michael Gomez, Klaus Keller, Robert Nicholas, Alfonso\n  Mejia", "title": "Regional Flood Risk Projections under Climate Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flood-related risks to people and property are expected to increase in the\nfuture due to environmental and demographic changes. It is important to\nquantify and effectively communicate flood hazards and exposure to inform the\ndesign and implementation of flood risk management strategies. Here we develop\nan integrated modeling framework to assess projected changes in regional\nriverine flood inundation risks. The framework samples climate model outputs to\nforce a hydrologic model and generate streamflow projections. Together with a\nstatistical and hydraulic model, we use the projected streamflow to map the\nuncertainty of flood inundation projections for extreme flood events. We\nimplement the framework for rivers across the state of Pennsylvania, United\nStates. Our projections suggest that flood hazards and exposure across\nPennsylvania are overall increasing with future climate change. Specific\nregions, including the main stem Susquehanna River, lower portion of the\nAllegheny basin and central portion of Delaware River basin, demonstrate higher\nflood inundation risks. In our analysis, the climate uncertainty dominates the\noverall uncertainty surrounding the flood inundation projection chain. The\ncombined hydrologic and hydraulic uncertainties can account for as much as 37%\nof the total uncertainty. We discuss how this framework can provide regional\nand dynamic flood-risk assessments and help to inform the design of\nrisk-management strategies.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 03:13:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharma", "Sanjib", ""], ["Gomez", "Michael", ""], ["Keller", "Klaus", ""], ["Nicholas", "Robert", ""], ["Mejia", "Alfonso", ""]]}, {"id": "2010.05079", "submitter": "Indrajit Ghosh", "authors": "Tanujit Chakraborty, Indrajit Ghosh, Tirna Mahajan and Tejasvi Arora", "title": "Nowcasting of COVID-19 confirmed cases: Foundations, trends, and\n  challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) has become a public health emergency\nof international concern affecting more than 200 countries and territories\nworldwide. As of September 30, 2020, it has caused a pandemic outbreak with\nmore than 33 million confirmed infections and more than 1 million reported\ndeaths worldwide. Several statistical, machine learning, and hybrid models have\npreviously tried to forecast COVID-19 confirmed cases for profoundly affected\ncountries. Due to extreme uncertainty and nonstationarity in the time series\ndata, forecasting of COVID-19 confirmed cases has become a very challenging\njob. For univariate time series forecasting, there are various statistical and\nmachine learning models available in the literature. But, epidemic forecasting\nhas a dubious track record. Its failures became more prominent due to\ninsufficient data input, flaws in modeling assumptions, high sensitivity of\nestimates, lack of incorporation of epidemiological features, inadequate past\nevidence on effects of available interventions, lack of transparency, errors,\nlack of determinacy, and lack of expertise in crucial disciplines. This chapter\nfocuses on assessing different short-term forecasting models that can forecast\nthe daily COVID-19 cases for various countries. In the form of an empirical\nstudy on forecasting accuracy, this chapter provides evidence to show that\nthere is no universal method available that can accurately forecast pandemic\ndata. Still, forecasters' predictions are useful for the effective allocation\nof healthcare resources and will act as an early-warning system for government\npolicymakers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 19:51:45 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Ghosh", "Indrajit", ""], ["Mahajan", "Tirna", ""], ["Arora", "Tejasvi", ""]]}, {"id": "2010.05083", "submitter": "Patrizio Vanella", "authors": "Patrizio Vanella, Ugofilippo Basellini, Berit Lange", "title": "Assessing Excess Mortality in Times of Pandemics Based on Principal\n  Component Analysis of Weekly Mortality Data -- The Case of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current outbreak of COVID-19 has called renewed attention to the need for\nsound statistical analysis for monitoring mortality patterns and trends over\ntime. Excess mortality has been suggested as the most appropriate indicator to\nmeasure the overall burden of the pandemic on mortality. As such, excess\nmortality has received considerable interest during the first months of the\nCOVID-19 pandemic. Previous approaches to estimate excess mortality are\nsomewhat limited, as they do not include sufficiently long-term trends,\ncorrelations among different demographic and geographic groups, and the\nautocorrelations in the mortality time series. This might lead to biased\nestimates of excess mortality, as random mortality fluctuations may be\nmisinterpreted as excess mortality. We present a blend of classical\nepidemiological approaches to estimating excess mortality during extraordinary\nevents with an established demographic approach in mortality forecasting,\nnamely a Lee-Carter type model, which covers the named limitations and draws a\nmore realistic picture of the excess mortality. We illustrate our approach\nusing weekly age- and sex-specific mortality data for 19 countries and the\ncurrent COVID-19 pandemic as a case study. Our proposed model provides a\ngeneral framework that can be applied to future pandemics as well as to monitor\nexcess mortality from specific causes of deaths.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 20:03:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Vanella", "Patrizio", ""], ["Basellini", "Ugofilippo", ""], ["Lange", "Berit", ""]]}, {"id": "2010.05258", "submitter": "Torkan Gholamalizadeh", "authors": "Torkan Gholamalizadeh, Sune Darkner, Paolo Maria Cattaneo, Peter\n  S{\\o}ndergaard, Kenny Erleben", "title": "Mandibular Teeth Movement Variations in Tipping Scenario: A Finite\n  Element Study on Several Patients", "comments": "Accepted in the MICCAI 2020 Workshop on Computational Biomechanics\n  for Medicine XV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on computational modeling of tooth movement in orthodontic\ntreatments are limited to a single model and fail in generalizing the\nsimulation results to other patients. To this end, we consider multiple\npatients and focus on tooth movement variations under the identical load and\nboundary conditions both for intra- and inter-patient analyses. We introduce a\nnovel computational analysis tool based on finite element models (FEMs)\naddressing how to assess initial tooth displacement in the mandibular dentition\nacross different patients for uncontrolled tipping scenarios with different\nload magnitudes applied to the mandibular dentition. This is done by modeling\nthe movement of each patient's tooth as a nonlinear function of both load and\ntooth size. As the size of tooth can affect the resulting tooth displacement, a\ncombination of two clinical biomarkers obtained from the tooth anatomy, i.e.,\ncrown height and root volume, is considered to make the proposed model\ngeneralizable to different patients and teeth.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 14:51:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gholamalizadeh", "Torkan", ""], ["Darkner", "Sune", ""], ["Cattaneo", "Paolo Maria", ""], ["S\u00f8ndergaard", "Peter", ""], ["Erleben", "Kenny", ""]]}, {"id": "2010.05320", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Kaiying Ji and Ufuk Beyaztas", "title": "Granger causality of bivariate stationary curve time series", "comments": "17 pages, 3 figures, to appear at the Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study causality between bivariate curve time series using the Granger\ncausality generalized measures of correlation. With this measure, we can\ninvestigate which curve time series Granger-causes the other; in turn, it helps\ndetermine the predictability of any two curve time series. Illustrated by a\nclimatology example, we find that the sea surface temperature Granger-causes\nthe sea-level atmospheric pressure. Motivated by a portfolio management\napplication in finance, we single out those stocks that lead or lag behind\nDow-Jones industrial averages. Given a close relationship between S&P 500 index\nand crude oil price, we determine the leading and lagging variables.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:03:18 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:55:57 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shang", "Han Lin", ""], ["Ji", "Kaiying", ""], ["Beyaztas", "Ufuk", ""]]}, {"id": "2010.05504", "submitter": "Ajmal Oodally", "authors": "Ajmal Oodally and Estelle Kuhn and Klara Goethals and Luc Duchateau", "title": "Modeling dependent survival data through random effects with spatial\n  correlation at the subject level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical phenomena such as infectious diseases are often investigated by\nfollowing up subjects longitudinally, thus generating time to event data. The\nspatial aspect of such data is also of primordial importance, as many\ninfectious diseases are transmitted from one subject to another. In this paper,\na spatially correlated frailty model is introduced that accommodates for the\ncorrelation between subjects based on the distance between them. Estimates are\nobtained through a stochastic approximation version of the Expectation\nMaximization algorithm combined with a Monte-Carlo Markov Chain, for which\nconvergence is proven. The novelty of this model is that spatial correlation is\nintroduced for survival data at the subject level, each subject having its own\nfrailty. This univariate spatially correlated frailty model is used to analyze\nspatially dependent malaria data, and its results are compared with other\nstandard models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:57:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Oodally", "Ajmal", ""], ["Kuhn", "Estelle", ""], ["Goethals", "Klara", ""], ["Duchateau", "Luc", ""]]}, {"id": "2010.05589", "submitter": "Nomvelo Sibisi", "authors": "Nomvelo Sibisi", "title": "Growth of Random Trees by Leaf Attachment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the growth of a time-ordered rooted tree by probabilistic attachment\nof new vertices to leaves. We construct a likelihood function of the leaves\nbased on the connectivity of the tree. We take such connectivity to be induced\nby the merging of directed ordered paths from leaves to the root. Combining the\nlikelihood with an assigned prior distribution leads to a posterior leaf\ndistribution from which we sample attachment points for new vertices. We\npresent computational examples of such Bayesian tree growth. Although the\ndiscussion is generic, the initial motivation for the paper is the concept of a\ndistributed ledger, which may be regarded as a time-ordered random tree that\ngrows by probabilistic leaf attachment.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:29:32 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 10:37:39 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 11:32:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sibisi", "Nomvelo", ""]]}, {"id": "2010.05593", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno, Abhik Ghosh, Ayanendranath Basu, Claudio\n  Agostinelli", "title": "Robust Estimation under Linear Mixed Models: The Minimum Density Power\n  Divergence Approach", "comments": "25 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life data sets can be analyzed using Linear Mixed Models (LMMs).\nSince these are ordinarily based on normality assumptions, under small\ndeviations from the model the inference can be highly unstable when the\nassociated parameters are estimated by classical methods. On the other hand,\nthe density power divergence (DPD) family, which measures the discrepancy\nbetween two probability density functions, has been successfully used to build\nrobust estimators with high stability associated with minimal loss in\nefficiency. Here, we develop the minimum DPD estimator (MDPDE) for independent\nbut non identically distributed observations in LMMs. We prove the theoretical\nproperties, including consistency and asymptotic normality. The influence\nfunction and sensitivity measures are studied to explore the robustness\nproperties. As a data based choice of the MDPDE tuning parameter $\\alpha$ is\nvery important, we propose two candidates as \"optimal\" choices, where\noptimality is in the sense of choosing the strongest downweighting that is\nnecessary for the particular data set. We conduct a simulation study comparing\nthe proposed MDPDE, for different values of $\\alpha$, with the S-estimators,\nM-estimators and the classical maximum likelihood estimator, considering\ndifferent levels of contamination. Finally, we illustrate the performance of\nour proposal on a real-data example.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:50:50 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "2010.05684", "submitter": "Jack Wilkinson Dr", "authors": "Jack Wilkinson, Jonathan Huang, Antonia Marsden, Michael Harhay, Andy\n  Vail, Stephen A Roberts", "title": "The implications of outcome truncation in reproductive medicine RCTs: a\n  simulation platform for trialists and simulation study", "comments": "63 pages (including supplementary files). 49 Figures (7 in main\n  document, 42 supplementary figures). Code available at, https://osf.io/gzqbr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomised controlled trials in reproductive medicine are often subject to\noutcome truncation, where study outcomes are only defined in a subset of\nparticipants. Examples include birthweight (measurable only in the subgroup of\nparticipants who give birth) and miscarriage (which can only occur in\nparticipants who become pregnant). These are typically analysed by making a\ncomparison between treatment arms within the subgroup (comparing birthweights\nin the subgroup who gave birth, or miscarriages in the subgroup who became\npregnant). However, this approach does not represent a randomised comparison\nwhen treatment influences the probability of being observed (i.e. survival).\nThe practical implications of this for reproductive trials are unclear. We\ndeveloped a simulation platform to investigate the implications of outcome\ntruncation for reproductive medicine trials. We used this to perform a\nsimulation study, in which we considered the bias, Type 1 error, coverage, and\nprecision of standard statistical analyses for truncated continuous and binary\noutcomes. Increasing treatment effect on the intermediate variable, strength of\nconfounding between the intermediate and outcome variables, and interactions\nbetween treatment and confounder were found to adversely affect performance.\nHowever, within parameter ranges we would consider to be more realistic, the\nadverse effects were generally not drastic. For binary outcomes, the study\nhighlighted that outcome truncation may lead to none of the participants in a\nstudy arm experiencing the outcome event. This was found to have severe\nconsequences for inferences, and this may have implications for meta-analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:25:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wilkinson", "Jack", ""], ["Huang", "Jonathan", ""], ["Marsden", "Antonia", ""], ["Harhay", "Michael", ""], ["Vail", "Andy", ""], ["Roberts", "Stephen A", ""]]}, {"id": "2010.05783", "submitter": "Irwin McNeely", "authors": "Trey McNeely, Niccol\\`o Dalmasso, Kimberly M. Wood, Ann B. Lee", "title": "Structural Forecasting for Tropical Cyclone Intensity Prediction:\n  Providing Insight with Deep Learning", "comments": "To appear in the Tackling Climate Change with Machine Learning\n  workshop at NeurIPS 2020 (Proposals Track) 3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical cyclone (TC) intensity forecasts are ultimately issued by human\nforecasters. The human in-the-loop pipeline requires that any forecasting\nguidance must be easily digestible by TC experts if it is to be adopted at\noperational centers like the National Hurricane Center. Our proposed framework\nleverages deep learning to provide forecasters with something neither\nend-to-end prediction models nor traditional intensity guidance does: a\npowerful tool for monitoring high-dimensional time series of key physically\nrelevant predictors and the means to understand how the predictors relate to\none another and to short-term intensity changes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:01:06 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 13:15:37 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 14:38:24 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["McNeely", "Trey", ""], ["Dalmasso", "Niccol\u00f2", ""], ["Wood", "Kimberly M.", ""], ["Lee", "Ann B.", ""]]}, {"id": "2010.05895", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna R\n  Narayanan, Xiaoning Qian", "title": "BayReL: Bayesian Relational Learning for Multi-omics Data Integration", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.LG q-bio.MN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput molecular profiling technologies have produced\nhigh-dimensional multi-omics data, enabling systematic understanding of living\nsystems at the genome scale. Studying molecular interactions across different\ndata types helps reveal signal transduction mechanisms across different classes\nof molecules. In this paper, we develop a novel Bayesian representation\nlearning method that infers the relational interactions across multi-omics data\ntypes. Our method, Bayesian Relational Learning (BayReL) for multi-omics data\nintegration, takes advantage of a priori known relationships among the same\nclass of molecules, modeled as a graph at each corresponding view, to learn\nview-specific latent variables as well as a multi-partite graph that encodes\nthe interactions across views. Our experiments on several real-world datasets\ndemonstrate enhanced performance of BayReL in inferring meaningful interactions\ncompared to existing baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:43:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 20:34:30 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 07:22:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Hasanzadeh", "Arman", ""], ["Duffield", "Nick", ""], ["Narayanan", "Krishna R", ""], ["Qian", "Xiaoning", ""]]}, {"id": "2010.05898", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar, Jens Schreiber, Stephan Vogt, Andr\\'e Gensler,\n  Bernhard Sick", "title": "Quantile Surfaces -- Generalizing Quantile Regression to Multivariate\n  Targets", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), currently under review, 15 page, 23 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a novel approach to multivariate probabilistic\nforecasting. Our approach is based on an extension of single-output quantile\nregression (QR) to multivariate-targets, called quantile surfaces (QS). QS uses\na simple yet compelling idea of indexing observations of a probabilistic\nforecast through direction and vector length to estimate a central tendency. We\nextend the single-output QR technique to multivariate probabilistic targets. QS\nefficiently models dependencies in multivariate target variables and represents\nprobability distributions through discrete quantile levels. Therefore, we\npresent a novel two-stage process. In the first stage, we perform a\ndeterministic point forecast (i.e., central tendency estimation). Subsequently,\nwe model the prediction uncertainty using QS involving neural networks called\nquantile surface regression neural networks (QSNN). Additionally, we introduce\nnew methods for efficient and straightforward evaluation of the reliability and\nsharpness of the issued probabilistic QS predictions. We complement this by the\ndirectional extension of the Continuous Ranked Probability Score (CRPS) score.\nFinally, we evaluate our novel approach on synthetic data and two currently\nresearched real-world challenges in two different domains: First, probabilistic\nforecasting for renewable energy power generation, second, short-term cyclists\ntrajectory forecasting for autonomously driving vehicles. Especially for the\nlatter, our empirical results show that even a simple one-layer QSNN\noutperforms traditional parametric multivariate forecasting techniques, thus\nimproving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:35:37 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bieshaar", "Maarten", ""], ["Schreiber", "Jens", ""], ["Vogt", "Stephan", ""], ["Gensler", "Andr\u00e9", ""], ["Sick", "Bernhard", ""]]}, {"id": "2010.05942", "submitter": "Mengyang Gu", "authors": "Jianzhong Wu, Mengyang Gu", "title": "Emulating the First Principles of Matter: A Probabilistic Roadmap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides a tutorial overview of first principles methods to\ndescribe the properties of matter at the ground state or equilibrium. It begins\nwith a brief introduction to quantum and statistical mechanics for predicting\nthe electronic structure and diverse static properties of of many-particle\nsystems useful for practical applications. Pedagogical examples are given to\nillustrate the basic concepts and simple applications of quantum Monte Carlo\nand density functional theory -- two representative methods commonly used in\nthe literature of first principles modeling. In addition, this chapter\nhighlights the practical needs for the integration of physics-based modeling\nand data-science approaches to reduce the computational cost and expand the\nscope of applicability. A special emphasis is placed on recent developments of\nstatistical surrogate models to emulate first principles calculation from a\nprobabilistic point of view. The probabilistic approach provides an internal\nassessment of the approximation accuracy of emulation that quantifies the\nuncertainty in predictions. Various recent advances toward this direction\nestablish a new marriage between Gaussian processes and first principles\ncalculation, with physical properties, such as translational, rotational, and\npermutation symmetry, naturally encoded in new kernel functions. Finally, it\nconcludes with some prospects on future advances in the field toward faster yet\nmore accurate computation leveraging a synergetic combination {of} novel\ntheoretical concepts and efficient numerical algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:50:36 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wu", "Jianzhong", ""], ["Gu", "Mengyang", ""]]}, {"id": "2010.06072", "submitter": "Alejandro Cohen", "authors": "Alejandro Cohen, Nir Shlezinger, Amit Solomon, Yonina C. Eldar, and\n  Muriel M\\'edard", "title": "Multi-Level Group Testing with Application to One-Shot Pooled COVID-19\n  Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in containing the Coronoavirus disease 2019\n(COVID-19) pandemic stems from the difficulty in carrying out efficient mass\ndiagnosis over large populations. The leading method to test for COVID-19\ninfection utilizes qualitative polymerase chain reaction, implemented using\ndedicated machinery which can simultaneously process a limited amount of\nsamples. A candidate method to increase the test throughput is to examine\npooled samples comprised of a mixture of samples from different patients. In\nthis work we study pooling-based COVID-19 tests. We identify the specific\nrequirements of COVID-19 testing, including the need to characterize the\ninfection level and to operate in a one-shot fashion, which limit the\napplication of traditional group-testing (GT) methods. We then propose a\nmulti-level GT scheme, designed specifically to meet the unique requirements of\nCOVID-19 tests, while exploiting the strength of GT theory to enable accurate\nrecovery using much fewer tests than patients. Our numerical results\ndemonstrate that multi-level GT reliably and efficiently detects the infection\nlevels, while achieving improved accuracy over previously proposed one-shot\nCOVID-19 pooled-testing methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:18:07 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cohen", "Alejandro", ""], ["Shlezinger", "Nir", ""], ["Solomon", "Amit", ""], ["Eldar", "Yonina C.", ""], ["M\u00e9dard", "Muriel", ""]]}, {"id": "2010.06090", "submitter": "Shrabanti Chowdhury", "authors": "Saptarshi Chatterjee, Shrabanti Chowdhury and Sanjib Basu", "title": "A Model-free Approach for Testing Association", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The question of association between outcome and feature is generally framed\nin the context of a model on functional and distributional forms. Our\nmotivating application is that of identifying serum biomarkers of angiogenesis,\nenergy metabolism, apoptosis, and inflammation, predictive of recurrence after\nlung resection in node-negative non-small cell lung cancer patients with tumor\nstage T2a or less. We propose an omnibus approach for testing association that\nis free of assumptions on functional forms and distributions and can be used as\na black box method. This proposed maximal permutation test is based on the idea\nof thresholding, is readily implementable and is computationally efficient. We\nillustrate that the proposed omnibus tests maintain their levels and have\nstrong power as black box tests for detecting linear, nonlinear and\nquantile-based associations, even with outlier-prone and heavy-tailed error\ndistributions and under nonparametric setting. We additionally illustrate the\nuse of this approach in model-free feature screening and further examine the\nlevel and power of these tests for binary outcome. We compare the performance\nof the proposed omnibus tests with comparator methods in our motivating\napplication to identify preoperative serum biomarkers associated with non-small\ncell lung cancer recurrence in early stage patients.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 00:27:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chatterjee", "Saptarshi", ""], ["Chowdhury", "Shrabanti", ""], ["Basu", "Sanjib", ""]]}, {"id": "2010.06110", "submitter": "Xuefei Guan", "authors": "Jingjing He, Xuefei Guan", "title": "Bayesian inference under small sample size -- A noninformative prior\n  approach", "comments": "22 pages, 52 (sub)figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian inference method for problems with small samples and sparse data\nis presented in this paper. A general type of prior ($\\propto 1/\\sigma^{q}$) is\nproposed to formulate the Bayesian posterior for inference problems under small\nsample size. It is shown that this type of prior can represents a broad range\nof priors such as classical noninformative priors and asymptotically locally\ninvariant priors. It is further shown in this study that such priors can be\nderived as the limiting states of Normal-Inverse-Gamma conjugate priors,\nallowing for analytical evaluations of Bayesian posteriors and predictors. The\nperformance of different noninformative priors under small sample size is\ncompared using the global likelihood. The method of Laplace approximation is\nemployed to evaluate the global likelihood. A numerical linear regression\nproblem and a realistic fatigue reliability problem are used to demonstrate the\nmethod and identify the optimal noninformative prior. Results indicate the\npredictor using Jeffreys' prior outperforms others. The advantage of the\nnoninformative Bayesian estimator over the regular least square estimator under\nsmall sample size is shown.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:47:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["He", "Jingjing", ""], ["Guan", "Xuefei", ""]]}, {"id": "2010.06227", "submitter": "Jonathan Berrisch", "authors": "Jonathan Berrisch, Florian Ziel", "title": "Modeling and Probababilistic Forecasting of Natural Gas Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of modeling and forecasting European\nDay-Ahead and Month-Ahead natural gas prices. For this, we propose two distinct\nprobabilistic models that can be utilized in risk- and portfolio management. We\nuse daily pricing data ranging from 2011 to 2020. Extensive descriptive data\nanalysis shows that both time series feature heavy tails, conditional\nheteroscedasticity, and show asymmetric behavior in their differences. We\npropose state-space time series models under skewed, heavy-tailed distribution\nto capture all stylized facts in the data. They include the impact of\nautocorrelation, seasonality, risk premia, temperature, storage levels, the\nprice of European Emission Allowances, and related fuel prices of oil, coal,\nand electricity. We provide a rigorous model diagnostic and interpret all model\ncomponents in detail. Additionally, we conduct a probabilistic forecasting\nstudy with significance test and compare the predictive performance against\nliterature benchmarks. The proposed Day-Ahead (Month-Ahead) model leads to a\n$13\\%$ ($9$\\%) reduction in out of sample CRPS compared to the best performing\nbenchmark model, mainly due to adequate modeling of the volatility and heavy\ntails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:22:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Berrisch", "Jonathan", ""], ["Ziel", "Florian", ""]]}, {"id": "2010.06333", "submitter": "Tero L\\\"ahderanta", "authors": "Leena Ruha and Tero L\\\"ahderanta and Lauri Lov\\'en and Markku Kuismin\n  and Teemu Lepp\\\"anen and Jukka Riekki and Mikko J. Sillanp\\\"a\\\"a", "title": "Capacitated spatial clustering with multiple constraints and attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacitated spatial clustering, a type of unsupervised machine learning\nmethod, is often used to tackle problems in compressing, classifying, logistic\noptimization and infrastructure optimization. Depending on the application at\nhand, a wide set of extensions may be necessary in clustering.\n  In this article we propose a number of novel extensions to PACK that is a\nnovel capacitated spatial clustering method. These extensions are relocation\nand location preference of cluster centers, outliers, and non-spatial\nattributes. The strength of PACK is that it can consider all of these\nextensions jointly. We demonstrate the usefulness PACK with a real world\nexample in edge computing server placement for a city region with various\ndifferent set ups, where we take into consideration outliers, center placement,\nand non-spatial attributes. Different setups are evaluated with summary\nstatistics on spatial proximity and attribute similarity. As a result, the\nsimilarity of the clusters was improved at best by 53%, while simultaneously\nthe proximity degraded only 18%. In alternate scenarios, both proximity and\nsimilarity were improved. The different extensions proved to provide a valuable\nway to include non-spatial information into the cluster analysis, and attain\nbetter overall proximity and similarity. Furthermore, we provide easy-to-use\nsoftware tools (rpack) for conducting clustering analyses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:31:13 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 18:00:08 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 13:44:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ruha", "Leena", ""], ["L\u00e4hderanta", "Tero", ""], ["Lov\u00e9n", "Lauri", ""], ["Kuismin", "Markku", ""], ["Lepp\u00e4nen", "Teemu", ""], ["Riekki", "Jukka", ""], ["Sillanp\u00e4\u00e4", "Mikko J.", ""]]}, {"id": "2010.06370", "submitter": "Subhrajyoty Roy", "authors": "Ritwik Bhaduri, Subhrajyoty Roy and Sankar K. Pal", "title": "Rough-Fuzzy CPD: A Gradual Change Point Detection Algorithm", "comments": "25 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changepoint detection is the problem of finding abrupt or gradual changes in\ntime series data when the distribution of the time series changes\nsignificantly. There are many sophisticated statistical algorithms for solving\nchangepoint detection problem, although there is not much work devoted towards\ngradual changepoints as compared to abrupt ones. Here we present a new approach\nto solve changepoint detection problem using fuzzy rough set theory which is\nable to detect such gradual changepoints. An expression for the rough-fuzzy\nestimate of changepoints is derived along with its mathematical properties\nconcerning fast computation. In a statistical hypothesis testing framework,\nasymptotic distribution of the proposed statistic on both single and multiple\nchangepoints is derived under null hypothesis enabling multiple changepoint\ndetection. Extensive simulation studies have been performed to investigate how\nsimple crude statistical measures of disparity can be subjected to improve\ntheir efficiency in estimation of gradual changepoints. Also, the said\nrough-fuzzy estimate is robust to signal-to-noise ratio, high degree of\nfuzziness in true changepoints and also to hyper parameter values. Simulation\nstudies reveal that the proposed method beats other fuzzy methods and also\npopular crisp methods like WBS, PELT and BOCD in detecting gradual\nchangepoints. The applicability of the estimate is demonstrated using multiple\nreal-life datasets including Covid-19. We have developed the python package\n\"roufcp\" for broader dissemination of the methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:36:46 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Bhaduri", "Ritwik", ""], ["Roy", "Subhrajyoty", ""], ["Pal", "Sankar K.", ""]]}, {"id": "2010.06373", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "Generalized Rescaled Polya urn and its statistical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Generalized Rescaled Polya (GRP) urn, that provides a\ngenerative model for a chi-squared test of goodness of fit for the long-term\nprobabilities of clustered data, with independence between clusters and\ncorrelation, due to a reinforcement mechanism, inside each cluster. We apply\nthe proposed test to a data set of Twitter posts about COVID-19 pandemic: in a\nfew words, for a classical chi-squared test the data result strongly\nsignificant for the rejection of the null hypothesis (the daily long-run\nsentiment rate remains constant), but, taking into account the correlation\namong data, the introduced test leads to a different conclusion. Beside the\nstatistical application, we point out that the GRP urn is a simple variant of\nthe standard Eggenberger-Polya urn, that, with suitable choices of the\nparameters, shows \"local\" reinforcement, almost sure convergence of the\nempirical mean to a deterministic limit and different asymptotic behaviours of\nthe predictive mean. Moreover, the study of this model provides the opportunity\nto analyze stochastic approximation dynamics, that are unusual in the related\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:30:02 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:14:54 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 10:14:38 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "2010.06476", "submitter": "J. Emmanuel Johnson", "authors": "J. Emmanuel Johnson, Valero Laparra, Maria Piles, Gustau Camps-Valls", "title": "Gaussianizing the Earth: Multidimensional Information Measures for Earth\n  Data Analysis", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory is an excellent framework for analyzing Earth system data\nbecause it allows us to characterize uncertainty and redundancy, and is\nuniversally interpretable. However, accurately estimating information content\nis challenging because spatio-temporal data is high-dimensional, heterogeneous\nand has non-linear characteristics. In this paper, we apply multivariate\nGaussianization for probability density estimation which is robust to\ndimensionality, comes with statistical guarantees, and is easy to apply. In\naddition, this methodology allows us to estimate information-theoretic measures\nto characterize multivariate densities: information, entropy, total\ncorrelation, and mutual information. We demonstrate how information theory\nmeasures can be applied in various Earth system data analysis problems. First\nwe show how the method can be used to jointly Gaussianize radar backscattering\nintensities, synthesize hyperspectral data, and quantify of information content\nin aerial optical images. We also quantify the information content of several\nvariables describing the soil-vegetation status in agro-ecosystems, and\ninvestigate the temporal scales that maximize their shared information under\nextreme events such as droughts. Finally, we measure the relative information\ncontent of space and time dimensions in remote sensing products and model\nsimulations involving long records of key variables such as precipitation,\nsensible heat and evaporation. Results confirm the validity of the method, for\nwhich we anticipate a wide use and adoption. Code and demos of the implemented\nalgorithms and information-theory measures are provided.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:30:34 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 10:10:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Johnson", "J. Emmanuel", ""], ["Laparra", "Valero", ""], ["Piles", "Maria", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2010.06501", "submitter": "Sanjib Sharma", "authors": "Rocky Talchabhadel, Ganesh R. Ghimire, Sanjib Sharma, Piyush Dahal,\n  Jeeban Panthi, Rupesh Baniya, Jayaram Pudashine, Bhesh Raj Thapa, Shakti PC,\n  Binod Parajuli", "title": "Weather Radar in Nepal: Opportunities and Challenges in Mountainous\n  Region", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme rainfall is one of the major causes of natural hazards (for example\nflood, landslide, and debris flow) in the central Himalayan region, Nepal. The\nperformance of strategies to manage these risks relies on the accuracy of\nquantitative rainfall estimates. Rain gauges have traditionally been used to\nmeasure the amount of rainfall at a given location. The point measurement often\nmisrepresents the basin estimates, because of limited density and high spatial\nvariability of rainfall fields across the Himalayas. The Department of\nHydrology and Meteorology (DHM), Nepal has planned to install a network of\nthree weather radars that cover the entire country. So far, the first weather\nradar has been installed in 2019 in the western region of the country. Two more\nradars will be added for the planned radar network in the near future covering\nthe central and eastern regions of the country. Here we introduce the first\ninstalled weather radar in Nepal. We highlight both the opportunities and\nchallenges with the radar observation in the mountainous regions. Radar\nrainfall estimates across the Himalayas are critical to issue severe weather\nwarnings; forecast floods and landslides; and inform decision making in a broad\nrange of sectors, including water and energy, construction, transportation, and\nagriculture.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 16:52:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Talchabhadel", "Rocky", ""], ["Ghimire", "Ganesh R.", ""], ["Sharma", "Sanjib", ""], ["Dahal", "Piyush", ""], ["Panthi", "Jeeban", ""], ["Baniya", "Rupesh", ""], ["Pudashine", "Jayaram", ""], ["Thapa", "Bhesh Raj", ""], ["PC", "Shakti", ""], ["Parajuli", "Binod", ""]]}, {"id": "2010.06515", "submitter": "Boya Zhang", "authors": "Boya Zhang, Robert B. Gramacy, Leah Johnson, Kenneth A. Rose, Eric\n  Smith", "title": "Batch-sequential design and heteroskedastic surrogate modeling for delta\n  smelt conservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delta smelt is an endangered fish species in the San Francisco estuary that\nhave shown an overall population decline over the past 30 years. Researchers\nhave developed a stochastic, agent-based simulator to virtualize the system,\nwith the goal of understanding the relative contribution of natural and\nanthropogenic factors suggested as playing a role in their decline. However,\nthe input configuration space is high-dimensional, running the simulator is\ntime-consuming, and its noisy outputs change nonlinearly in both mean and\nvariance. Getting enough runs to effectively learn input--output dynamics\nrequires both a nimble modeling strategy and parallel supercomputer evaluation.\nRecent advances in heteroskedastic Gaussian process (HetGP) surrogate modeling\nhelps, but little is known about how to appropriately plan experiments for\nhighly distributed simulator evaluation. We propose a batch sequential design\nscheme, generalizing one-at-a-time variance-based active learning for HetGP\nsurrogates, as a means of keeping multi-core cluster nodes fully engaged with\nexpensive runs. Our acquisition strategy is carefully engineered to favor\nselection of replicates which boost statistical and computational efficiencies\nwhen training surrogates to isolate signal in high noise regions. Design and\nmodeling performance is illustrated on a range of toy examples before embarking\non a large-scale smelt simulation campaign and downstream high-fidelity input\nsensitivity analysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:14:37 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 02:05:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhang", "Boya", ""], ["Gramacy", "Robert B.", ""], ["Johnson", "Leah", ""], ["Rose", "Kenneth A.", ""], ["Smith", "Eric", ""]]}, {"id": "2010.06517", "submitter": "Germain Garcia-Zanabria", "authors": "Garcia-Zanabria, Germain and Silveira, Jaqueline Alvarenga and Poco,\n  Jorge and Paiva, Afonso and Nery, Marcelo Batista and Silva, Claudio T and de\n  Abreu, Sergio Franca Adorno and Nonato, Luis Gustavo", "title": "CrimAnalyzer: Understanding Crime Patterns in S\\~ao Paulo City", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2947515", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  S\\~ao Paulo is the largest city in South America, with high criminality\nrates. The number and type of crimes varies considerably around the city,\nassuming different patterns depending on urban and social characteristics. In\nthis scenario, enabling tools to explore particular locations of the city is\nvery important for domain experts to understand how urban features as to\nmobility, passersby behavior, and urban infrastructures can influence the\nquantity and type of crimes. In present work, we present CrimAnalyzer, a\nvisualization assisted analytic tool that allows users to analyze crime\nbehavior in specific regions of a city, providing new methodologies to identify\nlocal crime hotspots and their corresponding patterns over time. CrimAnalyzer\nhas been developed from the demand of experts in criminology and it deals with\nthree major challenges: i) flexibility to explore local regions and understand\ntheir crime patterns, ii) Identification of not only prevalent hotspots in\nterms of number of crimes but also hotspots where crimes are frequent but not\nin large amount, and iii) understand the dynamic of crime patterns over time.\nThe effectiveness and usefulness of the proposed system are demonstrated by\nqualitative/quantitative comparisons as well as case studies involving real\ndata and run by domain experts.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 23:08:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Garcia-Zanabria", "", ""], ["Germain", "", ""], ["Silveira", "", ""], ["Alvarenga", "Jaqueline", ""], ["Poco", "", ""], ["Jorge", "", ""], ["Paiva", "", ""], ["Afonso", "", ""], ["Nery", "", ""], ["Batista", "Marcelo", ""], ["Silva", "", ""], ["T", "Claudio", ""], ["Abreu", "de", ""], ["Adorno", "Sergio Franca", ""], ["Nonato", "", ""], ["Gustavo", "Luis", ""]]}, {"id": "2010.06518", "submitter": "Thomas Jaki", "authors": "Thomas Jaki, Helen Barnett, Andrew Titman, Pavel Mozgunov", "title": "A Seamless Phase I/II Platform Design with a Time-To-Event Efficacy\n  Endpoint for Potential COVID-19 Therapies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the search for effective treatments for COVID-19, initial emphasis has\nbeen on re-purposed treatments. To maximise the chances of finding successful\ntreatments, novel treatments that have been developed for this disease in\nparticular, are needed. In this manuscript we describe and evaluate the\nstatistical design of the AGILE platform, an adaptive randomized seamless Phase\nI/II trial platform that seeks to quickly establish a safe range of doses and\ninvestigates treatments for potential efficacy using a Bayesian sequential\ntrial design. Both single agent and combination treatments are considered. We\nfind that the design can identify potential treatments that are safe and\nefficacious reliably with small to moderate sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:03:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Jaki", "Thomas", ""], ["Barnett", "Helen", ""], ["Titman", "Andrew", ""], ["Mozgunov", "Pavel", ""]]}, {"id": "2010.06538", "submitter": "Carlos M. Ortiz", "authors": "Javier Rubio-Herrero, Carlos Ortiz Marrero, Wai-Tong Louis Fan", "title": "Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven\n  Modeling of Air Pollutants", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-SA-157007", "categories": "stat.AP cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric modeling has recently experienced a surge with the advent of deep\nlearning. Most of these models, however, predict concentrations of pollutants\nfollowing a data-driven approach in which the physical laws that govern their\nbehaviors and relationships remain hidden. With the aid of real-world air\nquality data collected hourly in different stations throughout Madrid, we\npresent an empirical approach using data-driven techniques with the following\ngoals: (1) Find parsimonious systems of ordinary differential equations via\nsparse identification of nonlinear dynamics (SINDy) that model the\nconcentration of pollutants and their changes over time; (2) assess the\nperformance and limitations of our models using stability analysis; (3)\nreconstruct the time series of chemical pollutants not measured in certain\nstations using delay coordinate embedding results. Our results show that\nAkaike's Information Criterion can work well in conjunction with best subset\nregression as to find an equilibrium between sparsity and goodness of fit. We\nalso find that, due to the complexity of the chemical system under study,\nidentifying the dynamics of this system over longer periods of time require\nhigher levels of data filtering and smoothing. Stability analysis for the\nreconstructed ordinary differential equations (ODEs) reveals that more than\nhalf of the physically relevant critical points are saddle points, suggesting\nthat the system is unstable even under the idealized assumption that all\nenvironmental conditions are constant over time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:46:07 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 20:29:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rubio-Herrero", "Javier", ""], ["Marrero", "Carlos Ortiz", ""], ["Fan", "Wai-Tong Louis", ""]]}, {"id": "2010.06567", "submitter": "Kevin Kunzmann", "authors": "Kevin Kunzmann, Michael J. Grayling, Kim M. Lee, David S. Robertson,\n  Kaspar Rufibach, James M. S. Wason", "title": "Conditional Power and Friends: The Why and How of (Un)planned, Unblinded\n  Sample Size Recalculations in Confirmatory Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting the final sample size of a trial to the evidence accruing during the\ntrial is a natural way to address planning uncertainty. Designs with adaptive\nsample size need to account for their optional stopping to guarantee strict\ntype-I error-rate control. A variety of different methods to maintain type-I\nerror-rate control after unplanned changes of the initial sample size have been\nproposed in the literature. This makes interim analyses for the purpose of\nsample size recalculation feasible in a regulatory context. Since the sample\nsize is usually determined via an argument based on the power of the trial, an\ninterim analysis raises the question of how the final sample size should be\ndetermined conditional on the accrued information. Conditional power is a\nconcept often put forward in this context. Since it depends on the unknown\neffect size, we take a strict estimation perspective and compare assumed\nconditional power, observed conditional power, and predictive power with\nrespect to their properties as estimators of the unknown conditional power. We\nthen demonstrate that pre-planning an interim analysis using methodology for\nunplanned interim analyses is ineffective and naturally leads to the concept of\noptimal two-stage designs. We conclude that unplanned design adaptations should\nonly be conducted as reaction to trial-external new evidence, operational needs\nto violate the originally chosen design, or post hoc changes in the objective\ncriterion. Finally, we show that commonly discussed sample size recalculation\nrules can lead to paradoxical outcomes and propose two alternative ways of\nreacting to newly emerging trial-external evidence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:32:27 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kunzmann", "Kevin", ""], ["Grayling", "Michael J.", ""], ["Lee", "Kim M.", ""], ["Robertson", "David S.", ""], ["Rufibach", "Kaspar", ""], ["Wason", "James M. S.", ""]]}, {"id": "2010.06588", "submitter": "Chaogui Kang", "authors": "Devashish Khulbe, Chaogui Kang, Stanislav Sobolevsky", "title": "Transportation Interventions Reshaping NYC Commute: the Probabilistic\n  Simulation Framework Assessing the Impacts of Ridesharing and Manhattan\n  Congestion Surcharge", "comments": "32 pages, 9 figures, 3 3ables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding holistic impact of planned transportation solutions and\ninterventions on urban systems is challenged by their complexity but critical\nfor decision making. The cornerstone for such impact assessments is estimating\nthe transportation mode-shift resulting from the intervention. And while\ntransportation planning has well-established models for the mode-choice\nassessment such as the nested multinomial logit model, an individual choice\nsimulation could be better suited for addressing the mode-shift allowing to\nconsistently account for individual preferences. In addition, no model\nperfectly represents the reality while the available ground truth data on the\nactual transportation choices needed to infer the model is often incomplete or\ninconsistent. The present paper addresses those challenges by offering an\nindividual mode-choice and mode-shift simulation model and the Bayesian\ninference framework. It accounts for uncertainties in the data as well as the\nmodel estimate and translates them into uncertainties of the resulting\nmode-shift and the impacts. The framework is evaluated on the two intervention\ncases: introducing ride-sharing for-hire-vehicles in NYC as well the recent\nintroduction of the Manhattan Congestion Surcharge. Being successfully\nevaluated on the cases above, the framework can be used for assessing\nmode-shift and resulting economic, social and environmental implications for\nany future urban transportation solutions and policies being considered by\ndecision-makers or transportation companies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:41:49 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Khulbe", "Devashish", ""], ["Kang", "Chaogui", ""], ["Sobolevsky", "Stanislav", ""]]}, {"id": "2010.06830", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Ian Hunter", "title": "Coarse-Grained Nonlinear System Identification", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Coarse-Grained Nonlinear Dynamics, an efficient and universal\nparameterization of nonlinear system dynamics based on the Volterra series\nexpansion. These models require a number of parameters only quasilinear in the\nsystem's memory regardless of the order at which the Volterra expansion is\ntruncated; this is a superpolynomial reduction in the number of parameters as\nthe order becomes large. This efficient parameterization is achieved by\ncoarse-graining parts of the system dynamics that depend on the product of\ntemporally distant input samples; this is conceptually similar to the\ncoarse-graining that the fast multipole method uses to achieve $\\mathcal{O}(n)$\nsimulation of n-body dynamics. Our efficient parameterization of nonlinear\ndynamics can be used for regularization, leading to Coarse-Grained Nonlinear\nSystem Identification, a technique which requires very little experimental data\nto identify accurate nonlinear dynamic models. We demonstrate the properties of\nthis approach on a simple synthetic problem. We also demonstrate this approach\nexperimentally, showing that it identifies an accurate model of the nonlinear\nvoltage to luminosity dynamics of a tungsten filament with less than a second\nof experimental data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:45:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Spanbauer", "Span", ""], ["Hunter", "Ian", ""]]}, {"id": "2010.06937", "submitter": "Martin Tveten", "authors": "Martin Tveten, Idris A. Eckley, Paul Fearnhead", "title": "Scalable changepoint and anomaly detection in cross-correlated data with\n  an application to condition monitoring", "comments": "48 pages, 25 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a condition monitoring application arising from subsea\nengineering we derive a novel, scalable approach to detecting anomalous mean\nstructure in a subset of correlated multivariate time series. Given the need to\nanalyse such series efficiently we explore a computationally efficient\napproximation of the maximum likelihood solution to the resulting modelling\nframework, and develop a new dynamic programming algorithm for solving the\nresulting Binary Quadratic Programme when the precision matrix of the time\nseries at any given time-point is banded. Through a comprehensive simulation\nstudy, we show that the resulting methods perform favourably compared to\ncompeting methods both in the anomaly and change detection settings, even when\nthe sparsity structure of the precision matrix estimate is misspecified. We\nalso demonstrate its ability to correctly detect faulty time-periods of a pump\nwithin the motivating application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:37:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 18:49:09 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tveten", "Martin", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2010.06999", "submitter": "Sebastian M\\\"uller", "authors": "Abraham Gutierrez, Sebastian M\\\"uller", "title": "Estimations of means and variances in a Markov linear model", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression models and ANOVA are probably the most frequently\napplied methods of all statistical analyses. We study the case where the\npredictors are qualitative variables, and the response variable is\nquantitative. In this case, we propose an alternative to the classic approaches\nthat does not assume homoscedasticity but assumes that a Markov chain can\ndescribe the covariates' correlations. This approach transforms the dependent\ncovariates using a change of measure to independent covariates. The transformed\nestimates allow a pairwise comparison of the mean and variance of the\ncontribution of different values of the covariates. We show that under standard\nmoment conditions, the estimators are asymptotically normally distributed. We\ntest our method with data from simulations and apply it to several classic data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:24:08 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 09:13:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gutierrez", "Abraham", ""], ["M\u00fcller", "Sebastian", ""]]}, {"id": "2010.07107", "submitter": "Janne R\\\"aty", "authors": "Janne R\\\"aty, Rasmus Astrup and Johannes Breidenbach", "title": "Prediction and model-assisted estimation of diameter distributions using\n  Norwegian national forest inventory and airborne laser scanning data", "comments": "Accepted preprint; Canadian Journal of Forest Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diameter at breast height (DBH) distributions offer valuable information for\noperational and strategic forest management decisions. We predicted DBH\ndistributions using Norwegian national forest inventory and airborne laser\nscanning data and compared the predictive performances of linear mixed-effects\n(PPM), generalized linear-mixed (GLM) and k nearest neighbor (NN) models. While\nGLM resulted in smaller prediction errors than PPM, both were clearly\noutperformed by NN. We therefore studied the ability of the NN model to improve\nthe precision of stem frequency estimates by DBH classes in the 8.7 Mha study\narea using a model-assisted (MA) estimator suitable for systematic sampling. MA\nestimates yielded greater than or approximately equal efficiencies as direct\nestimates using field data only. The relative efficiencies (REs) associated\nwith the MA estimates ranged between 0.95-1.47 and 0.96-1.67 for 2 and 6 cm DBH\nclass widths, respectively, when dominant tree species were assumed to be\nknown. The use of a predicted tree species map, instead of the observed\ninformation, decreased the REs by up to 10%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:03:35 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:04:06 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["R\u00e4ty", "Janne", ""], ["Astrup", "Rasmus", ""], ["Breidenbach", "Johannes", ""]]}, {"id": "2010.07348", "submitter": "Adam Peterson", "authors": "Adam Peterson, Veronica Berrocal, Emma Sanchez-Vaznaugh, Brisa Sanchez", "title": "How Close and How Much? Linking Health Outcomes to Built Environment\n  Spatial Distributions", "comments": "23 pages manuscript with 5 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Built environment features (BEFs) refer to aspects of the human constructed\nenvironment, which may in turn support or restrict health related behaviors and\nthus impact health. In this paper we are interested in understanding whether\nthe spatial distribution and quantity of fast food restaurants (FFRs) influence\nthe risk of obesity in schoolchildren. To achieve this goal, we propose a\ntwo-stage Bayesian hierarchical modeling framework. In the first stage,\nexamining the position of FFRs relative to that of some reference locations -\nin our case, schools - we model the distances of FFRs from these reference\nlocations as realizations of Inhomogenous Poisson processes (IPP). With the\ngoal of identifying representative spatial patterns of exposure to FFRs, we\nmodel the intensity functions of the IPPs using a Bayesian non-parametric\nviewpoint and specifying a Nested Dirichlet Process prior. The second stage\nmodel relates exposure patterns to obesity, offering two different approaches\nto accommodate uncertainty in the exposure patterns estimated in the first\nstage: in the first approach the odds of obesity at the school level is\nregressed on cluster indicators, each representing a major pattern of exposure\nto FFRs. In the second, we employ Bayesian Kernel Machine regression to relate\nthe odds of obesity to the multivariate vector reporting the degree of\nsimilarity of a given school to all other schools. Our analysis on the\ninfluence of patterns of FFR occurrence on obesity among Californian\nschoolchildren has indicated that, in 2010, among schools that are consistently\nassigned to a cluster, there is a lower odds of obesity amongst 9th graders who\nattend schools with most distant FFR occurrences in a 1-mile radius as compared\nto others.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:29:54 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Peterson", "Adam", ""], ["Berrocal", "Veronica", ""], ["Sanchez-Vaznaugh", "Emma", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2010.07417", "submitter": "Adriaan-Alexander Ludl", "authors": "Adriaan-Alexander Ludl and Tom Michoel", "title": "Comparison between instrumental variable and mediation-based methods for\n  reconstructing causal gene networks in yeast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal gene networks model the flow of information within a cell, but\nreconstructing them from omics data is challenging because correlation does not\nimply causation. Combining genomics and transcriptomics data from a segregating\npopulation allows to orient the direction of causality between gene expression\ntraits using genomic variants. Instrumental-variable methods (IV) use a local\nexpression quantitative trait locus (eQTL) as a randomized instrument for a\ngene's expression level, and assign target genes based on distal eQTL\nassociations. Mediation-based methods (ME) additionally require that distal\neQTL associations are mediated by the source gene. Here we used Findr, a\nsoftware providing uniform implementations of IV, ME, and coexpression-based\nmethods, a recent dataset of 1,012 segregants from a cross between two budding\nyeast strains, and the YEASTRACT database of known transcriptional interactions\nto compare causal gene network inference methods. We found that causal\ninference methods result in a significant overlap with the ground-truth,\nwhereas coexpression did not perform better than random. A subsampling analysis\nrevealed that the performance of ME decreases at large sample sizes, due to a\nloss of sensitivity when residual correlations become significant. IV methods\ncontain false positive predictions, due to genomic linkage between eQTL\ninstruments. IV and ME methods also have complementary roles for identifying\ncausal genes underlying transcriptional hotspots. IV methods correctly\npredicted STB5 targets for a hotspot centred on the transcription factor STB5,\nwhereas ME failed due to Stb5p auto-regulating its own expression. ME suggests\na new candidate gene, DNM1, for a hotspot on Chr XII, where IV methods could\nnot distinguish between multiple genes located within the hotspot.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:02:23 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 22:21:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ludl", "Adriaan-Alexander", ""], ["Michoel", "Tom", ""]]}, {"id": "2010.07460", "submitter": "Peyman Mohtat", "authors": "Valentin Sulzer, Peyman Mohtat, Suhak Lee, Jason B. Siegel, Anna G.\n  Stefanopoulou", "title": "Promise and Challenges of a Data-Driven Approach for Battery Lifetime\n  Prognostics", "comments": "7 pages, 7 figures, Submitted to American Controls Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent data-driven approaches have shown great potential in early prediction\nof battery cycle life by utilizing features from the discharge voltage curve.\nHowever, these studies caution that data-driven approaches must be combined\nwith specific design of experiments in order to limit the range of aging\nconditions, since the expected life of Li-ion batteries is a complex function\nof various aging factors. In this work, we investigate the performance of the\ndata-driven approach for battery lifetime prognostics with Li-ion batteries\ncycled under a variety of aging conditions, in order to determine when the\ndata-driven approach can successfully be applied. Results show a correlation\nbetween the variance of the discharge capacity difference and the end-of-life\nfor cells aged under a wide range of charge/discharge C-rates and operating\ntemperatures. This holds despite the different conditions being used not only\nto cycle the batteries but also to obtain the features: the features are\ncalculated directly from cycling data without separate slow characterization\ncycles at a controlled temperature. However, the correlation weakens\nconsiderably when the voltage data window for feature extraction is reduced, or\nwhen features from the charge voltage curve instead of discharge are used. As\ndeep constant-current discharges rarely happen in practice, this imposes new\nchallenges for applying this method in a real-world system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:16:49 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Sulzer", "Valentin", ""], ["Mohtat", "Peyman", ""], ["Lee", "Suhak", ""], ["Siegel", "Jason B.", ""], ["Stefanopoulou", "Anna G.", ""]]}, {"id": "2010.07462", "submitter": "Hee-Seok Oh", "authors": "Wookyeong Song and Hee-Seok Oh and Yaeji Lim and Ying Kuen Cheung", "title": "Multi-feature Clustering of Step Data using Multivariate Functional\n  Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new statistical method for clustering step data, a\npopular form of health record data easily obtained from wearable devices. Since\nstep data are high-dimensional and zero-inflated, classical methods such as\nK-means and partitioning around medoid (PAM) cannot be applied directly. The\nproposed method is a novel combination of newly constructed variables that\nreflect the inherent features of step data, such as quantity, strength, and\npattern, and a multivariate functional principal component analysis that can\nintegrate all the features of the step data for clustering. The proposed method\nis implemented by applying a conventional clustering method such as K-means and\nPAM to the multivariate functional principal component scores obtained from\nthese variables. Simulation studies and real data analysis demonstrate\nsignificant improvement in clustering quality.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:17:55 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Song", "Wookyeong", ""], ["Oh", "Hee-Seok", ""], ["Lim", "Yaeji", ""], ["Cheung", "Ying Kuen", ""]]}, {"id": "2010.07594", "submitter": "Xiaohan Yan", "authors": "William B. Nicholson, Xiaohan Yan", "title": "An Improved Online Penalty Parameter Selection Procedure for\n  $\\ell_1$-Penalized Autoregressive with Exogenous Variables", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent developments in the high-dimensional statistical time series\nliterature have centered around time-dependent applications that can be adapted\nto regularized least squares. Of particular interest is the lasso, which both\nserves to regularize and provide feature selection. The lasso requires the\nspecification of a penalty parameter that determines the degree of sparsity to\nimpose. The most popular penalty parameter selection approaches that respect\ntime dependence are very computationally intensive and are not appropriate for\nmodeling certain classes of time series. We propose enhancing a canonical time\nseries model, the autoregressive model with exogenous variables, with a novel\nonline penalty parameter selection procedure that takes advantage of the\nsequential nature of time series data to improve both computational performance\nand forecast accuracy relative to existing methods in both a simulation and\nempirical application involving macroeconomic indicators.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:32:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nicholson", "William B.", ""], ["Yan", "Xiaohan", ""]]}, {"id": "2010.07683", "submitter": "Stephen Wu", "authors": "Stephen Wu, Hironao Yamada, Yoshihiro Hayashi, Massimiliano Zamengo,\n  Ryo Yoshida", "title": "Potentials and challenges of polymer informatics: exploiting machine\n  learning for polymer design", "comments": "This is an English translation of the Japanese manuscript published\n  in Proceedings of the Institute of Statistical Mathematics (2021 special\n  issue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been rapidly growing demand of polymeric materials coming from\ndifferent aspects of modern life because of the highly diverse physical and\nchemical properties of polymers. Polymer informatics is an interdisciplinary\nresearch field of polymer science, computer science, information science and\nmachine learning that serves as a platform to exploit existing polymer data for\nefficient design of functional polymers. Despite many potential benefits of\nemploying a data-driven approach to polymer design, there has been notable\nchallenges of the development of polymer informatics attributed to the complex\nhierarchical structures of polymers, such as the lack of open databases and\nunified structural representation. In this study, we review and discuss the\napplications of machine learning on different aspects of the polymer design\nprocess through four perspectives: polymer databases, representation\n(descriptor) of polymers, predictive models for polymer properties, and polymer\ndesign strategy. We hope that this paper can serve as an entry point for\nresearchers interested in the field of polymer informatics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:02:19 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wu", "Stephen", ""], ["Yamada", "Hironao", ""], ["Hayashi", "Yoshihiro", ""], ["Zamengo", "Massimiliano", ""], ["Yoshida", "Ryo", ""]]}, {"id": "2010.07695", "submitter": "Michele Santacatterina", "authors": "Michele Santacatterina", "title": "Robust weights that optimally balance confounders for estimating\n  marginal hazard ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate balance is crucial in obtaining unbiased estimates of treatment\neffects in observational studies. Methods based on inverse probability weights\nhave been widely used to estimate treatment effects with observational data.\nMachine learning techniques have been proposed to estimate propensity scores.\nThese techniques however target accuracy instead of covariate balance. Methods\nthat target covariate balance have been successfully proposed and largely\napplied to estimate treatment effects on continuous outcomes. However, in many\nmedical and epidemiological applications, the interest lies in estimating\ntreatment effects on time-to-event outcomes. With this type of data, one of the\nmost common estimands of interest is the marginal hazard ratio of the Cox\nproportional hazard model. In this paper, we start by presenting robust\northogonality weights (ROW), a set of weights obtained by solving a quadratic\nconstrained optimization problem that maximizes precision while constraining\ncovariate balance defined as the sample correlation between confounders and\ntreatment. By doing so, ROW optimally deal with both binary and continuous\ntreatments. We then evaluate the performance of the proposed weights in\nestimating marginal hazard ratios of binary and continuous treatments with\ntime-to-event outcomes in a simulation study. We finally apply ROW on the\nevaluation of the effect of hormone therapy on time to coronary heart disease\nand on the effect of red meat consumption on time to colon cancer among 24,069\npostmenopausal women enrolled in the Women's Health Initiative observational\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:12:45 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:34:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Santacatterina", "Michele", ""]]}, {"id": "2010.07857", "submitter": "Antonia Arsova", "authors": "Florian Ziel and Antonia Arsova", "title": "On cointegration for modeling and forecasting wind power production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study evaluates the performance of cointegrated vector autoregressive\n(VAR) models for very short- and short-term wind power forecasting. Preliminary\nresults for a German data set comprising six wind power production time series\nindicate that taking into account potential cointegrating relations between the\nindividual series can improve forecasts at short-term time horizons.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:24:48 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Ziel", "Florian", ""], ["Arsova", "Antonia", ""]]}, {"id": "2010.08016", "submitter": "Nick Doudchenko", "authors": "Nick Doudchenko and Evgeni Drynkin", "title": "Estimation of Discrete Choice Models: A Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method of estimation for discrete choice\ndemand models when individual level data are available. The method employs a\ntwo-step procedure. Step 1 predicts the choice probabilities as functions of\nthe observed individual level characteristics. Step 2 estimates the structural\nparameters of the model using the estimated choice probabilities at a\nparticular point of interest and the moment restrictions. In essence, the\nmethod uses nonparametric approximation (followed by) moment estimation. Hence\nthe name---NAME. We use simulations to compare the performance of NAME with the\nstandard methodology. We find that our method improves precision as well as\nconvergence time. We supplement the analysis by providing the large sample\nproperties of the proposed estimator.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:52:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Doudchenko", "Nick", ""], ["Drynkin", "Evgeni", ""]]}, {"id": "2010.08076", "submitter": "Xiaoyi Yang", "authors": "Xiaoyi Yang, Nynke M.D. Niezink and Rebecca Nugent", "title": "Learning Social Networks from Text Data using Covariate Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing and characterizing the impact of historical figures can be\nchallenging, but unraveling their social structures perhaps even more so.\nHistorical social network analysis methods can help and may also illuminate\npeople who have been overlooked by historians but turn out to be influential\nsocial connection points. Text data, such as biographies, can be a useful\nsource of information about the structure of historical social networks but can\nalso introduce challenges in identifying links. The Local Poisson Graphical\nLasso model leverages the number of co-mentions in the text to measure\nrelationships between people and uses a conditional independence structure to\nmodel a social network. This structure will reduce the tendency to overstate\nthe relationship between \"friends of friends\", but given the historical high\nfrequency of common names, without additional distinguishing information, we\ncan still introduce incorrect links. In this work, we extend the Local Poisson\nGraphical Lasso model with a (multiple) penalty structure that incorporates\ncovariates giving increased link probabilities to people with shared covariate\ninformation. We propose both greedy and Bayesian approaches to estimate the\npenalty parameters. We present results on data simulated with characteristics\nof historical networks and show that this type of penalty structure can improve\nnetwork recovery as measured by precision and recall. We also illustrate the\napproach on biographical data of individuals who lived in early modern Britain,\ntargeting the period from 1500 to 1575.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 00:07:48 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yang", "Xiaoyi", ""], ["Niezink", "Nynke M. D.", ""], ["Nugent", "Rebecca", ""]]}, {"id": "2010.08102", "submitter": "Paul Raschky", "authors": "Klaus Ackermann, Simon D. Angus, Paul A. Raschky", "title": "Estimating Sleep & Work Hours from Alternative Data by Segmented\n  Functional Classification Analysis (SFCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternative data is increasingly adapted to predict human and economic\nbehaviour. This paper introduces a new type of alternative data by\nre-conceptualising the internet as a data-driven insights platform at global\nscale. Using data from a unique internet activity and location dataset drawn\nfrom over 1.5 trillion observations of end-user internet connections, we\nconstruct a functional dataset covering over 1,600 cities during a 7 year\nperiod with temporal resolution of just 15min. To predict accurate temporal\npatterns of sleep and work activity from this data-set, we develop a new\ntechnique, Segmented Functional Classification Analysis (SFCA), and compare its\nperformance to a wide array of linear, functional, and classification methods.\nTo confirm the wider applicability of SFCA, in a second application we predict\nsleep and work activity using SFCA from US city-wide electricity demand\nfunctional data. Across both problems, SFCA is shown to out-perform current\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:13:14 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ackermann", "Klaus", ""], ["Angus", "Simon D.", ""], ["Raschky", "Paul A.", ""]]}, {"id": "2010.08134", "submitter": "Aditya Mishra", "authors": "Aditya Mishra, Dipak K. Dey, Yong Chen, Kun Chen", "title": "Generalized Co-sparse Factor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression techniques are commonly applied to explore the\nassociations between large numbers of outcomes and predictors. In real-world\napplications, the outcomes are often of mixed types, including continuous\nmeasurements, binary indicators, and counts, and the observations may also be\nincomplete. Building upon the recent advances in mixed-outcome modeling and\nsparse matrix factorization, generalized co-sparse factor regression (GOFAR) is\nproposed, which utilizes the flexible vector generalized linear model framework\nand encodes the outcome dependency through a sparse singular value\ndecomposition (SSVD) of the integrated natural parameter matrix. To avoid the\nestimation of the notoriously difficult joint SSVD, GOFAR proposes both\nsequential and parallel unit-rank estimation procedures. By combining the ideas\nof alternating convex search and majorization-minimization, an efficient\nalgorithm with guaranteed convergence is developed to solve the sparse\nunit-rank problem and implemented in the R package gofar. Extensive simulation\nstudies and two real-world applications demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:39:16 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mishra", "Aditya", ""], ["Dey", "Dipak K.", ""], ["Chen", "Yong", ""], ["Chen", "Kun", ""]]}, {"id": "2010.08152", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A multinomial truncated D-vine copula mixed model for the joint\n  meta-analysis of multiple diagnostic tests", "comments": "arXiv admin note: text overlap with arXiv:2006.09278", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an extensive literature on methods for meta-analysis of diagnostic\nstudies, but it mainly focuses on a single test. However, the better\nunderstanding of a particular disease has led to the development of multiple\ntests. A multinomial generalized linear mixed model (GLMM) is recently proposed\nfor the joint meta-analysis of studies comparing multiple tests. We propose a\nnovel model for the joint meta-analysis of multiple tests, which assumes\nindependent multinomial distributions for the counts of each combination of\ntest results in diseased and non-diseased patients, conditional on the latent\nvector of probabilities of each combination of test results in diseased and\nnon-diseased patients. For the random effects distribution of the latent\nproportions, we employ a truncated drawable vine copula that can cover flexible\ndependence structures. The proposed model includes the multinomial GLMM as a\nspecial case, but can also operate on the original scale of the latent\nproportions. Our methodology is demonstrated with a simulation study and using\na meta-analysis of screening for Down syndrome with two tests: shortened\nhumerus and shortened femur. The comparison of our method with the multinomial\nGLMM yields findings in the real data meta-analysis that change the current\nconclusions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:19:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "2010.08186", "submitter": "Saleh Shahinfar", "authors": "Saleh Shahinfar, Paul Meek, Greg Falzon", "title": "How many images do I need? Understanding how sample size per class\n  affects deep learning model performance metrics for balanced designs in\n  autonomous wildlife monitoring", "comments": null, "journal-ref": "Ecological Informatics, 2020, 57:101085", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) algorithms are the state of the art in automated\nclassification of wildlife camera trap images. The challenge is that the\necologist cannot know in advance how many images per species they need to\ncollect for model training in order to achieve their desired classification\naccuracy. In fact there is limited empirical evidence in the context of camera\ntrapping to demonstrate that increasing sample size will lead to improved\naccuracy. In this study we explore in depth the issues of deep learning model\nperformance for progressively increasing per class (species) sample sizes. We\nalso provide ecologists with an approximation formula to estimate how many\nimages per animal species they need for certain accuracy level a priori. This\nwill help ecologists for optimal allocation of resources, work and efficient\nstudy design. In order to investigate the effect of number of training images;\nseven training sets with 10, 20, 50, 150, 500, 1000 images per class were\ndesigned. Six deep learning architectures namely ResNet-18, ResNet-50,\nResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a\ncommon exclusive testing set of 250 images per class. The whole experiment was\nrepeated on three similar datasets from Australia, Africa and North America and\nthe results were compared. Simple regression equations for use by practitioners\nto approximate model performance metrics are provided. Generalized additive\nmodels (GAM) are shown to be effective in modelling DL performance metrics\nbased on the number of training images per class, tuning scheme and dataset.\n  Key-words: Camera Traps, Deep Learning, Ecological Informatics, Generalised\nAdditive Models, Learning Curves, Predictive Modelling, Wildlife.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:28:35 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shahinfar", "Saleh", ""], ["Meek", "Paul", ""], ["Falzon", "Greg", ""]]}, {"id": "2010.08259", "submitter": "Demetrio Lacava", "authors": "Demetrio Lacava and Giampiero M. Gallo and Edoardo Otranto", "title": "Unconventional Policies Effects on Stock Market Volatility: A MAP\n  Approach", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking the European Central Bank unconventional policies as a reference, we\nsuggest a class of Multiplicative Error Models (MEM) taylored to analyze the\nimpact such policies have on stock market volatility. The new set of models,\ncalled MEM with Asymmetry and Policy effects (MAP), keeps the base volatility\ndynamics separate from a component reproducing policy effects, with an increase\nin volatility on announcement days and a decrease unfolding implementation\neffects. When applied to four Eurozone markets, a Model Confidence Set approach\nfinds a significant improvement of the forecasting power of the proxy after the\nExpanded Asset Purchase Programme implementation; a multi--step ahead\nforecasting exercise estimates the duration of the effect, and, by shocking the\npolicy variable, we are able to quantify the reduction in volatility which is\nmore marked for debt--troubled countries.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:22:05 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:45:54 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lacava", "Demetrio", ""], ["Gallo", "Giampiero M.", ""], ["Otranto", "Edoardo", ""]]}, {"id": "2010.08463", "submitter": "Andrii Babii", "authors": "Andrii Babii and Xi Chen and Eric Ghysels and Rohit Kumar", "title": "Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory\n  and an Application to Racial Justice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of asymmetries in prediction problems arising in economics has\nbeen recognized for a long time. In this paper, we focus on binary choice\nproblems in a data-rich environment with general loss functions. In contrast to\nthe asymmetric regression problems, the binary choice with general loss\nfunctions and high-dimensional datasets is challenging and not well understood.\nEconometricians have studied binary choice problems for a long time, but the\nliterature does not offer computationally attractive solutions in data-rich\nenvironments. In contrast, the machine learning literature has many\ncomputationally attractive algorithms that form the basis for much of the\nautomated procedures that are implemented in practice, but it is focused on\nsymmetric loss functions that are independent of individual characteristics.\nOne of the main contributions of our paper is to show that the theoretically\nvalid predictions of binary outcomes with arbitrary loss functions can be\nachieved via a very simple reweighting of the logistic regression, or other\nstate-of-the-art machine learning techniques, such as boosting or (deep) neural\nnetworks. We apply our analysis to racial justice in pretrial detention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:01:20 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:14:42 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 18:32:22 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Babii", "Andrii", ""], ["Chen", "Xi", ""], ["Ghysels", "Eric", ""], ["Kumar", "Rohit", ""]]}, {"id": "2010.08495", "submitter": "Fan Yin", "authors": "Fan Yin, Guanyu Hu, Weining Shen", "title": "Analysis of professional basketball field goal attempts via a Bayesian\n  matrix clustering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric matrix clustering approach to analyze the\nlatent heterogeneity structure in the shot selection data collected from\nprofessional basketball players in the National Basketball Association (NBA).\nThe proposed method adopts a mixture of finite mixtures framework and fully\nutilizes the spatial information via a mixture of matrix normal distribution\nrepresentation. We propose an efficient Markov chain Monte Carlo algorithm for\nposterior sampling that allows simultaneous inference on both the number of\nclusters and the cluster configurations. We also establish large sample\nconvergence properties for the posterior distribution. The excellent empirical\nperformance of the proposed method is demonstrated via simulation studies and\nan application to shot chart data from selected players in the 2017 18 NBA\nregular season.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:53:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yin", "Fan", ""], ["Hu", "Guanyu", ""], ["Shen", "Weining", ""]]}, {"id": "2010.08527", "submitter": "Xianghao Zhan", "authors": "Xianghao Zhan, Yuzhe Liu, Samuel J. Raymond, Hossein Vahid Alizadeh,\n  August G. Domel, Olivier Gevaert, Michael Zeineh, Gerald Grant, David B.\n  Camarillo", "title": "Deep Learning Head Model for Real-time Estimation of Entire Brain\n  Deformation in Concussion", "comments": "12 pages, 6 figures, IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.LG physics.bio-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Many recent studies have suggested that brain deformation\nresulting from a head impact is linked to the corresponding clinical outcome,\nsuch as mild traumatic brain injury (mTBI). Even though several finite element\n(FE) head models have been developed and validated to calculate brain\ndeformation based on impact kinematics, the clinical application of these FE\nhead models is limited due to the time-consuming nature of FE simulations. This\nwork aims to accelerate the process of brain deformation calculation and thus\nimprove the potential for clinical applications. Methods: We propose a deep\nlearning head model with a five-layer deep neural network and feature\nengineering, and trained and tested the model on 1803 total head impacts from a\ncombination of head model simulations and on-field college football and mixed\nmartial arts impacts. Results: The proposed deep learning head model can\ncalculate the maximum principal strain for every element in the entire brain in\nless than 0.001s (with an average root mean squared error of 0.025, and with a\nstandard deviation of 0.002 over twenty repeats with random data partition and\nmodel initialization). The contributions of various features to the predictive\npower of the model were investigated, and it was noted that the features based\non angular acceleration were found to be more predictive than the features\nbased on angular velocity. Conclusion: Trained using the dataset of 1803 head\nimpacts, this model can be applied to various sports in the calculation of\nbrain strain with accuracy, and its applicability can even further be extended\nby incorporating data from other types of head impacts. Significance: In\naddition to the potential clinical application in real-time brain deformation\nmonitoring, this model will help researchers estimate the brain strain from a\nlarge number of head impacts more efficiently than using FE models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:37:59 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 18:50:29 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhan", "Xianghao", ""], ["Liu", "Yuzhe", ""], ["Raymond", "Samuel J.", ""], ["Alizadeh", "Hossein Vahid", ""], ["Domel", "August G.", ""], ["Gevaert", "Olivier", ""], ["Zeineh", "Michael", ""], ["Grant", "Gerald", ""], ["Camarillo", "David B.", ""]]}, {"id": "2010.08601", "submitter": "Honggao Cao", "authors": "Feng Zhang, Ruite Guo and Honggao Cao", "title": "Information Coefficient as a Performance Measure of Stock Selection\n  Models", "comments": "15 pages, 2 figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information coefficient (IC) is a widely used metric for measuring investment\nmanagers' skills in selecting stocks. However, its adequacy and effectiveness\nfor evaluating stock selection models has not been clearly understood, as IC\nfrom a realistic stock selection model can hardly be materially different from\nzero and is often accompanies with high volatility. In this paper, we\ninvestigate the behavior of IC as a performance measure of stick selection\nmodels. Through simulation and simple statistical modeling, we examine the IC\nbehavior both statically and dynamically. The examination helps us propose two\npractical procedures that one may use for IC-based ongoing performance\nmonitoring of stock selection models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:40:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Feng", ""], ["Guo", "Ruite", ""], ["Cao", "Honggao", ""]]}, {"id": "2010.08628", "submitter": "S. Stanley Young", "authors": "Warren B. Kindzierski, S. Stanley Young, Terry G. Meyer and John D.\n  Dunn", "title": "Evaluation of a meta-analysis of ambient air quality as a risk factor\n  for asthma exacerbation", "comments": "Paper with supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False-positive results and bias may be common features of the biomedical\nliterature today, including risk factor-chronic disease research. A study was\nundertaken to assess the reliability of base studies used in a meta-analysis\nexamining whether carbon monoxide, particulate matter 10 and 2.5 micro molar,\nsulfur dioxide, nitrogen dioxide and ozone are risk factors for asthma\nexacerbation (hospital admission and emergency room visits for asthma attack).\nThe number of statistical tests and models were counted in 17 randomly selected\nbase papers from 87 used in the meta-analysis. P-value plots for each air\ncomponent were constructed to evaluate the effect heterogeneity of p-values\nused from all 87 base papers The number of statistical tests possible in the 17\nselected base papers was large, median=15,360 (interquartile range=1,536 to\n40,960), in comparison to results presented. Each p-value plot showed a\ntwo-component mixture with small p-values less than .001 while other p-values\nappeared random (p-values greater than .05). Given potentially large numbers of\nstatistical tests conducted in the 17 selected base papers, p-hacking cannot be\nruled out as explanations for small p-values. Our interpretation of the\nmeta-analysis is that the random p-values indicating null associations are more\nplausible and that the meta-analysis will not likely replicate in the absence\nof bias. We conclude the meta-analysis and base papers used are unreliable and\ndo not offer evidence of value to inform public health practitioners about air\nquality as a risk factor for asthma exacerbation. The following areas are\ncrucial for enabling improvements in risk factor chronic disease observational\nstudies at the funding agency and journal level: preregistration, changes in\nfunding agency and journal editor (and reviewer) practices, open sharing of\ndata and facilitation of reproducibility research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:02:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kindzierski", "Warren B.", ""], ["Young", "S. Stanley", ""], ["Meyer", "Terry G.", ""], ["Dunn", "John D.", ""]]}, {"id": "2010.08875", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Estimating efficacy of measles supplementary immunization activities via\n  discrete-time modeling of disease incidence time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measles is a significant source of global disease burden and child mortality.\nMeasles vaccination through routine immunization (RI) programs in high-burden\nsettings remains a challenge due to poor health care infrastructure and access.\nSupplementary immunization activities (SIA) in the form of vaccination\ncampaigns are therefore implemented to prevent measles outbreaks by reducing\nthe size of the susceptible population. The SIA efficacy, defined as the\nfraction of susceptible population immunized by an SIA, is a critical metric\nfor assessing campaign effectiveness. We propose a discrete-time hidden Markov\nmodel for estimating SIA efficacy and forecasting future incidence trends using\nreported measles incidence data. Our approach extends the time-series\nsusceptible-infected-recovered (TSIR) framework by adding a model component to\ncapture the impact of SIAs on the susceptible population. It also accounts for\nunder-reporting and its associated uncertainty via a two-stage estimation\nprocedure with uncertainty propagation. The proposed model can be used to\nestimate the underlying susceptible population dynamics, assess how many\nsusceptible people were immunized by past SIAs, and forecast incidence trends\nin the future under various hypothetical SIA scenarios. We examine model\nperformance via simulations under various levels of under-reporting, and apply\nthe model to analyze monthly reported measles incidence in Benin from 2012 to\n2018.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:56:36 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2010.08941", "submitter": "Joseph Resch", "authors": "Joseph Resch, Abhyuday Mandal, Pritam Ranjan", "title": "Inverse Problem for Dynamic Computer Simulators via Multiple\n  Scalar-valued Contour Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a dynamic computer simulator that produces a\ntime-series response $y_t(x)$ over $L$ time points, for every given input\nparameter $x$. We propose a method for solving inverse problems, which refer to\nthe finding of a set of inputs that generates a pre-specified simulator output.\nInspired by the sequential approach of contour estimation via expected\nimprovement criterion developed by Ranjan et al. (2008, DOI:\n10.1198/004017008000000541), our proposed method discretizes the target\nresponse series on $k \\; (\\ll L)$ time points, and then iteratively solves $k$\nscalar-valued inverse problems with respect to the discretized targets. We also\npropose to use spline smoothing of the target response series to identify the\noptimal number of knots, $k$, and the actual location of the knots for\ndiscretization. The performance of the proposed methods is compared for several\ntest-function based computer simulators and the motivating real application\nthat uses a rainfall-runoff measurement model named Matlab-Simulink model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 08:28:12 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 04:50:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Resch", "Joseph", ""], ["Mandal", "Abhyuday", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2010.09031", "submitter": "Gustau Camps-Valls", "authors": "Gustau Camps-Valls, Daniel H. Svendsen, Jordi Cort\\'es-Andr\\'es,\n  \\'Alvaro Moreno-Mart\\'inez, Adri\\'an P\\'erez-Suay, Jose Adsuara, Irene\n  Mart\\'in, Maria Piles, Jordi Mu\\~noz-Mar\\'i, Luca Martino", "title": "Living in the Physics and Machine Learning Interplay for Earth\n  Observation", "comments": "24 pages, 10 figures, 3 tables, expanded AAAI PGAI 2020 Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most problems in Earth sciences aim to do inferences about the system, where\naccurate predictions are just a tiny part of the whole problem. Inferences mean\nunderstanding variables relations, deriving models that are physically\ninterpretable, that are simple parsimonious, and mathematically tractable.\nMachine learning models alone are excellent approximators, but very often do\nnot respect the most elementary laws of physics, like mass or energy\nconservation, so consistency and confidence are compromised. In this paper, we\ndescribe the main challenges ahead in the field, and introduce several ways to\nlive in the Physics and machine learning interplay: to encode differential\nequations from data, constrain data-driven models with physics-priors and\ndependence constraints, improve parameterizations, emulate physical models, and\nblend data-driven and process-based models. This is a collective long-term AI\nagenda towards developing and applying algorithms capable of discovering\nknowledge in the Earth system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:58:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Camps-Valls", "Gustau", ""], ["Svendsen", "Daniel H.", ""], ["Cort\u00e9s-Andr\u00e9s", "Jordi", ""], ["Moreno-Mart\u00ednez", "\u00c1lvaro", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Adsuara", "Jose", ""], ["Mart\u00edn", "Irene", ""], ["Piles", "Maria", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Martino", "Luca", ""]]}, {"id": "2010.09107", "submitter": "Chen Xu", "authors": "Chen Xu, Yao Xie", "title": "Conformal prediction interval for dynamic time-series", "comments": "Accepted as a long talk/oral (3% of total submissions) in the\n  Proceedings of the 38th International Conference on Machine Learning, PMLR\n  139, 2021 (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a method to construct distribution-free prediction intervals for\ndynamic time-series, called \\Verb|EnbPI| that wraps around any bootstrap\nensemble estimator to construct sequential prediction intervals. \\Verb|EnbPI|\nis closely related to the conformal prediction (CP) framework but does not\nrequire data exchangeability. Theoretically, these intervals attain\nfinite-sample, \\textit{approximately valid} marginal coverage for broad classes\nof regression functions and time-series with strongly mixing stochastic errors.\nComputationally, \\Verb|EnbPI| avoids overfitting and requires neither\ndata-splitting nor training multiple ensemble estimators; it efficiently\naggregates bootstrap estimators that have been trained. In general,\n\\Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many\nprediction intervals sequentially, and well-suited to a wide range of\nregression functions. We perform extensive real-data analyses to demonstrate\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:05:32 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 14:31:22 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 16:43:03 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 17:41:46 GMT"}, {"version": "v5", "created": "Mon, 10 May 2021 13:15:53 GMT"}, {"version": "v6", "created": "Sat, 15 May 2021 22:21:53 GMT"}, {"version": "v7", "created": "Sun, 23 May 2021 07:58:06 GMT"}, {"version": "v8", "created": "Wed, 2 Jun 2021 02:14:46 GMT"}, {"version": "v9", "created": "Wed, 16 Jun 2021 14:03:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xu", "Chen", ""], ["Xie", "Yao", ""]]}, {"id": "2010.09353", "submitter": "Daniel Grose", "authors": "Alex Fisch, Daniel Grose, Idris A. Eckley, Paul Fearnhead, Lawrence\n  Bardwell", "title": "anomaly : Detection of Anomalous Structure in Time Series Data", "comments": "31 pages, 10 figures. An R package that implements the methods\n  discussed in the paper can be obtained from The Comprehensive R Archive\n  Network (CRAN) via https://cran.r-project.org/web/packages/anomaly/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the contemporary challenges in anomaly detection is the ability to\ndetect, and differentiate between, both point and collective anomalies within a\ndata sequence or time series. The \\pkg{anomaly} package has been developed to\nprovide users with a choice of anomaly detection methods and, in particular,\nprovides an implementation of the recently proposed CAPA family of anomaly\ndetection algorithms. This article describes the methods implemented whilst\nalso highlighting their application to simulated data as well as real data\nexamples contained in the package.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:38:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Fisch", "Alex", ""], ["Grose", "Daniel", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""], ["Bardwell", "Lawrence", ""]]}, {"id": "2010.09376", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Yuanhua Feng, Wolfgang Karl H\\\"ardle", "title": "A data-driven P-spline smoother and the P-Spline-GARCH-models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Penalized spline smoothing of time series and its asymptotic properties are\nstudied. A data-driven algorithm for selecting the smoothing parameter is\ndeveloped. The proposal is applied to define a semiparametric extension of the\nwell-known Spline-GARCH, called a P-Spline-GARCH, based on the log-data\ntransformation of the squared returns. It is shown that now the errors process\nis exponentially strong mixing with finite moments of all orders. Asymptotic\nnormality of the P-spline smoother in this context is proved. Practical\nrelevance of the proposal is illustrated by data examples and simulation. The\nproposal is further applied to value at risk and expected shortfall.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 10:49:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Feng", "Yuanhua", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2010.09549", "submitter": "Sergey Tarima", "authors": "Sergey Tarima and Zhanna Zenkova", "title": "Use of Uncertain Additional Information in Newsvendor Models", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The newsvendor problem is a popular inventory management problem in supply\nchain management and logistics. Solutions to the newsvendor problem determine\noptimal inventory levels. This model is typically fully determined by a\npurchase and sale prices and a distribution of random market demand. From a\nstatistical point of view, this problem is often considered as a quantile\nestimation of a critical fractile which maximizes anticipated profit. The\ndistribution of demand is a random variable and is often estimated on historic\ndata. In an ideal situation, when the probability distribution of the demand is\nknown, one can determine the quantile of a critical fractile minimizing a\nparticular loss function. Since maximum likelihood estimation is asymptotically\nefficient, under certain regularity assumptions, the maximum likelihood\nestimators are used for the quantile estimation problem. Then, the Cramer-Rao\nlower bound determines the lowest possible asymptotic variance. Can one find a\nquantile estimate with a smaller variance then the Cramer-Rao lower bound? If a\nrelevant additional information is available then the answer is yes. Additional\ninformation may be available in different forms. This manuscript considers\nminimum variance and minimum mean squared error estimation for incorporating\nadditional information for estimating optimal inventory levels. By a more\nprecise assessment of optimal inventory levels, we maximize expected profit\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:25:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tarima", "Sergey", ""], ["Zenkova", "Zhanna", ""]]}, {"id": "2010.09563", "submitter": "Andreas Markoulidakis Mr", "authors": "Andreas Markoulidakis, Khadijeh Taiyari, Peter Holmans, Philip\n  Pallmann, Monica Busse, Mark D. Godley, Beth Ann Griffin", "title": "A tutorial comparing different covariate balancing methods with an\n  application evaluating the causal effects of substance use treatment programs\n  for adolescents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials are the gold standard for measuring causal\neffects. However, they are often not always feasible, and causal treatment\neffects must be estimated from observational data. Observational studies do not\nallow robust conclusions about causal relationships unless statistical\ntechniques account for the imbalance of pretreatment confounders across groups\nwhile key assumptions hold. Propensity score and balance weighting (PSBW) are\nuseful techniques that aim to reduce the imbalances between treatment groups by\nweighting the groups to look alike on the observed confounders. There are many\nmethods available to estimate PSBW. However, it is unclear a priori which will\nachieve the best trade-off between covariate balance and effective sample size.\nMoreover, it is critical to assess the validity of key assumptions required for\nrobust estimation of the needed treatment effects, including the overlap and no\nunmeasured confounding assumptions. We present a step-by-step guide to\ncovariate balancing strategies, including how to evaluate overlap, obtain\nestimates of PSBW, check for covariate balance, and assess sensitivity to\nunobserved confounding. We compare the performance of several estimation\nmethods using a case study examining the relative effectiveness of substance\nuse treatment programs and provide a user-friendly web application that can\nimplement the proposed steps.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:47:28 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:41:34 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 14:58:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Markoulidakis", "Andreas", ""], ["Taiyari", "Khadijeh", ""], ["Holmans", "Peter", ""], ["Pallmann", "Philip", ""], ["Busse", "Monica", ""], ["Godley", "Mark D.", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2010.09797", "submitter": "Dara Bahri", "authors": "Dara Bahri, Che Zheng, Yi Tay, Donald Metzler, Andrew Tomkins", "title": "Surprise: Result List Truncation via Extreme Value Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in information retrieval has largely been centered around ranking and\nrelevance: given a query, return some number of results ordered by relevance to\nthe user. The problem of result list truncation, or where to truncate the\nranked list of results, however, has received less attention despite being\ncrucial in a variety of applications. Such truncation is a balancing act\nbetween the overall relevance, or usefulness of the results, with the user cost\nof processing more results. Result list truncation can be challenging because\nrelevance scores are often not well-calibrated. This is particularly true in\nlarge-scale IR systems where documents and queries are embedded in the same\nmetric space and a query's nearest document neighbors are returned during\ninference. Here, relevance is inversely proportional to the distance between\nthe query and candidate document, but what distance constitutes relevance\nvaries from query to query and changes dynamically as more documents are added\nto the index. In this work, we propose Surprise scoring, a statistical method\nthat leverages the Generalized Pareto distribution that arises in extreme value\ntheory to produce interpretable and calibrated relevance scores at query time\nusing nothing more than the ranked scores. We demonstrate its effectiveness on\nthe result list truncation task across image, text, and IR datasets and compare\nit to both classical and recent baselines. We draw connections to hypothesis\ntesting and $p$-values.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:15:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Bahri", "Dara", ""], ["Zheng", "Che", ""], ["Tay", "Yi", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2010.09911", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Kristen M. Altenburger and Farshad Kooti", "title": "Causal Network Motifs: Identifying Heterogeneous Spillover Effects in\n  A/B Tests", "comments": "12 pages; to appear in the Web Conference (WWW) 2021", "journal-ref": null, "doi": "10.1145/3442381.3449845", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments, or \"A/B\" tests, remain the gold standard for\nevaluating the causal effect of a policy intervention or product change.\nHowever, experimental settings, such as social networks, where users are\ninteracting and influencing one another, may violate conventional assumptions\nof no interference for credible causal inference. Existing solutions to the\nnetwork setting include accounting for the fraction or count of treated\nneighbors in a user's network, yet most current methods do not account for the\nlocal network structure beyond simply counting the number of neighbors. Our\nstudy provides an approach that accounts for both the local structure in a\nuser's social network via motifs as well as the treatment assignment conditions\nof neighbors. We propose a two-part approach. We first introduce and employ\n\"causal network motifs\", which are network motifs that characterize the\nassignment conditions in local ego networks; and then we propose a tree-based\nalgorithm for identifying different network interference conditions and\nestimating their average potential outcomes. Our approach can account for\nsocial network theories, such as structural diversity and echo chambers, and\nalso can help specify network interference conditions that are suitable to each\nexperiment. We test our method on a synthetic network setting and on a\nreal-world experiment on a large-scale network, which highlight how accounting\nfor local structures can better account for different interference patterns in\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:01:40 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 21:06:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Yuan", "Yuan", ""], ["Altenburger", "Kristen M.", ""], ["Kooti", "Farshad", ""]]}, {"id": "2010.09958", "submitter": "Shaoyang Ning", "authors": "Dingdong Yi, Shaoyang Ning, Chia-Jung Chang, S. C. Kou", "title": "Forecasting unemployment using Internet search data via PRISM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data generated from the Internet offer great potential for predictive\nanalysis. Here we focus on using online users' Internet search data to forecast\nunemployment initial claims weeks into the future, which provides timely\ninsights into the direction of the economy. To this end, we present a novel\nmethod PRISM (Penalized Regression with Inferred Seasonality Module), which\nuses publicly available online search data from Google. PRISM is a\nsemi-parametric method, motivated by a general state-space formulation, and\nemploys nonparametric seasonal decomposition and penalized regression. For\nforecasting unemployment initial claims, PRISM outperforms all previously\navailable methods, including forecasting during the 2008-2009 financial crisis\nperiod and near-future forecasting during the COVID-19 pandemic period, when\nunemployment initial claims both rose rapidly. The timely and accurate\nunemployment forecasts by PRISM could aid government agencies and financial\ninstitutions to assess the economic trend and make well-informed decisions,\nespecially in the face of economic turbulence.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 01:58:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 21:35:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yi", "Dingdong", ""], ["Ning", "Shaoyang", ""], ["Chang", "Chia-Jung", ""], ["Kou", "S. C.", ""]]}, {"id": "2010.10079", "submitter": "Dinghuai Zhang", "authors": "Yanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville,\n  Zhanxing Zhu", "title": "Neural Approximate Sufficient Statistics for Implicit Models", "comments": "ICLR2021 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the fundamental problem of how to automatically construct summary\nstatistics for implicit generative models where the evaluation of the\nlikelihood function is intractable, but sampling data from the model is\npossible. The idea is to frame the task of constructing sufficient statistics\nas learning mutual information maximizing representations of the data with the\nhelp of deep neural networks. The infomax learning procedure does not need to\nestimate any density or density ratio. We apply our approach to both\ntraditional approximate Bayesian computation and recent neural likelihood\nmethods, boosting their performance on a range of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:11:40 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:35:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Yanzhi", ""], ["Zhang", "Dinghuai", ""], ["Gutmann", "Michael", ""], ["Courville", "Aaron", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "2010.10195", "submitter": "Jack Wilkinson Dr", "authors": "Jack Wilkinson, Stephen A Roberts, Andy Vail", "title": "Multivariate prediction of mixed, multilevel, sequential outcomes\n  arising from in vitro fertilisation", "comments": "28 pages, 2 figures, Stan and R code at osf.io/pmrn3/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vitro fertilization (IVF) comprises a sequence of interventions concerned\nwith the creation and culture of embryos which are then transferred to the\npatient's uterus. While the clinically important endpoint is birth, the\nresponses to each stage of treatment contain additional information about the\nreasons for success or failure. As such, the ability to predict not only the\noverall outcome of the cycle, but also the stage-specific responses, can be\nuseful. This could be done by developing separate models for each response\nvariable, but recent work has suggested that it may be advantageous to use a\nmultivariate approach to model all outcomes simultaneously. Here, joint\nanalysis of the sequential responses is complicated by mixed outcome types\ndefined at two levels (patient and embryo). A further consideration is whether\nand how to incorporate information about the response at each stage in models\nfor subsequent stages. We develop a case study using routinely collected data\nfrom a large reproductive medicine unit in order to investigate the feasibility\nand potential utility of multivariate prediction in IVF. We consider two\npossible scenarios. In the first, stage-specific responses are to be predicted\nprior to treatment commencement. In the second, responses are predicted\ndynamically, using the outcomes of previous stages as predictors. In both\nscenarios, we fail to observe benefits of joint modelling approaches compared\nto fitting separate regression models for each response variable.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:09:53 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wilkinson", "Jack", ""], ["Roberts", "Stephen A", ""], ["Vail", "Andy", ""]]}, {"id": "2010.10320", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "Excess deaths, baselines, Z-scores, P-scores and peaks", "comments": "20 pages 9 figures 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Covid-19 epidemic has lead to comparisons of the countries\nsuffering from it. These are based on the number of excess deaths attributed\neither directly or indirectly to the epidemic. Unfortunately the data on which\nsuch comparisons rely are often incomplete and unreliable. This article\ndiscusses problems of interpretation of data even when the data is largely\naccurate and delayed by at most two to three weeks. This applies to the Office\nof National Statistics in the UK, the Statistisches Bundesamt in Germany and\nthe Belgian statistical office Statbel. The data in the article is taken from\nthese three sources. The number of excess deaths is defined as the number of\ndeaths minus the baseline, the definition of which varies from country to\ncountry. In the UK it is the average number of deaths over the last five years,\nin Germany it is over the last four years and in Belgium over the last 11\nyears. This means that in all cases the individual baselines depend strongly on\nthe timing and intensity of adverse factors such as past influenza epidemics\nand heat waves. This makes cross-country comparisons difficult. A baseline\ndefined as the number the number of deaths in the absence of adverse factors\ncan be operationalized by taking say the 10\\% quantile of the number of deaths.\nThis varies little over time and European countries within given age groups. It\ntherefore enables more robust and accurate comparisons of different countries.\nThe article criticizes the use of Z-scores which distort the comparison between\ncountries. Finally the problem of describing past epidemics by their timing,\nthat is start and finish and time of the maximum, and by their effect, the\nheight of the maximum and the total number of deaths, is considered.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:40:39 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "2010.10415", "submitter": "Andrea Cappozzo", "authors": "Andrea Cappozzo, Ludovic Duponchel, Francesca Greselin, Thomas Brendan\n  Murphy", "title": "Robust variable selection in the framework of classification with label\n  noise and outliers: applications to spectroscopic data in agri-food", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of high-dimensional spectroscopic data is a common task in\nanalytical chemistry. Well-established procedures like support vector machines\n(SVMs) and partial least squares discriminant analysis (PLS-DA) are the most\ncommon methods for tackling this supervised learning problem. Nonetheless,\ninterpretation of these models remains sometimes difficult, and solutions based\non feature selection are often adopted as they lead to the automatic\nidentification of the most informative wavelengths. Unfortunately, for some\ndelicate applications like food authenticity, mislabeled and adulterated\nspectra occur both in the calibration and/or validation sets, with dramatic\neffects on the model development, its prediction accuracy and robustness.\nMotivated by these issues, the present paper proposes a robust model-based\nmethod that simultaneously performs variable selection, outliers and label\nnoise detection. We demonstrate the effectiveness of our proposal in dealing\nwith three agri-food spectroscopic studies, where several forms of\nperturbations are considered. Our approach succeeds in diminishing problem\ncomplexity, identifying anomalous spectra and attaining competitive predictive\naccuracy considering a very low number of selected wavelengths.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:24:08 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:24:37 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cappozzo", "Andrea", ""], ["Duponchel", "Ludovic", ""], ["Greselin", "Francesca", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2010.10471", "submitter": "Chayut Wongkamthong", "authors": "Chayut Wongkamthong, Olanrewaju Akande", "title": "A Comparative Study of Imputation Methods for Multivariate Ordinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data remains a very common problem in large datasets, including\nsurvey and census data containing many ordinal responses, such as political\npolls and opinion surveys. Multiple imputation (MI) is usually the go-to\napproach for analyzing such incomplete datasets, and there are indeed several\nimplementations of MI, including methods using generalized linear models,\ntree-based models, and Bayesian non-parametric models. However, there is\nlimited research on the statistical performance of these methods for\nmultivariate ordinal data. In this article, we perform an empirical evaluation\nof several MI methods, including MI by chained equations (MICE) using\nmultinomial logistic regression models, MICE using proportional odds logistic\nregression models, MICE using classification and regression trees, MICE using\nrandom forest, MI using Dirichlet process (DP) mixtures of products of\nmultinomial distributions, and MI using DP mixtures of multivariate normal\ndistributions. We evaluate the methods using simulation studies based on\nordinal variables selected from the 2018 American Community Survey (ACS). Under\nour simulation settings, the results suggest that MI using proportional odds\nlogistic regression models, classification and regression trees and DP mixtures\nof multinomial distributions generally outperform the other methods. In certain\nsettings, MI using multinomial logistic regression models is able to achieve\ncomparable performance, depending on the missing data mechanism and amount of\nmissing data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:30:57 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 01:11:37 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 00:48:03 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 08:26:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wongkamthong", "Chayut", ""], ["Akande", "Olanrewaju", ""]]}, {"id": "2010.10587", "submitter": "Ramya Hariharan", "authors": "Ramya Hariharan", "title": "When to Relax Social Distancing Measures? An ARIMA Based Forecasting\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spread of the novel coronavirus across various countries is wide and\nrapid. The number of confirmed cases and the reproduction number are some of\nthe epidemiological parameters utilized in scientific studies for the analysis\nand prediction of the viral transmission. The positive rate, an indicator on\nthe extent of testing the population, aids in understanding the severity of the\ninfection in a given geographic location. The positive rate for selected\ncountries has been considered in this study to construct ARIMA based\nstatistical models. The goodness of fit of the models are verified by the\ninvestigation of residuals, Box-Luang test and the forecast error values. The\npositive rates forecasted by the ARIMA models are utilized to investigate the\nscope for implementation of relaxations in social distancing measures in some\ncountries and the necessity to tighten the rules further in some other\ncountries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:48:34 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Hariharan", "Ramya", ""]]}, {"id": "2010.10588", "submitter": "Georgia Salanti", "authors": "Georgia Salanti, Adriani Nikolakopoulou, Orestis Efthimou, Dimitris\n  Mavridis, Matthias Egger, Ian R. White", "title": "Introducing the treatment hierarchy question in network meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Comparative effectiveness research using network meta-analysis\ncan present a hierarchy of competing treatments, from the least to most\npreferable option. However, the research question associated with the hierarchy\nof multiple interventions is never clearly defined in published reviews.\nMethods and Results: We introduce the notion of a treatment hierarchy question\nthat describes the criterion for choosing a specific treatment over one or more\ncompeting alternatives. For example, stakeholders might ask which treatment is\nmost likely to improve mean survival by at least 2 years or which treatment is\nassociated with the longest mean survival. The answers to these two questions\nare not necessarily the same. We discuss the most commonly used ranking metrics\n(quantities that describe or compare the estimated treatment-specific effects),\nhow the metrics produce a treatment hierarchy and the type of treatment\nhierarchy question that each metric can answer. We show that the ranking\nmetrics encompass the uncertainty in the estimation of the treatment effects in\ndifferent ways, which results in different treatment hierarchies. Conclusions:\nNetwork meta-analyses that aim to rank treatments should state in the protocol\nthe treatment hierarchy question they aim to address and employ the appropriate\nranking metric to answer it.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:57:08 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Salanti", "Georgia", ""], ["Nikolakopoulou", "Adriani", ""], ["Efthimou", "Orestis", ""], ["Mavridis", "Dimitris", ""], ["Egger", "Matthias", ""], ["White", "Ian R.", ""]]}, {"id": "2010.10739", "submitter": "Shirley Rojas-Salazar", "authors": "Shirley Rojas-Salazar, Erin M. Schliep, Christopher K. Wikle and\n  Matthew Hawkey", "title": "A Bayesian Hidden Semi-Markov Model with Covariate-Dependent State\n  Duration Parameters for High-Frequency Data from Wearable Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collected by wearable devices in sports provide valuable information\nabout an athlete's behavior such as their activity, performance, and ability.\nThese time series data can be studied with approaches such as hidden Markov and\nsemi-Markov models (HMM and HSMM) for varied purposes including activity\nrecognition and event detection. HSMMs extend the HMM by explicitly modeling\nthe time spent in each state. In a discrete-time HSMM, the duration in each\nstate can be modeled with a zero-truncated Poisson distribution, where the\nduration parameter may be state-specific but constant in time. We extend the\nHSMM by allowing the state-specific duration parameters to vary in time and\nmodel them as a function of known covariates derived from the wearable device\nand observed over a period of time leading up to a state transition. In\naddition, we propose a data subsampling approach given that high-frequency data\nfrom wearable devices can violate the conditional independence assumption of\nthe HSMM. We apply the model to wearable device data collected on a soccer\nreferee in a Major League Soccer game. We model the referee's physiological\nresponse to the game demands and identify important time-varying effects of\nthese demands associated with the duration in each state.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 03:21:13 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rojas-Salazar", "Shirley", ""], ["Schliep", "Erin M.", ""], ["Wikle", "Christopher K.", ""], ["Hawkey", "Matthew", ""]]}, {"id": "2010.10922", "submitter": "S. Stanley Young", "authors": "S. Stanley Young, Kai-Chieh Cheng, Jin Hua Chen, Shu-Chuan Chen,\n  Warren B. Kindzierski", "title": "Reliability of meta-analysis of an association between ambient air\n  quality and development of asthma later in life", "comments": "68 pages including supplemental material. arXiv admin note: text\n  overlap with arXiv:2010.08628", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Claims from observational studies often fail to replicate. A study was\nundertaken to assess the reliability of cohort studies used in a highly cited\nmeta-analysis of the association between ambient nitrogen dioxide, NO2, and\nfine particulate matter, PM2.5, concentrations early in life and development of\nasthma later in life. The numbers of statistical tests possible were estimated\nfor 19 base papers considered for the meta-analysis. A p-value plot for NO2 and\nPM2.5 was constructed to evaluate effect heterogeneity of p-values used from\nthe base papers. The numbers of statistical tests possible in the base papers\nwere large - median 13,824, interquartile range 1,536-221,184; range 96-42M, in\ncomparison to statistical test results presented. Statistical test results\ndrawn from the base papers are unlikely to provide unbiased measures for\nmeta-analysis. The p-value plot indicated that heterogeneity of the NO2 results\nacross the base papers is consistent with a two-component mixture. First, it\nmakes no sense to average across a mixture in meta-analysis. Second, the shape\nof the p-value plot for NO2 appears consistent with the possibility of analysis\nmanipulation to obtain small p-values in several of the cohort studies. As for\nPM2.5, all corresponding p-values fall on a 45-degree line indicating complete\nrandomness rather than a true association. Our interpretation of the\nmeta-analysis is that the random p-values indicating no cause-effect\nassociations are more plausible and that their meta-analysis will not likely\nreplicate in the absence of bias. We conclude that claims made in the base\npapers used for meta-analysis are unreliable due to bias induced by multiple\ntesting and multiple modelling, MTMM. We also show there is evidence that the\nheterogeneity across the base papers used for meta-analysis is more complex\nthan simple sampling from a normal process.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 00:17:38 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Young", "S. Stanley", ""], ["Cheng", "Kai-Chieh", ""], ["Chen", "Jin Hua", ""], ["Chen", "Shu-Chuan", ""], ["Kindzierski", "Warren B.", ""]]}, {"id": "2010.11330", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Nina Katz-Christy, Marianthi-Anna Kioumourtzoglou,\n  Robbie M. Parks, Andrea Schumacher, G. Brooke Anderson", "title": "Integrated causal-predictive machine learning models for tropical\n  cyclone epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic preparedness has been shown to reduce the adverse health impacts of\nhurricanes and tropical storms, referred to collectively as tropical cyclones\n(TCs), but its protective impact could be enhanced by a more comprehensive and\nrigorous characterization of TC epidemiology. To generate the insights and\ntools necessary for high-precision TC preparedness, we develop and apply a\nnovel Bayesian machine learning approach that standardizes estimation of\nhistoric TC health impacts, discovers common patterns and sources of\nheterogeneity in those health impacts, and enables identification of\ncommunities at highest health risk for future TCs. The model integrates (1) a\ncausal inference component to quantify the immediate health impacts of recent\nhistoric TCs at high spatial resolution and (2) a predictive component that\ncaptures how TC meteorological features and socioeconomic/demographic\ncharacteristics of impacted communities are associated with health impacts. We\napply it to a rich data platform containing detailed historic TC exposure\ninformation and Medicare claims data. The health outcomes used in our analyses\nare all-cause mortality and cardiovascular- and respiratory-related\nhospitalizations. We report a high degree of heterogeneity in the acute health\nimpacts of historic TCs at both the TC level and the community level, with\nsubstantial increases in respiratory hospitalizations, on average, during a\ntwo-week period surrounding TCs. TC sustained windspeeds are found to be the\nprimary driver of increased mortality and respiratory risk. Our modeling\napproach has broader utility for predicting the health impacts of many types of\nextreme climate events.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:06:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Katz-Christy", "Nina", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Parks", "Robbie M.", ""], ["Schumacher", "Andrea", ""], ["Anderson", "G. Brooke", ""]]}, {"id": "2010.11385", "submitter": "Dawei Ding", "authors": "Dawei Ding, George Karabatsos", "title": "Dirichlet Process Mixture Models with Shrinkage Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dirichlet Process Mixture (DPM) models for prediction and\ncluster-wise variable selection, based on two choices of shrinkage baseline\nprior distributions for the linear regression coefficients, namely the\nHorseshoe prior and Normal-Gamma prior. We show in a simulation study that each\nof the two proposed DPM models tend to outperform the standard DPM model based\non the non-shrinkage normal prior, in terms of predictive, variable selection,\nand clustering accuracy. This is especially true for the Horseshoe model, and\nwhen the number of covariates exceeds the within-cluster sample size. A real\ndata set is analyzed to illustrate the proposed modeling methodology, where\nboth proposed DPM models again attained better predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:22:10 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 03:55:10 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 17:24:59 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ding", "Dawei", ""], ["Karabatsos", "George", ""]]}, {"id": "2010.11405", "submitter": "Huijing Jiang", "authors": "Ta-Hsin Li, Huijing Jiang, Kevin Tran, Gigi Yuen-Reed, Bob Kelley,\n  Thomas Halvorson", "title": "A Systematic Approach to Surveillance and Detection of Hierarchical\n  Healthcare Cost Drivers and Utilization Offsets", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.08237", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is strong interest among healthcare payers to identify emerging\nhealthcare cost drivers to support early intervention. However, many challenges\narise in analyzing large, high dimensional, and noisy healthcare data. In this\npaper, we propose a systematic approach that utilizes hierarchical search\nstrategies and enhanced statistical process control (SPC) algorithms to surface\nhigh impact cost drivers. Our approach aims to provide interpretable, detailed,\nand actionable insights of detected change patterns attributing to multiple\nclinical factors. We also proposed an algorithm to identify comparable\ntreatment offsets at the population level and quantify the cost impact on their\nutilization changes. To illustrate our approach, we apply it to the IBM Watson\nHealth MarketScan Commercial Database and organized the detected emerging\ndrivers into 5 categories for reporting. We also discuss some findings in this\nanalysis and potential actions in mitigating the impact of the drivers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:08:29 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Li", "Ta-Hsin", ""], ["Jiang", "Huijing", ""], ["Tran", "Kevin", ""], ["Yuen-Reed", "Gigi", ""], ["Kelley", "Bob", ""], ["Halvorson", "Thomas", ""]]}, {"id": "2010.11449", "submitter": "Andrew Song", "authors": "Andrew H. Song, Demba Ba, Emery N. Brown", "title": "PLSO: A generative framework for decomposing nonstationary time-series\n  into piecewise stationary oscillatory components", "comments": "Uncertainty in Artificial Intelligence (UAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the slowly time-varying spectral content of real-world\ntime-series, a common paradigm is to partition the data into approximately\nstationary intervals and perform inference in the time-frequency domain.\nHowever, this approach lacks a corresponding nonstationary time-domain\ngenerative model for the entire data and thus, time-domain inference occurs in\neach interval separately. This results in distortion/discontinuity around\ninterval boundaries and can consequently lead to erroneous inferences based on\nany quantities derived from the posterior, such as the phase. To address these\nshortcomings, we propose the Piecewise Locally Stationary Oscillation (PLSO)\nmodel for decomposing time-series data with slowly time-varying spectra into\nseveral oscillatory, piecewise-stationary processes. PLSO, as a nonstationary\ntime-domain generative model, enables inference on the entire time-series\nwithout boundary effects and simultaneously provides a characterization of its\ntime-varying spectral properties. We also propose a novel two-stage inference\nalgorithm that combines Kalman theory and an accelerated proximal gradient\nalgorithm. We demonstrate these points through experiments on simulated data\nand real neural data from the rat and the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:17:07 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 20:17:34 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 15:32:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Song", "Andrew H.", ""], ["Ba", "Demba", ""], ["Brown", "Emery N.", ""]]}, {"id": "2010.11514", "submitter": "Hanmo Li", "authors": "Hanmo Li and Mengyang Gu", "title": "Robust estimation of SARS-CoV-2 epidemic in US counties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak is asynchronous in US counties. Mitigating the COVID-19\ntransmission requires not only the state and federal level order of protective\nmeasures such as social distancing and testing, but also public awareness of\ntime-dependent risk and reactions at county and community levels. We propose a\nrobust approach to estimate the heterogeneous progression of SARS-CoV-2 at all\nUS counties having no less than 2 COVID-19 associated deaths, and we use the\ndaily probability of contracting (PoC) SARS-CoV-2 for a susceptible individual\nto quantify the risk of SARS-CoV-2 transmission in a community. We found that\nshortening by $5\\%$ of the infectious period of SARS-CoV-2 can reduce around\n$39\\%$ (or $78$K, $95\\%$ CI: $[66$K $, 89$K $]$) of the COVID-19 associated\ndeaths in the US as of 20 September 2020. Our findings also indicate that\nreducing infection and deaths by a shortened infectious period is more\npronounced for areas with the effective reproduction number close to 1,\nsuggesting that testing should be used along with other mitigation measures,\nsuch as social distancing and facial mask-wearing, to reduce the transmission\nrate. Our deliverable includes a dynamic county-level map for local officials\nto determine optimal policy responses and for the public to better understand\nthe risk of contracting SARS-CoV-2 on each day.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:18:07 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 05:20:57 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 01:50:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Hanmo", ""], ["Gu", "Mengyang", ""]]}, {"id": "2010.11617", "submitter": "Gaetano Perone", "authors": "Gaetano Perone", "title": "Comparison of ARIMA, ETS, NNAR and hybrid models to forecast the second\n  wave of COVID-19 hospitalizations in Italy", "comments": "19 pages, 8 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease (COVID-19) is a severe ongoing novel pandemic that has\nemerged in Wuhan, China, in December 2019. As of October 13, the outbreak has\nspread rapidly across the world, affecting over 38 million people, and causing\nover 1 million deaths. In this article, I analysed several time series\nforecasting methods to predict the spread of COVID-19 second wave in Italy,\nover the period after October 13, 2020. I used an autoregressive model (ARIMA),\nan exponential smoothing state space model (ETS), a neural network\nautoregression model (NNAR), and the following hybrid combinations of them:\nARIMA-ETS, ARIMA-NNAR, ETS-NNAR, and ARIMA-ETS-NNAR. About the data, I\nforecasted the number of patients hospitalized with mild symptoms, and in\nintensive care units (ICU). The data refer to the period February 21,\n2020-October 13, 2020 and are extracted from the website of the Italian\nMinistry of Health (www.salute.gov.it). The results show that i) the hybrid\nmodels, except for ARIMA-ETS, are better at capturing the linear and non-linear\nepidemic patterns, by outperforming the respective single models; and ii) the\nnumber of COVID-19-related hospitalized with mild symptoms and in ICU will\nrapidly increase in the next weeks, by reaching the peak in about 50-60 days,\ni.e. in mid-December 2020, at least. To tackle the upcoming COVID-19 second\nwave, on one hand, it is necessary to hire healthcare workers and implement\nsufficient hospital facilities, protective equipment, and ordinary and\nintensive care beds; and on the other hand, it may be useful to enhance social\ndistancing by improving public transport and adopting the double-shifts\nschooling system, for example.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:29:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Perone", "Gaetano", ""]]}, {"id": "2010.11641", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Gennady V. Khvorykh and Andrey V. Khrunin and\n  Stefan Nikoli\\'c and Makhmud Shaban and Elizaveta A. Petrova and Evgeniya A.\n  Koltsova and Fouzi Takelait and Dmitrii Egurnov", "title": "Object-Attribute Biclustering for Elimination of Missing Genotypes in\n  Ischemic Stroke Genome-Wide Data", "comments": "Accepted to AIST 2020", "journal-ref": "AIST 2020 (CCIS series)", "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Missing genotypes can affect the efficacy of machine learning approaches to\nidentify the risk genetic variants of common diseases and traits. The problem\noccurs when genotypic data are collected from different experiments with\ndifferent DNA microarrays, each being characterised by its pattern of uncalled\n(missing) genotypes. This can prevent the machine learning classifier from\nassigning the classes correctly. To tackle this issue, we used well-developed\nnotions of object-attribute biclusters and formal concepts that correspond to\ndense subrelations in the binary relation $\\textit{patients} \\times\n\\textit{SNPs}$. The paper contains experimental results on applying a\nbiclustering algorithm to a large real-world dataset collected for studying the\ngenetic bases of ischemic stroke. The algorithm could identify large dense\nbiclusters in the genotypic matrix for further processing, which in return\nsignificantly improved the quality of machine learning classifiers. The\nproposed algorithm was also able to generate biclusters for the whole dataset\nwithout size constraints in comparison to the In-Close4 algorithm for\ngeneration of formal concepts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:27:43 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 10:29:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Khvorykh", "Gennady V.", ""], ["Khrunin", "Andrey V.", ""], ["Nikoli\u0107", "Stefan", ""], ["Shaban", "Makhmud", ""], ["Petrova", "Elizaveta A.", ""], ["Koltsova", "Evgeniya A.", ""], ["Takelait", "Fouzi", ""], ["Egurnov", "Dmitrii", ""]]}, {"id": "2010.11826", "submitter": "Sophie Mathieu", "authors": "Sophie Mathieu and Rainer von Sachs and V\\'eronique Delouille and\n  Laure Lef\\`evre and Christian Ritter", "title": "Nonparametric robust monitoring of time series panel data", "comments": "58 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, a control procedure is required to detect potential\ndeviations in a panel of serially correlated processes. It is common that the\nprocesses are corrupted by noise and that no prior information about the\nin-control data are available for that purpose. This paper suggests a general\nnonparametric monitoring scheme for supervising such a panel with time-varying\nmean and variance. The method is based on a control chart designed by block\nbootstrap, which does not require parametric assumptions on the distribution of\nthe data. The procedure is tailored to cope with strong noise, potentially\nmissing values and absence of in-control series, which is tackled by an\nintelligent exploitation of the information in the panel. Our methodology is\ncompleted by support vector machine procedures to estimate magnitude and form\nof the encountered deviations (such as stepwise shifts or functional drifts).\nThis scheme, though generic in nature, is able to treat an important applied\ndata problem: the control of deviations in a subset of sunspot number\nobservations which are part of the International Sunspot Number, a world\nreference for long-term solar activity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:03:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mathieu", "Sophie", ""], ["von Sachs", "Rainer", ""], ["Delouille", "V\u00e9ronique", ""], ["Lef\u00e8vre", "Laure", ""], ["Ritter", "Christian", ""]]}, {"id": "2010.11841", "submitter": "Fabian Stephany", "authors": "Fabian Stephany", "title": "When Does it Pay Off to Learn a New Skill? Revealing the Complementary\n  Benefit of Cross-Skilling", "comments": "25 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.SI q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work examines the economic benefits of learning a new skill from a\ndifferent domain: cross-skilling. To assess this, a network of skills from the\njob profiles of 14,790 online freelancers is constructed. Based on this skill\nnetwork, relationships between 3,480 different skills are revealed and marginal\neffects of learning a new skill can be calculated via workers' wages. The\nresults indicate that learning in-demand skills, such as popular programming\nlanguages, is beneficial in general, and that diverse skill sets tend to be\nprofitable, too. However, the economic benefit of a new skill is individual, as\nit complements the existing skill bundle of each worker. As technological and\nsocial transformation is reshuffling jobs' task profiles at a fast pace, the\nfindings of this study help to clarify skill sets required for designing\nindividual re-skilling pathways. This can help to increase employability and\nreduce labour market shortages.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:23:10 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 16:21:48 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 17:59:51 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 15:51:03 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Stephany", "Fabian", ""]]}, {"id": "2010.12017", "submitter": "Behram Wali", "authors": "Behram Wali, Asad Khattak", "title": "Harnessing Ambient Sensing & Naturalistic Driving Systems to Understand\n  Links Between Driving Volatility and Crash Propensity in School Zones: A\n  generalized hierarchical mixed logit framework", "comments": null, "journal-ref": "Volume 114, May 2020, Pages 405-424", "doi": "10.1016/j.trc.2020.01.028", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of seemingly unstructured big data, and through seamless\nintegration of computation and physical components, cyber-physical systems\n(CPS) provide an innovative way to enhance safety and resiliency of transport\ninfrastructure. This study focuses on real world microscopic driving behavior\nand its relevance to school zone safety expanding the capability, usability,\nand safety of dynamic physical systems through data analytics. Driving behavior\nand school zone safety is a public health concern. The sequence of\ninstantaneous driving decisions and its variations prior to involvement in\nsafety critical events, defined as driving volatility, can be a leading\nindicator of safety. By harnessing unique naturalistic data on more than 41,000\nnormal, crash, and near-crash events featuring over 9.4 million temporal\nsamples of real-world driving, a characterization of volatility in microscopic\ndriving decisions is sought at school and non-school zone locations. A big data\nanalytic methodology is proposed for quantifying driving volatility in\nmicroscopic real-world driving decisions. Eight different volatility measures\nare then linked with detailed event specific characteristics, health history,\ndriving history, experience, and other factors to examine crash propensity at\nschool zones. A comprehensive yet fully flexible state-of-the-art generalized\nmixed logit framework is employed to fully account for distinct yet related\nmethodological issues of scale and random heterogeneity, containing multinomial\nlogit, random parameter logit, scaled logit, hierarchical scaled logit, and\nhierarchical generalized mixed logit as special cases. The results reveal that\nboth for school and non-school locations, drivers exhibited greater intentional\nvolatility prior to safety-critical events... ...\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:20:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad", ""]]}, {"id": "2010.12040", "submitter": "Konstantinos Demertzis", "authors": "Konstantinos Demertzis, Lykourgos Magafas and Dimitrios Tsiotas", "title": "Flattening the COVID-19 Curve: The \"Greek\" case in the Global Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global crisis caused by the COVID-19 pandemic, in conjunction with the\neconomic consequences and the collapse of health systems, has raised serious\nconcerns in Europe, which is the most affected continent by the pandemic since\nit recorded 2,388,694 cases and 190,091 deaths (39.6% of the worldwide total),\nof which 71.7% (136,238) are in the United Kingdom (43,414), Italy (34,708),\nFrance (29,778), and Spain (28,338). Unlike other countries, Greece, with about\n310 confirmed cases and 18 deaths per million, is one bright exception in the\nstudy and analysis of this phenomenon. Focusing on the peculiarities of the\ndisease spreading in Greece, both in epidemiological and in implementation\nterms, this paper applies an exploratory analysis of COVID-19 temporal spread\nin Greece and proposes a methodological approach for the modeling and\nprediction of the disease based on the Regression Splines algorithm and the\nchange rate of the total infections. Also, it proposes a hybrid spline\nregression and complex network model of social distance measures evaluating and\ninterpreting the spread of the disease. The overall approach contributes to\ndecision making and support of the public health system and to the fight\nagainst the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:11:44 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Demertzis", "Konstantinos", ""], ["Magafas", "Lykourgos", ""], ["Tsiotas", "Dimitrios", ""]]}, {"id": "2010.12300", "submitter": "Neil Walton", "authors": "Neil Walton, Yuqing Zhang", "title": "Perturbed Pricing", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple randomized rule for the optimization of prices in revenue\nmanagement with contextual information. It is known that the certainty\nequivalent pricing rule, albeit popular, is sub-optimal. We show that, by\nallowing a small amount of randomization around these certainty equivalent\nprices, the benefits of optimal pricing and low regret are achievable.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:12:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Walton", "Neil", ""], ["Zhang", "Yuqing", ""]]}, {"id": "2010.12383", "submitter": "Panayiota Touloupou", "authors": "Panayiota Touloupou, Renata Retkute, T Deirdre Hollingsworth, Simon E.\n  F. Spencer", "title": "Statistical methods for linking geostatistical maps and transmission\n  models: Application to lymphatic filariasis in East Africa", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases remain one of the major causes of human mortality and\nsuffering. Mathematical models have been established as an important tool for\ncapturing the features that drive the spread of the disease, predicting the\nprogression of an epidemic and hence guiding the development of strategies to\ncontrol it. Another important area of epidemiological interest is the\ndevelopment of geostatistical methods for the analysis of data from spatially\nreferenced prevalence surveys. Maps of prevalence are useful, not only for\nenabling a more precise disease risk stratification, but also for guiding the\nplanning of more reliable spatial control programmes by identifying affected\nareas. Despite the methodological advances that have been made in each area\nindependently, efforts to link transmission models and geostatistical maps have\nbeen limited. Motivated by this fact, we developed a Bayesian approach that\ncombines fine-scale geostatistical maps of disease prevalence with transmission\nmodels to provide quantitative, spatially explicit projections of the current\nand future impact of control programs against a disease. These estimates can\nthen be used at a local level to identify the effectiveness of suggested\nintervention schemes and allow investigation of alternative strategies. The\nmethodology has been applied to lymphatic filariasis in East Africa to provide\nestimates of the impact of different intervention strategies against the\ndisease.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:46:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Touloupou", "Panayiota", ""], ["Retkute", "Renata", ""], ["Hollingsworth", "T Deirdre", ""], ["Spencer", "Simon E. F.", ""]]}, {"id": "2010.12449", "submitter": "Stefanie Schwaar", "authors": "Stefanie Schwaar", "title": "A Data-driven Change-point Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The q-weighted CUSUM and their corresponding estimator are well known\nstatistics for change-point detection and estimation. They have the difficulty\nthat the performance is highly dependent on the location of the change. An\nadaptive estimator with data-driven weights is presented to overcome this\nproblem, and it is shown that the corresponding adaptive change-point tests are\nvalid.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:43:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Schwaar", "Stefanie", ""]]}, {"id": "2010.12471", "submitter": "Bangyao Zhao", "authors": "Bangyao Zhao, Lili Zhao", "title": "Data Mining in Large Frequency Tables With Ontology, with an Application\n  to the Vaccine Adverse Event Reporting System", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vaccine safety is a concerning problem of the public, and many signal\ndetecting methods have been developed to identify relative risks between\nvaccines and adverse events (AEs). Those methods usually focus on individual\nAEs, where the randomness of data is high. The results often turn out to be\ninaccurate and lack of clinical meaning. The AE ontology contains information\nabout biological similarity of AEs. Based on this, we extend the concept of\nrelative risks (RRs) to AE group level, which allows the possibility of more\naccurate and meaningful estimation by utilizing data from the whole group. In\nthis paper, we propose the method zGPS.AO (Zero Inflated Gamma Poisson Shrinker\nwith AE ontology) based on the zero inflated negative binomial distribution.\nThis model has two purples: a regression model estimating group level RRs, and\na empirical bayes framework to evaluate AE level RRs. The regression part can\nhandle both excess zeros and over dispersion in the data, and the empirical\nmethod borrows information from both group level and AE level to reduce data\nnoise and stabilize the AE level result. We have demonstrate the unbiaseness\nand low variance features of our model with simulated data, and obtained\nmeaningful results coherent with previous studies on the VAERS (Vaccine Adverse\nEvent Reporting System) database. The proposed methods are implemented in the R\npackage zGPS.AO, which can be installed from the Comprehensive R Archive\nNetwork, CRAN. The results on VAERS data are visualized using the interactive\nweb app Rshiny.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:13:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhao", "Bangyao", ""], ["Zhao", "Lili", ""]]}, {"id": "2010.12474", "submitter": "Soraia Pereira", "authors": "Soraia Pereira, Raquel Menezes, Maria Manuel Ang\\'elico, Tiago Marques", "title": "Geostatistical models for zero-inflated data and extreme values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spatial distribution of animals, during all their life\nphases, as well as how the distributions are influenced by environmental\ncovariates, is a fundamental requirement for the effective management of animal\npopulations. Several geostatistical models have been proposed in the\nliterature, however often the data structure presents an excess of zeros and\nextreme values, which can lead to unreliable estimates when these are ignored\nin the modelling process. To deal with these issues, we propose a\npoint-referenced zero-inflated model to model the probability of presence\ntogether with the positive observations and a point-referenced generalised\nPareto model for the extremes. Finally, we combine the results of these two\nmodels to get the spatial predictions of the variable of interest. We follow a\nBayesian approach and the inference is made using the package R-INLA in the\nsoftware R. Our proposed methodology was illustrated through the analysis of\nthe spatial distribution of sardine eggs density (eggs/$m^3$). The results\nshowed that the combined model for zero-inflated and extreme values improved\nthe spatial prediction accuracy. Accordingly, our conclusion is that it is\nrelevant to consider the data structure in the modelling process. Also, the\nhierarchical model considered can be widely applicable in many ecological\nproblems and even in other contexts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:18:44 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pereira", "Soraia", ""], ["Menezes", "Raquel", ""], ["Ang\u00e9lico", "Maria Manuel", ""], ["Marques", "Tiago", ""]]}, {"id": "2010.12521", "submitter": "Luca Merlo", "authors": "Antonello Maruotti, Luca Merlo and Lea Petrella", "title": "A two-part finite mixture quantile regression model for semi-continuous\n  longitudinal data", "comments": null, "journal-ref": "Statistical Modelling (2021): 1471082X21993603", "doi": "10.1177/1471082X21993603", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a two-part finite mixture quantile regression model for\nsemi-continuous longitudinal data. The proposed methodology allows\nheterogeneity sources that influence the model for the binary response\nvariable, to influence also the distribution of the positive outcomes. As is\ncommon in the quantile regression literature, estimation and inference on the\nmodel parameters are based on the Asymmetric Laplace distribution. Maximum\nlikelihood estimates are obtained through the EM algorithm without parametric\nassumptions on the random effects distribution. In addition, a penalized\nversion of the EM algorithm is presented to tackle the problem of variable\nselection. The proposed statistical method is applied to the well-known RAND\nHealth Insurance Experiment dataset which gives further insights on its\nempirical behavior.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:37:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Maruotti", "Antonello", ""], ["Merlo", "Luca", ""], ["Petrella", "Lea", ""]]}, {"id": "2010.12679", "submitter": "Marco Mingione", "authors": "Pierfrancesco Alaimo Di Loro, Fabio Divino, Alessio Farcomeni,\n  Giovanna Jona Lasinio, Gianfranco Lovison, Antonello Maruotti and Marco\n  Mingione", "title": "Nowcasting COVID-19 incidence indicators during the Italian first\n  outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel parametric regression model is proposed to fit incidence data\ntypically collected during epidemics. The proposal is motivated by real-time\nmonitoring and short-term forecasting of the main epidemiological indicators\nwithin the first outbreak of COVID-19 in Italy. Accurate short-term\npredictions, including the potential effect of exogenous or external variables\nare provided; this ensures to accurately predict important characteristics of\nthe epidemic (e.g., peak time and height), allowing for a better allocation of\nhealth resources over time. Parameters estimation is carried out in a maximum\nlikelihood framework. All computational details required to reproduce the\napproach and replicate the results are provided.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:42:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Di Loro", "Pierfrancesco Alaimo", ""], ["Divino", "Fabio", ""], ["Farcomeni", "Alessio", ""], ["Lasinio", "Giovanna Jona", ""], ["Lovison", "Gianfranco", ""], ["Maruotti", "Antonello", ""], ["Mingione", "Marco", ""]]}, {"id": "2010.12833", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis, Simon Michael Papalexiou,\n  Andreas Langousis, Sina Khatami, Elena Volpi, Salvatore Grimaldi", "title": "Global-scale massive feature extraction from monthly hydroclimatic time\n  series: Statistical characterizations, spatial patterns and hydrological\n  similarity", "comments": null, "journal-ref": "Science of the Total Environment 767 (2021) 144612", "doi": "10.1016/j.scitotenv.2020.144612", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hydroclimatic time series analysis focuses on a few feature types (e.g.,\nautocorrelations, trends, extremes), which describe a small portion of the\nentire information content of the observations. Aiming to exploit a larger part\nof the available information and, thus, to deliver more reliable results (e.g.,\nin hydroclimatic time series clustering contexts), here we approach\nhydroclimatic time series analysis differently, i.e., by performing massive\nfeature extraction. In this respect, we develop a big data framework for\nhydroclimatic variable behaviour characterization. This framework relies on\napproximately 60 diverse features and is completely automatic (in the sense\nthat it does not depend on the hydroclimatic process at hand). We apply the new\nframework to characterize mean monthly temperature, total monthly precipitation\nand mean monthly river flow. The applications are conducted at the global scale\nby exploiting 40-year-long time series originating from over 13 000 stations.\nWe extract interpretable knowledge on seasonality, trends, autocorrelation,\nlong-range dependence and entropy, and on feature types that are met less\nfrequently. We further compare the examined hydroclimatic variable types in\nterms of this knowledge and, identify patterns related to the spatial\nvariability of the features. For this latter purpose, we also propose and\nexploit a hydroclimatic time series clustering methodology. This new\nmethodology is based on Breiman's random forests. The descriptive and\nexploratory insights gained by the global-scale applications prove the\nusefulness of the adopted feature compilation in hydroclimatic contexts.\nMoreover, the spatially coherent patterns characterizing the clusters delivered\nby the new methodology build confidence in its future exploitation...\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:27:17 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 23:31:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""], ["Papalexiou", "Simon Michael", ""], ["Langousis", "Andreas", ""], ["Khatami", "Sina", ""], ["Volpi", "Elena", ""], ["Grimaldi", "Salvatore", ""]]}, {"id": "2010.12977", "submitter": "Srikanta Sannigrahi", "authors": "Srikanta Sannigrahi, Qi Zhang, Francesco Pilla, Bidroha Basu, Arunima\n  Sarkar Basu", "title": "Effects of West Coast forest fire emissions on atmospheric environment:\n  A coupled satellite and ground-based assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forest fires have a profound impact on the atmospheric environment and air\nquality across the ecosystems. The recent west coast forest fire in the United\nStates of America (USA) has broken all the past records and caused severe\nenvironmental and public health burdens. As of middle September, nearly 6\nmillion acres forest area were burned, and more than 25 casualties were\nreported so far. In this study, both satellite and in-situ air pollution data\nwere utilized to examine the effects of this unprecedented wildfire on the\natmospheric environment. The spatiotemporal concentrations of total six air\npollutants, i.e. carbon monoxide (CO), nitrogen dioxide (NO2), sulfur dioxide\n(SO2), ozone (O3), particulate matter (PM2.5 and PM10), and aerosol index (AI),\nwere measured for the periods of 15 August to 15 September for 2020 (fire year)\nand 2019 (reference year). The in-situ data-led measurements show that the\nhighest increases in CO (ppm), PM2.5, and PM10 concentrations ({\\mu}g/m3) were\nclustered around the west coastal fire-prone states, during the 15 August - 15\nSeptember period. The average CO concentration (ppm) was increased most\nsignificantly in Oregon (1147.10), followed by Washington (812.76), and\nCalifornia (13.17). Meanwhile, the concentration ({\\mu}g/m3) in particulate\nmatter (both PM2.5 and PM10), was increased in all three states affected\nseverely by wildfires. Changes (positive) in both PM2.5 and PM10 were measured\nhighest in Washington (45.83 and 88.47 for PM2.5 and PM10), followed by Oregon\n(41.99 and 62.75 for PM2.5 and PM10), and California (31.27 and 35.04 for PM2.5\nand PM10). The average level of exposure to CO, PM2.5, and PM10 was also\nmeasured for all the three fire-prone states. The results of the exposure\nassessment revealed a strong tradeoff association between wildland fire and\nlocal/regional air quality standard.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 20:39:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sannigrahi", "Srikanta", ""], ["Zhang", "Qi", ""], ["Pilla", "Francesco", ""], ["Basu", "Bidroha", ""], ["Basu", "Arunima Sarkar", ""]]}, {"id": "2010.13011", "submitter": "Danielle Braun", "authors": "Gavin Lee, Qing Zhang, Jane W. Liang, Theodore Huang, Christine\n  Choirat, Giovanni Parmigiani, Danielle Braun", "title": "PanelPRO: A R package for multi-syndrome, multi-gene risk modeling for\n  individuals with a family history of cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying individuals who are at high risk of cancer due to inherited\ngermline mutations is critical for effective implementation of personalized\nprevention strategies. Most existing models to identify these individuals focus\non specific syndromes by including family and personal history for a small\nnumber of cancers. Recent evidence from multi-gene panel testing has shown that\nmany syndromes once thought to be distinct are overlapping, motivating the\ndevelopment of models that incorporate family history information on several\ncancers and predict mutations for more comprehensive panels of genes.\n  Once such class of models are Mendelian risk prediction models, which use\nfamily history information and Mendelian laws of inheritance to estimate the\nprobability of carrying genetic mutations, as well as future risk of developing\nassociated cancers. To flexibly model the complexity of many cancer-mutation\nassociations, we present a new software tool called PanelPRO, a R package that\nextends the previously developed BayesMendel R package to user-selected lists\nof susceptibility genes and associated cancers. The model identifies\nindividuals at an increased risk of carrying cancer susceptibility gene\nmutations and predicts future risk of developing hereditary cancers associated\nwith those genes. Additional functionalities adjust for prophylactic\ninterventions, known genetic testing results, and risk modifiers such as race\nand ancestry. The package comes with a customizable database with default\nparameter values estimated from published studies.\n  The PanelPRO package is open-source and provides a fast and flexible back-end\nfor multi-gene, multi-cancer risk modeling with pedigree data. The software\nenables the identification of high-risk individuals, which will have an impact\non personalized prevention strategies for cancer and individualized decision\nmaking about genetic testing.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 01:23:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lee", "Gavin", ""], ["Zhang", "Qing", ""], ["Liang", "Jane W.", ""], ["Huang", "Theodore", ""], ["Choirat", "Christine", ""], ["Parmigiani", "Giovanni", ""], ["Braun", "Danielle", ""]]}, {"id": "2010.13029", "submitter": "Gemeng Zhang", "authors": "Gemeng Zhang, Aiying Zhang, Biao Cai, Zhuozhuo Tu, Vince D. Calhoun,\n  Yu-Ping Wang", "title": "Detecting abnormal connectivity in schizophrenia via a joint directed\n  acyclic graph estimation model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity (FC) has been widely used to study brain network\ninteractions underlying the emerging cognition and behavior of an individual.\nFC is usually defined as the correlation or partial correlation between brain\nregions. Although FC is proved to be a good starting point to understand the\nbrain organization, it fails to tell the causal relationship or the direction\nof interactions. Many directed acyclic graph (DAG) based methods were applied\nto study the directed interactions using functional magnetic resonance imaging\n(fMRI) data but the performance was severely limited by the small sample size\nand high dimensionality, hindering its applications. To overcome the obstacles,\nwe propose a score based joint directed acyclic graph model to estimate the\ndirected FC in fMRI data. Instead of using a combinatorial optimization\nframework, the structure of DAG is characterized with an algebra equation and\nfurther regularized with sparsity and group similarity terms. The simulation\nresults have demonstrated the improved accuracy of the proposed model in\ndetecting causality as compared to other existing methods. In our case-control\nstudy of the MIND Clinical Imaging Consortium (MCIC) data, we have successfully\nidentified decreased functional integration, disrupted hub structures and\ncharacteristic edges (CtEs) in schizophrenia (SZ) patients. Further comparison\nbetween the results from directed FC and undirected FC illustrated the their\ndifferent emphasis on selected features. We speculate that combining the\nfeatures from undirected graphical model and directed graphical model might be\na promising way to do FC analysis.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 04:24:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Gemeng", ""], ["Zhang", "Aiying", ""], ["Cai", "Biao", ""], ["Tu", "Zhuozhuo", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2010.13061", "submitter": "Trong-Nghia Nguyen", "authors": "T.-N. Nguyen, M.-N. Tran, and R. Kohn", "title": "Recurrent Conditional Heteroskedasticity", "comments": "47 pages, 17 figures, 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of financial volatility models, which we call the\nREcurrent Conditional Heteroskedastic (RECH) models, to improve both the\nin-sample analysis and out-of-sample forecast performance of the traditional\nconditional heteroskedastic models. In particular, we incorporate auxiliary\ndeterministic processes, governed by recurrent neural networks, into the\nconditional variance of the traditional conditional heteroskedastic models,\ne.g. the GARCH-type models, to flexibly capture the dynamics of the underlying\nvolatility. The RECH models can detect interesting effects in financial\nvolatility overlooked by the existing conditional heteroskedastic models such\nas the GARCH (Bollerslev, 1986), GJR (Glosten et al., 1993) and EGARCH (Nelson,\n1991). The new models often have good out-of-sample forecasts while still\nexplain well the stylized facts of financial volatility by retaining the\nwell-established structures of the econometric GARCH-type models. These\nproperties are illustrated through simulation studies and applications to four\nreal stock index datasets. An user-friendly software package together with the\nexamples reported in the paper are available at https://github.com/vbayeslab.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 08:09:29 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nguyen", "T. -N.", ""], ["Tran", "M. -N.", ""], ["Kohn", "R.", ""]]}, {"id": "2010.13164", "submitter": "Nafiseh Ghoroghchian Ms.", "authors": "Nafiseh Ghoroghchian, Stark C. Draper, and Roman Genov", "title": "A Hierarchical Graph Signal Processing Approach to Inference from\n  Spatiotemporal Signals", "comments": null, "journal-ref": "In 2018 29th Biennial Symposium on Communications (BSC) (pp. 1-5).\n  IEEE (2018, June)", "doi": "10.1109/BSC.2018.8494688", "report-no": null, "categories": "eess.SP cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the emerging area of graph signal processing (GSP), we introduce\na novel method to draw inference from spatiotemporal signals. Data acquisition\nin different locations over time is common in sensor networks, for diverse\napplications ranging from object tracking in wireless networks to medical uses\nsuch as electroencephalography (EEG) signal processing. In this paper we\nleverage novel techniques of GSP to develop a hierarchical feature extraction\napproach by mapping the data onto a series of spatiotemporal graphs. Such a\nmodel maps signals onto vertices of a graph and the time-space dependencies\namong signals are modeled by the edge weights. Signal components acquired from\ndifferent locations and time often have complicated functional dependencies.\nAccordingly, their corresponding graph weights are learned from data and used\nin two ways. First, they are used as a part of the embedding related to the\ntopology of graph, such as density. Second, they provide the connectivities of\nthe base graph for extracting higher level GSP-based features. The latter\ninclude the energies of the signal's graph Fourier transform in different\nfrequency bands. We test our approach on the intracranial EEG (iEEG) data set\nof the Kaggle epileptic seizure detection contest. In comparison to the winning\ncode, the results show a slight net improvement and up to 6 percent improvement\nin per subject analysis, while the number of features are decreased by 75\npercent on average.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 17:08:13 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ghoroghchian", "Nafiseh", ""], ["Draper", "Stark C.", ""], ["Genov", "Roman", ""]]}, {"id": "2010.13190", "submitter": "Srikanth Chandar", "authors": "Srikanth Chandar, Muvazima Mansoor, Mohina Ahmadi, Hrishikesh Badve,\n  Deepesh Sahoo, Bharath Katragadda", "title": "Machine Learning Based Network Coverage Guidance System", "comments": "5 pages, 3 figures, Submitted to ITNAC IEEE 2020, Melbourne", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of 4G, there has been a huge consumption of data and the\navailability of mobile networks has become paramount. Also, with the burst of\nnetwork traffic based on user consumption, data availability and network\nanomalies have increased substantially. In this paper, we introduce a novel\napproach, to identify the regions that have poor network connectivity thereby\nproviding feedback to both the service providers to improve the coverage as\nwell as to the customers to choose the network judiciously. In addition to\nthis, the solution enables customers to navigate to a better mobile network\ncoverage area with stronger signal strength location using Machine Learning\nClustering Algorithms, whilst deploying it as a Mobile Application. It also\nprovides a dynamic visual representation of varying network strength and range\nacross nearby geographical areas.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 19:01:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chandar", "Srikanth", ""], ["Mansoor", "Muvazima", ""], ["Ahmadi", "Mohina", ""], ["Badve", "Hrishikesh", ""], ["Sahoo", "Deepesh", ""], ["Katragadda", "Bharath", ""]]}, {"id": "2010.13259", "submitter": "Kleyton Da Costa Mr.", "authors": "Kleyton Vieira Sales da Costa and Felipe Leite Coelho da Silva and\n  Josiane da Silva Cordeiro Coelho", "title": "Forecasting Quarterly Brazilian GDP: Univariate Models Approach", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gross domestic product (GDP) is an important economic indicator that\naggregates useful information to assist economic agents and policymakers in\ntheir decision-making process. In this context, GDP forecasting becomes a\npowerful decision optimization tool in several areas. In order to contribute in\nthis direction, we investigated the efficiency of classical time series models\nand the class of state-space models, applied to Brazilian gross domestic\nproduct. The models used were: a Seasonal Autoregressive Integrated Moving\nAverage (SARIMA) and a Holt-Winters method, which are classical time series\nmodels; and the dynamic linear model, a state-space model. Based on statistical\nmetrics of model comparison, the dynamic linear model presented the best\nforecasting model and fit performance for the analyzed period, also\nincorporating the growth rate structure significantly.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 00:25:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["da Costa", "Kleyton Vieira Sales", ""], ["da Silva", "Felipe Leite Coelho", ""], ["Coelho", "Josiane da Silva Cordeiro", ""]]}, {"id": "2010.13332", "submitter": "Tianchen Xu", "authors": "Tianchen Xu, Kun Chen, Gen Li", "title": "The More Data, the Better? Demystifying Deletion-Based Methods in Linear\n  Regression with Missing Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two deletion-based methods for dealing with the problem of missing\nobservations in linear regression analysis. One is the complete-case analysis\n(CC, or listwise deletion) that discards all incomplete observations and only\nuses common samples for ordinary least-squares estimation. The other is the\navailable-case analysis (AC, or pairwise deletion) that utilizes all available\ndata to estimate the covariance matrices and applies these matrices to\nconstruct the normal equation. We show that the estimates from both methods are\nasymptotically unbiased and further compare their asymptotic variances in some\ntypical situations. Surprisingly, using more data (i.e., AC) does not\nnecessarily lead to better asymptotic efficiency in many scenarios. Missing\npatterns, covariance structure and true regression coefficient values all play\na role in determining which is better. We further conduct simulation studies to\ncorroborate the findings and demystify what has been missed or misinterpreted\nin the literature. Some detailed proofs and simulation results are available in\nthe online supplemental materials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:25:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Tianchen", ""], ["Chen", "Kun", ""], ["Li", "Gen", ""]]}, {"id": "2010.13375", "submitter": "Janet Aisbett", "authors": "Janet Aisbett, Eric J. Drinkwater, Kenneth L. Quarrie, Stephen\n  Woodcock", "title": "Advancing statistical decision-making in sports science", "comments": "23 pages plus 5 pages Supplemental material; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The magnitude-based decisions (MBD) procedure was developed within sports\nscience as an alternative to null hypothesis significance tests. It aimed to\nemphasise effect sizes and discourage dichotomous decision-making. The use of\nMBD was banned by some sports science journals following claims it lacks a\ntheoretical foundation and leads to high Type I error rates. To address these\nclaims, we first generalise contour-enhanced funnel plots to allow for ranges\nof meaningful effect sizes, then relate regions defined in these plots to the\ndecisions made by MBD. We then mathematically show how MBD fits within a class\nof multiple decision procedures. We have implemented this theoretically sound\nversion of MBD as a visualisation tool that supports generalised funnel plots.\nThe use of MBD could encourage researchers to plan test directionalities, test\nlevels and error definitions, and the visualisation tool may help stakeholders\nengage with the design of analyses and the interpretation of trial findings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:03:49 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 01:34:32 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 03:59:32 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Aisbett", "Janet", ""], ["Drinkwater", "Eric J.", ""], ["Quarrie", "Kenneth L.", ""], ["Woodcock", "Stephen", ""]]}, {"id": "2010.13599", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow and Cyrus Samii and Ye Wang", "title": "Design-Based Inference for Spatial Experiments with Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider design-based causal inference in settings where randomized\ntreatments have effects that bleed out into space in complex ways that overlap\nand in violation of the standard \"no interference\" assumption for many causal\ninference methods. We define a spatial \"average marginalized response,\" which\ncharacterizes how, in expectation, units of observation that are a specified\ndistance from an intervention point are affected by treatments at that point,\naveraging over effects emanating from other intervention points. We establish\nconditions for non-parametric identification, asymptotic distributions of\nestimators, and recovery of structural effects. We propose methods for both\nsample-theoretic and permutation-based inference. We provide illustrations\nusing randomized field experiments on forest conservation and health.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:15:31 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:14:02 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Wang", "Ye", ""]]}, {"id": "2010.13604", "submitter": "Stefan Stein", "authors": "Stefan Stein, Chenlei Leng", "title": "A Sparse $\\beta$-Model with Covariates for Networks", "comments": "73 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of networks are increasingly encountered in modern science\nand humanity. This paper concerns a new generative model, suitable for sparse\nnetworks commonly observed in practice, to capture degree heterogeneity and\nhomophily, two stylized features of a typical network. The former is achieved\nby differentially assigning parameters to individual nodes, while the latter is\nmaterialized by incorporating covariates. Similar models in the literature for\nheterogeneity often include as many nodal parameters as the number of nodes,\nleading to over-parametrization and, as a result, strong requirements on the\ndensity of the network. For parameter estimation, we propose the use of the\npenalized likelihood method with an $\\ell_1$ penalty on the nodal parameters,\ngiving rise to a convex optimization formulation which immediately connects our\nestimation procedure to the LASSO literature. We highlight the differences of\nour approach to the LASSO method for logistic regression, emphasizing the\nfeasibility of our model to conduct inference for sparse networks, study the\nfinite-sample error bounds on the excess risk and the $\\ell_1$-error of the\nresulting estimator, and develop a central limit theorem for the parameter\nassociated with the covariates. Simulation and data analysis corroborate the\ndeveloped theory. As a by-product of our main theory, we study what we call the\nErd\\H{o}s-R\\'{e}nyi model with covariates and develop the associated\nstatistical inference for sparse networks, which can be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:19:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Stein", "Stefan", ""], ["Leng", "Chenlei", ""]]}, {"id": "2010.13682", "submitter": "Timothy DeLise", "authors": "Timothy DeLise", "title": "Data Segmentation via t-SNE, DBSCAN, and Random Forest", "comments": "7 pages, 7 figures, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research proposes a data segmentation algorithm which combines t-SNE,\nDBSCAN, and Random Forest classifier to form an end-to-end pipeline that\nseparates data into natural clusters and produces a characteristic profile of\neach cluster based on the most important features. Out-of-sample cluster labels\ncan be inferred, and the technique generalizes well on real data sets. We\ndescribe the algorithm and provide case studies using the Iris and MNIST data\nsets, as well as real social media site data from Instagram. This is a proof of\nconcept and sets the stage for further in-depth theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:59:15 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 18:41:52 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["DeLise", "Timothy", ""]]}, {"id": "2010.13704", "submitter": "Denis Rustand", "authors": "Denis Rustand, Janet van Niekerk, Haavard Rue, Christophe Tournigand,\n  Virginie Rondeau, Laurent Briollais", "title": "Bayesian Estimation of Two-Part Joint Models for a Longitudinal\n  Semicontinuous Biomarker and a Terminal Event with R-INLA: Interests for\n  Cancer Clinical Trial Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-part joint models for a longitudinal semicontinuous biomarker and a\nterminal event have been recently introduced based on frequentist estimation.\nThe biomarker distribution is decomposed into a probability of positive value\nand the expected value among positive values. Shared random effects can\nrepresent the association structure between the biomarker and the terminal\nevent. The computational burden increases compared to standard joint models\nwith a single regression model for the biomarker. In this context, the\nfrequentist estimation implemented in the R package frailtypack can be\nchallenging for complex models (i.e., large number of parameters and dimension\nof the random effects). As an alternative, we propose a Bayesian estimation of\ntwo-part joint models based on the Integrated Nested Laplace Approximation\n(INLA) algorithm to alleviate the computational burden and fit more complex\nmodels. Our simulation studies show that R-INLA reduces the computation time\nsubstantially as well as the variability of the parameter estimates and\nimproves the model convergence compared to frailtypack. We contrast the\nBayesian and frequentist approaches in the analysis of two randomized cancer\nclinical trials (GERCOR and PRIME studies), where R-INLA suggests a stronger\nassociation between the biomarker and the risk of event. Moreover, the Bayesian\napproach was able to characterize subgroups of patients associated with\ndifferent responses to treatment in the PRIME study while frailtypack had\nconvergence issues. Our study suggests that the Bayesian approach using R-INLA\nalgorithm enables broader applications of the two-part joint model to clinical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:41:09 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:56:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rustand", "Denis", ""], ["van Niekerk", "Janet", ""], ["Rue", "Haavard", ""], ["Tournigand", "Christophe", ""], ["Rondeau", "Virginie", ""], ["Briollais", "Laurent", ""]]}, {"id": "2010.13898", "submitter": "Jinghang Lin", "authors": "Jinghang Lin, Xiaoran Tong, Chenxi Li, Qing Lu", "title": "Expectile Neural Networks for Genetic Data Analysis of Complex Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genetic etiologies of common diseases are highly complex and\nheterogeneous. Classic statistical methods, such as linear regression, have\nsuccessfully identified numerous genetic variants associated with complex\ndiseases. Nonetheless, for most complex diseases, the identified variants only\naccount for a small proportion of heritability. Challenges remain to discover\nadditional variants contributing to complex diseases. Expectile regression is a\ngeneralization of linear regression and provides completed information on the\nconditional distribution of a phenotype of interest. While expectile regression\nhas many nice properties and holds great promise for genetic data analyses\n(e.g., investigating genetic variants predisposing to a high-risk population),\nit has been rarely used in genetic research. In this paper, we develop an\nexpectile neural network (ENN) method for genetic data analyses of complex\ndiseases. Similar to expectile regression, ENN provides a comprehensive view of\nrelationships between genetic variants and disease phenotypes and can be used\nto discover genetic variants predisposing to sub-populations (e.g., high-risk\ngroups). We further integrate the idea of neural networks into ENN, making it\ncapable of capturing non-linear and non-additive genetic effects (e.g.,\ngene-gene interactions). Through simulations, we showed that the proposed\nmethod outperformed an existing expectile regression when there exist complex\nrelationships between genetic variants and disease phenotypes. We also applied\nthe proposed method to the genetic data from the Study of Addiction: Genetics\nand Environment(SAGE), investigating the relationships of candidate genes with\nsmoking quantity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:07:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lin", "Jinghang", ""], ["Tong", "Xiaoran", ""], ["Li", "Chenxi", ""], ["Lu", "Qing", ""]]}, {"id": "2010.14026", "submitter": "Matthias Kormaksson", "authors": "Matthias Kormaksson (1), Luke J. Kelly (2), Xuan Zhu (1), Sibylle\n  Haemmerle (1), Luminita Pricop (1), and David Ohlssen (1) ((1) Novartis\n  Pharmaceuticals Corporation, (2) Oxford University)", "title": "Sequential knockoffs for continuous and categorical predictors: with\n  application to a large Psoriatic Arthritis clinical trial pool", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knockoffs provide a general framework for controlling the false discovery\nrate when performing variable selection. Much of the Knockoffs literature\nfocuses on theoretical challenges and we recognize a need for bringing some of\nthe current ideas into practice. In this paper we propose a sequential\nalgorithm for generating knockoffs when underlying data consists of both\ncontinuous and categorical (factor) variables. Further, we present a heuristic\nmultiple knockoffs approach that offers a practical assessment of how robust\nthe knockoff selection process is for a given data set. We conduct extensive\nsimulations to validate performance of the proposed methodology. Finally, we\ndemonstrate the utility of the methods on a large clinical data pool of more\nthan $2,000$ patients with psoriatic arthritis evaluated in 4 clinical trials\nwith an IL-17A inhibitor, secukinumab (Cosentyx), where we determine prognostic\nfactors of a well established clinical outcome. The analyses presented in this\npaper could provide a wide range of applications to commonly encountered data\nsets in medical practice and other fields where variable selection is of\nparticular interest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:11:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kormaksson", "Matthias", ""], ["Kelly", "Luke J.", ""], ["Zhu", "Xuan", ""], ["Haemmerle", "Sibylle", ""], ["Pricop", "Luminita", ""], ["Ohlssen", "David", ""]]}, {"id": "2010.14128", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, D. Sirl, S. Preston, I. L. Dryden, M. J. A. Ellis, B.\n  Perrat, J. Goulding", "title": "The Bayesian Spatial Bradley--Terry Model: Urban Deprivation Modeling in\n  Tanzania", "comments": "23 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the most deprived regions of any country or city is key if policy\nmakers are to design successful interventions. However, locating areas with the\ngreatest need is often surprisingly challenging in developing countries. Due to\nthe logistical challenges of traditional household surveying, official\nstatistics can be slow to be updated; estimates that exist can be coarse, a\nconsequence of prohibitive costs and poor infrastructures; and mass\nurbanisation can render manually surveyed figures rapidly out-of-date.\nComparative judgement models, such as the Bradley--Terry model, offer a\npromising solution. Leveraging local knowledge, elicited via comparisons of\ndifferent areas' affluence, such models can both simplify logistics and\ncircumvent biases inherent to house-hold surveys. Yet widespread adoption\nremains limited, due to the large amount of data existing approaches still\nrequire. We address this via development of a novel Bayesian Spatial\nBradley--Terry model, which substantially decreases the amount of data\ncomparisons required for effective inference. This model integrates a network\nrepresentation of the city or country, along with assumptions of spatial\nsmoothness that allow deprivation in one area to be informed by neighbouring\nareas. We demonstrate the practical effectiveness of this method, through a\nnovel comparative judgement data set collected in Dar es Salaam, Tanzania.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:40:26 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 09:06:45 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 13:15:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Seymour", "R. G.", ""], ["Sirl", "D.", ""], ["Preston", "S.", ""], ["Dryden", "I. L.", ""], ["Ellis", "M. J. A.", ""], ["Perrat", "B.", ""], ["Goulding", "J.", ""]]}, {"id": "2010.14262", "submitter": "Stefano Puliti", "authors": "Stefano Puliti (1), Johannes Breidenbach (1), Johannes Schumacher (1),\n  Marius Hauglin (1), Torgeir Ferdinand Klingenberg (2), Rasmus Astrup (1) ((1)\n  Norwegian Institute for Bioeconomy Research (NIBIO) Division of Forest and\n  Forest Resources National Forest Inventory department, (2) Norwegian Mapping\n  Authority (Kartverket) Land Mapping Division)", "title": "Above-ground biomass change estimation using national forest inventory\n  data with Sentinel-2 and Landsat 8", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aimed at estimating total forest above-ground net change (Delta\nAGB, Mt) over five years (2014-2019) based on model-assisted estimation\nutilizing freely available satellite imagery. The study was conducted for a\nboreal forest area (approx. 1.4 Mill hectares) in Norway where bi-temporal\nnational forest inventory (NFI), Sentinel-2, and Landsat data were available.\nBiomass change was modelled based on a direct approach. The precision of\nestimates using only the NFI data in a basic expansion estimator were compared\nto four different alternative model-assisted estimates using 1) Sentinel-2 or\nLandsat data, and 2) using bi- or uni-temporal remotely sensed data. We found\nthat the use of remotely sensed data improved the precision of the purely\nfield-based estimates by a factor of up to three. The most precise estimates\nwere found for the model-assisted estimation using bi-temporal Sentinel-2\n(standard error; SE= 1.7 Mt). However, the decrease in precision when using\nLandsat data was small (SE= 1.92 Mt). In addition, we found that Delta AGB\ncould be precisely estimated also when remotely sensed data were available only\nat the end of the monitoring period. We conclude that satellite optical data\ncan considerably improve Delta AGB estimates, even in those cases where\nrepeated and coincident NFI data are available. The free availability, global\ncoverage, frequent update, and long-term time horizon make data from programs\nsuch as Sentinel-2 and Landsat a valuable data source for a consistent and\ndurable monitoring of forest carbon dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:02:07 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Puliti", "Stefano", ""], ["Breidenbach", "Johannes", ""], ["Schumacher", "Johannes", ""], ["Hauglin", "Marius", ""], ["Klingenberg", "Torgeir Ferdinand", ""], ["Astrup", "Rasmus", ""]]}, {"id": "2010.14359", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa and Vaibhav Rajan", "title": "Improved Inference of Gaussian Mixture Copula Model for Clustering and\n  Reproducibility Analysis using Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas provide a modular parameterization of multivariate distributions that\ndecouples the modeling of marginals from the dependencies between them.\nGaussian Mixture Copula Model (GMCM) is a highly flexible copula that can model\nmany kinds of multi-modal dependencies, as well as asymmetric and tail\ndependencies. They have been effectively used in clustering non-Gaussian data\nand in Reproducibility Analysis, a meta-analysis method designed to verify the\nreliability and consistency of multiple high-throughput experiments. Parameter\nestimation for GMCM is challenging due to its intractable likelihood. The best\nprevious methods have maximized a proxy-likelihood through a Pseudo Expectation\nMaximization (PEM) algorithm. They have no guarantees of convergence or\nconvergence to the correct parameters. In this paper, we use Automatic\nDifferentiation (AD) tools to develop a method, called AD-GMCM, that can\nmaximize the exact GMCM likelihood. In our simulation studies and experiments\nwith real data, AD-GMCM finds more accurate parameter estimates than PEM and\nyields better performance in clustering and Reproducibility Analysis. We\ndiscuss the advantages of an AD-based approach, to address problems related to\nmonotonic increase of likelihood and parameter identifiability in GMCM. We also\nanalyze, for GMCM, two well-known cases of degeneracy of maximum likelihood in\nGMM that can lead to spurious clustering solutions. Our analysis shows that,\nunlike GMM, GMCM is not affected in one of the cases.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:37:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "2010.14491", "submitter": "Lijing Wang", "authors": "Lijing Wang, Aniruddha Adiga, Srinivasan Venkatramanan, Jiangzhuo\n  Chen, Bryan Lewis, Madhav Marathe", "title": "Examining Deep Learning Models with Multiple Data Sources for COVID-19\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic represents the most significant public health disaster\nsince the 1918 influenza pandemic. During pandemics such as COVID-19, timely\nand reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep\nlearning-based time series models for forecasting have recently gained\npopularity and have been successfully used for epidemic forecasting. Here we\nfocus on the design and analysis of deep learning-based models for COVID-19\nforecasting. We implement multiple recurrent neural network-based deep learning\nmodels and combine them using the stacking ensemble technique. In order to\nincorporate the effects of multiple factors in COVID-19 spread, we consider\nmultiple sources such as COVID-19 confirmed and death case count data and\ntesting data for better predictions. To overcome the sparsity of training data\nand to address the dynamic correlation of the disease, we propose\nclustering-based training for high-resolution forecasting. The methods help us\nto identify the similar trends of certain groups of regions due to various\nspatio-temporal effects. We examine the proposed method for forecasting weekly\nCOVID-19 new confirmed cases at county-, state-, and country-level. A\ncomprehensive comparison between different time series models in COVID-19\ncontext is conducted and analyzed. The results show that simple deep learning\nmodels can achieve comparable or better performance when compared with more\ncomplicated models. We are currently integrating our methods as a part of our\nweekly forecasts that we provide state and federal authorities.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:52:02 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 22:46:40 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wang", "Lijing", ""], ["Adiga", "Aniruddha", ""], ["Venkatramanan", "Srinivasan", ""], ["Chen", "Jiangzhuo", ""], ["Lewis", "Bryan", ""], ["Marathe", "Madhav", ""]]}, {"id": "2010.14889", "submitter": "Manoj Kumar Babu", "authors": "Manoj Babu, Pasquale Franciosa, Prashanth Shekar, Darek Ceglarek", "title": "Object shape error modelling and simulation during early design stage by\n  morphing Gaussian Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric and dimensional variations in objects are caused by inevitable\nuncertainties in manufacturing processes and often lead to product quality\nissues. Failing to model the effect object shape errors, i.e., geometric and\ndimensional errors of parts, early during design phase inhibits the ability to\npredict such quality issues; consequently leading to expensive design changes\nafter freezing of design. State-of-Art methodologies for modelling and\nsimulating object shape error have limited defect fidelity, data versatility,\nand designer centricity that prevent their effective application during early\ndesign phase. Overcoming these limitations a novel Morphing Gaussian Random\nField (MGRF) methodology for object shape error modelling and simulation is\npresented in this paper. The MGRF methodology has (i) high defect fidelity and\nis capable of simulating various part defects including local and global\ndeformations, and technological patterns; (ii) high data versatility and can\neffectively simulate non-ideal parts under the constraint of limited data\navailability and can utilise historical non-ideal part data; (iii) designer\ncentric capabilities such as performing `what if?' analysis of practically\nrelevant defects; and (iv) capability to generate non-ideal parts conforming to\nstatistical form tolerance specification. The aforementioned capabilities\nenable MGRF methodology to accurately model and simulate the effect of object\nshape variations on product quality during the early design phase. This is\nachieved by first, modelling the spatial correlation in the deviations of the\npart from its design nominal using Gaussian Random Field and then, utilising\nthe modelled spatial correlations to generate non-ideal parts by conditional\nsimulations. Practical applications of developed MGRF methodology and its\nadvantages are demonstrated using sport-utility-vehicle door parts.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:17:24 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 19:28:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Babu", "Manoj", ""], ["Franciosa", "Pasquale", ""], ["Shekar", "Prashanth", ""], ["Ceglarek", "Darek", ""]]}, {"id": "2010.15207", "submitter": "Andrew Lawson", "authors": "Andrew B. Lawson and Joanne Kim", "title": "Space-Time Covid-19 Bayesian SIR modeling in South Carolina", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242777", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 pandemic has spread across the world since the beginning of\n2020. Many regions have experienced its effects. The state of South Carolina in\nthe USA has seen cases since early March 2020 and a primary peak in early April\n2020. A lockdown was imposed on April 6th but lifting of restrictions started\non April 24th. The daily case and death data as reported by NCHS (deaths) via\nthe New York Times GitHUB repository have been analyzed and approaches to\nmodeling of the data are presented. Prediction is also considered and the role\nof asymptomatic transmission is assessed as a latent unobserved effect. Two\ndifferent time periods are examined and one step prediction is provided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:13:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lawson", "Andrew B.", ""], ["Kim", "Joanne", ""]]}, {"id": "2010.15263", "submitter": "Gustavo Schwenkler", "authors": "Jean-Paul Renne and Guillaume Roussellet and Gustavo Schwenkler", "title": "Preventing COVID-19 Fatalities: State versus Federal Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are COVID-19 fatalities large when a federal government does not enforce\ncontainment policies and instead allow states to implement their own policies?\nWe answer this question by developing a stochastic extension of a SIRD\nepidemiological model for a country composed of multiple states. Our model\nallows for interstate mobility. We consider three policies: mask mandates,\nstay-at-home orders, and interstate travel bans. We fit our model to daily U.S.\nstate-level COVID-19 death counts and exploit our estimates to produce various\npolicy counterfactuals. While the restrictions imposed by some states inhibited\na significant number of virus deaths, we find that more than two-thirds of U.S.\nCOVID-19 deaths could have been prevented by late November 2020 had the federal\ngovernment enforced federal mandates as early as some of the earliest states\ndid. Our results quantify the benefits of early actions by a federal government\nfor the containment of a pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 22:28:42 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 03:14:04 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 17:47:27 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 22:27:39 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Renne", "Jean-Paul", ""], ["Roussellet", "Guillaume", ""], ["Schwenkler", "Gustavo", ""]]}, {"id": "2010.15351", "submitter": "Yves Isma\\\"el Ngounou Bakam", "authors": "Yves I. Ngounou Bakam and Denys Pommeret", "title": "Nonparametric estimation of copulas and copula densities by orthogonal\n  projections", "comments": "42 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study nonparametric estimators of copulas and copula\ndensities. We first focus our study on a density copula estimator based on a\npolynomial orthogonal projection of the joint density. A new copula estimator\nis then deduced. Its asymptotic properties are studied: we provide a large\nfunctional class for which this construction is optimal in the minimax and\nmaxiset sense and we propose a method selection for the smoothing parameter. An\nintensive simulation study shows the very good performance of both copulas and\ncopula densities estimators which we compare to a large panel of competitors. A\nreal dataset in actuarial science illustrates this approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 04:24:31 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Bakam", "Yves I. Ngounou", ""], ["Pommeret", "Denys", ""]]}, {"id": "2010.15368", "submitter": "Chi Chang", "authors": "Chi Chang, Kimberly Kelly, M. Lee Van Horn, Richard T. Houang, Joseph\n  Gardiner, Laurie Van Egeren, Heng-Chieh Wu", "title": "Classification Accuracy and Parameter Estimation in Multilevel Contexts:\n  A Study of Conditional Nonparametric Multilevel Latent Class Analysis", "comments": "40 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research has two aims. First, to demonstrate the utility\nconditional nonparametric multilevel latent class analysis (NP-MLCA) for\nmulti-site program evaluation using an empirical dataset. Second, to\ninvestigate how classification accuracy and parameter estimation of a\nconditional NP-MLCA are affected by six study factors: the quality of latent\nclass indicators, the number of latent class indicators, level-1 covariate\neffects, cross-level covariate effects, the number of level-2 units, and the\nsize of level-2 units. A total of 96 conditions was examined using a simulation\nstudy. The resulting classification accuracy rates, the power and type-I error\nof cross-level covariate effects and contextual effects suggest that the\nnonparametric multilevel latent class model can be applied broadly in\nmultilevel contexts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:12:11 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 06:32:52 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Chang", "Chi", ""], ["Kelly", "Kimberly", ""], ["Van Horn", "M. Lee", ""], ["Houang", "Richard T.", ""], ["Gardiner", "Joseph", ""], ["Van Egeren", "Laurie", ""], ["Wu", "Heng-Chieh", ""]]}, {"id": "2010.15427", "submitter": "Laurent Duval", "authors": "Arthur Marmin and Marc Castella and Jean-Christophe Pesquet and\n  Laurent Duval", "title": "Sparse Signal Reconstruction for Nonlinear Models via Piecewise Rational\n  Optimization", "comments": null, "journal-ref": "Signal Processing, Volume 179, February 2021, 107835 Signal\n  Processing Volume 179, February 2021, 107835", "doi": "10.1016/j.sigpro.2020.107835", "report-no": null, "categories": "math.OC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to reconstruct sparse signals degraded by a nonlinear\ndistortion and acquired at a limited sampling rate. Our method formulates the\nreconstruction problem as a nonconvex minimization of the sum of a data fitting\nterm and a penalization term. In contrast with most previous works which settle\nfor approximated local solutions, we seek for a global solution to the obtained\nchallenging nonconvex problem. Our global approach relies on the so-called\nLasserre relaxation of polynomial optimization. We here specifically include in\nour approach the case of piecewise rational functions, which makes it possible\nto address a wide class of nonconvex exact and continuous relaxations of the\n$\\ell_0$ penalization function. Additionally, we study the complexity of the\noptimization problem. It is shown how to use the structure of the problem to\nlighten the computational burden efficiently. Finally, numerical simulations\nillustrate the benefits of our method in terms of both global optimality and\nsignal reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:05:19 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:20:22 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Marmin", "Arthur", ""], ["Castella", "Marc", ""], ["Pesquet", "Jean-Christophe", ""], ["Duval", "Laurent", ""]]}, {"id": "2010.15588", "submitter": "Carlos Medel-Ram\\'irez", "authors": "Carlos Medel-Ramirez, Hilario Medel-Lopez", "title": "Impact of (SARS-CoV-2) COVID 19 on the indigenous language-speaking\n  population in Mexico", "comments": "20 pages, 1 figure, 2 maps", "journal-ref": null, "doi": "10.13140/RG.2.2.12730.82887/2", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The importance of the working document is that it allows the analysis of the\ninformation and the status of cases associated with (SARS-CoV-2) COVID-19 as\nopen data at the municipal, state and national level, with a daily record of\npatients, according to a age, sex, comorbidities, for the condition of\n(SARS-CoV-2) COVID-19 according to the following characteristics: a) Positive,\nb) Negative, c) Suspicious. Likewise, it presents information related to the\nidentification of an outpatient and / or hospitalized patient, attending to\ntheir medical development, identifying: a) Recovered, b) Deaths and c) Active,\nin Phase 3 and Phase 4, in the five main population areas speaker of indigenous\nlanguage in the State of Veracruz - Mexico. The data analysis is carried out\nthrough the application of a data mining algorithm, which provides the\ninformation, fast and timely, required for the estimation of Medical Care\nScenarios of (SARS-CoV-2) COVID-19, as well as for know the impact on the\nindigenous language-speaking population in Mexico.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:24:53 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Medel-Ramirez", "Carlos", ""], ["Medel-Lopez", "Hilario", ""]]}, {"id": "2010.15677", "submitter": "Sonja J\\\"ackle", "authors": "Sonja J\\\"ackle, Elias R\\\"oger, Volker Dicken, Benjamin Geisler, Jakob\n  Schumacher and Max Westphal", "title": "A statistical model to assess risk for supporting SARS-CoV-2 quarantine\n  decisions", "comments": "This manuscript is a preprint and is submitted to IJERPH", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In February 2020 the first human infection with SARS-CoV-2 was reported in\nGermany. Since then the local public health offices have been responsible to\nmonitor and react to the dynamics of the pandemic. One of their major tasks is\nto contain the spread of the virus after potential spreading events, for\nexample when one or multiple participants have a positive test result after a\ngroup meeting (e.g. at school, at a sports event or at work). In this case,\ncontacts of the infected person have to be traced and potentially are\nquarantined (at home) for a period of time. When all relevant contact persons\nobtain a negative polymerase chain reaction (PCR) test result, the quarantine\nmay be stopped. However, tracing and testing of all contacts is time-consuming,\ncostly and (thus) not always feasible. This motivates our work, in which we\npresent a statistical model for the probability that no transmission of\nSars-CoV-2 occurred given an arbitrary number of test results at potentially\ndifferent timepoints. Hereby, the time-dependent sensitivity and specificity of\nthe conducted PCR test are taken in account. We employ a parametric Bayesian\nmodel which can be adopted to different situations when specific prior\nknowledge is available. This is illustrated for group events in German school\nclasses and applied to exemplary real-world data from this context. Our\napproach has the potential to support important quarantine decisions with the\ngoal to achieve a better balance between necessary containment of the pandemic\nand preservation of social and economic life. The focus of future work should\nbe on further refinement and evaluation of quarantine decisions based on our\nstatistical model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:23:00 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:48:16 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 09:38:37 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 11:45:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["J\u00e4ckle", "Sonja", ""], ["R\u00f6ger", "Elias", ""], ["Dicken", "Volker", ""], ["Geisler", "Benjamin", ""], ["Schumacher", "Jakob", ""], ["Westphal", "Max", ""]]}, {"id": "2010.15754", "submitter": "Srikanta Sannigrahi", "authors": "Arabinda Maiti, Qi Zhang, Srikanta Sannigrahi, Suvamoy Pramanik, Suman\n  Chakraborti, Francesco Pilla", "title": "Spatiotemporal effects of the causal factors on COVID-19 incidences in\n  the contiguous United States", "comments": null, "journal-ref": "Sustainable Cities and Society, 2021", "doi": "10.1016/j.scs.2021.102784", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since December 2019, the world has been witnessing the gigantic effect of an\nunprecedented global pandemic called Severe Acute Respiratory Syndrome\nCoronavirus (SARS-CoV-2) - COVID-19. So far, 38,619,674 confirmed cases and\n1,093,522 confirmed deaths due to COVID-19 have been reported. In the United\nStates (US), the cases and deaths are recorded as 7,833,851 and 215,199.\nSeveral timely researches have discussed the local and global effects of the\nconfounding factors on COVID-19 casualties in the US. However, most of these\nstudies considered little about the time varying associations between and among\nthese factors, which are crucial for understanding the outbreak of the present\npandemic. Therefore, this study adopts various relevant approaches, including\nlocal and global spatial regression models and machine learning to explore the\ncausal effects of the confounding factors on COVID-19 counts in the contiguous\nUS. Totally five spatial regression models, spatial lag model (SLM), ordinary\nleast square (OLS), spatial error model (SEM), geographically weighted\nregression (GWR) and multiscale geographically weighted regression (MGWR), are\nperformed at the county scale to take into account the scale effects on\nmodelling. For COVID-19 cases, ethnicity, crime, and income factors are found\nto be the strongest covariates and explain the maximum model variances. For\nCOVID-19 deaths, both (domestic and international) migration and income factors\nplay a crucial role in explaining spatial differences of COVID-19 death counts\nacross counties. The local coefficient of determination (R2) values derived\nfrom the GWR and MGWR models are found very high over the\nWisconsin-Indiana-Michigan (the Great Lake) region, as well as several parts of\nTexas, California, Mississippi and Arkansas.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:58:47 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Maiti", "Arabinda", ""], ["Zhang", "Qi", ""], ["Sannigrahi", "Srikanta", ""], ["Pramanik", "Suvamoy", ""], ["Chakraborti", "Suman", ""], ["Pilla", "Francesco", ""]]}, {"id": "2010.15777", "submitter": "Srikanta Sannigrahi", "authors": "Arabinda Maiti, Suman Chakraborti, Suvamoy Pramanik, Srikanta\n  Sannigrahi", "title": "COVID-19 incidences and its association with environmental quality: A\n  country-level assessment in India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study explored the association between the five key air pollutants\n(Nitrogen Dioxide (NO2), Sulphur Dioxide (SO2), Particulate Matter (PM2.5,\nPM10), and Carbon Monoxide (CO)) and COVID-19 incidences in India. The COVID-19\nconfirmed cases, air pollution concentration and meteorological variables\n(temperature, wind speed, surface pressure) for district and city scale were\nobtained for 2019 and 2020. The location-based air pollution observations were\nconverted to a raster surface using interpolation. The deaths and positive\ncases are reported so far were found highest in Mumbai (436 and 11394),\nfollowed by Ahmedabad (321 and 4991), Pune (129 and 2129), Kolkata (99 and\n783), Indore (83 and 1699), Jaipur (53 and 1111), Ujjain (42 and 201), Surat\n(37 and 799), Vadodara (31 and 400), Chennai (23 and 2647), Bhopal (22 and\n652), Thane (21 and 1889), respectively. Unlike the other studies, this study\nhas not found any substantial association between air pollution and COVID-19\nincidences at the district level. Considering the number of confirmed cases,\nthe coefficient of determination (R2) values estimated as 0.003 for PM2.5,\n0.002 for PM10 and SO2, 0.001 for CO, and 0.0002 for NO2, respectively. This\nsuggests an absolute no significant association between air pollution and\nCOVID-19 incidences (both confirmed cases and death) in India. The same\nassociation was observed for the number of deaths as well. For COVID-19\nconfirmed cases, none of the five pollutants has exhibited any statistically\nsignificant association. Additionally, except the wind speed, the climate\nvariables have no produced any statistically significant association with the\nCOVID-19 incidences.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:24:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Maiti", "Arabinda", ""], ["Chakraborti", "Suman", ""], ["Pramanik", "Suvamoy", ""], ["Sannigrahi", "Srikanta", ""]]}, {"id": "2010.15835", "submitter": "Dean Eckles", "authors": "Jeremy Yang, Dean Eckles, Paramveer Dhillon, Sinan Aral", "title": "Targeting for long-term outcomes", "comments": "main text is 24 pages, with 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-makers often want to target interventions (e.g., marketing\ncampaigns) so as to maximize an outcome that is observed only in the long-term.\nThis typically requires delaying decisions until the outcome is observed or\nrelying on simple short-term proxies for the long-term outcome. Here we build\non the statistical surrogacy and off-policy learning literature to impute the\nmissing long-term outcomes and then approximate the optimal targeting policy on\nthe imputed outcomes via a doubly-robust approach. We apply our approach in\nlarge-scale proactive churn management experiments at The Boston Globe by\ntargeting optimal discounts to its digital subscribers to maximize their\nlong-term revenue. We first show that conditions for validity of average\ntreatment effect estimation with imputed outcomes are also sufficient for valid\npolicy evaluation and optimization; furthermore, these conditions can be\nsomewhat relaxed for policy optimization. We then validate this approach\nempirically by comparing it with a policy learned on the ground truth long-term\noutcomes and show that they are statistically indistinguishable. Our approach\nalso outperforms a policy learned on short-term proxies for the long-term\noutcome. In a second field experiment, we implement the optimal targeting\npolicy with additional randomized exploration, which allows us to update the\noptimal policy for each new cohort of customers to account for potential\nnon-stationarity. Over three years, our approach had a net-positive revenue\nimpact in the range of $4-5 million compared to The Boston Globe's current\npolicies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:31:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Yang", "Jeremy", ""], ["Eckles", "Dean", ""], ["Dhillon", "Paramveer", ""], ["Aral", "Sinan", ""]]}, {"id": "2010.15889", "submitter": "Amanda Stathopoulos", "authors": "Elisa Borowski and Jason Soria and Joseph Schofer and Amanda\n  Stathopoulos", "title": "Disparities in ridesourcing demand for mobility resilience: A multilevel\n  analysis of neighborhood effects in Chicago, Illinois", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility resilience refers to the ability of individuals to complete their\ndesired travel despite unplanned disruptions to the transportation system. The\npotential of new on-demand mobility options, such as ridesourcing services, to\nfill unpredicted gaps in mobility is an underexplored source of adaptive\ncapacity. Applying a natural experiment approach to newly released ridesourcing\ndata, we examine variation in the gap-filling role of on-demand mobility during\nsudden shocks to a transportation system by analyzing the change in use of\nridesourcing during unexpected rail transit service disruptions across the\nracially and economically diverse city of Chicago. Using a multilevel mixed\nmodel, we control not only for the immediate station attributes where the\ndisruption occurs, but also for the broader context of the community area and\ncity quadrant in a three-level structure. Thereby the unobserved variability\nacross neighborhoods can be associated with differences in factors such as\ntransit ridership, or socio-economic status of residents, in addition to\ncontrolling for station level effects. Our findings reveal that individuals use\nridesourcing as a gap-filling mechanism during rail transit disruptions, but\nthere is strong variation across situational and locational contexts.\nSpecifically, our results show larger increases in transit disruption\nresponsive ridesourcing during weekdays, nonholidays, and more severe\ndisruptions, as well as in community areas that have higher percentages of\nWhite residents and transit commuters, and on the more affluent northside of\nthe city. These findings point to new insights with far-reaching implications\non how ridesourcing complements existing transport networks by providing added\ncapacity during disruptions but does not appear to bring equitable gap-filling\nbenefits to low-income communities of color that typically have more limited\nmobility options.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 19:06:37 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Borowski", "Elisa", ""], ["Soria", "Jason", ""], ["Schofer", "Joseph", ""], ["Stathopoulos", "Amanda", ""]]}, {"id": "2010.15943", "submitter": "Michael Law", "authors": "Brian McNair, Eric Margolin, Michael Law, Ya'acov Ritov", "title": "The Hot Hand and Its Effect on the NBA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to revisit and expand upon previous work on the \"hot hand\"\nphenomenon in basketball, specifically in the NBA. Using larger, modern data\nsets, we test streakiness of shooting patterns and the presence of hot hand\nbehavior in free throw shooting, while going further by examining league-wide\nhot hand trends and the changes in individual player behavior. Additionally, we\nperform simulations in order to assess their power. While we find no evidence\nof the hot hand in game-play and only weak evidence in free throw trials, we\nfind that some NBA players exhibit behavioral changes based on the outcome of\ntheir previous shot.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:59:33 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["McNair", "Brian", ""], ["Margolin", "Eric", ""], ["Law", "Michael", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2010.15972", "submitter": "Karthik Srinivasan", "authors": "Karthik Srinivasan, Amit Kumar, Parameshwaran Iyer, Abhinav Joshi", "title": "Manufacturing Process Optimization using Statistical Methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response Surface Methodology (RSM) introduced in the paper (Box & Wilson,\n1951) explores the relationships between explanatory and response variables in\ncomplex settings and provides a framework to identify correct settings for the\nexplanatory variables to yield the desired response. RSM involves setting up\nsequential experimental designs followed by application of elementary\noptimization methods to identify direction of improvement in response. In this\npaper, an application of RSM using a two-factor two-level Central Composite\nDesign (CCD) is explained for a diesel engine nozzle manufacturing sub-process.\nThe analysis shows that one of the factors has a significant influence in\nimproving desired values of the response. The implementation of RSM is done\nusing the DoE plug-in available in R software.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:28:45 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Srinivasan", "Karthik", ""], ["Kumar", "Amit", ""], ["Iyer", "Parameshwaran", ""], ["Joshi", "Abhinav", ""]]}, {"id": "2010.16025", "submitter": "Rushani Wijesuriya", "authors": "Rushani Wijesuriya, Margarita Moreno-Betancur, John B. Carlin, Anurika\n  P. De Silva and Katherine J. Lee", "title": "Evaluation of approaches for accommodating interactions and non-linear\n  terms in multiple imputation of incomplete three-level data", "comments": "34 pages, 5 tables and 9 figures (without additional files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-level data structures arising from repeated measures on individuals\nclustered within larger units are common in health research studies. Missing\ndata are prominent in such studies and are often handled via multiple\nimputation (MI). Although several MI approaches can be used to account for the\nthree-level structure, including adaptations to single- and two-level\napproaches, when the substantive analysis model includes interactions or\nquadratic effects these too need to be accommodated in the imputation model. In\nsuch analyses, substantive model compatible (SMC) MI has shown great promise in\nthe context of single-level data. While there have been recent developments in\nmultilevel SMC MI, to date only one approach that explicitly handles incomplete\nthree-level data is available. Alternatively, researchers can use pragmatic\nadaptations to single- and two-level MI approaches, or two-level SMC-MI\napproaches. We describe the available approaches and evaluate them via\nsimulation in the context of a three three-level random effects analysis models\ninvolving an interaction between the incomplete time-varying exposure and time,\nan interaction between the time-varying exposure and an incomplete time-fixed\nconfounder, or a quadratic effect of the exposure. Results showed that all\napproaches considered performed well in terms of bias and precision when the\ntarget analysis involved an interaction with time, but the three-level SMC MI\napproach performed best when the target analysis involved an interaction\nbetween the time-varying exposure and an incomplete time-fixed confounder, or a\nquadratic effect of the exposure. We illustrate the methods using data from the\nChildhood to Adolescence Transition Study.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:23:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wijesuriya", "Rushani", ""], ["Moreno-Betancur", "Margarita", ""], ["Carlin", "John B.", ""], ["De Silva", "Anurika P.", ""], ["Lee", "Katherine J.", ""]]}, {"id": "2010.16129", "submitter": "Yuki Atsusaka", "authors": "Yuki Atsusaka and Randolph T. Stevenson", "title": "A Bias-Corrected Estimator for the Crosswise Model with Inattentive\n  Respondents", "comments": "26 pages, 5 figures (main text); 22 pages, 9 figures (Online\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crosswise model is an increasingly popular survey technique to elicit\ncandid answers from respondents on sensitive questions. Recent studies,\nhowever, point out that in the presence of inattentive respondents, the\nconventional estimator of the prevalence of a sensitive attribute is biased\ntoward 0.5. To remedy this problem, we propose a simple design-based bias\ncorrection using an anchor question that has a sensitive item with known\nprevalence. We demonstrate that we can easily estimate and correct for the bias\narising from inattentive respondents without measuring individual-level\nattentiveness. We also offer several useful extensions of our estimator,\nincluding a sensitivity analysis for the conventional estimator, a strategy for\nweighting, a framework for multivariate regressions in which a latent sensitive\ntrait is used as an outcome or a predictor, and tools for power analysis and\nparameter selection. Our method can be easily implemented through our\nopen-source software, cWise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:03:21 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:46:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Atsusaka", "Yuki", ""], ["Stevenson", "Randolph T.", ""]]}, {"id": "2010.16297", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Satyam Dwivedi, Petre Stoica", "title": "Robust Localization in Wireless Networks From Corrupted Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of timing-based localization in wireless networks,\nwhen an unknown fraction of data is corrupted by nonideal signal conditions.\nWhile timing-based techniques enable accurate localization, they are also\nsensitive to such corrupted data. We develop a robust method that is applicable\nto a range of localization techniques, including time-of-arrival,\ntime-difference-of-arrival and time-difference in schedule-based transmissions.\nThe method is nonparametric and requires only an upper bound on the fraction of\ncorrupted data, thus obviating distributional assumptions of the corrupting\nnoise distribution. The robustness of the method is demonstrated in numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:22:09 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 10:12:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Dwivedi", "Satyam", ""], ["Stoica", "Petre", ""]]}]