[{"id": "0908.0067", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Making statistical methods in management research more useful: some\n  suggestions from a case study", "comments": "27 pages, 2 figures. New version has amended title, revised abstract,\n  and the rest of the paper has been simplified", "journal-ref": "Slightly revised version published in Sage Open, vol 3, no 1, 2013", "doi": "10.1177/2158244013476873", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a critique of the methods used in a typical paper. This leads to\nthree broad conclusions about the conventional use of statistical methods.\nFirst, results are often reported in an unnecessarily obscure manner. Second,\nthe null hypothesis testing paradigm is deeply flawed: estimating the size of\neffects and citing confidence intervals or levels is usually better. Third,\nthere are several issues, independent of the particular statistical concepts\nemployed, which limit the value of any statistical approach: e.g. difficulties\nof generalizing to different contexts, and the weakness of some research in\nterms of the size of the effects found. The first two of these are easily\nremedied: I illustrate some of the possibilities by re-analyzing the data from\nthe case study article. The third means that in some contexts a statistical\napproach may not be worthwhile. My case study is a management paper, but\nsimilar problems arise in other social sciences. Keywords: Confidence,\nHypothesis testing, Null hypothesis significance tests, Philosophy of\nstatistics, Statistical methods, User-friendliness.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2009 11:09:25 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2010 13:21:46 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2012 08:53:19 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2012 12:45:00 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2012 15:47:32 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "0908.0145", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina and Fred L. Mannering", "title": "Empirical assessment of the impact of highway design exceptions on the\n  frequency and severity of vehicle accidents", "comments": "24 pages, 1 figure, 2 tables, the final version is available in\n  Accident Analysis and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compliance to standardized highway design criteria is considered essential to\nensure the roadway safety. However, for a variety of reasons, situations arise\nwhere exceptions to standard-design criteria are requested and accepted after\nreview. This research explores the impact that design exceptions have on the\naccident severity and accident frequency in Indiana. Data on accidents at\nroadway sites with and without design exceptions are used to estimate\nappropriate statistical models for the frequency and severity accidents at\nthese sites using some of the most recent statistical advances with mixing\ndistributions. The results of the modeling process show that presence of\napproved design exceptions has not had a statistically significant effect on\nthe average frequency or severity of accidents -- suggesting that current\nprocedures for granting design exceptions have been sufficiently rigorous to\navoid adverse safety impacts.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2009 18:04:24 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0908.1144", "submitter": "Melanie A. Wilson", "authors": "Melanie A. Wilson, Edwin S. Iversen, Merlise A. Clyde, Scott C.\n  Schmidler, Joellen M. Schildkraut", "title": "Bayesian model search and multilevel inference for SNP association\n  studies", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS322 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 3, 1342-1364", "doi": "10.1214/09-AOAS322", "report-no": "IMS-AOAS-AOAS322", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances in genotyping have given rise to hypothesis-based\nassociation studies of increasing scope. As a result, the scientific hypotheses\naddressed by these studies have become more complex and more difficult to\naddress using existing analytic methodologies. Obstacles to analysis include\ninference in the face of multiple comparisons, complications arising from\ncorrelations among the SNPs (single nucleotide polymorphisms), choice of their\ngenetic parametrization and missing data. In this paper we present an efficient\nBayesian model search strategy that searches over the space of genetic markers\nand their genetic parametrization. The resulting method for Multilevel\nInference of SNP Associations, MISA, allows computation of multilevel posterior\nprobabilities and Bayes factors at the global, gene and SNP level, with the\nprior distribution on SNP inclusion in the model providing an intrinsic\nmultiplicity correction. We use simulated data sets to characterize MISA's\nstatistical power, and show that MISA has higher power to detect association\nthan standard procedures. Using data from the North Carolina Ovarian Cancer\nStudy (NCOCS), MISA identifies variants that were not identified by standard\nmethods and have been externally ``validated'' in independent studies. We\nexamine sensitivity of the NCOCS results to prior choice and method for\nimputing missing data. MISA is available in an R package on CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2009 03:52:51 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2009 20:52:57 GMT"}, {"version": "v3", "created": "Fri, 12 Nov 2010 14:43:02 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Wilson", "Melanie A.", ""], ["Iversen", "Edwin S.", ""], ["Clyde", "Merlise A.", ""], ["Schmidler", "Scott C.", ""], ["Schildkraut", "Joellen M.", ""]]}, {"id": "0908.1407", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee and Vinod Sharma", "title": "Generalized Analysis of a Distributed Energy Efficient Algorithm for\n  Change Detection", "comments": "Accepted as a short paper in Proc. of the 12th ACM International\n  Symposium on Modeling, Analysis and Simulation of Wireless and Mobile Systems\n  (MSWiM), Tenerife, Canary Islands, Spain, Oct 26-30, 2009. Please contact\n  vinod@ece.iisc.ernet.in or banerje5@illinois.edu for any clarifications. Also\n  visit: http://www.ece.iisc.ernet.in/~vinod/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.PF math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An energy efficient distributed Change Detection scheme based on Page's CUSUM\nalgorithm was presented in \\cite{icassp}. In this paper we consider a\nnonparametric version of this algorithm. In the algorithm in \\cite{icassp},\neach sensor runs CUSUM and transmits only when the CUSUM is above some\nthreshold. The transmissions from the sensors are fused at the physical layer.\nThe channel is modeled as a Multiple Access Channel (MAC) corrupted with noise.\nThe fusion center performs another CUSUM to detect the change. In this paper,\nwe generalize the algorithm to also include nonparametric CUSUM and provide a\nunified analysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2009 21:21:00 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Banerjee", "Taposh", ""], ["Sharma", "Vinod", ""]]}, {"id": "0908.1602", "submitter": "Assaf Oron", "authors": "Assaf P. Oron", "title": "Is There Statistical Evidence that the Oregon Payday-Loan Rate Cap Hurts\n  Consumers?", "comments": "22 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. A recent unpublished manuscript whose conclusions were widely circulated\nin the electronic media (Zinman, 2009) claimed that Oregon 2007 payday loan\n(PL) rate-limiting regulations (hereafter, \"Cap\") have hurt borrowers.\n  2. The report's main conclusion, phrased in cause-and-effect language in the\nabstract - \"...restricting access caused deterioration in the overall financial\ncondition of the Oregon households...\" - relies on a single, small-sample\nsurvey funded by the payday-lending industry (PLI). The survey is fraught with\nmethodological flaws.\n  3. Moreover, survey results do not support the claim that Oregon borrowers\nfared worse than Washington borrowers, on any variable that can be plausibly\nattributed to the Cap.\n  4. In fact, Oregon respondents fared better than Washington respondents on\ntwo key variables: on-time bill payment rate and avoiding phone-line\ndisconnects. On all other relevant variables they fared similarly to Washington\nrespondents. In short, the reported claim is baseless.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2009 00:36:55 GMT"}], "update_date": "2009-08-13", "authors_parsed": [["Oron", "Assaf P.", ""]]}, {"id": "0908.1735", "submitter": "Robin Ryder", "authors": "Robin J. Ryder and Geoff K. Nicholls", "title": "Missing data in a stochastic Dollo model for cognate data, and its\n  application to the dating of Proto-Indo-European", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nicholls and Gray (2008) describe a phylogenetic model for trait data. They\nuse their model to estimate branching times on Indo-European language trees\nfrom lexical data. Alekseyenko et al. (2008) extended the model and give\napplications in genetics. In this paper we extend the inference to handle data\nmissing at random. When trait data are gathered, traits are thinned in a way\nthat depends on both the trait and missing-data content. Nicholls and Gray\n(2008) treat missing records as absent traits. Hittite has 12% missing trait\nrecords. Its age is poorly predicted in their cross-validation. Our prediction\nis consistent with the historical record. Nicholls and Gray (2008) dropped\nseven languages with too much missing data. We fit all twenty four languages in\nthe lexical data of Ringe (2002). In order to model spatial-temporal rate\nheterogeneity we add a catastrophe process to the model. When a language passes\nthrough a catastrophe, many traits change at the same time. We fit the full\nmodel in a Bayesian setting, via MCMC.\n  We validate our fit using Bayes factors to test known age constraints. We\nreject three of thirty historically attested constraints. Our main result is a\nunimodel posterior distribution for the age of Proto-Indo-European centered at\n8400 years BP with 95% HPD equal 7100-9800 years BP.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2009 15:41:16 GMT"}], "update_date": "2009-08-13", "authors_parsed": [["Ryder", "Robin J.", ""], ["Nicholls", "Geoff K.", ""]]}, {"id": "0908.2053", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng, Yichao Wu", "title": "Network exploration via the adaptive LASSO and SCAD penalties", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS215 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 521-541", "doi": "10.1214/08-AOAS215", "report-no": "IMS-AOAS-AOAS215", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are frequently used to explore networks, such as genetic\nnetworks, among a set of variables. This is usually carried out via exploring\nthe sparsity of the precision matrix of the variables under consideration.\nPenalized likelihood methods are often used in such explorations. Yet,\npositive-definiteness constraints of precision matrices make the optimization\nproblem challenging. We introduce nonconcave penalties and the adaptive LASSO\npenalty to attenuate the bias problem in the network estimation. Through the\nlocal linear approximation to the nonconcave penalty functions, the problem of\nprecision matrix estimation is recast as a sequence of penalized likelihood\nproblems with a weighted $L_1$ penalty and solved using the efficient algorithm\nof Friedman et al. [Biostatistics 9 (2008) 432--441]. Our estimation schemes\nare applied to two real datasets. Simulation experiments and asymptotic theory\nare used to justify our proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 12:44:53 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Wu", "Yichao", ""]]}, {"id": "0908.2062", "submitter": "Art B. Owen", "authors": "Art B. Owen, Patrick O. Perry", "title": "Bi-cross-validation of the SVD and the nonnegative matrix factorization", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS227 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 564-594", "doi": "10.1214/08-AOAS227", "report-no": "IMS-AOAS-AOAS227", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a form of bi-cross-validation (BCV) for choosing the\nrank in outer product models, especially the singular value decomposition (SVD)\nand the nonnegative matrix factorization (NMF). Instead of leaving out a set of\nrows of the data matrix, we leave out a set of rows and a set of columns, and\nthen predict the left out entries by low rank operations on the retained data.\nWe prove a self-consistency result expressing the prediction error as a\nresidual from a low rank approximation. Random matrix theory and some empirical\nresults suggest that smaller hold-out sets lead to more over-fitting, while\nlarger ones are more prone to under-fitting. In simulated examples we find that\na method leaving out half the rows and half the columns performs well.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 13:20:59 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Owen", "Art B.", ""], ["Perry", "Patrick O.", ""]]}, {"id": "0908.2066", "submitter": "Tom Britton", "authors": "Tom Britton, Theodore Kypraios, Philip O'Neill", "title": "Statistical inference for stochastic epidemic models with three levels\n  of mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic epidemic model is defined in which each individual belongs to a\nhousehold, a secondary grouping (typically school or workplace) and also the\ncommunity as a whole. Moreover, infectious contacts take place in these three\nsettings according to potentially different rates. For this model we consider\nhow different kinds of data can be used to estimate the infection rate\nparameters with a view to understanding what can and cannot be inferred, and\nwith what precision. Among other things we find that temporal data can be of\nconsiderable inferential benefit compared to final size data, that the degree\nof heterogeneity in the data can have a considerable effect on inference for\nnon-household transmission, and that inferences can be materially different\nfrom those obtained from a model with two levels of mixing.\n  Keywords: Basic reproduction number, Bayesian inference, Epidemic model,\nInfectious disease data, Markov chain Monte Carlo, Networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 13:52:33 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Britton", "Tom", ""], ["Kypraios", "Theodore", ""], ["O'Neill", "Philip", ""]]}, {"id": "0908.2069", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "Simple Error Scattering Model for improved Information Reconciliation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementations of quantum key distribution as available nowadays suffer from\ninefficiencies due to post processing of the raw key that severely cuts down\nthe final secure key rate. We present a simple model for the error scattering\nacross the raw key and derive \"closed form\" expressions for the probability of\na parity check failure, or experiencing more than some fixed number of errors.\nOur results can serve for improvement for key establishment, as information\nreconciliation via interactive error correction and privacy amplification rests\non mostly unproven assumptions. We support those hypotheses on statistical\ngrounds.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 13:56:30 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "0908.2196", "submitter": "Stephen McIntyre", "authors": "Stephen McIntyre (Climate Audit), Ross McKitrick (University of\n  Guelph)", "title": "The Consistency of Modeled and Observed Temperature Trends in the\n  Tropical Troposphere: A Comment on Santer et al (2008)", "comments": "4 pages; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Santer et al (2008) (S08) compared climate models and observations in the\ntropical troposphere and reported that \"there is no longer a serious\ndiscrepancy between modeled and observed trends in tropical lapse rates.\" They\nfound no statistically significant differences between modeled (ensemble mean)\ntrends and observed trends at the T2LT and T2 layers, and they found no\nsignificant difference between observed and modeled surface-minus-troposphere\nlapse rates. However they only used data over the 1979-1999 period. Using the\nS08 methodology on up-to-date data, we find a statistically significant\ndiscrepancy between observations and models with respect to trends in the UAH\ndata, as well as lapse rate trends comparing either RSS or UAH to the HADCRUT3v\nland-ocean surface trend.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2009 20:26:53 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["McIntyre", "Stephen", "", "Climate Audit"], ["McKitrick", "Ross", "", "University of\n  Guelph"]]}, {"id": "0908.2296", "submitter": "Peter G. M. van der Heijden", "authors": "Dankmar B\\\"ohning, Peter G. M. van der Heijden", "title": "A covariate adjustment for zero-truncated approaches to estimating the\n  size of hidden and elusive populations", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS214 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 595-610", "doi": "10.1214/08-AOAS214", "report-no": "IMS-AOAS-AOAS214", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the estimation of population size from one-source\ncapture--recapture data, that is, a list in which individuals can potentially\nbe found repeatedly and where the question is how many individuals are missed\nby the list. As a typical example, we provide data from a drug user study in\nBangkok from 2001 where the list consists of drug users who repeatedly contact\ntreatment institutions. Drug users with 1, 2, 3$,...$ contacts occur, but drug\nusers with zero contacts are not present, requiring the size of this group to\nbe estimated. Statistically, these data can be considered as stemming from a\nzero-truncated count distribution. We revisit an estimator for the population\nsize suggested by Zelterman that is known to be robust under potential\nunobserved heterogeneity. We demonstrate that the Zelterman estimator can be\nviewed as a maximum likelihood estimator for a locally truncated Poisson\nlikelihood which is equivalent to a binomial likelihood. This result allows the\nextension of the Zelterman estimator by means of logistic regression to include\nobserved heterogeneity in the form of covariates. We also review an estimator\nproposed by Chao and explain why we are not able to obtain similar results for\nthis estimator. The Zelterman estimator is applied in two case studies, the\nfirst a drug user study from Bangkok, the second an illegal immigrant study in\nthe Netherlands. Our results suggest the new estimator should be used, in\nparticular, if substantial unobserved heterogeneity is present.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 07:35:22 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["B\u00f6hning", "Dankmar", ""], ["van der Heijden", "Peter G. M.", ""]]}, {"id": "0908.2300", "submitter": "Francesco Bartolucci", "authors": "Francesco Bartolucci, Monia Lupparelli, Giorgio E. Montanari", "title": "Latent Markov model for longitudinal binary data: An application to the\n  performance evaluation of nursing homes", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS230 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 611-636", "doi": "10.1214/08-AOAS230", "report-no": "IMS-AOAS-AOAS230", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance evaluation of nursing homes is usually accomplished by the\nrepeated administration of questionnaires aimed at measuring the health status\nof the patients during their period of residence in the nursing home. We\nillustrate how a latent Markov model with covariates may effectively be used\nfor the analysis of data collected in this way. This model relies on a not\ndirectly observable Markov process, whose states represent different levels of\nthe health status. For the maximum likelihood estimation of the model we apply\nan EM algorithm implemented by means of certain recursions taken from the\nliterature on hidden Markov chains. Of particular interest is the estimation of\nthe effect of each nursing home on the probability of transition between the\nlatent states. We show how the estimates of these effects may be used to\nconstruct a set of scores which allows us to rank these facilities in terms of\ntheir efficacy in taking care of the health conditions of their patients. The\nmethod is used within an application based on data concerning a set of nursing\nhomes located in the Region of Umbria, Italy, which were followed for the\nperiod 2003--2005.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 07:56:30 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Lupparelli", "Monia", ""], ["Montanari", "Giorgio E.", ""]]}, {"id": "0908.2310", "submitter": "C\\'eline L\\'evy-Leduc", "authors": "C\\'eline L\\'evy-Leduc, Fran\\c{c}ois Roueff", "title": "Detection and localization of change-points in high-dimensional network\n  traffic data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS232 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 637-662", "doi": "10.1214/08-AOAS232", "report-no": "IMS-AOAS-AOAS232", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and efficient method, that we shall call TopRank in the\nfollowing paper, for detecting change-points in high-dimensional data. This\nissue is of growing concern to the network security community since network\nanomalies such as Denial of Service (DoS) attacks lead to changes in Internet\ntraffic. Our method consists of a data reduction stage based on record\nfiltering, followed by a nonparametric change-point detection test based on\n$U$-statistics. Using this approach, we can address massive data streams and\nperform anomaly detection and localization on the fly. We show how it applies\nto some real Internet traffic provided by France-T\\'el\\'ecom (a French Internet\nservice provider) in the framework of the ANR-RNRT OSCAR project. This approach\nis very attractive since it benefits from a low computational load and is able\nto detect and localize several types of network anomalies. We also assess the\nperformance of the TopRank algorithm using synthetic data and compare it with\nalternative approaches based on random aggregation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 08:39:16 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "0908.2313", "submitter": "D. Draper", "authors": "D. Fouskakis, I. Ntzoufras, D. Draper", "title": "Bayesian variable selection using cost-adjusted BIC, with application to\n  cost-effective measurement of quality of health care", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS207 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 663-690", "doi": "10.1214/08-AOAS207", "report-no": "IMS-AOAS-AOAS207", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of quality of health care measurement, one approach to assessing\npatient sickness at admission involves a logistic regression of mortality\nwithin 30 days of admission on a fairly large number of sickness indicators (on\nthe order of 100) to construct a sickness scale, employing classical variable\nselection methods to find an ``optimal'' subset of 10--20 indicators. Such\n``benefit-only'' methods ignore the considerable differences among the sickness\nindicators in cost of data collection, an issue that is crucial when admission\nsickness is used to drive programs (now implemented or under consideration in\nseveral countries, including the U.S. and U.K.) that attempt to identify\nsubstandard hospitals by comparing observed and expected mortality rates (given\nadmission sickness). When both data-collection cost and accuracy of prediction\nof 30-day mortality are considered, a large variable-selection problem arises\nin which costly variables that do not predict well enough should be omitted\nfrom the final scale. In this paper (a) we develop a method for solving this\nproblem based on posterior model odds, arising from a prior distribution that\n(1) accounts for the cost of each variable and (2) results in a set of\nposterior model probabilities that corresponds to a generalized cost-adjusted\nversion of the Bayesian information criterion (BIC), and (b) we compare this\nmethod with a decision-theoretic cost-benefit approach based on maximizing\nexpected utility. We use reversible-jump Markov chain Monte Carlo (RJMCMC)\nmethods to search the model space, and we check the stability of our findings\nwith two variants of the MCMC model composition ($\\mathit{MC}^3$) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 09:19:18 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["Fouskakis", "D.", ""], ["Ntzoufras", "I.", ""], ["Draper", "D.", ""]]}, {"id": "0908.2409", "submitter": "Ann Lee", "authors": "Ann B. Lee, Diana Luca, and Kathryn Roeder", "title": "A Spectral Graph Approach to Discovering Genetic Ancestry", "comments": "6 figures", "journal-ref": "Annals of Applied Statistics, 4(1), 179-201, 2010", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping human genetic variation is fundamentally interesting in fields such\nas anthropology and forensic inference. At the same time patterns of genetic\ndiversity confound efforts to determine the genetic basis of complex disease.\nDue to technological advances it is now possible to measure hundreds of\nthousands of genetic variants per individual across the genome. Principal\ncomponent analysis (PCA) is routinely used to summarize the genetic similarity\nbetween subjects. The eigenvectors are interpreted as dimensions of ancestry.\nWe build on this idea using a spectral graph approach. In the process we draw\non connections between multidimensional scaling and spectral kernel methods.\nOur approach, based on a spectral embedding derived from the normalized\nLaplacian of a graph, can produce more meaningful delineation of ancestry than\nby using PCA. The method is stable to outliers and can more easily incorporate\ndifferent similarity measures of genetic data than PCA. We illustrate a new\nalgorithm for genetic clustering and association analysis on a large,\ngenetically heterogeneous sample.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 18:30:55 GMT"}], "update_date": "2010-07-13", "authors_parsed": [["Lee", "Ann B.", ""], ["Luca", "Diana", ""], ["Roeder", "Kathryn", ""]]}, {"id": "0908.2616", "submitter": "Assaf Oron", "authors": "Assaf P. Oron, David Azriel and Peter D. Hoff", "title": "Convergence of Nonparametric Long-Memory Phase I Designs", "comments": "New version uploaded. Lemma added that proves convergence of running\n  point estimates based on martingale theory. Also simulation study added", "journal-ref": "International Journal of Biostatistics, Volume 7, Issue 1, Article\n  39, 2011", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine nonparametric dose-finding designs that use toxicity estimates\nbased on all available data at each dose allocation decision. We prove that one\nsuch design family, called here \"interval design\", converges almost surely to\nthe maximum tolerated dose (MTD), if the MTD is the only dose level whose\ntoxicity rate falls within the pre-specified interval around the desired target\nrate. Another nonparametric family, called \"point design\", has a positive\nprobability of not converging. In a numerical sensitivity study, a diverse\nsample of dose-toxicity scenarios was randomly generated. On this sample, the\n\"interval design\" convergence conditions are met far more often than the\nconditions for one-parameter design convergence (the Shen-O'Quigley\nconditions), suggesting that the interval-design conditions are less\nrestrictive. Implications of these theoretical and numerical results for\nsmall-sample behavior of the designs, and for future research, are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 19:41:07 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2010 14:06:10 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Oron", "Assaf P.", ""], ["Azriel", "David", ""], ["Hoff", "Peter D.", ""]]}, {"id": "0908.2804", "submitter": "Moritz Heene Dr.", "authors": "Peter H. Schonemann, Moritz Heene", "title": "Predictive validities: figures of merit or veils of deception?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ETS has recently released new estimates of validities of the GRE for\npredicting cumulative graduate GPA. They average in the middle thirties - twice\nas high as those previously reported by a number of independent investigators.\nIt is shown in the first part of this paper that this unexpected finding can be\ntraced to a flawed methodology that tends to inflate multiple correlation\nestimates, especially those of populations values near zero. Secondly, the\nissue of upward corrections of validity estimates for restriction of range is\ntaken up. It is shown that they depend on assumptions that are rarely met by\nthe data. Finally, it is argued more generally that conventional test theory,\nwhich is couched in terms of correlations and variances, is not only\nunnecessarily abstract but, more importantly, incomplete, since the practical\nutility of a test does not only depend on its validity, but also on base-rates\nand admission quotas. A more direct and conclusive method for gauging the\nutility of a test involves misclassification rates, and entirely dispenses with\nquestionable assumptions and post-hoc \"corrections\". On applying this approach\nto the GRE, it emerges (1) that the GRE discriminates against ethnic and\neconomic minorities, and (2) that it often produces more erroneous decisions\nthan a purely random admissions policy would.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 19:55:12 GMT"}], "update_date": "2009-08-20", "authors_parsed": [["Schonemann", "Peter H.", ""], ["Heene", "Moritz", ""]]}, {"id": "0908.2858", "submitter": "Merlise A. Clyde", "authors": "Xi Kathy Zhou, Merlise A. Clyde, James Garrett, Viridiana Lourdes,\n  Michael O'Connell, Giovanni Parmigiani, David J. Turner, Tim Wiles", "title": "Statistical methods for automated drug susceptibility testing: Bayesian\n  minimum inhibitory concentration prediction from growth curves", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS217 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 710-730", "doi": "10.1214/08-AOAS217", "report-no": "IMS-AOAS-AOAS217", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determination of the minimum inhibitory concentration (MIC) of a drug that\nprevents microbial growth is an important step for managing patients with\ninfections. In this paper we present a novel probabilistic approach that\naccurately estimates MICs based on a panel of multiple curves reflecting\nfeatures of bacterial growth. We develop a probabilistic model for determining\nwhether a given dilution of an antimicrobial agent is the MIC given features of\nthe growth curves over time. Because of the potentially large collection of\nfeatures, we utilize Bayesian model selection to narrow the collection of\npredictors to the most important variables. In addition to point estimates of\nMICs, we are able to provide posterior probabilities that each dilution is the\nMIC based on the observed growth curves. The methods are easily automated and\nhave been incorporated into the Becton--Dickinson PHOENIX automated\nsusceptibility system that rapidly and accurately classifies the resistance of\na large number of microorganisms in clinical samples. Over seventy-five studies\nto date have shown this new method provides improved estimation of MICs over\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 06:26:47 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Zhou", "Xi Kathy", ""], ["Clyde", "Merlise A.", ""], ["Garrett", "James", ""], ["Lourdes", "Viridiana", ""], ["O'Connell", "Michael", ""], ["Parmigiani", "Giovanni", ""], ["Turner", "David J.", ""], ["Wiles", "Tim", ""]]}, {"id": "0908.2862", "submitter": "Peter J. Green", "authors": "Peter J. Green, Julia Mortera", "title": "Sensitivity of inferences in forensic genetics to assumptions about\n  founding genes", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS235 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 731-763", "doi": "10.1214/09-AOAS235", "report-no": "IMS-AOAS-AOAS235", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many forensic genetics problems can be handled using structured systems of\ndiscrete variables, for which Bayesian networks offer an appealing practical\nmodeling framework, and allow inferences to be computed by probability\npropagation methods. However, when standard assumptions are violated--for\nexample, when allele frequencies are unknown, there is identity by descent or\nthe population is heterogeneous--dependence is generated among founding genes,\nthat makes exact calculation of conditional probabilities by propagation\nmethods less straightforward. Here we illustrate different methodologies for\nassessing sensitivity to assumptions about founders in forensic genetics\nproblems. These include constrained steepest descent, linear fractional\nprogramming and representing dependence by structure. We illustrate these\nmethods on several forensic genetics examples involving criminal\nidentification, simple and complex disputed paternity and DNA mixtures.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 06:58:36 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Green", "Peter J.", ""], ["Mortera", "Julia", ""]]}, {"id": "0908.2886", "submitter": "Brisa N. S\\'anchez", "authors": "Brisa N. S\\'anchez, Esben Budtz-J{\\o}rgensen, Louise M. Ryan", "title": "An estimating equations approach to fitting latent exposure models with\n  longitudinal health outcomes", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS226 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 830-856", "doi": "10.1214/08-AOAS226", "report-no": "IMS-AOAS-AOAS226", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of data arising from environmental health studies which collect\na large number of measures of exposure can benefit from using latent variable\nmodels to summarize exposure information. However, difficulties with estimation\nof model parameters may arise since existing fitting procedures for linear\nlatent variable models require correctly specified residual variance structures\nfor unbiased estimation of regression parameters quantifying the association\nbetween (latent) exposure and health outcomes. We propose an estimating\nequations approach for latent exposure models with longitudinal health outcomes\nwhich is robust to misspecification of the outcome variance. We show that\ncompared to maximum likelihood, the loss of efficiency of the proposed method\nis relatively small when the model is correctly specified. The proposed\nequations formalize the ad-hoc regression on factor scores procedure, and\ngeneralize regression calibration. We propose two weighting schemes for the\nequations, and compare their efficiency. We apply this method to a study of the\neffects of in-utero lead exposure on child development.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 09:27:52 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["S\u00e1nchez", "Brisa N.", ""], ["Budtz-J\u00f8rgensen", "Esben", ""], ["Ryan", "Louise M.", ""]]}, {"id": "0908.2901", "submitter": "Yili Hong", "authors": "Yili Hong, William Q. Meeker, James D. McCalley", "title": "Prediction of remaining life of power transformers based on left\n  truncated and right censored lifetime data", "comments": "Published in at http://dx.doi.org/10.1214/00-AOAS231 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 857-879", "doi": "10.1214/00-AOAS231", "report-no": "IMS-AOAS-AOAS231", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the remaining life of high-voltage power transformers is an\nimportant issue for energy companies because of the need for planning\nmaintenance and capital expenditures. Lifetime data for such transformers are\ncomplicated because transformer lifetimes can extend over many decades and\ntransformer designs and manufacturing practices have evolved. We were asked to\ndevelop statistically-based predictions for the lifetimes of an energy\ncompany's fleet of high-voltage transmission and distribution transformers. The\ncompany's data records begin in 1980, providing information on installation and\nfailure dates of transformers. Although the dataset contains many units that\nwere installed before 1980, there is no information about units that were\ninstalled and failed before 1980. Thus, the data are left truncated and right\ncensored. We use a parametric lifetime model to describe the lifetime\ndistribution of individual transformers. We develop a statistical procedure,\nbased on age-adjusted life distributions, for computing a prediction interval\nfor remaining life for individual transformers now in service. We then extend\nthese ideas to provide predictions and prediction intervals for the cumulative\nnumber of failures, over a range of time, for the overall fleet of\ntransformers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 10:57:25 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Hong", "Yili", ""], ["Meeker", "William Q.", ""], ["McCalley", "James D.", ""]]}, {"id": "0908.2904", "submitter": "Ryan J. Tibshirani", "authors": "Ryan J. Tibshirani, Robert Tibshirani", "title": "A bias correction for the minimum error rate in cross-validation", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS224 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 822-829", "doi": "10.1214/08-AOAS224", "report-no": "IMS-AOAS-AOAS224", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning parameters in supervised learning problems are often estimated by\ncross-validation. The minimum value of the cross-validation error can be biased\ndownward as an estimate of the test error at that same value of the tuning\nparameter. We propose a simple method for the estimation of this bias that uses\ninformation from the cross-validation process. As a result, it requires\nessentially no additional computation. We apply our bias estimate to a number\nof popular classifiers in various settings, and examine its performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 11:09:38 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Tibshirani", "Ryan J.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0908.3047", "submitter": "Sascha Vongehr", "authors": "Sascha Vongehr, Shaochun Tang, Xiangkang Meng", "title": "Collision statistics of clusters: From Poisson model to Poisson mixtures", "comments": "22 pages, 4 figures, to appear in Chin Phys B", "journal-ref": "Chin. Phys. B Vol. 19, No. 2 (2010) 023602", "doi": null, "report-no": null, "categories": "physics.chem-ph physics.atm-clus physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clusters traverse a gas and collide with gas particles. The gas particles are\nadsorbed and the clusters become hosts. If the clusters are size selected, the\nnumber of guests will be Poisson distributed. We review this by showcasing four\nlaboratory procedures that all rely on the validity of the Poisson model. The\neffects of a statistical distribution of the cluster sizes in a beam of\nclusters are discussed. We derive the average collision rates. Additionally, we\npresent Poisson mixture models that involve also standard deviations. We derive\nthe collision statistics for common size distributions of hosts and also for\nsome generalizations thereof. The models can be applied to large noble gas\nclusters traversing doping gas. While outlining how to fit a generalized\nPoisson to the statistics, we still find even these Poisson models to be often\ninsufficient.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2009 08:55:58 GMT"}], "update_date": "2010-02-07", "authors_parsed": [["Vongehr", "Sascha", ""], ["Tang", "Shaochun", ""], ["Meng", "Xiangkang", ""]]}, {"id": "0908.3250", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "G. Rochefort, F. Champagnat, G. Le Besnerais, J.-F. Giovannelli", "title": "An Improved Observation Model for Super-Resolution under Affine Motion", "comments": null, "journal-ref": "IEEE Trans. Image Processing, vol. 15, no. 11, pp. 3325-3337,\n  November, 2006", "doi": "10.1109/TIP.2006.881996", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) techniques make use of subpixel shifts between frames\nin an image sequence to yield higher-resolution images. We propose an original\nobservation model devoted to the case of non isometric inter-frame motion as\nrequired, for instance, in the context of airborne imaging sensors. First, we\ndescribe how the main observation models used in the SR literature deal with\nmotion, and we explain why they are not suited for non isometric motion. Then,\nwe propose an extension of the observation model by Elad and Feuer adapted to\naffine motion. This model is based on a decomposition of affine transforms into\nsuccessive shear transforms, each one efficiently implemented by row-by-row or\ncolumn-by-column 1-D affine transforms.\n  We demonstrate on synthetic and real sequences that our observation model\nincorporated in a SR reconstruction technique leads to better results in the\ncase of variable scale motions and it provides equivalent results in the case\nof isometric motions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 13:42:51 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Rochefort", "G.", ""], ["Champagnat", "F.", ""], ["Besnerais", "G. Le", ""], ["Giovannelli", "J. -F.", ""]]}, {"id": "0908.3258", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "J.-F. Giovannelli, J. Idier, R. Boubertakh, A. Herment", "title": "Unsupervised Frequency Tracking beyond the Nyquist Limit using Markov\n  Chains", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 50, no. 12, pp.\n  2905-2914, December, 2002", "doi": "10.1109/TSP.2002.805501", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of a sequence of frequencies from a\ncorresponding sequence of signals. This problem arises in fields such as\nDoppler imaging where its specificity is twofold. First, only short noisy data\nrecords are available (typically four sample long) and experimental constraints\nmay cause spectral aliasing so that measurements provide unreliable, ambiguous\ninformation. Second, the frequency sequence is smooth. Here, this information\nis accounted for by a Markov model and application of the Bayes rule yields the\na posteriori density. The maximum a postariori is computed by a combination of\nViterbi and descent procedures. One of the major features of the method is that\nit is entirely unsupervised. Adjusting the hyperparameters that balance\ndata-based and prior-based information is done automatically by ML using an\nEM-based gradient algorithm. We compared the proposed estimate to a reference\none and found that it performed better: variance was greatly reduced and\ntracking was correct, even beyond the Nyquist frequency.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 15:20:44 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Giovannelli", "J. -F.", ""], ["Idier", "J.", ""], ["Boubertakh", "R.", ""], ["Herment", "A.", ""]]}, {"id": "0908.3259", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "J.-F. Giovannelli, J. Idier, G. Desodt, D. Muller", "title": "Regularized adaptive long autoregressive spectral analysis", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 39, no.\n  10, pp. 2194-2202, October 2001", "doi": "10.1109/36.957282", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to adaptive long autoregressive spectral analysis when\n(i) very few data are available, (ii) information does exist beforehand\nconcerning the spectral smoothness and time continuity of the analyzed signals.\nThe contribution is founded on two papers by Kitagawa and Gersch. The first one\ndeals with spectral smoothness, in the regularization framework, while the\nsecond one is devoted to time continuity, in the Kalman formalism. The present\npaper proposes an original synthesis of the two contributions: a new\nregularized criterion is introduced that takes both information into account.\nThe criterion is efficiently optimized by a Kalman smoother. One of the major\nfeatures of the method is that it is entirely unsupervised: the problem of\nautomatically adjusting the hyperparameters that balance data-based versus\nprior-based information is solved by maximum likelihood. The improvement is\nquantified in the field of meteorological radar.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 15:45:10 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Giovannelli", "J. -F.", ""], ["Idier", "J.", ""], ["Desodt", "G.", ""], ["Muller", "D.", ""]]}, {"id": "0908.3262", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "J.-F. Giovannelli, J. Idier", "title": "Bayesian interpretation of periodograms", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 49, no. 7, pp.\n  1988-1996, July 2001", "doi": "10.1109/78.928692", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual nonparametric approach to spectral analysis is revisited within the\nregularization framework. Both usual and windowed periodograms are obtained as\nthe squared modulus of the minimizer of regularized least squares criteria.\nThen, particular attention is paid to their interpretation within the Bayesian\nstatistical framework. Finally, the question of unsupervised hyperparameter and\nwindow selection is addressed. It is shown that maximum likelihood solution is\nboth formally achievable and practically useful.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 16:18:15 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Giovannelli", "J. -F.", ""], ["Idier", "J.", ""]]}, {"id": "0908.3856", "submitter": "Simone Pigolotti", "authors": "Alberto Bernacchia, Simone Pigolotti", "title": "Self-consistent method for density estimation", "comments": "21 pages, 5 figures", "journal-ref": "Journal of the Royal Statistical Society: Series B, Volume 73(3)\n  pp 407-422, 2011", "doi": "10.1111/j.1467-9868.2011.00772.x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of a density profile from experimental data points is a\nchallenging problem, usually tackled by plotting a histogram. Prior assumptions\non the nature of the density, from its smoothness to the specification of its\nform, allow the design of more accurate estimation procedures, such as Maximum\nLikelihood. Our aim is to construct a procedure that makes no explicit\nassumptions, but still providing an accurate estimate of the density. We\nintroduce the self-consistent estimate: the power spectrum of a candidate\ndensity is given, and an estimation procedure is constructed on the assumption,\nto be released \\emph{a posteriori}, that the candidate is correct. The\nself-consistent estimate is defined as a prior candidate density that precisely\nreproduces itself. Our main result is to derive the exact expression of the\nself-consistent estimate for any given dataset, and to study its properties.\nApplications of the method require neither priors on the form of the density\nnor the subjective choice of parameters. A cutoff frequency, akin to a bin size\nor a kernel bandwidth, emerges naturally from the derivation. We apply the\nself-consistent estimate to artificial data generated from various\ndistributions and show that it reaches the theoretical limit for the scaling of\nthe square error with the dataset size.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2009 16:23:07 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2009 08:31:40 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2010 18:28:25 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2010 16:53:58 GMT"}, {"version": "v5", "created": "Thu, 29 Jul 2010 16:23:07 GMT"}, {"version": "v6", "created": "Mon, 13 Dec 2010 13:20:04 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bernacchia", "Alberto", ""], ["Pigolotti", "Simone", ""]]}, {"id": "0908.3962", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Ruediger Mutz, Hans-Dieter Daniel", "title": "New approaches for increasing the reliability of the h index research\n  performance measurement", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the year 2005 Jorge Hirsch introduced the h index for quantifying the\nresearch output of scientists. Today, the h index is a widely accepted\nindicator of research performance. The h index has been criticized for its\ninsufficient reliability - the ability to discriminate reliably between\nmeaningful amounts of research performance. Taking as an example an extensive\ndata set with bibliometric data on scientists working in the field of molecular\nbiology, we compute h2 lower, h2 upper, and sRM values and present them as\ncomplementary approaches that improve the reliability of the h index research\nperformance measurement.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 09:33:41 GMT"}], "update_date": "2009-08-28", "authors_parsed": [["Bornmann", "Lutz", ""], ["Mutz", "Ruediger", ""], ["Daniel", "Hans-Dieter", ""]]}, {"id": "0908.4310", "submitter": "Beatriz Susana Marron", "authors": "Beatriz Marron", "title": "Co-occurrence Matrix and Fractal Dimension for Image Segmentation", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important tasks in image processing problem and machine\nvision is object recognition, and the success of many proposed methods relies\non a suitable choice of algorithm for the segmentation of an image. This paper\nfocuses on how to apply texture operators based on the concept of fractal\ndimension and cooccurence matrix, to the problem of object recognition and a\nnew method based on fractal dimension is introduced. Several images, in which\nthe result of the segmentation can be shown, are used to illustrate the use of\neach method and a comparative study of each operator is made.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2009 01:48:16 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 11:53:23 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Marron", "Beatriz", ""]]}, {"id": "0908.4334", "submitter": "S\\'ilvio Duarte Queir\\'os M.", "authors": "Silvio M. Duarte Queiros", "title": "One and two side generalisations of the log-Normal distribution by means\n  of a new product definition", "comments": "25 pages, 7 figures", "journal-ref": "Physica A 391, 3594 (2012)", "doi": "10.1016/j.physa.2012.01.050", "report-no": null, "categories": "math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we introduce a generalisation of the log-Normal\ndistribution that is inspired by a modification of the Kaypten multiplicative\nprocess using the $q$-product of Borges [Physica A \\textbf{340}, 95 (2004)].\nDepending on the value of q the distribution increases the tail for small (when\n$q<1$) or large (when $q>1$) values of the variable upon analysis. The usual\nlog-Normal distribution is retrieved when $q=1$. The main statistical features\nof this distribution are presented as well as a related random number\ngenerators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we\nillustrate the application of this distribution studying the adjustment of a\nset of variables of biological and financial origin.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2009 12:08:00 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Queiros", "Silvio M. Duarte", ""]]}]