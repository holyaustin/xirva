[{"id": "1211.0130", "submitter": "Isabel Serra", "authors": "Joan del castillo, Jalila Daoudi and Isabel Serra", "title": "The full-tails gamma distribution applied to model extreme values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we show the relationship between the Pareto distribution and\nthe gamma distribution. This shows that the second one, appropriately extended,\nexplains some anomalies that arise in the practical use of extreme value\ntheory. The results are useful to certain phenomena that are fitted by the\nPareto distribution but, at the same time, they present a deviation from this\nlaw for very large values. Two examples of data analysis with the new model are\nprovided. The first one is on the influence of climate variability on the\noccurrence of tropical cyclones. The second one on the analysis of aggregate\nloss distributions associated to operational risk management.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 09:13:14 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["del castillo", "Joan", ""], ["Daoudi", "Jalila", ""], ["Serra", "Isabel", ""]]}, {"id": "1211.0312", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "The Strength of Arcs and Edges in Interaction Networks: Elements of a\n  Model-Based Approach", "comments": "23 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing interaction networks, it is common to interpret the amount of\ninteraction between two nodes as the strength of their relationship. We argue\nthat this interpretation may not be appropriate, since the interaction between\na pair of nodes could potentially be explained only by characteristics of the\nnodes that compose the pair and, however, not by pair-specific features. In\ninteraction networks, where edges or arcs are count-valued, the above scenario\ncorresponds to a model of independence for the expected interaction in the\nnetwork, and consequently we propose the notions of arc strength, and edge\nstrength to be understood as departures from this model of independence. We\ndiscuss how our notion of arc/edge strength can be used as a guidance to study\nnetwork structure, and in particular we develop a latent arc strength\nstochastic blockmodel for directed interaction networks. We illustrate our\napproach studying the interaction between the Kolkata users of the myGamma\nmobile network.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 21:29:25 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 21:37:58 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1211.0313", "submitter": "Paulo Urriza", "authors": "Paulo Urriza, Eric Rebeiz, Danijela Cabric", "title": "Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic\n  Correlation Significance Test", "comments": "26 pages, 8 figures, submitted to IEEE JSAC: Cognitive Radio Series.\n  arXiv admin note: substantial text overlap with arXiv:1210.8176", "journal-ref": null, "doi": "10.1109/JSAC.2013.131118", "report-no": null, "categories": "cs.PF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze a spectrum sensing method based on\ncyclostationarity specifically targeted for receivers with multiple antennas.\nThis detection method is used for determining the presence or absence of\nprimary users in cognitive radio networks based on the eigenvalues of the\ncyclic covariance matrix of received signals. In particular, the cyclic\ncorrelation significance test is used to detect a specific signal-of-interest\nby exploiting knowledge of its cyclic frequencies. Analytical expressions for\nthe probability of detection and probability of false-alarm under both\nspatially uncorrelated or spatially correlated noise are derived and verified\nby simulation. The detection performance in a Rayleigh flat-fading environment\nis found and verified through simulations. One of the advantages of the\nproposed method is that the detection threshold is shown to be independent of\nboth the number of samples and the noise covariance, effectively eliminating\nthe dependence on accurate noise estimation. The proposed method is also shown\nto provide higher detection probability and better robustness to noise\nuncertainty than existing multiple-antenna cyclostationary-based spectrum\nsensing algorithms under both AWGN as well as a quasi-static Rayleigh fading\nchannel.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 21:30:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Urriza", "Paulo", ""], ["Rebeiz", "Eric", ""], ["Cabric", "Danijela", ""]]}, {"id": "1211.0374", "submitter": "Robert McKilliam", "authors": "Robby G. McKilliam, Barry G. Quinn, I. Vaughan L. Clarkson, Bill Moran\n  and Badri N. Vellambi", "title": "Polynomial phase estimation by phase unwrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the coefficients of a noisy polynomial phase signal is important\nin fields including radar, biology and radio communications. One approach\nattempts to perform polynomial regression on the phase of the signal. This is\ncomplicated by the fact that the phase is wrapped modulo 2\\pi and must be\nunwrapped before regression can be performed. In this paper we consider an\nestimator that performs phase unwrapping in a least squares manner. We describe\nthe asymptotic properties of this estimator, showing that it is strongly\nconsistent and asymptotically normally distributed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 06:12:40 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["McKilliam", "Robby G.", ""], ["Quinn", "Barry G.", ""], ["Clarkson", "I. Vaughan L.", ""], ["Moran", "Bill", ""], ["Vellambi", "Badri N.", ""]]}, {"id": "1211.0381", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Loet Leydesdorff, Ruediger Mutz", "title": "The use of percentiles and percentile rank classes in the analysis of\n  bibliometric data: Opportunities and limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percentiles have been established in bibliometrics as an important\nalternative to mean-based indicators for obtaining a normalized citation impact\nof publications. Percentiles have a number of advantages over standard\nbibliometric indicators used frequently: for example, their calculation is not\nbased on the arithmetic mean which should not be used for skewed bibliometric\ndata. This study describes the opportunities and limits and the advantages and\ndisadvantages of using percentiles in bibliometrics. We also address problems\nin the calculation of percentiles and percentile rank classes for which there\nis not (yet) a satisfactory solution. It will be hard to compare the results of\ndifferent percentile-based studies with each other unless it is clear that the\nstudies were done with the same choices for percentile calculation and rank\nassignment.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 07:25:37 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""], ["Mutz", "Ruediger", ""]]}, {"id": "1211.0437", "submitter": "Jean-Patrick Baudry", "authors": "Jean-Patrick Baudry, Margarida Cardoso, Gilles Celeux, Maria Jos\\'e\n  Amorim, Ana Sousa Ferreira", "title": "Enhancing the selection of a model-based clustering with external\n  qualitative variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cluster analysis, it can be useful to interpret the partition built from\nthe data in the light of external categorical variables which were not directly\ninvolved to cluster the data. An approach is proposed in the model-based\nclustering context to select a model and a number of clusters which both fit\nthe data well and take advantage of the potential illustrative ability of the\nexternal variables. This approach makes use of the integrated joint likelihood\nof the data and the partitions at hand, namely the model-based partition and\nthe partitions associated to the external variables. It is noteworthy that each\nmixture model is fitted by the maximum likelihood methodology to the data,\nexcluding the external variables which are used to select a relevant mixture\nmodel only. Numerical experiments illustrate the promising behaviour of the\nderived criterion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 12:36:26 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 13:07:07 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Baudry", "Jean-Patrick", ""], ["Cardoso", "Margarida", ""], ["Celeux", "Gilles", ""], ["Amorim", "Maria Jos\u00e9", ""], ["Ferreira", "Ana Sousa", ""]]}, {"id": "1211.0501", "submitter": "Fintan Costello", "authors": "Fintan Costello and Paul Watts", "title": "Surprisingly Rational: Probability theory plus noise explains biases in\n  judgment", "comments": "64 pages. Final preprint version. In press, Psychological Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The systematic biases seen in people's probability judgments are typically\ntaken as evidence that people do not reason about probability using the rules\nof probability theory, but instead use heuristics which sometimes yield\nreasonable judgments and sometimes systematic biases. This view has had a major\nimpact in economics, law, medicine, and other fields; indeed, the idea that\npeople cannot reason with probabilities has become a widespread truism. We\npresent a simple alternative to this view, where people reason about\nprobability according to probability theory but are subject to random variation\nor noise in the reasoning process. In this account the effect of noise is\ncancelled for some probabilistic expressions: analysing data from two\nexperiments we find that, for these expressions, people's probability judgments\nare strikingly close to those required by probability theory. For other\nexpressions this account produces systematic deviations in probability\nestimates. These deviations explain four reliable biases in human probabilistic\nreasoning (conservatism, subadditivity, conjunction and disjunction fallacies).\nThese results suggest that people's probability judgments embody the rules of\nprobability theory, and that biases in those judgments are due to the effects\nof random noise.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 14:57:42 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2013 13:40:00 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 09:15:06 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Costello", "Fintan", ""], ["Watts", "Paul", ""]]}, {"id": "1211.0656", "submitter": "Gregory C. Levine", "authors": "G. C. Levine, B. Caravan and J. E. Cerise", "title": "Electoral Susceptibility", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the United States electoral system, a candidate is elected indirectly by\nwinning a majority of electoral votes cast by individual states, the election\nusually being decided by the votes cast by a small number of \"swing states\"\nwhere the two candidates historically have roughly equal probabilities of\nwinning. The effective value of a swing state in deciding the election is\ndetermined not only by the number of its electoral votes but by the frequency\nof its appearance in the set of winning partitions of the electoral college.\nSince the electoral vote values of swing states are not identical, the presence\nor absence of a state in a winning partition is generally correlated with the\nfrequency of appearance of other states and, hence, their effective values. We\nquantify the effective value of states by an {\\sl electoral susceptibility},\n$\\chi_j$, the variation of the winning probability with the \"cost\" of changing\nthe probability of winning state $j$. We study $\\chi_j$ for realistic data\naccumulated for the 2012 U.S. presidential election and for a simple model with\na Zipf's law type distribution of electoral votes. In the latter model we show\nthat the susceptibility for small states is largest in \"one-sided\" electoral\ncontests and smallest in close contests. We draw an analogy to models of\nentropically driven interactions in poly-disperse colloidal solutions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 00:54:59 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Levine", "G. C.", ""], ["Caravan", "B.", ""], ["Cerise", "J. E.", ""]]}, {"id": "1211.0757", "submitter": "Ju Sun", "authors": "Ju Sun, Yuqian Zhang, John Wright", "title": "Efficient Point-to-Subspace Query in $\\ell^1$: Theory and Applications\n  in Computer Vision", "comments": "To appear in NIPS workshop on big learning, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. We show in theory that Cauchy random embedding of the objects into\nsignificantly-lower-dimensional spaces helps preserve the identity of the\nnearest subspace with constant probability. This offers the possibility of\nefficiently selecting several candidates for accurate search. We sketch\npreliminary experiments on robust face and digit recognition to corroborate our\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 04:15:25 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Sun", "Ju", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""]]}, {"id": "1211.0882", "submitter": "Roland Langrock", "authors": "Roland Langrock, Ruth King", "title": "Maximum likelihood estimation of mark-recapture-recovery models in the\n  presence of continuous covariates", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS644 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1709-1732", "doi": "10.1214/13-AOAS644", "report-no": "IMS-AOAS-AOAS644", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider mark-recapture-recovery (MRR) data of animals where the model\nparameters are a function of individual time-varying continuous covariates. For\nsuch covariates, the covariate value is unobserved if the corresponding\nindividual is unobserved, in which case the survival probability cannot be\nevaluated. For continuous-valued covariates, the corresponding likelihood can\nonly be expressed in the form of an integral that is analytically intractable\nand, to date, no maximum likelihood approach that uses all the information in\nthe data has been developed. Assuming a first-order Markov process for the\ncovariate values, we accomplish this task by formulating the MRR setting in a\nstate-space framework and considering an approximate likelihood approach which\nessentially discretizes the range of covariate values, reducing the integral to\na summation. The likelihood can then be efficiently calculated and maximized\nusing standard techniques for hidden Markov models. We initially assess the\napproach using simulated data before applying to real data relating to Soay\nsheep, specifying the survival probability as a function of body mass. Models\nthat have previously been suggested for the corresponding covariate process are\ntypically of the form of diffusive random walks. We consider an alternative\nnondiffusive AR(1)-type model which appears to provide a significantly better\nfit to the Soay sheep data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 14:57:36 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 16:42:04 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2013 07:07:14 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langrock", "Roland", ""], ["King", "Ruth", ""]]}, {"id": "1211.0887", "submitter": "Roland Langrock", "authors": "Roland Langrock, Nils-Bastian Heidenreich and Stefan Sperlich", "title": "Kernel-based semiparametric multinomial logit modelling of political\n  party affiliation", "comments": null, "journal-ref": "Statistical Methods & Applications, 2014, Vol. 23, Issue 3, pages\n  435-449", "doi": "10.1007/s10260-014-0261-z", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional, parametric multinomial logit models are in general not\nsufficient for detecting the complex patterns voter profiles nowadays typically\nexhibit. In this manuscript, we use a semiparametric multinomial logit model to\ngive a detailed analysis of the composition of a subsample of the German\nelectorate in 2006. Germany is a particularly strong case for more flexible\nnonparametric approaches in this context, since due to the reunification and\nthe preceding different political histories the composition of the electorate\nis very complex and nuanced. Our analysis reveals strong interactions of the\ncovariates age and income, and highly nonlinear shapes of the factor impacts\nfor each party's likelihood to be voted. Notably, we develop and provide a\nsmoothed likelihood estimator for semiparametric multinomial logit models,\nwhich can be applied also in other application fields, such as, e.g.,\nmarketing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 15:15:11 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 12:16:39 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Langrock", "Roland", ""], ["Heidenreich", "Nils-Bastian", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1211.0938", "submitter": "Murphy Choy", "authors": "Murphy Choy, Michelle Cheong, Ma Nang Laik, Koo Ping Shung", "title": "US Presidential Election 2012 Prediction using Census Corrected Twitter\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  US Presidential Election 2012 has been a very tight race between the two key\ncandidates. There were intense battle between the two key candidates. The\nelection reflects the sentiment of the electorate towards the achievements of\nthe incumbent President Obama. The campaign lasted several months and the\neffects can be felt in the internet and twitter. The presidential debates\ninjected new vigor in the challenger's campaign and successfully captured the\nelectorate of several states posing a threat to the incumbent's position. Much\nof the sentiment in the election has been captured in the online discussions.\nIn this paper, we will be using the original model described in Choy et. al.\n(2011) using twitter data to forecast the next US president.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 17:29:46 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 17:37:29 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2012 11:41:09 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Choy", "Murphy", ""], ["Cheong", "Michelle", ""], ["Laik", "Ma Nang", ""], ["Shung", "Koo Ping", ""]]}, {"id": "1211.1154", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Loet Leydesdorff", "title": "The validation of (advanced) bibliometric indicators through peer\n  assessments: A comparative study using data from InCites and F1000", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data of F1000 provide us with the unique opportunity to investigate the\nrelationship between peers' ratings and bibliometric metrics on a broad and\ncomprehensive data set with high-quality ratings. F1000 is a post-publication\npeer review system of the biomedical literature. The comparison of metrics with\npeer evaluation has been widely acknowledged as a way of validating metrics.\nBased on the seven indicators offered by InCites, we analyzed the validity of\nraw citation counts (Times Cited, 2nd Generation Citations, and 2nd Generation\nCitations per Citing Document), normalized indicators (Journal Actual/Expected\nCitations, Category Actual/Expected Citations, and Percentile in Subject Area),\nand a journal based indicator (Journal Impact Factor). The data set consists of\n125 papers published in 2008 and belonging to the subject category cell biology\nor immunology. As the results show, Percentile in Subject Area achieves the\nhighest correlation with F1000 ratings; we can assert that for further three\nother indicators (Times Cited, 2nd Generation Citations, and Category\nActual/Expected Citations) the 'true' correlation with the ratings reaches at\nleast a medium effect size.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 09:25:31 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1211.1171", "submitter": "Antonio Punzo", "authors": "Salvatore Ingrassia and Simona C. Minotti and Antonio Punzo and\n  Giorgio Vittadini", "title": "Generalized Linear Gaussian Cluster-Weighted Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-Weighted Modeling (CWM) is a flexible mixture approach for modeling\nthe joint probability of data coming from a heterogeneous population as a\nweighted sum of the products of marginal distributions and conditional\ndistributions. In this paper, we introduce a wide family of Cluster Weighted\nmodels in which the conditional distributions are assumed to belong to the\nexponential family with canonical links which will be referred to as\nGeneralized Linear Gaussian Cluster Weighted Models. Moreover, we show that, in\na suitable sense, mixtures of generalized linear models can be considered as\nnested in Generalized Linear Gaussian Cluster Weighted Models. The proposal is\nillustrated through many numerical studies based on both simulated and real\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 10:10:22 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2012 16:17:47 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Ingrassia", "Salvatore", ""], ["Minotti", "Simona C.", ""], ["Punzo", "Antonio", ""], ["Vittadini", "Giorgio", ""]]}, {"id": "1211.1183", "submitter": "Antonio Punzo", "authors": "Angelo Mazza and Antonio Punzo and Brian McGuire", "title": "KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) models are a class of statistical models used to\ndescribe the response behaviors of individuals to a set of items having a\ncertain number of options. They are adopted by researchers in social science,\nparticularly in the analysis of performance or attitudinal data, in psychology,\neducation, medicine, marketing and other fields where the aim is to measure\nlatent constructs. Most IRT analyses use parametric models that rely on\nassumptions that often are not satisfied. In such cases, a nonparametric\napproach might be preferable; nevertheless, there are not many software\napplications allowing to use that. To address this gap, this paper presents the\nR package KernSmoothIRT. It implements kernel smoothing for the estimation of\noption characteristic curves, and adds several plotting and analytical tools to\nevaluate the whole test/questionnaire, the items, and the subjects. In order to\nshow the package's capabilities, two real datasets are used, one employing\nmultiple-choice responses, and the other scaled responses.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 11:26:30 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 09:56:38 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Mazza", "Angelo", ""], ["Punzo", "Antonio", ""], ["McGuire", "Brian", ""]]}, {"id": "1211.1184", "submitter": "Antonio Punzo", "authors": "Angelo Mazza and Antonio Punzo", "title": "DBKGrad: An R Package for Mortality Rates Graduation by Fixed and\n  Adaptive Discrete Beta Kernel Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel smoothing represents a useful approach in the graduation of mortality\nrates. Though there exist several options for performing kernel smoothing in\nstatistical software packages, there have been very few contributions to date\nthat have focused on applications of these techniques in the graduation\ncontext. Also, although it has been shown that the use of a variable or\nadaptive smoothing parameter, based on the further information provided by the\nexposed to the risk of death, provides additional benefits, specific\ncomputational tools for this approach are essentially absent. Furthermore,\nlittle attention has been given to providing methods in available software for\nany kind of subsequent analysis with respect to the graduated mortality rates.\nTo facilitate analyses in the field, the R package DBKGrad is introduced. Among\nthe available kernel approaches, it considers a recent discrete beta kernel\nestimator, in both its fixed and adaptive variants. In this approach, boundary\nbias is automatically reduced and age is pragmatically considered as a discrete\nvariable. The bandwidth, fixed or adaptive, is allowed to be manually given by\nthe user or selected by cross-validation. Pointwise confidence intervals, for\neach considered age, are also provided. An application to mortality rates from\nthe Sicily Region (Italy) for the year 2008 is also presented to exemplify the\nuse of the package.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 11:27:08 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Mazza", "Angelo", ""], ["Punzo", "Antonio", ""]]}, {"id": "1211.1323", "submitter": "Claudia Beleites", "authors": "Claudia Beleites and Ute Neugebauer and Thomas Bocklitz and Christoph\n  Krafft and J\\\"urgen Popp", "title": "Sample Size Planning for Classification Models", "comments": "The paper is published in Analytica Chimica Acta (special issue\n  \"CAC2012\"). This is a reformatted version of the accepted manuscript with few\n  typos corrected and links to the official publicaion, including the\n  supplementary material (pages 11 - 16 and supplementary-* files in the\n  source). The slides of the presentation at Clircon (2015-04-22, Exeter, UK)\n  are available as ancillary pdf file", "journal-ref": "Analytica Chimica Acta, 760 (2013) 25 - 33", "doi": "10.1016/j.aca.2012.11.007", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biospectroscopy, suitably annotated and statistically independent samples\n(e. g. patients, batches, etc.) for classifier training and testing are scarce\nand costly. Learning curves show the model performance as function of the\ntraining sample size and can help to determine the sample size needed to train\ngood classifiers. However, building a good model is actually not enough: the\nperformance must also be proven. We discuss learning curves for typical small\nsample size situations with 5 - 25 independent samples per class. Although the\nclassification models achieve acceptable performance, the learning curve can be\ncompletely masked by the random testing uncertainty due to the equally limited\ntest sample size. In consequence, we determine test sample sizes necessary to\nachieve reasonable precision in the validation and find that 75 - 100 samples\nwill usually be needed to test a good but not perfect classifier. Such a data\nset will then allow refined sample size planning on the basis of the achieved\nperformance. We also demonstrate how to calculate necessary sample sizes in\norder to show the superiority of one classifier over another: this often\nrequires hundreds of statistically independent test samples or is even\ntheoretically impossible. We demonstrate our findings with a data set of ca.\n2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and\nthree tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive\nsimulation that allows precise determination of the actual performance of the\nmodels in question.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:42:00 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 13:28:52 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 09:09:38 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Beleites", "Claudia", ""], ["Neugebauer", "Ute", ""], ["Bocklitz", "Thomas", ""], ["Krafft", "Christoph", ""], ["Popp", "J\u00fcrgen", ""]]}, {"id": "1211.1365", "submitter": "Philip Jonathan", "authors": "Kevin Ewans and Philip Jonathan", "title": "Evaluating environmental joint extremes for the offshore industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding extreme ocean environments and their interaction with fixed and\nfloating structures is critical for the design of offshore and coastal\nfacilities. The joint effect of various ocean variables on extreme responses of\noffshore structures is fundamental in determining the design loads. For\nexample, it is known that mean values of wave periods tend to increase with\nincreasing storm intensity, and a floating system responds in a complex way to\nboth variables.\n  However, specification of joint extremes in design criteria has often been\nsomewhat \\textit{ad hoc}, being based on fairly arbitrary combinations of\nextremes of variables estimated independently. Such approaches are even\noutlined in design guidelines. Mathematically more consistent estimates of the\njoint occurrence of extreme environmental variables fall into two camps in the\noffshore industry -- response-based and response-independent. Both are outlined\nhere, with emphasis on response-independent methods, particularly those based\non the conditional extremes model recently introduced by Heffernan and Tawn\n(2004) which has a solid theoretical motivation. Several applications using the\nnew methods are presented.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 20:27:43 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Ewans", "Kevin", ""], ["Jonathan", "Philip", ""]]}, {"id": "1211.1405", "submitter": "Radu Craiu Dr", "authors": "Lizhen Xu, Radu V. Craiu and Lei Sun", "title": "Bayesian Latent Variable Modeling of Longitudinal Family Data for\n  Genetic Pleiotropy Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by genetic association studies of pleiotropy, we propose here a\nBayesian latent variable approach to jointly study multiple outcomes or\nphenotypes. The proposed method models both continuous and binary phenotypes,\nand it accounts for serial and familial correlations when longitudinal and\npedigree data have been collected. We present a Bayesian estimation method for\nthe model parameters, and we develop a novel MCMC algorithm that builds upon\nhierarchical centering and parameter expansion techniques to efficiently sample\nthe posterior distribution. We discuss phenotype and model selection in the\nBayesian setting, and we study the performance of two selection strategies\nbased on Bayes factors and spike-and-slab priors. We evaluate the proposed\nmethod via extensive simulations and demonstrate its utility with an\napplication to a genome-wide association study of various complication\nphenotypes related to type 1 diabetes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 21:35:00 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Xu", "Lizhen", ""], ["Craiu", "Radu V.", ""], ["Sun", "Lei", ""]]}, {"id": "1211.1409", "submitter": "Philip Jonathan", "authors": "Bill Hirst, Philip Jonathan, Fernando Gonzalez del Cueto, David\n  Randell and Oliver Kosut", "title": "Locating and quantifying gas emission sources using remotely obtained\n  concentration data", "comments": null, "journal-ref": null, "doi": "10.1016/j.atmosenv.2013.03.044", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for detecting, locating and quantifying sources of gas\nemissions to the atmosphere using remotely obtained gas concentration data; the\nmethod is applicable to gases of environmental concern. We demonstrate its\nperformance using methane data collected from aircraft. Atmospheric point\nconcentration measurements are modelled as the sum of a spatially and\ntemporally smooth atmospheric background concentration, augmented by\nconcentrations due to local sources. We model source emission rates with a\nGaussian mixture model and use a Markov random field to represent the\natmospheric background concentration component of the measurements. A Gaussian\nplume atmospheric eddy dispersion model represents gas dispersion between\nsources and measurement locations. Initial point estimates of background\nconcentrations and source emission rates are obtained using mixed L2-L1\noptimisation over a discretised grid of potential source locations. Subsequent\nreversible jump Markov chain Monte Carlo inference provides estimated values\nand uncertainties for the number, emission rates and locations of sources\nunconstrained by a grid. Source area, atmospheric background concentrations and\nother model parameters are also estimated. We investigate the performance of\nthe approach first using a synthetic problem, then apply the method to real\ndata collected from an aircraft flying over: a 1600 km^2 area containing two\nlandfills, then a 225 km^2 area containing a gas flare stack.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 22:02:03 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 16:47:27 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2013 14:13:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Hirst", "Bill", ""], ["Jonathan", "Philip", ""], ["del Cueto", "Fernando Gonzalez", ""], ["Randell", "David", ""], ["Kosut", "Oliver", ""]]}, {"id": "1211.1654", "submitter": "Yue Wu", "authors": "Yue Wu, Sos Agaian, and Joseph P. Noonan", "title": "A New Randomness Evaluation Method with Applications to Image Shuffling\n  and Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter discusses the problem of testing the degree of randomness within\nan image, particularly for a shuffled or encrypted image. Its key contributions\nare: 1) a mathematical model of perfectly shuffled images; 2) the derivation of\nthe theoretical distribution of pixel differences; 3) a new $Z$-test based\napproach to differentiate whether or not a test image is perfectly shuffled;\nand 4) a randomized algorithm to unbiasedly evaluate the degree of randomness\nwithin a given image. Simulation results show that the proposed method is\nrobust and effective in evaluating the degree of randomness within an image,\nand may often be more suitable for image applications than commonly used\ntesting schemes designed for binary data like NIST 800-22. The developed method\nmay be also useful as a first step in determining whether or not a shuffling or\nencryption scheme is suitable for a particular cryptographic application.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 19:57:13 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Wu", "Yue", ""], ["Agaian", "Sos", ""], ["Noonan", "Joseph P.", ""]]}, {"id": "1211.1694", "submitter": "Nesreen Ahmed", "authors": "Ayman Farahat and Nesreen Ahmed and Utpal Dholakia", "title": "Does a Daily Deal Promotion Signal a Distressed Business? An Empirical\n  Investigation of Small Business Survival", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last four years, daily deals have emerged from nowhere to become a\nmulti-billion dollar industry world-wide. Daily deal sites such as Groupon and\nLivingsocial offer products and services at deep discounts to consumers via\nemail and social networks. As the industry matures, there are many questions\nregarding the impact of daily deals on the marketplace. Important questions in\nthis regard concern the reasons why businesses decide to offer daily deals and\ntheir longer-term impact on businesses. In the present paper, we investigate\nwhether the unobserved factors that make marketers run daily deals are\ncorrelated with the unobserved factors that influence the business, In\nparticular, we employ the framework of seemingly unrelated regression to model\nthe correlation between the errors in predicting whether a business uses a\ndaily deal and the errors in predicting the business' survival. Our analysis\nconsists of the survival of 985 small businesses that offered daily deals\nbetween January and July 2011 in the city of Chicago. Our results indicate that\nthere is a statistically significant correlation between the unobserved factors\nthat influence the business' decision to offer a daily deal and the unobserved\nfactors that impact its survival. Furthermore, our results indicate that the\ncorrelation coefficient is significant in certain business categories (e.g.\nrestaurants).\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 21:27:16 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Farahat", "Ayman", ""], ["Ahmed", "Nesreen", ""], ["Dholakia", "Utpal", ""]]}, {"id": "1211.1717", "submitter": "Lawrence Murray", "authors": "John Parslow, Noel Cressie, Edward P. Campbell, Emlyn Jones and\n  Lawrence Murray", "title": "Bayesian Learning and Predictability in a Stochastic Nonlinear Dynamical\n  Model", "comments": null, "journal-ref": null, "doi": "10.1890/12-0312.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference methods are applied within a Bayesian hierarchical\nmodelling framework to the problems of joint state and parameter estimation,\nand of state forecasting. We explore and demonstrate the ideas in the context\nof a simple nonlinear marine biogeochemical model. A novel approach is proposed\nto the formulation of the stochastic process model, in which ecophysiological\nproperties of plankton communities are represented by autoregressive stochastic\nprocesses. This approach captures the effects of changes in plankton\ncommunities over time, and it allows the incorporation of literature metadata\non individual species into prior distributions for process model parameters.\nThe approach is applied to a case study at Ocean Station Papa, using Particle\nMarkov chain Monte Carlo computational techniques. The results suggest that, by\ndrawing on objective prior information, it is possible to extract useful\ninformation about model state and a subset of parameters, and even to make\nuseful long-term forecasts, based on sparse and noisy observations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 22:51:10 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Parslow", "John", ""], ["Cressie", "Noel", ""], ["Campbell", "Edward P.", ""], ["Jones", "Emlyn", ""], ["Murray", "Lawrence", ""]]}, {"id": "1211.1992", "submitter": "Ephraim M. Hanks", "authors": "Ephraim M. Hanks, Mevin B. Hooten, Mat W. Alldredge", "title": "Continuous-time discrete-space models for animal movement", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS803 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 145-165", "doi": "10.1214/14-AOAS803", "report-no": "IMS-AOAS-AOAS803", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processes influencing animal movement and resource selection are complex\nand varied. Past efforts to model behavioral changes over time used Bayesian\nstatistical models with variable parameter space, such as reversible-jump\nMarkov chain Monte Carlo approaches, which are computationally demanding and\ninaccessible to many practitioners. We present a continuous-time discrete-space\n(CTDS) model of animal movement that can be fit using standard generalized\nlinear modeling (GLM) methods. This CTDS approach allows for the joint modeling\nof location-based as well as directional drivers of movement. Changing behavior\nover time is modeled using a varying-coefficient framework which maintains the\ncomputational simplicity of a GLM approach, and variable selection is\naccomplished using a group lasso penalty. We apply our approach to a study of\ntwo mountain lions (Puma concolor) in Colorado, USA.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 21:22:40 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 12:32:17 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hanks", "Ephraim M.", ""], ["Hooten", "Mevin B.", ""], ["Alldredge", "Mat W.", ""]]}, {"id": "1211.2332", "submitter": "John Rodgers  Smith", "authors": "John R. Smith, J. J. Llovera-Gonzalez, Stephen P. Smith", "title": "Speckle Patterns and 2-Dimensional Spatial Models", "comments": "6 pages, 6 figures, presented at Technolaser 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The result of 2-dimensional Gaussian lattice fit to a speckle intensity\npattern based on a linear model that includes nearest-neighbor interactions is\npresented. We also include a Monte Carlo simulation of the same spatial speckle\npattern that takes the nearest-neighbor interactions into account. These\nnearest-neighbor interactions lead to a spatial variance structure on the\nlattice. The resulting spatial pattern fluctuates in value from point to point\nin a manner characteristic of a stationary stochastic process. The value at a\nlattice point in the simulation is interpreted as an inten-sity level and the\ndifference in values in neighboring cells produces a fluctuating intensity\npattern on the lattice. Changing the size of the mesh changes the relative size\nof the speckles. Increasing the mesh size tends to average out the intensity in\nthe direction of the mean of the stationary process.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 15:22:10 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Smith", "John R.", ""], ["Llovera-Gonzalez", "J. J.", ""], ["Smith", "Stephen P.", ""]]}, {"id": "1211.2349", "submitter": "Salvatore Micciche' Dr.", "authors": "Salvatore Miccich\\`e", "title": "Gene-based and semantic structure of the Gene Ontology as a complex\n  network", "comments": "10 pages, 5 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen the advent and consolidation of ontology based tools\nfor the identification and biological interpretation of classes of genes, such\nas the Gene Ontology. The information accumulated time-by-time and included in\nthe GO is encoded in the definition of terms and in the setting up of semantic\nrelations amongst terms. This approach might be usefully complemented by a\nbottom-up approach based on the knowledge of relationships amongst genes. To\nthis end, we investigate the Gene Ontology from a complex network perspective.\nWe consider the semantic network of terms naturally associated with the\nsemantic relationships provided by the Gene Ontology consortium and a\ngene-based weighted network in which the nodes are the terms and a link between\nany two terms is set up whenever genes are annotated in both terms. One aim of\nthe present paper is to understand whether the semantic and the gene-based\nnetwork share the same structural properties or not. We then consider network\ncommunities. The identification of communities in the SVNs network can\ntherefore be the basis of a simple protocol aiming at fully exploiting the\npossible relationships amongst terms, thus improving the knowledge of the\nsemantic structure of GO. This is also important from a biomedical point of\nview, as it might reveal how genes over-expressed in a certain term also affect\nother biological functions not directly linked by the GO semantics. As a\nby-product, we present a simple methodology that allows to have a first glance\ninsight about the biological characterization of groups of GO terms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 20:57:32 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Miccich\u00e8", "Salvatore", ""]]}, {"id": "1211.2378", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE, CHD Castellucio), Marc Muselli (SPE), Christophe\n  Paoli (SPE), Marie Laure Nivet (SPE)", "title": "Hybrid methodology for hourly global radiation forecasting in\n  Mediterranean area", "comments": null, "journal-ref": null, "doi": "10.1016/j.renene.2012.10.049", "report-no": null, "categories": "cs.NE cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The renewable energies prediction and particularly global radiation\nforecasting is a challenge studied by a growing number of research teams. This\npaper proposes an original technique to model the insolation time series based\non combining Artificial Neural Network (ANN) and Auto-Regressive and Moving\nAverage (ARMA) model. While ANN by its non-linear nature is effective to\npredict cloudy days, ARMA techniques are more dedicated to sunny days without\ncloud occurrences. Thus, three hybrids models are suggested: the first proposes\nsimply to use ARMA for 6 months in spring and summer and to use an optimized\nANN for the other part of the year; the second model is equivalent to the first\nbut with a seasonal learning; the last model depends on the error occurred the\nprevious hour. These models were used to forecast the hourly global radiation\nfor five places in Mediterranean area. The forecasting performance was compared\namong several models: the 3 above mentioned models, the best ANN and ARMA for\neach location. In the best configuration, the coupling of ANN and ARMA allows\nan improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum\nin winter (0.9%) where ANN alone is the best.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 07:16:56 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Voyant", "Cyril", "", "SPE, CHD Castellucio"], ["Muselli", "Marc", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"]]}, {"id": "1211.2481", "submitter": "Natesh Pillai", "authors": "Tirthankar Dasgupta, Natesh S. Pillai, Donald B. Rubin", "title": "Causal inference from $2^k$ factorial designs using the potential\n  outcomes model", "comments": "Preliminary version; comments welcome. Added figures and a table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for causal inference from two-level factorial designs is\nproposed. The framework utilizes the concept of potential outcomes that lies at\nthe center stage of causal inference and extends Neyman's repeated sampling\napproach for estimation of causal effects and randomization tests based on\nFisher's sharp null hypothesis to the case of 2-level factorial experiments.\nThe framework allows for statistical inference from a finite population,\npermits definition and estimation of estimands other than \"average factorial\neffects\" and leads to more flexible inference procedures than those based on\nordinary least squares estimation from a linear model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 23:49:09 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2012 18:13:03 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Dasgupta", "Tirthankar", ""], ["Pillai", "Natesh S.", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1211.2679", "submitter": "Dan  Shen", "authors": "Dan Shen, Haipeng Shen, Hongtu Zhu, and J. S. Marron", "title": "High Dimensional Principal Component Scores and Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis is a useful dimension reduction and data\nvisualization method. However, in high dimension, low sample size asymptotic\ncontexts, where the sample size is fixed and the dimension goes to infinity,a\nparadox has arisen. In particular, despite the useful real data insights\ncommonly obtained from principal component score visualization, these scores\nare not consistent even when the sample eigen-vectors are consistent. This\nparadox is resolved by asymptotic study of the ratio between the sample and\npopulation principal component scores. In particular, it is seen that this\nproportion converges to a non-degenerate random variable. The realization is\nthe same for each data point, i.e. there is a common random rescaling, which\nappears for each eigen-direction. This then gives inconsistent axis labels for\nthe standard scores plot, yet the relative positions of the points (typically\nthe main visual content) are consistent. This paradox disappears when the\nsample size goes to infinity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 16:15:21 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2012 17:56:22 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2012 03:03:04 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Shen", "Dan", ""], ["Shen", "Haipeng", ""], ["Zhu", "Hongtu", ""], ["Marron", "J. S.", ""]]}, {"id": "1211.2945", "submitter": "Irina  Biktasheva", "authors": "Cen Wan, Irina V. Biktasheva, Steven Lane", "title": "The application of a perceptron model to classify an individual's\n  response to a proposed loading dose regimen of Warfarin", "comments": "12 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dose regimen of Warfarin is separated into two phases. Firstly a loading\ndose is given, which is designed to bring the International Normalisation Ratio\n(INR) to within therapeutic range. Then a stable maintenance dose is given to\nmaintain the INR within therapeutic range. In the United Kingdom (UK) the\nloading dose is usually given as three individual daily doses, the standard\nloading dose being 10mg on days one and two and 5mgs on day three, which can be\nvaried at the discretion of the clinician. However, due to the large\ninter-individual variation in the response to Warfarin therapy, it is difficult\nto identify which patients will reach the narrow therapeutic window for target\nINR, and which will be above or below the therapeutic window. The aim of this\nresearch was to develop a methodology using a neural networks classification\nalgorithm and data mining techniques to predict for a given loading dose and\npatient characteristics if the patient is more likely to achieve target INR or\nmore likely to be above or below therapeutic range.\n  Multilayer perceptron (MLP) and 10-fold stratified cross validation\nalgorithms were used to determine an artificial neural network to classify\npatients' response to their initial Warfarin loading dose. The resulting neural\nnetwork model correctly classifies an individual's response to their Warfarin\nloading dose over 80% of the time. As well as taking into account the initial\nloading dose, the final model also includes demographic, genetic and a number\nof other potential confounding factors. With this model clinicians can\npredetermine whether a given loading regimen, along with specific patient\ncharacteristics will achieve a therapeutic response for a particular patient.\nThus tailoring the loading dose regimen to meet the individual needs of the\npatient and reducing the risk of adverse drug reactions associated with\nWarfarin.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 10:32:22 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Wan", "Cen", ""], ["Biktasheva", "Irina V.", ""], ["Lane", "Steven", ""]]}, {"id": "1211.2958", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Study design in causal models", "comments": "The example on the MORGAM Project extended is in this version", "journal-ref": "Scandinavian Journal of Statistics, Volume 42, Issue 2, pages\n  361-377, 2015", "doi": "10.1111/sjos.12110", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal assumptions, the study design and the data are the elements\nrequired for scientific inference in empirical research. The research is\nadequately communicated only if all of these elements and their relations are\ndescribed precisely. Causal models with design describe the study design and\nthe missing data mechanism together with the causal structure and allow the\ndirect application of causal calculus in the estimation of the causal effects.\nThe flow of the study is visualized by ordering the nodes of the causal diagram\nin two dimensions by their causal order and the time of the observation.\nConclusions whether a causal or observational relationship can be estimated\nfrom the collected incomplete data can be made directly from the graph. Causal\nmodels with design offer a systematic and unifying view scientific inference\nand increase the clarity and speed of communication. Examples on the causal\nmodels for a case-control study, a nested case-control study, a clinical trial\nand a two-stage case-cohort study are presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 11:33:47 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 13:32:40 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 11:26:41 GMT"}, {"version": "v4", "created": "Thu, 24 Apr 2014 05:13:52 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1211.2961", "submitter": "Jonathan Stroud", "authors": "Jonathan R. Stroud, Michael S. Johannes", "title": "Bayesian modeling and forecasting of 24-hour high-frequency volatility:\n  A case study of the financial crisis", "comments": "48 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper estimates models of high frequency index futures returns using\n`around the clock' 5-minute returns that incorporate the following key\nfeatures: multiple persistent stochastic volatility factors, jumps in prices\nand volatilities, seasonal components capturing time of the day patterns,\ncorrelations between return and volatility shocks, and announcement effects. We\ndevelop an integrated MCMC approach to estimate interday and intraday\nparameters and states using high-frequency data without resorting to various\naggregation measures like realized volatility. We provide a case study using\nfinancial crisis data from 2007 to 2009, and use particle filters to construct\nlikelihood functions for model comparison and out-of-sample forecasting from\n2009 to 2012. We show that our approach improves realized volatility forecasts\nby up to 50% over existing benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 11:51:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 15:17:39 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Stroud", "Jonathan R.", ""], ["Johannes", "Michael S.", ""]]}, {"id": "1211.3010", "submitter": "Sriharsha Veeramachaneni", "authors": "Sriharsha Veeramachaneni", "title": "Time-series Scenario Forecasting", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require the ability to judge uncertainty of time-series\nforecasts. Uncertainty is often specified as point-wise error bars around a\nmean or median forecast. Due to temporal dependencies, such a method obscures\nsome information. We would ideally have a way to query the posterior\nprobability of the entire time-series given the predictive variables, or at a\nminimum, be able to draw samples from this distribution. We use a Bayesian\ndictionary learning algorithm to statistically generate an ensemble of\nforecasts. We show that the algorithm performs as well as a physics-based\nensemble method for temperature forecasts for Houston. We conclude that the\nmethod shows promise for scenario forecasting where physics-based methods are\nabsent.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 14:54:47 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1211.3087", "submitter": "Massimiliano Ignaccolo Dr.", "authors": "Massimiliano Ignaccolo and Marco Marani", "title": "Metastatistics of Extreme Values and its Application in Hydrology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel statistical treatment, the \"metastatistics of extreme\nevents\", for calculating the frequency of extreme events. This approach, which\nis of general validity, is the proper statistical framework to address the\nproblem of data with statistical inhomogeneities. By use of artificial\nsequences, we show that the metastatistics produce the correct predictions\nwhile the traditional approach based on the generalized extreme value\ndistribution does not. An application of the metastatistics methodology to the\ncase of extreme event to rainfall daily precipitation is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 19:40:30 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Ignaccolo", "Massimiliano", ""], ["Marani", "Marco", ""]]}, {"id": "1211.3210", "submitter": "Alice Cleynen", "authors": "Alice Cleynen and The Minh Luong and Guillem Rigaill and Gregory Nuel", "title": "Fast estimation of the ICL criterion for change-point detection problems\n  with applications to Next-Generation Sequencing data", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Integrated Completed Likelihood (ICL) as a\nuseful criterion for estimating the number of changes in the underlying\ndistribution of data in problems where detecting the precise location of these\nchanges is the main goal. The exact computation of the ICL requires O(Kn2)\noperations (with K the number of segments and n the number of data-points)\nwhich is prohibitive in many practical situations with large sequences of data.\nWe describe a framework to estimate the ICL with O(Kn) complexity. Our approach\nis general in the sense that it can accommodate any given model distribution.\nWe checked the run-time and validity of our approach on simulated data and\ndemonstrate its good performance when analyzing real Next-Generation Sequencing\n(NGS) data using a negative binomial model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 05:50:46 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2013 09:43:31 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Cleynen", "Alice", ""], ["Luong", "The Minh", ""], ["Rigaill", "Guillem", ""], ["Nuel", "Gregory", ""]]}, {"id": "1211.3211", "submitter": "Kensuke Sekihara", "authors": "Kensuke Sekihara, Hagai Attias, Julia P. Owen, Srikantan S. Nagarajan", "title": "Effectiveness of sparse Bayesian algorithm for MVAR coefficient\n  estimation in MEG/EEG source-space causality analysis", "comments": "Proceedings of the 8th Annual Conference of Non-invasive Functional\n  Source Imaging held at Banff, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the effectiveness of a sparse Bayesian algorithm to\nestimate multivariate autoregressive coefficients when a large amount of\nbackground interference exists. This paper employs computer experiments to\ncompare two methods in the source-space causality analysis: the conventional\nleast-squares method and a sparse Bayesian method. Results of our computer\nexperiments show that the interference affects the least-squares method in a\nvery severe manner. It produces large false-positive results, unless the\nsignal-to-interference ratio is very high. On the other hand, the sparse\nBayesian method is relatively insensitive to the existence of interference.\nHowever, this robustness of the sparse Bayesian method is attained on the\nscarifies of the detectability of true causal relationship. Our experiments\nalso show that the surrogate data bootstrapping method tends to give a\nstatistical threshold that are too low for the sparse method.\n  The permutation-test-based method gives a higher (more conservative)\nthreshold and it should be used with the sparse Bayesian method whenever the\ncontrol period is available.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 06:36:15 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Sekihara", "Kensuke", ""], ["Attias", "Hagai", ""], ["Owen", "Julia P.", ""], ["Nagarajan", "Srikantan S.", ""]]}, {"id": "1211.3603", "submitter": "Francois Orieux", "authors": "F. Orieux, J.-F. Giovannelli, T. Rodet and A. Abergel", "title": "Estimating hyperparameters and instrument parameters in regularized\n  inversion. Illustration for SPIRE/Herschel map making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NA stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe regularized methods for image reconstruction and focus on the\nquestion of hyperparameter and instrument parameter estimation, i.e.\nunsupervised and myopic problems. We developed a Bayesian framework that is\nbased on the \\post density for all unknown quantities, given the observations.\nThis density is explored by a Markov Chain Monte-Carlo sampling technique based\non a Gibbs loop and including a Metropolis-Hastings step. The numerical\nevaluation relies on the SPIRE instrument of the Herschel observatory. Using\nsimulated and real observations, we show that the hyperparameters and\ninstrument parameters are correctly estimated, which opens up many perspectives\nfor imaging in astrophysics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 13:37:04 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Orieux", "F.", ""], ["Giovannelli", "J. -F.", ""], ["Rodet", "T.", ""], ["Abergel", "A.", ""]]}, {"id": "1211.3706", "submitter": "Daniel Runcie Daniel Runcie", "authors": "Daniel E Runcie, Sayan Mukherjee", "title": "Bayesian Sparse Factor Analysis of Genetic Covariance Matrices", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": "10.1534/genetics.113.151217", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative genetic studies that model complex, multivariate phenotypes are\nimportant for both evolutionary prediction and artificial selection. For\nexample, changes in gene expression can provide insight into developmental and\nphysiological mechanisms that link genotype and phenotype. However, classical\nanalytical techniques are poorly suited to quantitative genetic studies of gene\nexpression where the number of traits assayed per individual can reach many\nthousand. Here, we derive a Bayesian genetic sparse factor model for estimating\nthe genetic covariance matrix (G-matrix) of high-dimensional traits, such as\ngene expression, in a mixed effects model. The key idea of our model is that we\nneed only consider G-matrices that are biologically plausible. An organism's\nentire phenotype is the result of processes that are modular and have limited\ncomplexity. This implies that the G-matrix will be highly structured. In\nparticular, we assume that a limited number of intermediate traits (or factors,\ne.g., variations in development or physiology) control the variation in the\nhigh-dimensional phenotype, and that each of these intermediate traits is\nsparse -- affecting only a few observed traits. The advantages of this approach\nare two-fold. First, sparse factors are interpretable and provide biological\ninsight into mechanisms underlying the genetic architecture. Second, enforcing\nsparsity helps prevent sampling errors from swamping out the true signal in\nhigh-dimensional data. We demonstrate the advantages of our model on simulated\ndata and in an analysis of a published Drosophila melanogaster gene expression\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 19:41:59 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2013 23:30:13 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Runcie", "Daniel E", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1211.3720", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Xiaodong Wang", "title": "Sequential Decentralized Parameter Estimation under Randomly Observed\n  Fisher Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decentralized estimation using wireless sensor\nnetworks. Specifically, we propose a novel framework based on level-triggered\nsampling, a non-uniform sampling strategy, and sequential estimation. The\nproposed estimator can be used as an asymptotically optimal fixed-sample-size\ndecentralized estimator under non-fading listening channels (through which\nsensors collect their observations), as an alternative to the one-shot\nestimators commonly found in the literature. It can also be used as an\nasymptotically optimal sequential decentralized estimator under fading\nlistening channels. We show that the optimal centralized estimator under\nGaussian noise is characterized by two processes, namely the observed Fisher\ninformation U_t, and the observed correlation V_t. It is noted that under\nnon-fading listening channels only V_t is random, whereas under fading\nlistening channels both U_t and V_t are random. In the proposed scheme, each\nsensor computes its local random process(es), and sends a single bit to the\nfusion center (FC) whenever the local random process(es) pass(es) certain\npredefined levels. The FC, upon receiving a bit from a sensor, updates its\napproximation to the corresponding global random process, and accordingly its\nestimate. The sequential estimation process terminates when the observed Fisher\ninformation (or the approximation to it) reaches a target value. We provide an\nasymptotic analysis for the proposed estimator and also the one based on\nconventional uniform-in-time sampling under both non-fading and fading\nchannels; and determine the conditions under which they are asymptotically\noptimal, consistent, and asymptotically unbiased. Analytical results, together\nwith simulation results, demonstrate the superiority of the proposed estimator\nbased on level-triggered sampling over the traditional decentralized estimator\nbased on uniform sampling.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 20:21:39 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2013 17:56:02 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1211.3792", "submitter": "Stefanka Chukova S", "authors": "Sima Varnosafaderani and Stefanka Chukova", "title": "Modeling Repairs of Systems with a Bathtub-Shaped Failure Rate Function", "comments": "9 pages, 46th Annual ORSNZ Conference, Dec, 2012 Victoria University\n  of Wellington, Wellington, New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the reliability literature on modeling the effect of repairs on\nsystems assumes the failure rate functions are monotonically increasing. For\nsystems with non-monotonic failure rate functions, most models deal with\nminimal repairs (which do not affect the working condition of the system) or\nreplacements (which return the working condition to that of a new and identical\nsystem). We explore a new approach to model repairs of a system with a\nnon-monotonic failure rate function; in particular, we consider systems with a\nbathtub-shaped failure rate function. We propose a repair model specified in\nterms of modifications to the virtual age function of the system, while\npreserving the usual definitions of the types of repair (minimal, imperfect and\nperfect repairs) and distinguishing between perfect repair and replacement. In\naddition, we provide a numerical illustration of the proposed repair model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 03:47:40 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Varnosafaderani", "Sima", ""], ["Chukova", "Stefanka", ""]]}, {"id": "1211.3967", "submitter": "Joseph Dureau", "authors": "Antoine Basset, Joseph Dureau, S\\'ebastien Ballesteros, Bernard\n  Cazelles", "title": "Quick overview of inference methods in PLoM: combining fast and exact\n  plug-and-play algorithms to achieve flexibility, precision and efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overview of inference methods in PLoM.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 17:50:28 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Basset", "Antoine", ""], ["Dureau", "Joseph", ""], ["Ballesteros", "S\u00e9bastien", ""], ["Cazelles", "Bernard", ""]]}, {"id": "1211.4259", "submitter": "Laurent Jacob", "authors": "Laurent Jacob and Johann Gagnon-Bartsch and Terence P. Speed", "title": "Correcting gene expression data when neither the unwanted variation nor\n  the factor of interest are observed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with large scale gene expression studies, observations are\ncommonly contaminated by unwanted variation factors such as platforms or\nbatches. Not taking this unwanted variation into account when analyzing the\ndata can lead to spurious associations and to missing important signals. When\nthe analysis is unsupervised, e.g., when the goal is to cluster the samples or\nto build a corrected version of the dataset - as opposed to the study of an\nobserved factor of interest - taking unwanted variation into account can become\na difficult task. The unwanted variation factors may be correlated with the\nunobserved factor of interest, so that correcting for the former can remove the\nlatter if not done carefully. We show how negative control genes and replicate\nsamples can be used to estimate unwanted variation in gene expression, and\ndiscuss how this information can be used to correct the expression data or\nbuild estimators for unsupervised problems. The proposed methods are then\nevaluated on three gene expression datasets. They generally manage to remove\nunwanted variation without losing the signal of interest and compare favorably\nto state of the art corrections.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 21:30:46 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Jacob", "Laurent", ""], ["Gagnon-Bartsch", "Johann", ""], ["Speed", "Terence P.", ""]]}, {"id": "1211.4282", "submitter": "Konrad Menzel", "authors": "Victor Chernozhukov, Emre Kocatulum, Konrad Menzel", "title": "Inference on Sets in Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of inference on a class of sets\ndescribing a collection of admissible models as solutions to a single smooth\ninequality. Classical and recent examples include, among others, the\nHansen-Jagannathan (HJ) sets of admissible stochastic discount factors,\nMarkowitz-Fama (MF) sets of mean-variances for asset portfolio returns, and the\nset of structural elasticities in Chetty (2012)'s analysis of demand with\noptimization frictions. We show that the econometric structure of the problem\nallows us to construct convenient and powerful confidence regions based upon\nthe weighted likelihood ratio and weighted Wald (directed weighted Hausdorff)\nstatistics. The statistics we formulate differ (in part) from existing\nstatistics in that they enforce either exact or first order equivariance to\ntransformations of parameters, making them especially appealing in the target\napplications. Moreover, the resulting inference procedures are also more\npowerful than the structured projection methods, which rely upon building\nconfidence sets for the frontier-determining sufficient parameters (e.g.\nfrontier-spanning portfolios), and then projecting them to obtain confidence\nsets for HJ sets or MF sets. Lastly, the framework we put forward is also\nuseful for analyzing intersection bounds, namely sets defined as solutions to\nmultiple smooth inequalities, since multiple inequalities can be conservatively\napproximated by a single smooth inequality. We present two empirical examples\nthat show how the new econometric methods are able to generate sharp economic\nconclusions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 01:39:51 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Kocatulum", "Emre", ""], ["Menzel", "Konrad", ""]]}, {"id": "1211.4763", "submitter": "Madan Kundu", "authors": "Madan G. Kundu, Jaroslaw Harezlak and Timothy W. Randolph", "title": "Longitudinal Functional Models with Structured Penalties", "comments": "23 pages, 5 figures", "journal-ref": "Statistical Modelling, 2016", "doi": "10.1177/1471082X15626291", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses estimation in a longitudinal regression model for\nassociation between a scalar outcome and a set of longitudinally-collected\nfunctional covariates or predictor curves. The framework consists of estimating\na time-varying coefficient function that is modeled as a linear combination of\ntime-invariant functions but having time-varying coefficients. The estimation\nprocedure exploits the equivalence between penalized least squares estimation\nand a linear mixed model representation. The process is empirically evaluated\nwith several simulations and it is applied to analyze the neurocognitive\nimpairment of HIV patients and its association with longitudinally-collected\nmagnetic resonance spectroscopy curves.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 14:59:29 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 04:23:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan G.", ""], ["Harezlak", "Jaroslaw", ""], ["Randolph", "Timothy W.", ""]]}, {"id": "1211.5167", "submitter": "Jessika Trancik", "authors": "Luis M. A. Bettencourt, Jessika E. Trancik, Jasleen Kaur", "title": "Determinants of the Pace of Global Innovation in Energy Technologies", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0067864", "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the factors driving innovation in energy technologies is of\ncritical importance to mitigating climate change and addressing other\nenergy-related global challenges. Low levels of innovation, measured in terms\nof energy patent filings, were noted in the 1980s and 90s as an issue of\nconcern and were attributed to low investment in public and private research\nand development (R&D). Here we build a comprehensive global database of energy\npatents covering the period 1970-2009 which is unique in its temporal and\ngeographical scope. Analysis of the data reveals a recent, marked departure\nfrom historical trends. A sharp increase in rates of patenting has occurred\nover the last decade, particularly in renewable technologies, despite continued\nlow levels of R&D funding. To solve the puzzle of fast innovation despite\nmodest R&D increases we develop a model that explains the nonlinear response\nobserved in the empirical data of technological innovation to various types of\ninvestment. The model reveals a regular relationship between patents, R&D\nfunding, and growing markets across technologies, and accurately predicts\npatenting rates at different stages of technological maturity and market\ndevelopment. We show quantitatively how growing markets have formed a vital\ncomplement to public R&D in driving innovative activity; these two forms of\ninvestment have each leveraged the effect of the other in driving patenting\ntrends over long periods of time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 23:06:49 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Bettencourt", "Luis M. A.", ""], ["Trancik", "Jessika E.", ""], ["Kaur", "Jasleen", ""]]}, {"id": "1211.5472", "submitter": "Joseph Dureau", "authors": "Joseph Dureau, Konstantinos Kalogeropoulos, Peter Vickerman, Michael\n  Pickles, Marie-Claude Boily", "title": "A Bayesian approach to estimate changes in condom use from limited HIV\n  prevalence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of HIV large scale interventions programme is becoming\nincreasingly important, but impact estimates frequently hinge on knowledge of\nchanges in behaviour such as the frequency of condom use (CU) over time, or\nother self-reported behaviour changes, for which we generally have limited or\npotentially biased data. We employ a Bayesian inference methodology that\nincorporates a dynamic HIV transmission dynamics model to estimate CU time\ntrends from HIV prevalence data. Estimation is implemented via particle Markov\nChain Monte Carlo methods, applied for the first time in this context. The\npreliminary choice of the formulation for the time varying parameter reflecting\nthe proportion of CU is critical in the context studied, due to the very\nlimited amount of CU and HIV data available We consider various novel\nformulations to explore the trajectory of CU in time, based on diffusion-driven\ntrajectories and smooth sigmoid curves. Extensive series of numerical\nsimulations indicate that informative results can be obtained regarding the\namplitude of the increase in CU during an intervention, with good levels of\nsensitivity and specificity performance in effectively detecting changes. The\napplication of this method to a real life problem illustrates how it can help\nevaluate HIV intervention from few observational studies and suggests that\nthese methods can potentially be applied in many different contexts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 11:22:11 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 07:01:12 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Dureau", "Joseph", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Vickerman", "Peter", ""], ["Pickles", "Michael", ""], ["Boily", "Marie-Claude", ""]]}, {"id": "1211.5562", "submitter": "Jithin K. Sreedharan", "authors": "Jithin K. Sreedharan and Vinod Sharma", "title": "Spectrum Sensing using Distributed Sequential Detection via Noisy\n  Reporting MAC", "comments": "13 pages. 12 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers cooperative spectrum sensing algorithms for Cognitive\nRadios which focus on reducing the number of samples to make a reliable\ndetection. We develop an energy efficient detector with low detection delay\nusing decentralized sequential hypothesis testing. Our algorithm at the\nCognitive Radios employs an asynchronous transmission scheme which takes into\naccount the noise at the fusion center. We start with a distributed algorithm,\nDualSPRT, in which Cognitive Radios sequentially collect the observations, make\nlocal decisions using SPRT (Sequential Probability Ratio Test) and send them to\nthe fusion center. The fusion center sequentially processes these received\nlocal decisions corrupted by noise, using an SPRT-like procedure to arrive at a\nfinal decision. We theoretically analyse its probability of error and average\ndetection delay. We also asymptotically study its performance. Even though\nDualSPRT performs asymptotically well, a modification at the fusion node\nprovides more control over the design of the algorithm parameters which then\nperforms better at the usual operating probabilities of error in Cognitive\nRadio systems. We also analyse the modified algorithm theoretically. Later we\nmodify these algorithms to handle uncertainties in SNR and fading.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 17:43:59 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 08:21:16 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Sreedharan", "Jithin K.", ""], ["Sharma", "Vinod", ""]]}, {"id": "1211.5805", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer, Daniel Foreman-Mackey, David W. Hogg", "title": "Probabilistic Catalogs for Crowded Stellar Fields", "comments": "Accepted for publication in The Astronomical Journal", "journal-ref": null, "doi": "10.1088/0004-6256/146/1/7", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and implement a probabilistic (Bayesian) method for producing\ncatalogs from images of stellar fields. The method is capable of inferring the\nnumber of sources N in the image and can also handle the challenges introduced\nby noise, overlapping sources, and an unknown point spread function (PSF). The\nluminosity function of the stars can also be inferred even when the precise\nluminosity of each star is uncertain, via the use of a hierarchical Bayesian\nmodel. The computational feasibility of the method is demonstrated on two\nsimulated images with different numbers of stars. We find that our method\nsuccessfully recovers the input parameter values along with principled\nuncertainties even when the field is crowded. We also compare our results with\nthose obtained from the SExtractor software. While the two approaches largely\nagree about the fluxes of the bright stars, the Bayesian approach provides more\naccurate inferences about the faint stars and the number of stars, particularly\nin the crowded case.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 20:52:59 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 21:02:52 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Foreman-Mackey", "Daniel", ""], ["Hogg", "David W.", ""]]}, {"id": "1211.5908", "submitter": "Sascha Kurz", "authors": "Sascha Kurz, Nicola Maaser, and Stefan Napel", "title": "On the Egalitarian Weights of Nations", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voters from m disjoint constituencies (regions, federal states, etc.) are\nrepresented in an assembly which contains one delegate from each constituency\nand applies a weighted voting rule. All agents are assumed to have\nsingle-peaked preferences over an interval; each delegate's preferences match\nhis constituency's median voter; and the collective decision equals the\nassembly's Condorcet winner. We characterize the asymptotic behavior of the\nprobability of a given delegate determining the outcome (i.e., being the\nweighted median of medians) in order to address a contentious practical\nquestion: which voting weights w_1, ..., w_m ought to be selected if\nconstituency sizes differ and all voters are to have a priori equal influence\non collective decisions? It is shown that if ideal point distributions have\nidentical median M and are suitably continuous, the probability for a given\ndelegate i's ideal point \\lambda_i being the Condorcet winner becomes\nasymptotically proportional to i's voting weight w_i times \\lambda_i's density\nat M as $m\\to \\infty$. Indirect representation of citizens is approximately\negalitarian for weights proportional to the square root of constituency sizes\nif all individual ideal points are i.i.d. In contrast, weights that are linear\nin-- or, better, induce a Shapley value linear in-- size are egalitarian when\npreferences are sufficiently strongly affiliated within constituencies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 10:52:36 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Kurz", "Sascha", ""], ["Maaser", "Nicola", ""], ["Napel", "Stefan", ""]]}, {"id": "1211.6143", "submitter": "Qiyu Jin", "authors": "Qiyu Jin, Ion Grama, Quansheng Liu", "title": "Convergence Theorems for the Non-Local Means Filter", "comments": "11 pages, 1figures, 3tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish convergence theorems for the Non-Local Means\nFilter in removing the additive Gaussian noise. We employ the techniques of\n\"Oracle\" estimation to determine the order of the widths of the similarity\npatches and search windows in the aforementioned filter. We propose a practical\nchoice of these parameters which improve the restoration quality of the filter\ncompared with the usual choice of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 21:48:41 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Jin", "Qiyu", ""], ["Grama", "Ion", ""], ["Liu", "Quansheng", ""]]}, {"id": "1211.6198", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Zengyou He and Weichuan Yu", "title": "Running PeptideProphet Separately on Replicates Improves Peptide\n  Identification Results", "comments": "Due to an error", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited spectrum coverage is a problem in shotgun proteomics. Replicates are\ngenerated to improve the spectrum coverage. When integrating peptide\nidentification results obtained from replicates, the state-of-the-art algorithm\nPeptideProphet combines Peptide-Spectrum Matches (PSMs) before building the\nstatistical model to calculate peptide probabilities.\n  In this paper, we find the connection between merging results of replicates\nand Bagging, which is a standard routine to improve the power of statistical\nmethods. Following Bagging's philosophy, we propose to run PeptideProphet\nseparately on each replicate and combine the outputs to obtain the final\npeptide probabilities. In our experiments, we show that the proposed routine\ncan improve PeptideProphet consistently on a standard protein dataset, a Human\ndataset and a Yeast dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 02:42:15 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 01:49:07 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2012 06:14:12 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Yang", "Chao", ""], ["He", "Zengyou", ""], ["Yu", "Weichuan", ""]]}, {"id": "1211.6631", "submitter": "Onur Ozdemir", "authors": "Onur Ozdemir, Pramod K. Varshney, Wei Su, and Andrew L. Drozd", "title": "Asymptotic Properties of Likelihood Based Linear Modulation\n  Classification Systems", "comments": "12 pages double-column, 6 figures, submitted to IEEE Transactions on\n  Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of linear modulation classification using likelihood based\nmethods is considered. Asymptotic properties of most commonly used classifiers\nin the literature are derived. These classifiers are based on hybrid likelihood\nratio test (HLRT) and average likelihood ratio test (ALRT), respectively. Both\na single-sensor setting and a multi-sensor setting that uses a distributed\ndecision fusion approach are analyzed. For a modulation classification system\nusing a single sensor, it is shown that HLRT achieves asymptotically vanishing\nprobability of error (Pe) whereas the same result cannot be proven for ALRT. In\na multi-sensor setting using soft decision fusion, conditions are derived under\nwhich Pe vanishes asymptotically. Furthermore, the asymptotic analysis of the\nfusion rule that assumes independent sensor decisions is carried out.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 15:22:29 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Ozdemir", "Onur", ""], ["Varshney", "Pramod K.", ""], ["Su", "Wei", ""], ["Drozd", "Andrew L.", ""]]}, {"id": "1211.6674", "submitter": "Alexandre Renaux", "authors": "Dinh Thang Vu, Alexandre Renaux, Remy Boyer, Sylvie Marcos", "title": "Some results on the Weiss-Weinstein bound for conditional and\n  unconditional signal models in array processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Weiss-Weinstein bound is analyzed in the context of\nsources localization with a planar array of sensors. Both conditional and\nunconditional source signal models are studied. First, some results are given\nin the multiple sources context without specifying the structure of the\nsteering matrix and of the noise covariance matrix. Moreover, the case of an\nuniform or Gaussian prior are analyzed. Second, these results are applied to\nthe particular case of a single source for two kinds of array geometries: a\nnon-uniform linear array (elevation only) and an arbitrary planar (azimuth and\nelevation) array.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 17:36:46 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Vu", "Dinh Thang", ""], ["Renaux", "Alexandre", ""], ["Boyer", "Remy", ""], ["Marcos", "Sylvie", ""]]}, {"id": "1211.6834", "submitter": "Ting Huang", "authors": "Zengyou He, Ting Huang, Peijun Zhu", "title": "On unbiased performance evaluation for protein inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter is a response to the comments of Serang (2012) on Huang and He\n(2012) in Bioinformatics. Serang (2012) claimed that the parameters for the\nFido algorithm should be specified using the grid search method in Serang et\nal. (2010) so as to generate a deserved accuracy in performance comparison. It\nseems that it is an argument on parameter tuning. However, it is indeed the\nissue of how to conduct an unbiased performance evaluation for comparing\ndifferent protein inference algorithms. In this letter, we would explain why we\ndon't use the grid search for parameter selection in Huang and He (2012) and\nshow that this procedure may result in an over-estimated performance that is\nunfair to competing algorithms. In fact, this issue has also been pointed out\nby Li and Radivojac (2012).\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 07:54:45 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["He", "Zengyou", ""], ["Huang", "Ting", ""], ["Zhu", "Peijun", ""]]}, {"id": "1211.6915", "submitter": "Ishapathik Das", "authors": "I. Das and S. Mukhopadhyay", "title": "On generalized multinomial models and joint percentile estimation", "comments": "28 pages, 4 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a family of link functions for the multinomial response\nmodel. The link family includes the multicategorical logistic link as one of\nits members. Conditions for the local orthogonality of the link and the\nregression parameters are given. It is shown that local orthogonality of the\nparameters in a neighbourhood makes the link family location and scale\ninvariant. Confidence regions for jointly estimating the percentiles based on\nthe parametric family of link functions are also determined. A numerical\nexample based on a combination drug study is used to illustrate the proposed\nparametric link family and the confidence regions for joint percentile\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 13:56:36 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Das", "I.", ""], ["Mukhopadhyay", "S.", ""]]}]