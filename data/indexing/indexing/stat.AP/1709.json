[{"id": "1709.00071", "submitter": "Nick Obradovich", "authors": "Patrick Baylis, Nick Obradovich, Yury Kryvasheyeu, Haohui Chen,\n  Lorenzo Coviello, Esteban Moro, Manuel Cebrian, James H. Fowler", "title": "Weather impacts expressed sentiment", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0195750", "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct the largest ever investigation into the relationship between\nmeteorological conditions and the sentiment of human expressions. To do this,\nwe employ over three and a half billion social media posts from tens of\nmillions of individuals from both Facebook and Twitter between 2009 and 2016.\nWe find that cold temperatures, hot temperatures, precipitation, narrower daily\ntemperature ranges, humidity, and cloud cover are all associated with worsened\nexpressions of sentiment, even when excluding weather-related posts. We compare\nthe magnitude of our estimates with the effect sizes associated with notable\nhistorical events occurring within our data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 20:36:07 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Baylis", "Patrick", ""], ["Obradovich", "Nick", ""], ["Kryvasheyeu", "Yury", ""], ["Chen", "Haohui", ""], ["Coviello", "Lorenzo", ""], ["Moro", "Esteban", ""], ["Cebrian", "Manuel", ""], ["Fowler", "James H.", ""]]}, {"id": "1709.00151", "submitter": "Yu Wang", "authors": "Yu Wang, Nhu D. Le, James V. Zidek", "title": "Approximately Optimal Subset Selection for Statistical Design and\n  Modelling", "comments": "14 pages, 3 figures, 1 table; Added examples in statistical design", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal subset selection from a set of correlated\nrandom variables. In particular, we consider the associated combinatorial\noptimization problem of maximizing the determinant of a symmetric positive\ndefinite matrix that characterizes the chosen subset. This problem arises in\nmany domains, such as experimental designs, regression modeling, and\nenvironmental statistics. We establish an efficient polynomial-time algorithm\nusing Determinantal Point Process for approximating the optimal solution to the\nproblem. We demonstrate the advantages of our methods by presenting\ncomputational results for both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:34:37 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 22:53:00 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 22:43:44 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Yu", ""], ["Le", "Nhu D.", ""], ["Zidek", "James V.", ""]]}, {"id": "1709.00181", "submitter": "Asad Zaman", "authors": "Asad Zaman, Peter J. Rousseeuw, Mehmet Orhan", "title": "Econometric applications of high-breakdown robust regression techniques", "comments": "8 pages, adds paragraph omitted from final print version in journal", "journal-ref": "Economics Letters Volume 71, Issue 1, April 2001, Pages 1-8", "doi": "10.1016/S0165-1765(00)00404-3", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A literature search shows that robust regression techniques are rarely used\nin applied econometrics. We list several misconceptions about robustness which\nlead to this situation. We show that most data sets are not normal, least\nsquares performs very poorly even in large data sets with small numbers of\noutliers, and that commonly used techniques for achieving robustness fail to do\nso. We then provide newly developed techniques from the statistics literature\nwhich are easy to understand, and achieve robustness. We show the practical use\nof these techniques by re-analyzing three regression models from recent\nliterature, and arriving at different conclusions from those reached by the\nauthors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 07:25:34 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zaman", "Asad", ""], ["Rousseeuw", "Peter J.", ""], ["Orhan", "Mehmet", ""]]}, {"id": "1709.00197", "submitter": "Khai X. Chiong", "authors": "Khai Chiong, Sha Yang, Richard Chen", "title": "Understanding the Effect of Incentivized Advertising along the\n  Conversion Funnel", "comments": "Section 5 of this draft contains an error that renders the estimation\n  results incorrect. This draft is withdrawn because a fix is not readily\n  available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to combat ad annoyance in mobile apps, publishers have\nintroduced a new ad format called \"Incentivized Advertising\" or \"Rewarded\nAdvertising\", whereby users receive rewards in exchange for watching ads. There\nis much debate in the industry regarding its' effectiveness. On the one hand,\nincentivized advertising is less intrusive and annoying, but on the other hand,\nusers might be more interested in the rewards rather than the ad content. Using\na large dataset of 1 million impressions from a mobile advertising platform,\nand in three separate quasi-experimental approaches, we find that incentivized\nadvertising leads to lower users' click-through rates, but a higher overall\ninstall rate of the advertised app.\n  In the second part, we study the mechanism of how incentivized advertising\naffects users' behavior. We test the hypothesis that incentivized advertising\ncauses a temptation effect, whereby users prefer to collect and enjoy their\nrewards immediately, instead of pursuing the ads. We find the temptation effect\nis stronger when (i) users have to wait longer before receiving the rewards and\nwhen (ii) the value of the reward is relatively larger. We further find support\nthat incentivized advertising has a positive effect of reducing ad annoyance --\nan effect that is stronger for small-screen mobile devices, where advertising\nis more annoying. Finally, we take the publisher's perspective and quantify the\noverall effect on ad revenue. Our difference-in-differences estimates suggest\nswitching to incentivized advertising would increase the publisher's revenue by\n$3.10 per 1,000 impressions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 08:42:52 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 23:47:50 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 00:34:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chiong", "Khai", ""], ["Yang", "Sha", ""], ["Chen", "Richard", ""]]}, {"id": "1709.00310", "submitter": "Murat Uney Dr", "authors": "Kimin Kim, Murat Uney, Bernard Mulgrew", "title": "Detection via simultaneous trajectory estimation and long time\n  integration", "comments": "submitted to the IEEE Transactions on Aerospace and Electronic\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we consider the detection of manoeuvring small objects with\nradars. Such objects induce low signal to noise ratio (SNR) reflections in the\nreceived signal. We consider both co-located and separated transmitter/receiver\npairs, i.e., mono-static and bi-static configurations, respectively, as well as\nmulti-static settings involving both types. We propose a detection approach\nwhich is capable of coherently integrating these reflections within a coherent\nprocessing interval (CPI) in all these configurations and continuing\nintegration for an arbitrarily long time across consecutive CPIs. We estimate\nthe complex value of the reflection coefficients for integration while\nsimultaneously estimating the object trajectory. Compounded with this is the\nestimation of the unknown time reference shift of the separated transmitters\nnecessary for coherent processing. Detection is made by using the resulting\nintegration value in a Neyman-Pearson test against a constant false alarm rate\nthreshold. We demonstrate the efficacy of our approach in a simulation example\nwith a very low SNR object which cannot be detected with conventional\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 13:40:53 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 13:35:08 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 15:36:22 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kim", "Kimin", ""], ["Uney", "Murat", ""], ["Mulgrew", "Bernard", ""]]}, {"id": "1709.00350", "submitter": "Ricardo Barros Lourenco", "authors": "Ricardo Barros Louren\\c{c}o", "title": "Assessing verticalization effects on urban safety perception", "comments": "2017 SIGSPATIAL Student Research Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe an experiment with the modeling of urban verticalization effects\non perceived safety scores as obtained with computer vision on Google\nStreetview data for New York City. Preliminary results suggests that for\nsmaller buildings (between one and seven floors), perceived safety increases\nwith building height, but that for high-rise buildings, perceived safety\ndecreases with increased height. We also determined that while height\ncontributing for this relation, other zonal aspects also influences the\nperceived safety scores, suggesting spatial structuring also influences such\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:49:12 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Louren\u00e7o", "Ricardo Barros", ""]]}, {"id": "1709.00399", "submitter": "Adway Mitra", "authors": "Adway Mitra", "title": "Bayesian approach to Spatio-temporally Consistent Simulation of Daily\n  Monsoon Rainfall over India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of rainfall over a region for long time-sequences can be very\nuseful for planning and policy-making, especially in India where the economy is\nheavily reliant on monsoon rainfall. However, such simulations should be able\nto preserve the known spatial and temporal characteristics of rainfall over\nIndia. General Circulation Models (GCMs) are unable to do so, and various\nrainfall generators designed by hydrologists using stochastic processes like\nGaussian Processes are also difficult to apply over the vast and highly diverse\nlandscape of India. In this paper, we explore a series of Bayesian models based\non conditional distributions of latent variables that describe weather\nconditions at specific locations and over the whole country. During parameter\nestimation from observed data, we use spatio-temporal smoothing using Markov\nRandom Field so that the parameters learnt are spatially and temporally\ncoherent. Also, we use a nonparametric spatial clustering based on Chinese\nRestaurant Process to identify homogeneous regions, which are utilized by some\nof the proposed models to improve spatial correlations of the simulated\nrainfall. The models are able to simulate daily rainfall across India for\nyears, and can also utilize contextual information for conditional simulation.\nWe use two datasets of different spatial resolutions over India, and focus on\nthe period 2000-2015. We propose a large number of metrics to study the\nspatio-temporal properties of the simulations by the models, and compare them\nwith the observed data to evaluate the strengths and weaknesses of the models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 17:40:21 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Mitra", "Adway", ""]]}, {"id": "1709.00566", "submitter": "Ting Li", "authors": "Ting Li, Bingyi Jing, Ningchen Ying, Xianshi Yu", "title": "Adaptive Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preprocessing data is an important step before any data analysis. In this\npaper, we focus on one particular aspect, namely scaling or normalization. We\nanalyze various scaling methods in common use and study their effects on\ndifferent statistical learning models. We will propose a new two-stage scaling\nmethod. First, we use some training data to fit linear regression model and\nthen scale the whole data based on the coefficients of regression. Simulations\nare conducted to illustrate the advantages of our new scaling method. Some real\ndata analysis will also be given.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 11:47:01 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Ting", ""], ["Jing", "Bingyi", ""], ["Ying", "Ningchen", ""], ["Yu", "Xianshi", ""]]}, {"id": "1709.00576", "submitter": "Saeid Rezakhah", "authors": "Akram Kohansal, Saeid Rezakhah", "title": "Inference of $R=P(Y<X)$ for two-parameter Rayleigh distribution based on\n  progressively censored samples", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on independent progressively Type-II censored samples from\ntwo-parameter Rayleigh distributions with the same location parameter but\ndifferent scale parameters, the UMVUE and maximum likelihood estimator of\n$R=P(Y<X)$ are obtained. Also the exact, asymptotic and bootstrap confidence\nintervals for $R$ are evaluated. Using Gibbs {sampling,} the Bayes estimator\nand corresponding credible interval for $R$ are obtained too. Applying Monte\nCarlo {simulations,} we compare the performances of the different estimation\nmethods. Finally we make use of simulated data and two real data sets to show\nthe competitive performance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 13:22:33 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kohansal", "Akram", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1709.00623", "submitter": "John Aston", "authors": "D. Pigoli, J.A.D. Aston, F. Ferraty, A. Mazumder, C. Richards and\n  M.J.R. Hall", "title": "Estimation of temperature-dependent growth profiles for the assessment\n  of time of hatching in forensic entomology", "comments": "33 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic entomology contributes important information to crime scene\ninvestigations. In this paper, we propose a method to estimate the hatching\ntime of larvae (or maggots) based on their lengths, the temperature profile at\nthe crime scene and experimental data on larval development. This requires the\nestimation of a time-dependent growth curve from experiments where larvae have\nbeen exposed to a relatively small number of constant temperature profiles.\nSince the temperature influences the developmental speed, a crucial step is the\ntime alignment of the curves at different temperatures. We propose a model for\ntime varying temperature profiles based on the local growth rate estimated from\nthe experimental data. This allows us to estimate the most likely hatching time\nfor a sample of larvae from the crime scene. Asymptotic properties are provided\nfor the estimators of the growth curves and the hatching time. We explore via\nsimulations the robustness of the method to errors in the estimated temperature\nprofile. We also apply the methodology to data from two criminal cases from the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 20:07:45 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Pigoli", "D.", ""], ["Aston", "J. A. D.", ""], ["Ferraty", "F.", ""], ["Mazumder", "A.", ""], ["Richards", "C.", ""], ["Hall", "M. J. R.", ""]]}, {"id": "1709.00743", "submitter": "Mohsen Kamrani", "authors": "Mohsen Kamrani, Behram Wali and Asad J. Khattak", "title": "Can Data Generated by Connected Vehicles Enhance Safety? A proactive\n  approach to intersection safety management", "comments": null, "journal-ref": "Transportation Research Record: Journal of the Transportation\n  Research Board (2017), 2659-09", "doi": "10.3141/2659-09", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, evaluation of intersection safety has been largely reactive,\nbased on historical crash frequency data. However, the emerging data from\nConnected and Automated Vehicles (CAVs) can complement historical data and help\nin proactively identify intersections which have high levels of variability in\ninstantaneous driving behaviors prior to the occurrence of crashes. Based on\ndata from Safety Pilot Model Deployment in Ann Arbor, Michigan, this study\ndeveloped a unique database that integrates intersection crash and inventory\ndata with more than 65 million real-world Basic Safety Messages logged by 3,000\nconnected vehicles, providing a more complete picture of operations and safety\nperformance of intersections. As a proactive safety measure and a leading\nindicator of safety, this study introduces location-based volatility (LBV),\nwhich quantifies variability in instantaneous driving decisions at\nintersections. LBV represents the driving performance of connected vehicle\ndrivers traveling through a specific intersection. As such, by using\ncoefficient of variation as a standardized measure of relative dispersion, LBVs\nare calculated for 116 intersections in Ann Arbor. To quantify relationships\nbetween intersection-specific volatilities and crash frequencies, rigorous\nfixed- and random-parameter Poisson regression models are estimated. While\ncontrolling for exposure related factors, the results provide evidence of\nstatistically significant (5% level) positive association between\nintersection-specific volatility and crash frequencies for signalized\nintersections. The implications of the findings for proactive intersection\nsafety management are discussed in the paper.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 16:48:54 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kamrani", "Mohsen", ""], ["Wali", "Behram", ""], ["Khattak", "Asad J.", ""]]}, {"id": "1709.01143", "submitter": "Chrisovaladis Malesios", "authors": "C. Malesios, N. Demiris, Z. Abas, K. Dadousis and T. Koutroumanidis", "title": "Modeling Sheep pox Disease from the 1994-1998 Epidemic in Evros\n  Prefecture, Greece", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sheep pox is a highly transmissible disease which can cause serious loss of\nlivestock and can therefore have major economic impact. We present data from\nsheep pox epidemics which occurred between 1994 and 1998. The data include\nweekly records of infected farms as well as a number of covariates. We\nimplement Bayesian stochastic regression models which, in addition to various\nexplanatory variables like seasonal and environmental/meteorological factors,\nalso contain serial correlation structure based on variants of the\nOrnstein-Uhlenbeck process. We take a predictive view in model selection by\nutilizing deviance-based measures. The results indicate that seasonality and\nthe number of infected farms are important predictors for sheep pox incidence.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 15:02:50 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Malesios", "C.", ""], ["Demiris", "N.", ""], ["Abas", "Z.", ""], ["Dadousis", "K.", ""], ["Koutroumanidis", "T.", ""]]}, {"id": "1709.01282", "submitter": "Gloria Soatti", "authors": "Gloria Soatti, Monica Nicoli, Nil Garcia, Benoit Denis, Ronald Raulefs\n  and Henk Wymeersch", "title": "Implicit Cooperative Positioning in Vehicular Networks", "comments": "15 pages, 10 figures, in review, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Absolute positioning of vehicles is based on Global Navigation Satellite\nSystems (GNSS) combined with on-board sensors and high-resolution maps. In\nCooperative Intelligent Transportation Systems (C-ITS), the positioning\nperformance can be augmented by means of vehicular networks that enable\nvehicles to share location-related information. This paper presents an Implicit\nCooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle\n(V2V) connectivity in an innovative manner, avoiding the use of explicit V2V\nmeasurements such as ranging. In the ICP approach, vehicles jointly localize\nnon-cooperative physical features (such as people, traffic lights or inactive\ncars) in the surrounding areas, and use them as common noisy reference points\nto refine their location estimates. Information on sensed features are fused\nthrough V2V links by a consensus procedure, nested within a message passing\nalgorithm, to enhance the vehicle localization accuracy. As positioning does\nnot rely on explicit ranging information between vehicles, the proposed ICP\nmethod is amenable to implementation with off-the-shelf vehicular communication\nhardware. The localization algorithm is validated in different traffic\nscenarios, including a crossroad area with heterogeneous conditions in terms of\nfeature density and V2V connectivity, as well as a real urban area by using\nSimulation of Urban MObility (SUMO) for traffic data generation. Performance\nresults show that the proposed ICP method can significantly improve the vehicle\nlocation accuracy compared to the stand-alone GNSS, especially in harsh\nenvironments, such as in urban canyons, where the GNSS signal is highly\ndegraded or denied.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 08:32:46 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Soatti", "Gloria", ""], ["Nicoli", "Monica", ""], ["Garcia", "Nil", ""], ["Denis", "Benoit", ""], ["Raulefs", "Ronald", ""], ["Wymeersch", "Henk", ""]]}, {"id": "1709.01413", "submitter": "Bradley Saul", "authors": "Bradley C. Saul, Michael G. Hudgens", "title": "The Calculus of M-estimation in R with geex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M-estimation, or estimating equation, methods are widely applicable for point\nestimation and asymptotic inference. In this paper, we present an R package\nthat can find roots and compute the empirical sandwich variance estimator for\nany set of user-specified, unbiased estimating equations. Examples from the\nM-estimation primer by Stefanski and Boos (2002) demonstrate use of the\nsoftware. The package also includes a framework for finite sample variance\ncorrections and a website with an extensive collection of tutorials.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 14:27:49 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 19:03:39 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Saul", "Bradley C.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "1709.01449", "submitter": "Jonah Gabry", "authors": "Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, Andrew\n  Gelman", "title": "Visualization in Bayesian workflow", "comments": "17 pages, 11 Figures. Includes supplementary material", "journal-ref": "J. R. Stat. Soc. A (2019) 182: 389-402", "doi": "10.1111/rssa.12378", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian data analysis is about more than just computing a posterior\ndistribution, and Bayesian visualization is about more than trace plots of\nMarkov chains. Practical Bayesian data analysis, like all data analysis, is an\niterative process of model building, inference, model checking and evaluation,\nand model expansion. Visualization is helpful in each of these stages of the\nBayesian workflow and it is indispensable when drawing inferences from the\ntypes of modern, high-dimensional models that are used by applied researchers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:26:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 19:12:48 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 18:46:49 GMT"}, {"version": "v4", "created": "Thu, 22 Feb 2018 18:10:26 GMT"}, {"version": "v5", "created": "Sat, 9 Jun 2018 00:37:21 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Gabry", "Jonah", ""], ["Simpson", "Daniel", ""], ["Vehtari", "Aki", ""], ["Betancourt", "Michael", ""], ["Gelman", "Andrew", ""]]}, {"id": "1709.01596", "submitter": "Jonathan C. Mattingly", "authors": "Gregory Herschlag, Robert Ravier, Jonathan C. Mattingly", "title": "Evaluating Partisan Gerrymandering in Wisconsin", "comments": "Slightly updated version of initially released report dated September\n  2, 2017. Typos were corrected and some wording improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the extent of gerrymandering for the 2010 General Assembly\ndistrict map of Wisconsin. We find that there is substantial variability in the\nelection outcome depending on what maps are used. We also found robust evidence\nthat the district maps are highly gerrymandered and that this gerrymandering\nlikely altered the partisan make up of the Wisconsin General Assembly in some\nelections. Compared to the distribution of possible redistricting plans for the\nGeneral Assembly, Wisconsin's chosen plan is an outlier in that it yields\nresults that are highly skewed to the Republicans when the statewide proportion\nof Democratic votes comprises more than 50-52% of the overall vote (with the\nprecise threshold depending on the election considered). Wisconsin's plan acts\nto preserve the Republican majority by providing extra Republican seats even\nwhen the Democratic vote increases into the range when the balance of power\nwould shift for the vast majority of redistricting plans.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:06:25 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Herschlag", "Gregory", ""], ["Ravier", "Robert", ""], ["Mattingly", "Jonathan C.", ""]]}, {"id": "1709.01720", "submitter": "Eitam Sheetrit", "authors": "Eitam Sheetrit, Nir Nissim, Denis Klimov, Lior Fuchs, Yuval Elovici,\n  Yuval Shahar", "title": "Temporal Pattern Discovery for Accurate Sepsis Diagnosis in ICU Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a condition caused by the body's overwhelming and life-threatening\nresponse to infection, which can lead to tissue damage, organ failure, and\nfinally death. Common signs and symptoms include fever, increased heart rate,\nincreased breathing rate, and confusion. Sepsis is difficult to predict,\ndiagnose, and treat. Patients who develop sepsis have an increased risk of\ncomplications and death and face higher health care costs and longer\nhospitalization. Today, sepsis is one of the leading causes of mortality among\npopulations in intensive care units (ICUs). In this paper, we look at the\nproblem of early detection of sepsis by using temporal data mining. We focus on\nthe use of knowledge-based temporal abstraction to create meaningful\ninterval-based abstractions, and on time-interval mining to discover frequent\ninterval-based patterns. We used 2,560 cases derived from the MIMIC-III\ndatabase. We found that the distribution of the temporal patterns whose\nfrequency is above 10% discovered in the records of septic patients during the\nlast 6 and 12 hours before onset of sepsis is significantly different from that\ndistribution within a similar period, during an equivalent time window during\nhospitalization, in the records of non-septic patients. This discovery is\nencouraging for the purpose of performing an early diagnosis of sepsis using\nthe discovered patterns as constructed features.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:42:25 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Sheetrit", "Eitam", ""], ["Nissim", "Nir", ""], ["Klimov", "Denis", ""], ["Fuchs", "Lior", ""], ["Elovici", "Yuval", ""], ["Shahar", "Yuval", ""]]}, {"id": "1709.01753", "submitter": "Steven Ellis", "authors": "Steven P. Ellis", "title": "Concurrence Topology of Some Cancer Genomics Data", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topological data analysis method \"concurrence topology\" is applied to\nmutation frequencies in 69 genes in glioblastoma data. In dimension 1 some\napparent \"mutual exclusivity\" is found. By simulation of data having\napproximately the same second order dependence structure as that found in the\ndata, it appears that one triple of mutations, PTEN, RB1, TP53, exhibits mutual\nexclusivity that depends on special features of the third order dependence and\nmay reflect global dependence among a larger group of genes. A bootstrap\nanalysis suggests that this form of mutual exclusivity is not uncommon in the\npopulation from which the data were drawn.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 10:03:03 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ellis", "Steven P.", ""]]}, {"id": "1709.02046", "submitter": "Dhagash Mehta", "authors": "John W R Morgan, Dhagash Mehta, David J Wales", "title": "Properties of Kinetic Transition Networks for Atomic Clusters and Glassy\n  Solids", "comments": "23 pages, 19 figures. Accepted for publication in Physical Chemistry\n  Chemical Physics", "journal-ref": null, "doi": "10.1039/C7CP03346J", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database of minima and transition states corresponds to a network where the\nminima represent nodes and the transition states correspond to edges between\nthe pairs of minima they connect via steepest-descent paths. Here we construct\nnetworks for small clusters bound by the Morse potential for a selection of\nphysically relevant parameters, in two and three dimensions. The properties of\nthese unweighted and undirected networks are analysed to examine two features:\nwhether they are small-world, where the shortest path between nodes involves\nonly a small number or edges; and whether they are scale-free, having a degree\ndistribution that follows a power law. Small-world character is present, but\nstatistical tests show that a power law is not a good fit, so the networks are\nnot scale-free. These results for clusters are compared with the corresponding\nproperties for the molecular and atomic structural glass formers\northo-terphenyl and binary Lennard-Jones. These glassy systems do not show\nsmall-world properties, suggesting that such behaviour is linked to the\nstructure-seeking landscapes of the Morse clusters.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 02:06:53 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Morgan", "John W R", ""], ["Mehta", "Dhagash", ""], ["Wales", "David J", ""]]}, {"id": "1709.02166", "submitter": "Dominik Liebl", "authors": "Dominik Liebl, Stefan Rameseder, and Christoph Rust", "title": "Improving Estimation in Functional Linear Regression with Points of\n  Impact: Insights into Google AdWords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional linear regression model with points of impact is a recent\naugmentation of the classical functional linear model with many practically\nimportant applications. In this work, however, we demonstrate that the existing\ndata-driven procedure for estimating the parameters of this regression model\ncan be very instable and inaccurate. The tendency to omit relevant points of\nimpact is a particularly problematic aspect resulting in omitted-variable\nbiases. We explain the theoretical reason for this problem and propose a new\nsequential estimation algorithm that leads to significantly improved estimation\nresults. Our estimation algorithm is compared with the existing estimation\nprocedure using an in-depth simulation study. The applicability is demonstrated\nusing data from Google AdWords, today's most important platform for online\nadvertisements. The \\textsf{R}-package \\texttt{FunRegPoI} and additional\n\\textsf{R}-codes are provided in the online supplementary material.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 10:10:44 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 08:14:09 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 23:22:38 GMT"}, {"version": "v4", "created": "Sat, 11 Jan 2020 22:20:24 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Liebl", "Dominik", ""], ["Rameseder", "Stefan", ""], ["Rust", "Christoph", ""]]}, {"id": "1709.02191", "submitter": "George J Vathakkattil", "authors": "George Vathakkattil Joseph, Guangbo Hao, Vikram Pakrashi", "title": "Extreme Value Estimates using Vibration Energy Harvesting", "comments": null, "journal-ref": null, "doi": "10.1016/j.jsv.2018.08.045", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the possibility of utilising energy harvesting from\nmechanical vibrations to estimate extreme value responses of the host structure\nand demonstrates the calibration of these estimates for excitation spectra\ntypical to natural systems. For illustrative purposes, a cantilever type energy\nharvester is considered for wind excitation. The extreme value estimates are\nestablished through a Generalised Pareto Distribution (GPD). Classically\nwell-known Kaimal and Davenport spectra for wind have been considered in this\npaper for comparison purposes. The work also demonstrates how return levels can\nbe mapped using energy harvesting levels and indicates that vibration energy\nharvesting, in its own right has the potential to be used for extreme value\nanalysis and estimates. The work has impact on health monitoring and assessment\nof built infrastructure in various stages of repair or disrepair and exposed to\nnature throughout their lifetime.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 11:49:05 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Joseph", "George Vathakkattil", ""], ["Hao", "Guangbo", ""], ["Pakrashi", "Vikram", ""]]}, {"id": "1709.02223", "submitter": "Konstantinos Spiliopoulos", "authors": "Siragan Gailus and Konstantinos Spiliopoulos", "title": "Discrete-Time Statistical Inference for Multiscale Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical inference for small-noise-perturbed multiscale dynamical\nsystems under the assumption that we observe a single time series from the slow\nprocess only. We construct estimators for both averaging and homogenization\nregimes, based on an appropriate misspecified model motivated by a second-order\nstochastic Taylor expansion of the slow process with respect to a function of\nthe time-scale separation parameter. In the case of a fixed number of\nobservations, we establish consistency, asymptotic normality, and asymptotic\nstatistical efficiency of a minimum contrast estimator (MCE), the limiting\nvariance having been identified explicitly; we furthermore establish\nconsistency and asymptotic normality of a simplified minimum constrast\nestimator (SMCE), which is however not in general efficient. These results are\nthen extended to the case of high-frequency observations under a condition\nrestricting the rate at which the number of observations may grow vis-\\`a-vis\nthe separation of scales. Numerical simulations illustrate the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:12:20 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 01:18:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gailus", "Siragan", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1709.02357", "submitter": "Adam Derek Cobb", "authors": "Adam D. Cobb, Andrew Markham, Stephen J. Roberts", "title": "Learning from lions: inferring the utility of agents from their\n  trajectories", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a model using Gaussian processes to infer a spatio-temporal vector\nfield from observed agent trajectories. Significant landmarks or influence\npoints in agent surroundings are jointly derived through vector calculus\noperations that indicate presence of sources and sinks. We evaluate these\ninfluence points by using the Kullback-Leibler divergence between the posterior\nand prior Laplacian of the inferred spatio-temporal vector field. Through\nlocating significant features that influence trajectories, our model aims to\ngive greater insight into underlying causal utility functions that determine\nagent decision-making. A key feature of our model is that it infers a joint\nGaussian process over the observed trajectories, the time-varying vector field\nof utility and canonical vector calculus operators. We apply our model to both\nsynthetic data and lion GPS data collected at the Bubye Valley Conservancy in\nsouthern Zimbabwe.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:10:12 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Cobb", "Adam D.", ""], ["Markham", "Andrew", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1709.02404", "submitter": "Pierre Masselot", "authors": "Pierre Masselot, Fateh Chebana, Diane B\\'elanger, Andr\\'e St-Hilaire,\n  Belkacem Abdous, Pierre Gosselin, Taha B.M.J. Ouarda", "title": "EMD-regression for modelling multi-scale relationships, and application\n  to weather-related cardiovascular mortality", "comments": null, "journal-ref": "Science of The Total Environment 612 (2018): 1018-1029", "doi": "10.1016/j.scitotenv.2017.08.276", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a number of environmental studies, relationships between natural processes\nare often assessed through regression analyses, using time series data. Such\ndata are often multi-scale and non-stationary, leading to a poor accuracy of\nthe resulting regression models and therefore to results with moderate\nreliability. To deal with this issue, the present paper introduces the\nEMD-regression methodology consisting in applying the empirical mode\ndecomposition (EMD) algorithm on data series and then using the resulting\ncomponents in regression models. The proposed methodology presents a number of\nadvantages. First, it accounts of the issues of non-stationarity associated to\nthe data series. Second, this approach acts as a scan for the relationship\nbetween a response variable and the predictors at different time scales,\nproviding new insights about this relationship. To illustrate the proposed\nmethodology it is applied to study the relationship between weather and\ncardiovascular mortality in Montreal, Canada. The results shed new knowledge\nconcerning the studied relationship. For instance, they show that the humidity\ncan cause excess mortality at the monthly time scale, which is a scale not\nvisible in classical models. A comparison is also conducted with state of the\nart methods which are the generalized additive models and distributed lag\nmodels, both widely used in weather-related health studies. The comparison\nshows that EMD-regression achieves better prediction performances and provides\nmore details than classical models concerning the relationship.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 18:42:12 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Masselot", "Pierre", ""], ["Chebana", "Fateh", ""], ["B\u00e9langer", "Diane", ""], ["St-Hilaire", "Andr\u00e9", ""], ["Abdous", "Belkacem", ""], ["Gosselin", "Pierre", ""], ["Ouarda", "Taha B. M. J.", ""]]}, {"id": "1709.02532", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence", "comments": "34 pages, 10 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:36:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 05:59:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:56:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:21:46 GMT"}, {"version": "v5", "created": "Sun, 25 Feb 2018 22:58:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1709.02577", "submitter": "Zhijian He", "authors": "Zhijian He and Xiaoqun Wang", "title": "An integrated quasi-Monte Carlo method for handling high dimensional\n  problems with discontinuities in financial engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) method is a useful numerical tool for pricing and\nhedging of complex financial derivatives. These problems are usually of high\ndimensionality and discontinuities. The two factors may significantly\ndeteriorate the performance of the QMC method. This paper develops an\nintegrated method that overcomes the challenges of the high dimensionality and\ndiscontinuities concurrently. For this purpose, a smoothing method is proposed\nto remove the discontinuities for some typical functions arising from financial\nengineering. To make the smoothing method applicable for more general\nfunctions, a new path generation method is designed for simulating the paths of\nthe underlying assets such that the resulting function has the required form.\nThe new path generation method has an additional power to reduce the effective\ndimension of the target function. Our proposed method caters for a large\nvariety of model specifications, including the Black-Scholes, exponential\nnormal inverse Gaussian L\\'evy, and Heston models. Numerical experiments\ndealing with these models show that in the QMC setting the proposed smoothing\nmethod in combination with the new path generation method can lead to a\ndramatic variance reduction for pricing exotic options with discontinuous\npayoffs and for calculating options' Greeks. The investigation on the effective\ndimension and the related characteristics explains the significant enhancement\nof the combined procedure.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 07:37:35 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 07:54:59 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["He", "Zhijian", ""], ["Wang", "Xiaoqun", ""]]}, {"id": "1709.02798", "submitter": "Alberto Carrassi", "authors": "Alberto Carrassi, Marc Bocquet, Laurent Bertino and Geir Evensen", "title": "Data Assimilation in the Geosciences - An overview on methods, issues\n  and perspectives", "comments": "79 pages, 10 figures, Invited review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We commonly refer to state-estimation theory in geosciences as data\nassimilation. This term encompasses the entire sequence of operations that,\nstarting from the observations of a system, and from additional statistical and\ndynamical information (such as a dynamical evolution model), provides an\nestimate of its state. Data assimilation is standard practice in numerical\nweather prediction, but its application is becoming widespread in many other\nareas of climate, atmosphere, ocean and environment modeling; in all\ncircumstances where one intends to estimate the state of a large dynamical\nsystem based on limited information. While the complexity of data assimilation,\nand of the methods thereof, stands on its interdisciplinary nature across\nstatistics, dynamical systems and numerical optimization, when applied to\ngeosciences an additional difficulty arises by the continually increasing\nsophistication of the environmental models. Thus, in spite of data assimilation\nbeing nowadays ubiquitous in geosciences, it has so far remained a topic mostly\nreserved to experts. We aim this overview article at geoscientists with a\nbackground in mathematical and physical modeling, who are interested in the\nrapid development of data assimilation and its growing domains of application\nin environmental science, but so far have not delved into its conceptual and\nmethodological complexities.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 16:15:04 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 00:27:46 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 13:56:19 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Carrassi", "Alberto", ""], ["Bocquet", "Marc", ""], ["Bertino", "Laurent", ""], ["Evensen", "Geir", ""]]}, {"id": "1709.02831", "submitter": "Hasinur Rahaman Khan", "authors": "Shaila Sharmin and Md Hasinur Rahaman Khan", "title": "Analysis of Unobserved Heterogeneity via Accelerated Failure Time Models\n  Under Bayesian and Classical Approaches", "comments": "23 pages, 4 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with unobserved heterogeneity in the survival dataset\nthrough Accelerated Failure Time (AFT) models under both frameworks--Bayesian\nand classical. The Bayesian approach of dealing with unobserved heterogeneity\nhas recently been discussed in Vallejos and Steel (2017), where mixture models\nare used to diminish the effect that anomalous observations or some kinds of\ncovariates which are not included in the survival models. The frailty models\nalso deal with this kind of unobserved variability under classical framework\nand have been used by practitioners as alternative to Bayesian. We discussed\nboth approaches of dealing with unobserved heterogeneity with their pros and\ncons when a family of rate mixtures of Weibul distributions and a set of random\neffect distributions were used under Bayesian and classical approaches\nrespectively. We investigated how much the classical estimates differ with the\nBayesian estimates, although the paradigm of estimation methods are different.\nTwo real data examples--a bone marrow transplants data and a kidney infection\ndata have been used to illustrate the performances of the methods. In both\nsituations, it is observed that the use of an Inverse-Gaussian mixture\ndistribution outperforms the other possibilities. It is also noticed that the\nestimates of the frailty models are generally somewhat underestimated by\ncomparing with the estimates of their counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 18:49:58 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Sharmin", "Shaila", ""], ["Khan", "Md Hasinur Rahaman", ""]]}, {"id": "1709.02907", "submitter": "Pritam Ranjan", "authors": "Natalia V. Bhattacharjee and Pritam Ranjan and Abhyuday Mandal and\n  Ernest W. Tollner", "title": "A History Matching Approach for Calibrating Hydrological Models", "comments": null, "journal-ref": "Environmental and Ecological Statistics, 26(1), 87-105, 2019", "doi": "10.1007/s10651-019-00420-9", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of hydrological time-series models is a challenging task since\nthese models give a wide spectrum of output series and calibration procedures\nrequire significant amount of time. From a statistical standpoint, this model\nparameter estimation problem simplifies to finding an inverse solution of a\ncomputer model that generates pre-specified time-series output (i.e., realistic\noutput series). In this paper, we propose a modified history matching approach\nfor calibrating the time-series rainfall-runoff models with respect to the real\ndata collected from the state of Georgia, USA. We present the methodology and\nillustrate the application of the algorithm by carrying a simulation study and\nthe two case studies. Several goodness-of-fit statistics were calculated to\nassess the model performance. The results showed that the proposed history\nmatching algorithm led to a significant improvement, of 30% and 14% (in terms\nof root mean squared error) and 26% and 118% (in terms of peak percent\nthreshold statistics), for the two case-studies with Matlab-Simulink and SWAT\nmodels, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:31:17 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 10:26:20 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 05:43:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bhattacharjee", "Natalia V.", ""], ["Ranjan", "Pritam", ""], ["Mandal", "Abhyuday", ""], ["Tollner", "Ernest W.", ""]]}, {"id": "1709.03003", "submitter": "Inon Sharony", "authors": "Inon Sharony", "title": "Scaled Rate Optimization for Beta-Binomial Models", "comments": "16 pages, 1 table, 1 algorithm, 1 program", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rates of binomial processes are modeled using beta-binomial distributions\n(for example, from Beta Regression). We treat the offline optimization scenario\nand then the online one, where we optimize the exploration-exploitation\nproblem. The rates given by two processes are compared through their\ndistributions, but we would like to optimize the net payout (given a constant\nvalue per successful event, unique for each of the processes). The result is an\nanalytically-closed, probabilistic, hypergeometric expression for comparing the\npayout distributions of two processes. To conclude, we contrast this Bayesian\nresult with an alternative frequentist approach and find 4.5 orders of\nmagnitude improvement in performance, for a numerical accuracy level of 0.01%.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 20:37:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Sharony", "Inon", ""]]}, {"id": "1709.03346", "submitter": "Pierre Alquier", "authors": "L\\'ena Carel and Pierre Alquier", "title": "Simultaneous Dimension Reduction and Clustering via the NMF-EM Algorithm", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, 2021, vol. 15, no.\n  1, pp. 231-260", "doi": "10.1007/s11634-020-00398-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are among the most popular tools for clustering. However, when\nthe dimension and the number of clusters is large, the estimation of the\nclusters become challenging, as well as their interpretation. Restriction on\nthe parameters can be used to reduce the dimension. An example is given by\nmixture of factor analyzers for Gaussian mixtures. The extension of MFA to\nnon-Gaussian mixtures is not straightforward. We propose a new constraint for\nparameters in non-Gaussian mixture model: the $K$ components parameters are\ncombinations of elements from a small dictionary, say $H$ elements, with $H \\ll\nK$. Including a nonnegative matrix factorization (NMF) in the EM algorithm\nallows us to simultaneously estimate the dictionary and the parameters of the\nmixture. We propose the acronym NMF-EM for this algorithm, implemented in the R\npackage {\\tt nmfem}. This original approach is motivated by passengers\nclustering from ticketing data: we apply NMF-EM to data from two Transdev\npublic transport networks. In this case, the words are easily interpreted as\ntypical slots in a timetable.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 11:58:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 21:29:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Carel", "L\u00e9na", ""], ["Alquier", "Pierre", ""]]}, {"id": "1709.03431", "submitter": "Peida Zhan", "authors": "Peida Zhan, Hong Jiao, Dandan Liao", "title": "A Longitudinal Higher-Order Diagnostic Classification Model", "comments": "35 pages, 12 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing diagnostic feedback about growth is crucial to formative decisions\nsuch as targeted remedial instructions or interventions. This paper proposed a\nlongitudinal higher-order diagnostic classification modeling approach for\nmeasuring growth. The new modeling approach is able to provide quantitative\nvalues of overall and individual growth by constructing a multidimensional\nhigher-order latent structure to take into account the correlations among\nmultiple latent attributes that are examined across different occasions. In\naddition, potential local item dependence among anchor (or repeated) items can\nalso be taken into account. Model parameter estimation is explored in a\nsimulation study. An empirical example is analyzed to illustrate the\napplications and advantages of the proposed modeling approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:16:31 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 13:24:39 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Zhan", "Peida", ""], ["Jiao", "Hong", ""], ["Liao", "Dandan", ""]]}, {"id": "1709.03563", "submitter": "Laura Anderlucci", "authors": "Laura Anderlucci, Angela Montanari and Cinzia Viroli", "title": "The Importance of Being Clustered: Uncluttering the Trends of Statistics\n  from 1970 to 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we retrace the recent history of statistics by analyzing all\nthe papers published in five prestigious statistical journals since 1970,\nnamely: Annals of Statistics, Biometrika, Journal of the American Statistical\nAssociation, Journal of the Royal Statistical Society, series B and Statistical\nScience. The aim is to construct a kind of \"taxonomy\" of the statistical papers\nby organizing and by clustering them in main themes. In this sense being\nidentified in a cluster means being important enough to be uncluttered in the\nvast and interconnected world of the statistical research. Since the main\nstatistical research topics naturally born, evolve or die during time, we will\nalso develop a dynamic clustering strategy, where a group in a time period is\nallowed to migrate or to merge into different groups in the following one.\nResults show that statistics is a very dynamic and evolving science, stimulated\nby the rise of new research questions and types of data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 19:59:27 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Anderlucci", "Laura", ""], ["Montanari", "Angela", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1709.03787", "submitter": "Balazs Vedres", "authors": "Balazs Vedres", "title": "Forbidden triads and Creative Success in Jazz: The Miles Davis Factor", "comments": null, "journal-ref": "Applied Network Science (2017) 2:31", "doi": "10.1007/s41109-017-0051-2", "report-no": null, "categories": "cs.SI nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article argues for the importance of forbidden triads - open triads with\nhigh-weight edges - in predicting success in creative fields. Forbidden triads\nhad been treated as a residual category beyond closed and open triads, yet I\nargue that these structures provide opportunities to combine socially evolved\nstyles in new ways. Using data on the entire history of recorded jazz from 1896\nto 2010, I show that observed collaborations have tolerated the openness of\nhigh weight triads more than expected, observed jazz sessions had more\nforbidden triads than expected, and the density of forbidden triads contributed\nto the success of recording sessions, measured by the number of record releases\nof session material. The article also shows that the sessions of Miles Davis\nhad received an especially high boost from forbidden triads.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 11:28:25 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Vedres", "Balazs", ""]]}, {"id": "1709.04038", "submitter": "Yang-Hui He", "authors": "Yang-Hui He", "title": "A Visualization of the Classical Musical Tradition", "comments": "9 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study of around 13,000 musical compositions from the Western classical\ntradition is carried out, spanning 33 major composers from the Baroque to the\nRomantic, with a focus on the usage of major/minor key signatures. A\n2-dimensional chromatic diagram is proposed to succinctly visualize the data.\nThe diagram is found to be useful not only in distinguishing style and period,\nbut also in tracking the career development of a particular composer.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 19:37:26 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["He", "Yang-Hui", ""]]}, {"id": "1709.04070", "submitter": "Christopher Rook", "authors": "Christopher J. Rook", "title": "Multivariate Density Modeling for Retirement Finance", "comments": "Full C/C++ implementation is included in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior to the financial crisis mortgage securitization models increased in\nsophistication as did products built to insure against losses. Layers of\ncomplexity formed upon a foundation that could not support it and as the\nfoundation crumbled the housing market followed. That foundation was the\nGaussian copula which failed to correctly model failure-time correlations of\nderivative securities in duress. In retirement, surveys suggest the greatest\nfear is running out of money and as retirement decumulation models become\nincreasingly sophisticated, large financial firms and robo-advisors may\nguarantee their success. Similar to an investment bank failure the event of\nretirement ruin is driven by outliers and correlations in times of stress. It\nwould be desirable to have a foundation able to support the increased\ncomplexity before it forms however the industry currently relies upon similar\nGaussian (or lognormal) dependence structures. We propose a multivariate\ndensity model having fixed marginals that is tractable and fits data which are\nskewed, heavy-tailed, multimodal, i.e., of arbitrary complexity allowing for a\nrich correlation structure. It is also ideal for stress-testing a retirement\nplan by fitting historical data seeded with black swan events. A preliminary\nsection reviews all concepts before they are used and fully documented C/C++\nsource code is attached making the research self-contained. Lastly, we take the\nopportunity to challenge existing retirement finance dogma and also review some\nrecent criticisms of retirement ruin probabilities and their suggested\nreplacement metrics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:12:04 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Rook", "Christopher J.", ""]]}, {"id": "1709.04196", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Hans K\\\"unsch", "title": "Particle Filters and Data Assimilation", "comments": "To appear in `Annual Review of Statistics and Its Application'", "journal-ref": null, "doi": "10.1146/annurev-statistics-031017-100232", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 08:59:35 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Fearnhead", "Paul", ""], ["K\u00fcnsch", "Hans", ""]]}, {"id": "1709.04556", "submitter": "Brendon Brewer", "authors": "Michael C. Rowe, Brendon J. Brewer", "title": "AMORPH: A statistical program for characterizing amorphous materials by\n  X-ray diffraction", "comments": "Accepted for publication in Computers and Geosciences. 24 pages, 13\n  figures. Software available at https://bitbucket.org/eggplantbren/amorph", "journal-ref": null, "doi": "10.1016/j.cageo.2018.07.004", "report-no": null, "categories": "physics.data-an physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AMORPH utilizes a new Bayesian statistical approach to interpreting X-ray\ndiffraction results of samples with both crystalline and amorphous components.\nAMORPH fits X-ray diffraction patterns with a mixture of narrow and wide\ncomponents, simultaneously inferring all of the model parameters and\nquantifying their uncertainties. The program simulates background patterns\npreviously applied manually, providing reproducible results, and significantly\nreducing inter- and intra-user biases. This approach allows for the\nquantification of amorphous and crystalline materials and for the\ncharacterization of the amorphous component, including properties such as the\ncentre of mass, width, skewness, and nongaussianity of the amorphous component.\nResults demonstrate the applicability of this program for calculating amorphous\ncontents of volcanic materials and independently modeling their properties in\ncompositionally variable materials.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 22:31:31 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 00:53:26 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Rowe", "Michael C.", ""], ["Brewer", "Brendon J.", ""]]}, {"id": "1709.04702", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Trait evolution with jumps: illusionary normality", "comments": "http://kkzmbm.mimuw.edu.pl/?pageId=4&sprawId=23", "journal-ref": "Proceedings of the XXIII National Conference on Applications of\n  Mathematics in Biology and Medicine. 2017, pp. 23-28", "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic comparative methods for real-valued traits usually make use of\nstochastic process whose trajectories are continuous. This is despite\nbiological intuition that evolution is rather punctuated than gradual. On the\nother hand, there has been a number of recent proposals of evolutionary models\nwith jump components. However, as we are only beginning to understand the\nbehaviour of branching Ornstein-Uhlenbeck (OU) processes the asymptotics of\nbranching OU processes with jumps is an even greater unknown. In this work we\nbuild up on a previous study concerning OU with jumps evolution on a pure birth\ntree. We introduce an extinction component and explore via simulations, its\neffects on the weak convergence of such a process. We furthermore, also use\nthis work to illustrate the simulation and graphic generation possibilities of\nthe mvSLOUCH package.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:14:07 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 09:02:41 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1709.04743", "submitter": "Sebastian Lerch", "authors": "Alexander Jordan, Fabian Kr\\\"uger, Sebastian Lerch", "title": "Evaluating probabilistic forecasts with scoringRules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts in the form of probability distributions over future\nevents have become popular in several fields including meteorology, hydrology,\neconomics, and demography. In typical applications, many alternative\nstatistical models and data sources can be used to produce probabilistic\nforecasts. Hence, evaluating and selecting among competing methods is an\nimportant task. The scoringRules package for R provides functionality for\ncomparative evaluation of probabilistic models based on proper scoring rules,\ncovering a wide range of situations in applied work. This paper discusses\nimplementation and usage details, presents case studies from meteorology and\neconomics, and points to the relevant background literature.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:55:24 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:24:06 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Jordan", "Alexander", ""], ["Kr\u00fcger", "Fabian", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1709.04835", "submitter": "James Fry", "authors": "J.T. Fry, Matt Slifko, Scotland Leman", "title": "Generalized Biplots for Multidimensional Scaled Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction and visualization is a staple of data analytics. Methods\nsuch as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS)\nprovide low dimensional (LD) projections of high dimensional (HD) data while\npreserving an HD relationship between observations. Traditional biplots assign\nmeaning to the LD space of a PCA projection by displaying LD axes for the\nattributes. These axes, however, are specific to the linear projection used in\nPCA. MDS projections, which allow for arbitrary stress and dissimilarity\nfunctions, require special care when labeling the LD space. We propose an\niterative scheme to plot an LD axis for each attribute based on the\nuser-specified stress and dissimilarity metrics. We discuss the details of our\ngeneral biplot methodology, its relationship with PCA-derived biplots, and\nprovide examples using real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:03:33 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 16:30:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Fry", "J. T.", ""], ["Slifko", "Matt", ""], ["Leman", "Scotland", ""]]}, {"id": "1709.05006", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Alexander Cloninger and Ronald R. Coifman", "title": "Two-sample Statistics Based on Anisotropic Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)\nstatistic for measuring the distance between two distributions given\nfinitely-many multivariate samples. When the distributions are locally\nlow-dimensional, the proposed test can be made more powerful to distinguish\ncertain alternatives by incorporating local covariance matrices and\nconstructing an anisotropic kernel. The kernel matrix is asymmetric; it\ncomputes the affinity between $n$ data points and a set of $n_R$ reference\npoints, where $n_R$ can be drastically smaller than $n$. While the proposed\nstatistic can be viewed as a special class of Reproducing Kernel Hilbert Space\nMMD, the consistency of the test is proved, under mild assumptions of the\nkernel, as long as $\\|p-q\\| \\sqrt{n} \\to \\infty $, and a finite-sample lower\nbound of the testing power is obtained. Applications to flow cytometry and\ndiffusion MRI datasets are demonstrated, which motivate the proposed approach\nto compare distributions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 23:06:19 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 15:39:36 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 21:56:28 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1709.05189", "submitter": "Michael Spence", "authors": "Michael A Spence, Julia L. Blanchard, Axel G. Rossberg, Michael R.\n  Heath, Johanna J Heymans, Steven Mackinson, Natalia Serpetti, Douglas Speirs,\n  Robert B. Thorpe and Paul G. Blackwell", "title": "Multi-model ensembles for ecosystem prediction", "comments": "28 pages and 6 figures + 13 pages appendix and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When making predictions about ecosystems, we often have available a number of\ndifferent ecosystem models that attempt to represent their dynamics in a\ndetailed mechanistic way. Each of these can be used as simulators of\nlarge-scale experiments and make forecasts about the fate of ecosystems under\ndifferent scenarios in order to support the development of appropriate\nmanagement strategies. However, structural differences, systematic\ndiscrepancies and uncertainties lead to different models giving different\npredictions under these scenarios. This is further complicated by the fact that\nthe models may not be run with the same species or functional groups, spatial\nstructure or time scale. Rather than simply trying to select a 'best' model, or\ntaking some weighted average, it is important to exploit the strengths of each\nof the available models, while learning from the differences between them. To\nachieve this, we construct a flexible statistical model of the relationships\nbetween a collection or 'ensemble' of mechanistic models and their biases,\nallowing for structural and parameter uncertainty and for different ways of\nrepresenting reality. Using this statistical meta-model, we can combine prior\nbeliefs, model estimates and direct observations using Bayesian methods, and\nmake coherent predictions of future outcomes under different scenarios with\nrobust measures of uncertainty. In this paper we present the modelling\nframework and discuss results obtained using a diverse ensemble of models in\nscenarios involving future changes in fishing levels. These examples illustrate\nthe value of our approach in predicting outcomes for possible strategies\npertaining to climate and fisheries policy aimed at improving food security and\nmaintaining ecosystem integrity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 13:22:44 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Spence", "Michael A", ""], ["Blanchard", "Julia L.", ""], ["Rossberg", "Axel G.", ""], ["Heath", "Michael R.", ""], ["Heymans", "Johanna J", ""], ["Mackinson", "Steven", ""], ["Serpetti", "Natalia", ""], ["Speirs", "Douglas", ""], ["Thorpe", "Robert B.", ""], ["Blackwell", "Paul G.", ""]]}, {"id": "1709.05275", "submitter": "Ye Liang", "authors": "Ye Liang, Yang Li, Bin Zhang", "title": "Bayesian Nonparametric Inference for Panel Count Data with an\n  Informative Observation Process", "comments": "25 pages, 7 figures", "journal-ref": "Biometrical Journal (2018), 60(3), 583-596", "doi": "10.1002/bimj.201700176", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the panel count data analysis for recurrent events is\nconsidered. Such analysis is useful for studying tumor or infection recurrences\nin both clinical trial and observational studies. A bivariate Gaussian Cox\nprocess model is proposed to jointly model the observation process and the\nrecurrent event process. Bayesian nonparametric inference is proposed for\nsimultaneously estimating regression parameters, bivariate frailty effects and\nbaseline intensity functions. Inference is done through Markov chain Monte\nCarlo, with fully developed computational techniques. Predictive inference is\nalso discussed under the Bayesian setting. The proposed method is shown to be\nefficient via simulation studies. A clinical trial dataset on skin cancer\npatients is analyzed to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:40:14 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 21:37:15 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liang", "Ye", ""], ["Li", "Yang", ""], ["Zhang", "Bin", ""]]}, {"id": "1709.05328", "submitter": "Xi Luo", "authors": "Yi Zhao and Xi Luo", "title": "Granger Mediation Analysis of Multiple Time Series with an Application\n  to fMRI", "comments": "59 pages. Presented at the 2017 ENAR, JSM, and other meetings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It becomes increasingly popular to perform mediation analysis for complex\ndata from sophisticated experimental studies. In this paper, we present Granger\nMediation Analysis (GMA), a new framework for causal mediation analysis of\nmultiple time series. This framework is motivated by a functional magnetic\nresonance imaging (fMRI) experiment where we are interested in estimating the\nmediation effects between a randomized stimulus time series and brain activity\ntime series from two brain regions. The stable unit treatment assumption for\ncausal mediation analysis is thus unrealistic for this type of time series\ndata. To address this challenge, our framework integrates two types of models:\ncausal mediation analysis across the variables and vector autoregressive models\nacross the temporal observations. We further extend this framework to handle\nmultilevel data to address individual variability and correlated errors between\nthe mediator and the outcome variables. These models not only provide valid\ncausal mediation for time series data but also model the causal dynamics across\ntime. We show that the modeling parameters in our models are identifiable, and\nwe develop computationally efficient methods to maximize the likelihood-based\noptimization criteria. Simulation studies show that our method reduces the\nestimation bias and improve statistical power, compared to existing approaches.\nOn a real fMRI data set, our approach not only infers the causal effects of\nbrain pathways but accurately captures the feedback effect of the outcome\nregion on the mediator region.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:47:51 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1709.05515", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Suhas N., Talasila Sai Teja and Anshul Juneja", "title": "Some variations on Ensembled Random Survival Forest with application to\n  Cancer Research", "comments": "16 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel implementation of adaboost for prediction\nof survival function. We take different variations of the algorithm and compare\nthe algorithms based on system run time and root mean square error. Our\nconstruction includes right censoring data and competing risk data too. We take\ndifferent data set to illustrate the performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:12:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 19:24:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["N.", "Suhas", ""], ["Teja", "Talasila Sai", ""], ["Juneja", "Anshul", ""]]}, {"id": "1709.05534", "submitter": "Camilo Jose Torres-Jimenez", "authors": "Camilo Jose Torres-Jimenez and Alvaro Mauricio Montenegro-Diaz", "title": "An alternative to continuous univariate distributions supported on a\n  bounded interval: The BMT distribution", "comments": "30 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the BMT distribution as an unimodal alternative\nto continuous univariate distributions supported on a bounded interval. The\nideas behind the mathematical formulation of this new distribution come from\ncomputer aid geometric design, specifically from Bezier curves. First, we\nreview general properties of a distribution given by parametric equations and\nextend the definition of a Bezier distribution. Then, after proposing the BMT\ncumulative distribution function, we derive its probability density function\nand a closed-form expression for quantile function, median, interquartile\nrange, mode, and moments. The domain change from [0,1] to [c,d] is mentioned.\nEstimation of parameters is approached by the methods of maximum likelihood and\nmaximum product of spacing. We test the numerical estimation procedures using\nsome simulated data. Usefulness and flexibility of the new distribution are\nillustrated in three real data sets. The BMT distribution has a significant\npotential to estimate domain parameters and to model data outside the scope of\nthe beta or similar distributions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 15:48:19 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Torres-Jimenez", "Camilo Jose", ""], ["Montenegro-Diaz", "Alvaro Mauricio", ""]]}, {"id": "1709.05548", "submitter": "Evgeny Burnaev", "authors": "Rodrigo Rivera and Evgeny Burnaev", "title": "Forecasting of commercial sales with large scale Gaussian Processes", "comments": "1o pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper argues that there has not been enough discussion in the field of\napplications of Gaussian Process for the fast moving consumer goods industry.\nYet, this technique can be important as it e.g., can provide automatic feature\nrelevance determination and the posterior mean can unlock insights on the data.\nSignificant challenges are the large size and high dimensionality of commercial\ndata at a point of sale. The study reviews approaches in the Gaussian Processes\nmodeling for large data sets, evaluates their performance on commercial sales\nand shows value of this type of models as a decision-making tool for\nmanagement.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:51:58 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Rivera", "Rodrigo", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1709.05551", "submitter": "Kris Sankaran", "authors": "Kris Sankaran, Diego Garcia-Olano, Mobin Javed, Maria Fernanda\n  Alcala-Durand, Adolfo De Un\\'anue, Paul van der Boor, Eric Potash, Roberto\n  S\\'anchez Avalos, Luis I\\~naki Alberro Encinas, Rayid Ghani", "title": "Applying Machine Learning Methods to Enhance the Distribution of Social\n  Services in Mexico", "comments": "This work was done as part of the 2016 Eric & Wendy Schmidt Data\n  Science for Social Good Summer Fellowship at the University of Chicago", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Government of Mexico's social development agency, SEDESOL, is responsible\nfor the administration of social services and has the mission of lifting\nMexican families out of poverty. One key challenge they face is matching people\nwho have social service needs with the services SEDESOL can provide accurately\nand efficiently. In this work we describe two specific applications implemented\nin collaboration with SEDESOL to enhance their distribution of social services.\nThe first problem relates to systematic underreporting on applications for\nsocial services, which makes it difficult to identify where to prioritize\noutreach. Responding that five people reside in a home when only three do is a\ntype of underreporting that could occur while a social worker conducts a home\nsurvey with a family to determine their eligibility for services. The second\ninvolves approximating multidimensional poverty profiles across households.\nThat is, can we characterize different types of vulnerabilities -- for example,\nfood insecurity and lack of health services -- faced by those in poverty?\n  We detail the problem context, available data, our machine learning\nformulation, experimental results, and effective feature sets. As far as we are\naware this is the first time government data of this scale has been used to\ncombat poverty within Mexico. We found that survey data alone can suggest\npotential underreporting.\n  Further, we found geographic features useful for housing and service related\nindicators and transactional data informative for other dimensions of poverty.\nThe results from our machine learning system for estimating poverty profiles\nwill directly help better match 7.4 million individuals to social programs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 18:57:37 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Sankaran", "Kris", ""], ["Garcia-Olano", "Diego", ""], ["Javed", "Mobin", ""], ["Alcala-Durand", "Maria Fernanda", ""], ["De Un\u00e1nue", "Adolfo", ""], ["van der Boor", "Paul", ""], ["Potash", "Eric", ""], ["Avalos", "Roberto S\u00e1nchez", ""], ["Encinas", "Luis I\u00f1aki Alberro", ""], ["Ghani", "Rayid", ""]]}, {"id": "1709.05613", "submitter": "Subrata Chakraborty", "authors": "S. Chakraborty, S. H. Ong and C. M. Ng", "title": "A new probability model with support on unit interval: Structural\n  properties, regression of bounded response and applications", "comments": "37 pages; 2 Figures, 3 Table, Pre-print Version-4.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new distribution on (0, 1), generalized Log-Lindley distribution, is\nproposed by extending the Log-Lindley distribution. This new distribution is\nshown to be a weighted Log-Lindley distribution. Important probabilistic and\nstatistical properties have been derived. An interesting characterization of\nthe weighted distribution in terms of Kullback-Liebler distance and weighted\nentropy has also been obtained. A useful result in insurance for the distorted\npremium principal is presented and verified with numerical calculations. New\nregression models for bounded responses based on this distribution and their\napplication is illustrated by considering modeling a real life data on risk\nmanagement and another data set on outpatient health expenditure in comparison\nwith beta regression and Log-Lindley regression models. Much better fits for\nboth data sets justify the relevance of the new distribution in statistical\nmodeling and analysis. Furthermore this generalization, apart from adding\nflexibility for modelling, retains the compactness and tractability of\nstatistical quantities required for statistical analysis, which is a feature of\nthe Log-Lindley distribution. Thus, the generalized Log-Lindley distribution\nshould be a useful addition to statistical models for practitioners.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 06:51:38 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 05:52:26 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 18:35:56 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 05:28:42 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Chakraborty", "S.", ""], ["Ong", "S. H.", ""], ["Ng", "C. M.", ""]]}, {"id": "1709.05619", "submitter": "Yuntian Chen", "authors": "Yuntian Chen, Su Jiang, Dongxiao Zhang, Chaoyang Liu", "title": "An adsorbed gas estimation model for shale gas reservoirs via\n  statistical learning", "comments": null, "journal-ref": "Applied Energy, 2017, 197: 327-341", "doi": "10.1016/j.apenergy.2017.04.029", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shale gas plays an important role in reducing pollution and adjusting the\nstructure of world energy. Gas content estimation is particularly significant\nin shale gas resource evaluation. There exist various estimation methods, such\nas first principle methods and empirical models. However, resource evaluation\npresents many challenges, especially the insufficient accuracy of existing\nmodels and the high cost resulting from time-consuming adsorption experiments.\nIn this research, a low-cost and high-accuracy model based on geological\nparameters is constructed through statistical learning methods to estimate\nadsorbed shale gas content\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 07:31:20 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Chen", "Yuntian", ""], ["Jiang", "Su", ""], ["Zhang", "Dongxiao", ""], ["Liu", "Chaoyang", ""]]}, {"id": "1709.05637", "submitter": "Giuseppe Sanfilippo", "authors": "Andrea Capotorti, Frank Lad and Giuseppe Sanfilippo", "title": "Reassessing Accuracy Rates of Median Decisions", "comments": null, "journal-ref": "The American Statistician, American Statistical Association, vol.\n  61, no. 2, pp. 132-138, 2007", "doi": "10.1198/000313007X190943", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how Bruno de Finetti's fundamental theorem of prevision has\ncomputable applications in statistical problems that involve only partial\ninformation. Specifically, we assess accuracy rates for median decision\nprocedures used in the radiological diagnosis of asbestosis. Conditional\nexchangeability of individual radiologists' diagnoses is recognized as more\nappropriate than independence which is commonly presumed. The FTP yields\ncoherent bounds on probabilities of interest when available information is\ninsufficient to determine a complete distribution. Further assertions that are\nnatural to the problem motivate a partial ordering of conditional\nprobabilities, extending the computation from a linear to a quadratic\nprogramming problem.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 10:26:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Capotorti", "Andrea", ""], ["Lad", "Frank", ""], ["Sanfilippo", "Giuseppe", ""]]}, {"id": "1709.05906", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Sanku Dey and Debasis Kundu", "title": "Bayesian analysis of three parameter singular Marshall-Olkin bivariate\n  Pareto distribution", "comments": "23 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides bayesian analysis of singular Marshall-Olkin bivariate\nPareto distribution. We consider three parameter singular Marshall-Olkin\nbivariate Pareto distribution. We consider two types of prior - reference prior\nand gamma prior. Bayes estimate of the parameters are calculated based on slice\ncum gibbs sampler and Lindley approximation. Credible interval is also provided\nfor all methods and all prior distributions. A data analysis is kept for\nillustrative purpose.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:13:12 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 02:06:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""], ["Kundu", "Debasis", ""]]}, {"id": "1709.06111", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis, Takanori Furukawa, Norman van Rhijn, Michael\n  Bromley, Elaine Bignell, Magnus Rattray", "title": "Bayesian detection of piecewise linear trends in replicated time-series\n  with application to growth data modelling", "comments": "Accepted to International Journal of Biostatistics", "journal-ref": "The International Journal Of Biostatistics, 2019", "doi": "10.1515/ijb-2018-0052", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the situation where a temporal process is composed of contiguous\nsegments with differing slopes and replicated noise-corrupted time series\nmeasurements are observed. The unknown mean of the data generating process is\nmodelled as a piecewise linear function of time with an unknown number of\nchange-points. We develop a Bayesian approach to infer the joint posterior\ndistribution of the number and position of change-points as well as the unknown\nmean parameters. A-priori, the proposed model uses an overfitting number of\nmean parameters but, conditionally on a set of change-points, only a subset of\nthem influences the likelihood. An exponentially decreasing prior distribution\non the number of change-points gives rise to a posterior distribution\nconcentrating on sparse representations of the underlying sequence. A\nMetropolis-Hastings Markov chain Monte Carlo (MCMC) sampler is constructed for\napproximating the posterior distribution. Our method is benchmarked using\nsimulated data and is applied to uncover differences in the dynamics of fungal\ngrowth from imaging time course data collected from different strains. The\nsource code is available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 18:14:10 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 20:17:32 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 09:06:30 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Furukawa", "Takanori", ""], ["van Rhijn", "Norman", ""], ["Bromley", "Michael", ""], ["Bignell", "Elaine", ""], ["Rattray", "Magnus", ""]]}, {"id": "1709.06134", "submitter": "Yu Zheng", "authors": "Zhe Wang, Yu Zheng, David C. Zhu, Jian Ren and Tongtong Li", "title": "Discrete Dynamic Causal Modeling and Its Relationship with Directed\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the discrete Dynamic Causal Modeling (DDCM) and its\nrelationship with Directed Information (DI). We prove the conditional\nequivalence between DDCM and DI in characterizing the causal relationship\nbetween two brain regions. The theoretical results are demonstrated using fMRI\ndata obtained under both resting state and stimulus based state. Our numerical\nanalysis is consistent with that reported in previous study.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:43:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Zhe", ""], ["Zheng", "Yu", ""], ["Zhu", "David C.", ""], ["Ren", "Jian", ""], ["Li", "Tongtong", ""]]}, {"id": "1709.06211", "submitter": "Marie-Abele Bind", "authors": "Marie-Abele C. Bind and Donald B. Rubin", "title": "Bridging observational studies and randomized experiments by embedding\n  the former in the latter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health effects of environmental exposures have been studied for decades,\ntypically using standard regression models to assess exposure-outcome\nassociations found in observational non-experimental data. We propose and\nillustrate a different approach to examine causal effects of environmental\nexposures on health outcomes from observational data. Our strategy attempts to\nstructure the observational data to approximate data from a hypothetical, but\nrealistic, randomized experiment. This approach, based on insights from\nclassical experimental design, involves four stages, and relies on modern\ncomputing to implement the effort in two of the four stages.More specifically,\nour strategy involves: 1) a conceptual stage that involves the precise\nformulation of the causal question in terms of a hypothetical randomized\nexperiment where the exposure is assigned to units; 2) a design stage that\nattempts to reconstruct (or approximate) a randomized experiment before any\noutcome data are observed, 3) a statistical analysis comparing the outcomes of\ninterest in the exposed and non-exposed units of the hypothetical randomized\nexperiment, and 4) a summary stage providing conclusions about statistical\nevidence for the sizes of possible causal effects of the exposure on outcomes.\nWe illustrate our approach using an example examining the effect of parental\nsmoking on children's lung function collected in families living in East Boston\nin the 1970's. To complement the traditional purely model-based approaches, our\nstrategy, which includes outcome free matched-sampling, provides workable tools\nto quantify possible detrimental exposure effects on human health outcomes\nespecially because it also includes transparent diagnostics to assess the\nassumptions of the four-stage statistical approach being applied.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 00:51:54 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Bind", "Marie-Abele C.", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1709.06272", "submitter": "Udaysinh T. Bhosale", "authors": "Udaysinh T. Bhosale", "title": "Entanglement transitions induced by large deviations", "comments": "12 pages, 4 figures. Comments are welcome", "journal-ref": "Phys. Rev. E 96, 062149 (2017)", "doi": "10.1103/PhysRevE.96.062149", "report-no": null, "categories": "quant-ph math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability of large deviations of the smallest Schmidt eigenvalue for\nrandom pure states of bipartite systems, denoted as $A$ and $B$, is computed\nanalytically using a Coulomb gas method. It is shown that this probability, for\nlarge $N$, goes as $\\exp[-\\beta N^2\\Phi(\\zeta)]$, where the parameter $\\beta$\nis the Dyson index of the ensemble, $\\zeta$ is the large deviation parameter\nwhile the rate function $\\Phi(\\zeta)$ is calculated exactly. Corresponding\nequilibrium Coulomb charge density is derived for its large deviations. Effects\nof the large deviations of the extreme (largest and smallest) Schmidt\neigenvalues on the bipartite entanglement are studied using the von Neumann\nentropy. Effect of these deviations is also studied on the entanglement between\nsubsystems $1$ and $2$, obtained by further partitioning the subsystem $A$,\nusing the properties of the density matrix's partial transpose\n$\\rho_{12}^\\Gamma$. The density of states of $\\rho_{12}^\\Gamma$ is found to be\nclose to the Wigner's semicircle law with these large deviations. The\nentanglement properties are captured very well by a simple random matrix model\nfor the partial transpose. The model predicts the entanglement transition\nacross a critical large deviation parameter $\\zeta$. Log negativity is used to\nquantify the entanglement between subsystems $1$ and $2$. Analytical formulas\nfor it are derived using the simple model. Numerical simulations are in\nexcellent agreement with the analytical results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 07:04:59 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 11:40:51 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 06:15:19 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bhosale", "Udaysinh T.", ""]]}, {"id": "1709.06320", "submitter": "Jiali Mei", "authors": "Jiali Mei, Yohann De Castro, Yannig Goude, Jean-Marc Aza\\\"is, Georges\n  H\\'ebrail", "title": "Nonnegative matrix factorization with side information for time series\n  recovery and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the reconstruction and the prediction of electricity\nconsumption, we extend Nonnegative Matrix Factorization~(NMF) to take into\naccount side information (column or row features). We consider general linear\nmeasurement settings, and propose a framework which models non-linear\nrelationships between features and the response variables. We extend previous\ntheoretical results to obtain a sufficient condition on the identifiability of\nthe NMF in this setting. Based the classical Hierarchical Alternating Least\nSquares~(HALS) algorithm, we propose a new algorithm (HALSX, or Hierarchical\nAlternating Least Squares with eXogeneous variables) which estimates the\nfactorization model. The algorithm is validated on both simulated and real\nelectricity consumption datasets as well as a recommendation dataset, to show\nits performance in matrix recovery and prediction for new rows and columns.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:55:07 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Mei", "Jiali", ""], ["De Castro", "Yohann", ""], ["Goude", "Yannig", ""], ["Aza\u00efs", "Jean-Marc", ""], ["H\u00e9brail", "Georges", ""]]}, {"id": "1709.06365", "submitter": "Wray Buntine", "authors": "He Zhao, Lan Du, Wray Buntine, Gang Liu", "title": "MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "comments": "To appear in ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the text content, documents and their associated words usually come\nwith rich sets of meta informa- tion, such as categories of documents and\nsemantic/syntactic features of words, like those encoded in word embeddings.\nIncorporating such meta information directly into the generative process of\ntopic models can improve modelling accuracy and topic quality, especially in\nthe case where the word-occurrence information in the training data is\ninsufficient. In this paper, we present a topic model, called MetaLDA, which is\nable to leverage either document or word meta information, or both of them\njointly. With two data argumentation techniques, we can derive an efficient\nGibbs sampling algorithm, which benefits from the fully local conjugacy of the\nmodel. Moreover, the algorithm is favoured by the sparsity of the meta\ninformation. Extensive experiments on several real world datasets demonstrate\nthat our model achieves comparable or improved performance in terms of both\nperplexity and topic quality, particularly in handling sparse texts. In\naddition, compared with other models using meta information, our model runs\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 12:09:21 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Zhao", "He", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""], ["Liu", "Gang", ""]]}, {"id": "1709.06556", "submitter": "Matjaz Perc", "authors": "Kristina Ban, Matjaz Perc, Zoran Levnajic", "title": "Robust clustering of languages across Wikipedia growth", "comments": "9 two-column pages, 7 figures; accepted for publication in Royal\n  Society Open Science", "journal-ref": "R. Soc. Open Sci. 4, 171217 (2017)", "doi": "10.1098/rsos.171217", "report-no": null, "categories": "cs.DL cs.SI physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is the largest existing knowledge repository that is growing on a\ngenuine crowdsourcing support. While the English Wikipedia is the most\nextensive and the most researched one with over five million articles,\ncomparatively little is known about the behavior and growth of the remaining\n283 smaller Wikipedias, the smallest of which, Afar, has only one article. Here\nwe use a subset of this data, consisting of 14962 different articles, each of\nwhich exists in 26 different languages, from Arabic to Ukrainian. We study the\ngrowth of Wikipedias in these languages over a time span of 15 years. We show\nthat, while an average article follows a random path from one language to\nanother, there exist six well-defined clusters of Wikipedias that share common\ngrowth patterns. The make-up of these clusters is remarkably robust against the\nmethod used for their determination, as we verify via four different clustering\nmethods. Interestingly, the identified Wikipedia clusters have little\ncorrelation with language families and groups. Rather, the growth of Wikipedia\nacross different languages is governed by different factors, ranging from\nsimilarities in culture to information literacy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:35:52 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Ban", "Kristina", ""], ["Perc", "Matjaz", ""], ["Levnajic", "Zoran", ""]]}, {"id": "1709.06737", "submitter": "Markus Viljanen", "authors": "Markus Viljanen, Antti Airola, Anne-Maarit Majanoja, Jukka Heikkonen,\n  Tapio Pahikkala", "title": "Measuring Player Retention and Monetization using the Mean Cumulative\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game analytics supports game development by providing direct quantitative\nfeedback about player experience. Player retention and monetization in\nparticular have become central business statistics in free-to-play game\ndevelopment. Many metrics have been used for this purpose. However, game\ndevelopers often want to perform analytics in a timely manner before all users\nhave churned from the game. This causes data censoring which makes many metrics\nbiased. In this work, we introduce how the Mean Cumulative Function (MCF) can\nbe used to generalize many academic metrics to censored data. The MCF allows us\nto estimate the expected value of a metric over time, which for example may be\nthe number of game sessions, number of purchases, total playtime and lifetime\nvalue. Furthermore, the popular retention rate metric is the derivative of this\nestimate applied to the expected number of distinct days played. Statistical\ntools based on the MCF allow game developers to determine whether a given\nchange improves a game, or whether a game is yet good enough for public\nrelease. The advantages of this approach are demonstrated on a real\nin-development free-to-play mobile game, the Hipster Sheep.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 07:00:11 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Viljanen", "Markus", ""], ["Airola", "Antti", ""], ["Majanoja", "Anne-Maarit", ""], ["Heikkonen", "Jukka", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1709.07032", "submitter": "Ramon Iglesias", "authors": "Ramon Iglesias and Federico Rossi and Kevin Wang and David Hallac and\n  Jure Leskovec and Marco Pavone", "title": "Data-Driven Model Predictive Control of Autonomous Mobility-on-Demand\n  Systems", "comments": "Submitted to the International Conference on Robotics and Automation\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to present an end-to-end, data-driven framework to\ncontrol Autonomous Mobility-on-Demand systems (AMoD, i.e. fleets of\nself-driving vehicles). We first model the AMoD system using a time-expanded\nnetwork, and present a formulation that computes the optimal rebalancing\nstrategy (i.e., preemptive repositioning) and the minimum feasible fleet size\nfor a given travel demand. Then, we adapt this formulation to devise a Model\nPredictive Control (MPC) algorithm that leverages short-term demand forecasts\nbased on historical data to compute rebalancing strategies. We test the\nend-to-end performance of this controller with a state-of-the-art LSTM neural\nnetwork to predict customer demand and real customer data from DiDi Chuxing: we\nshow that this approach scales very well for large systems (indeed, the\ncomputational complexity of the MPC algorithm does not depend on the number of\ncustomers and of vehicles in the system) and outperforms state-of-the-art\nrebalancing strategies by reducing the mean customer wait time by up to to\n89.6%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:50:39 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Iglesias", "Ramon", ""], ["Rossi", "Federico", ""], ["Wang", "Kevin", ""], ["Hallac", "David", ""], ["Leskovec", "Jure", ""], ["Pavone", "Marco", ""]]}, {"id": "1709.07107", "submitter": "Jabed H. Tomal", "authors": "Jabed H Tomal and Jan JH Ciborowski", "title": "Statistical methods for estimating ecological breakpoints and prediction\n  intervals", "comments": "Submitted to Ecological Modelling (30 Pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationships among ecological variables are usually obtained by fitting\nstatistical models that go through the conditional means of the dependent\nvariables. For example, the nonparametric loess and the parametric piecewise\nlinear regression models, which pass through the conditional mean of the\nresponse variable given the predictor, are used to analyze simple to complex\nrelationships among variables. We used loess and bootstrapped confidence\ninterval to subjectively identify the number and positions of potential\necological breakpoints in a bivariate relationship, and a piecewise linear\nregression model (PLRM) to quantitatively estimate the location of breakpoints\nand the associated precision. We also estimated breakpoint location and\nprecision using a piecewise linear quantile regression model (PQRM), which is\nfitted to the quantiles of the conditional distribution of the response\nvariable given the predictor and provides much richer information in terms of\nestimating relationships and breakpoints. We compared the precision of\nbreakpoints estimated by PQRM relative to PLRM. We compared the precision of\nthe methods using two examples from the ecological literature suspected to\nexhibit multiple breakpoints: relating a Fish Index of Biotic Integrity (an\nindex of wetlands' fish community 'health') to the amount of human activity in\nwetlands' adjacent watersheds; and relating the biomass of cyanobacteria to the\ntotal phosphorus concentration in Canadian lakes. Statistically significant\nbreakpoints were detected for both datasets, demarcating the boundaries of\nthree line segments with markedly different slopes. We recommend the piecewise\nlinear quantile regression as an effective means of characterizing bivariate\nenvironmental relationships where the scatter of points represents natural\nenvironmental variation rather than measurement error.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 00:09:25 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 18:50:54 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Tomal", "Jabed H", ""], ["Ciborowski", "Jan JH", ""]]}, {"id": "1709.07285", "submitter": "Carel F.W. Peeters", "authors": "Francisca A. de Leeuw, Carel F.W. Peeters, Maartje I. Kester, Amy C.\n  Harms, Eduard A. Struys, Thomas Hankemeier, Herman W.T. van Vlijmen, Sven J.\n  van der Lee, Cornelia M. van Duijn, Philip Scheltens, Ay\\c{s}e Demirkan, Mark\n  A. van de Wiel, Wiesje M. van der Flier, Charlotte E. Teunissen", "title": "Blood-based metabolic signatures in Alzheimer's disease", "comments": "Postprint, 76 pages, 32 figures, includes supplementary material", "journal-ref": "Alzheimer's & Dementia: Diagnosis, Assessment & Disease\n  Monitoring, 8 (2017): 196-207", "doi": "10.1016/j.dadm.2017.07.006", "report-no": null, "categories": "q-bio.NC q-bio.MN stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction: Identification of blood-based metabolic changes might provide\nearly and easy-to-obtain biomarkers.\n  Methods: We included 127 AD patients and 121 controls with\nCSF-biomarker-confirmed diagnosis (cut-off tau/A$\\beta_{42}$: 0.52). Mass\nspectrometry platforms determined the concentrations of 53 amine, 22 organic\nacid, 120 lipid, and 40 oxidative stress compounds. Multiple signatures were\nassessed: differential expression (nested linear models), classification\n(logistic regression), and regulatory (network extraction).\n  Results: Twenty-six metabolites were differentially expressed. Metabolites\nimproved the classification performance of clinical variables from 74% to 79%.\nNetwork models identified 5 hubs of metabolic dysregulation: Tyrosine,\nglycylglycine, glutamine, lysophosphatic acid C18:2 and platelet activating\nfactor C16:0. The metabolite network for APOE $\\epsilon$4 negative AD patients\nwas less cohesive compared to the network for APOE $\\epsilon$4 positive AD\npatients.\n  Discussion: Multiple signatures point to various promising peripheral markers\nfor further validation. The network differences in AD patients according to\nAPOE genotype may reflect different pathways to AD.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 12:47:43 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["de Leeuw", "Francisca A.", ""], ["Peeters", "Carel F. W.", ""], ["Kester", "Maartje I.", ""], ["Harms", "Amy C.", ""], ["Struys", "Eduard A.", ""], ["Hankemeier", "Thomas", ""], ["van Vlijmen", "Herman W. T.", ""], ["van der Lee", "Sven J.", ""], ["van Duijn", "Cornelia M.", ""], ["Scheltens", "Philip", ""], ["Demirkan", "Ay\u015fe", ""], ["van de Wiel", "Mark A.", ""], ["van der Flier", "Wiesje M.", ""], ["Teunissen", "Charlotte E.", ""]]}, {"id": "1709.07302", "submitter": "Richard La", "authors": "Richard J. La", "title": "Influence of Clustering on Cascading Failures in Interdependent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the influence of clustering, more specifically triangles, on\ncascading failures in interdependent networks or systems, in which we model the\ndependence between comprising systems using a dependence graph. First, we\npropose a new model that captures how the presence of triangles in the\ndependence graph alters the manner in which failures transmit from affected\nsystems to others. Unlike existing models, the new model allows us to\napproximate the failure propagation dynamics using a multi-type branching\nprocess, even with triangles. Second, making use of the model, we provide a\nsimple condition that indicates how increasing clustering will affect the\nlikelihood that a random failure triggers a cascade of failures, which we call\nthe probability of cascading failures (PoCF). In particular, our condition\nreveals an intriguing observation that the influence of clustering on PoCF\ndepends on the vulnerability of comprising systems to an increasing number of\nfailed neighboring systems and the current PoCF, starting with different types\nof failed systems. Our numerical studies hint that increasing clustering\nimpedes cascading failures under both (truncated) power law and Poisson degree\ndistributions. Furthermore, our finding suggests that, as the degree\ndistribution becomes more concentrated around the mean degree with smaller\nvariance, increasing clustering will have greater impact on the PoCF. A\nnumerical investigation of networks with Poisson and power law degree\ndistributions reflects this finding and demonstrates that increasing clustering\nreduces the PoCF much faster under Poisson degree distributions in comparison\nto power law degree distributions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 13:12:33 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 18:24:54 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["La", "Richard J.", ""]]}, {"id": "1709.07381", "submitter": "Bashar Ahmad", "authors": "Bashar I. Ahmad, Patrick M. Langdon, Simon J. Godsill, Mauricio\n  Delgado and Thomas Popham", "title": "If and When a Driver or Passenger is Returning to Vehicle: Framework to\n  Infer Intent and Arrival Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a probabilistic framework for the sequential estimation\nof the likelihood of a driver or passenger(s) returning to the vehicle and time\nof arrival, from the available partial track of the user location. The latter\ncan be provided by a smartphone navigational service and/or other dedicated\n(e.g. RF based) user-to-vehicle positioning solution. The introduced novel\napproach treats the tackled problem as an intent prediction task within a\nBayesian formulation, leading to an efficient implementation of the inference\nroutine with notably low training requirements. It effectively captures the\nlong term dependencies in the trajectory followed by the driver/passenger to\nthe vehicle, as dictated by intent, via a bridging distribution. Two examples\nare shown to demonstrate the efficacy of this flexible low-complexity\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 15:43:23 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ahmad", "Bashar I.", ""], ["Langdon", "Patrick M.", ""], ["Godsill", "Simon J.", ""], ["Delgado", "Mauricio", ""], ["Popham", "Thomas", ""]]}, {"id": "1709.07471", "submitter": "Robert Cox", "authors": "Robert W Cox and Paul A Taylor", "title": "Stability of Spatial Smoothness and Cluster-Size Threshold Estimates in\n  FMRI using AFNI", "comments": "4 figures, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent analysis of FMRI datasets [K Mueller et al, Front Hum Neurosci\n11:345], the estimated spatial smoothness parameters and the statistical\nsignificance of clusters were found to depend strongly on the resampled voxel\nsize (for the same data, over a range of 1 to 3 mm) in one popular FMRI\nanalysis software package (SPM12). High sensitivity of thresholding results on\nsuch an arbitrary parameter as final spatial grid size is an undesirable\nfeature in a processing pipeline. Here, we examine the stability of spatial\nsmoothness and cluster-volume threshold estimates with respect to voxel\nresampling size in the AFNI software package's pipeline. A publicly available\ncollection of resting-state and task FMRI datasets from 78 subjects was\nanalyzed using standard processing steps in AFNI. We found that the spatial\nsmoothness and cluster-volume thresholds are fairly stable over the voxel\nresampling size range of 1 to 3 mm, in contradistinction to the reported\nresults from SPM12.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 18:23:37 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Cox", "Robert W", ""], ["Taylor", "Paul A", ""]]}, {"id": "1709.07524", "submitter": "Lendie Follett", "authors": "Lendie Follett and Cindy Yu", "title": "Achieving Parsimony in Bayesian VARs with the Horseshoe Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a vector autoregression (VAR) model, or any multivariate\nregression model, the number of relevant predictors may be small relative to\nthe information set available from which to build a prediction equation. It is\nwell known that forecasts based off of (un-penalized) least squares estimates\ncan overfit the data and lead to poor predictions. Since the Minnesota prior\nwas proposed (Doan et al. (1984)), there have been many methods developed\naiming at improving prediction performance. In this paper we propose the\nhorseshoe prior (Carvalho et al. (2010), Carvalho et al. (2009)) in the context\nof a Bayesian VAR. The horseshoe prior is a unique shrinkage prior scheme in\nthat it shrinks irrelevant signals rigorously to 0 while allowing large signals\nto remain large and practically unshrunk. In an empirical study, we show that\nthe horseshoe prior competes favorably with shrinkage schemes commonly used in\nBayesian VAR models as well as with a prior that imposes true sparsity in the\ncoefficient vector. Additionally, we propose the use of particle Gibbs with\nbackwards simulation (Lindsten et al. (2012), Andrieu et al. (2010)) for the\nestimation of the time-varying volatility parameters. We provide a detailed\ndescription of all MCMC methods used in the supplementary material that is\navailable online.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 21:46:52 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Follett", "Lendie", ""], ["Yu", "Cindy", ""]]}, {"id": "1709.07637", "submitter": "Bruno Sudret", "authors": "I. Abdallah, C. Lataniotis and B. Sudret", "title": "Hierarchical Kriging for multi-fidelity aero-servo-elastic simulators -\n  Application to extreme loads on wind turbines", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-011", "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we consider multi-fidelity surrogate modelling to fuse\nthe output of multiple aero-servo-elastic computer simulators of varying\ncomplexity. In many instances, predictions from multiple simulators for the\nsame quantity of interest on a wind turbine are available. In this type of\nsituation, there is strong evidence that fusing the output from multiple\naero-servo-elastic simulators yields better predictive ability and lower model\nuncertainty than using any single simulator. Hierarchical Kriging is a\nmulti-fidelity surrogate modelling method in which the Kriging surrogate model\nof the cheap (low-fidelity) simulator is used as a trend of the Kriging\nsurrogate model of the higher fidelity simulator. We propose a parametric\napproach to Hierarchical Kriging where the best surrogate models are selected\nbased on evaluating all possible combinations of the available Kriging\nparameters candidates. The parametric Hierarchical Kriging approach is\nillustrated by fusing the extreme flapwise bending moment at the blade root of\na large multi-megawatt wind turbine as a function of wind velocity, turbulence\nand wind shear exponent in the presence of model uncertainty and\nheterogeneously noisy output. The extreme responses are obtained by two widely\naccepted wind turbine specific aero-servo-elastic computer simulators, FAST and\nBladed. With limited high-fidelity simulations, Hierarchical Kriging produces\nmore accurate predictions of validation data compared to conventional Kriging.\nIn addition, contrary to conventional Kriging, Hierarchical Kriging is shown to\nbe a robust surrogate modelling technique because it is less sensitive to the\nchoice of the Kriging parameters and the choice of the estimation error.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 08:45:15 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Abdallah", "I.", ""], ["Lataniotis", "C.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.07662", "submitter": "Tom Reynkens", "authors": "Jan Beirlant, Andrzej Kijko, Tom Reynkens and John H.J. Einmahl", "title": "Estimating the maximum possible earthquake magnitude using extreme value\n  methodology: the Groningen case", "comments": null, "journal-ref": "Natural Hazards (2018)", "doi": "10.1007/s11069-017-3162-2", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area-characteristic, maximum possible earthquake magnitude $T_M$ is\nrequired by the earthquake engineering community, disaster management agencies\nand the insurance industry. The Gutenberg-Richter law predicts that earthquake\nmagnitudes $M$ follow a truncated exponential distribution. In the geophysical\nliterature several estimation procedures were proposed, see for instance Kijko\nand Singh (Acta Geophys., 2011) and the references therein. Estimation of $T_M$\nis of course an extreme value problem to which the classical methods for\nendpoint estimation could be applied. We argue that recent methods on truncated\ntails at high levels (Beirlant et al., Extremes, 2016; Electron. J. Stat.,\n2017) constitute a more appropriate setting for this estimation problem. We\npresent upper confidence bounds to quantify uncertainty of the point estimates.\nWe also compare methods from the extreme value and geophysical literature\nthrough simulations. Finally, the different methods are applied to the\nmagnitude data for the earthquakes induced by gas extraction in the Groningen\nprovince of the Netherlands.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 09:51:25 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 15:15:23 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Beirlant", "Jan", ""], ["Kijko", "Andrzej", ""], ["Reynkens", "Tom", ""], ["Einmahl", "John H. J.", ""]]}, {"id": "1709.07716", "submitter": "M.I. Borrajo", "authors": "M.I. Borrajo, W. Gonz\\'alez-Manteiga, M.D. Mart\\'inez-Miranda", "title": "Testing first-order intensity model in non-homogeneous Poisson point\n  processes with covariates", "comments": "30 pages (23 main doc + 7 appendix); 9 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the first-order intensity function is one of the main aims in point\nprocess theory, and it has been approached so far from different perspectives.\nOne appealing model describes the intensity as a function of a spatial\ncovariate. In the recent literature, estimation theory and several applications\nhave been developed assuming this model, but without formally checking this\nassumption. In this paper we address this problem for a non-homogeneous Poisson\npoint process, by proposing a new test based on an $L^2$-distance. We also\nprove the asymptotic normality of the statistic and we suggest a bootstrap\nprocedure to accomplish the calibration. Two applications with real data are\npresented and a simulation study to better understand the performance of our\nproposals is accomplished. Finally some possible extensions of the present work\nto non-Poisson processes and to a multi-dimensional covariate context are\ndetailed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 12:36:43 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 12:11:43 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Borrajo", "M. I.", ""], ["Gonz\u00e1lez-Manteiga", "W.", ""], ["Mart\u00ednez-Miranda", "M. D.", ""]]}, {"id": "1709.07798", "submitter": "Zhigang Li", "authors": "Zhigang Li, Katherine Lee, Margaret R. Karagas, Juliette C. Madan,\n  Anne G. Hoen, A. James O'Malley, Hongzhe Li", "title": "Conditional regression based on a multivariate zero-inflated logistic\n  normal model for microbiome relative abundance data", "comments": "Corresponding contact: Zhigang.Li@dartmouth.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human microbiome plays critical roles in human health and has been linked\nto many diseases. While advanced sequencing technologies can characterize the\ncomposition of the microbiome in unprecedented detail, it remains challenging\nto disentangle the complex interplay between human microbiome and disease risk\nfactors due to the complicated nature of microbiome data. Excessive numbers of\nzero values, high dimensionality, the hierarchical phylogenetic tree and\ncompositional structure are compounded and consequently make existing methods\ninadequate to appropriately address these issues. We propose a multivariate\ntwo-part zero-inflated logistic normal (MZILN) model to analyze the association\nof disease risk factors with individual microbial taxa and overall microbial\ncommunity composition. This approach can naturally handle excessive numbers of\nzeros and the compositional data structure with the discrete part and the\nlogistic-normal part of the model. For parameter estimation, an estimating\nequations approach is employed that enables us to address the complex\ninter-taxa correlation structure induced by the hierarchical phylogenetic tree\nstructure and the compositional data structure. This model is able to\nincorporate standard regularization approaches to deal with high\ndimensionality. Simulation shows that our model outperforms existing methods.\nOur approach is also compared to others using the analysis of real microbiome\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:58:02 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 04:00:44 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Li", "Zhigang", ""], ["Lee", "Katherine", ""], ["Karagas", "Margaret R.", ""], ["Madan", "Juliette C.", ""], ["Hoen", "Anne G.", ""], ["O'Malley", "A. James", ""], ["Li", "Hongzhe", ""]]}, {"id": "1709.07915", "submitter": "Amir Karami", "authors": "George Shaw Jr., and Amir Karami", "title": "Computational Content Analysis of Negative Tweets for Obesity, Diet,\n  Diabetes, and Exercise", "comments": "The 2017 Annual Meeting of the Association for Information Science\n  and Technology (ASIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media based digital epidemiology has the potential to support faster\nresponse and deeper understanding of public health related threats. This study\nproposes a new framework to analyze unstructured health related textual data\nvia Twitter users' post (tweets) to characterize the negative health sentiments\nand non-health related concerns in relations to the corpus of negative\nsentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the\ncollection of 6 million Tweets for one month, this study identified the\nprominent topics of users as it relates to the negative sentiments. Our\nproposed framework uses two text mining methods, sentiment analysis and topic\nmodeling, to discover negative topics. The negative sentiments of Twitter users\nsupport the literature narratives and the many morbidity issues that are\nassociated with DDEO and the linkage between obesity and diabetes. The\nframework offers a potential method to understand the publics' opinions and\nsentiments regarding DDEO. More importantly, this research provides new\nopportunities for computational social scientists, medical experts, and public\nhealth professionals to collectively address DDEO-related issues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:18:42 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Shaw", "George", "Jr."], ["Karami", "Amir", ""]]}, {"id": "1709.07916", "submitter": "Amir Karami", "authors": "Amir Karami, Alicia A. Dahl, Gabrielle Turner-McGrievy, Hadi Kharrazi,\n  Jr., George Shaw", "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "comments": "International Journal of Information Management (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media provide a platform for users to express their opinions and share\ninformation. Understanding public health opinions on social media, such as\nTwitter, offers a unique approach to characterizing common health issues such\nas diabetes, diet, exercise, and obesity (DDEO), however, collecting and\nanalyzing a large scale conversational public health data set is a challenging\nresearch task. The goal of this research is to analyze the characteristics of\nthe general public's opinions in regard to diabetes, diet, exercise and obesity\n(DDEO) as expressed on Twitter. A multi-component semantic and linguistic\nframework was developed to collect Twitter data, discover topics of interest\nabout DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8%\nof tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity.\nThe strongest correlation among the topics was determined between exercise and\nobesity. Other notable correlations were: diabetes and obesity, and diet and\nobesity DDEO terms were also identified as subtopics of each of the DDEO\ntopics. The frequent subtopics discussed along with Diabetes, excluding the\nDDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer.\nThe non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities,\nweight loss, religious, and mental health, while subtopics for Exercise\nincluded computer games, brain, fitness, and daily plan. Non-DDEO subtopics for\nObesity included Alzheimer, cancer, and children. With 2.67 billion social\nmedia users in 2016, publicly available data such as Twitter posts can be\nutilized to support clinical providers, public health experts, and social\nscientists in better understanding common public opinions in regard to\ndiabetes, diet, exercise, and obesity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:19:49 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Karami", "Amir", ""], ["Dahl", "Alicia A.", ""], ["Turner-McGrievy", "Gabrielle", ""], ["Kharrazi,", "Hadi", "Jr."], ["Shaw", "George", ""]]}, {"id": "1709.08036", "submitter": "Guillaume Basse", "authors": "Guillaume Basse, Avi Feller, Panos Toulis", "title": "Conditional randomization tests of causal effects with interference\n  between units", "comments": "Accepted for publication in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many causal questions involve interactions between units, also known as\ninterference, for example between individuals in households, students in\nschools, or firms in markets. In this paper, we formalize the concept of a\nconditioning mechanism, which provides a framework for constructing valid and\npowerful randomization tests under general forms of interference. We describe\nour framework in the context of two-stage randomized designs and apply our\napproach to a randomized evaluation of an intervention targeting student\nabsenteeism in the School District of Philadelphia. We show improvements over\nexisting methods in terms of computational and statistical power.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 11:22:10 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 16:48:03 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 04:17:00 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Basse", "Guillaume", ""], ["Feller", "Avi", ""], ["Toulis", "Panos", ""]]}, {"id": "1709.08221", "submitter": "Mark Steel", "authors": "Mark F.J. Steel", "title": "Model Averaging and its Use in Economics", "comments": "forthcoming; accepted version", "journal-ref": "Journal of Economic Literature, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of model averaging has become an important tool to deal with model\nuncertainty, for example in situations where a large amount of different\ntheories exist, as are common in economics. Model averaging is a natural and\nformal response to model uncertainty in a Bayesian framework, and most of the\npaper deals with Bayesian model averaging. The important role of the prior\nassumptions in these Bayesian procedures is highlighted. In addition,\nfrequentist model averaging methods are also discussed. Numerical methods to\nimplement these methods are explained, and I point the reader to some freely\navailable computational resources. The main focus is on uncertainty regarding\nthe choice of covariates in normal linear regression models, but the paper also\ncovers other, more challenging, settings, with particular emphasis on sampling\nmodels commonly used in economics. Applications of model averaging in economics\nare reviewed and discussed in a wide range of areas, among which growth\neconomics, production modelling, finance and forecasting macroeconomic\nquantities.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 16:49:05 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 09:29:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 17:33:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Steel", "Mark F. J.", ""]]}, {"id": "1709.08238", "submitter": "Martin Gould", "authors": "Martin D. Gould, Nikolaus Hautsch, Sam D. Howison, and Mason A. Porter", "title": "Counterparty Credit Limits: The Impact of a Risk-Mitigation Measure on\n  Everyday Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR econ.EM math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 18:53:41 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 20:45:13 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 19:35:56 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gould", "Martin D.", ""], ["Hautsch", "Nikolaus", ""], ["Howison", "Sam D.", ""], ["Porter", "Mason A.", ""]]}, {"id": "1709.08371", "submitter": "Kento Kawakami", "authors": "Kento Kawakami, Masato Kikuchi, Mitsuo Yoshida, Eiko Yamamoto, Kyoji\n  Umemura", "title": "Finding Association Rules by Direct Estimation of Likelihood Ratios", "comments": "The 2017 International Conference On Advanced Informatics: Concepts,\n  Theory And Application (ICAICTA2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a cost function that corresponds to the mean square\nerrors between estimated values and true values of conditional probability in a\ndiscrete distribution. We then obtain the values that minimize the cost\nfunction. This minimization approach can be regarded as the direct estimation\nof likelihood ratios because the estimation of conditional probability can be\nregarded as the estimation of likelihood ratio by the definition of conditional\nprobability. When we use the estimated value as the strength of association\nrules for data mining, we find that it outperforms a well-used method called\nApriori.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:35:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Kawakami", "Kento", ""], ["Kikuchi", "Masato", ""], ["Yoshida", "Mitsuo", ""], ["Yamamoto", "Eiko", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1709.08516", "submitter": "Marcus Cordi", "authors": "Marcus Cordi, Damien Challet, Ioane Muni Toke", "title": "Testing the causality of Hawkes processes with time reversal", "comments": "13 pages, 14 figures, 2 tables", "journal-ref": null, "doi": "10.1088/1742-5468/aaac3f", "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that univariate and symmetric multivariate Hawkes processes are only\nweakly causal: the true log-likelihoods of real and reversed event time vectors\nare almost equal, thus parameter estimation via maximum likelihood only weakly\ndepends on the direction of the arrow of time. In ideal (synthetic) conditions,\ntests of goodness of parametric fit unambiguously reject backward event times,\nwhich implies that inferring kernels from time-symmetric quantities, such as\nthe autocovariance of the event rate, only rarely produce statistically\nsignificant fits. Finally, we find that fitting financial data with\nmany-parameter kernels may yield significant fits for both arrows of time for\nthe same event time vector, sometimes favouring the backward time direction.\nThis goes to show that a significant fit of Hawkes processes to real data with\nflexible kernels does not imply a definite arrow of time unless one tests it.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:33:44 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Cordi", "Marcus", ""], ["Challet", "Damien", ""], ["Toke", "Ioane Muni", ""]]}, {"id": "1709.08608", "submitter": "Pierre  Barbillon", "authors": "Jordi Ferrer Savall, Damien Franqueville, Pierre Barbillon, Cyril\n  Benhamou, Patrick Durand, Marie-Luce Taupin, Herv\\'e Monod and Jean-Louis\n  Drouet", "title": "Sensitivity analysis of spatio-temporal models describing nitrogen\n  transfers, transformations and losses at the landscape scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling complex systems such as agroecosystems often requires the\nquantification of a large number of input factors. Sensitivity analyses are\nuseful to determine the appropriate spatial and temporal resolution of models\nand to reduce the number of factors to be measured or estimated accurately.\nComprehensive spatial and temporal sensitivity analyses were applied to the\nNitroScape model, a deterministic spatially distributed model describing\nnitrogen transfers and transformations in rural landscapes. Simulations were\nled on a theoretical landscape that represented five years of intensive farm\nmanagement and covering an area of $3\\, km^2$. Cluster analyses were applied to\nsummarize the results of the sensitivity analysis on the ensemble of model\noutputs. The methodology we applied is useful to synthesize sensitivity\nanalyses of models with multiple space-time input and output variables and\ncould be ported to other models than NitroScape.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 17:24:43 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 16:29:40 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Savall", "Jordi Ferrer", ""], ["Franqueville", "Damien", ""], ["Barbillon", "Pierre", ""], ["Benhamou", "Cyril", ""], ["Durand", "Patrick", ""], ["Taupin", "Marie-Luce", ""], ["Monod", "Herv\u00e9", ""], ["Drouet", "Jean-Louis", ""]]}, {"id": "1709.08626", "submitter": "Bruno Sudret", "authors": "E. Torre, S. Marelli, P. Embrechts, B. Sudret", "title": "A general framework for data-driven uncertainty quantification under\n  complex input dependencies using vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-012", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems subject to uncertain inputs produce uncertain responses. Uncertainty\nquantification (UQ) deals with the estimation of statistics of the system\nresponse, given a computational model of the system and a probabilistic model\nof its inputs. In engineering applications it is common to assume that the\ninputs are mutually independent or coupled by a Gaussian or elliptical\ndependence structure (copula). In this paper we overcome such limitations by\nmodelling the dependence structure of multivariate inputs as vine copulas. Vine\ncopulas are models of multivariate dependence built from simpler pair-copulas.\nThe vine representation is flexible enough to capture complex dependencies.\nThis paper formalises the framework needed to build vine copula models of\nmultivariate inputs and to combine them with virtually any UQ method. The\nframework allows for a fully automated, data-driven inference of the\nprobabilistic input model on available input data. The procedure is exemplified\non two finite element models of truss structures, both subject to inputs with\nnon-Gaussian dependence structures. For each case, we analyse the moments of\nthe model response (using polynomial chaos expansions), and perform a\nstructural reliability analysis to calculate the probability of failure of the\nsystem (using the first order reliability method and importance sampling).\nReference solutions are obtained by Monte Carlo simulation. The results show\nthat, while the Gaussian assumption yields biased statistics, the vine copula\nrepresentation achieves significantly more precise estimates, even when its\nstructure needs to be fully inferred from a limited amount of observations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:17:21 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 12:38:20 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Torre", "E.", ""], ["Marelli", "S.", ""], ["Embrechts", "P.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.08684", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "Identification of Causalities in Spatio-temporal Data", "comments": "15 pages, 4 figures. Forthcoming in Proceedings of SAGEO2017\n  Conference. Translated from French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the understanding of strongly coupled\nspatio-temporal processes by describing a generic method based on Granger\ncausality. The method is validated by the robust identification of causality\nregimes and of their phase diagram for an urban morphogenesis model that\ncouples network growth with density. The application to the real case study of\nGreater Paris transportation projects shows a link between territorial\ndynamics, more particularly of real estate and socio-economic, and the\nanticipated network growth. We finally discuss potential extensions to other\ntemporal and spatial scales.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 19:12:48 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Raimbault", "Juste", ""]]}, {"id": "1709.08776", "submitter": "Tony Wong", "authors": "Tony E. Wong, Alexandra Klufas, Vivek Srikrishnan, Klaus Keller", "title": "Neglecting Model Structural Uncertainty Underestimates Upper Tails of\n  Flood Hazard", "comments": null, "journal-ref": null, "doi": "10.1088/1748-9326/aacb3d", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coastal flooding drives considerable risks to many communities, but\nprojections of future flood risks are deeply uncertain. The paucity of\nobservations of extreme events often motivates the use of statistical\napproaches to model the distribution of extreme storm surge events. A key deep\nuncertainty that is often overlooked is model structural uncertainty. There is\ncurrently no strong consensus among experts regarding which class of\nstatistical model to use as a best practice. Robust management of coastal\nflooding risks requires coastal managers to consider the distinct possibility\nof non-stationarity in storm surges. This increases the complexity of the\npotential models to use, which tends to increase the data required to constrain\nthe model. Here, we use a Bayesian model averaging approach to analyze the\nbalance between model complexity sufficient to capture decision-relevant risks\nand data availability to constrain complex model structures. We characterize\ndeep model structural uncertainty through a set of calibration experiments.\nSpecifically, we calibrate a set of models ranging in complexity using\nlong-term tide gauge observations from the Netherlands and the United States.\nWe find that in both cases, roughly half the model weight is associated with\nnon-stationary models. Our approach provides a formal framework to integrate\ninformation across model structures, in light of the potentially sizable\nmodeling uncertainties. By combining information from multiple models, our\ninference sharpens for the projected storm surge 100-year return levels, and\nestimated return levels increase by several centimeters. We assess the impacts\nof data availability through a set of experiments with temporal subsets and\nmodel comparison metrics. Our analysis suggests about 70 years of data are\nrequired to stabilize estimates of the 100-year return level, for the locations\nand methods considered here.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:18:22 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 01:37:02 GMT"}, {"version": "v3", "created": "Sat, 3 Feb 2018 22:30:29 GMT"}, {"version": "v4", "created": "Sun, 3 Jun 2018 14:20:45 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Wong", "Tony E.", ""], ["Klufas", "Alexandra", ""], ["Srikrishnan", "Vivek", ""], ["Keller", "Klaus", ""]]}, {"id": "1709.08862", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela", "title": "Bayesian Inference of Spreading Processes on Networks", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2018.0129", "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases are studied to understand their spreading mechanisms, to\nevaluate control strategies and to predict the risk and course of future\noutbreaks. Because people only interact with a small number of individuals, and\nbecause the structure of these interactions matters for spreading processes,\nthe pairwise relationships between individuals in a population can be usefully\nrepresented by a network. Although the underlying processes of transmission are\ndifferent, the network approach can be used to study the spread of pathogens in\na contact network or the spread of rumors in an online social network. We study\nsimulated simple and complex epidemics on synthetic networks and on two\nempirical networks, a social / contact network in an Indian village and an\nonline social network in the U.S. Our goal is to learn simultaneously about the\nspreading process parameters and the source node (first infected node) of the\nepidemic, given a fixed and known network structure, and observations about\nstate of nodes at several points in time. Our inference scheme is based on\napproximate Bayesian computation (ABC), an inference technique for complex\nmodels with likelihood functions that are either expensive to evaluate or\nanalytically intractable. ABC enables us to adopt a Bayesian approach to the\nproblem despite the posterior distribution being very complex. Our method is\nagnostic about the topology of the network and the nature of the spreading\nprocess. It generally performs well and, somewhat counter-intuitively, the\ninference problem appears to be easier on more heterogeneous network\ntopologies, which enhances its future applicability to real-world settings\nwhere few networks have homogeneous topologies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:00:46 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 21:35:41 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 13:55:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1709.09117", "submitter": "Matthew Shum", "authors": "Mogens Fosgerau and Emerson Melo and Andre de Palma and Matthew Shum", "title": "Discrete Choice and Rational Inattention: a General Equivalence Result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a general equivalence between discrete choice and\nrational inattention models. Matejka and McKay (2015, AER) showed that when\ninformation costs are modelled using the Shannon entropy function, the\nresulting choice probabilities in the rational inattention model take the\nmultinomial logit form. By exploiting convex-analytic properties of the\ndiscrete choice model, we show that when information costs are modelled using a\nclass of generalized entropy functions, the choice probabilities in any\nrational inattention model are observationally equivalent to some additive\nrandom utility discrete choice model and vice versa. Thus any additive random\nutility model can be given an interpretation in terms of boundedly rational\nbehavior. This includes empirically relevant specifications such as the probit\nand nested logit models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:31:59 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Fosgerau", "Mogens", ""], ["Melo", "Emerson", ""], ["de Palma", "Andre", ""], ["Shum", "Matthew", ""]]}, {"id": "1709.09150", "submitter": "Leonardo Bastos S", "authors": "Leonardo Bastos, Theodoros Economou, Marcelo Gomes, Daniel Villela,\n  Flavio Coelho, Oswaldo Cruz, Oliver Stoner, Trevor Bailey, Claudia Code\\c{c}o", "title": "Modelling reporting delays for disease surveillance data", "comments": null, "journal-ref": "Statistics in Medicine, 38, 22, 2019, 4363-4377", "doi": "10.1002/sim.8303", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One difficulty for real-time tracking of epidemics is related to reporting\ndelay. The reporting delay may be due to laboratory confirmation, logistic\nproblems, infrastructure difficulties and so on. The ability to correct the\navailable information as quickly as possible is crucial, in terms of decision\nmaking such as issuing warnings to the public and local authorities. A Bayesian\nhierarchical modelling approach is proposed as a flexible way of correcting the\nreporting delays and to quantify the associated uncertainty. Implementation of\nthe model is fast, due to the use of the integrated nested Laplace\napproximation (INLA). The approach is illustrated on dengue fever incidence\ndata in Rio de Janeiro, and Severe Acute Respiratory Illness (SARI) data in\nParan\\'a state, Brazil.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 17:40:16 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 17:46:00 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 22:53:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bastos", "Leonardo", ""], ["Economou", "Theodoros", ""], ["Gomes", "Marcelo", ""], ["Villela", "Daniel", ""], ["Coelho", "Flavio", ""], ["Cruz", "Oswaldo", ""], ["Stoner", "Oliver", ""], ["Bailey", "Trevor", ""], ["Code\u00e7o", "Claudia", ""]]}, {"id": "1709.09268", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Nima Bari, Roman Vichr, Farhad A. Goodarzi", "title": "FSL-BM: Fuzzy Supervised Learning with Binary Meta-Feature for\n  Classification", "comments": "FICC2018", "journal-ref": null, "doi": "10.1007/978-3-030-03405-4_46", "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel real-time Fuzzy Supervised Learning with Binary\nMeta-Feature (FSL-BM) for big data classification task. The study of real-time\nalgorithms addresses several major concerns, which are namely: accuracy, memory\nconsumption, and ability to stretch assumptions and time complexity. Attaining\na fast computational model providing fuzzy logic and supervised learning is one\nof the main challenges in the machine learning. In this research paper, we\npresent FSL-BM algorithm as an efficient solution of supervised learning with\nfuzzy logic processing using binary meta-feature representation using Hamming\nDistance and Hash function to relax assumptions. While many studies focused on\nreducing time complexity and increasing accuracy during the last decade, the\nnovel contribution of this proposed solution comes through integration of\nHamming Distance, Hash function, binary meta-features, binary classification to\nprovide real time supervised method. Hash Tables (HT) component gives a fast\naccess to existing indices; and therefore, the generation of new indices in a\nconstant time complexity, which supersedes existing fuzzy supervised algorithms\nwith better or comparable results. To summarize, the main contribution of this\ntechnique for real-time Fuzzy Supervised Learning is to represent hypothesis\nthrough binary input as meta-feature space and creating the Fuzzy Supervised\nHash table to train and validate model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 21:52:41 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 23:34:10 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kowsari", "Kamran", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Goodarzi", "Farhad A.", ""]]}, {"id": "1709.09298", "submitter": "German A. Schnaidt Grez", "authors": "German A. Schnaidt Grez and Brani Vidakovic", "title": "An Empirical approach to Survival Density Estimation for\n  randomly-censored data using Wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimation is a classical problem in statistics and has received\nconsiderable attention when both the data has been fully observed and in the\ncase of partially observed (censored) samples. In survival analysis or clinical\ntrials, a typical problem encountered in the data collection stage is that the\nsamples may be censored from the right. The variable of interest could be\nobserved partially due to the presence of a set of events that occur at random\nand potentially censor the data. Consequently, developing a methodology that\nenables robust estimation of the lifetimes in such setting is of high interest\nfor researchers.\n  In this paper, we propose a non-parametric linear density estimator using\nempirical wavelet coefficients that are fully data driven. We derive an\nasymptotically unbiased estimator constructed from the complete sample based on\nan inductive bias correction procedure. Also, we provide upper bounds for the\nbias and analyze the large sample behavior of the expected $\\mathbb{L}_{2}$\nestimation error based on the approach used by Stute (1995), showing that the\nestimates are asymptotically normal and possess global mean square consistency.\n  In addition, we evaluate the proposed approach via a theoretical simulation\nstudy using different exemplary baseline distributions with different sample\nsizes. In this study, we choose a censoring scheme that produces a censoring\nproportion of 40\\% on average. Finally, we apply the proposed estimator to real\ndata-sets previously published, showing that the proposed wavelet estimator\nprovides a robust and useful tool for the non-parametric estimation of the\nsurvival time density function.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 01:17:41 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 01:18:39 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Grez", "German A. Schnaidt", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1709.09321", "submitter": "Mikyoung Jun", "authors": "Mikyoung Jun, Courtney Schumacher, R. Saravanan", "title": "Global multivariate point pattern models for rain type occurrence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek statistical methods to study the occurrence of multiple rain types\nobserved by satellite on a global scale. The main scientific interests are to\nrelate rainfall occurrence with various atmospheric state variables and to\nstudy the dependence between the occurrences of multiple types of rainfall\n(e.g. short-lived and intense versus long-lived and weak; the heights of the\nrain clouds are also considered). Commonly in point process model literature,\nthe spatial domain is assumed to be a small, and thus planar domain. We\nconsider the log-Gaussian Cox Process (LGCP) models on the surface of a sphere\nand take advantage of cross-covariance models for spatial processes on a global\nscale to model the stochastic intensity function of the LGCP models. We present\nanalysis results for rainfall observations from the TRMM satellite and\natmospheric state variables from MERRA-2 reanalysis data over the tropical\nEastern and Western Pacific Ocean, as well as over the entire tropical and\nsubtropical ocean regions. Statistical inference is done through Monte Carlo\nlikelihood approximation for LGCP models. We employ covariance approximation to\ndeal with massive data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 03:39:29 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 13:21:59 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Jun", "Mikyoung", ""], ["Schumacher", "Courtney", ""], ["Saravanan", "R.", ""]]}, {"id": "1709.09412", "submitter": "Federico Pascucci", "authors": "F. Pascucci, N. Rinke, C. Schiermeyer, V. Berkhahn, B. Friedrich", "title": "A discrete choice model for solving conflict situations between\n  pedestrians and vehicles in shared space", "comments": "13 pages, 5 tables, 6 figures. This paper will be submitted soon to\n  Transportation Research Part C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When streets are designed according to the shared space principle, road user\nare encouraged to interact spontaneously with each other for negotiating the\nspace. These interaction mechanisms do not follow clearly defined traffic rules\nbut rather psychological and social principles related to aspects of safety,\ncomfort and time pressure. However, these principles are hard to capture and to\nquantify, thus making it difficult to simulate the behavior of road users. This\nwork investigates traffic conflict situations between pedestrians and motorized\nvehicles, with the main objective to formulate a discrete choice model for the\nidentification of the proper conflict solving strategy. A shared space street\nin Hamburg, Germany, with high pedestrian volumes is used as a case study for\nmodel formulation and calibration. Conflict situations are detected by an\nautomatic procedure of trajectory prediction and comparison. Standard evasive\nactions are identified, both for pedestrians and vehicles, by observing\nbehavioral patterns. A set of potential parameters, which may affect the choice\nof the evasive action, is formulated and tested for significance. These include\ngeometrical aspects, like distance and speed of the conflicting users, as well\nas conflict-specific ones, like time to collision. A multinomial logit model is\nfinally calibrated and validated on real situations. The developed approach is\nrealistic and ready for implementation in motion models for shared space or any\nother less organized traffic environment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 09:40:41 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Pascucci", "F.", ""], ["Rinke", "N.", ""], ["Schiermeyer", "C.", ""], ["Berkhahn", "V.", ""], ["Friedrich", "B.", ""]]}, {"id": "1709.09512", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "Simultaneous-equation Estimation without Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a single equation in a system of linear equations, estimation by\ninstrumental variables is the standard approach. In practice, however, it is\noften difficult to find valid instruments. This paper proposes a maximum\nlikelihood method that does not require instrumental variables; it is\nillustrated by simulation and with a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:45:22 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "1709.09583", "submitter": "Stephan Smeekes", "authors": "Lenard Lieb and Stephan Smeekes", "title": "Inference for Impulse Responses under Model Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many macroeconomic applications, confidence intervals for impulse\nresponses are constructed by estimating VAR models in levels - ignoring\ncointegration rank uncertainty. We investigate the consequences of ignoring\nthis uncertainty. We adapt several methods for handling model uncertainty and\nhighlight their shortcomings. We propose a new method -\nWeighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty\ninto account in a data-driven way. In simulations the WIMP outperforms all\nother methods considered, delivering intervals that are robust to rank\nuncertainty, yet not overly conservative. We also study potential ramifications\nof rank uncertainty on applied macroeconomic analysis by re-assessing the\neffects of fiscal policy shocks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:35:17 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 12:55:54 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 11:52:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lieb", "Lenard", ""], ["Smeekes", "Stephan", ""]]}, {"id": "1709.09786", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "An Introduction to Radar Sliding Window Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An introduction to the theory of sliding window detection processes, used as\nalternatives to optimal Neyman-Pearson based radar detectors, is presented.\nIncluded is an outline of their historical development, together with an\nexplanation for the resurgence of interest in such detectors for operation in\nmodern maritime surveillance radar clutter. In particular, recent research has\ndeveloped criteria that enables one to construct such detection processes with\nthe desired constant false alarm rate property for a comprehensive class of\nclutter model. The chapter also includes some examples of the performance of\nsuch detectors in a specific interference environment.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 02:41:04 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1709.09955", "submitter": "M. Merce Claramunt", "authors": "Anna Casta\\~ner (UB), M Merc\\`e Claramunt (UB)", "title": "Equilibrium distributions and discrete Schur-constant models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Schur-constant equilibrium distribution models of\ndimension n for arithmetic non-negative random variables. Such a model is\ndefined through the (several orders) equilibrium distributions of a univariate\nsurvival function. First, the bivariate case is considered and analyzed in\ndepth, stressing the main characteristics of the Poisson case. The analysis is\nthen extended to the multivariate case. Several properties are derived,\nincluding the implicit correlation and the distribution of the sum.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 13:45:12 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Casta\u00f1er", "Anna", "", "UB"], ["Claramunt", "M Merc\u00e8", "", "UB"]]}, {"id": "1709.10074", "submitter": "Maria Montez-Rath", "authors": "Maria E. Montez-Rath, Kristopher Kapphahn, Maya B. Mathur, Natasha\n  Purington, Vilija R. Joyce and Manisha Desai", "title": "Simulating realistically complex comparative effectiveness studies with\n  time-varying covariates and right-censored outcomes", "comments": "25 pages, 6 tables, 6 figures (2 supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies are useful for evaluating and developing statistical\nmethods for the analyses of complex problems. Performance of methods may be\naffected by multiple complexities present in real scenarios. Generating\nsufficiently realistic data for this purpose, however, can be challenging. Our\nstudy of the comparative effectiveness of HIV protocols on the risk of\ncardiovascular disease -- involving the longitudinal assessment of HIV patients\n-- is such an example. The correlation structure across covariates and within\nsubjects over time must be considered as well as right-censoring of the outcome\nof interest, time to myocardial infarction. A challenge in simulating the\ncovariates is to incorporate a joint distribution for variables of mixed type\n-- continuous, binary or polytomous. An additional challenge is incorporating\nwithin-subject correlation where some variables may vary over time and others\nmay remain static. To address these issues, we extend the work of Demirtas and\nDoganay (2012). Identifying a model from which to simulate the right-censored\noutcome as a function of these covariates builds on work developed by Sylvestre\nand Abrahamowicz (2007). In this paper, we describe a cohesive and\nuser-friendly approach accompanied by R code to simulate comparative\neffectiveness studies with right-censored outcomes that are functions of\ntime-varying covariates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 17:29:50 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Montez-Rath", "Maria E.", ""], ["Kapphahn", "Kristopher", ""], ["Mathur", "Maya B.", ""], ["Purington", "Natasha", ""], ["Joyce", "Vilija R.", ""], ["Desai", "Manisha", ""]]}, {"id": "1709.10298", "submitter": "Nadim Ballout", "authors": "Nadim Ballout, Vivian Viallon", "title": "Structure estimation of binary graphical models on stratified data:\n  application to the description of injury tables for victims of road accidents", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are used in many applications such as medical diagnostic,\ncomputer security, etc. More and more often, the estimation of such models has\nto be performed on several predefined strata of the whole population. For\ninstance, in epidemiology and clinical research, strata are often defined\naccording to age, gender, treatment or disease type, etc. In this article, we\npropose new approaches aimed at estimating binary graphical models on such\nstrata. Our approaches are obtained by combining well-known methods when\nestimating one single binary graphical model, with penalties encouraging\nstructured sparsity, and which have recently been shown appropriate when\ndealing with stratified data. Empirical comparions on synthetic data highlight\nthat our approaches generally outperform the competitors we considered. An\napplication is provided where we study associations among injuries suffered by\nvictims of road accidents according to road user type.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:27:18 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Ballout", "Nadim", ""], ["Viallon", "Vivian", ""]]}, {"id": "1709.10335", "submitter": "Xiatong Cai", "authors": "Xiatong Cai (1), Guangpeng Pei (1 and 2), Yuen Zhu (1), Donggang Guo\n  (1), Hua Li (1) ((1) School of Environment Science and Resources, Shanxi\n  University, Taiyuan, China, (2) Institute of Resources and Environment\n  Engineering, Shanxi University, Taiyuan, China.)", "title": "Redefine the correlation coefficient by experiment methods", "comments": "19pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the establishment of global biological monitor network and development\nof remote sensing technology, data won't be a limitation, but the variance\nbrought by spatial heterogeneous and fractal will influence correlation\ncoefficient significantly with the enlarged sample scale. Those impede us to\nfind more intrinsic principle in ecology. Ecology is based on experiment, and\nthe experiment methods won't change with spatial. In that condition, if we\nconstruct a system to evaluate the experimental difference, that may benefit\nthe study in spatial ecology. In that condition, we give a synthesis discussion\nfrom the concept of data, experiment to analysis methods,and revise as well as\ndevelop correlation and regression method to make it suit the demand of spatial\nanalysis. An experimental correlation system is established in this report.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:24:12 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Cai", "Xiatong", "", "1 and 2"], ["Pei", "Guangpeng", "", "1 and 2"], ["Zhu", "Yuen", ""], ["Guo", "Donggang", ""], ["Li", "Hua", ""]]}, {"id": "1709.10401", "submitter": "Thakshila Wimalajeewa", "authors": "Thakshila Wimalajeewa and Pramod K. Varshney", "title": "Application of Compressive Sensing Techniques in Distributed Sensor\n  Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey paper, our goal is to discuss recent advances of compressive\nsensing (CS) based solutions in wireless sensor networks (WSNs) including the\nmain ongoing/recent research efforts, challenges and research trends in this\narea. In WSNs, CS based techniques are well motivated by not only the sparsity\nprior observed in different forms but also by the requirement of efficient\nin-network processing in terms of transmit power and communication bandwidth\neven with nonsparse signals. In order to apply CS in a variety of WSN\napplications efficiently, there are several factors to be considered beyond the\nstandard CS framework. We start the discussion with a brief introduction to the\ntheory of CS and then describe the motivational factors behind the potential\nuse of CS in WSN applications. Then, we identify three main areas along which\nthe standard CS framework is extended so that CS can be efficiently applied to\nsolve a variety of problems specific to WSNs. In particular, we emphasize on\nthe significance of extending the CS framework to (i). take communication\nconstraints into account while designing projection matrices and reconstruction\nalgorithms for signal reconstruction in centralized as well in decentralized\nsettings, (ii) solve a variety of inference problems such as detection,\nclassification and parameter estimation, with compressed data without signal\nreconstruction and (iii) take practical communication aspects such as\nmeasurement quantization, physical layer secrecy constraints, and imperfect\nchannel conditions into account. Finally, open research issues and challenges\nare discussed in order to provide perspectives for future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 03:19:33 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 23:30:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}]