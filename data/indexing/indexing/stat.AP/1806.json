[{"id": "1806.00034", "submitter": "Xiaoyue Niu", "authors": "Xiaoyue Niu and James L. Rosenberger", "title": "Near-Balanced Incomplete Block Designs with An Application to Poster\n  Competitions", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judging scholarly posters creates a challenge to assign the judges\nefficiently. If there are many posters and few reviews per judge, the commonly\nused Balanced Incomplete Block Design is not a feasible option. An additional\nchallenge is an unknown number of judges before the event. We propose two\nconnected near-balanced incomplete block designs that both satisfy the\nrequirements of our setting: one that generates a connected assignment and\nbalances the treatments and another one that further balances pairs of\ntreatments. We describe both fixed and random effects models to estimate the\npopulation marginal means of the poster scores and rationalize the use of the\nrandom effects model. We evaluate the estimation accuracy and efficiency,\nespecially the winning chance of the truly best posters, of the two designs in\ncomparison with a random assignment via simulation studies. The two proposed\ndesigns both demonstrate accuracy and efficiency gains over the random\nassignment.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:14:20 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Niu", "Xiaoyue", ""], ["Rosenberger", "James L.", ""]]}, {"id": "1806.00204", "submitter": "Anil Gore Dr.", "authors": "Sharayu Paranjpe, Anil Gore", "title": "Adversity Index for Clinical Trials: An Inclusive Approach for Analysis\n  of Safety Data", "comments": "25 pages, 3 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for analysis of adverse event data in\nclinical trials. The method is illustrated by application to data on 4 phase\nIII clinical trials, two on breast cancer and two on diabetes mellitus.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 05:50:51 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Paranjpe", "Sharayu", ""], ["Gore", "Anil", ""]]}, {"id": "1806.00225", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Ernst Wit", "title": "Model-based clustering for populations of networks", "comments": "The final (published) version of the article can be downloaded for\n  free (Open Access) from the editor's website (click on the DOI link below)", "journal-ref": "Statistical Modelling, 2020, 20 (1), 9-29", "doi": "10.1177/1471082X19871128", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently obtaining data on populations of networks was typically rare.\nHowever, with the advancement of automatic monitoring devices and the growing\nsocial and scientific interest in networks, such data has become more widely\navailable. From sociological experiments involving cognitive social structures\nto fMRI scans revealing large-scale brain networks of groups of patients, there\nis a growing awareness that we urgently need tools to analyse populations of\nnetworks and particularly to model the variation between networks due to\ncovariates. We propose a model-based clustering method based on mixtures of\ngeneralized linear (mixed) models that can be employed to describe the joint\ndistribution of a populations of networks in a parsimonious manner and to\nidentify subpopulations of networks that share certain topological properties\nof interest (degree distribution, community structure, effect of covariates on\nthe presence of an edge, etc.). Maximum likelihood estimation for the proposed\nmodel can be efficiently carried out with an implementation of the EM\nalgorithm. We assess the performance of this method on simulated data and\nconclude with an example application on advice networks in a small business.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 07:48:28 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 13:23:55 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 10:13:21 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 12:00:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Signorelli", "Mirko", ""], ["Wit", "Ernst", ""]]}, {"id": "1806.00385", "submitter": "Mohamed-Salem Ahmed", "authors": "Mohamed-Salem Ahmed, Mamadou N'diaye, Mohammed Kadi Attouch, Sophie\n  Dabo-Niang", "title": "k-nearest neighbors prediction and classification for spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric predictor and a supervised classification based on\nthe regression function estimate of a spatial real variable using k-nearest\nneighbors method (k-NN). Under some assumptions, we establish almost complete\nor sure convergence of the proposed estimates which incorporate a spatial\nproximity between observations. Numerical results on simulated and real fish\ndata illustrate the behavior of the given predictor and classification method.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 15:07:08 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ahmed", "Mohamed-Salem", ""], ["N'diaye", "Mamadou", ""], ["Attouch", "Mohammed Kadi", ""], ["Dabo-Niang", "Sophie", ""]]}, {"id": "1806.00665", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Estimating Local Daytime Population Density from Census and Payroll Data", "comments": "Regional Studies, Regional Science, 2018", "journal-ref": null, "doi": "10.1080/21681376.2018.1455535", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daytime population density reflects where people commute and spend their\nwaking hours. It carries significant weight as urban planners and engineers\nsite transportation infrastructure and utilities, plan for disaster recovery,\nand assess urban vitality. Various methods with various drawbacks exist to\nestimate daytime population density across a metropolitan area, such as using\ncensus data, travel diaries, GPS traces, or publicly available payroll data.\nThis study estimates the San Francisco Bay Area's tract-level daytime\npopulation density from US Census and LEHD LODES data. Estimated daytime\ndensities are substantially more concentrated than corresponding nighttime\npopulation densities, reflecting regional land use patterns. We conclude with a\ndiscussion of biases, limitations, and implications of this methodology.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 16:33:56 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "1806.00740", "submitter": "Tianyu Shi", "authors": "Tianyu Shi, Jiayan Guo, Xuxin Cheng, Yu hao", "title": "How does climate change influence regional stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, different places have different region stability, which is\ninfluenced by lots of factors. In this paper ,it is aimed to analyze the\ninfluence of climate change on regional stability. several factors that may\ninfluence the region stability are proposed. Then Principle Components Analysis\n(PCA) was used to select the most relevant factors. After that ,a BP neural\nnetwork is established considering all the principle components to evaluate the\nRegion Stability (RS). Subsequently, the specific influence of the climate\nchange is analyzed and the results showed that long term average precipitation\nis a main climate factor influencing the RS.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 05:52:39 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 18:44:10 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Shi", "Tianyu", ""], ["Guo", "Jiayan", ""], ["Cheng", "Xuxin", ""], ["hao", "Yu", ""]]}, {"id": "1806.01094", "submitter": "Sebastian Weichwald", "authors": "Niklas Pfister, Sebastian Weichwald, Peter B\\\"uhlmann, Bernhard\n  Sch\\\"olkopf", "title": "Robustifying Independent Component Analysis by Adjusting for Group-Wise\n  Stationary Noise", "comments": "equal contribution between Pfister and Weichwald", "journal-ref": "Journal of Machine Learning Research, 20(147):1-50, 2019. (\n  http://www.jmlr.org/papers/v20/18-399.html )", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce coroICA, confounding-robust independent component analysis, a\nnovel ICA algorithm which decomposes linearly mixed multivariate observations\ninto independent components that are corrupted (and rendered dependent) by\nhidden group-wise stationary confounding. It extends the ordinary ICA model in\na theoretically sound and explicit way to incorporate group-wise (or\nenvironment-wise) confounding. We show that our proposed general noise model\nallows to perform ICA in settings where other noisy ICA procedures fail.\nAdditionally, it can be used for applications with grouped data by adjusting\nfor different stationary noise within each group. Our proposed noise model has\na natural relation to causality and we explain how it can be applied in the\ncontext of causal inference. In addition to our theoretical framework, we\nprovide an efficient estimation procedure and prove identifiability of the\nunmixing matrix under mild assumptions. Finally, we illustrate the performance\nand robustness of our method on simulated data, provide audible and visual\nexamples, and demonstrate the applicability to real-world scenarios by\nexperiments on publicly available Antarctic ice core data as well as two EEG\ndata sets. We provide a scikit-learn compatible pip-installable Python package\ncoroICA as well as R and Matlab implementations accompanied by a documentation\nat https://sweichwald.de/coroICA/\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:17:14 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:02:44 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 14:38:49 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Pfister", "Niklas", ""], ["Weichwald", "Sebastian", ""], ["B\u00fchlmann", "Peter", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1806.01345", "submitter": "Jean-Jacques De Groote Dr.", "authors": "Daniela M L Barbato and Jean-Jacques De Groote", "title": "Extracting relevant structures from self-determination theory\n  questionnaires via Information Bottleneck method", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the application of Information Bottleneck as a\nmethod to investigate properties of questionnaires developed for the study of\nmotivational profiles based on self-determination theory. Founded on\ninformation theory, the Information Bottleneck method compresses\nmultidimensional categorical data into clusters by minimizing the loss of\nrelevant information. It does not require linearity such as Pearson\ncorrelation. For exploratory data analysis, it allows investigating the\nstructure of questionnaire items grouping with a hierarchical partition as\nperceived by the sample, based only on conditional probability distributions\ncalculated from answers to the questionnaire items. We applied the approach to\nan instrument adapted from the academic motivation scale. The Information\nBottleneck approach showed a partition of items aligned with the\nself-determination continuum. The results of this first application of\nInformation Bottleneck to psychometric studies suggested that this procedure\ncan could be useful as a consistent, complementary analysis for traditional\nexploratory procedures such as factor analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:33:53 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Barbato", "Daniela M L", ""], ["De Groote", "Jean-Jacques", ""]]}, {"id": "1806.01403", "submitter": "Meridith Bartley", "authors": "Meridith L. Bartley, Ephraim Hanks, David Hughes", "title": "A Bayesian Penalized Hidden Markov Model for Ant Interactions", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between social animals provide insights into the exchange and\nflow of nutrients, disease, and social contacts. We consider a chamber level\nanalysis of trophallaxis interactions between carpenter ants\n(\\textit{Camponotus pennsylvanicus}) over 4 hours of second-by-second\nobservations. The data show clear switches between fast and slow modes of\ntrophallaxis. However, fitting a standard hidden Markov model (HMM) results in\nan estimated hidden state process that is overfit to this high resolution data,\nas the state process fluctuates an order of magnitude more quickly than is\nbiologically reasonable. We propose a novel approach for penalized estimation\nof HMMs through a Bayesian ridge prior on the state transition rates while also\nincorporating biologically motivated covariates. This penalty induces\nsmoothing, limiting the rate of state switching that combines with appropriate\ncovariates within the colony to ensure more biologically feasible results. We\ndevelop a Markov chain Monte Carlo algorithm to perform Bayesian inference\nbased on discretized observations of the contact network.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 21:43:24 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Bartley", "Meridith L.", ""], ["Hanks", "Ephraim", ""], ["Hughes", "David", ""]]}, {"id": "1806.01513", "submitter": "Peiran Liu", "authors": "Peiran Liu, Adrian E. Raftery", "title": "Accounting for Uncertainty About Past Values In Probabilistic\n  Projections of the Total Fertility Rate for All Countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the 1940s, population projections have in most cases been produced\nusing the deterministic cohort component method. However, in 2015, for the\nfirst time, in a major advance, the United Nations issued official\nprobabilistic population projections for all countries based on Bayesian\nhierarchical models for total fertility and life expectancy. The estimates of\nthese models and the resulting projections are conditional on the UN's official\nestimates of past values. However, these past values are themselves uncertain,\nparticularly for the majority of the world's countries that do not have\nlongstanding high-quality vital registration systems, when they rely on surveys\nand censuses with their own biases and measurement errors. This paper is a\nfirst attempt to remedy this for total fertility rates, by extending the UN\nmodel for the future to take account of uncertainty about past values. This is\ndone by adding an additional level to the hierarchical model to represent the\nmultiple data sources, in each case estimating their bias and measurement error\nvariance. We assess the method by out-of-sample predictive validation. While\nthe prediction intervals produced by the current method have somewhat less than\nnominal coverage, we find that our proposed method achieves close to nominal\ncoverage. The prediction intervals become wider for countries for which the\nestimates of past total fertility rates rely heavily on surveys rather than on\nvital registration data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 06:31:06 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Liu", "Peiran", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1806.01595", "submitter": "Jean-Louis Foulley JLF", "authors": "Jean-Louis Foulley and Gilles Celeux", "title": "A penalty criterion for score forecasting in soccer", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note proposes a penalty criterion for assessing correct score\nforecasting in a soccer match. The penalty is based on hierarchical priorities\nfor such a forecast i.e., i) Win, Draw and Loss exact prediction and ii)\nnormalized Euclidian distance between actual and forecast scores. The procedure\nis illustrated on typical scores, and different alternatives on the penalty\ncomponents are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:17:57 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Foulley", "Jean-Louis", ""], ["Celeux", "Gilles", ""]]}, {"id": "1806.01757", "submitter": "Minhui Zheng", "authors": "Minhui Zheng and Bruce D. Spencer", "title": "Estimating Shortest Path Length Distributions via Random Walk Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a network, the shortest paths between nodes are of great importance as\nthey allow the fastest and strongest interaction between nodes. However\nmeasuring the shortest paths between all nodes in a large network is\ncomputationally expensive. In this paper we propose a method to estimate the\nshortest path length (SPL) distribution of a network by random walk sampling.\nTo deal with the unequal inclusion probabilities of dyads (pairs of nodes) in\nthe sample, we generalize the usage of Hansen-Hurwitz estimator and\nHorvitz-Thompson estimator (and their ratio forms) and apply them to the\nsampled dyads. Based on theory of Markov chains we prove that the selection\nprobability of a dyad is proportional to the product of the degrees of the two\nnodes. To approximate the actual SPL for a dyad, we use the observed SPL in the\ninduced subgraph for networks with large degree variability, i.e., the standard\ndeviation is at least two times of the mean, and for networks with small degree\nvariability, estimate the SPL using landmarks for networks with small degree\nvariability. By simulation studies and applications to real networks, we find\nthat 1) for large networks, high estimation accuracy can be achieved by using a\nsingle random or multiple random walks with total number of steps equal to at\nleast 20% of the nodes in the network; 2) the estimation performance increases\nas the network size increases but tends to stabilize when the network is large\nenough; 3) a single random walk performs as well as multiple random walks; 4)\nthe Horvitz-Thompson ratio estimator performs best among the four estimators.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:54:50 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 15:52:51 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zheng", "Minhui", ""], ["Spencer", "Bruce D.", ""]]}, {"id": "1806.01782", "submitter": "Ibraheem Kasim Ibraheem AL-Timeemee", "authors": "Ibraheem Kasim Ibraheem", "title": "Adaptive System Identification Using LMS Algorithm Integrated with\n  Evolutionary Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System identification is an exceptionally expansive topic and of remarkable\nsignificance in the discipline of signal processing and communication. Our goal\nin this paper is to show how simple adaptive FIR and IIR filters can be used in\nsystem modeling and demonstrating the application of adaptive system\nidentification. The main objective of our research is to study the LMS\nalgorithm and its improvement by the genetic search approach, namely, LMS-GA,\nto search the multi-modal error surface of the IIR filter to avoid local minima\nand finding the optimal weight vector when only measured or estimated data are\navailable. Convergence analysis of the LMS algorithm in the case of coloured\ninput signal, i.e., correlated input signal is demonstrated on adaptive FIR\nfilter via power spectral density of the input signals and Fourier transform of\nthe autocorrelation matrix of the input signal. Simulations have been carried\nout on adaptive filtering of FIR and IIR filters and tested on white and\ncoloured input signals to validate the powerfulness of the genetic-based LMS\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:30:10 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:54:37 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Ibraheem", "Ibraheem Kasim", ""]]}, {"id": "1806.01930", "submitter": "Lorenz Gilch", "authors": "Lorenz A. Gilch and Sebastian M\\\"uller", "title": "On Elo based prediction models for the FIFA Worldcup 2018", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for the analysis and prediction of a football\nchampionship. It is based on Poisson regression models that include the Elo\npoints of the teams as covariates and incorporates differences of team-specific\neffects. These models for the prediction of the FIFA World Cup 2018 are fitted\non all football games on neutral ground of the participating teams since 2010.\nBased on the model estimates for single matches Monte-Carlo simulations are\nused to estimate probabilities for reaching the different stages in the FIFA\nWorld Cup 2018 for all teams. We propose two score functions for ordinal random\nvariables that serve together with the rank probability score for the\nvalidation of our models with the results of the FIFA World Cups 2010 and 2014.\nAll models favor Germany as the new FIFA World Champion. All possible courses\nof the tournament and their probabilities are visualized using a single Sankey\ndiagram.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 20:36:30 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Gilch", "Lorenz A.", ""], ["M\u00fcller", "Sebastian", ""]]}, {"id": "1806.01947", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, Paul Fearnhead", "title": "A linear time method for the detection of point and collective anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of efficiently identifying anomalies in data sequences is an\nimportant statistical problem that now arises in many applications. Whilst\nthere has been substantial work aimed at making statistical analyses robust to\noutliers, or point anomalies, there has been much less work on detecting\nanomalous segments, or collective anomalies, particularly in those settings\nwhere point anomalies might also occur. In this article, we introduce\nCollective And Point Anomalies (CAPA), a computationally efficient approach\nthat is suitable when collective anomalies are characterised by either a change\nin mean, variance, or both, and distinguishes them from point anomalies.\nTheoretical results establish the consistency of CAPA at detecting collective\nanomalies and, as a by-product, the consistency of a popular penalised cost\nbased change in mean and variance detection method. Empirical results show that\nCAPA has close to linear computational cost as well as being more accurate at\ndetecting and locating collective anomalies than other approaches. We\ndemonstrate the utility of CAPA through its ability to detect exoplanets from\nlight curve data from the Kepler telescope.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:02:53 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 11:57:18 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1806.01998", "submitter": "Barmak Mostofian", "authors": "Barmak Mostofian and Daniel M. Zuckerman", "title": "Statistical uncertainty analysis for small-sample, high log-variance\n  data: Cautions for bootstrapping and Bayesian bootstrapping", "comments": "Added whole new section on the analysis of continuous distributions", "journal-ref": null, "doi": "10.1016/j.bpj.2018.11.779", "report-no": null, "categories": "stat.AP q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in molecular simulations allow the evaluation of previously\nunattainable observables, such as rate constants for protein folding. However,\nthese calculations are usually computationally expensive and even significant\ncomputing resources may result in a small number of independent estimates\nspread over many orders of magnitude. Such small-sample, high \"log-variance\"\ndata are not readily amenable to analysis using the standard uncertainty (i.e.,\n\"standard error of the mean\") because unphysical negative limits of confidence\nintervals result. Bootstrapping, a natural alternative guaranteed to yield a\nconfidence interval within the minimum and maximum values, also exhibits a\nstriking systematic bias of the lower confidence limit in log space. As we\nshow, bootstrapping artifactually assigns high probability to improbably low\nmean values. A second alternative, the Bayesian bootstrap strategy, does not\nsuffer from the same deficit and is more logically consistent with the type of\nconfidence interval desired. The Bayesian bootstrap provides uncertainty\nintervals that are more reliable than those from the standard bootstrap method,\nbut must be used with caution nevertheless. Neither standard nor Bayesian\nbootstrapping can overcome the intrinsic challenge of under-estimating the mean\nfrom small-size, high log-variance samples. Our conclusions are based on\nextensive analysis of model distributions and re-analysis of multiple\nindependent atomistic simulations. Although we only analyze rate constants,\nsimilar considerations will apply to related calculations, potentially\nincluding highly non-linear averages like the Jarzynski relation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 03:52:34 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 00:25:17 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 20:29:33 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Mostofian", "Barmak", ""], ["Zuckerman", "Daniel M.", ""]]}, {"id": "1806.02078", "submitter": "Kunjin Chen", "authors": "Kunjin Chen, Qin Wang, Ziyu He, Kunlong Chen, Jun Hu, Jinliang He", "title": "Convolutional Sequence to Sequence Non-intrusive Load Monitoring", "comments": "This paper is submitted to IET-The Journal of Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolutional sequence to sequence non-intrusive load monitoring model is\nproposed in this paper. Gated linear unit convolutional layers are used to\nextract information from the sequences of aggregate electricity consumption.\nResidual blocks are also introduced to refine the output of the neural network.\nThe partially overlapped output sequences of the network are averaged to\nproduce the final output of the model. We apply the proposed model to the REDD\ndataset and compare it with the convolutional sequence to point model in the\nliterature. Results show that the proposed model is able to give satisfactory\ndisaggregation performance for appliances with varied characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 09:19:08 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chen", "Kunjin", ""], ["Wang", "Qin", ""], ["He", "Ziyu", ""], ["Chen", "Kunlong", ""], ["Hu", "Jun", ""], ["He", "Jinliang", ""]]}, {"id": "1806.02149", "submitter": "Sharif Mahmood", "authors": "Sharif Mahmood", "title": "The Performance of Largest Caliper Matching: A Monte Carlo Simulation\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an investigation of estimating treatment effect using\ndifferent matching methods. The study proposed a new method which is\ncomputationally efficient and convenient in implication-'largest caliper\nmatching' and compared the performance with other five popular matching methods\nby simulation. The bias, empirical standard deviation and the mean square error\nof the estimates in the simulation are checked under different treatment\nprevalence and different distributions of covariates. A Monte Carlo simulation\nstudy and a real data example are employed to measure the performance of these\nmethods. It is shown that matched samples improve estimation of the population\ntreatment effect in a wide range of settings. It reduces the bias if the data\ncontains the selection on observables and treatment imbalances. Also, findings\nabout the relative performance of the different matching methods are provided\nto help practitioners determine which method should be used under certain\nsituations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:48:24 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Mahmood", "Sharif", ""]]}, {"id": "1806.02228", "submitter": "Eva B\\\"orgens", "authors": "Eva B\\\"orgens, Denise Dettmering, Florian Seitz", "title": "Observing water level extremes in the Mekong River Basin: The benefit of\n  long-repeat orbit missions in a multi-mission satellite altimetry approach", "comments": "published in Journal of Hydrology", "journal-ref": null, "doi": "10.1016/j.jhydrol.2018.12.041", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-mission altimetric water level observations of rivers are spatially\nand temporally limited, and thus they are often unable to quantify the full\nextent of extreme flood events. Moreover, only missions with a short-repeat\norbit, such as Envisat, Jason-2, or SARAL, could provide meaningful time series\nof water level variations directly. However, long or non-repeat orbit missions\nsuch as CryoSat-2 have a very dense spatial resolution under the trade-off of a\nrepeat time insufficient for time series extraction. Combining data from\nmultiple altimeter missions into a multi-mission product allows for increasing\nthe spatial and temporal resolution of the data. In this study, we combined\nwater level data from CryoSat-2 with various observations from other altimeter\nmissions in the Mekong River Basin between 2008 and 2016 into one multi-mission\nwater level time series using the approach of universal kriging. In contrast to\nformer multi-mission altimetry methods, this approach allows for the\nincorporation of CryoSat-2 data as well as data from other long or non-repeat\norbit missions, such as Envisat-EM or SARAL-DP. Additionally, for the first\ntime, data from tributaries are incorporated. The multi-mission time series\nincluding CryoSat-2 data adequately reflects the general inter-annual flood\nbehaviour and the extreme floodings in 2008 and 2011. It performs better than\nsingle-mission time series or multi-mission time series based only on\nshort-repeat orbit data. The Probability of Detection of the floodings with the\nmulti-mission altimetry was around 80\\% while Envisat and Jason-2\nsingle-mission altimetry could only detect around 40% of the floodings\ncorrectly. However, small flash floods still remain undetectable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:45:53 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 06:28:07 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 09:29:40 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["B\u00f6rgens", "Eva", ""], ["Dettmering", "Denise", ""], ["Seitz", "Florian", ""]]}, {"id": "1806.02307", "submitter": "Timothy Brathwaite", "authors": "Timothy Brathwaite", "title": "Check yourself before you wreck yourself: Assessing discrete choice\n  models through predictive simulations", "comments": "24 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, discrete choice modelers develop ever-more advanced models and\nestimation methods. Compared to the impressive progress in model development\nand estimation, model-checking techniques have lagged behind. Often, choice\nmodelers use only crude methods to assess how well an estimated model\nrepresents reality. Such methods usually stop at checking parameter signs,\nmodel elasticities, and ratios of model coefficients. In this paper, I greatly\nexpand the discrete choice modelers' assessment toolkit by introducing model\nchecking procedures based on graphical displays of predictive simulations.\n  Overall, my contributions are as follows. Methodologically, I introduce a\ngeneral and 'semi-automatic' algorithm for checking discrete choice models via\npredictive simulations. By combining new graphical displays with existing\nplots, I introduce methods for checking one's data against one's model in terms\nof the model's predicted distributions of choices (P(Y)), choices given\nexplanatory variables (P(Y|X)), and explanatory variables given choices\n(P(X|Y)). Empirically, I demonstrate my proposed methods by checking the models\nfrom Brownstone and Train (1998). Through this case study, I show that my\nproposed methods can point out lack-of-model-fit in one's models and suggest\nconcrete model improvements that substantively change the results of one's\npolicy analysis. Moreover, the case study highlights a practical trade-off\nbetween precision and robustness in model checking.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:12:09 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 17:59:24 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Brathwaite", "Timothy", ""]]}, {"id": "1806.02388", "submitter": "Lei Lin", "authors": "Lei Lin", "title": "Efficient Collection of Connected Vehicle Data based on Compressive\n  Sensing", "comments": "Submitted to The 21st IEEE International Conference on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connected vehicles (CVs) can capture and transmit detailed data like vehicle\nposition, speed and so on through vehicle-to-vehicle and\nvehicle-to-infrastructure communications. The wealth of CV data provides new\nopportunities to improve the safety, mobility, and sustainability of\ntransportation systems. However, the potential data explosion likely will\noverburden storage and communication systems. To solve this issue, we design a\nreal-time compressive sensing (CS) approach which allows CVs to collect and\ncompress data in real-time and can recover the original data accurately and\nefficiently when it is necessary. The CS approach is applied to recapture 10\nmillion CV Basic Safety Message speed samples from the Safety Pilot Model\nDeployment program. With a compression ratio of 0.2, it is found that the CS\napproach can recover the original speed data with the root mean squared error\nas low as 0.05. The recovery performances of the CS approach are further\nexplored by time-of-day and acceleration. The results show that the CS approach\nperforms better in data recovery when CV speeds are steady or changing\nsmoothly.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:09:57 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Lin", "Lei", ""]]}, {"id": "1806.02420", "submitter": "Charles Matthews", "authors": "Charles Matthews and Bradly Stadie and Jonathan Weare and Mihai\n  Anitescu and Christopher Demarco", "title": "Simulating the stochastic dynamics and cascade failure of power networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale power networks, the failure of particular transmission lines\ncan offload power to other lines and cause self-protection trips to activate,\ninstigating a cascade of line failures. In extreme cases, this can bring down\nthe entire network. Learning where the vulnerabilities are and the expected\ntimescales for which failures are likely is an active area of research. In this\narticle we present a novel stochastic dynamics model for a large-scale power\nnetwork along with a framework for efficient computer simulation of the model\nincluding long timescale events such as cascade failure. We build on an\nexisting Hamiltonian formulation and introduce stochastic forcing and damping\ncomponents to simulate small perturbations to the network. Our model and\nsimulation framework allow assessment of the particular weaknesses in a power\nnetwork that make it susceptible to cascade failure, along with the timescales\nand mechanism for expected failures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:49:42 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Matthews", "Charles", ""], ["Stadie", "Bradly", ""], ["Weare", "Jonathan", ""], ["Anitescu", "Mihai", ""], ["Demarco", "Christopher", ""]]}, {"id": "1806.02475", "submitter": "Alireza Ebrahimvandi", "authors": "Alireza Ebrahimvandi, Niyousha Hosseinichimeh, Jay Iams", "title": "Understanding State level Variations in U.S. Infant Mortality: 2000 to\n  2015", "comments": null, "journal-ref": "Am J Perinatol. 2019 Oct;36(12):1271-1277", "doi": "10.1055/s-0038-1675835", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To exploit state variations in infant mortality, identify\ndiagnoses that contributed to reduction of the infant mortality rate (IMR), and\nexamine factors associated with preterm related mortality rate (PMR). Methods:\nUsing linked birth-infant deaths files, we examined patterns in the leading\ncauses of IMR. We compared these rates at both national and state levels to\nfind reduction trends. Creating a cross-sectional time series of states' PMR\nand some explanatory variables, we implemented a fixed-effect regression model.\nResults: We found substantial state-level variations in changes of the IMR\n(range= -2.87 to 2.08) and PMR (-1.77 to 0.67). Twenty-one states in which the\nIMR declined more than the national average of 0.99 (6.89 to 5.90) were labeled\nas successful. In the successful states, we found a reduction in the PMR\naccounted for the largest decline in the IMR (0.90 fewer deaths). Changes in\nthe other subgroups of leading causes did not differ significantly in\nsuccessful and unsuccessful states. Policy Implications: Although its impact is\nnot large, reducing the percentage of pregnant women with inadequate care is\none of the mechanisms through which the PMR might decrease.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 00:53:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ebrahimvandi", "Alireza", ""], ["Hosseinichimeh", "Niyousha", ""], ["Iams", "Jay", ""]]}, {"id": "1806.02588", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu, Elaine M. Bettaney, Benjamin Paul Chamberlain", "title": "Designing Experiments to Measure Incrementality on Facebook", "comments": "Accepted into 2018 AdKDD & TargetAd Workshop in conjunction with KDD\n  2018; 6 pages, 4 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of Facebook advertising has risen dramatically in recent\nyears, with the platform accounting for almost 20% of the global online ad\nspend in 2017. An important consideration in advertising is incrementality: how\nmuch of the change in an experimental metric is an advertising campaign\nresponsible for. To measure incrementality, Facebook provide lift studies. As\nFacebook lift studies differ from standard A/B tests, the online\nexperimentation literature does not describe how to calculate parameters such\nas power and minimum sample size. Facebook also offer multi-cell lift tests,\nwhich can be used to compare campaigns that don't have statistically identical\naudiences. In this case, there is no literature describing how to measure the\nsignificance of the difference in incrementality between cells, or how to\nestimate the power or minimum sample size. We fill these gaps in the literature\nby providing the statistical power and required sample size calculation for\nFacebook lift studies. We then generalise the statistical significance, power,\nand required sample size calculation to multi-cell lift studies. We represent\nour results theoretically in terms of the distributions of test metrics and in\npractical terms relating to the metrics used by practitioners, making all of\nour code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:51:23 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 12:09:30 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Bettaney", "Elaine M.", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1806.02714", "submitter": "Yemeserach Mekonnen", "authors": "Syed Rahman, Haneen Aburub, Yemeserach Mekonnen, and Arif I.Sarwat", "title": "A Study of EV BMS Cyber Security Based on Neural Network SOC Prediction", "comments": "5 pages, 13 figures Accepted to 2018 IEEE PES Transmission and\n  Distribution Conference & Exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent changes to greenhouse gas emission policies are catalyzing the\nelectric vehicle (EV) market making it readily accessible to consumers. While\nthere are challenges that arise with dense deployment of EVs, one of the major\nfuture concerns is cyber security threat. In this paper, cyber security threats\nin the form of tampering with EV battery's State of Charge (SOC) was explored.\nA Back Propagation (BP) Neural Network (NN) was trained and tested based on\nexperimental data to estimate SOC of battery under normal operation and\ncyber-attack scenarios. NeuralWare software was used to run scenarios.\nDifferent statistic metrics of the predicted values were compared against the\nactual values of the specific battery tested to measure the stability and\naccuracy of the proposed BP network under different operating conditions. The\nresults showed that BP NN was able to capture and detect the false entries due\nto a cyber-attack on its network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:50:44 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Rahman", "Syed", ""], ["Aburub", "Haneen", ""], ["Mekonnen", "Yemeserach", ""], ["Sarwat", "Arif I.", ""]]}, {"id": "1806.02825", "submitter": "Biplav Srivastava", "authors": "Ramashish Gaurav, Biplav Srivastava", "title": "Estimating Train Delays in a Large Rail Network Using a Zero Shot Markov\n  Model", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India runs the fourth largest railway transport network size carrying over 8\nbillion passengers per year. However, the travel experience of passengers is\nfrequently marked by delays, i.e., late arrival of trains at stations, causing\ninconvenience. In a first, we study the systemic delays in train arrivals using\nn-order Markov frameworks and experiment with two regression based models.\nUsing train running-status data collected for two years, we report on an\nefficient algorithm for estimating delays at railway stations with near\naccurate results. This work can help railways to manage their resources, while\nalso helping passengers and businesses served by them to efficiently plan their\nactivities.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 02:08:40 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gaurav", "Ramashish", ""], ["Srivastava", "Biplav", ""]]}, {"id": "1806.02890", "submitter": "Lei Lin", "authors": "Lei Lin", "title": "A Comprehensive Framework for Dynamic Bike Rebalancing in a Large Bike\n  Sharing Network", "comments": "Accepted by THE 7TH INTERNATIONAL SYMPOSIUM ON DYNAMIC TRAFFIC\n  ASSIGNMENT, Hongkong, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bike sharing is a vital component of a modern multi-modal transportation\nsystem. However, its implementation can lead to bike supply-demand imbalance\ndue to fluctuating spatial and temporal demands. This study proposes a\ncomprehensive framework to develop optimal dynamic bike rebalancing strategies\nin a large bike sharing network. It consists of three components, including a\nstation-level pick-up/drop-off prediction model, station clustering model, and\ncapacitated location-routing optimization model. For the first component, we\npropose a powerful deep learning model called graph convolution neural network\nmodel (GCNN) with data-driven graph filter (DDGF), which can automatically\nlearn the hidden spatial-temporal correlations among stations to provide more\naccurate predictions; for the second component, we apply a graph clustering\nalgorithm labeled the Community Detection algorithm to cluster stations that\nlocate geographically close to each other and have a small net demand gap;\nlast, a capacitated location-routing problem (CLRP) is solved to deal with the\ncombination of two types of decision variables: the locations of bike\ndistribution centers and the design of distribution routes for each cluster.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:30:49 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Lin", "Lei", ""]]}, {"id": "1806.02905", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang, Genevera I. Allen, Hongtu Zhu, David Dunson", "title": "Tensor network factorizations: Relationships between brain structural\n  connectomes and traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced brain imaging techniques make it possible to measure individuals'\nstructural connectomes in large cohort studies non-invasively. The structural\nconnectome is initially shaped by genetics and subsequently refined by the\nenvironment. It is extremely interesting to study relationships between\nstructural connectomes and environment factors or human traits, such as\nsubstance use and cognition. Due to limitations in structural connectome\nrecovery, previous studies largely focus on functional connectomes. Questions\nremain about how well structural connectomes can explain variance in different\nhuman traits. Using a state-of-the-art structural connectome processing\npipeline and a novel dimensionality reduction technique applied to data from\nthe Human Connectome Project (HCP), we show strong relationships between\nstructural connectomes and various human traits. Our dimensionality reduction\napproach uses a tensor characterization of the connectome and relies on a\ngeneralization of principal components analysis. We analyze over 1100 scans for\n1076 subjects from the HCP and the Sherbrooke test-retest data set, as well as\n$175$ human traits that measure domains including cognition, substance use,\nmotor, sensory and emotion. We find that structural connectomes are associated\nwith many traits. Specifically, fluid intelligence, language comprehension, and\nmotor skills are associated with increased cortical-cortical brain structural\nconnectivity, while the use of alcohol, tobacco, and marijuana are associated\nwith decreased cortical-cortical connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 21:11:56 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Allen", "Genevera I.", ""], ["Zhu", "Hongtu", ""], ["Dunson", "David", ""]]}, {"id": "1806.03045", "submitter": "Andriy Temko Dr", "authors": "Mark O'Sullivan, Emanuel Popovici, Andrea Bocchino, Conor O'Mahony,\n  Geraldine Boylan, Andriy Temko", "title": "System Level Framework for Assessing the Accuracy of Neonatal EEG\n  Acquisition", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant research has been conducted in recent years to design low-cost\nalternatives to the current EEG monitoring systems used in healthcare\nfacilities. Testing such systems on a vulnerable population such as newborns is\ncomplicated due to ethical and regulatory considerations that slow down the\ntechnical development. This paper presents and validates a method for\nquantifying the accuracy of neonatal EEG acquisition systems and electrode\ntechnologies via clinical data simulations that do not require neonatal\nparticipants. The proposed method uses an extensive neonatal EEG database to\nsimulate analogue signals, which are subsequently passed through electrical\nmodels of the skin-electrode interface, which are developed using wet and dry\nEEG electrode designs. The signal losses in the system are quantified at each\nstage of the acquisition process for electrode and acquisition board losses.\nSNR, correlation and noise values were calculated. The results verify that\nlow-cost EEG acquisition systems are capable of obtaining clinical grade EEG.\nAlthough dry electrodes result in a significant increase in the skin-electrode\nimpedance, accurate EEG recordings are still achievable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:34:44 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["O'Sullivan", "Mark", ""], ["Popovici", "Emanuel", ""], ["Bocchino", "Andrea", ""], ["O'Mahony", "Conor", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.03046", "submitter": "Andriy Temko Dr", "authors": "Oksana Semenova, Giorgia Carra, Gordon Lightbody, Geraldine Boylan,\n  Eugene Dempsey, Andriy Temko", "title": "Heart Rate Variability during Periods of Low Blood Pressure as a\n  Predictor of Short-Term Outcome in Preterms", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient management of low blood pressure (BP) in preterm neonates remains\nchallenging with a considerable variability in clinical practice. The ability\nto assess preterm wellbeing during episodes of low BP will help to decide when\nand whether hypotension treatment should be initiated. This work aims to\ninvestigate the relationship between heart rate variability (HRV), BP and the\nshort-term neurological outcome in preterm infants less than 32 weeks\ngestational age (GA). The predictive power of common HRV features with respect\nto the outcome is assessed and shown to improve when HRV is observed during\nepisodes of low mean arterial pressure (MAP) - with a single best feature\nleading to an AUC of 0.87. Combining multiple features with a boosted decision\ntree classifier achieves an AUC of 0.97. The work presents a promising step\ntowards the use of multimodal data in building an objective decision support\ntool for clinical prediction of short-term outcome in preterms who suffer\nepisodes of low BP.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:35:19 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Semenova", "Oksana", ""], ["Carra", "Giorgia", ""], ["Lightbody", "Gordon", ""], ["Boylan", "Geraldine", ""], ["Dempsey", "Eugene", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.03047", "submitter": "Andriy Temko Dr", "authors": "Sergi Gomez, Mark O'Sullivan, Emanuel Popovici, Sean Mathieson,\n  Geraldine Boylan, Andriy Temko", "title": "On sound-based interpretation of neonatal EEG", "comments": "ISSC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SD eess.AS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant training is required to visually interpret neonatal EEG signals.\nThis study explores alternative sound-based methods for EEG interpretation\nwhich are designed to allow for intuitive and quick differentiation between\nhealthy background activity and abnormal activity such as seizures. A novel\nmethod based on frequency and amplitude modulation (FM/AM) is presented. The\nalgorithm is tuned to facilitate the audio domain perception of rhythmic\nactivity which is specific to neonatal seizures. The method is compared with\nthe previously developed phase vocoder algorithm for different time compressing\nfactors. A survey is conducted amongst a cohort of non-EEG experts to\nquantitatively and qualitatively examine the performance of sound-based methods\nin comparison with the visual interpretation. It is shown that both\nsonification methods perform similarly well, with a smaller inter-observer\nvariability in comparison with visual. A post-survey analysis of results is\nperformed by examining the sensitivity of the ear to frequency evolution in\naudio.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:40:32 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gomez", "Sergi", ""], ["O'Sullivan", "Mark", ""], ["Popovici", "Emanuel", ""], ["Mathieson", "Sean", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.03208", "submitter": "Gunther Schauberger", "authors": "Andreas Groll and Christophe Ley and Gunther Schauberger and Hans Van\n  Eetvelde", "title": "Prediction of the FIFA World Cup 2018 - A random forest approach with an\n  emphasis on estimated team ability parameters", "comments": "First revised version, corrected typo in introduction when referring\n  to the winning probabilities derived by Zeileis, Leitner, and Hornik (2018),\n  which are for Germany 15.8% instead of 12.8%. Second revised version, slight\n  changes in notation in Section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare three different modeling approaches for the scores\nof soccer matches with regard to their predictive performances based on all\nmatches from the four previous FIFA World Cups 2002 - 2014: Poisson regression\nmodels, random forests and ranking methods. While the former two are based on\nthe teams' covariate information, the latter method estimates adequate ability\nparameters that reflect the current strength of the teams best. Within this\ncomparison the best-performing prediction methods on the training data turn out\nto be the ranking methods and the random forests. However, we show that by\ncombining the random forest with the team ability parameters from the ranking\nmethods as an additional covariate we can improve the predictive power\nsubstantially. Finally, this combination of methods is chosen as the final\nmodel and based on its estimates, the FIFA World Cup 2018 is simulated\nrepeatedly and winning probabilities are obtained for all teams. The model\nslightly favors Spain before the defending champion Germany. Additionally, we\nprovide survival probabilities for all teams and at all tournament stages as\nwell as the most probable tournament outcome.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:04:45 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 07:25:46 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 15:16:22 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Groll", "Andreas", ""], ["Ley", "Christophe", ""], ["Schauberger", "Gunther", ""], ["Van Eetvelde", "Hans", ""]]}, {"id": "1806.03211", "submitter": "Jordan Dworkin", "authors": "Jordan D. Dworkin, Russell T. Shinohara, Danielle S. Bassett", "title": "The landscape of NeuroImage-ing research", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2018.09.005", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the field of neuroimaging grows, it can be difficult for scientists within\nthe field to gain and maintain a detailed understanding of its ever-changing\nlandscape. While collaboration and citation networks highlight important\ncontributions within the field, the roles of and relations among specific areas\nof study can remain quite opaque. Here, we apply techniques from network\nscience to map the landscape of neuroimaging research documented in the journal\nNeuroImage over the past decade. We create a network in which nodes represent\nresearch topics, and edges give the degree to which these topics tend to be\ncovered in tandem. The network displays small-world architecture, with\ncommunities characterized by common imaging modalities and medical\napplications, and with bridges that integrate these distinct subfields. Using\nnode-level analysis, we quantify the structural roles of individual topics\nwithin the neuroimaging landscape, and find high levels of clustering within\nthe structural MRI subfield as well as increasing participation among topics\nrelated to psychiatry. The overall prevalence of a topic is unrelated to the\nprevalence of its neighbors, but the degree to which a topic becomes more or\nless popular over time is strongly related to changes in the prevalence of its\nneighbors. Broadly, this work presents a cohesive model for understanding the\nlandscape of neuroimaging research across the field, in broad subfields, and\nwithin specific topic areas.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:14:06 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Dworkin", "Jordan D.", ""], ["Shinohara", "Russell T.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1806.03659", "submitter": "Bachirou Tadd\\'e", "authors": "Bachirou O. Tadd\\'e, H\\'el\\`ene Jacqmin-Gadda, Jean-Fran\\c{c}ois\n  Dartigues, Daniel Commenges, C\\'ecile Proust-Lima", "title": "Dynamic Modelling of Multivariate Dimensions and Their Temporal\n  Relationships using Latent Processes: Application to Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease gradually affects several components including the\ncerebral dimension with brain atrophies, the cognitive dimension with a decline\nin various functions and the functional dimension with impairment in the daily\nliving activities. Understanding how such dimensions interconnect is crucial\nfor AD research. However it requires to simultaneously capture the dynamic and\nmultidimensional aspects, and to explore temporal relationships between\ndimensions. We propose an original dynamic structural model that accounts for\nall these features. The model defines dimensions as latent processes and\ncombines a multivariate linear mixed model and a system of difference equations\nto model trajectories and temporal relationships between latent processes in\nfinely discrete time. Dimensions are simultaneously related to their observed\n(possibly multivariate) markers through nonlinear equations of observation.\nParameters are estimated in the maximum likelihood framework enjoying a closed\nform for the likelihood. We demonstrate in a simulation study that this dynamic\nmodel in discrete time benefits the same causal interpretation of temporal\nrelationships as models defined in continuous time as long as the\ndiscretization step remains small. The model is then applied to the data of the\nAlzheimer's Disease Neuroimaging Initiative. Three longitudinal dimensions\n(cerebral anatomy, cognitive ability and functional autonomy) measured by 6\nmarkers are analyzed and their temporal structure is contrasted between\ndifferent clinical stages of Alzheimer's disease. Keywords: causality,\ndifference equations, latent process, longitudinal data, mixed models,\nmultivariate data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 13:42:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 21:24:31 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 19:35:38 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 19:11:23 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Tadd\u00e9", "Bachirou O.", ""], ["Jacqmin-Gadda", "H\u00e9l\u00e8ne", ""], ["Dartigues", "Jean-Fran\u00e7ois", ""], ["Commenges", "Daniel", ""], ["Proust-Lima", "C\u00e9cile", ""]]}, {"id": "1806.03729", "submitter": "Johannes Martini", "authors": "Johannes W R Martini and Francisco Rosales and Ngoc-Thuy Ha and Thomas\n  Kneib and Johannes Heise and Valentin Wimmer", "title": "Lost in translation: On the impact of data coding on penalized\n  regression with interactions", "comments": null, "journal-ref": "G3 (2019) https://doi.org/10.1534/g3.118.200961", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression approaches are standard tools in quantitative genetics.\nIt is known that the fit of an \\emph{ordinary least squares} (OLS) regression\nis independent of certain transformations of the coding of the predictor\nvariables, and that the standard mixed model \\emph{ridge regression best linear\nunbiased prediction} (RRBLUP) is neither affected by translations of the\nvariable coding, nor by global scaling. However, it has been reported that an\nextended version of this mixed model, which incorporates interactions by\nproducts of markers as additional predictor variables is affected by\ntranslations of the marker coding. In this work, we identify the cause of this\nloss of invariance in a general context of penalized regression on polynomials\nin the predictor variables. We show that in most cases, translating the coding\nof the predictor variables has an impact on effect estimates, with an exception\nwhen only the size of the coefficients of monomials of highest total degree are\npenalized. The invariance of RRBLUP can thus be considered as a special case of\nthis setting, with a polynomial of total degree 1, where the size of the fixed\neffect (total degree 0) is not penalized but all coefficients of monomials of\ntotal degree 1 are. The extended RRBLUP, which includes interactions, is not\ninvariant to translations because it does not only penalize interactions (total\ndegree 2), but also additive effects (total degree 1). Our observations are not\nrestricted to ridge regression, but generally valid for penalized regressions,\nfor instance also for the $\\ell_1$ penalty of LASSO.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 21:46:41 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Martini", "Johannes W R", ""], ["Rosales", "Francisco", ""], ["Ha", "Ngoc-Thuy", ""], ["Kneib", "Thomas", ""], ["Heise", "Johannes", ""], ["Wimmer", "Valentin", ""]]}, {"id": "1806.03954", "submitter": "Eardi Lila", "authors": "Eardi Lila, Simon Arridge, John A. D. Aston", "title": "Representation and reconstruction of covariance operators in linear\n  inverse problems", "comments": "40 pages", "journal-ref": null, "doi": "10.1088/1361-6420/ab8713", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for the reconstruction and representation of\nfunctions in a setting where these objects cannot be directly observed, but\nonly indirect and noisy measurements are available, namely an inverse problem\nsetting. The proposed methodology can be applied either to the analysis of\nindirectly observed functional images or to the associated covariance\noperators, representing second-order information, and thus lying on a\nnon-Euclidean space. To deal with the ill-posedness of the inverse problem, we\nexploit the spatial structure of the sample data by introducing a flexible\nregularizing term embedded in the model. Thanks to its efficiency, the proposed\nmodel is applied to MEG data, leading to a novel approach to the investigation\nof functional connectivity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:23:13 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 16:38:32 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 18:59:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lila", "Eardi", ""], ["Arridge", "Simon", ""], ["Aston", "John A. D.", ""]]}, {"id": "1806.04098", "submitter": "Henrique Bolfarine", "authors": "Henrique Bolfarine and Lina Thomas and Anatoly Yambartsev", "title": "Network reconstruction with local partial correlation: a comparative\n  evaluation", "comments": "8 pages, 3 figures, 1 table, submited", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, various methods have been proposed for the\nreconstruction of networks modeled as Gaussian Graphical Models. In this work,\nwe analyzed three different approaches: the Graphical Lasso (GLasso), the\nGraphical Ridge (GGMridge), and the Local Partial Correlation (LPC). For the\nevaluation of the methods, we used high dimensional data generated from\nsimulated random graphs (Erd\\\"os-R\\'enyi, Barab\\'asi-Albert, Watts-Strogatz).\nThe performance was assessed through the Receiver Operating Characteristic\n(ROC) curve. In addition, the methods were used to reconstruct the\nco-expression network for differentially expressed genes in human cervical\ncancer data. The LPC method outperformed the GLasso in most simulated cases.\nThe GGMridge produced better ROC curves then both the other methods. Finally,\nLPC and GGMridge obtained similar outcomes in real data studies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 16:42:04 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:28:08 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Bolfarine", "Henrique", ""], ["Thomas", "Lina", ""], ["Yambartsev", "Anatoly", ""]]}, {"id": "1806.04200", "submitter": "Bret Zeldow", "authors": "Bret Zeldow, Vincent Lo Re III, Jason Roy", "title": "A semiparametric modeling approach using Bayesian Additive Regression\n  Trees with an application to evaluate heterogeneous treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Additive Regression Trees (BART) is a flexible machine learning\nalgorithm capable of capturing nonlinearities between an outcome and covariates\nand interaction among covariates. We extend BART to a semiparametric regression\nframework in which the conditional expectation of an outcome is a function of\ntreatment, its effect modifiers, and confounders. The confounders, not of\nscientific interest, are allowed to have unspecified functional form, while\ntreatment and other covariates that do have scientific importance are given the\nusual linear form from parametric regression. The result is a Bayesian\nsemiparametric linear regression model where the posterior distribution of the\nparameters of the linear part can be interpreted as in parametric Bayesian\nregression. This is useful in situations where a subset of the variables are of\nsubstantive interest and the others are nuisance variables that we would like\nto control for. An example of this occurs in causal modeling with the\nstructural mean model (SMM). Under certain causal assumptions, our method can\nbe used as a Bayesian SMM. Our methods are demonstrated with simulation studies\nand an application to dataset involving adults with HIV/Hepatitis C coinfection\nwho newly initiate antiretroviral therapy. The methods are available in an R\npackage semibart.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:12:07 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Zeldow", "Bret", ""], ["Re", "Vincent Lo", "III"], ["Roy", "Jason", ""]]}, {"id": "1806.04408", "submitter": "Anil Gore Dr.", "authors": "Anil Gore, Sharayu Paranjpe", "title": "How effective is your blinding?", "comments": "14 pages 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report answers queries about extending the blinding index approach to a\nsituation with measurements at multiple time points. The key question is how to\ntest if there is progressive unblinding. A related question is how to apportion\nextent of unblinding between primary cause (trial design) and secondary cause\n(AE or efficacy).\n  It is indeed possible to answer these questions. Sections 2 to 5 develop the\nnarrative. Section 6 addresses the basic question about testing for progressive\nunblinding. The strategy is to use various statistical methods available in\nliterature. One method is generalized McNemar test for marginal homogeneity.\nSecond is a weighted least squares approach described by Stokes et al (2000).\nThird is application of polytomous logistic regression. Fourth and last is a\nsimulation approach. This is the key part of the report. Other questions are\nanswered in Section 7. Numerical illustrations are given based on hypothetical\ndata (since a trial is yet to be conducted). Relevant program code is available\nif needed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:29:09 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gore", "Anil", ""], ["Paranjpe", "Sharayu", ""]]}, {"id": "1806.04420", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot, Guillaume Lecuelle, Pascal Schlich, Michel Visalli", "title": "Estimating finite mixtures of semi-Markov chains: an application to the\n  segmentation of temporal sensory data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In food science, it is of great interest to get information about the\ntemporal perception of aliments to create new products, to modify existing ones\nor more generally to understand the perception mechanisms. Temporal Dominance\nof Sensations (TDS) is a technique to measure temporal perception which\nconsists in choosing sequentially attributes describing a food product over\ntasting. This work introduces new statistical models based on finite mixtures\nof semi-Markov chains in order to describe data collected with the TDS\nprotocol, allowing different temporal perceptions for a same product within a\npopulation. The identifiability of the parameters of such mixture models is\ndiscussed. Sojourn time distributions are fitted with gamma probability\ndistribution and a penalty is added to the log likelihood to ensure convergence\nof the EM algorithm to a non degenerate solution. Information criterions are\nemployed for determining the number of mixture components. Then, the individual\nqualitative trajectories are clustered with the help of the maximum a\nposteriori probability (MAP) approach. A simulation study confirms the good\nbehavior of the proposed estimation procedure. The methodology is illustrated\non an example of consumers perception of a Gouda cheese and assesses the\nexistence of several behaviors in terms of perception of this product.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:51:32 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 15:17:43 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Lecuelle", "Guillaume", ""], ["Schlich", "Pascal", ""], ["Visalli", "Michel", ""]]}, {"id": "1806.04549", "submitter": "Maria H\\\"ugle", "authors": "Maria H\\\"ugle, Simon Heller, Manuel Watter, Manuel Blum, Farrokh\n  Manzouri, Matthias D\\\"umpelmann, Andreas Schulze-Bonhage, Peter Woias,\n  Joschka Boedecker", "title": "Early Seizure Detection with an Energy-Efficient Convolutional Neural\n  Network on an Implantable Microcontroller", "comments": "Accepted at IJCNN 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489493", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implantable, closed-loop devices for automated early detection and\nstimulation of epileptic seizures are promising treatment options for patients\nwith severe epilepsy that cannot be treated with traditional means. Most\napproaches for early seizure detection in the literature are, however, not\noptimized for implementation on ultra-low power microcontrollers required for\nlong-term implantation. In this paper we present a convolutional neural network\nfor the early detection of seizures from intracranial EEG signals, designed\nspecifically for this purpose. In addition, we investigate approximations to\ncomply with hardware limits while preserving accuracy. We compare our approach\nto three previously proposed convolutional neural networks and a feature-based\nSVM classifier with respect to detection accuracy, latency and computational\nneeds. Evaluation is based on a comprehensive database with long-term EEG\nrecordings. The proposed method outperforms the other detectors with a median\nsensitivity of 0.96, false detection rate of 10.1 per hour and median detection\ndelay of 3.7 seconds, while being the only approach suited to be realized on a\nlow power microcontroller due to its parsimonious use of computational and\nmemory resources.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:15:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["H\u00fcgle", "Maria", ""], ["Heller", "Simon", ""], ["Watter", "Manuel", ""], ["Blum", "Manuel", ""], ["Manzouri", "Farrokh", ""], ["D\u00fcmpelmann", "Matthias", ""], ["Schulze-Bonhage", "Andreas", ""], ["Woias", "Peter", ""], ["Boedecker", "Joschka", ""]]}, {"id": "1806.04634", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Paul M. Thompson, Greg Ver Steeg", "title": "Measures of Tractography Convergence", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we use information theory to understand the empirical\nconvergence rate of tractography, a widely-used approach to reconstruct\nanatomical fiber pathways in the living brain. Based on diffusion MRI data,\ntractography is the starting point for many methods to study brain\nconnectivity. Of the available methods to perform tractography, most\nreconstruct a finite set of streamlines, or 3D curves, representing probable\nconnections between anatomical regions, yet relatively little is known about\nhow the sampling of this set of streamlines affects downstream results, and how\nexhaustive the sampling should be. Here we provide a method to measure the\ninformation theoretic surprise (self-cross entropy) for tract sampling schema.\nWe then empirically assess four streamline methods. We demonstrate that the\nrelative information gain is very low after a moderate number of streamlines\nhave been generated for each tested method. The results give rise to several\nguidelines for optimal sampling in brain connectivity analyses.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:30:25 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Moyer", "Daniel", ""], ["Thompson", "Paul M.", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "1806.05035", "submitter": "Bruno Sudret", "authors": "S. J. Peter, A. Siviglia, J. Nagel, S. Marelli, R. M. Boes, D. Vetsch\n  and B. Sudret", "title": "Development of probabilistic dam breach model using Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dam breach models are commonly used to predict outflow hydrographs of\npotentially failing dams and are key ingredients for evaluating flood risk. In\nthis paper a new dam breach modeling framework is introduced that shall improve\nthe reliability of hydrograph predictions of homogeneous earthen embankment\ndams. Striving for a small number of parameters, the simplified physics-based\nmodel describes the processes of failing embankment dams by breach enlargement,\ndriven by progressive surface erosion. Therein the erosion rate of dam material\nis modeled by empirical sediment transport formulations. Embedding the model\ninto a Bayesian multilevel framework allows for quantitative analysis of\ndifferent categories of uncertainties. To this end, data available in\nliterature of observed peak discharge and final breach width of historical dam\nfailures was used to perform model inversion by applying Markov Chain Monte\nCarlo simulation. Prior knowledge is mainly based on non-informative\ndistribution functions. The resulting posterior distribution shows that the\nmain source of uncertainty is a correlated subset of parameters, consisting of\nthe residual error term and the epistemic term quantifying the breach erosion\nrate. The prediction intervals of peak discharge and final breach width are\ncongruent with values known from literature. To finally predict the outflow\nhydrograph for real case applications, an alternative residual model was\nformulated that assumes perfect data and a perfect model. The fully\nprobabilistic fashion of hydrograph prediction has the potential to improve the\nadequate risk management of downstream flooding.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:48:19 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Peter", "S. J.", ""], ["Siviglia", "A.", ""], ["Nagel", "J.", ""], ["Marelli", "S.", ""], ["Boes", "R. M.", ""], ["Vetsch", "D.", ""], ["Sudret", "B.", ""]]}, {"id": "1806.05131", "submitter": "Furong Sun", "authors": "Furong Sun, Robert B. Gramacy, Benjamin Haaland, Siyuan Lu, Youngdeok\n  Hwang", "title": "Synthesizing simulation and field data of solar irradiance", "comments": "25 pages,7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the intensity and amount of sunlight as a function of location and\ntime is an essential component in identifying promising locations for\neconomical solar farming. Although weather models and irradiance data are\nrelatively abundant, these have yet, to our knowledge, been hybridized on a\ncontinental scale. Rather, much of the emphasis in the literature has been on\nshort-term localized forecasting. This is probably because the amount of data\ninvolved in a more global analysis is prohibitive with the canonical toolkit,\nvia the Gaussian process (GP). Here we show how GP surrogate and discrepancy\nmodels can be combined to tractably and accurately predict solar irradiance on\ntime-aggregated and daily scales with measurements at thousands of sites across\nthe continental United States. Our results establish short term accuracy of\nbias-corrected weather-based simulation of irradiance, when realizations are\navailable in real space-time (e.g., in future days), and provide accurate\nsurrogates for smoothing in the more common situation where reliable weather\ndata is not available (e.g., in future years).\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:27:48 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 01:43:34 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 21:02:24 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 18:30:18 GMT"}, {"version": "v5", "created": "Sat, 22 Jun 2019 21:07:49 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sun", "Furong", ""], ["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""], ["Lu", "Siyuan", ""], ["Hwang", "Youngdeok", ""]]}, {"id": "1806.05208", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Yuming Yang, Ryan Baker, Christopher Brooks", "title": "Enabling End-To-End Machine Learning Replicability: A Case Study in\n  Educational Data Mining", "comments": "arXiv admin note: text overlap with arXiv:1801.05236", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning techniques has expanded in education research,\ndriven by the rich data from digital learning environments and institutional\ndata warehouses. However, replication of machine learned models in the domain\nof the learning sciences is particularly challenging due to a confluence of\nexperimental, methodological, and data barriers. We discuss the challenges of\nend-to-end machine learning replication in this context, and present an\nopen-source software toolkit, the MOOC Replication Framework (MORF), to address\nthem. We demonstrate the use of MORF by conducting a replication at scale, and\nprovide a complete executable container, with unique DOIs documenting the\nconfigurations of each individual trial, for replication or future extension at\nhttps://github.com/educational-technology-collective/fy2015-replication. This\nwork demonstrates an approach to end-to-end machine learning replication which\nis relevant to any domain with large, complex or multi-format,\nprivacy-protected data with a consistent schema.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 18:27:32 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 20:57:22 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Gardner", "Josh", ""], ["Yang", "Yuming", ""], ["Baker", "Ryan", ""], ["Brooks", "Christopher", ""]]}, {"id": "1806.05232", "submitter": "Staci Hepler", "authors": "Staci Hepler, Erin McKnight, Andrea Bonny, and David Kline", "title": "A latent spatial factor approach for synthesizing opioid associated\n  deaths and treatment admissions in Ohio counties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Opioid misuse is a major public health issue in the United States\nand in particular Ohio. However, the burden of the epidemic is challenging to\nquantify as public health surveillance measures capture different aspects of\nthe problem. Here we synthesize county-level death and treatment counts to\ncompare the relative burden across counties and assess associations with social\nenvironmental covariates. Methods: We construct a generalized spatial factor\nmodel to jointly model death and treatment rates for each county. For each\noutcome, we specify a spatial rates parameterization for a Poisson regression\nmodel with spatially varying factor loadings. We use a conditional\nautoregressive model to account for spatial dependence within a Bayesian\nframework. Results: The estimated spatial factor was highest in the southern\nand southwestern counties of the state, representing a higher burden of the\nopioid epidemic. We found that relatively high rates of treatment contributed\nto the factor in the southern part of the state; whereas, relatively higher\nrates of death contributed in the southwest. The estimated factor was also\npositively associated with the proportion of residents aged 18-64 on disability\nand negatively associated with the proportion of residents reporting white\nrace. Conclusions: We synthesized the information in the opioid associated\ndeath and treatment counts through a spatial factor model to estimate a latent\nfactor representing the consensus between the two surveillance measures. We\nbelieve this framework provides a coherent approach to describe the epidemic\nwhile leveraging information from multiple surveillance measures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:16:54 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Hepler", "Staci", ""], ["McKnight", "Erin", ""], ["Bonny", "Andrea", ""], ["Kline", "David", ""]]}, {"id": "1806.05281", "submitter": "Johnatan Cardona Jim\\'enez", "authors": "Johnatan Cardona Jim\\'enez, Carlos Alberto de Bragan\\c{c}a Pereira,\n  Victor Fossaluza", "title": "Full Bayesian Modeling for fMRI Group Analysis", "comments": "Thesis draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging or functional MRI (fMRI) is a\nnon-invasive way to assess brain activity by detecting changes associated with\nblood flow. In this work, we propose a full Bayesian procedure to analyze fMRI\ndata for individual and group stages. For the individual stage we use a\nmultivariate dynamic linear model (MDLM), where the temporal dependence is\nmodeled through the state parameters and the spatial dependence is modeled only\nlocally, taking the nearest neighbors of each voxel location. For the group\nstage we take advantage of the posterior distribution of the state parameters\nobtained in the individual stage and create a new posterior distribution that\nrepresents the updated beliefs for the group analysis. Since the posterior\ndistribution for the state parameters is indexed by the time $t$, we propose an\nalgorithm that allows on-line estimated curves of the state parameters to be\ndrawn and posterior probabilities computed in order to assess brain activation\nfor both individual and group analysis. We propose an alternative analysis for\nthe group stage using a Gaussian process ANOVA model, where the on-line\nestimated curves obtained in the individual stage are modeled as a functional\nresponse. Finally, we assess our proposed modeling procedure using real\nresting-state data and computing empirical false-positive brain activation\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:55:15 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Jim\u00e9nez", "Johnatan Cardona", ""], ["Pereira", "Carlos Alberto de Bragan\u00e7a", ""], ["Fossaluza", "Victor", ""]]}, {"id": "1806.05287", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron", "title": "Asymptotic distribution of least square estimators for linear models\n  with dependent errors", "comments": "18 pages, 2 figures", "journal-ref": "Statistics, 53:4, (2019), 885-902", "doi": "10.1080/02331888.2019.1593987", "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the usual linear regression model in the case\nwhere the error process is assumed strictly stationary. We use a result from\nHannan (1973), who proved a Central Limit Theorem for the usual least square\nestimator under general conditions on the design and on the error process.\nWhatever the design satisfying Hannan's conditions, we define an estimator of\nthe covariance matrix and we prove its consistency under very mild conditions.\nAs an application, we show how to modify the usual tests on the linear model in\nthis dependent context, in such a way that the type-I error rate remains\nasymptotically correct, and we illustrate the performance of this procedure\nthrough different sets of simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 22:12:46 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 16:22:36 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Caron", "Emmanuel", ""]]}, {"id": "1806.05362", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang, Raya Horesh, Karthikeyan N. Ramamurthy, Lingfei Wu,\n  Jinfeng Yi, Kryn Anderson, Kush R. Varshney", "title": "Financial Forecasting and Analysis for Low-Wage Workers", "comments": "Presented at the Data For Good Exchange 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the plethora of financial services and products on the market\nnowadays, there is a lack of such services and products designed especially for\nthe low-wage population. Approximately 30% of the U.S. working population\nengage in low-wage work, and many of them lead a paycheck-to-paycheck\nlifestyle. Financial planning advice needs to explicitly address their\nfinancial instability.\n  We propose a system of data mining techniques on small-scale transactions\ndata to improve automatic and personalized financial planning advice to\nlow-wage workers. We propose robust methods for accurate prediction of bank\naccount balances and automatic extraction of recurring transactions and\nunexpected large expenses. We formulate a hybrid method consisting of\nhistorical data averaging and a regularized regression framework for\nprediction. To uncover recurring transactions, we use a heuristic approach that\ncapitalizes on transaction descriptions. Our methods achieve higher performance\ncompared to conventional approaches and state-of-the-art predictive methods in\nreal financial transactions data.\n  In collaboration with Neighborhood Trust Financial Partners, the proposed\nmethods will upgrade the functionalities in WageGoal, Neighborhood Trust\nFinancial Partners' web-based application that provides budgeting and cash flow\nmanagement services to a user base comprising mostly low-income individuals.\nThe proposed methods will therefore have a direct impact on the individuals who\nare or will be connected to the product.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:49:50 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 05:03:14 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 16:12:49 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Zhang", "Wenyu", ""], ["Horesh", "Raya", ""], ["Ramamurthy", "Karthikeyan N.", ""], ["Wu", "Lingfei", ""], ["Yi", "Jinfeng", ""], ["Anderson", "Kryn", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1806.05424", "submitter": "Andrew Golightly", "authors": "Yingying Lai, Andrew Golightly, Richard Boys", "title": "Sequential Bayesian inference for spatio-temporal models of temperature\n  and humidity data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a spatio-temporal model to forecast sensor output at five\nlocations in North East England. The signal is described using coupled dynamic\nlinear models, with spatial effects specified by a Gaussian process. Data\nstreams are analysed using a stochastic algorithm which sequentially\napproximates the parameter posterior through a series of reweighting and\nresampling steps. An iterated batch importance sampling scheme is used to\ncircumvent particle degeneracy through a resample-move step. The algorithm is\nmodified to make it more efficient and parallisable. The model is shown to give\na good description of the underlying process and provide reasonable forecast\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:15:27 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Lai", "Yingying", ""], ["Golightly", "Andrew", ""], ["Boys", "Richard", ""]]}, {"id": "1806.05429", "submitter": "Jasper Jonathan Velthoen", "authors": "Jasper Velthoen, Juan-Juan Cai, Geurt Jongbloed and Maurice Schmeits", "title": "Improving precipitation forecasts using extreme quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to estimate extreme precipitation forecast quantiles, we propose a\nnonparametric regression model that features a constant extreme value index.\nUsing local linear quantile regression and an extrapolation technique from\nextreme value theory, we develop an estimator for conditional quantiles\ncorresponding to extreme high probability levels. We establish uniform\nconsistency and asymptotic normality of the estimators. In a simulation study,\nwe examine the performance of our estimator on finite samples in comparison\nwith a method assuming linear quantiles. On a precipitation data set in the\nNetherlands, these estimators have greater predictive skill compared to the\nupper member of ensemble forecasts provided by a numerical weather prediction\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:24:22 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 08:21:11 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Velthoen", "Jasper", ""], ["Cai", "Juan-Juan", ""], ["Jongbloed", "Geurt", ""], ["Schmeits", "Maurice", ""]]}, {"id": "1806.05450", "submitter": "Athira Subhash", "authors": "Athira Subhash, Muralikrishnan Srinivasan, Sheetal Kalyani", "title": "Asymptotic maximum order statistic for SIR in $\\kappa-\\mu$ shadowed\n  fading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using tools from extreme value theory (EVT), it is proved that, when the user\nsignal and the interferer signals undergo independent and non-identically\ndistributed (i.n.i.d.) $\\kappa-\\mu$ shadowed fading, the limiting distribution\nof the maximum of $L$ independent and identically distributed (i.i.d.)\nsignal-to-interference ratio (SIR) random variables (RVs) is a Frechet\ndistribution. It is observed that this limiting distribution is close to the\ntrue distribution of maximum, for maximum SIR evaluated over moderate $L$.\nFurther, moments of the maximum RV is shown to converge to the moments of the\nFrechet RV. Also, the rate of convergence of the actual distribution of the\nmaximum to the Frechet distribution is derived and is analyzed for different\n$\\kappa$ and $\\mu$ parameters. Finally, results from stochastic ordering are\nused to analyze the variation in the limiting distribution with respect to the\nvariation in source fading parameters. These results are then used to derive\nupper bound for the rate in Full Array Selection (FAS) schemes for antenna\nselection and the asymptotic outage probability and the ergodic rate in\nmaximum-sum-capacity (MSC) scheduling systems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 10:21:00 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 04:34:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Subhash", "Athira", ""], ["Srinivasan", "Muralikrishnan", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1806.05496", "submitter": "Richard Boys", "authors": "Richard J. Boys and Peter M. Philipson", "title": "On the ranking of Test match batsmen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking sportsmen whose careers took place in different eras is often a\ncontentious issue and the topic of much debate. In this paper we focus on\ncricket and examine what conclusions may be drawn about the ranking of Test\nbatsmen using data on batting scores from the first Test in 1877 onwards. The\noverlapping nature of playing careers is exploited to form a bridge from past\nto present so that all players can be compared simultaneously, rather than just\nrelative to their contemporaries. The natural variation in runs scored by a\nbatsman is modelled by an additive log-linear model with year, age and\ncricket-specific components used to extract the innate ability of an individual\ncricketer. Incomplete innings are handled via censoring and a zero-inflated\ncomponent is incorporated into the model to allow for an excess of frailty at\nthe start of an innings. The innings-by-innings variation of runs scored by\neach batsman leads to uncertainty in their ranking position. A Bayesian\napproach is used to fit the model and realisations from the posterior\ndistribution are obtained by deploying a Markov Chain Monte Carlo algorithm.\nPosterior summaries of innate player ability are then used to assess\nuncertainty in ranking position and this is contrasted with rankings determined\nvia the posterior mean runs scored. Posterior predictive checks show that the\nmodel provides a reasonably accurate description of runs scored.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:21:24 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Boys", "Richard J.", ""], ["Philipson", "Peter M.", ""]]}, {"id": "1806.05563", "submitter": "Haidar Almohri", "authors": "Haidar Almohri, Ratna Babu Chinnam, Mark Colosimo", "title": "Data-Driven Analytics for Benchmarking and Optimizing Retail Store\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing competitiveness and increasing availability of data is generating\ntremendous interest in data-driven analytics across industries. In the retail\nsector, stores need targeted guidance to improve both the efficiency and\neffectiveness of individual stores based on their specific locations,\ndemographics, and environment. We propose an effective data-driven framework\nfor internal benchmarking that can lead to targeted guidance for individual\nstores. In particular, we propose an objective method for segmenting stores\nusing a model-based clustering technique that accounts for similarity in store\nperformance dynamics. The proposed method relies on an effective Finite Mixture\nof Regressions technique based on competitive learning for carrying out the\nmodel-based clustering with `must-link' constraints and modeling store\nperformance. We also propose an optimization framework to derive tailored\nrecommendations for individual stores within store clusters that jointly\nimproves profitability for the store while also improving sales to satisfy\nfranchiser requirements. We validate the methods using synthetic experiments as\nwell as a real-world automotive dealership network study for a leading global\nautomotive manufacturer.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:07:30 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Almohri", "Haidar", ""], ["Chinnam", "Ratna Babu", ""], ["Colosimo", "Mark", ""]]}, {"id": "1806.05744", "submitter": "Bamdad Hosseini Dr.", "authors": "Juan G. Garcia, Bamdad Hosseini, John M Stockie", "title": "Simultaneous model calibration and source inversion in atmospheric\n  dispersion models", "comments": null, "journal-ref": "Pure and Applied Geophysics 178(3):757-776, 2021", "doi": "10.1007/s00024-019-02348-4", "report-no": null, "categories": "math.NA cs.NA physics.ao-ph physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a cost-effective method for model calibration and solution of\nsource inversion problems in atmospheric dispersion modelling. We use Gaussian\nprocess emulations of atmospheric dispersion models within a Bayesian framework\nfor solution of inverse problems. The model and source parameters are treated\nas unknowns and we obtain point estimates and approximation of uncertainties\nfor sources while simultaneously calibrating the forward model. The method is\nvalidated in the context of an industrial case study involving emissions from a\nsmelting operation for which cumulative monthly measurements of zinc\nparticulate depositions are available.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 21:21:24 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 21:00:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Garcia", "Juan G.", ""], ["Hosseini", "Bamdad", ""], ["Stockie", "John M", ""]]}, {"id": "1806.05769", "submitter": "Raymundo Arroyave", "authors": "Pejman Honarmandi, Thien Chi Duong, Seyede Fatemeh Ghoreishi, Douglas\n  Allaire, Raymundo Arroyave", "title": "Bayesian Uncertainty Quantification and Information Fusion in\n  CALPHAD-based Thermodynamic Modeling", "comments": "22 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Calculation of phase diagrams is one of the fundamental tools in alloy\ndesign---more specifically under the framework of Integrated Computational\nMaterials Engineering. Uncertainty quantification of phase diagrams is the\nfirst step required to provide confidence for decision making in property- or\nperformance-based design. As a manner of illustration, a thorough probabilistic\nassessment of the CALPHAD model parameters is performed against the available\ndata for a Hf-Si binary case study using a Markov Chain Monte Carlo sampling\napproach. The plausible optimum values and uncertainties of the parameters are\nthus obtained, which can be propagated to the resulting phase diagram. Using\nthe parameter values obtained from deterministic optimization in a\ncomputational thermodynamic assessment tool (in this case Thermo-Calc) as the\nprior information for the parameter values and ranges in the sampling process\nis often necessary to achieve a reasonable cost for uncertainty quantification.\nThis brings up the problem of finding an appropriate CALPHAD model with\nhigh-level of confidence which is a very hard and costly task that requires\nconsiderable expert skill. A Bayesian hypothesis testing based on Bayes'\nfactors is proposed to fulfill the need of model selection in this case, which\nis applied to compare four recommended models for the Hf-Si system. However, it\nis demonstrated that information fusion approaches, i.e., Bayesian model\naveraging and an error correlation-based model fusion, can be used to combine\nthe useful information existing in all the given models rather than just using\nthe best selected model, which may lack some information about the system being\nmodelled.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:37:22 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 01:54:53 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 14:41:27 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Honarmandi", "Pejman", ""], ["Duong", "Thien Chi", ""], ["Ghoreishi", "Seyede Fatemeh", ""], ["Allaire", "Douglas", ""], ["Arroyave", "Raymundo", ""]]}, {"id": "1806.05821", "submitter": "Anil Gore Dr.", "authors": "Madhusmita Panda, Sharayu Paranjpe, Anil Gore", "title": "Measuring intergroup agreement and disagreement", "comments": "18 pages 2 tables 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the need to assess the degree of agreement between\ntwo independent groups of raters. It proposes two new methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:22:53 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Panda", "Madhusmita", ""], ["Paranjpe", "Sharayu", ""], ["Gore", "Anil", ""]]}, {"id": "1806.05829", "submitter": "Jerome-Alexis Chevalier", "authors": "J\\'er\\^ome-Alexis Chevalier, Joseph Salmon, Bertrand Thirion (CEA)", "title": "Statistical Inference with Ensemble of Clustered Desparsified Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging involves high-dimensional data, yet their acquisition is\nobtained for limited samples. Multivariate predictive models have become\npopular in the last decades to fit some external variables from imaging data,\nand standard algorithms yield point estimates of the model parameters. It is\nhowever challenging to attribute confidence to these parameter estimates, which\nmakes solutions hardly trustworthy. In this paper we present a new algorithm\nthat assesses parameters statistical significance and that can scale even when\nthe number of predictors p $\\ge$ 10^5 is much higher than the number of samples\nn $\\le$ 10^3 , by lever-aging structure among features. Our algorithm combines\nthree main ingredients: a powerful inference procedure for linear models --the\nso-called Desparsified Lasso-- feature clustering and an ensembling step. We\nfirst establish that Desparsified Lasso alone cannot handle n p regimes; then\nwe demonstrate that the combination of clustering and ensembling provides an\naccurate solution, whose specificity is controlled. We also demonstrate\nstability improvements on two neuroimaging datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 07:14:39 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Chevalier", "J\u00e9r\u00f4me-Alexis", "", "CEA"], ["Salmon", "Joseph", "", "CEA"], ["Thirion", "Bertrand", "", "CEA"]]}, {"id": "1806.05924", "submitter": "Daniel Andrade", "authors": "Daniel Andrade, Akiko Takeda, Kenji Fukumizu", "title": "Robust Bayesian Model Selection for Variable Clustering with the\n  Gaussian Graphical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable clustering is important for explanatory analysis. However, only few\ndedicated methods for variable clustering with the Gaussian graphical model\nhave been proposed. Even more severe, small insignificant partial correlations\ndue to noise can dramatically change the clustering result when evaluating for\nexample with the Bayesian Information Criteria (BIC). In this work, we try to\naddress this issue by proposing a Bayesian model that accounts for negligible\nsmall, but not necessarily zero, partial correlations. Based on our model, we\npropose to evaluate a variable clustering result using the marginal likelihood.\nTo address the intractable calculation of the marginal likelihood, we propose\ntwo solutions: one based on a variational approximation, and another based on\nMCMC. Experiments on simulated data shows that the proposed method is similarly\naccurate as BIC in the no noise setting, but considerably more accurate when\nthere are noisy partial correlations. Furthermore, on real data the proposed\nmethod provides clustering results that are intuitively sensible, which is not\nalways the case when using BIC or its extensions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 12:06:03 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Andrade", "Daniel", ""], ["Takeda", "Akiko", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1806.06012", "submitter": "Debasis Kundu Professor", "authors": "Deepak Prajapati, Sharmistha Mitra, Debasis Kundu", "title": "A New Decision Theoretic Sampling Plan for Exponential Distribution\n  under Type-I Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new decision theoretic sampling plan (DSP) is proposed for\nType-I censored exponential distribution. The proposed DSP is based on a new\nestimator of the expected lifetime of an exponential distribution which always\nexists, unlike the usual maximum likelihood estimator. The DSP is a\nmodification of the Bayesian variable sampling plan of \\cite{Lam:1994}. An\noptimum DSP is derived in the sense that it minimizes the Bayes risk. In terms\nof the Bayes risks, it performs better than Lam's sampling plan and its\nperformance is as good as the Bayesian sampling plan of \\cite{LLH:2002},\nalthough implementation of the DSP is very simple. Analytically it is more\ntractable than the Bayesian sampling plan of \\cite{LLH:2002}, and it can be\neasily generalized for any other loss functions also. A finite algorithm is\nprovided to obtain the optimal plan and the corresponding minimum Bayes risk is\ncalculated. Extensive numerical comparisons with the optimal Bayesian sampling\nplan proposed by \\cite{LLH:2002} are made. The results have been extended for\nthree degree polynomial loss function and for Type-I hybrid censoring scheme.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:27:37 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Prajapati", "Deepak", ""], ["Mitra", "Sharmistha", ""], ["Kundu", "Debasis", ""]]}, {"id": "1806.06020", "submitter": "Thevaa Chandereng", "authors": "Thevaa Chandereng, Xiaodan Wei and Rick Chappell", "title": "Imbalanced Randomization in Clinical Trials", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8539", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomization is a common technique used in clinical trials to eliminate\npotential bias and confounders in a patient population. Equal allocation to\ntreatment groups is the standard due to its optimal efficiency in many cases.\nHowever, in certain scenarios, unequal allocation can improve efficiency. In\nsuperiority trials with more than two groups, the optimal randomization is not\nalways a balanced randomization. In non-inferiority trials, additive margin\nwith equal variance is the only instance with balanced randomization. Optimal\nrandomization for non-inferiority trials can be far from 1:1 and can greatly\nimprove efficiency, a fact which is commonly overlooked. A tool for sample size\ncalculation for non-inferiority trials with additive or multiplicative margin\nwith normal, binomial or Poisson distribution is available at\nhttp://www.statlab.wisc.edu/shiny/SSNI/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 15:36:57 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:29:47 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 22:12:38 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chandereng", "Thevaa", ""], ["Wei", "Xiaodan", ""], ["Chappell", "Rick", ""]]}, {"id": "1806.06118", "submitter": "Yi Zhao", "authors": "Yi Zhao and Martin A. Lindquist and Brian S. Caffo", "title": "Sparse Principal Component based High-Dimensional Mediation Analysis", "comments": "24 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1016/j.csda.2019.106835", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis aims to quantify the intermediate effect of a\nmediator on the causal pathway from treatment to outcome. With multiple\nmediators, which are potentially causally dependent, the possible decomposition\nof pathway effects grows exponentially with the number of mediators. Huang and\nPan (2016) introduced a principal component analysis (PCA) based approach to\naddress this challenge, in which the transformed mediators are conditionally\nindependent given the orthogonality of the PCs. However, the transformed\nmediator PCs, which are linear combinations of original mediators, are\ndifficult to interpret. In this study, we propose a sparse high-dimensional\nmediation analysis approach by adopting the sparse PCA method introduced by Zou\nand others (2006) to the mediation setting. We apply the approach to a\ntask-based functional magnetic resonance imaging study, and show that our\nproposed method is able to detect biologically meaningful results related to\nthe identified mediator.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:31:41 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhao", "Yi", ""], ["Lindquist", "Martin A.", ""], ["Caffo", "Brian S.", ""]]}, {"id": "1806.06194", "submitter": "Jianhua Xu", "authors": "Jianhua Xu", "title": "Wavelet regression: An approach for undertaking multi-time scale\n  analyses of hydro-climate relationships", "comments": "8 pages, 3 figures", "journal-ref": "MethodsX, 2018, Volume 5, Pages 561-568", "doi": "10.1016/j.mex.2018.05.005", "report-no": null, "categories": "stat.AP nlin.PS physics.ao-ph physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous studies showed that hydro-climate processes are stochastic and\ncomplex systems, and it is difficult to discover the hidden patterns in the all\nnon-stationary data and thoroughly understand the hydro-climate relationships.\nFor the purpose to show multi-time scale responses of a hydrological variable\nto climate change, we developed an integrated approach by combining wavelet\nanalysis and regression method, which is called wavelet regression (WR). The\ncustomization and the advantage of this approach over the existing methods are\npresented below: (1) The patterns in the data series of a hydrological variable\nand its related climatic factors are revealed by the wavelet analysis at\ndifferent time scales. (2) The hydro-climate relationship of each pattern is\nrevealed by the regression method based on the results of wavelet analysis. (3)\nThe advantage of this approach over the existing methods is that the approach\nprovides a routing to discover the hidden patterns in the stochastic and\nnon-stationary data and quantitatively describe the hydro-climate relationships\nat different time scales.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 07:01:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Xu", "Jianhua", ""]]}, {"id": "1806.06285", "submitter": "Manav Vohra", "authors": "Manav Vohra, Alen Alexanderian, Cosmin Safta, and Sankaran Mahadevan", "title": "Sensitivity-driven adaptive construction of reduced-space surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a systematic approach for surrogate model construction in reduced\ninput parameter spaces. A sparse set of model evaluations in the original input\nspace is used to approximate derivative based global sensitivity measures\n(DGSMs) for individual uncertain inputs of the model. An iterative screening\nprocedure is developed that exploits DGSM estimates in order to identify the\nunimportant inputs. The screening procedure forms an integral part of an\noverall framework for adaptive construction of a surrogate in the reduced\nspace. The framework is tested for computational efficiency through an initial\nimplementation in simple test cases such as the classic Borehole function, and\na semilinear elliptic PDE with a random source term. The framework is then\ndeployed for a realistic application from chemical kinetics, where we study the\nignition delay in an H2/O2 reaction mechanism with 19 uncertain rate constants.\nIt is observed that significant computational gains can be attained by\nconstructing accurate low-dimensional surrogates using the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 19:38:27 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Vohra", "Manav", ""], ["Alexanderian", "Alen", ""], ["Safta", "Cosmin", ""], ["Mahadevan", "Sankaran", ""]]}, {"id": "1806.06380", "submitter": "Jianhua Xu", "authors": "Chunmeng Wei, Jianhua Xu", "title": "Warming trend in cold season of the Yangtze River Delta and its\n  correlation with Siberian high", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the meteorological data from 1960 to 2010, we investigated the\ntemperature variation in the Yangtze River Delta (YRD) by using Mann-Kendall\nnonparametric test and explored the correlation between the temperature in the\ncold season and the Siberian high intensity (SHI) by using correlation analysis\nmethod. The main results are that (a) the temperature in YRD increased\nremarkably during the study period, (b) the warming trend in the cold season\nmade the higher contribution to annual warming, and (c) there was a significant\nnegative correlation between the temperature in the cold season and the SHI.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 13:20:55 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Wei", "Chunmeng", ""], ["Xu", "Jianhua", ""]]}, {"id": "1806.06403", "submitter": "Roberto de la Cruz", "authors": "Roberto de la Cruz and Jan-Ulrich Kreft", "title": "Geometric mean extension for data sets with zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are numerous examples in different research fields where the use of the\ngeometric mean is more appropriate than the arithmetic mean. However, the\ngeometric mean has a serious limitation in comparison with the arithmetic mean.\nMeans are used to summarize the information in a large set of values in a\nsingle number; yet, the geometric mean of a data set with at least one zero is\nalways zero. As a result, the geometric mean does not capture any information\nabout the non-zero values. The purpose of this short contribution is to review\nsolutions proposed in the literature that enable the computation of the\ngeometric mean of data sets containing zeros and to show that they do not\nfulfil the `recovery' or `monotonicity' conditions that we define. The standard\ngeometric mean should be recovered from the modified geometric mean if the data\nset does not contain any zeros (recovery condition). Also, if the values of an\nordered data set are greater one by one than the values of another data set\nthen the modified geometric mean of the first data set must be greater than the\nmodified geometric mean of the second data set (monotonicity condition). We\nthen formulate a modified version of the geometric mean that can handle zeros\nwhile satisfying both desired conditions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:00:45 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 14:46:35 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["de la Cruz", "Roberto", ""], ["Kreft", "Jan-Ulrich", ""]]}, {"id": "1806.06460", "submitter": "Jan Mandel", "authors": "Adam K. Kochanski, Aim\\'e Fournier, and Jan Mandel", "title": "Experimental Design of a Prescribed Burn Instrumentation", "comments": "35 pages, 4 tables, 28 figures", "journal-ref": "Atmosphere 2018, 9, 296", "doi": "10.3390/atmos9080296", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational data collected during experiments, such as the planned Fire and\nSmoke Model Evaluation Experiment (FASMEE), are critical for progressing and\ntransitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM\ninto operational use. Historical meteorological data, representing typical\nweather conditions for the anticipated burn locations and times, have been\nprocessed to initialize and run a set of simulations representing the planned\nexperimental burns. Based on an analysis of these numerical simulations, this\npaper provides recommendations on the experimental setup that include the\nignition procedures, size and duration of the burns, and optimal sensor\nplacement. New techniques are developed to initialize coupled fire-atmosphere\nsimulations with weather conditions typical of the planned burn locations and\ntime of the year. Analysis of variation and sensitivity analysis of simulation\ndesign to model parameters by repeated Latin Hypercube Sampling are used to\nassess the locations of the sensors. The simulations provide the locations of\nthe measurements that maximize the expected variation of the sensor outputs\nwith the model parameters.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 22:36:59 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 04:51:26 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kochanski", "Adam K.", ""], ["Fournier", "Aim\u00e9", ""], ["Mandel", "Jan", ""]]}, {"id": "1806.06551", "submitter": "Lubna Amro", "authors": "Burim Ramosaj, Lubna Amro, Markus Pauly", "title": "A cautionary tale on using imputation methods for inference in matched\n  pairs design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation procedures in biomedical fields have turned into statistical\npractice, since further analyses can be conducted ignoring the former presence\nof missing values. In particular, non-parametric imputation schemes like the\nrandom forest or a combination with the stochastic gradient boosting have shown\nfavorable imputation performance compared to the more traditionally used MICE\nprocedure. However, their effect on valid statistical inference has not been\nanalyzed so far. This paper closes this gap by investigating their validity for\ninferring mean differences in incompletely observed pairs while opposing them\nto a recent approach that only works with the given observations at hand. Our\nfindings indicate that machine learning schemes for (multiply) imputing missing\nvalues may inflate type-I-error or result in comparably low power in small to\nmoderate matched pairs, even after modifying the test statistics using Rubin's\nmultiple imputation rule. In addition to an extensive simulation study, an\nillustrative data example from a breast cancer gene study has been considered.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:48:32 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 13:50:20 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Ramosaj", "Burim", ""], ["Amro", "Lubna", ""], ["Pauly", "Markus", ""]]}, {"id": "1806.06696", "submitter": "Fan Bu", "authors": "Fan Bu, Sonia Xu, Katherine Heller, and Alexander Volfovsky", "title": "SMOGS: Social Network Metrics of Game Success", "comments": null, "journal-ref": "PMLR 2019 89:2406-2414", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops metrics from a social network perspective that are\ndirectly translatable to the outcome of a basketball game. We extend a\nstate-of-the-art multi-resolution stochastic process approach to modeling\nbasketball by modeling passes between teammates as directed dynamic relational\nlinks on a network and introduce multiplicative latent factors to study\nhigher-order patterns in players' interactions that distinguish a successful\ngame from a loss. Parameters are estimated using a Markov chain Monte Carlo\nsampler. Results in simulation experiments suggest that the sampling scheme is\neffective in recovering the parameters. We then apply the model to the first\nhigh-resolution optical tracking dataset collected in college basketball games.\nThe learned latent factors demonstrate significant differences between players'\npassing and receiving tendencies in a loss than those in a win. The model is\napplicable to team sports other than basketball, as well as other time-varying\nnetwork observations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:56:13 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bu", "Fan", ""], ["Xu", "Sonia", ""], ["Heller", "Katherine", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1806.06777", "submitter": "Li Ma", "authors": "Shai Gorsky and Li Ma", "title": "Multiscale Fisher's Independence Test for Multivariate Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying dependency in multivariate data is a common inference task that\narises in numerous applications. However, existing nonparametric independence\ntests typically require computation that scales at least quadratically with the\nsample size, making it difficult to apply them to massive data. Moreover,\nresampling is usually necessary to evaluate the statistical significance of the\nresulting test statistics at finite sample sizes, further worsening the\ncomputational burden. We introduce a scalable, resampling-free approach to\ntesting the independence between two random vectors by breaking down the task\ninto simple univariate tests of independence on a collection of 2x2 contingency\ntables constructed through sequential coarse-to-fine discretization of the\nsample space, transforming the inference task into a multiple testing problem\nthat can be completed with almost linear complexity with respect to the sample\nsize. To address increasing dimensionality, we introduce a coarse-to-fine\nsequential adaptive procedure that exploits the spatial features of dependency\nstructures to more effectively examine the sample space. We derive a\nfinite-sample theory that guarantees the inferential validity of our adaptive\nprocedure at any given sample size. In particular, we show that our approach\ncan achieve strong control of the family-wise error rate without resampling or\nlarge-sample approximation. We demonstrate the substantial computational\nadvantage of the procedure in comparison to existing approaches as well as its\ndecent statistical power under various dependency scenarios through an\nextensive simulation study, and illustrate how the divide-and-conquer nature of\nthe procedure can be exploited to not just test independence but to learn the\nnature of the underlying dependency. Finally, we demonstrate the use of our\nmethod through analyzing a large data set from a flow cytometry experiment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:38:54 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 19:02:19 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 13:48:05 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:13:48 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 17:21:40 GMT"}, {"version": "v6", "created": "Tue, 14 Jan 2020 19:09:36 GMT"}, {"version": "v7", "created": "Wed, 7 Jul 2021 11:37:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Gorsky", "Shai", ""], ["Ma", "Li", ""]]}, {"id": "1806.06974", "submitter": "Glen DePalma", "authors": "Glen DePalma and Bruce A. Craig", "title": "Bayesian monotonic errors-in-variables models with applications to\n  pathogen susceptibility testing", "comments": null, "journal-ref": "Statistics in Medicine. 2018. 37:478-502", "doi": "10.1002/sim.7533", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug dilution (MIC) and disk diffusion (DIA) are the two most common\nantimicrobial susceptibility assays used by hospitals and clinics to determine\nan unknown pathogen's susceptibility to various antibiotics. Since only one\nassay is commonly used, it is important that the two assays give similar\nresults. Calibration of the DIA assay to the MIC assay is typically done using\nthe error-rate bounded method, which selects DIA breakpoints that minimize the\nobserved discrepancies between the two assays. In 2000, Craig proposed a\nmodel-based approach that specifically models the measurement error and\nrounding processes of each assay, the underlying pathogen distribution, and the\ntrue monotonic relationship between the two assays. The two assays are then\ncalibrated by focusing on matching the probabilities of correct classification\n(susceptible, indeterminant, and resistant). This approach results in greater\nprecision and accuracy for estimating DIA breakpoints. In this paper, we expand\nthe flexibility of the model-based method by introducing a Bayesian\nfour-parameter logistic model (extending Craig's original three-parameter\nmodel) as well as a Bayesian nonparametric spline model to describe the\nrelationship between the two assays. We propose two ways to handle spline knot\nselection, considering many equally-spaced knots but restricting overfitting\nvia a random walk prior and treating the number and location of knots as\nadditional unknown parameters. We demonstrate the two approaches via a series\nof simulation studies and apply the methods to two real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:35:36 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["DePalma", "Glen", ""], ["Craig", "Bruce A.", ""]]}, {"id": "1806.07016", "submitter": "Michael Gechter", "authors": "Michael Gechter, Cyrus Samii, Rajeev Dehejia, Cristian Pop-Eleches", "title": "Evaluating Ex Ante Counterfactual Predictions Using Ex Post Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a formal, decision-based method for comparing the performance of\ncounterfactual treatment regime predictions using the results of experiments\nthat give relevant information on the distribution of treated outcomes. Our\napproach allows us to quantify and assess the statistical significance of\ndifferential performance for optimal treatment regimes estimated from\nstructural models, extrapolated treatment effects, expert opinion, and other\nmethods. We apply our method to evaluate optimal treatment regimes for\nconditional cash transfer programs across countries where predictions are\ngenerated using data from experimental evaluations in other countries and\npre-program data in the country of interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:33:42 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 19:02:26 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gechter", "Michael", ""], ["Samii", "Cyrus", ""], ["Dehejia", "Rajeev", ""], ["Pop-Eleches", "Cristian", ""]]}, {"id": "1806.07274", "submitter": "Vincent Chin", "authors": "Vincent Chin, David Gunawan, Denzil G. Fiebig, Robert Kohn, Scott A.\n  Sisson", "title": "Efficient data augmentation for multivariate probit models with panel\n  data: An application to general practitioner decision-making about\n  contraceptives", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12393", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of estimating a multivariate probit model\nin a panel data setting with emphasis on sampling a high-dimensional\ncorrelation matrix and improving the overall efficiency of the data\naugmentation approach. We reparameterise the correlation matrix in a principled\nway and then carry out efficient Bayesian inference using Hamiltonian Monte\nCarlo. We also propose a novel antithetic variable method to generate samples\nfrom the posterior distribution of the random effects and regression\ncoefficients, resulting in significant gains in efficiency. We apply the\nmethodology by analysing stated preference data obtained from Australian\ngeneral practitioners evaluating alternative contraceptive products. Our\nanalysis suggests that the joint probability of discussing combinations of\ncontraceptive products with a patient shows medical practice variation among\nthe general practitioners, which indicates some resistance to even discuss\nthese products, let alone recommend them.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:22:29 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 09:06:02 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Chin", "Vincent", ""], ["Gunawan", "David", ""], ["Fiebig", "Denzil G.", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1806.07320", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Simultaneous Signal Subspace Rank and Model Selection with an\n  Application to Single-snapshot Source Localization", "comments": "5 pages, 4 figures, To appear in the Proceedings of the 26th European\n  Signal Processing Conference (EUSIPCO 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CV math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for model selection in linear regression\nby utilizing the solution path of $\\ell_1$ regularized least-squares (LS)\napproach (i.e., Lasso). This method applies the complex-valued least angle\nregression and shrinkage (c-LARS) algorithm coupled with a generalized\ninformation criterion (GIC) and referred to as the c-LARS-GIC method.\nc-LARS-GIC is a two-stage procedure, where firstly precise values of the\nregularization parameter, called knots, at which a new predictor variable\nenters (or leaves) the active set are computed in the Lasso solution path.\nActive sets provide a nested sequence of regression models and GIC then selects\nthe best model. The sparsity order of the chosen model serves as an estimate of\nthe model order and the LS fit based only on the active set of the model\nprovides an estimate of the regression parameter vector. We then consider a\nsource localization problem, where the aim is to detect the number of impinging\nsource waveforms at a sensor array as well to estimate their\ndirection-of-arrivals (DoA-s) using only a single-snapshot measurement. We\nillustrate via simulations that, after formulating the problem as a grid-based\nsparse signal reconstruction problem, the proposed c-LARS-GIC method detects\nthe number of sources with high probability while at the same time it provides\naccurate estimates of source locations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:01:48 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1806.07427", "submitter": "Anshul Agarwal", "authors": "Anshul Agarwal", "title": "Validation of Inventory models for Single-echelon Supply Chain using\n  Discrete-event Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inventory decision frameworks proposed in the literature for single-echelon\nsupply chain systems rely on assumptions to obtain closed form expressions. In\nparticular, two such frameworks - one conventional and the other with a demand\nundershoot - determine optimal reorder point for a desired $\\beta$ or Type-II\nservice level. In this work we assess the accuracy and applicability of these\nframeworks with the help of a discrete event simulation framework developed in\nSimPy. While in several cases the closed form literature models under-predict\nthe service level, and thus result in a higher reorder point and safety stock,\nwe observe some situations where model predicted service levels match the\nsimulated results with statistical significance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 19:02:25 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Agarwal", "Anshul", ""]]}, {"id": "1806.07440", "submitter": "Lucas Massaroppe Dr.", "authors": "Lucas Massaroppe and Luiz A. Baccal\\'a", "title": "Kernel Methods for Nonlinear Connectivity Detection", "comments": "14 pages, 14 figures, preliminary version being prepared for\n  submission to a refereed journal", "journal-ref": null, "doi": "10.3390/e21060610", "report-no": null, "categories": "eess.SP math.DS stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the presence of nonlinear coupling between time\nseries may be detected employing kernel feature space representations alone\ndispensing with the need to go back to solve the pre-image problem to gauge\nmodel adequacy. As a consequence, the canonical methodology for model\nconstruction, diagnostics, and Granger connectivity inference applies with no\nchange other than computation using kernels in lieu of second-order moments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 19:35:52 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Massaroppe", "Lucas", ""], ["Baccal\u00e1", "Luiz A.", ""]]}, {"id": "1806.07605", "submitter": "Alice Le Brigant", "authors": "Alice Le Brigant (ENAC), St\\'ephane Puechmorel (ENAC)", "title": "Optimal Riemannian quantization with an application to air traffic\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of optimal quantization is to find the best approximation of a\nprobability distribution by a discrete measure with finite support. When\ndealing with empirical distributions, this boils down to finding the best\nsummary of the data by a smaller number of points, and automatically yields a\nK-means-type clustering. In this paper, we introduce Competitive Learning\nRiemannian Quantization (CLRQ), an online algorithm that computes the optimal\nsummary when the data does not belong to a vector space, but rather a\nRiemannian manifold. We prove its convergence and show simulated examples on\nthe sphere and the hyperbolic plane. We also provide an application to real\ndata by using CLRQ to create summaries of images of covariance matrices\nestimated from air traffic images. These summaries are representative of the\nair traffic complexity and yield clusterings of the airspaces into zones that\nare homogeneous with respect to that criterion. They can then be compared using\ndiscrete optimal transport and be further used as inputs of a machine learning\nalgorithm or as indexes in a traffic database.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 08:28:22 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 09:42:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Brigant", "Alice Le", "", "ENAC"], ["Puechmorel", "St\u00e9phane", "", "ENAC"]]}, {"id": "1806.08059", "submitter": "Andrew Karl", "authors": "Andrew T. Karl", "title": "Avoiding Bias Due to Nonrandom Scheduling When Modeling Trends in\n  Home-Field Advantage", "comments": "The discussion on bias in mixed model estimators that appears in this\n  paper has been superseded by arXiv:2003.08087", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for estimating home-field advantage (HFA) include\nmodeling the difference between home and away scores as a function of the\ndifference between home and away team ratings that are treated either as fixed\nor random effects. We uncover an upward bias in the mixed model HFA estimates\nthat is due to the nonrandom structure of the schedule -- and thus the random\neffect design matrix -- and explore why the fixed effects model is not subject\nto the same bias. Intraconference HFAs and standard errors are calculated for\neach of 3 college sports and 3 professional sports over 18 seasons and then\nfitted with conference-specific slopes and intercepts to measure the potential\nlinear population trend in HFA.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 03:56:58 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 22:27:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Karl", "Andrew T.", ""]]}, {"id": "1806.08069", "submitter": "Young-Jin Park", "authors": "Young-Jin Park, Piyush M. Tagade, Han-Lim Choi", "title": "Deep Gaussian Process-Based Bayesian Inference for Contaminant Source\n  Localization", "comments": "28 pages, 14 figures, submitted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2867687", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Bayesian framework for localization of multiple sources\nin the event of accidental hazardous contaminant release. The framework\nassimilates sensor measurements of the contaminant concentration with an\nintegrated multizone computational fluid dynamics (multizone-CFD) based\ncontaminant fate and transport model. To ensure online tractability, the\nframework uses deep Gaussian process (DGP) based emulator of the multizone-CFD\nmodel. To effectively represent the transient response of the multizone-CFD\nmodel, the DGP emulator is reformulated using a matrix-variate Gaussian process\nprior. The resultant deep matrix-variate Gaussian process emulator (DMGPE) is\nused to define the likelihood of the Bayesian framework, while Markov Chain\nMonte Carlo approach is used to sample from the posterior distribution. The\nproposed method is evaluated for single and multiple contaminant sources\nlocalization tasks modeled by CONTAM simulator in a single-story building of 30\nzones, demonstrating that proposed approach accurately perform inference on\nlocations of contaminant sources. Moreover, the DMGP emulator outperforms both\nGP and DGP emulator with fewer number of hyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 05:32:42 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Park", "Young-Jin", ""], ["Tagade", "Piyush M.", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1806.08324", "submitter": "Anand Avati", "authors": "Anand Avati, Tony Duan, Sharon Zhou, Kenneth Jung, Nigam H. Shah and\n  Andrew Ng", "title": "Countdown Regression: Sharp and Calibrated Survival Predictions", "comments": "UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic survival predictions from models trained with Maximum\nLikelihood Estimation (MLE) can have high, and sometimes unacceptably high\nvariance. The field of meteorology, where the paradigm of maximizing sharpness\nsubject to calibration is popular, has addressed this problem by using scoring\nrules beyond MLE, such as the Continuous Ranked Probability Score (CRPS). In\nthis paper we present the \\emph{Survival-CRPS}, a generalization of the CRPS to\nthe survival prediction setting, with right-censored and interval-censored\nvariants. We evaluate our ideas on the mortality prediction task using two\ndifferent Electronic Health Record (EHR) data sets (STARR and MIMIC-III)\ncovering millions of patients, with suitable deep neural network architectures:\na Recurrent Neural Network (RNN) for STARR and a Fully Connected Network (FCN)\nfor MIMIC-III. We compare results between the two scoring rules while keeping\nthe network architecture and data fixed, and show that models trained with\nSurvival-CRPS result in sharper predictive distributions compared to those\ntrained by MLE, while still maintaining calibration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:12:10 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 23:12:23 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Avati", "Anand", ""], ["Duan", "Tony", ""], ["Zhou", "Sharon", ""], ["Jung", "Kenneth", ""], ["Shah", "Nigam H.", ""], ["Ng", "Andrew", ""]]}, {"id": "1806.08433", "submitter": "Henry Laniado Professor", "authors": "Juan C. Duque, Henry Laniado and Adriano Polo", "title": "S-maup: Statistic test to measure the sensitivity to the Modifiable\n  Areal Unit Problem", "comments": "Paper with 10 pages and 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0207377", "report-no": "2303867", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a nonparametric statistical test, $S$-maup, to measure the\nsensitivity of a spatially intensive variable to the effects of the Modifiable\nAreal Unit Problem (MAUP). $S$-maup is the first statistic of its type and\nfocuses on determining how much the distribution of the variable, at its\nhighest level of spatial disaggregation, will change when it is spatially\naggregated. Through a computational experiment, we obtain the basis for the\ndesign of the statistical test under the null hypothesis of non-sensitivity to\nMAUP. We performed a simulation study for approaching the empirical\ndistribution of the statistical test, obtaining its critical values, and\ncomputing its power and size. The results indicate that the power of the\nstatistic is good if the sample (number of areas) grows, and in general, the\nsize decreases with increasing sample number. Finally, an empirical application\nis made using the Mincer equation in South Africa.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 21:56:44 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Duque", "Juan C.", ""], ["Laniado", "Henry", ""], ["Polo", "Adriano", ""]]}, {"id": "1806.08468", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Jonathan C. Spencer, Ziqi Chen, Christopher G. Brinton,\n  Mung Chiang", "title": "Personalized Thread Recommendation for MOOC Discussion Forums", "comments": "To appear at ECML-PKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social learning, i.e., students learning from each other through social\ninteractions, has the potential to significantly scale up instruction in online\neducation. In many cases, such as in massive open online courses (MOOCs),\nsocial learning is facilitated through discussion forums hosted by course\nproviders. In this paper, we propose a probabilistic model for the process of\nlearners posting on such forums, using point processes. Different from existing\nworks, our method integrates topic modeling of the post text, timescale\nmodeling of the decay in post activity over time, and learner topic interest\nmodeling into a single model, and infers this information from user data. Our\nmethod also varies the excitation levels induced by posts according to the\nthread structure, to reflect typical notification settings in discussion\nforums. We experimentally validate the proposed model on three real-world MOOC\ndatasets, with the largest one containing up to 6,000 learners making 40,000\nposts in 5,000 threads. Results show that our model excels at thread\nrecommendation, achieving significant improvement over a number of baselines,\nthus showing promise of being able to direct learners to threads that they are\ninterested in more efficiently. Moreover, we demonstrate analytics that our\nmodel parameters can provide, such as the timescales of different topic\ncategories in a course.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 02:16:14 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Lan", "Andrew S.", ""], ["Spencer", "Jonathan C.", ""], ["Chen", "Ziqi", ""], ["Brinton", "Christopher G.", ""], ["Chiang", "Mung", ""]]}, {"id": "1806.08507", "submitter": "Haidar Almohri", "authors": "Haidar Almohri, Arash Ali Amini, Ratna Babu Chinnam", "title": "Grouped Mixture of Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finite Mixture of Regressions (FMR) models are among the most widely used\napproaches in dealing with the heterogeneity among the observations in\nregression problems. One of the limitations of current approaches is their\ninability to incorporate group structure in data when available. In some\napplications, it is desired to cluster groups of observations together rather\nthan the individual ones. In this work, we extend the FMR framework to allow\nfor group structure among observations, and call the resulting model the\nGrouped Mixture of Regressions (GMR). We derive a fast fitting algorithm based\non the Expectation-Maximization (EM) idea. We also show how the group structure\ncan improve prediction by sharing information among members of each group, as\nreflected in the posterior predictive density under GMR. %that they don't\nconsider clustering the data when there is group structure. In other words,\nsometimes it is desired to force the algorithm to cluster groups/blocks of\nobservations, instead of individual observations. %In this work, we propose a\nmaximum likelihood approach to cluster groups of observations. We call this\nalgorithm Group Mixture of Regressions (GMR). Expectation Maximization (EM) is\nemployed to maximize the likelihood. Posterior prediction density for\npredicting new observations is also derived and presented.\n  The performance of the approach is assessed using both synthetic data as well\nas a real-world example.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 06:16:03 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Almohri", "Haidar", ""], ["Amini", "Arash Ali", ""], ["Chinnam", "Ratna Babu", ""]]}, {"id": "1806.08819", "submitter": "Benjamin Huynh", "authors": "Benjamin Q. Huynh and Sanjay Basu", "title": "Forecasting Internally Displaced Population Migration Patterns in Syria\n  and Yemen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Armed conflict has led to an unprecedented number of internally displaced\npersons (IDPs) - individuals who are forced out of their homes but remain\nwithin their country. IDPs often urgently require shelter, food, and\nhealthcare, yet prediction of when large fluxes of IDPs will cross into an area\nremains a major challenge for aid delivery organizations. Accurate forecasting\nof IDP migration would empower humanitarian aid groups to more effectively\nallocate resources during conflicts. We show that monthly flow of IDPs from\nprovince to province in both Syria and Yemen can be accurately forecasted one\nmonth in advance, using publicly available data. We model monthly IDP flow\nusing data on food price, fuel price, wage, geospatial, and news data. We find\nthat machine learning approaches can more accurately forecast migration trends\nthan baseline persistence models. Our findings thus potentially enable\nproactive aid allocation for IDPs in anticipation of forecasted arrivals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:53:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Huynh", "Benjamin Q.", ""], ["Basu", "Sanjay", ""]]}, {"id": "1806.08915", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek", "title": "DALEX: explainers for complex predictive models", "comments": "12 pages", "journal-ref": "Journal of Machine Learning Research 19 (2018) 1-5", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is invaded by elastic, yet complex methods such as neural\nnetworks or ensembles (model stacking, boosting or bagging). Such methods are\nusually described by a large number of parameters or hyper parameters - a price\nthat one needs to pay for elasticity. The very number of parameters makes\nmodels hard to understand. This paper describes a consistent collection of\nexplainers for predictive models, a.k.a. black boxes. Each explainer is a\ntechnique for exploration of a black box model. Presented approaches are\nmodel-agnostic, what means that they extract useful information from any\npredictive method despite its internal structure. Each explainer is linked with\na specific aspect of a model. Some are useful in decomposing predictions, some\nserve better in understanding performance, while others are useful in\nunderstanding importance and conditional responses of a particular variable.\nEvery explainer presented in this paper works for a single model or for a\ncollection of models. In the latter case, models can be compared against each\nother. Such comparison helps to find strengths and weaknesses of different\napproaches and gives additional possibilities for model validation. Presented\nexplainers are implemented in the DALEX package for R. They are based on a\nuniform standardized grammar of model exploration which may be easily extended.\nThe current implementation supports the most popular frameworks for\nclassification and regression.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 06:28:38 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:15:54 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Biecek", "Przemyslaw", ""]]}, {"id": "1806.08998", "submitter": "Sirio Legramanti", "authors": "Sirio Legramanti", "title": "Bayesian Analysis of Privacy Attacks on GPS Trajectories", "comments": "6 pages, LaTeX; more concise and partially rewritten with respect to\n  the first version, but same results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of applications for sharing GPS trajectories raises serious\nprivacy concerns, in particular about users' home addresses. In this paper we\nshow that a Bayesian approach is natural and effective for a rigorous analysis\nof home-identification attacks and their countermeasures, in terms of privacy.\nWe focus on a family of countermeasures named \"privacy-region strategies\",\nconsisting in publishing each trajectory from the first exit to the last\nentrance from/into a privacy region. Their performance is studied through\nsimulations on Brownian motions.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 16:21:24 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 14:24:11 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Legramanti", "Sirio", ""]]}, {"id": "1806.09244", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Igor Oliveira, Renato L. F. Cunha, Bruno Silva, Marco A. S. Netto", "title": "A Scalable Machine Learning System for Pre-Season Agriculture Yield\n  Forecast", "comments": "8 pages, 5 figures, Submitted to 14th IEEE eScience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yield forecast is essential to agriculture stakeholders and can be obtained\nwith the use of machine learning models and data coming from multiple sources.\nMost solutions for yield forecast rely on NDVI (Normalized Difference\nVegetation Index) data, which is time-consuming to be acquired and processed.\nTo bring scalability for yield forecast, in the present paper we describe a\nsystem that incorporates satellite-derived precipitation and soil properties\ndatasets, seasonal climate forecasting data from physical models and other\nsources to produce a pre-season prediction of soybean/maize yield---with no\nneed of NDVI data. This system provides significantly useful results by the\nexempting the need for high-resolution remote-sensing data and allowing farmers\nto prepare for adverse climate influence on the crop cycle. In our studies, we\nforecast the soybean and maize yields for Brazil and USA, which corresponded to\n44% of the world's grain production in 2016. Results show the error metrics for\nsoybean and maize yield forecasts are comparable to similar systems that only\nprovide yield forecast information in the first weeks to months of the crop\ncycle.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 01:05:19 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 22:27:53 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Oliveira", "Igor", ""], ["Cunha", "Renato L. F.", ""], ["Silva", "Bruno", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "1806.09347", "submitter": "Mario Fordellone Dr", "authors": "Mario Fordellone, Andrea Bellincontro, Fabio Mencarelli", "title": "Partial least squares discriminant analysis: A dimensionality reduction\n  method to classify hyperspectral data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of more sophisticated spectroscopic methods allows\nacqui- sition of high dimensional datasets from which valuable information may\nbe extracted using multivariate statistical analyses, such as dimensionality\nreduction and automatic classification (supervised and unsupervised). In this\nwork, a supervised classification through a partial least squares discriminant\nanalysis (PLS-DA) is performed on the hy- perspectral data. The obtained\nresults are compared with those obtained by the most commonly used\nclassification approaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:37:27 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Fordellone", "Mario", ""], ["Bellincontro", "Andrea", ""], ["Mencarelli", "Fabio", ""]]}, {"id": "1806.09386", "submitter": "Maike Hohberg", "authors": "Maike Hohberg, Peter P\\\"utz, Thomas Kneib", "title": "Treatment effects beyond the mean using GAMLSS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces distributional regression, also known as generalized\nadditive models for location, scale and shape (GAMLSS), as a modeling framework\nfor analyzing treatment effects beyond the mean. By relating each parameter of\nthe response distribution to explanatory variables, GAMLSS model the treatment\neffect on the whole conditional distribution. Additionally, any nonnormal\noutcome and nonlinear effects of explanatory variables can be incorporated. We\nelaborate on the combination of GAMLSS with program evaluation methods in\neconomics and provide practical guidance on the usage of GAMLSS by reanalyzing\ndata from the Mexican \\textit{Progresa} program. Contrary to expectations, no\nsignificant effects of a cash transfer on the conditional inequality level\nbetween treatment and control group are found.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 11:16:27 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 10:32:49 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 06:31:39 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Hohberg", "Maike", ""], ["P\u00fctz", "Peter", ""], ["Kneib", "Thomas", ""]]}, {"id": "1806.09440", "submitter": "Petri Varvia", "authors": "Petri Varvia, Timo L\\\"ahivaara, Matti Maltamo, Petteri Packalen, Aku\n  Sepp\\\"anen", "title": "Gaussian process regression for forest attribute estimation from\n  airborne laser scanning data", "comments": "Accepted for publication in IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2018.2883495", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the analysis of airborne laser scanning (ALS) data often provides\nreliable estimates for certain forest stand attributes -- such as total volume\nor basal area -- there is still room for improvement, especially in estimating\nspecies-specific attributes. Moreover, while information on the estimate\nuncertainty would be useful in various economic and environmental analyses on\nforests, a computationally feasible framework for uncertainty quantifying in\nALS is still missing. In this article, the species-specific stand attribute\nestimation and uncertainty quantification (UQ) is approached using Gaussian\nprocess regression (GPR), which is a nonlinear and nonparametric machine\nlearning method. Multiple species-specific stand attributes are estimated\nsimultaneously: tree height, stem diameter, stem number, basal area, and stem\nvolume. The cross-validation results show that GPR yields on average an\nimprovement of 4.6\\% in estimate RMSE over a state-of-the-art k-nearest\nneighbors (kNN) implementation, negligible bias and well performing UQ\n(credible intervals), while being computationally fast. The performance\nadvantage over kNN and the feasibility of credible intervals persists even when\nsmaller training sets are used.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:17:35 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 10:14:43 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Varvia", "Petri", ""], ["L\u00e4hivaara", "Timo", ""], ["Maltamo", "Matti", ""], ["Packalen", "Petteri", ""], ["Sepp\u00e4nen", "Aku", ""]]}, {"id": "1806.09473", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Ryan R. Wilson, George M. Durner,\n  Todd C. Atwood", "title": "Accounting for phenology in the analysis of animal movement", "comments": "Correction to caption of Figure 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of animal tracking data provides an important source of\nscientific understanding and discovery in ecology. Observations of animal\ntrajectories using telemetry devices provide researchers with information about\nthe way animals interact with their environment and each other. For many\nspecies, specific geographical features in the landscape can have a strong\neffect on behavior. Such features may correspond to a single point (e.g., dens\nor kill sites), or to higher-dimensional subspaces (e.g., rivers or lakes).\nFeatures may be relatively static in time (e.g., coastlines or home-range\ncenters), or may be dynamic (e.g., sea ice extent or areas of high-quality\nforage for herbivores). We introduce a novel model for animal movement that\nincorporates active selection for dynamic features in a landscape.\n  Our approach is motivated by the study of polar bear (Ursus maritimus)\nmovement. During the sea ice melt season, polar bears spend much of their time\non sea ice above shallow, biologically productive water where they hunt seals.\nThe changing distribution and characteristics of sea ice throughout the late\nspring through early fall means that the location of valuable habitat is\nconstantly shifting. We develop a model for the movement of polar bears that\naccounts for the effect of this important landscape feature. We introduce a\ntwo-stage procedure for approximate Bayesian inference that allows us to\nanalyze over 300,000 observed locations of 186 polar bears from 2012--2016. We\nuse our proposed model to answer a particular question posed by wildlife\nmanagers who seek to cluster polar bears from the Beaufort and Chukchi seas\ninto sub-populations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:07:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 15:29:01 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 18:42:06 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Wilson", "Ryan R.", ""], ["Durner", "George M.", ""], ["Atwood", "Todd C.", ""]]}, {"id": "1806.09690", "submitter": "Alexander Petersen", "authors": "Alexander Petersen, Sean Deoni, and Hans-Georg M\\\"uller", "title": "Fr\\'echet Estimation of Time-Varying Covariance Matrices From Sparse\n  Data, With Application to the Regional Co-Evolution of Myelination in the\n  Developing Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing brain development for small infants is important for determining\nhow the human brain grows during the early period of life when the rate of\nbrain growth is at its peak. The development of MRI techniques has enabled the\nquantification of brain development. A key quantity that can be extracted from\nMRI measurements is the level of myelination, where myelin acts as an insulator\naround nerve fibers and its deployment makes nerve pulse propagation more\nefficient. The co-variation of myelin deployment across different brain regions\nprovides insights into the co-development of brain regions and can be assessed\nas a correlation matrix that varies with age. Typically, available data for\neach child are very sparse, due to the cost and logistic difficulties of\narranging MRI brain scans for infants. We showcase here a method where data per\nsubject are limited to measurements taken at only one random age, while aiming\nat the time-varying dynamics. This situation is encountered more generally in\ncross-sectional studies where one observes $p$-dimensional vectors at one\nrandom time point per subject and is interested in the $p \\times p$ correlation\nmatrix function over the time domain. The challenge is that at each observation\ntime one observes only a $p$-vector of measurements but not a covariance or\ncorrelation matrix. For such very sparse data, we develop a Fr\\'echet\nestimation method. Given a metric on the space of covariance matrices, the\nproposed method generates a matrix function where at each time the matrix is a\nnon-negative definite covariance matrix, for which we demonstrate consistency\nproperties. We discuss how this approach can be applied to myelin data in the\ndeveloping brain and what insights can be gained.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 20:24:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Petersen", "Alexander", ""], ["Deoni", "Sean", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1806.09692", "submitter": "Benjamin Goldstein", "authors": "Benjamin A. Goldstein, Matthew Phelan, Neha J. Pagidipati, Rury R.\n  Holman, Michael J. Pencina Elizabeth A Stuart", "title": "An Outcome Model Approach to Translating a Randomized Controlled Trial\n  Results to a Target Population", "comments": "2 Tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participants enrolled into randomized controlled trials (RCTs) often do not\nreflect real-world populations. Previous research in how best to translate RCT\nresults to target populations has focused on weighting RCT data to look like\nthe target data. Simulation work, however, has suggested that an outcome model\napproach may be preferable. Here we describe such an approach using source data\nfrom the 2x2 factorial NAVIGATOR trial which evaluated the impact of valsartan\nand nateglinide on cardiovascular outcomes and new-onset diabetes in a\npre-diabetic population. Our target data consisted of people with pre-diabetes\nserviced at our institution. We used Random Survival Forests to develop\nseparate outcome models for each of the 4 treatments, estimating the 5-year\nrisk difference for progression to diabetes and estimated the treatment effect\nin our local patient populations, as well as sub-populations, and the results\ncompared to the traditional weighting approach. Our models suggested that the\ntreatment effect for valsartan in our patient population was the same as in the\ntrial, whereas for nateglinide treatment effect was stronger than observed in\nthe original trial. Our effect estimates were more efficient than the weighting\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 20:33:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Goldstein", "Benjamin A.", ""], ["Phelan", "Matthew", ""], ["Pagidipati", "Neha J.", ""], ["Holman", "Rury R.", ""], ["Stuart", "Michael J. Pencina Elizabeth A", ""]]}, {"id": "1806.09736", "submitter": "Amir Karami", "authors": "Amir Karami and Noelle M. Pendergraft", "title": "Computational Analysis of Insurance Complaints: GEICO Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online environment has provided a great opportunity for insurance\npolicyholders to share their complaints with respect to different services.\nThese complaints can reveal valuable information for insurance companies who\nseek to improve their services; however, analyzing a huge number of online\ncomplaints is a complicated task for human and must involve computational\nmethods to create an efficient process. This research proposes a computational\napproach to characterize the major topics of a large number of online\ncomplaints. Our approach is based on using the topic modeling approach to\ndisclose the latent semantic of complaints. The proposed approach deployed on\nthousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates\nthat there are 30 major complains in four categories: (1) customer service, (2)\ninsurance coverage, paperwork, policy, and reports, (3) legal issues, and (4)\ncosts, estimates, and payments. This research approach can be used in other\napplications to explore a large number of reviews.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:12:14 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Karami", "Amir", ""], ["Pendergraft", "Noelle M.", ""]]}, {"id": "1806.09803", "submitter": "Pieter Segaert", "authors": "Peter Leoni and Pieter Segaert and Sven Serneels and Tim Verdonck", "title": "Multivariate Constrained Robust M-Regression for Shaping Forward Curves\n  in Electricity Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multivariate constrained robust M-regression (MCRM) method\nis developed to estimate shaping coefficients for electricity forward prices.\nAn important benefit of the new method is that model arbitrage can be ruled out\nat an elementary level, as all shaping coefficients are treated simultaneously.\nMoreover, the new method is robust to outliers, such that the provided results\nare stable and not sensitive to isolated sparks or dips in the market. An\nefficient algorithm is presented to estimate all shaping coefficients at a low\ncomputational cost. To illustrate its good performance, the method is applied\nto German electricity prices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:02:59 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Leoni", "Peter", ""], ["Segaert", "Pieter", ""], ["Serneels", "Sven", ""], ["Verdonck", "Tim", ""]]}, {"id": "1806.09866", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Julius Gordon", "title": "Long-term stock index forecasting based on text mining of regulatory\n  disclosures", "comments": "Accepted at Decision Support Systems journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Share valuations are known to adjust to new information entering the market,\nsuch as regulatory disclosures. We study whether the language of such news\nitems can improve short-term and especially long-term (24 months) forecasts of\nstock indices. For this purpose, this work utilizes predictive models suited to\nhigh-dimensional data and specifically compares techniques for data-driven and\nknowledge-driven dimensionality reduction in order to avoid overfitting. Our\nexperiments, based on 75,927 ad hoc announcements from 1996-2016, reveal the\nfollowing results: in the long run, text-based models succeed in reducing\nforecast errors below baseline predictions from historic lags at a\nstatistically significant level. Our research provides implications to business\napplications of decision-support in financial markets, especially given the\ngrowing prevalence of index ETFs (exchange traded funds).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 09:22:58 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Gordon", "Julius", ""]]}, {"id": "1806.09896", "submitter": "Roberta De Vito", "authors": "Roberta De Vito, Ruggero Bellio, Lorenzo Trippa and Giovanni\n  Parmigiani", "title": "Bayesian Multi-study Factor Analysis for High-throughput Biological Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a new modeling strategy for joint unsupervised analysis\nof multiple high-throughput biological studies. As in Multi-study Factor\nAnalysis, our goals are to identify both common factors shared across studies\nand study-specific factors. Our approach is motivated by the growing body of\nhigh-throughput studies in biomedical research, as exemplified by the\ncomprehensive set of expression data on breast tumors considered in our case\nstudy. To handle high-dimensional studies, we extend Multi-study Factor\nAnalysis using a Bayesian approach that imposes sparsity. Specifically, we\ngeneralize the sparse Bayesian infinite factor model to multiple studies. We\nalso devise novel solutions for the identification of the loading matrices: we\nrecover the loading matrices of interest ex-post, by adapting the orthogonal\nProcrustes approach. Computationally, we propose an efficient and fast Gibbs\nsampling approach. Through an extensive simulation analysis, we show that the\nproposed approach performs very well in a range of different scenarios, and\noutperforms standard Factor analysis in all the scenarios identifying\nreplicable signal in unsupervised genomic applications. The results of our\nanalysis of breast cancer gene expression across seven studies identified\nreplicable gene patterns, clearly related to well-known breast cancer pathways.\nAn R package is implemented and available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 10:56:54 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["De Vito", "Roberta", ""], ["Bellio", "Ruggero", ""], ["Trippa", "Lorenzo", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "1806.09949", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot and Anne De Moliner Anne and Camelia Goga", "title": "Conditional bias robust estimation of the total of curve data by\n  sampling in a finite population: an illustration on electricity load curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For marketing or power grid management purposes, many studies based on the\nanalysis of the total electricity consumption curves of groups of customers are\nnow carried out by electricity companies. Aggregated total or mean load curves\nare estimated using individual curves measured at fine time grid and collected\naccording to some sampling design. Due to the skewness of the distribution of\nelectricity consumptions, these samples often contain outlying curves which may\nhave an important impact on the usual estimation procedures. We introduce\nseveral robust estimators of the total consumption curve which are not\nsensitive to such outlying curves. These estimators are based on the\nconditional bias approach and robust functional methods. We also derive mean\nsquare error estimators of these robust estimators and finally, we evaluate and\ncompare the performance of the suggested estimators on Irish electricity data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 12:49:52 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 11:08:14 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Anne", "Anne De Moliner", ""], ["Goga", "Camelia", ""]]}, {"id": "1806.09996", "submitter": "Yong Luo", "authors": "Luo Yong", "title": "LOO and WAIC as Model Selection Methods for Polytomous Items", "comments": "39 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watanabe-Akaike information criterion (WAIC; Watanabe, 2010) and\nleave-one-out cross validation (LOO) are two fully Bayesian model selection\nmethods that have been shown to perform better than other traditional\ninformation-criterion based model selection methods such as AIC, BIC, and DIC\nin the context of dichotomous IRT model selection. In this paper, we\ninvestigated whether such superior performances of WAIC and LOO can be\ngeneralized to scenarios of polytomous IRT model selection. Specifically, we\nconducted a simulation study to compare the statistical power rates of WAIC and\nLOO with those of AIC, BIC, AICc, SABIC, and DIC in selecting the optimal model\namong a group of polytomous IRT ones. We also used a real data set to\ndemonstrate the use of LOO and WAIC for polytomous IRT model selection. The\nfindings suggest that while all seven methods have excellent statistical power\n(greater than 0.93) to identify the true polytomous IRT model, WAIC and LOO\nseem to have slightly lower statistical power than DIC, the performance of\nwhich is marginally inferior to those of the other four frequentist methods.\nKeywords: polytomous IRT, Bayesian, MCMC, model comparison.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:56:26 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Yong", "Luo", ""]]}, {"id": "1806.10009", "submitter": "Yong Luo", "authors": "Luo Yong", "title": "Item Parameter Recovery for the Two-Parameter Testlet Model with\n  Different Estimation Methods", "comments": "36 pages, 6 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The testlet model is a popular statistical approach widely used by\nresearchers and practitioners to address local item dependence (LID), a\nviolation of the local independence assumption in item response theory (IRT)\nwhich can cause various deleterious psychometric consequences. Same as other\npsychometric models, the utility of the testlet model relies heavily on\naccurate estimation of its model parameters. The two-parameter logistic (2PL)\ntestlet model has only been systematically investigated in the psychometric\nliterature regarding its model parameter recovery with one full information\nestimation methods, namely Markov chain Monte Carlo (MCMC) method, although\nthere are other estimation methods available such as marginal maximum\nlikelihood estimation (MMLE) and limited information estimation methods.\n  In the current study, a comprehensive simulation study was conducted to\ninvestigate how MCMC, MMLE, and one limited information estimation method\n(WLSMV), all implemented in Mplus, recovered the item parameters and the\ntestlet variance parameter of the 2PL testlet model. The manipulated factors\nwere sample size and testlet effect magnitude, and parameter recovery were\nevaluated with bias, standard error, and root mean square error. We found that\nthere were no statistically significant differences regarding parameter\nrecovery between the three methods. When both sample size and magnitude of\ntestlet variance were small, both WLSMV and MCMC had convergence issues, which\ndid not occur to MCMC regardless of sample size and testlet variance. A real\ndataset from a high-stakes test was used to demonstrate the estimation of the\n2PL testlet model with the three estimation methods.\n  Keywords: IRT, testlet model, estimation, full-information,\nlimited-information.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:07:36 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Yong", "Luo", ""]]}, {"id": "1806.10046", "submitter": "Lei Lin", "authors": "Lei Lin, Weizi Li, Srinivas Peeta", "title": "A Compressive Sensing Approach for Connected Vehicle Data Capture and\n  Recovery and its Impact on Travel Time Estimation", "comments": "Submitted to IEEE Intelligent Transportation Systems Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connected vehicles (CVs) can capture and transmit detailed data such as\nvehicle position and speed through vehicle-to-vehicle and\nvehicle-to-infrastructure communications. The wealth of CV data provides new\nopportunities to improve safety and mobility of transportation systems.\nHowever, it is likely to overburden storage and communication systems. To\nmitigate this issue, we propose a compressive sensing (CS) approach that allows\nCVs to capture and compress data in real-time and later recover the original\ndata accurately and efficiently. The approach is evaluated using two case\nstudies. In the first study, we use this approach to recapture 10 million CV\nBasic Safety Message (BSM) speed samples. It can recover the original speed\ndata with root-mean-squared error as low as 0.05. We also explore recovery\nperformance for other BSM variables. In the second study, a freeway traffic\nsimulation model is built to evaluate the impact of this approach on travel\ntime estimation. Multiple scenarios with various CV market penetration rates,\nOn-board Unit (OBU) capacities, compression ratios, arrival rate patterns, and\ndata capture rates are simulated. The results show that the approach provides\nmore accurate estimation than conventional data collection methods, through up\nto 65% relative reduction in travel time estimation error. Even when the\ncompression ratio is low, the approach can provide accurate estimation, thereby\nreducing OBU hardware costs. Further, it can improve accuracy of travel time\nestimation when CVs are in traffic congestion as it provides a broader\nspatial-temporal coverage of traffic conditions and can accurately and\nefficiently recover the original CV data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:05:51 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 18:35:58 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Lin", "Lei", ""], ["Li", "Weizi", ""], ["Peeta", "Srinivas", ""]]}, {"id": "1806.10111", "submitter": "Soroush Amirhashchi", "authors": "Amin Hassan Zadeh and Soroush Amirhashchi", "title": "Modelling Joint Lifetimes of Couples by Using Bivariate Phase-type\n  Distributions", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many insurance products and pension plans provide benefits which are related\nto couples, and thus under influence of the survival status of two lives. Some\nstudies show the future lifetime of couples is correlated. Three reasons are\navailable to confirm this fact: (1) catastrophe events that affect both lives,\n(2) the impact of spousal death and (3) the long-term association due to common\nlife style. Dependence between lifetimes of couples could have a financial\nimpact on insurance companies and pension plans providers. In this paper, we\nuse a health index called physiological age in a Markov process context by that\nwe model aging process of joint and last survivor statuses. Under this model,\nfuture joint lifetime of couples follows a bivariate phase-type distribution.\nThe model has physical interpretation and closed-form expressions for actuarial\nquantities and owns tractable computation for the other ones. We use the model\nto pricing products relevant to couples annuities and life insurances.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:15:32 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zadeh", "Amin Hassan", ""], ["Amirhashchi", "Soroush", ""]]}, {"id": "1806.10185", "submitter": "Diego A Martinez PhD", "authors": "Diego A. Martinez, Mehdi Jalalpour, David T. Efron, Scott R. Levin", "title": "How to Assess the Impact of Quality and Patient Safety Interventions\n  with Routinely Collected Longitudinal Data", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Measuring the effect of patient safety improvement efforts is needed to\ndetermine their value but is difficult due to the inherent complexities of\nhospital operations. In this paper, we show by case study how interrupted time\nseries design can be used to isolate and measure the impact of interventions\nwhile accounting for confounders often present in complex health delivery\nsystems. We searched for time-stamped data from electronic medical records and\noperating room information systems associated with perioperative patient flow\nin a large, urban, academic hospital in Baltimore, Maryland. We limited the\nsearched to those adult cases performed between January 2015 and March 2017. We\nused segmented regression and Box-Jenkins methods to measure the effect of\nperioperative throughput improvement efforts and account for the loss of high\nvolume surgeons, surgical volume, and occupancy. We identified a significant\ndecline of operating room exit delays of about 50%, achieved in 6 months and\nsustained over 14 months. By longitudinal assessment of intervention effects,\nrather than cross-sectional comparison, our measurement tool estimated and\nprovided inferences of change-points over time while taking into account the\nmagnitude of other latent systems factors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:44:23 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Martinez", "Diego A.", ""], ["Jalalpour", "Mehdi", ""], ["Efron", "David T.", ""], ["Levin", "Scott R.", ""]]}, {"id": "1806.10273", "submitter": "Helio M. de Oliveira", "authors": "H. M. de Oliveira and F. Chaves", "title": "von Mises Tapering: A Circular Data Windowing", "comments": "5 pages, 5 figures", "journal-ref": "XXXVI SIMPOSIO BRASILEIRO DE TELECOMUNICACOES E PROCESSAMENTO DE\n  SINAIS-SBrT2018", "doi": "10.14209/SBRT.2018.179", "report-no": null, "categories": "eess.SP cs.NA math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous standard windowing is revisited and a new taper shape is\nintroduced, which is based on the normal circular distribution by von Mises.\nContinuous-time windows are considered and their spectra obtained. A brief\ncomparison with classical window families is performed in terms of their\nspectral properties. These windows can be used as an alternative in spectral\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:30:34 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Chaves", "F.", ""]]}, {"id": "1806.10412", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini", "title": "Filtering Procedures for Sensor Data in Basketball", "comments": "19 pages, 11 figures", "journal-ref": "Metulini, R. (2017), Filtering Procedures for Sensor Data in\n  Basketball, Statistics \\& Applications. Vol. 2", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data Analytics help team sports' managers in their decisions by\nprocessing a number of different kind of data. With the advent of Information\nTechnologies, collecting, processing and storing big amounts of sport data in\ndifferent form became possible. A problem that often arises when using sport\ndata regards the need for automatic data cleaning procedures. In this paper we\ndevelop a data cleaning procedure for basketball which is based on players'\ntrajectories. Starting from a data matrix that tracks the movements of the\nplayers on the court at different moments in the game, we propose an algorithm\nto automatically drop inactive moments making use of available sensor data. The\nalgorithm also divides the game into sorted actions and labels them as\noffensive or defensive. The algorithm's parameters are validated using proper\nrobustness checks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:20:31 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Metulini", "Rodolfo", ""]]}, {"id": "1806.10639", "submitter": "Vianey Leos Barajas", "authors": "Vianey Leos-Barajas and Th\\'eo Michelot", "title": "An Introduction to Animal Movement Modeling with Hidden Markov Models\n  using Stan for Bayesian Inference", "comments": "29 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hidden Markov models (HMMs) are popular time series model in many fields\nincluding ecology, economics and genetics. HMMs can be defined over discrete or\ncontinuous time, though here we only cover the former. In the field of movement\necology in particular, HMMs have become a popular tool for the analysis of\nmovement data because of their ability to connect observed movement data to an\nunderlying latent process, generally interpreted as the animal's unobserved\nbehavior. Further, we model the tendency to persist in a given behavior over\ntime. Notation presented here will generally follow the format of Zucchini et\nal. (2016) and cover HMMs applied in an unsupervised case to animal movement\ndata, specifically positional data. We provide Stan code to analyze movement\ndata of the wild haggis as presented first in Michelot et al. (2016).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 18:41:15 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Leos-Barajas", "Vianey", ""], ["Michelot", "Th\u00e9o", ""]]}, {"id": "1806.10655", "submitter": "Ahmed Attia", "authors": "Ahmed Attia and Emil Constantinescu", "title": "An Optimal Experimental Design Framework for Adaptive Inflation and\n  Covariance Localization for Ensemble Filters", "comments": "31 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an optimal experimental design framework for adapting the\ncovariance inflation and localization in data assimilation problems. Covariance\ninflation and localization are ubiquitously employed to alleviate the effect of\nusing ensembles of finite sizes in all practical data assimilation systems. The\nchoice of both the inflation factor and the localization radius can have a\nsignificant impact on the performance of the assimilation scheme. These\nparameters are generally tuned by trial and error, rendering them expensive to\noptimize in practice. Spatially and temporally varying inflation parameter and\nlocalization radii have been recently proposed and have been empirically proven\nto enhance the performance of the employed assimilation filter. In this study,\nwe present a variational framework for adaptive tuning of the inflation and\nlocalization parameters. Each of these parameters is optimized independently,\nwith an objective to minimize the uncertainty in the posterior state. The\nproposed framework does not assume uncorrelated observations or prior errors\nand can in principle be applied without expert knowledge about the model and\nthe observations. Thus, it is adequate for handling dense as well as sparse\nobservational networks. We present the mathematical formulation, algorithmic\ndescription of the approach, and numerical experiments using the two-layer\nLorenz-96 model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:36:44 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 22:43:48 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Attia", "Ahmed", ""], ["Constantinescu", "Emil", ""]]}, {"id": "1806.10692", "submitter": "Arya Farahi", "authors": "Jacob Abernethy, Alex Chojnacki, Arya Farahi, Eric Schwartz, Jared\n  Webb", "title": "ActiveRemediation: The Search for Lead Pipes in Flint, Michigan", "comments": "10 pages, 10 figures, To appear in KDD 2018, For associated\n  promotional video, see https://www.youtube.com/watch?v=YbIn_axYu9E", "journal-ref": null, "doi": "10.1145/3219819.3219896", "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detail our ongoing work in Flint, Michigan to detect pipes made of lead\nand other hazardous metals. After elevated levels of lead were detected in\nresidents' drinking water, followed by an increase in blood lead levels in area\nchildren, the state and federal governments directed over $125 million to\nreplace water service lines, the pipes connecting each home to the water\nsystem. In the absence of accurate records, and with the high cost of\ndetermining buried pipe materials, we put forth a number of predictive and\nprocedural tools to aid in the search and removal of lead infrastructure.\nAlongside these statistical and machine learning approaches, we describe our\ninteractions with government officials in recommending homes for both\ninspection and replacement, with a focus on the statistical model that adapts\nto incoming information. Finally, in light of discussions about increased\nspending on infrastructure development by the federal government, we explore\nhow our approach generalizes beyond Flint to other municipalities nationwide.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 13:04:53 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 17:10:17 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Abernethy", "Jacob", ""], ["Chojnacki", "Alex", ""], ["Farahi", "Arya", ""], ["Schwartz", "Eric", ""], ["Webb", "Jared", ""]]}, {"id": "1806.10698", "submitter": "Yura Perov N", "authors": "Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie\n  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,\n  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri", "title": "A comparative study of artificial intelligence and human doctors for the\n  purpose of triage and diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online symptom checkers have significant potential to improve patient care,\nhowever their reliability and accuracy remain variable. We hypothesised that an\nartificial intelligence (AI) powered triage and diagnostic system would compare\nfavourably with human doctors with respect to triage and diagnostic accuracy.\nWe performed a prospective validation study of the accuracy and safety of an AI\npowered triage and diagnostic system. Identical cases were evaluated by both an\nAI system and human doctors. Differential diagnoses and triage outcomes were\nevaluated by an independent judge, who was blinded from knowing the source (AI\nsystem or human doctor) of the outcomes. Independently of these cases,\nvignettes from publicly available resources were also assessed to provide a\nbenchmark to previous studies and the diagnostic component of the MRCGP exam.\nOverall we found that the Babylon AI powered Triage and Diagnostic System was\nable to identify the condition modelled by a clinical vignette with accuracy\ncomparable to human doctors (in terms of precision and recall). In addition, we\nfound that the triage advice recommended by the AI System was, on average,\nsafer than that of human doctors, when compared to the ranges of acceptable\ntriage provided by independent expert judges, with only a minimal reduction in\nappropriateness.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 21:18:37 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Razzaki", "Salman", ""], ["Baker", "Adam", ""], ["Perov", "Yura", ""], ["Middleton", "Katherine", ""], ["Baxter", "Janie", ""], ["Mullarkey", "Daniel", ""], ["Sangar", "Davinder", ""], ["Taliercio", "Michael", ""], ["Butt", "Mobasher", ""], ["Majeed", "Azeem", ""], ["DoRosario", "Arnold", ""], ["Mahoney", "Megan", ""], ["Johri", "Saurabh", ""]]}, {"id": "1806.10749", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis", "title": "On Adaptive Linear-Quadratic Regulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY eess.SP math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of adaptive control policies is assessed through the regret with\nrespect to the optimal regulator, which reflects the increase in the operating\ncost due to uncertainty about the dynamics parameters. However, available\nresults in the literature do not provide a quantitative characterization of the\neffect of the unknown parameters on the regret. Further, there are problems\nregarding the efficient implementation of some of the existing adaptive\npolicies. Finally, results regarding the accuracy with which the system's\nparameters are identified are scarce and rather incomplete.\n  This study aims to comprehensively address these three issues. First, by\nintroducing a novel decomposition of adaptive policies, we establish a sharp\nexpression for the regret of an arbitrary policy in terms of the deviations\nfrom the optimal regulator. Second, we show that adaptive policies based on\nslight modifications of the Certainty Equivalence scheme are efficient.\nSpecifically, we establish a regret of (nearly) square-root rate for two\nfamilies of randomized adaptive policies. The presented regret bounds are\nobtained by using anti-concentration results on the random matrices employed\nfor randomizing the estimates of the unknown parameters. Moreover, we study the\nminimal additional information on dynamics matrices that using them the regret\nwill become of logarithmic order. Finally, the rates at which the unknown\nparameters of the system are being identified are presented.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:58:26 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 13:00:27 GMT"}, {"version": "v3", "created": "Sun, 30 Jun 2019 19:55:38 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2020 02:22:09 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1806.10794", "submitter": "Jianhua Xu", "authors": "Jianhua Xu, Nanshan Ai, Yan Lu, Yong Chen, Yiying Ling, Wenze Yue", "title": "Quantitative analysis on the disparity of regional economic development\n  in China and its evolution from 1952 to 2000", "comments": "15 pages, 3 figures", "journal-ref": "Regional Development Studies, 2003, 9: 115-129", "doi": null, "report-no": null, "categories": "stat.AP econ.EM nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domestic and foreign scholars have already done much research on regional\ndisparity and its evolution in China, but there is a big difference in\nconclusions. What is the reason for this? We think it is mainly due to\ndifferent analytic approaches, perspectives, spatial units, statistical\nindicators and different periods for studies. On the basis of previous analyses\nand findings, we have done some further quantitative computation and empirical\nstudy, and revealed the inter-provincial disparity and regional disparity of\neconomic development and their evolution trends from 1952-2000. The results\nshows that (a) Regional disparity in economic development in China, including\nthe inter-provincial disparity, inter-regional disparity and intra-regional\ndisparity, has existed for years; (b) Gini coefficient and Theil coefficient\nhave revealed a similar dynamic trend for comparative disparity in economic\ndevelopment between provinces in China. From 1952 to 1978, except for the\n\"Great Leap Forward\" period, comparative disparity basically assumes a upward\ntrend and it assumed a slowly downward trend from 1979 to1990. Afterwards from\n1991 to 2000 the disparity assumed a slowly upward trend again; (c) A\ncomparison between Shanghai and Guizhou shows that absolute inter-provincial\ndisparity has been quite big for years; and (d) The Hurst exponent (H=0.5) in\nthe period of 1966-1978 indicates that the comparative inter-provincial\ndisparity of economic development showed a random characteristic, and in the\nHurst exponent (H>0.5) in period of 1979-2000 indicates that in this period the\nevolution of the comparative inter-provincial disparity of economic development\nin China has a long-enduring characteristic.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 06:56:57 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Xu", "Jianhua", ""], ["Ai", "Nanshan", ""], ["Lu", "Yan", ""], ["Chen", "Yong", ""], ["Ling", "Yiying", ""], ["Yue", "Wenze", ""]]}, {"id": "1806.10812", "submitter": "David Pozo", "authors": "Irina Lukicheva, David Pozo, Alexander Kulikov", "title": "Cyberattack Detection in Intelligent Grids Using Non-linear Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric power grids are evolving towards intellectualization such as Smart\nGrids or active-adaptive networks. Intelligent power network implies usage of\nsensors, smart meters, electronic devices and sophisticated communication\nnetwork. This leads to a strong dependence on information and communication\nnetworking that are prone to threats of cyberattacks, which challenges power\nsystem reliability and efficiency. Thus, significant attention should be paid\nto the Smart Grids security. Recently, it has been proven that False Data\nInjection Attacks (FDIA) could corrupt results of State Estimation (SE) without\nnoticing, therefore, leading to a possible mis-operation of the whole power\nsystem. In this paper, we introduce an algorithm for detecting cyberattacks\nbased on non-linear filtering by using cyber-physical information from\nKirchhoff laws. The proposed algorithm only needs data from adjacent nodes,\ntherefore can be locally and distributed implemented. Also, it requires very\nlow computational effort so that it can be run online, and it is suitable for\nimplementation in existing or new ad-hoc low-cost devices. The proposed\nalgorithm could be helpful to increase power system awareness against FDIA\ncomplementing the current SE implementations. The efficiency of the proposed\nalgorithm has been proved by mathematical simulations and computer modeling in\nPSCAD software. Our results show that the proposed methodology can detect\ncyberattacks to the SE in 99.9% of the cases with very little false alarms on\nthe identification of spoiled measurements (4.6%).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 08:09:01 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 13:40:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Lukicheva", "Irina", ""], ["Pozo", "David", ""], ["Kulikov", "Alexander", ""]]}, {"id": "1806.10873", "submitter": "Seth Nabarro", "authors": "Seth Nabarro, Tristan Fletcher and John Shawe-Taylor", "title": "Spatiotemporal Prediction of Ambulance Demand using Gaussian Process\n  Regression", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting when and where ambulance call-outs occur can reduce\nresponse times and ensure the patient receives urgent care sooner. Here we\npresent a novel method for ambulance demand prediction using Gaussian process\nregression (GPR) in time and geographic space. The method exhibits superior\naccuracy to MEDIC, a method which has been used in industry. The use of GPR has\nadditional benefits such as the quantification of uncertainty with each\nprediction, the choice of kernel functions to encode prior knowledge and the\nability to capture spatial correlation. Measures to increase the utility of GPR\nin the current context, with large training sets and a Poisson-distributed\noutput, are outlined.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 10:45:26 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Nabarro", "Seth", ""], ["Fletcher", "Tristan", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1806.11237", "submitter": "Rodney Sparapani", "authors": "Rodney Sparapani, Brent R. Logan, Robert E. McCulloch and Purushottam\n  W. Laud", "title": "Nonparametric competing risks analysis using Bayesian Additive\n  Regression Trees (BART)", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time-to-event studies are complicated by the presence of competing\nrisks. Such data are often analyzed using Cox models for the cause specific\nhazard function or Fine-Gray models for the subdistribution hazard. In practice\nregression relationships in competing risks data with either strategy are often\ncomplex and may include nonlinear functions of covariates, interactions,\nhigh-dimensional parameter spaces and nonproportional cause specific or\nsubdistribution hazards. Model misspecification can lead to poor predictive\nperformance. To address these issues, we propose a novel approach to flexible\nprediction modeling of competing risks data using Bayesian Additive Regression\nTrees (BART). We study the simulation performance in two-sample scenarios as\nwell as a complex regression setting, and benchmark its performance against\nstandard regression techniques as well as random survival forests. We\nillustrate the use of the proposed method on a recently published study of\npatients undergoing hematopoietic stem cell transplantation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 00:49:03 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Sparapani", "Rodney", ""], ["Logan", "Brent R.", ""], ["McCulloch", "Robert E.", ""], ["Laud", "Purushottam W.", ""]]}, {"id": "1806.11253", "submitter": "Tauhid Zaman", "authors": "D. Scott Hunter and Tauhid Zaman", "title": "Optimizing Opinions with Stubborn Agents Under Time-Varying Dynamics", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimizing the placement of stubborn agents in a social network\nin order to maximally influence the population. We assume individuals in a\ndirected social network each have a latent opinion that evolves over time in\nresponse to social media posts by their neighbors. The individuals randomly\ncommunicate noisy versions of their latent opinion to their neighbors, causing\nthem to update their opinions using a time-varying update rule that has them\nbecome more stubborn with time and be less affected by new posts. The noisy\ncommunicated opinion and dynamic update rule are novel components of our model\nand they reflect realistic behaviors observed in many psychological studies.\n  We prove that under fairly general conditions, the opinions converge to an\nequilibrium in the presence of stubborn agents. What is surprising about this\nresult is that the equilibrium condition depends only upon the network\nstructure and the identity of the stubborn agents. The time-varying opinion\nupdate rules, which are heterogeneous across individuals, do not affect the\nequilibrium. We also prove bounds on the rate of convergence to this\nequilibrium.\n  We then use this model to develop a discrete optimization formulation for the\nproblem of maximally shifting the equilibrium opinions in a network by\ntargeting users with stubborn agents. We consider maximizing the mean opinion\nand also maximizing the number of individuals whose opinion exceeds a fixed\nthreshold. We show that the mean opinion is a monotone submodular function,\nallowing us to find a good solution using a greedy algorithm. We find that on\nreal social networks in Twitter consisting of tens of thousands of individuals,\na small number of stubborn agents can non-trivially influence the equilibrium\nopinions. Furthermore, we show that our greedy algorithm outperforms several\ncommon benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 02:50:21 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 22:59:12 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 15:18:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Hunter", "D. Scott", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1806.11370", "submitter": "Steffen Ventz", "authors": "Steffen Ventz, Matteo Cellamare, Sergio Bacallado and Lorenzo Trippa", "title": "Bayesian Uncertainty Directed Trial Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Bayesian response-adaptive designs unbalance randomization rates towards\nthe most promising arms with the goal of increasing the number of positive\ntreatment outcomes during the study, even though the primary aim of the trial\nis different. We discuss Bayesian uncertainty directed designs (BUD), a class\nof Bayesian designs in which the investigator specifies an information measure\ntailored to the experiment. All decisions during the trial are selected to\noptimize the available information at the end of the study. The approach can be\napplied to several designs, ranging from early stage multi-arm trials to\nbiomarker-driven and multi-endpoint studies. We discuss the asymptotic limit of\nthe patient allocation proportion to treatments, and illustrate the\nfinite-sample operating characteristics of BUD designs through examples,\nincluding multi-arm trials, biomarker-stratified trials, and trials with\nmultiple co-primary endpoints.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:04:55 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Ventz", "Steffen", ""], ["Cellamare", "Matteo", ""], ["Bacallado", "Sergio", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1806.11376", "submitter": "Albert Sol\\'e-Ribalta", "authors": "Albert Sol\\'e-Ribalta, Javier Borge-Holthoefer", "title": "Socio-economic constraints to maximum human lifespan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the demographic transition of the past century and a half,\nusing both empirical data and mathematical models, has rendered a wealth of\nwell-established facts, including the dramatic increases in life expectancy.\nDespite these insights, such analyses have also occasionally triggered debates\nwhich spill over many disciplines, from genetics, to biology, or demography.\nPerhaps the hottest discussion is happening around the question of maximum\nhuman lifespan, which --besides its fascinating historical and philosophical\ninterest-- poses urgent pragmatic warnings on a number of issues in public and\nprivate decision-making. In this paper, we add to the controversy some results\nwhich, based on purely statistical grounds, suggest that the maximum human\nlifespan is not fixed, or has not reached yet a plateau. Quite the contrary,\nanalysis on reliable data for over 150 years in more than 20 industrialized\ncountries point at a sustained increase in the maximum age at death.\nFurthermore, were this trend to continue, a limitless lifespan could be\nachieved by 2102. Finally, we quantify the dependence of increases in the\nmaximum lifespan on socio-economic factors. Our analysis indicates that in some\ncountries the observed rising patterns can only be sustained by progressively\nlarger increases in GDP, setting the problem of longevity in a context of\ndiminishing returns.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:28:20 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Sol\u00e9-Ribalta", "Albert", ""], ["Borge-Holthoefer", "Javier", ""]]}, {"id": "1806.11392", "submitter": "Richard Boys", "authors": "Stephen R. Johnson and Daniel A. Henderson and Richard J. Boys", "title": "Revealing subgroup structure in ranked data using a Bayesian WAND", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked data arise in many areas of application ranging from the ranking of\nup-regulated genes for cancer to the ranking of academic statistics journals.\nComplications can arise when rankers do not report a full ranking of all\nentities; for example, they might only report their top--$M$ ranked entities\nafter seeing some or all entities. It can also be useful to know whether\nrankers are equally informative, and whether some entities are effectively\njudged to be exchangeable. When there is important subgroup structure in the\ndata, summaries such as aggregate (overall) rankings can be misleading. In this\npaper we propose a flexible Bayesian nonparametric model for identifying\nheterogeneous structure and ranker reliability in ranked data. The model is a\nWeighted Adapted Nested Dirichlet (WAND) process mixture of Plackett-Luce\nmodels and inference proceeds through a simple and efficient Gibbs sampling\nscheme for posterior sampling. The richness of information in the posterior\ndistribution allows us to infer many details of the structure both between\nranker groups and between entity groups (within ranker groups), in contrast to\nmany other (Bayesian) analyses. We also examine how posterior predictive checks\ncan be used to identify lack of model fit. The methodology is illustrated using\nseveral simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:03:52 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 15:22:21 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Johnson", "Stephen R.", ""], ["Henderson", "Daniel A.", ""], ["Boys", "Richard J.", ""]]}]