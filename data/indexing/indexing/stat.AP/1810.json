[{"id": "1810.00031", "submitter": "Alejandro Noriega-Campero", "authors": "Alejandro Noriega-Campero, Michiel A. Bakker, Bernardo Garcia-Bulle,\n  Alex Pentland", "title": "Active Fairness in Algorithmic Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Society increasingly relies on machine learning models for automated decision\nmaking. Yet, efficiency gains from automation have come paired with concern for\nalgorithmic discrimination that can systematize inequality. Recent work has\nproposed optimal post-processing methods that randomize classification\ndecisions for a fraction of individuals, in order to achieve fairness measures\nrelated to parity in errors and calibration. These methods, however, have\nraised concern due to the information inefficiency, intra-group unfairness, and\nPareto sub-optimality they entail. The present work proposes an alternative\nactive framework for fair classification, where, in deployment, a\ndecision-maker adaptively acquires information according to the needs of\ndifferent groups or individuals, towards balancing disparities in\nclassification performance. We propose two such methods, where information\ncollection is adapted to group- and individual-level needs respectively. We\nshow on real-world datasets that these can achieve: 1) calibration and single\nerror parity (e.g., equal opportunity); and 2) parity in both false positive\nand false negative rates (i.e., equal odds). Moreover, we show that by\nleveraging their additional degree of freedom, active approaches can\nsubstantially outperform randomization-based classifiers previously considered\noptimal, while avoiding limitations such as intra-group unfairness.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 18:28:26 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 16:42:51 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Noriega-Campero", "Alejandro", ""], ["Bakker", "Michiel A.", ""], ["Garcia-Bulle", "Bernardo", ""], ["Pentland", "Alex", ""]]}, {"id": "1810.00210", "submitter": "Takahiro Yoshida", "authors": "Takahiro Yoshida, Rim Er-Rbib, Morito Tsutsumi", "title": "Which country epitomizes the world? A study from the perspective of\n  demographic composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic indicators are an essential element in considering various\nproblems in the social economy, such as predicting economic fluctuations and\nestablishing policies. The literature widely discusses the growth of the world\npopulation or issues pertaining to its aging, but has given little to no\nattention to population structures and transition patterns. In this article, we\ntake advantage of the characteristics of compositional data to examine the\ntransition of the world population structure. Using the Aitchison distance, we\nexamine the similarity of the world population structure from the 1990s to 2080\nand that of countries and regions in 2015 and create maps to illustrate the\nresults. Accordingly, we identify the following countries and regions as\nepitomes of the world population structure through different periods: India,\nNorthern Africa and South Africa, in the 1990s, South America in 2015 to 2030,\nOceania and Northern America in 2040, Uruguay and Puerto Rico in 2050 to 2060,\nand Italy and Japan in the distant future. We then cluster countries based on\nthe similarity of their population structures in 2015 and correspond each\ncluster to a certain period. We found that Russia and Western Europe gather in\na cluster that does not correspond to any period, indicating a recessive\npopulation structure.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 13:33:14 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Yoshida", "Takahiro", ""], ["Er-Rbib", "Rim", ""], ["Tsutsumi", "Morito", ""]]}, {"id": "1810.00211", "submitter": "Christian Steglich", "authors": "Christian Steglich", "title": "Why echo chambers form and network interventions fail: Selection\n  outpaces influence in dynamic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are online networking services complicit in facilitating social change for\nthe worse? In two empirically informed simulation studies, we give a\nproof-of-concept that the speed of networking and the amplification of network\nactors' relational preferences can have a profound impact on diffusion dynamics\non social networks, essentially counteracting the benefits that should accrue\nfrom networking according to the strength of weak ties argument. Our findings\ncan help understand variations in homogeneity of network neighbourhoods, i.e.,\nin the degree to which these neighbourhoods act as \"echo chambers\", as well as\nthe high context-dependency of success rates for a certain type of network\nintervention studies. They suggest that the general facilitation of\nconnectivity like it today happens on the internet, combined with the use of\npersonalisation algorithms, has strong and insufficiently understood effects on\ndynamic processes unfolding on the affected social networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 13:43:10 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Steglich", "Christian", ""]]}, {"id": "1810.00216", "submitter": "Alejandro Frery", "authors": "D\\'ebora Chan and Andrea Rey and Juliana Gambini and Alejandro C.\n  Frery", "title": "Parameter Estimation for the Single-Look $\\mathcal{G}^0$ Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical properties of Synthetic Aperture Radar (SAR) image texture\nreveals useful target characteristics. It is well-known that these images are\naffected by speckle, and prone to contamination as double bounce and corner\nreflectors. The $\\mathcal{G}^0$ distribution is flexible enough to model\ndifferent degrees of texture in speckled data. It is indexed by three\nparameters: $\\alpha$, related to the texture, $\\gamma$, a scale parameter, and\n$L$, the number of looks which is related to the signal-to-noise ratio. Quality\nestimation of $\\alpha$ is essential due to its immediate interpretability. In\nthis article, we compare the behavior of a number of parameter estimation\ntechniques in the noisiest case, namely single look data. We evaluate them\nusing Monte Carlo methods for non-contaminated and contaminated data,\nconsidering convergence rate, bias, mean squared error (MSE) and computational\ncost. The results are verified with simulated and actual SAR images.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 14:31:09 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chan", "D\u00e9bora", ""], ["Rey", "Andrea", ""], ["Gambini", "Juliana", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1810.00514", "submitter": "Narumasa Tsutsumida", "authors": "Narumasa Tsutsumida, Pedro Rodr\\'iguez-Veiga, Paul Harris, Heiko\n  Balzter, Alexis Comber", "title": "Investigating Spatial Error Structures in Continuous Raster Data", "comments": "This version of manuscript has been accepted for publication in\n  International Journal of Applied Earth Observation and Geoinformation on 28th\n  September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objective of this study is to investigate spatial structures of error in\nthe assessment of continuous raster data. The use of conventional diagnostics\nof error often overlooks the possible spatial variation in error because such\ndiagnostics report only average error or deviation between predicted and\nreference values. In this respect, this work uses a moving window (kernel)\napproach to generate geographically weighted (GW) versions of the mean signed\ndeviation, the mean absolute error and the root mean squared error and to\nquantify their spatial variations. Such approach computes local error\ndiagnostics from data weighted by its distance to the centre of a moving kernel\nand allows to map spatial surfaces of each type of error. In addition, a GW\ncorrelation analysis between predicted and reference values provides an\nalternative view of local error. Full abstract can be found in the pdf.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 03:13:15 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Tsutsumida", "Narumasa", ""], ["Rodr\u00edguez-Veiga", "Pedro", ""], ["Harris", "Paul", ""], ["Balzter", "Heiko", ""], ["Comber", "Alexis", ""]]}, {"id": "1810.00678", "submitter": "Hildete Pinheiro", "authors": "Hildete P. Pinheiro, Pranab K. Sen, Alu\\'isio Pinheiro and Samara F.\n  Kiihl", "title": "A nonparametric approach to assess undergraduate performance", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric methodologies are proposed to assess college students'\nperformance. Emphasis is given to gender and sector of High School. The\napplication concerns the University of Campinas, a research university in\nSoutheast Brazil. In Brazil college is based on a somewhat rigid set of\nsubjects for each major. Thence a student's relative performance can not be\naccurately measured by the Grade Point Average or by any other single measure.\nWe then define individual vectors of course grades. These vectors are used in\npairwise comparisons of common subject grades for individuals that entered\ncollege in the same year. The relative college performances of any two students\nis compared to their relative performances on the Entrance Exam Score. A test\nbased on generalized U-statistics is developed for homogeneity of some\npredefined groups. Asymptotic normality of the test statistic is true for both\nnull and alternative hypotheses. Maximum power is attained by employing the\nunion intersection principle.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:51:08 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Pinheiro", "Hildete P.", ""], ["Sen", "Pranab K.", ""], ["Pinheiro", "Alu\u00edsio", ""], ["Kiihl", "Samara F.", ""]]}, {"id": "1810.00709", "submitter": "Ruitao Lin", "authors": "Ruitao Lin, Robert L Coleman, Ying Yuan", "title": "TOP: Time-to-Event Bayesian Optimal Phase II Trial Design for Cancer\n  Immunotherapy", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immunotherapies have revolutionized cancer treatment. Unlike chemotherapies,\nimmune agents often take longer time to show benefit, and the complex and\nunique mechanism of action of these agents renders the use of multiple\nendpoints more appropriate in some trials. These new features of immunotherapy\nmake conventional phase II trial designs, which assume a single binary endpoint\nthat is quickly ascertainable, inefficient and dysfunctional. We propose a\nflexible and efficient time-to-event Bayesian optimal phase II (TOP) design.\nThe TOP design is efficient in that it allows real-time \"go/no-go\" interim\ndecision making in the presence of late-onset responses by using all available\ndata, and maximizes the statistical power for detecting effective treatments.\nTOP is flexible in the number of interim looks and capable of handling simple\nand complicated endpoints under a unified framework. We conduct simulation\nstudies to evaluate the operating characteristics of the TOP design.Compared to\nsome existing designs, the TOP design shortens the trial duration and has\nhigher power to detect effective treatment with well controlled type I errors.\nThe TOP design allows for making real-time \"go/no-go\" interim decisions in the\npresence of late-onset responses, and is capable of handling various types of\nendpoints under a unified framework. It is transparent and easy to implement as\nits decision rules can be tabulated and included in the protocol prior to the\nconduct of the trial. The TOP design provides a flexible, efficient and\neasy-to-implement method to accelerate and improve the development of\nimmunotherapies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:06:19 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Lin", "Ruitao", ""], ["Coleman", "Robert L", ""], ["Yuan", "Ying", ""]]}, {"id": "1810.00767", "submitter": "Maria Cuellar", "authors": "Maria Cuellar and Edward H. Kennedy", "title": "A nonparametric projection-based estimator for the probability of\n  causation, with application to water sanitation in Kenya", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current estimation methods for the probability of causation (PC) make strong\nparametric assumptions or are inefficient. We derive a nonparametric\ninfluence-function-based estimator for a projection of PC, which allows for\nsimple interpretation and valid inference by making weak structural\nassumptions. We apply our estimator to real data from an experiment in Kenya,\nwhich found, by estimating the average treatment effect, that protecting water\nsprings reduces childhood disease. However, before scaling up this\nintervention, it is important to determine whether it was the exposure, and not\nsomething else, that caused the outcome. Indeed, we find that some children,\nwho were exposed to a high concentration of bacteria in drinking water and had\na diarrheal disease, would likely have contracted the disease absent the\nexposure since the estimated PC for an average child in this study is 0.12 with\na 95% confidence interval of (0.11, 0.13). Our nonparametric method offers\nresearchers a way to estimate PC, which is essential if one wishes to determine\nnot only the average treatment effect, but also whether an exposure likely\ncaused the observed outcome.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 15:45:39 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:50:44 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 16:02:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Cuellar", "Maria", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1810.00908", "submitter": "Rahul Ghosal", "authors": "Indrabati Bhattacharya, Rahul Ghosal, Sujit Ghosh", "title": "A Statistical Exploration of Duckworth-Lewis Method Using Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duckworth-Lewis (D/L) method is the incumbent rain rule used to decide the\nresult of a limited overs cricket match should it not be able to reach its\nnatural conclusion. Duckworth and Lewis (1998) devised a two factor\nrelationship between the numbers of overs a team had remaining and the number\nof wickets they had lost in order to quantify the percentage resources a team\nhas at any stage of the match. As number of remaining overs decrease and lost\nwickets increase the resources are expected to decrease. The resource table\nwhich is still being used by ICC (International Cricket Council) for 50 overs\ncricket match suffers from lack of monotonicity both in numbers of overs left\nand number of wickets lost. We apply Bayesian inference to build a resource\ntable which overcomes the non monotonicity problem of the current D/L resource\ntable and show that it gives better prediction for teams in first innings score\nand hence it is more suitable for using in rain affected matches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:24:01 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Ghosal", "Rahul", ""], ["Ghosh", "Sujit", ""]]}, {"id": "1810.00919", "submitter": "Irene Epifanio", "authors": "Jes\\'us Moliner, Irene Epifanio", "title": "Robust multivariate and functional archetypal analysis with application\n  to financial time series analysis", "comments": "Physica A: Statistical Mechanics and its Applications, 2019", "journal-ref": null, "doi": "10.1016/j.physa.2018.12.036", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal analysis approximates data by means of mixtures of actual extreme\ncases (archetypoids) or archetypes, which are a convex combination of cases in\nthe data set. Archetypes lie on the boundary of the convex hull. This makes the\nanalysis very sensitive to outliers. A robust methodology by means of\nM-estimators for classical multivariate and functional data is proposed. This\nunsupervised methodology allows complex data to be understood even by\nnon-experts. The performance of the new procedure is assessed in a simulation\nstudy, where a comparison with a previous methodology for the multivariate case\nis also carried out, and our proposal obtains favorable results. Finally,\nrobust bivariate functional archetypoid analysis is applied to a set of\ncompanies in the S\\&P 500 described by two time series of stock quotes. A new\ngraphic representation is also proposed to visualize the results. The analysis\nshows how the information can be easily interpreted and how even non-experts\ncan gain a qualitative understanding of the data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:48:26 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 17:18:57 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Moliner", "Jes\u00fas", ""], ["Epifanio", "Irene", ""]]}, {"id": "1810.01537", "submitter": "Adriano Polpo", "authors": "Carlos Alberto de Braganca Pereira, Teresa Cristina Martins Dias,\n  Adriano Polpo", "title": "Odds for the Brazilian 2018 president elections: An application of\n  Bayesian statistics in contingency tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of these notes is to present an assessment of the probability of\na candidate be elected in a two-round presidential election. In the first\nround, all candidates can be voted on. If one of them has more than 50% of the\nvote (s)he is elected and there is no second round. If none of the candidates\nobtain more than 50% of the votes, then the top two candidates will be selected\nfor a second round. In this second round, the most voted candidate is elected.\nThis is the scenario of the Brazilian elections that are taking place at the\nmoment. We are calculating the odds associated with the 2018 presidential\nelections in Brazil. The first round is on October 7, and the second round is\non October 28, 2018.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 22:35:19 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Pereira", "Carlos Alberto de Braganca", ""], ["Dias", "Teresa Cristina Martins", ""], ["Polpo", "Adriano", ""]]}, {"id": "1810.01544", "submitter": "Jungseock Joo", "authors": "Jungseock Joo, Zachary C. Steinert-Threlkeld", "title": "Image as Data: Automated Visual Content Analysis for Political Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image data provide unique information about political events, actors, and\ntheir interactions which are difficult to measure from or not available in text\ndata. This article introduces a new class of automated methods based on\ncomputer vision and deep learning which can automatically analyze visual\ncontent data. Scholars have already recognized the importance of visual data\nand a variety of large visual datasets have become available. The lack of\nscalable analytic methods, however, has prevented from incorporating large\nscale image data in political analysis. This article aims to offer an in-depth\noverview of automated methods for visual content analysis and explains their\nusages and implementations. We further elaborate on how these methods and\nresults can be validated and interpreted. We then discuss how these methods can\ncontribute to the study of political communication, identity and politics,\ndevelopment, and conflict, by enabling a new set of research questions at\nscale.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 00:11:55 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Joo", "Jungseock", ""], ["Steinert-Threlkeld", "Zachary C.", ""]]}, {"id": "1810.01576", "submitter": "Tymon Sloczynski", "authors": "Tymon S{\\l}oczy\\'nski", "title": "Interpreting OLS Estimands When Treatment Effects Are Heterogeneous:\n  Smaller Groups Get Larger Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied work often studies the effect of a binary variable (\"treatment\")\nusing linear models with additive effects. I study the interpretation of the\nOLS estimands in such models when treatment effects are heterogeneous. I show\nthat the treatment coefficient is a convex combination of two parameters, which\nunder certain conditions can be interpreted as the average treatment effects on\nthe treated and untreated. The weights on these parameters are inversely\nrelated to the proportion of observations in each group. Reliance on these\nimplicit weights can have serious consequences for applied work, as I\nillustrate with two well-known applications. I develop simple diagnostic tools\nthat empirical researchers can use to avoid potential biases. Software for\nimplementing these methods is available in R and Stata. In an important special\ncase, my diagnostics only require the knowledge of the proportion of treated\nunits.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 04:01:27 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 17:39:19 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 22:04:18 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["S\u0142oczy\u0144ski", "Tymon", ""]]}, {"id": "1810.01675", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri, Subhro Ghosh, David J. Nott, Kim Cuc Pham", "title": "An easy-to-use empirical likelihood ABC method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientifically well-motivated statistical models in natural, engineering\nand environmental sciences are specified through a generative process, but in\nsome cases it may not be possible to write down a likelihood for these models\nanalytically. Approximate Bayesian computation (ABC) methods, which allow\nBayesian inference in these situations, are typically computationally\nintensive. Recently, computationally attractive empirical likelihood based ABC\nmethods have been suggested in the literature. These methods heavily rely on\nthe availability of a set of suitable analytically tractable estimating\nequations. We propose an easy-to-use empirical likelihood ABC method, where the\nonly inputs required are a choice of summary statistic, it's observed value,\nand the ability to simulate summary statistics for any parameter value under\nthe model. It is shown that the posterior obtained using the proposed method is\nconsistent, and its performance is explored using various examples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 10:37:31 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 11:20:19 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Ghosh", "Subhro", ""], ["Nott", "David J.", ""], ["Pham", "Kim Cuc", ""]]}, {"id": "1810.01692", "submitter": "William Thomson", "authors": "William Thomson, Sara Jabbari, Angela Taylor, Wiebke Arlt, David Smith", "title": "Simultaneous Parameter Estimation and Variable Selection via the LN-CASS\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian prior distribution, the Logit-Normal continuous\nanalogue of the spike-and-slab (LN-CASS), which enables flexible parameter\nestimation and variable/model selection in a variety of settings. We\ndemonstrate its use and efficacy in three case studies -- a simulation study\nand two studies on real biological data from the fields of metabolomics and\ngenomics. The prior allows the use of classical statistical models, which are\neasily interpretable and well-known to applied scientists, but performs\ncomparably to common machine learning methods in terms of generalisability to\npreviously unseen data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:24:24 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Thomson", "William", ""], ["Jabbari", "Sara", ""], ["Taylor", "Angela", ""], ["Arlt", "Wiebke", ""], ["Smith", "David", ""]]}, {"id": "1810.01710", "submitter": "Joakim Beck", "authors": "Marco Ballesio, Joakim Beck, Anamika Pandey, Laura Parisi, Erik von\n  Schwerin, Raul Tempone", "title": "Multilevel Monte Carlo Acceleration of Seismic Wave Propagation under\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret uncertainty in a model for seismic wave propagation by treating\nthe model parameters as random variables, and apply the Multilevel Monte Carlo\n(MLMC) method to reduce the cost of approximating expected values of selected,\nphysically relevant, quantities of interest (QoI) with respect to the random\nvariables. Targeting source inversion problems, where the source of an\nearthquake is inferred from ground motion recordings on the Earth's surface, we\nconsider two QoI that measure the discrepancies between computed seismic\nsignals and given reference signals: one QoI, $\\hbox{QoI}_E$, is defined in\nterms of the $L^2$-misfit, which is directly related to maximum likelihood\nestimates of the source parameters; the other, $\\hbox{QoI}_W$, is based on the\nquadratic Wasserstein distance between probability distributions, and\nrepresents one possible choice in a class of such misfit functions that have\nbecome increasingly popular to solve seismic inversion in recent years. We\nsimulate seismic wave propagation, including seismic attenuation, using a\npublicly available code in widespread use, based on the spectral element\nmethod. Using random coefficients and deterministic initial and boundary data,\nwe present benchmark numerical experiments with synthetic data in a\ntwo-dimensional physical domain and a one-dimensional velocity model where the\nassumed parameter uncertainty is motivated by realistic Earth models. Here, the\ncomputational cost of the standard Monte Carlo method was reduced by up to 97%\nfor $\\hbox{QoI}_E$, and up to 78% for $\\hbox{QoI}_W$, using a relevant range of\ntolerances. Shifting to three-dimensional domains is straight-forward and will\nfurther increase the relative computational work reduction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 12:12:46 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 10:35:53 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 13:18:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Ballesio", "Marco", ""], ["Beck", "Joakim", ""], ["Pandey", "Anamika", ""], ["Parisi", "Laura", ""], ["von Schwerin", "Erik", ""], ["Tempone", "Raul", ""]]}, {"id": "1810.01855", "submitter": "Prashanth R", "authors": "R Prashanth and Sumantra Dutta Roy", "title": "Early Detection of Parkinson's Disease through Patient Questionnaire and\n  Predictive Modelling", "comments": "Article accepted in the International journal of Medical Informatics", "journal-ref": "International journal of Medical Informatics, Vol 119, Pages\n  75-87, November 2018", "doi": "10.1016/j.ijmedinf.2018.09.008", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of Parkinson's disease (PD) is important which can enable\nearly initiation of therapeutic interventions and management strategies.\nHowever, methods for early detection still remain an unmet clinical need in PD.\nIn this study, we use the Patient Questionnaire (PQ) portion from the widely\nused Movement Disorder Society-Unified Parkinson's Disease Rating Scale\n(MDS-UPDRS) to develop prediction models that can classify early PD from\nhealthy normal using machine learning techniques that are becoming popular in\nbiomedicine: logistic regression, random forests, boosted trees and support\nvector machine (SVM). We carried out both subject-wise and record-wise\nvalidation for evaluating the machine learning techniques. We observe that\nthese techniques perform with high accuracy and high area under the ROC curve\n(both >95%) in classifying early PD and healthy normal. The logistic model\ndemonstrated statistically significant fit to the data indicating its\nusefulness as a predictive model. It is inferred that these prediction models\nhave the potential to aid clinicians in the diagnostic process by joining the\nitems of a questionnaire through machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 17:56:21 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Prashanth", "R", ""], ["Roy", "Sumantra Dutta", ""]]}, {"id": "1810.01981", "submitter": "Noah Haber", "authors": "Noah Haber, Paul Jake Robyn, Saidou Hamadou, Gervais Yama, Herve Hien,\n  Davy Louvouezo, G\\\"unther Fink", "title": "Surveyor Gender Modifies Average Survey Responses: Evidence from\n  Household Surveys in Four Sub-Saharan African Countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relatively little is known regarding the influence of surveyor traits on\nrespondent behavior. In this paper, we assess the extent to which survey gender\nmodifies average survey responses in the context of health-focused household\nsurveys.\n  We pool data from four recent health-focused household surveys using both\nmale and female surveyors: Burkina Faso (2014), Cameroon (2012), Central\nAfrican Republic (2012), and Republic of Congo (2014). In all surveys,\nsurveyors were pre-assigned to households based on an initial household\nlisting. We compare responses given to male and female surveyors across three\ndomains: household characteristics, child mortality and reproductive health.\nMultivariable regression with enumeration area fixed-effects were used to\nestimate response differentials.\n  A total of 22,835 household surveys were analyzed. The proportion of\ninterviews conducted by female interviews varied between 9 percent in Central\nAfrican Republic and 52 percent in Cameroon. Female surveyor gender increased\nthe odds of reporting asset ownership by 9.4% (OR 1.094, 95% CI: 1.024,1.169)\nand increased the odds of reporting a pregnancy-related event by 25% (OR 1.246\n(95% CI: 1.12,1.393). Being interviewed by a woman increased the odds of\nrespondents reporting a stillbirth by 29% (95% CI: 1.118,1.492), and the odds\nof reporting a miscarriage by 17% (95% CI: 1.072,1.284). Substantial\nheterogeneity in gender-specific reporting was found across the four countries.\nWe did not find evidence that the gender of the participant modified the effect\nof surveyor gender for household items.\n  Our results suggest that surveyor gender is highly predictive of survey\nresponses. For health surveys, female surveyors are likely to receive more\naccurate and consistent responses. More generally, social distance between\ninterviewers and interviewees should be minimized in large scale surveys.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 21:30:20 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Haber", "Noah", ""], ["Robyn", "Paul Jake", ""], ["Hamadou", "Saidou", ""], ["Yama", "Gervais", ""], ["Hien", "Herve", ""], ["Louvouezo", "Davy", ""], ["Fink", "G\u00fcnther", ""]]}, {"id": "1810.02125", "submitter": "Adriano Koshiyama", "authors": "Adriano Soares Koshiyama, Nick Firoozye and Philip Treleaven", "title": "A Machine Learning-based Recommendation System for Swaptions Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivative traders are usually required to scan through hundreds, even\nthousands of possible trades on a daily basis. Up to now, not a single solution\nis available to aid in their job. Hence, this work aims to develop a trading\nrecommendation system, and apply this system to the so-called Mid-Curve\nCalendar Spread (MCCS), an exotic swaption-based derivatives package. In\nsummary, our trading recommendation system follows this pipeline: (i) on a\ncertain trade date, we compute metrics and sensitivities related to an MCCS;\n(ii) these metrics are feed in a model that can predict its expected return for\na given holding period; and after repeating (i) and (ii) for all trades we\n(iii) rank the trades using some dominance criteria. To suggest that such\napproach is feasible, we used a list of 35 different types of MCCS; a total of\n11 predictive models; and 4 benchmark models. Our results suggest that in\ngeneral linear regression with lasso regularisation compared favourably to\nother approaches from a predictive and interpretability perspective.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:55:40 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Koshiyama", "Adriano Soares", ""], ["Firoozye", "Nick", ""], ["Treleaven", "Philip", ""]]}, {"id": "1810.02252", "submitter": "Jan Van Haaren", "authors": "Lotte Bransen, Jan Van Haaren", "title": "Measuring Football Players' On-the-ball Contributions From Passes During\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several performance metrics for quantifying the in-game performances of\nindividual football players have been proposed in recent years. Although the\nmajority of the on-the-ball actions during games constitutes of passes, many of\nthe currently available metrics focus on measuring the quality of shots only.\nTo help bridge this gap, we propose a novel approach to measure players'\non-the-ball contributions from passes during games. Our proposed approach\nmeasures the expected impact of each pass on the scoreline.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:40:22 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Bransen", "Lotte", ""], ["Van Haaren", "Jan", ""]]}, {"id": "1810.02285", "submitter": "Yanxun Xu", "authors": "Yanxun Xu, Florica Constantine, Yuan Yuan, Yili L. Pritchett", "title": "ASIED: A Bayesian Adaptive Subgroup-Identification Enrichment Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing targeted therapies based on patients' baseline characteristics and\ngenomic profiles such as biomarkers has gained growing interests in recent\nyears. Depending on patients' clinical characteristics, the expression of\nspecific biomarkers or their combinations, different patient subgroups could\nrespond differently to the same treatment. An ideal design, especially at the\nproof of concept stage, should search for such subgroups and make dynamic\nadaptation as the trial goes on. When no prior knowledge is available on\nwhether the treatment works on the all-comer population or only works on the\nsubgroup defined by one biomarker or several biomarkers, it is necessary to\nincorporate the adaptive estimation of the heterogeneous treatment effect to\nthe decision-making at interim analyses. To address this problem, we propose an\nAdaptive Subgroup-Identification Enrichment Design, ASIED, to simultaneously\nsearch for predictive biomarkers, identify the subgroups with differential\ntreatment effects, and modify study entry criteria at interim analyses when\njustified. More importantly, we construct robust quantitative decision-making\nrules for population enrichment when the interim outcomes are heterogeneous in\nthe context of a multilevel target product profile, which defines the minimal\nand targeted levels of treatment effect. Through extensive simulations, the\nASIED is demonstrated to achieve desirable operating characteristics and\ncompare favorably against alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 16:00:53 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 03:49:49 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Xu", "Yanxun", ""], ["Constantine", "Florica", ""], ["Yuan", "Yuan", ""], ["Pritchett", "Yili L.", ""]]}, {"id": "1810.02315", "submitter": "Derek Chang", "authors": "Derek Chang, Devendra Shelar, and Saurabh Amin", "title": "DER Allocation and Line Repair Scheduling for Storm-induced Failures in\n  Distribution Networks", "comments": "7 pages, 4 figures, accepted to 2018 IEEE SmartGridComm Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity distribution networks (DNs) in many regions are increasingly\nsubjected to disruptions caused by tropical storms. Distributed Energy\nResources (DERs) can act as temporary supply sources to sustain \"microgrids\"\nresulting from disruptions. In this paper, we investigate the problem of\nsuitable DER allocation to facilitate more efficient repair operations and\nfaster recovery. First, we estimate the failure probabilities of DN components\n(lines) using a stochastic model of line failures which parametrically depends\non the location-specific storm wind field. Next, we formulate a two-stage\nstochastic mixed integer program, which models the distribution utility's\ndecision to allocate DERs in the DN (pre-storm stage); and accounts for\nmulti-period decisions on optimal dispatch and line repair scheduling\n(post-storm stage). A key feature of this formulation is that it jointly\noptimizes electricity dispatch within the individual microgrids and the line\nrepair schedules to minimize the sum of the cost of DER allocation and cost due\nto lost load. To illustrate our approach, we use the sample average\napproximation method to solve our problem for a small-size DN under different\nstorm intensities and DER/crew constraints.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 16:55:35 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Chang", "Derek", ""], ["Shelar", "Devendra", ""], ["Amin", "Saurabh", ""]]}, {"id": "1810.02397", "submitter": "Soumen Dey", "authors": "Soumen Dey, Mohan Delampady, Arjun M. Gopalaswamy", "title": "Bayesian Model Selection for a Class of Spatially-Explicit Capture\n  Recapture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vast amount of ecological knowledge generated recently has hinged upon the\nability of model selection methods to discriminate among various ecological\nhypotheses. The last decade has seen the rise of Bayesian hierarchical models\nin ecology. Consequently, popular tools, such as the AIC, become largely\ninapplicable and other tools are not universally applicable. We focus on a\nclass of competing Bayesian spatially explicit capture recapture (SECR) models\nand first apply some of the recommended Bayesian model selection tools: (1)\nBayes Factor - using (a) Gelfand-Dey (b) harmonic mean methods, (2) DIC, (3)\nWAIC and (4) the posterior predictive loss function. In all, we evaluate 25\nvariants of model selection tools in our study. We evaluate these model\nselection tools from the standpoint of model selection and parameter estimation\nby contrasting the choice recommended by a tool with a `true' model. In all, we\ngenerate 120 simulated data sets using the true model and assess the frequency\nwith which the true model is selected and how well the tool estimates N\n(population size). We find that when information content is low, no particular\ntool can be recommended to help realise, simultaneously, both the goals of\nmodel selection and parameter estimation. In such scenarios, we recommend that\npractitioners utilise our application of Bayes Factor for parameter estimation\nand recommend the posterior predictive loss approach for model selection when\ninformation content is low. When both the objectives are taken together, we\nrecommend the use of our applications of Bayes Factor for Bayesian SECR models.\nOur study reveals that although new model selection tools are emerging (eg:\nWAIC) in the applied statistics literature, an uncritical absorption of these\nnew tools (i.e. without assessing their efficacies for the problem at hand)\ninto ecological practice may mislead inferences.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 19:16:47 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Dey", "Soumen", ""], ["Delampady", "Mohan", ""], ["Gopalaswamy", "Arjun M.", ""]]}, {"id": "1810.02456", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c and Alex Olshevsky and C\\'esar A. Uribe", "title": "Graph-Theoretic Analysis of Belief System Dynamics under Logic\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CY cs.MA cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion formation cannot be modeled solely as an ideological deduction from a\nset of principles; rather, repeated social interactions and logic constraints\namong statements are consequential in the construct of belief systems. We\naddress three basic questions in the analysis of social opinion dynamics: (i)\nWill a belief system converge? (ii) How long does it take to converge? (iii)\nWhere does it converge? We provide graph-theoretic answers to these questions\nfor a model of opinion dynamics of a belief system with logic constraints. Our\nresults make plain the implicit dependence of the convergence properties of a\nbelief system on the underlying social network and on the set of logic\nconstraints that relate beliefs on different statements. Moreover, we provide\nan explicit analysis of a variety of commonly used large-scale network models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 23:30:26 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 20:38:54 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1810.02467", "submitter": "Stephen Ellison", "authors": "Stephen L R Ellison", "title": "Applications of robust estimators of covariance in examination of\n  inter-laboratory study data", "comments": null, "journal-ref": "Analytical Methods, 2019, 11, 2639-2649", "doi": "10.1039/C8AY02724B", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates the use of selected robust estimators of covariance or\ncorrelation in the identification of anomalous laboratory results in\ninter-laboratory data. It is shown that robust estimators can substantially\nreduce the impact of outlying values on multivariate confidence regions and\nconsequently lead to sharper identification of anomalies, even where\ntraditional outlier detection may fail to locate anomalous results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 00:28:04 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 14:21:34 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Ellison", "Stephen L R", ""]]}, {"id": "1810.02518", "submitter": "arXiv Admin", "authors": "Rahul Makhijani", "title": "Social Choice Random Utility Models of Intransitive Pairwise Comparisons", "comments": "This article has been withdrawn by arXiv administrators due to an\n  irreconcilable authorship dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need for discrete choice models that account for the\ncomplex nature of human choices, escaping traditional behavioral assumptions\nsuch as the transitivity of pairwise preferences. Recently, several parametric\nmodels of intransitive comparisons have been proposed, but in all cases the\nmaximum likelihood problem is non-concave, making inference difficult. In this\nwork we generalize this trend, showing that there cannot exist any parametric\nmodel with a concave log-likelihood function that can exhibit intransitive\npreferences. Given this result, we motivate a new model for analyzing\nintransitivity in pairwise comparisons, taking inspiration from the Condorcet\nmethod (majority vote) in social choice theory. The Majority Vote model we\nanalyze is defined as a voting process over independent Random Utility Models\n(RUMs). We infer a multidimensional embedding of each object or player, in\ncontrast to the traditional one-dimensional embedding used by models such as\nthe Thurstone or Bradley-Terry-Luce (BTL) models. We show that a\nthree-dimensional majority vote model is capable of modeling arbitrarily strong\nand long intransitive cycles, and can also represent arbitrary pairwise\ncomparison probabilities on any triplet. We provide experimental results that\nsubstantiate our claims regarding the effectiveness of our model in capturing\nintransitivity for various pairwise choice tasks such as predicting choices in\nrecommendation systems, winners in online video games, and elections.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 05:26:29 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 18:51:21 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 15:39:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Makhijani", "Rahul", ""]]}, {"id": "1810.02618", "submitter": "Gustavo Thomas", "authors": "Gustavo Thomas (1), Luiz R. Nakamura (2), Rafael A. Moral (1) and\n  Clarice G.B. Dem\\'etrio (1) ( (1) Departamento de Ci\\^encias Exatas,\n  ESALQ/USP, Piracicaba, Brazil, (2) Departamento de Inform\\'atica e\n  Estat\\'istica, UFSC, Florian\\'opolis, Brazil)", "title": "Modeling data with zero inflation and overdispersion using GAMLSSs", "comments": "19 pages, 7 figures, presented at the 62nd International Biometric\n  Society Reunion (RBras) and 17th Symposium of Applied Statistics in Agronomy\n  (SEAGRO) in 2017 at Lavras-MG, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data with high frequencies of zeros are found in many areas, specially\nin biology. Statistical models to analyze such data started to be developed in\nthe 80s and are still a topic of active research. Such models usually assume a\nresponse distribution that belongs to the exponential family of distributions\nand the analysis is performed under the generalized linear models framework.\nHowever, the generalized additive models for location, scale and shape\n(GAMLSSs) represent a more general class of univariate models that can also be\nused to model zero inflated data. In this paper, the analysis of a data set\nwith excess of zeros and overdispersion is described using GAMLSSs. Specific\nGAMLSSs' tools were used in the analysis, which enhanced model comparison and\neased the interpretation of results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 11:27:05 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Thomas", "Gustavo", ""], ["Nakamura", "Luiz R.", ""], ["Moral", "Rafael A.", ""], ["Dem\u00e9trio", "Clarice G. B.", ""]]}, {"id": "1810.02669", "submitter": "Anders Eklund", "authors": "Anders Eklund, Hans Knutsson, Thomas E. Nichols", "title": "Reply to Chen et al.: Parametric methods for cluster inference perform\n  worse for two-sided t-tests", "comments": null, "journal-ref": "Human Brain Mapping, 2018", "doi": "10.1002/hbm.24465", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One-sided t-tests are commonly used in the neuroimaging field, but two-sided\ntests should be the default unless a researcher has a strong reason for using a\none-sided test. Here we extend our previous work on cluster false positive\nrates, which used one-sided tests, to two-sided tests. Briefly, we found that\nparametric methods perform worse for two-sided t-tests, and that non-parametric\nmethods perform equally well for one-sided and two-sided tests.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:24:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Eklund", "Anders", ""], ["Knutsson", "Hans", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "1810.02717", "submitter": "Shaoyang Ning", "authors": "Shaoyang Ning, Xi Qu, Victor Cai, Nathan Sanders", "title": "Clust-LDA: Joint Model for Text Mining and Author Group Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media corpora pose unique challenges and opportunities, including\ntypically short document lengths and rich meta-data such as author\ncharacteristics and relationships. This creates great potential for systematic\nanalysis of the enormous body of the users and thus provides implications for\nindustrial strategies such as targeted marketing. Here we propose a novel and\nstatistically principled method, clust-LDA, which incorporates authorship\nstructure into the topical modeling, thus accomplishing the task of the topical\ninferences across documents on the basis of authorship and, simultaneously, the\nidentification of groupings between authors. We develop an inference procedure\nfor clust-LDA and demonstrate its performance on simulated data, showing that\nclust-LDA out-performs the \"vanilla\" LDA on the topic identification task where\nauthors exhibit distinctive topical preference. We also showcase the empirical\nperformance of clust-LDA based on a real-world social media dataset from\nReddit.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:33:40 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Ning", "Shaoyang", ""], ["Qu", "Xi", ""], ["Cai", "Victor", ""], ["Sanders", "Nathan", ""]]}, {"id": "1810.03085", "submitter": "Gustavo Thomas", "authors": "(1) Gustavo Thomas, (2) Alexandre Igor de Azevedo Pereira, (1)\n  Cristian Marcelo Villegas Lobos and (1) Clarice G.B. Dem\\'etrio. ((1)\n  Department of Exact Sciences, ESALQ/USP, Piracicaba, SP, Brazil (2)\n  Department of Agronomy, IF Goiano, Uruta\\'i, GO, Brazil)", "title": "Analysis of a longitudinal multilevel experiment using GAMLSSs", "comments": "30 pages, 16 figures, received the prize of 2nd best oral\n  presentation at the XV MGEST, Minas Gerais meeting of Statistics, Belo\n  Horizonte, Brazil in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard procedures for analysing hierarquical or grouped data are by\n(non)linear mixed models or generalized mixed models. However, the generalized\nadditive models for location, scale and shape (GAMLSSs) also allow different\ntypes of random effects to be included in the model formulation. Even though\nalready popular in many areas of research, this type of models have not been\nfound to be used for mixed modeling purposes yet. Therefore, this paper\ndescribes the analysis of an experiment with plants' growth using mixed\nGAMLSSs, comparing it to a linear mixed model approach.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 05:00:20 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Thomas", "Gustavo", ""], ["Pereira", "Alexandre Igor de Azevedo", ""], ["Lobos", "Cristian Marcelo Villegas", ""], ["Dem\u00e9trio.", "Clarice G. B.", ""]]}, {"id": "1810.03279", "submitter": "Xu Gao", "authors": "Xu Gao, Weining Shen, Chee-Ming Ting, Steven C. Cramer, Ramesh\n  Srinivasan, Hernando Ombao", "title": "Modeling Brain Connectivity with Graphical Models on Frequency Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multichannel electroencephalograms (EEGs) have been widely used to study\ncortical connectivity during acquisition of motor skills. In this paper, we\nintroduce copula Gaussian graphical models on spectral domain to characterize\ndependence in oscillatory activity between channels. To obtain a simple and\nrobust representation of brain connectivity that can explain the most variation\nin the observed signals, we propose a framework based on maximizing penalized\nlikelihood with Lasso regularization to search for the sparse precision matrix.\nTo address the optimization problem, graphical Lasso, Ledoit-Wolf and sparse\nestimation of a covariance matrix (SPCOV) algorithms were modified and\nimplemented. Simulations show the benefit of using the proposed algorithms in\nterms of robustness and small estimation errors. Furthermore, analysis of the\nEEG data in a motor skill task conducted using algorithms of modified graphical\nLASSO and Ledoit-Wolf, reveal a sparse pattern of brain connectivity among\ncortices which is consistent with the results from other work in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 06:30:10 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Gao", "Xu", ""], ["Shen", "Weining", ""], ["Ting", "Chee-Ming", ""], ["Cramer", "Steven C.", ""], ["Srinivasan", "Ramesh", ""], ["Ombao", "Hernando", ""]]}, {"id": "1810.03477", "submitter": "Yuming Wang", "authors": "Bin Wang, Ruodu Wang and Yuming Wang", "title": "Compatible Matrices of Spearman's Rank Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a negative answer to a long-standing open problem\non the compatibility of Spearman's rho matrices. Following an equivalence of\nSpearman's rho matrices and linear correlation matrices for dimensions up to 9\nin the literature, we show non-equivalence for dimensions 12 or higher. In\nparticular, we connect this problem with the existence of a random vector under\nsome linear projection restrictions in two characterization results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:03:03 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 08:59:06 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 17:57:17 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Bin", ""], ["Wang", "Ruodu", ""], ["Wang", "Yuming", ""]]}, {"id": "1810.03576", "submitter": "Yawen Guan", "authors": "Yawen Guan, Margaret Johnson, Matthias Katzfuss, Elizabeth Mannshardt,\n  Kyle P Messier, Brian J Reich and Joon Jin Song", "title": "Fine-scale spatiotemporal air pollution analysis using mobile monitors\n  on Google Street View vehicles", "comments": "This manuscript has been approved for public access. Please put this\n  version online. Previously, this version was removed by arXiv administrators\n  because the author did not have the right to agree to our license at the time\n  of submission", "journal-ref": "Journal of the American Statistical Association (2020), 115(531),\n  1111-1124", "doi": "10.1080/01621459.2019.1665526", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are increasingly concerned with understanding their personal\nenvironment, including possible exposure to harmful air pollutants. In order to\nmake informed decisions on their day-to-day activities, they are interested in\nreal-time information on a localized scale. Publicly available, fine-scale,\nhigh-quality air pollution measurements acquired using mobile monitors\nrepresent a paradigm shift in measurement technologies. A methodological\nframework utilizing these increasingly fine-scale measurements to provide\nreal-time air pollution maps and short-term air quality forecasts on a\nfine-resolution spatial scale could prove to be instrumental in increasing\npublic awareness and understanding. The Google Street View study provides a\nunique source of data with spatial and temporal complexities, with the\npotential to provide information about commuter exposure and hot spots within\ncity streets with high traffic. We develop a computationally efficient\nspatiotemporal model for these data and use the model to make short-term\nforecasts and high-resolution maps of current air pollution levels. We also\nshow via an experiment that mobile networks can provide more nuanced\ninformation than an equally-sized fixed-location network. This modeling\nframework has important real-world implications in understanding citizens'\npersonal environments, as data production and real-time availability continue\nto be driven by the ongoing development and improvement of mobile measurement\ntechnologies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 16:54:28 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 21:13:57 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 18:47:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Guan", "Yawen", ""], ["Johnson", "Margaret", ""], ["Katzfuss", "Matthias", ""], ["Mannshardt", "Elizabeth", ""], ["Messier", "Kyle P", ""], ["Reich", "Brian J", ""], ["Song", "Joon Jin", ""]]}, {"id": "1810.03586", "submitter": "Yves Rozenholc", "authors": "Fuchen Liu, Charles-Andr\\'e Cu\\'enod, Isabelle Thomassin-Naggara,\n  St\\'ephane Chemouny and Yves Rozenholc", "title": "Hierarchical segmentation using equivalence test (HiSET): Application to\n  DCE image sequences", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical contrast enhanced (DCE) imaging allows non invasive access to\ntissue micro-vascularization. It appears as a promising tool to build imaging\nbiomark-ers for diagnostic, prognosis or anti-angiogenesis treatment monitoring\nof cancer. However, quantitative analysis of DCE image sequences suffers from\nlow signal to noise ratio (SNR). SNR may be improved by averaging functional\ninformation in a large region of interest when it is functionally homogeneous.\n  We propose a novel method for automatic segmentation of DCE image sequences\ninto functionally homogeneous regions, called DCE-HiSET. Using an observation\nmodel which depends on one parameter a and is justified a posteri-ori,\nDCE-HiSET is a hierarchical clustering algorithm. It uses the p-value of a\nmultiple equivalence test as dissimilarity measure and consists of two steps.\nThe first exploits the spatial neighborhood structure to reduce complexity and\ntakes advantage of the regularity of anatomical features, while the second\nrecovers (spatially) disconnected homogeneous structures at a larger (global)\nscale. Given a minimal expected homogeneity discrepancy for the multiple\nequivalence test, both steps stop automatically by controlling the Type I\nerror. This provides an adaptive choice for the number of clusters. Assuming\nthat the DCE image sequence is functionally piecewise constant with signals on\neach piece sufficiently separated, we prove that DCE-HiSET will retrieve the\nexact partition with high probability as soon as the number of images in the\nsequence is large enough. The minimal expected homogeneity discrepancy appears\nas the tuning parameter controlling the size of the segmentation.\n  DCE-HiSET has been implemented in C++ for 2D and 3D image sequences with\ncompetitive speed.\n  Keywords : DCE imaging, automatic clustering, hierarchical segmentation,\nequivalence test\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 17:27:48 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Liu", "Fuchen", ""], ["Cu\u00e9nod", "Charles-Andr\u00e9", ""], ["Thomassin-Naggara", "Isabelle", ""], ["Chemouny", "St\u00e9phane", ""], ["Rozenholc", "Yves", ""]]}, {"id": "1810.03727", "submitter": "Yuting Ji", "authors": "Yuting Ji, Elizabeth Buechler, and Ram Rajagopal", "title": "Data-Driven Load Modeling and Forecasting of Residential Appliances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expansion of residential demand response programs and increased\ndeployment of controllable loads will require accurate appliance-level load\nmodeling and forecasting. This paper proposes a conditional hidden semi-Markov\nmodel to describe the probabilistic nature of residential appliance demand, and\nan algorithm for short-term load forecasting. Model parameters are estimated\ndirectly from power consumption data using scalable statistical learning\nmethods. Case studies performed using sub-metered 1-minute power consumption\ndata from several types of appliances demonstrate the effectiveness of the\nmodel for load forecasting and anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 22:18:39 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Ji", "Yuting", ""], ["Buechler", "Elizabeth", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1810.03781", "submitter": "Zheng Hao", "authors": "Zheng Hao, Miao Liu and Xijin Ge", "title": "Evaluating the Effectiveness of Health Awareness Events by Google Search\n  Frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over two hundreds health awareness events take place in the United States in\norder to raise attention and educate the public about diseases. It would be\ninformative and instructive for the organization to know the impact of these\nevents, although such information could be difficult to measure. Here 46 events\nare selected and their data from 2004 to 2017 are downloaded from Google\nTrend(GT). We investigate whether the events effectively attract the public\nattention by increasing the search frequencies of certain keywords which we\ncall queries. Three statistical methods including Transfer Function Noise\nmodeling, Wilcoxon Rank Sum test, and Binomial inference are conducted on 46 GT\ndata sets. Our study show that 10 health awareness events are effective with\nevidence of a significant increase in search frequencies in the event months,\nand 28 events are ineffective, with the rest being classified as unclear.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 02:41:50 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Hao", "Zheng", ""], ["Liu", "Miao", ""], ["Ge", "Xijin", ""]]}, {"id": "1810.03814", "submitter": "Yueyong Shi", "authors": "Jian Huang, Yuling Jiao, Xiliang Lu, Yueyong Shi, Qinglong Yang", "title": "SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 04:44:42 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["Shi", "Yueyong", ""], ["Yang", "Qinglong", ""]]}, {"id": "1810.03855", "submitter": "Ivan Lazarevich", "authors": "Ivan Lazarevich, Ilya Prokin, and Boris Gutkin", "title": "Neural activity classification with machine learning models trained on\n  interspike interval series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flow of information through the brain is reflected by the activity\npatterns of neural cells. Indeed, these firing patterns are widely used as\ninput data to predictive models that relate stimuli and animal behavior to the\nactivity of a population of neurons. However, relatively little attention was\npaid to single neuron spike trains as predictors of cell or network properties\nin the brain. In this work, we introduce an approach to neuronal spike train\ndata mining which enables effective classification and clustering of neuron\ntypes and network activity states based on single-cell spiking patterns. This\napproach is centered around applying state-of-the-art time series\nclassification/clustering methods to sequences of interspike intervals recorded\nfrom single neurons. We demonstrate good performance of these methods in tasks\ninvolving classification of neuron type (e.g. excitatory vs. inhibitory cells)\nand/or neural circuit activity state (e.g. awake vs. REM sleep vs. nonREM sleep\nstates) on an open-access cortical spiking activity dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 08:35:47 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 15:00:52 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Lazarevich", "Ivan", ""], ["Prokin", "Ilya", ""], ["Gutkin", "Boris", ""]]}, {"id": "1810.03919", "submitter": "Leonardo Azevedo", "authors": "Leonardo Azevedo and Vasily Demyanov", "title": "Multi-scale uncertainty quantification in geostatistical seismic\n  inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical seismic inversion is commonly used to infer the spatial\ndistribution of the subsurface petro-elastic properties by perturbing the model\nparameter space through iterative stochastic sequential\nsimulations/co-simulations. The spatial uncertainty of the inferred\npetro-elastic properties is represented with the updated a posteriori variance\nfrom an ensemble of the simulated realizations. Within this setting, the\nlarge-scale geological (metaparameters) used to generate the petro-elastic\nrealizations, such as the spatial correlation model and the global a priori\ndistribution of the properties of interest, are assumed to be known and\nstationary for the entire inversion domain. This assumption leads to\nunderestimation of the uncertainty associated with the inverted models. We\npropose a practical framework to quantify uncertainty of the large-scale\ngeological parameters in seismic inversion. The framework couples\ngeostatistical seismic inversion with a stochastic adaptive sampling and\nBayesian inference of the metaparameters to provide a more accurate and\nrealistic prediction of uncertainty not restricted by heavy assumptions on\nlarge-scale geological parameters. The proposed framework is illustrated with\nboth synthetic and real case studies. The results show the ability retrieve\nmore reliable acoustic impedance models with a more adequate uncertainty spread\nwhen compared with conventional geostatistical seismic inversion techniques.\nThe proposed approach separately account for geological uncertainty at\nlarge-scale (metaparameters) and local scale (trace-by-trace inversion).\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:27:59 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Azevedo", "Leonardo", ""], ["Demyanov", "Vasily", ""]]}, {"id": "1810.04056", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas, Ioannis N. Daliakopoulos, and Tim. G. Benton", "title": "Data-driven competitive facilitative tree interactions and their\n  implications on nature-based solutions", "comments": null, "journal-ref": null, "doi": "10.1016/j.scitotenv.2018.09.349", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio temporal data are more ubiquitous and richer than even before and the\navailability of such data poses great challenges in data analytics. Ecological\nfacilitation, the positive effect of density of individuals on the individual's\nsurvival across a stress gradient, is a complex phenomenon. A large number of\ntree individuals coupled with soil moisture, temperature, and water stress data\nacross a long temporal period were followed. Data driven analysis in the\nabsence of hypothesis was performed. Information theoretic analysis of multiple\nstatistical models was employed in order to quantify the best data-driven index\nof vegetation density and spatial scale of interactions. Sequentially, tree\nsurvival was quantified as a function of the size of the individual, vegetation\ndensity, and time at the optimal spatial interaction scale. Land surface\ntemperature and soil moisture were also statistically explained by tree size,\ndensity, and time. Results indicated that in space both facilitation and\ncompetition coexist in the same ecosystem and the sign and magnitude of this\ndepend on the spatial scale. Overall, within the optimal data driven spatial\nscale, tree survival was best explained by the interaction between density and\nyear, sifting overall from facilitation to competition through time. However,\nsmall sized trees were always facilitated by increased densities, while large\nsized trees had either negative or no density effects. Tree size was more\nimportant predictor than density in survival and this has implications for\nnature based solutions: maintaining large tree individuals or planting species\nthat can become large-sized can safeguard against tree less areas by promoting\nsurvival at long time periods through harsh environmental conditions. Large\ntrees had also a significant effect in moderating land surface temperature.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:53:52 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Moustakas", "Aristides", ""], ["Daliakopoulos", "Ioannis N.", ""], ["Benton", "Tim. G.", ""]]}, {"id": "1810.04064", "submitter": "Miao Cheng", "authors": "Miao Cheng, Zunren Liu, Hongwei Zou, Ah Chung Tsoi", "title": "A Family of Maximum Margin Criterion for Adaptive Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, pattern analysis plays an important role in data mining and\nrecognition, and many variants have been proposed to handle complicated\nscenarios. In the literature, it has been quite familiar with high\ndimensionality of data samples, but either such characteristics or large data\nhave become usual sense in real-world applications. In this work, an improved\nmaximum margin criterion (MMC) method is introduced firstly. With the new\ndefinition of MMC, several variants of MMC, including random MMC, layered MMC,\n2D^2 MMC, are designed to make adaptive learning applicable. Particularly, the\nMMC network is developed to learn deep features of images in light of simple\ndeep networks. Experimental results on a diversity of data sets demonstrate the\ndiscriminant ability of proposed MMC methods are compenent to be adopted in\ncomplicated application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:45:53 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:47:47 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Cheng", "Miao", ""], ["Liu", "Zunren", ""], ["Zou", "Hongwei", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "1810.04087", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o and Csaba T\\'oth", "title": "University rankings from the revealed preferences of the applicants", "comments": "38 pages, 4 figures, 8 tables", "journal-ref": "European Journal of Operational Research, 286(1): 309-320, 2020", "doi": "10.1016/j.ejor.2020.03.008", "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A methodology is presented to rank universities on the basis of the lists of\nprogrammes the students applied for. We exploit a crucial feature of the\ncentralised assignment system to higher education in Hungary: a student is\nadmitted to the first programme where the score limit is achieved. This makes\nit possible to derive a partial preference order of each applicant. Our\napproach integrates the information from all students participating in the\nsystem, is free of multicollinearity among the indicators, and contains few ad\nhoc parameters. The procedure is implemented to rank faculties in the Hungarian\nhigher education between 2001 and 2016. We demonstrate that the ranking given\nby the least squares method has favourable theoretical properties, is robust\nwith respect to the aggregation of preferences, and performs well in practice.\nThe suggested ranking is worth considering as a reasonable alternative to the\nstandard composite indices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 15:44:53 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 12:42:43 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 10:17:51 GMT"}, {"version": "v4", "created": "Sun, 2 Jun 2019 10:57:09 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 12:51:55 GMT"}, {"version": "v6", "created": "Wed, 26 Feb 2020 14:14:50 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""], ["T\u00f3th", "Csaba", ""]]}, {"id": "1810.04099", "submitter": "Daniela Castro-Camilo", "authors": "Daniela Castro-Camilo, Rapha\\\"el Huser, H{\\aa}vard Rue", "title": "A spliced Gamma-Generalized Pareto model for short-term extreme wind\n  speed probabilistic forecasting", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable sources of energy such as wind power have become a sustainable\nalternative to fossil fuel-based energy. However, the uncertainty and\nfluctuation of the wind speed derived from its intermittent nature bring a\ngreat threat to the wind power production stability, and to the wind turbines\nthemselves. Lately, much work has been done on developing models to forecast\naverage wind speed values, yet surprisingly little has focused on proposing\nmodels to accurately forecast extreme wind speeds, which can damage the\nturbines. In this work, we develop a flexible spliced Gamma-Generalized Pareto\nmodel to forecast extreme and non-extreme wind speeds simultaneously. Our model\nbelongs to the class of latent Gaussian models, for which inference is\nconveniently performed based on the integrated nested Laplace approximation\nmethod. Considering a flexible additive regression structure, we propose two\nmodels for the latent linear predictor to capture the spatio-temporal dynamics\nof wind speeds. Our models are fast to fit and can describe both the bulk and\nthe tail of the wind speed distribution while producing short-term extreme and\nnon-extreme wind speed probabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:13:14 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 08:42:24 GMT"}, {"version": "v3", "created": "Sat, 29 Jun 2019 06:13:09 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Castro-Camilo", "Daniela", ""], ["Huser", "Rapha\u00ebl", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1810.04140", "submitter": "Katherine Wilson", "authors": "Katie Wilson, Jon Wakefield", "title": "Child Mortality Estimation Incorporating Summary Birth History Data", "comments": "50 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations' Sustainable Development Goal 3.2 aims to reduce under-5\nchild mortality to 25 deaths per 1,000 live births by 2030. Child mortality\ntends to be concentrated in developing regions where much of the information\nneeded to assess achievement of this goal comes from surveys and censuses. In\nboth, women are asked about their birth histories, but with varying degrees of\ndetail. Full birth history (FBH) data contain the reported dates of births and\ndeaths of every surveyed mother's children. In contrast, summary birth history\n(SBH) data contain only the total number of children born and total number of\nchildren who died for each mother. Specialized methods are needed to\naccommodate this type of data into analyses of child mortality trends. We\ndevelop a data augmentation scheme within a Bayesian framework where for SBH\ndata, birth and death dates are introduced as auxiliary variables. Since we\nspecify a full probability model for the data, many of the well-known biases\nthat exist in this data can be accommodated, along with space-time smoothing on\nthe underlying mortality rates. We illustrate our approach in a simulation,\nshowing that uncertainty is reduced when incorporating SBH data over simply\nanalyzing all available FBH data. We also apply our approach to data in the\nCentral region of Malawi. We compare with the well-known Brass method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:19:05 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wilson", "Katie", ""], ["Wakefield", "Jon", ""]]}, {"id": "1810.04195", "submitter": "Merlin Keller", "authors": "M. Keller, G. Damblin, A. Pasanisi, M. Schuman, P. Barbillon, F.\n  Ruggeri, E. Parent", "title": "Validation of a computer code for the energy consumption of a building,\n  with application to optimal electric bill pricing", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a practical Bayesian framework for the calibration\nand validation of a computer code, and apply it to a case study concerning the\nenergy consumption forecasting of a building. Validation allows to quantify\nforecasting uncertainties in view of the code's final use. Here we explore the\nsituation where an energy provider promotes new energy contracts for\nresidential buildings, tailored to each customer's needs, and including a\nguarantee of energy performance.\n  Based on power field measurements, collected from an experimental building\ncell over a certain time period, the code is calibrated, effectively reducing\nthe epistemic uncertainty affecting some code parameters (here albedo, thermal\nbridge factor and convective coefficient). Validation is conducted by testing\nthe goodness of fit of the code with respect to field measures, and then by\npropagating the a posteriori parametric uncertainty through the code, yielding\nprobabilistic forecasts of the average electric power delivered inside the cell\nover a given time period.\n  To illustrate the benefits of the proposed Bayesian validation framework, we\naddress the decision problem for an energy supplier offering a new type of\ncontract, wherein the customer pays a fixed fee chosen in advance, based on an\noverall energy consumption forecast. According to Bayesian decision theory, we\nshow how to choose such a fee optimally from the point of view of the supplier,\nin order to balance short-terms benefits with customer loyalty.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 15:13:19 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Keller", "M.", ""], ["Damblin", "G.", ""], ["Pasanisi", "A.", ""], ["Schuman", "M.", ""], ["Barbillon", "P.", ""], ["Ruggeri", "F.", ""], ["Parent", "E.", ""]]}, {"id": "1810.04281", "submitter": "Wolfram Gronwald", "authors": "Helena U. Zacharias, Michael Altenbuchinger, Stefan Solbrig, Andreas\n  Sch\\\"afer, Mustafa Buyukozkan, Ulla T. Schulthei{\\ss}, Fruzsina Kotsis, Anna\n  K\\\"ottgen, Jan Krumsiek, Fabian J. Theis, Rainer Spang, Peter J. Oefner,\n  Wolfram Gronwald, and GCKD study investigators", "title": "Fully integrative data analysis of NMR metabolic fingerprints with\n  comprehensive patient data: a case report based on the German Chronic Kidney\n  Disease (GCKD) study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omics data facilitate the gain of novel insights into the pathophysiology of\ndiseases and, consequently, their diagnosis, treatment, and prevention. To that\nend, it is necessary to integrate omics data with other data types such as\nclinical, phenotypic, and demographic parameters of categorical or continuous\nnature. Here, we exemplify this data integration issue for a study on chronic\nkidney disease (CKD), where complex clinical and demographic parameters were\nassessed together with one-dimensional (1D) 1H NMR metabolic fingerprints.\nRoutine analysis screens for associations of single metabolic features with\nclinical parameters, which requires confounding variables typically chosen by\nexpert knowledge to be taken into account. This knowledge can be incomplete or\nunavailable. The results of this article are manifold. We introduce a framework\nfor data integration that intrinsically adjusts for confounding variables. We\ngive its mathematical and algorithmic foundation, provide a state-of-the-art\nimplementation, and give several sanity checks. In particular, we show that the\ndiscovered associations remain significant after variable adjustment based on\nexpert knowledge. In contrast, we illustrate that the discovery of associations\nin routine analysis can be biased by incorrect or incomplete expert knowledge\nin univariate screening approaches. Finally, we exemplify how our data\nintegration approach reveals important associations between CKD comorbidities\nand metabolites. Moreover, we evaluate the predictive performance of the\nestimated models on independent validation data and contrast the results with a\nnaive screening approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:39:18 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Zacharias", "Helena U.", ""], ["Altenbuchinger", "Michael", ""], ["Solbrig", "Stefan", ""], ["Sch\u00e4fer", "Andreas", ""], ["Buyukozkan", "Mustafa", ""], ["Schulthei\u00df", "Ulla T.", ""], ["Kotsis", "Fruzsina", ""], ["K\u00f6ttgen", "Anna", ""], ["Krumsiek", "Jan", ""], ["Theis", "Fabian J.", ""], ["Spang", "Rainer", ""], ["Oefner", "Peter J.", ""], ["Gronwald", "Wolfram", ""], ["investigators", "GCKD study", ""]]}, {"id": "1810.04338", "submitter": "Qian Guan", "authors": "Qian Guan, Brian J. Reich, Eric B. Laber and Dipankar Bandyopadhyay", "title": "Bayesian Nonparametric Policy Search with Application to Periodontal\n  Recall Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tooth loss from periodontal disease is a major public health burden in the\nUnited States. Standard clinical practice is to recommend a dental visit every\nsix months; however, this practice is not evidence-based, and poor dental\noutcomes and increasing dental insurance premiums indicate room for\nimprovement. We consider a tailored approach that recommends recall time based\non patient characteristics and medical history to minimize disease progression\nwithout increasing resource expenditures. We formalize this method as a dynamic\ntreatment regime which comprises a sequence of decisions, one per stage of\nintervention, that follow a decision rule which maps current patient\ninformation to a recommendation for their next visit time. The dynamics of\nperiodontal health, visit frequency, and patient compliance are complex, yet\nthe estimated optimal regime must be interpretable to domain experts if it is\nto be integrated into clinical practice. We combine non-parametric Bayesian\ndynamics modeling with policy-search algorithms to estimate the optimal dynamic\ntreatment regime within an interpretable class of regimes. Both simulation\nexperiments and application to a rich database of electronic dental records\nfrom the HealthPartners HMO shows that our proposed method leads to better\ndental health without increasing the average recommended recall time relative\nto competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 02:32:12 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Guan", "Qian", ""], ["Reich", "Brian J.", ""], ["Laber", "Eric B.", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "1810.04419", "submitter": "Nicolas Raillard", "authors": "Nicolas Raillard, Marc Prevosto and H\\'el\\`ene Pineau", "title": "Comparison of 3-D contouring methodologies through the study of extreme\n  tension in a mooring line of a semi-submersible", "comments": "Accepted in Ocean Engineering", "journal-ref": null, "doi": "10.1016/j.oceaneng.2019.05.016", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Design optimization is crucial as offshore structures are exposed to deeper\nand harsher marine conditions. The structure behaviour is dependent on several\njoint environmental parameters (wind, wave, currents, etc.). Environmental\ncontours are useful representations to provide multivariate design conditions.\nHowever, these contours may lead to different design points depending on the\nmethod used to compute them, and thus may be misleading to structural engineer.\nIn this work, we propose to use a response meta-model for the inter-comparison\nof some state-of-the-art methods available for modelling multivariate extremes,\nin order to provide a straightforward methodology, focusing on the derivation\nof three-dimensional contours. The considered case study focuses on the tension\nin a mooring line of a semi-submersible platform. In a first step, the key\nmet-ocean parameters and the associated load model of the tension in the\nmooring line are set-up. Several multivariate extreme analysis methods are then\napplied to derive the environmental contours. These methods are chosen in order\nto cover all the possible dependence cases, from extremal dependence to\nextremal independence. Conditional Extreme and several extreme value dependence\nfunction models are investigated. The physical-space Huseby contouring method\nis used to derive environmental surface. A comparison with the extreme load\nextrapolated from the meta-model is provided to assess the performance of each\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 08:50:53 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 15:19:28 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 07:15:04 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Raillard", "Nicolas", ""], ["Prevosto", "Marc", ""], ["Pineau", "H\u00e9l\u00e8ne", ""]]}, {"id": "1810.04498", "submitter": "Jose Ameijeiras-Alonso", "authors": "Jose Ameijeiras-Alonso, Akli Benali, Rosa M. Crujeiras, Alberto\n  Rodr\\'iguez-Casal and Jos\\'e M.C. Pereira", "title": "Fire seasonality identification with multimodality tests", "comments": null, "journal-ref": "Annals of Applied Statistics 13, 2120-2139 (2019)", "doi": "10.1002/env.2501", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the role of vegetation fires in the Earth system has become an\nimportant environmental problem. Although fires time occurrence is mainly\ninfluenced by climate, human activity related with land use and management has\naltered fire patterns in several regions of the world. Hence, for a better\ninsight in fires regimes, it is of special interest to analyze where human\nactivity has influenced the fire seasonality. For doing so, multimodality tests\nare a useful tool for determining the number of fire peaks along the year. The\nperiodicity of climatological and human--altered fires and their complex\ndistributional features motivate the use of the nonparametric circular\nstatistics. The unsatisfactory performance of previous nonparametric proposals\nfor testing multimodality, in the circular case, justifies the introduction of\na new approach, accompanied by a correction of the False Discovery Rate with\nspatial dependence for a systematic application of the tests in a large area\nbetween Russia and Kazakhstan.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:57:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ameijeiras-Alonso", "Jose", ""], ["Benali", "Akli", ""], ["Crujeiras", "Rosa M.", ""], ["Rodr\u00edguez-Casal", "Alberto", ""], ["Pereira", "Jos\u00e9 M. C.", ""]]}, {"id": "1810.04654", "submitter": "Yung-Wen Liu", "authors": "Huiying Mao, Yung-wen Liu, Yuting Jia, Jay Nanduri", "title": "Adaptive Fraud Detection System Using Dynamic Risk Features", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eCommerce transaction frauds keep changing rapidly. This is the major issue\nthat prevents eCommerce merchants having a robust machine learning model for\nfraudulent transactions detection. The root cause of this problem is that rapid\nchanging fraud patterns alters underlying data generating system and causes the\nperformance deterioration for machine learning models. This phenomenon in\nstatistical modeling is called \"Concept Drift\". To overcome this issue, we\npropose an approach which adds dynamic risk features as model inputs. Dynamic\nrisk features are a set of features built on entity profile with fraud\nfeedback. They are introduced to quantify the fluctuation of probability\ndistribution of risk features from certain entity profile caused by concept\ndrift. In this paper, we also illustrate why this strategy can successfully\nhandle the effect of concept drift under statistical learning framework. We\nalso validate our approach on multiple businesses in production and have\nverified that the proposed dynamic model has a superior ROC curve than a static\nmodel built on the same data and training parameters.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 17:31:16 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Mao", "Huiying", ""], ["Liu", "Yung-wen", ""], ["Jia", "Yuting", ""], ["Nanduri", "Jay", ""]]}, {"id": "1810.04730", "submitter": "Feng Shi", "authors": "David Burstein, Franklin Kenter, Feng Shi", "title": "Leveraging local network communities to predict academic performance", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than 20 years, social network analysis of student collaboration\nnetworks has focused on a student's centrality to predict academic performance.\nAnd even though a growing amount of sociological literature has supported that\nacademic success is contagious, identifying central students in the network\nalone does not capture how peer interactions facilitate the spread of academic\nsuccess throughout the network. Consequently, we propose novel predictors that\ntreat academic success as a contagion by identifying a student's learning\ncommunity, consisting of the peers that are most likely to influence a\nstudent's performance in a course. We evaluate the importance of these learning\ncommunities by predicting academic outcomes in an introductory college\nstatistics course with 103 students. In particular, we observe that by\nincluding these learning community predictors, the resulting model is 68 times\nmore likely to be the correct model than the current state-of-the-art\ncentrality network models in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 19:58:01 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Burstein", "David", ""], ["Kenter", "Franklin", ""], ["Shi", "Feng", ""]]}, {"id": "1810.04734", "submitter": "Lennart Schneider", "authors": "Lennart Schneider, R. Philip Chalmers, Rudolf Debelak, Edgar C. Merkle", "title": "Model Selection of Nested and Non-Nested Item Response Models using\n  Vuong Tests", "comments": "This is an original manuscript / preprint of an article published by\n  Taylor & Francis in Multivariate Behavioral Research on the 18th September\n  2019, available online:\n  http://www.tandfonline.com/10.1080/00273171.2019.1664280. 49 pages, 3 tables,\n  5 figures", "journal-ref": null, "doi": "10.1080/00273171.2019.1664280", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply Vuong's (1989) general approach of model selection to\nthe comparison of nested and non-nested unidimensional and multidimensional\nitem response theory (IRT) models. Vuong's approach of model selection is\nuseful because it allows for formal statistical tests of both nested and\nnon-nested models. However, only the test of non-nested models has been applied\nin the context of IRT models to date. After summarizing the statistical theory\nunderlying the tests, we investigate the performance of all three distinct\nVuong tests in the context of IRT models using simulation studies and real\ndata. In the non-nested case we observed that the tests can reliably\ndistinguish between the graded response model and the generalized partial\ncredit model. In the nested case, we observed that the tests typically perform\nas well as or sometimes better than the traditional likelihood ratio test.\nBased on these results, we argue that Vuong's approach provides a useful set of\ntools for researchers and practitioners to effectively compare competing nested\nand non-nested IRT models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 20:19:46 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 21:39:37 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 13:57:45 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 12:53:08 GMT"}, {"version": "v5", "created": "Thu, 19 Sep 2019 09:19:16 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Schneider", "Lennart", ""], ["Chalmers", "R. Philip", ""], ["Debelak", "Rudolf", ""], ["Merkle", "Edgar C.", ""]]}, {"id": "1810.04748", "submitter": "Fabio Divino", "authors": "Fabio Divino, Johanna \\\"Arje, Antti Penttinen, Kristian Meissner,\n  Salme K\\\"arkk\\\"ainen", "title": "Empirical Bayes to assess ecological diversity and similarity with\n  overdispersion in multivariate counts", "comments": "40 pages, 10 figures, 5 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of diversity and similarity is relevant in monitoring the\nstatus of ecosystems. The respective indicators are based on the taxonomic\ncomposition of biological communities of interest, currently estimated through\nthe proportions computed from sampling multivariate counts. In this work we\npresent a novel method able to work with only one sample to estimate the\ntaxonomic composition when the data are affected by overdispersion. The\npresence of overdispersion in taxonomic counts may be the result of significant\nenvironmental factors which are often unobservable but influence communities.\nFollowing the empirical Bayes approach, we combine a Bayesian model with the\nmarginal likelihood method to jointly estimate the taxonomic proportions and\nthe level of overdispersion from one sample of multivariate counts. Our\nproposal is compared to the classical maximum likelihood method in an extensive\nsimulation study with different realistic scenarios. An application to real\ndata from aquatic biomonitoring is also presented. In both the simulation study\nand the real data application, we consider communities characterized by a large\nnumber of taxonomic categories, such as aquatic macroinvertebrates or bacteria\nwhich are often overdispersed. The applicative results demonstrate an overall\nsuperiority of the empirical Bayes method in almost all examined cases, for\nboth assessments of diversity and similarity. We would recommend practitioners\nin biomonitoring to use the proposed approach in addition to the traditional\nprocedures. The empirical Bayes estimation allows to better control the error\npropagation due to the presence of overdispersion in biological data, with a\nmore efficient managerial decision making.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 21:02:52 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Divino", "Fabio", ""], ["\u00c4rje", "Johanna", ""], ["Penttinen", "Antti", ""], ["Meissner", "Kristian", ""], ["K\u00e4rkk\u00e4inen", "Salme", ""]]}, {"id": "1810.04785", "submitter": "Rahul Ghosal", "authors": "Sedigheh Mirzaei Salehabadi, Debasis Sengupta, Rahul Ghosal", "title": "Estimating menarcheal age distribution from partially recalled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a cross-sectional study, adolescent and young adult females were asked to\nrecall the time of menarche, if experienced. Some respondents recalled the date\nexactly, some recalled only the month or the year of the event, and some were\nunable to recall anything. We consider estimation of the menarcheal age\ndistribution from this interval censored data. A~complicated interplay between\nage-at-event and calendar time, together with the evident fact of memory fading\nwith time, makes the censoring informative. We propose a model where the\nprobabilities of various types of recall would depend on the time since\nmenarche. For parametric estimation we model these probabilities using\nmultinomial regression function. Establishing consistency and asymptotic\nnormality of the parametric MLE requires a bit of tweaking of the standard\nasymptotic theory, as the data format varies from case to case. We also provide\na non-parametric MLE, propose a computationally simpler approximation, and\nestablish the consistency of both these estimators under mild conditions. We\nstudy the small sample performance of the parametric and non-parametric\nestimators through Monte Carlo simulations. Moreover, we provide a graphical\ncheck of the assumption of the multinomial model for the recall probabilities,\nwhich appears to hold for the menarcheal data set. Our analysis shows that the\nuse of the partially recalled part of the data indeed leads to smaller\nconfidence intervals of the survival function.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 23:49:50 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 20:37:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Salehabadi", "Sedigheh Mirzaei", ""], ["Sengupta", "Debasis", ""], ["Ghosal", "Rahul", ""]]}, {"id": "1810.04912", "submitter": "Claude Grasland", "authors": "Claude Grasland (GC, GIS CIST)", "title": "International news flows theory revisited through a space-time\n  interaction model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a quantitative model of the circulation of foreign news\nbased on a gravity-like model of spatial interaction disaggregated by time,\nmedia and countries of interest. The analysis of international RSS news stories\npublished by 31 daily newspapers in 2015 demonstrates, first, that many of the\nlaws of circulation of international news predicted half a century ago by\nGaltung and Ruge and by {\\\"O}stgaard are still valid. The salience of countries\nin media remains strongly determined by size effects (area, population), with\nprominent coverage of rich countries (GDP/capita) with elite status (permanent\nmembers of UNSC, the Holy See). The effect of geographical distance and a\ncommon language remains a major factor of media coverage in newsrooms.\nContradicting the flat world hypothesis, global journalism remains an\nexception, and provincialism is the rule. The disaggregation of the model by\nmedia demonstrates that newspapers are not following exactly the same rules and\nare more or less sensitive to distance, a common language or elite status. The\ndisaggregation of the model by week suggests that the rules governing foreign\nnews can also be temporarily modified by exceptional events that eliminate the\nusual effects of salience and relatedness, producing short periods of 'global\nconsensus' that can benefit small, poor and remote countries. This paper\nconcludes by recommending the use of a sample of carefully chosen diversified\nmedia rather than a large aggregation of data for global studies.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 09:17:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Grasland", "Claude", "", "GC, GIS CIST"]]}, {"id": "1810.05004", "submitter": "Longfei Wei", "authors": "Longfei Wei, Arif I. Sarwat", "title": "Hybrid integration of multilayer perceptrons and parametric models for\n  reliability forecasting in the smart grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The reliable power system operation is a major goal for electric utilities,\nwhich requires the accurate reliability forecasting to minimize the duration of\npower interruptions. Since weather conditions are usually the leading causes\nfor power interruptions in the smart grid, especially for its distribution\nnetworks, this paper comprehensively investigates the combined effect of\nvarious weather parameters on the reliability performance of distribution\nnetworks. Specially, a multilayer perceptron (MLP) based framework is proposed\nto forecast the daily numbers of sustained and momentary power interruptions in\none distribution management area using time series of common weather data.\nFirst, the parametric regression models are implemented to analyze the\nrelationship between the daily numbers of power interruptions and various\ncommon weather parameters, such as temperature, precipitation, air pressure,\nwind speed, and lightning. The selected weather parameters and corresponding\nparametric models are then integrated as inputs to formulate a MLP neural\nnetwork model to predict the daily numbers of power interruptions. A modified\nextreme learning machine (ELM) based hierarchical learning algorithm is\nintroduced for training the formulated model using realtime reliability data\nfrom an electric utility in Florida and common weather data from National\nClimatic Data Center (NCDC). In addition, the sensitivity analysis is\nimplemented to determine the various impacts of different weather parameters on\nthe daily numbers of power interruptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 19:12:03 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wei", "Longfei", ""], ["Sarwat", "Arif I.", ""]]}, {"id": "1810.05189", "submitter": "E. G. Patrick Bos", "authors": "E. G. Patrick Bos, Francisco-Shu Kitaura, Rien van de Weygaert", "title": "Bayesian cosmic density field inference from redshift space dark matter\n  maps", "comments": "34 pages, 25 figures, 1 table. Submitted to MNRAS. Accompanying code\n  at https://github.com/egpbos/barcode", "journal-ref": null, "doi": "10.1093/mnras/stz1864", "report-no": null, "categories": "astro-ph.CO stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-consistent Bayesian formalism to sample the primordial\ndensity fields compatible with a set of dark matter density tracers after\ncosmic evolution observed in redshift space. Previous works on density\nreconstruction did not self-consistently consider redshift space distortions or\nincluded an additional iterative distortion correction step. We present here\nthe analytic solution of coherent flows within a Hamiltonian Monte Carlo\nposterior sampling of the primordial density field. We test our method within\nthe Zel'dovich approximation, presenting also an analytic solution including\ntidal fields and spherical collapse on small scales using augmented Lagrangian\nperturbation theory. Our resulting reconstructed fields are isotropic and their\npower spectra are unbiased compared to the true one defined by our mock\nobservations. Novel algorithmic implementations are introduced regarding the\nmass assignment kernels when defining the dark matter density field and\noptimization of the time step in the Hamiltonian equations of motions. Our\nalgorithm, dubbed barcode, promises to be specially suited for analysis of the\ndark matter cosmic web down to scales of a few Megaparsecs. This large scale\nstructure is implied by the observed spatial distribution of galaxy clusters\n--- such as obtained from X-ray, SZ or weak lensing surveys --- as well as that\nof the intergalactic medium sampled by the Lyman alpha forest or perhaps even\nby deep hydrogen intensity mapping. In these cases, virialized motions are\nnegligible, and the tracers cannot be modeled as point-like objects. It could\nbe used in all of these contexts as a baryon acoustic oscillation\nreconstruction algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:07:54 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:33:25 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 12:16:00 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bos", "E. G. Patrick", ""], ["Kitaura", "Francisco-Shu", ""], ["van de Weygaert", "Rien", ""]]}, {"id": "1810.05204", "submitter": "Mehran Yarahmadi", "authors": "Mehran Yarahmadi and J. Robert Mahan", "title": "Verification of Two-Dimensional Monte Carlo Ray-Trace Methodology in\n  Radiation Heat Transfer Analysis", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.12632.14089", "report-no": null, "categories": "physics.comp-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the frequent appearance in the radiation heat transfer literature of\narticles describing Monte Carlo ray-trace (MCRT) applications to\ntwo-dimensional enclosures, no formal verification may be found of the method\ncommonly used to determine the directional distribution of diffuse emission and\nreflection when estimating two-dimensional radiation distribution factors.\nConsidered are two methods for determining the direction cosines in this\nsituation. The results are shown to be in agreement with those obtained in the\nlimiting case of a three-dimensional enclosure as one of its dimensions is\nincreased.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:58:30 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 19:56:43 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 17:44:33 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yarahmadi", "Mehran", ""], ["Mahan", "J. Robert", ""]]}, {"id": "1810.05297", "submitter": "Ick Hoon Jin", "authors": "Ick Hoon Jin, Fang Liu, Evercita C. Eugenio, Kisung You, Suyu Liu", "title": "Bayesian Hierarchical Spatial Model for Small Area Estimation with\n  Non-ignorable Nonresponses and Its Applications to the NHANES Dental Caries\n  Assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Health and Nutrition Examination Survey (NHANES) is a major\nprogram of the National Center for Health Statistics, designed to assess the\nhealth and nutritional status of adults and children in the United States. The\nanalysis of NHANES dental caries data faces several challenges, including (1)\nthe data were collected using a complex, multistage, stratified,\nunequal-probability sampling design; (2) the sample size of some primary\nsampling units (PSU), e.g., counties, is very small; (3) the measures of dental\ncaries have complicated structure and correlation, and (4) there is a\nsubstantial percentage of nonresponses, for which the missing data are expected\nto be not missing at random or non-ignorable. We propose a Bayesian\nhierarchical spatial model to address these analysis challenges. We develop a\ntwo-level Potts model that closely resembles the caries evolution process and\ncaptures complicated spatial correlations between teeth and surfaces of the\nteeth. By adding Bayesian hierarchies to the Potts model, we account for the\nmultistage survey sampling design and also enable information borrowing across\nPSUs for small area estimation. We incorporate sampling weights by including\nthem as a covariate in the model and adopt flexible B-splines to achieve robust\ninference. We account for non-ignorable missing outcomes and covariates using\nthe selection model. We use data augmentation coupled with the noisy exchange\nsampler to obtain the posterior of model parameters that involve\ndoubly-intractable normalizing constants. Our analysis results show strong\nspatial associations between teeth and tooth surfaces and that dental hygienic\nfactors, fluorosis and sealant reduce the risks of having dental diseases.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 00:18:24 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 20:39:06 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 10:26:20 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Jin", "Ick Hoon", ""], ["Liu", "Fang", ""], ["Eugenio", "Evercita C.", ""], ["You", "Kisung", ""], ["Liu", "Suyu", ""]]}, {"id": "1810.05450", "submitter": "Paul Kirk", "authors": "Oliver M. Crook, Laurent Gatto, Paul D. W. Kirk", "title": "Fast approximate inference for variable selection in Dirichlet process\n  mixtures, with an application to pan-cancer proteomics", "comments": "27 pages, 8 figures. For associated R package, see\n  https://github.com/ococrook/sugsvarsel", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet Process (DP) mixture model has become a popular choice for\nmodel-based clustering, largely because it allows the number of clusters to be\ninferred. The sequential updating and greedy search (SUGS) algorithm (Wang and\nDunson, 2011) was proposed as a fast method for performing approximate Bayesian\ninference in DP mixture models, by posing clustering as a Bayesian model\nselection (BMS) problem and avoiding the use of computationally costly Markov\nchain Monte Carlo methods. Here we consider how this approach may be extended\nto permit variable selection for clustering, and also demonstrate the benefits\nof Bayesian model averaging (BMA) in place of BMS. Through an array of\nsimulation examples and well-studied examples from cancer transcriptomics, we\nshow that our method performs competitively with the current state-of-the-art,\nwhile also offering computational benefits. We apply our approach to\nreverse-phase protein array (RPPA) data from The Cancer Genome Atlas (TCGA) in\norder to perform a pan-cancer proteomic characterisation of 5,157 tumour\nsamples. We have implemented our approach, together with the original SUGS\nalgorithm, in an open-source R package named sugsvarsel, which accelerates\nanalysis by performing intensive computations in C++ and provides automated\nparallel processing. The R package is freely available from:\nhttps://github.com/ococrook/sugsvarsel\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 11:17:17 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Crook", "Oliver M.", ""], ["Gatto", "Laurent", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1810.05497", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts and Anshumali Shrivastava", "title": "Probabilistic Blocking with An Application to the Syrian Conflict", "comments": "16 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1510.07714, arXiv:1710.02690", "journal-ref": "Steorts R.C., Shrivastava A. (2018) Probabilistic Blocking with an\n  Application to the Syrian Conflict. PSD (2018)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution seeks to merge databases as to remove duplicate entries\nwhere unique identifiers are typically unknown. We review modern blocking\napproaches for entity resolution, focusing on those based upon locality\nsensitive hashing (LSH). First, we introduce $k$-means locality sensitive\nhashing (KLSH), which is based upon the information retrieval literature and\nclusters similar records into blocks using a vector-space representation and\nprojections. Second, we introduce a subquadratic variant of LSH to the\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\npropose a weighted variant of DOPH. We illustrate each method on an application\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 01:16:31 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1810.05525", "submitter": "Xiangru Jian", "authors": "Xiangru Jian, Paulo J.M Monteiro, Kimberly E.Kurtis", "title": "Predicting the Expansion of Concrete Exposed to Sulfate Attack with a\n  Regression Model Based on a Performance Classification", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly describes the development of a new type of regression model\nto predict the long-term expansion of concrete subjected to a sulfate-rich\nenvironment. The experimental data originated from a long-term (40+ years),\nnonaccelerated test program performed by the U.S. Bureau of Reclamation (USBR).\nExpansion data of specimens composed of 54 different mixtures were measured\nperiodically throughout the entire test program. In this analysis, the mixtures\nwere first classified into three groups using K-means clustering based on their\nexpansion patterns. Within each group, the expansion rate was predicted as an\nexclusive regression function of the water-cement ratio (W/C), tricalcium\naluminate (C_3 A) content of cement, cement content of cement or time of\nexpansion. Then, a support vector machine (SVM) was employed to determine the\nclassification criteria by relying on the characteristics of the mixture\nproportions rather than the experimental performance, thereby enabling the\nmodel to offer predictions for new mixtures without test data. An analysis of\nthe model indicated that concrete specimens with different mixture proportions,\nespecially with different W/C values or C_3 A contents, are unlikely to share\nidentical expansion patterns and should be considered and predicted\nseparately.separately.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 01:07:51 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 02:39:21 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Jian", "Xiangru", ""], ["Monteiro", "Paulo J. M", ""], ["Kurtis", "Kimberly E.", ""]]}, {"id": "1810.05643", "submitter": "Shintaro Mori", "authors": "Shintaro Mori and Masato Hisakado and Kazuaki Nakayama", "title": "A voter model on networks and multivariate beta distribution", "comments": "16 pages, 6 figures", "journal-ref": "Phys. Rev. E 99, 052307 (2019)", "doi": "10.1103/PhysRevE.99.052307", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In elections, the vote shares or turnout rates show a strong spatial\ncorrelation. The logarithmic decay with distance suggests that a 2D noisy\ndiffusive equation describes the system. Based on the study of U.S.\npresidential elections data, it was determined that the fluctuations of vote\nshares also exhibit a strong and long-range spatial correlation. Previously, it\nwas considered difficult to induce strong and long-range spatial correlation of\nthe vote shares without breaking the empirically observed narrow distribution.\nWe demonstrate that a voter model on networks shows such a behavior. In the\nmodel, there are many voters in a node who are affected by the agents in the\nnode and by the agents in the linked nodes. A multivariate Wright-Fisher\ndiffusion equation for the joint probability density of the vote shares is\nderived. The stationary distribution is a multivariate generalization of the\nbeta distribution. In addition, we also estimate the equilibrium values and the\ncovariance matrix of the vote shares and obtain a correspondence with a\nmultivariate normal distribution. This approach largely simplifies the\ncalibration of the parameters in the modeling of elections.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 01:51:22 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 09:58:16 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Mori", "Shintaro", ""], ["Hisakado", "Masato", ""], ["Nakayama", "Kazuaki", ""]]}, {"id": "1810.05763", "submitter": "Alejandro Lim", "authors": "Alejandro Lim, Chin-Tsang Chiang, Jen-Chieh Teng", "title": "Estimating Robot Strengths with Application to Selection of Alliance\n  Members in FIRST Robotics Competitions", "comments": "16 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of the FIRST Robotics Competition (FRC) and its special\nplayoff system, robotics teams have longed to appropriately quantify the\nstrengths of their designed robots. The FRC includes a playground draft-like\nphase (alliance selection), arguably the most game-changing part of the\ncompetition, in which the top-8 robotics teams in a tournament based on the\nFRC's ranking system assess potential alliance members for the opportunity of\npartnering in a playoff stage. In such a three-versus-three competition,\nseveral measures and models have been used to characterize actual or relative\nrobot strengths. However, existing models are found to have poor predictive\nperformance due to their imprecise estimates of robot strengths caused by a\nsmall ratio of the number of observations to the number of robots. A more\ngeneral regression model with latent clusters of robot strengths is, thus,\nproposed to enhance their predictive capacities. Two effective estimation\nprocedures are further developed to simultaneously estimate the number of\nclusters, clusters of robots, and robot strengths. Meanwhile, some measures are\nused to assess the predictive ability of competing models, the agreement\nbetween published FRC measures of strength and model-based robot strengths of\nall, playoff, and FRC top-8 robots, and the agreement between FRC top-8 robots\nand model-based top robots. Moreover, the stability of estimated robot\nstrengths and accuracies is investigated to determine whether the scheduled\nmatches are excessive or insufficient. In the analysis of qualification data\nfrom the 2018 FRC Houston and Detroit championships, the predictive ability of\nour model is also shown to be significantly better than those of existing\nmodels. Teams who adopt the new model can now appropriately rank their\npreferences for playoff alliance partners with greater predictive capability\nthan before.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 23:56:29 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 19:24:08 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 18:54:48 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 18:10:36 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lim", "Alejandro", ""], ["Chiang", "Chin-Tsang", ""], ["Teng", "Jen-Chieh", ""]]}, {"id": "1810.05967", "submitter": "Luis Alberto Barboza", "authors": "Luis A. Barboza, Julien Emile-Geay, Bo Li, Wan He", "title": "Efficient Reconstructions of Common Era Climate via Integrated Nested\n  Laplace Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paleoclimate reconstruction on the Common Era (1-2000AD) provide critical\ncontext for recent warming trends. This work leverages integrated nested\nLaplace approximations (INLA) to conduct inference under a Bayesian\nhierarchical model using data from three sources: a state-of-the-art prox\ndatabase (PAGES 2k), surface temperature observations (HadCRUT4), and latest\nestimates of external forcings. INLA's computational efficiency allows to\nexplore several model formulations (with or without forcings, explicitly\nmodeling internal variability or not), as well as five data reduction\ntechniques. Two different validation exercises find a small impact of data\nreduction choices, but a large impact for model choice, with best results for\nthe two models that incorporate external forcings. These models confirm that\nman-made greenhouse gas emissions are the largest contributor to temperature\nvariability over the Common Era, followed by volcanic forcing. Solar effects\nare indistinguishable from zero. INLA provide an efficient way to estimate the\nposterior mean, comparable with the much costlier Monte Carlo Markov Chain\nprocedure, but with wider uncertainty bounds. We recommend using it for\nexploration of model designs, but full MCMC solutions should be used for proper\nuncertainty quantification.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 04:46:56 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 04:26:24 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Barboza", "Luis A.", ""], ["Emile-Geay", "Julien", ""], ["Li", "Bo", ""], ["He", "Wan", ""]]}, {"id": "1810.06167", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang, Daniel Gilbert, David Matteson", "title": "ABACUS: Unsupervised Multivariate Change Detection via Bayesian Source\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection involves segmenting sequential data such that observations\nin the same segment share some desired properties. Multivariate change\ndetection continues to be a challenging problem due to the variety of ways\nchange points can be correlated across channels and the potentially poor\nsignal-to-noise ratio on individual channels. In this paper, we are interested\nin locating additive outliers (AO) and level shifts (LS) in the unsupervised\nsetting. We propose ABACUS, Automatic BAyesian Changepoints Under Sparsity, a\nBayesian source separation technique to recover latent signals while also\ndetecting changes in model parameters. Multi-level sparsity achieves both\ndimension reduction and modeling of signal changes. We show ABACUS has\ncompetitive or superior performance in simulation studies against\nstate-of-the-art change detection methods and established latent variable\nmodels. We also illustrate ABACUS on two real application, modeling genomic\nprofiles and analyzing household electricity consumption.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 03:12:01 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhang", "Wenyu", ""], ["Gilbert", "Daniel", ""], ["Matteson", "David", ""]]}, {"id": "1810.06410", "submitter": "Alvin Vista", "authors": "Alvin Vista", "title": "Measuring religious morality using very limited poll responses:\n  Implementing \"big-data analytics\" to small data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion polls remain among the most efficient and widespread methods to\ncapture psycho-social data at large scales. However, there are limitations on\nthe logistics and structure of opinion polls that restrict the amount and type\nof information that can be collected. As a consequence, data from opinion polls\nare often reported in simple percentages and analyzed non-parametrically. In\nthis paper, response data on just four questions from a national opinion poll\nwere used to demonstrate that a parametric scale can be constructed using item\nresponse modeling approaches. Developing a parametric scale yields\ninterval-level measures which are more useful than the strictly ordinal-level\nmeasures obtained from Likert-type scales common in opinion polls. The metric\nthat was developed in this paper, a measure of religious morality, can be\nprocessed and used in a wider range of statistical analyses compared to\nconventional approaches of simply reporting percentages at item-level. Finally,\nthis paper reports the item parameters so that researchers can adopt these\nitems to future instruments and place their own results on the same scale,\nthereby allowing responses from future samples to be compared to the results\nfrom the representative data in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:45:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Vista", "Alvin", ""]]}, {"id": "1810.06608", "submitter": "Yawen Guan", "authors": "Yawen Guan, Christian Sampson, J. Derek Tucker, Won Chang, Anirban\n  Mondal, Murali Haran and Deborah Sulsky", "title": "Computer model calibration based on image warping metrics: an\n  application for sea ice deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic sea ice plays an important role in the global climate. Sea ice models\ngoverned by physical equations have been used to simulate the state of the ice\nincluding characteristics such as ice thickness, concentration, and motion.\nMore recent models also attempt to capture features such as fractures or leads\nin the ice. These simulated features can be partially misaligned or misshapen\nwhen compared to observational data, whether due to numerical approximation or\nincomplete physics. In order to make realistic forecasts and improve\nunderstanding of the underlying processes, it is necessary to calibrate the\nnumerical model to field data. Traditional calibration methods based on\ngeneralized least-square metrics are flawed for linear features such as sea ice\ncracks. We develop a statistical emulation and calibration framework that\naccounts for feature misalignment and misshapenness, which involves optimally\naligning model output with observed features using cutting edge image\nregistration techniques. This work can also have application to other physical\nmodels which produce coherent structures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:43:28 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 16:08:01 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 13:57:25 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Guan", "Yawen", ""], ["Sampson", "Christian", ""], ["Tucker", "J. Derek", ""], ["Chang", "Won", ""], ["Mondal", "Anirban", ""], ["Haran", "Murali", ""], ["Sulsky", "Deborah", ""]]}, {"id": "1810.06707", "submitter": "Magdalena Bennett", "authors": "Magdalena Bennett, Juan Pablo Vielma, and Jose R. Zubizarreta", "title": "Building Representative Matched Samples with Multi-valued Treatments in\n  Large Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new way of matching in observational studies that\novercomes three limitations of existing matching approaches. First, it directly\nbalances covariates with multi-valued treatments without requiring the\ngeneralized propensity score. Second, it builds self-weighted matched samples\nthat are representative of a target population by design. Third, it can handle\nlarge data sets, with hundreds of thousands of observations, in a couple of\nminutes. The key insights of this new approach to matching are balancing the\ntreatment groups relative to a target population and positing a linear-sized\nmixed integer formulation of the matching problem. We formally show that this\nformulation is more effective than alternative quadratic-sized formulations, as\nits reduction in size does not affect its strength from the standpoint of its\nlinear programming relaxation. We also show that this formulation can be used\nfor matching with distributional covariate balance in polynomial time under\ncertain assumptions on the covariates and that it can handle large data sets in\npractice even when the assumptions are not satisfied. This algorithmic\ncharacterization is key to handle large data sets. We illustrate this new\napproach to matching in both a simulation study and an observational study of\nthe impact of an earthquake on educational attainment. After matching, the\nresults can be visualized with simple and transparent graphical displays: while\nincreasing levels of exposure to the earthquake have a negative impact on\nschool attendance, there is no effect on college admission test scores.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 21:32:51 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 20:48:53 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Bennett", "Magdalena", ""], ["Vielma", "Juan Pablo", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "1810.07066", "submitter": "Christian Hans", "authors": "Christian A. Hans and Elin Klages", "title": "Very Short Term Time-Series Forecasting of Solar Irradiance Without\n  Exogenous Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares different forecasting methods and models to predict\naverage values of solar irradiance with a sampling time of 15 min over a\nprediction horizon of up to 3 h. The methods considered only require historic\nsolar irradiance values, the current time and geographical location, i.e., no\nexogenous inputs are used. Nearest neighbor regression (NNR) and autoregressive\nintegrated moving average (ARIMA) models are tested using different\nhyperparameters, e.g., the number of lags, or the size of the training data\nset, and data from different locations and seasons. The hyperparameters and\ntheir effect on the forecast quality are analyzed to identify properties which\nare likely to lead to good forecasts. Using these properties, a reduced search\nspace is derived to identify good forecasting models much faster.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 16:28:28 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 16:17:22 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hans", "Christian A.", ""], ["Klages", "Elin", ""]]}, {"id": "1810.07216", "submitter": "Hannah Druckenmiller", "authors": "Hannah Druckenmiller and Solomon Hsiang", "title": "Accounting for Unobservable Heterogeneity in Cross Section Using Spatial\n  First Differences", "comments": "42 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a cross-sectional research design to identify causal effects in\nthe presence of unobservable heterogeneity without instruments. When units are\ndense in physical space, it may be sufficient to regress the \"spatial first\ndifferences\" (SFD) of the outcome on the treatment and omit all covariates. The\nidentifying assumptions of SFD are similar in mathematical structure and\nplausibility to other quasi-experimental designs. We use SFD to obtain new\nestimates for the effects of time-invariant geographic factors, soil and\nclimate, on long-run agricultural productivities --- relationships crucial for\neconomic decisions, such as land management and climate policy, but notoriously\nconfounded by unobservables.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 18:19:38 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 13:16:59 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Druckenmiller", "Hannah", ""], ["Hsiang", "Solomon", ""]]}, {"id": "1810.07260", "submitter": "Shouhuai Xu", "authors": "Pang Du and Zheyuan Sun and Huashan Chen and Jin-Hee Cho and Shouhuai\n  Xu", "title": "Statistical Estimation of Malware Detection Metrics in the Absence of\n  Ground Truth", "comments": null, "journal-ref": "IEEE T-IFS (2018)", "doi": null, "report-no": null, "categories": "stat.AP cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate measurement of security metrics is a critical research problem\nbecause an improper or inaccurate measurement process can ruin the usefulness\nof the metrics, no matter how well they are defined. This is a highly\nchallenging problem particularly when the ground truth is unknown or noisy. In\ncontrast to the well perceived importance of defining security metrics, the\nmeasurement of security metrics has been little understood in the literature.\nIn this paper, we measure five malware detection metrics in the {\\em absence}\nof ground truth, which is a realistic setting that imposes many technical\nchallenges. The ultimate goal is to develop principled, automated methods for\nmeasuring these metrics at the maximum accuracy possible. The problem naturally\ncalls for investigations into statistical estimators by casting the measurement\nproblem as a {\\em statistical estimation} problem. We propose statistical\nestimators for these five malware detection metrics. By investigating the\nstatistical properties of these estimators, we are able to characterize when\nthe estimators are accurate, and what adjustments can be made to improve them\nunder what circumstances. We use synthetic data with known ground truth to\nvalidate these statistical estimators. Then, we employ these estimators to\nmeasure five metrics with respect to a large dataset collected from VirusTotal.\nWe believe our study touches upon a vital problem that has not been paid due\nattention and will inspire many future investigations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 02:40:31 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Du", "Pang", ""], ["Sun", "Zheyuan", ""], ["Chen", "Huashan", ""], ["Cho", "Jin-Hee", ""], ["Xu", "Shouhuai", ""]]}, {"id": "1810.07280", "submitter": "Liselotte Jauffred", "authors": "Per Lunnemann, Mogens H. Jensen and Liselotte Jauffred", "title": "Gender Bias in Nobel Prizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strikingly few Nobel laureates within medicine, natural and social sciences\nare women. Although it is obvious that there are fewer women researchers within\nthese fields, does this gender ratio still fully account for the low number of\nfemale Nobel laureates? We examine whether women are awarded the Nobel Prizes\nless often than the gender ratio suggests. Based on historical data across four\nscientific fields and a Bayesian hierarchical model, we quantify any possible\nbias. The model reveals, with exceedingly large confidence, that indeed women\nare strongly under-represented among Nobel laureates across all disciplines\nexamined.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 21:14:04 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Lunnemann", "Per", ""], ["Jensen", "Mogens H.", ""], ["Jauffred", "Liselotte", ""]]}, {"id": "1810.07318", "submitter": "Joshua Hewitt", "authors": "Joshua Hewitt, Miranda J. Fix, Jennifer A. Hoeting, and Daniel S.\n  Cooley", "title": "Improved return level estimation via a weighted likelihood, latent\n  spatial extremes model", "comments": "31 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty in return level estimates for rare events, like the intensity of\nlarge rainfall events, makes it difficult to develop strategies to mitigate\nrelated hazards, like flooding. Latent spatial extremes models reduce\nuncertainty by exploiting spatial dependence in statistical characteristics of\nextreme events to borrow strength across locations. However, these estimates\ncan have poor properties due to model misspecification: many latent spatial\nextremes models do not account for extremal dependence, which is spatial\ndependence in the extreme events themselves. We improve estimates from latent\nspatial extremes models that make conditional independence assumptions by\nproposing a weighted likelihood that uses the extremal coefficient to\nincorporate information about extremal dependence during estimation. This\napproach differs from, and is simpler than, directly modeling the spatial\nextremal dependence; for example, by fitting a max-stable process, which is\nchallenging to fit to real, large datasets. We adopt a hierarchical Bayesian\nframework for inference, use simulation to show the weighted model provides\nimproved estimates of high quantiles, and apply our model to improve return\nlevel estimates for Colorado rainfall events with 1% annual exceedance\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 23:52:54 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 19:09:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hewitt", "Joshua", ""], ["Fix", "Miranda J.", ""], ["Hoeting", "Jennifer A.", ""], ["Cooley", "Daniel S.", ""]]}, {"id": "1810.07450", "submitter": "Arnaud Mignan", "authors": "Arnaud Mignan", "title": "Generalized Earthquake Frequency-Magnitude Distribution Described by\n  Asymmetric Laplace Mixture Modelling", "comments": "30 pages, 9 figures, 1 table", "journal-ref": "Geophysical Journal International, 2019", "doi": "10.1093/gji/ggz373", "report-no": null, "categories": "physics.geo-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complete part of the earthquake frequency-magnitude distribution (FMD),\nabove completeness magnitude mc, is well described by the Gutenberg-Richter\nlaw. The parameter mc however varies in space due to the seismic network\nconfiguration, yielding a convoluted FMD shape below max(mc). This paper\ninvestigates the shape of the generalized FMD (GFMD), which may be described as\na mixture of elemental FMDs (eFMDs) defined as asymmetric Laplace distributions\nof mode mc [Mignan, 2012, https://doi.org/10.1029/2012JB009347]. An asymmetric\nLaplace mixture model (GFMD- ALMM) is thus proposed with its parameters\n(detection parameter kappa, Gutenberg-Richter beta-value, mc distribution, as\nwell as number K and weight w of eFMD components) estimated using a\nsemi-supervised hard expectation maximization approach including BIC penalties\nfor model complexity. The performance of the proposed method is analysed, with\nencouraging results obtained: kappa, beta, and the mc distribution range are\nretrieved for different GFMD shapes in simulations, as well as in regional\ncatalogues (southern and northern California, Nevada, Taiwan, France), in a\nglobal catalogue, and in an aftershock sequence (Christchurch, New Zealand). We\nfind max(mc) to be conservative compared to other methods, kappa = k/log(10) =\n3 in most catalogues (compared to beta = b/log(10) = 1), but also that biases\nin kappa and beta may occur when rounding errors are present below\ncompleteness. The GFMD-ALMM, by modelling different FMD shapes in an autonomous\nmanner, opens the door to new statistical analyses in the realm of incomplete\nseismicity data, which could in theory improve earthquake forecasting by\nconsidering c. ten times more events.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 09:31:59 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Mignan", "Arnaud", ""]]}, {"id": "1810.07496", "submitter": "Alexandra Sarafoglou", "authors": "Alexandra Sarafoglou, Anna van der Heijden, Tim Draws, Joran\n  Cornelisse, Eric-Jan Wagenmakers, Maarten Marsman", "title": "Combine Statistical Thinking With Scientific Practice: A Protocol of a\n  Bayesian Thesis Project For Undergraduate Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current developments in the statistics community suggest that modern\nstatistics education should be structured holistically, i.e., by allowing\nstudents to work with real data and answer concrete statistical questions, but\nalso by educating them about alternative statistical frameworks, such as\nBayesian statistics. In this article, we describe how we incorporated such a\nholistic structure in a Bayesian thesis project on ordered binomial\nprobabilities. The project was targeted at undergraduate students in psychology\nwith basic knowledge in Bayesian statistics and programming, but no formal\nmathematical training. The thesis project aimed to (1) convey the basic\nmathematical concepts of Bayesian inference, (2) let students experience the\nentire empirical cycle including the collection, analysis, and interpretation\nof data, and (3) teach students open science practices.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 12:06:19 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Sarafoglou", "Alexandra", ""], ["van der Heijden", "Anna", ""], ["Draws", "Tim", ""], ["Cornelisse", "Joran", ""], ["Wagenmakers", "Eric-Jan", ""], ["Marsman", "Maarten", ""]]}, {"id": "1810.07654", "submitter": "Vittorio Perduca", "authors": "Einar Holsb{\\o}, Vittorio Perduca", "title": "Shrinkage estimation of rate statistics", "comments": "12 pages, 12 figures", "journal-ref": "Case Studies in Business, Industry and Government Statistics -\n  CSBIGS, Vol 7 No 1 (2018)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple shrinkage estimator of rates based on Bayesian\nmethods. Our focus is on crime rates as a motivating example. The estimator\nshrinks each town's observed crime rate toward the country-wide average crime\nrate according to town size. By realistic simulations we confirm that the\nproposed estimator outperforms the maximum likelihood estimator in terms of\nglobal risk. We also show that it has better coverage properties.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 16:32:09 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Holsb\u00f8", "Einar", ""], ["Perduca", "Vittorio", ""]]}, {"id": "1810.07749", "submitter": "Melanie Weber", "authors": "Emil Saucan and Melanie Weber", "title": "Forman's Ricci curvature - From networks to hypernetworks", "comments": "to appear: Complex Networks '18 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks and their higher order generalizations, such as hypernetworks or\nmultiplex networks are ever more popular models in the applied sciences.\nHowever, methods developed for the study of their structural properties go\nlittle beyond the common name and the heavy reliance of combinatorial tools. We\nshow that, in fact, a geometric unifying approach is possible, by viewing them\nas polyhedral complexes endowed with a simple, yet, the powerful notion of\ncurvature - the Forman Ricci curvature. We systematically explore some aspects\nrelated to the modeling of weighted and directed hypernetworks and present\nexpressive and natural choices involved in their definitions. A benefit of this\napproach is a simple method of structure-preserving embedding of hypernetworks\nin Euclidean N-space. Furthermore, we introduce a simple and efficient manner\nof computing the well established Ollivier-Ricci curvature of a hypernetwork.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 19:42:22 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Saucan", "Emil", ""], ["Weber", "Melanie", ""]]}, {"id": "1810.07812", "submitter": "Caroline S. Wagner", "authors": "Caroline S. Wagner, Travis Whetsell, Jeroen Baas, Koen Jonkers", "title": "Openness and Impact of Leading Scientific Countries", "comments": null, "journal-ref": "Wagner, C. S., Whetsell, T., Baas, J., & Jonkers, K. (2018).\n  Openness and impact of leading scientific countries. Frontiers in Research\n  Metrics and Analytics, 3, 10", "doi": "10.3389/frma.2018.00010", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The rapid rise of international collaboration over the past three decades,\ndemonstrated in coauthorship of scientific articles, raises the question of\nwhether countries benefit from cooperative science and how this might be\nmeasured. We develop and compare measures to ask this question. For all source\npublications in 2013, we obtained from Elsevier national level full and\nfractional paper counts as well as accompanying field-weighted citation counts.\nThen we collected information from Elsevier on the percent of all\ninternationally coauthored papers for each country, as well as Organization for\nEconomic Cooperation and Development measures of the international mobility of\nthe scientific workforce in 2013, and conducted a principle component analysis\nthat produced an openness index. We added data from the OECD on government\nbudget allocation on research and development for 2011 to tie in the public\nspending that contributed to the 2013 output. We found that openness among\nadvanced science systems is strongly correlated with impact: the more\ninternationally engaged a nation is in terms of coauthorships and researcher\nmobility, the higher the impact of scientific work. The results have important\nimplications for policy making around investment, as well as the flows of\nstudents, researchers, and technical workers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 21:37:59 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Wagner", "Caroline S.", ""], ["Whetsell", "Travis", ""], ["Baas", "Jeroen", ""], ["Jonkers", "Koen", ""]]}, {"id": "1810.07868", "submitter": "\\'Alvaro Gonz\\'alez", "authors": "\\'Alvaro Corral and \\'Alvaro Gonz\\'alez", "title": "Power-law size distributions in geoscience revisited", "comments": null, "journal-ref": "Earth and Space Science, 2019", "doi": "10.1029/2018EA000479", "report-no": null, "categories": "physics.geo-ph nlin.AO physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size or energy of diverse structures or phenomena in geoscience appears\nto follow power-law distributions. A rigorous statistical analysis of such\nobservations is tricky, though. Observables can span several orders of\nmagnitude, but the range for which the power law may be valid is typically\ntruncated, usually because the smallest events are too tiny to be detected and\nthe largest ones are limited by the system size.\n  We revisit several examples of proposed power-law distributions dealing with\npotentially damaging natural phenomena. Adequate fits of the distributions of\nsizes are especially important in these cases, given that they may be used to\nassess long-term hazard. After reviewing the theoretical background for\npower-law distributions, we improve an objective statistical fitting method and\napply it to diverse data sets. The method is described in full detail and it is\neasy to implement.\n  Our analysis elucidates the range of validity of the power-law fit and the\ncorresponding exponent, and whether a power-law tail is improved by a truncated\nlog-normal. We confirm that impact fireballs and Californian earthquakes show\nuntruncated power-law behavior, whereas global earthquakes follow a double\npower law. Rain precipitation over space and time and tropical cyclones show a\ntruncated power-law regime. Karst sinkholes and wildfires, in contrast, are\nbetter described by truncated log-normals, although wildfires also may show\npower-law regimes. Our conclusions only apply to the analyzed data sets, but\nshow the potential of applying this robust statistical technique in the future.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 02:10:23 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 08:39:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Corral", "\u00c1lvaro", ""], ["Gonz\u00e1lez", "\u00c1lvaro", ""]]}, {"id": "1810.07876", "submitter": "Ick Hoon Jin", "authors": "Ick Hoon Jin, Minjeong Jeon, Michael Schweinberger, Lizhen Lin", "title": "Hierarchical Network Item Response Modeling for Discovering Differences\n  Between Innovation and Regular School Systems in Korea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The innovation school system has been implemented in Korea to cultivate a\nbottom-up educational culture in public education. Effectiveness of the\nprogram, however, has been under close scrutiny as numerous studies have\nreported varying results regarding the innovation school program's impact on\nstudents' non-cognitive outcomes. We proposed and applied a novel analytic\ntechnique, hierarchical network item response modeling to multilevel item\nresponse data, in order to discover and examine subtle and in-depth differences\nbetween innovation and regular school programs in terms of item and school\nnetwork structures. Our approach reveals that some schools are indeed rather\ndifferent from the others in terms of students' item responses, and those\ndifferences are not detected by conventional approaches (e.g., conventional\nmultilevel models). However, those differences do not appear to stem from the\nchoice of school program, suggesting that the innovation school system has no\nadvantages over the conventional school system.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 02:27:38 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 20:46:32 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 04:45:19 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Jin", "Ick Hoon", ""], ["Jeon", "Minjeong", ""], ["Schweinberger", "Michael", ""], ["Lin", "Lizhen", ""]]}, {"id": "1810.08029", "submitter": "Daniel Eck", "authors": "Daniel J. Eck", "title": "Challenging nostalgia and performance metrics in baseball", "comments": "Accepted at Chance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show that the great baseball players that started their careers before\n1950 are overrepresented among rankings of baseball's all time greatest\nplayers. The year 1950 coincides with the decennial US Census that is closest\nto when Major League Baseball (MLB) was integrated in 1947. We also show that\nperformance metrics used to compare players have substantial era biases that\nfavor players who started their careers before 1950. In showing that the these\nplayers are overrepresented, no individual statistics or era adjusted metrics\nare used. Instead, we argue that the eras in which players played are\nfundamentally different and are not comparable. In particular, there were\nsignificantly fewer eligible MLB players available at and before 1950. As a\nconsequence of this and other differences across eras, we argue that popular\nopinion, performance metrics, and expert opinion over include players that\nstarted their careers before 1950 in their rankings of baseball's all time\ngreatest players.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:06:10 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 01:33:17 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 20:21:06 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Eck", "Daniel J.", ""]]}, {"id": "1810.08032", "submitter": "Lee Richardson", "authors": "Francesca Matano, Lee F. Richardson, Taylor Pospisil, Collin Eubanks\n  and Jining Qin", "title": "Augmenting Adjusted Plus-Minus in Soccer with FIFA Ratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In basketball and hockey, state-of-the-art player value statistics are often\nvariants of Adjusted Plus-Minus (APM). But APM hasn't had the same impact in\nsoccer, since soccer games are low scoring with a low number of substitutions.\nIn soccer, perhaps the most comprehensive player value statistics come from\nvideo games, and in particular FIFA. FIFA ratings combine the subjective\nevaluations of over 9000 scouts, coaches, and season-ticket holders into\nratings for over 18,000 players. This paper combines FIFA ratings and APM into\na single metric, which we call Augmented APM. The key idea is recasting APM\ninto a Bayesian framework, and incorporating FIFA ratings into the prior\ndistribution. We show that Augmented APM predicts better than both standard APM\nand a model using only FIFA ratings. We also show that Augmented APM\ndecorrelates players that are highly collinear.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:14:40 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Matano", "Francesca", ""], ["Richardson", "Lee F.", ""], ["Pospisil", "Taylor", ""], ["Eubanks", "Collin", ""], ["Qin", "Jining", ""]]}, {"id": "1810.08255", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti, Kristian Lum, James E. Johndrow, David B. Dunson", "title": "Removing the influence of a group variable in high-dimensional\n  predictive modelling", "comments": "Update. 18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application areas, predictive models are used to support or make\nimportant decisions. There is increasing awareness that these models may\ncontain spurious or otherwise undesirable correlations. Such correlations may\narise from a variety of sources, including batch effects, systematic\nmeasurement errors, or sampling bias. Without explicit adjustment, machine\nlearning algorithms trained using these data can produce poor out-of-sample\npredictions which propagate these undesirable correlations. We propose a method\nto pre-process the training data, producing an adjusted dataset that is\nstatistically independent of the nuisance variables with minimum information\nloss. We develop a conceptually simple approach for creating an adjusted\ndataset in high-dimensional settings based on a constrained form of matrix\ndecomposition. The resulting dataset can then be used in any predictive\nalgorithm with the guarantee that predictions will be statistically independent\nof the group variable. We develop a scalable algorithm for implementing the\nmethod, along with theory support in the form of independence guarantees and\noptimality. The method is illustrated on some simulation examples and applied\nto two case studies: removing machine-specific correlations from brain scan\ndata, and removing race and ethnicity information from a dataset used to\npredict recidivism. That the motivation for removing undesirable correlations\nis quite different in the two applications illustrates the broad applicability\nof our approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:36:12 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 07:44:53 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Lum", "Kristian", ""], ["Johndrow", "James E.", ""], ["Dunson", "David B.", ""]]}, {"id": "1810.08564", "submitter": "Mingyuan Zhou", "authors": "Quan Zhang and Mingyuan Zhou", "title": "Nonparametric Bayesian Lomax delegate racing for survival analysis with\n  competing risks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Lomax delegate racing (LDR) to explicitly model the mechanism of\nsurvival under competing risks and to interpret how the covariates accelerate\nor decelerate the time to event. LDR explains non-monotonic covariate effects\nby racing a potentially infinite number of sub-risks, and consequently relaxes\nthe ubiquitous proportional-hazards assumption which may be too restrictive.\nMoreover, LDR is naturally able to model not only censoring, but also missing\nevent times or event types. For inference, we develop a Gibbs sampler under\ndata augmentation for moderately sized data, along with a stochastic gradient\ndescent maximum a posteriori inference algorithm for big data applications.\nIllustrative experiments are provided on both synthetic and real datasets, and\ncomparison with various benchmark algorithms for survival analysis with\ncompeting risks demonstrates distinguished performance of LDR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 15:57:22 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 00:48:10 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Quan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1810.08588", "submitter": "Chad Babcock", "authors": "Chad Babcock, Andrew O. Finley, Timothy G. Gregoire and Hans-Erik\n  Andersen", "title": "Remote sensing to reduce the effects of spatial autocorrelation on\n  design-based inference for forest inventory using systematic samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic sampling is often used to select plot locations for forest\ninventory estimation. However, it is not possible to derive a design-unbiased\nvariance estimator for a systematic sample using one random start. As a result,\nmany forest inventory analysts resort to applying variance estimators that are\ndesign-unbiased following simple random sampling to their systematic samples\neven though this typically leads to conservative estimates of error. We explore\nthe influence of spatial autocorrelation on variance estimation when systematic\nsampling is employed using repeated sampling. We generate a sequence of 1000\nsynthetic populations with increasing spatial autocorrelation and repeatedly\nsample from each to examine how the performance of estimators change as spatial\nautocorrelation changes. We also repeatedly sample from a tree census plot in\nHarvard Forest, Massachusetts and examine the performance of similar\nestimators. Results indicate that applying variance estimators that are\nunbiased following simple random sampling to systematic samples from\npopulations exhibiting stronger spatial autocorrelation tend to be more\nconservative. We also find that incorporating ancillary wall-to-wall\ncovariates, e.g., remote sensing data, using generalized regression estimators\ncan reduce variance over-estimation by explaining some or all of the spatial\nstructure in the population.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 17:14:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Babcock", "Chad", ""], ["Finley", "Andrew O.", ""], ["Gregoire", "Timothy G.", ""], ["Andersen", "Hans-Erik", ""]]}, {"id": "1810.08714", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Estimation of a functional single index model with dependent errors and\n  unknown error density", "comments": "31 pages, 8 figures, to appear in Communications in Statistics --\n  Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of error density estimation for a functional single index model\nwith dependent errors is studied. A Bayesian method is utilized to\nsimultaneously estimate the bandwidths in the kernel-form error density and\nregression function, under an autoregressive error structure. For estimating\nboth the regression function and error density, empirical studies show that the\nfunctional single index model gives improved estimation and prediction\naccuracies than any nonparametric functional regression considered.\nFurthermore, estimation of error density facilitates the construction of\nprediction interval for the response variable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 23:36:50 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1810.08807", "submitter": "Siddharth Arora Dr.", "authors": "S. Arora, N.P. Visanji, T.A. Mestre, A. Tsanas, A. AlDakheel, B.S.\n  Connolly, C. Gasca-Salas, D.S. Kern, J. Jain, E.J. Slow, A. Faust-Socher,\n  A.E. Lang, M.A. Little, and C. Marras", "title": "Investigating Voice as a Biomarker for leucine-rich repeat kinase\n  2-Associated Parkinson's Disease", "comments": "27 pages including supplemental information, Journal of Parkinson's\n  Disease, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential association between leucine-rich repeat kinase 2\n(LRRK2) mutations and voice. Sustained phonations ('aaah' sounds) were recorded\nfrom 7 individuals with LRRK2-associated Parkinson's disease (PD), 17\nparticipants with idiopathic PD (iPD), 20 non-manifesting LRRK2-mutation\ncarriers, 25 related non-carriers, and 26 controls. In distinguishing\nLRRK2-associated PD and iPD, the mean sensitivity was 95.4% (SD 17.8%) and mean\nspecificity was 89.6% (SD 26.5%). Voice features for non-manifesting carriers,\nrelated non-carriers, and controls were much less discriminatory. Vocal\ndeficits in LRRK2-associated PD may be different than those in iPD. These\npreliminary results warrant longitudinal analyses and replication in larger\ncohorts\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 14:04:49 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Arora", "S.", ""], ["Visanji", "N. P.", ""], ["Mestre", "T. A.", ""], ["Tsanas", "A.", ""], ["AlDakheel", "A.", ""], ["Connolly", "B. S.", ""], ["Gasca-Salas", "C.", ""], ["Kern", "D. S.", ""], ["Jain", "J.", ""], ["Slow", "E. J.", ""], ["Faust-Socher", "A.", ""], ["Lang", "A. E.", ""], ["Little", "M. A.", ""], ["Marras", "C.", ""]]}, {"id": "1810.09138", "submitter": "Fabio Divino", "authors": "Fabio Divino, Denekew Bitew Belay, Nico Keilman, Arnoldo Frigessi", "title": "Bayesian Modelling of Lexis Mortality Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a spatial approach to model and investigate mortality\ndata referenced over a Lexis structure. We decompose the force of mortality\ninto two interpretable components: a Markov random field, smooth with respect\nto time, age and cohort which explains the main pattern of mortality; and a\nsecondary component of independent shocks, accounting for additional non-smooth\nmortality. Inference is based on a hierarchical Bayesian approach with Markov\nchain Monte Carlo computations. We present an extensive application to data\nfrom the Human Mortality Database about 37 countries. For each country the\nprimary smooth surface and the secondary surface of additional mortality are\nestimated. The importance of each component is evaluated by the estimated value\nof the respective precision parameter. For several countries we discovered a\nband of extra mortality in the secondary surface across the time domain, in the\nage interval between 60 and 90 years, with a slightly positive slope. The band\nis significant in the most populated countries, but might be present also in\nthe others. The band represents a significant amount of extra mortality for the\nelderly population, which is otherwise incompatible with a regular and smooth\ndynamics in age, year and cohort.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 08:41:50 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Divino", "Fabio", ""], ["Belay", "Denekew Bitew", ""], ["Keilman", "Nico", ""], ["Frigessi", "Arnoldo", ""]]}, {"id": "1810.09165", "submitter": "Amir Weiss", "authors": "Amir Weiss and Arie Yeredor", "title": "A Maximum Likelihood-Based Minimum Mean Square Error Separation and\n  Estimation of Stationary Gaussian Sources from Noisy Mixtures", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2929473", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Independent Component Analysis (ICA), noisy mixtures pose a\ndilemma regarding the desired objective. On one hand, a \"maximally separating\"\nsolution, providing the minimal attainable Interference-to-Source-Ratio (ISR),\nwould often suffer from significant residual noise. On the other hand, optimal\nMinimum Mean Square Error (MMSE) estimation would yield estimates which are the\n\"closest possible\" to the true sources, often at the cost of compromised ISR.\nIn this work, we consider noisy mixtures of temporally-diverse stationary\nGaussian sources in a semi-blind scenario, which conveniently lends itself to\neither one of these objectives. We begin by deriving the ML Estimates (MLEs) of\nthe unknown (deterministic) parameters of the model: the mixing matrix and the\n(possibly different) noise variances in each sensor. We derive the likelihood\nequations for these parameters, as well as the corresponding Cram\\'er-Rao lower\nbound, and propose an iterative solution for obtaining the MLEs. Based on these\nMLEs, the asymptotically-optimal \"maximally separating\" solution can be readily\nobtained. However, we also present the ML-based MMSE estimate of the sources,\nalongside a frequency-domain-based computationally efficient scheme, exploiting\ntheir stationarity. We show that this estimate is asymptotically optimal and\nattains the (oracle) MMSE lower bound. Furthermore, for non-Gaussian signals,\nwe show that this estimate serves as a Quasi ML (QML)-based Linear MMSE (LMMSE)\nestimate, and attains the (oracle) LMMSE lower bound asymptotically. Empirical\nresults of three simulation experiments are presented, corroborating our\nanalytical derivations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 10:16:42 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Weiss", "Amir", ""], ["Yeredor", "Arie", ""]]}, {"id": "1810.09317", "submitter": "Vik Gopal", "authors": "De-Zhang Lee, Vik Gopal, Jia-Min Chan, Li-Shia Ng and Eng-Tat Ang", "title": "Assessing the Impact of Gamification on Self-Directed Learning in\n  Medical Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gamification refers to the process of adding game elements to a task. Of\nlate, this process has been introduced in pedagogical settings to capture the\nattention and interest of students. In our study, we apply the process to\nAnatomy students and assess the impact on their learning behaviour. We apply a\nnovel path analysis to assess the change in their learning behaviour after a\nsemester of games-enhanced small group sessions. We find that too much games\ncould reduce their enjoyment of the underlying learning. However, we also find\nthat students appreciate a change in the traditional model of instruction -\nthey embraced peer-to-peer learning in the classroom.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:29:06 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Lee", "De-Zhang", ""], ["Gopal", "Vik", ""], ["Chan", "Jia-Min", ""], ["Ng", "Li-Shia", ""], ["Ang", "Eng-Tat", ""]]}, {"id": "1810.09433", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh,\n  Mingyuan Zhou, and Xiaoning Qian", "title": "Bayesian multi-domain learning for cancer subtype discovery from\n  next-generation sequencing count data", "comments": "32nd Conference on Neural Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine aims for personalized prognosis and therapeutics by\nutilizing recent genome-scale high-throughput profiling techniques, including\nnext-generation sequencing (NGS). However, translating NGS data faces several\nchallenges. First, NGS count data are often overdispersed, requiring\nappropriate modeling. Second, compared to the number of involved molecules and\nsystem complexity, the number of available samples for studying complex\ndisease, such as cancer, is often limited, especially considering disease\nheterogeneity. The key question is whether we may integrate available data from\nall different sources or domains to achieve reproducible disease prognosis\nbased on NGS count data. In this paper, we develop a Bayesian Multi-Domain\nLearning (BMDL) model that derives domain-dependent latent representations of\noverdispersed count data based on hierarchical negative binomial factorization\nfor accurate cancer subtyping even if the number of samples for a specific\ncancer type is small. Experimental results from both our simulated and NGS\ndatasets from The Cancer Genome Atlas (TCGA) demonstrate the promising\npotential of BMDL for effective multi-domain learning without \"negative\ntransfer\" effects often seen in existing multi-task learning and transfer\nlearning methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 17:58:56 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Dadaneh", "Siamak Zamani", ""], ["Karbalayghareh", "Alireza", ""], ["Zhou", "Mingyuan", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1810.09521", "submitter": "Andr\\'as Zempl\\'eni Dr", "authors": "Szabolcs Majoros and Andr\\'as Zempl\\'eni", "title": "Multivariate stable distributions and their applications for modelling\n  cryptocurrency-returns", "comments": "29 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the known methodology for fitting stable\ndistributions to the multivariate case and apply the suggested method to the\nmodelling of daily cryptocurrency-return data. The investigated time period is\ncut into 10 non-overlapping sections, thus the changes can also be observed. We\napply bootstrap tests for checking the models and compare our approach to the\nmore traditional extreme-value and copula models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 19:56:34 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Majoros", "Szabolcs", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1810.09624", "submitter": "Earo Wang", "authors": "Earo Wang, Dianne Cook, Rob J Hyndman", "title": "Calendar-based graphics for visualizing people's daily schedules", "comments": "31 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calendars are broadly used in society to display temporal information, and\nevents. This paper describes a new R package with functionality to organize and\ndisplay temporal data, collected on sub-daily resolution, into a calendar\nlayout. The function `frame_calendar` uses linear algebra on the date variable\nto restructure data into a format lending itself to calendar layouts. The user\ncan apply the grammar of graphics to create plots inside each calendar cell,\nand thus the displays synchronize neatly with ggplot2 graphics. The motivating\napplication is studying pedestrian behavior in Melbourne, Australia, based on\ncounts which are captured at hourly intervals by sensors scattered around the\ncity. Faceting by the usual features such as day and month, was insufficient to\nexamine the behavior. Making displays on a monthly calendar format helps to\nunderstand pedestrian patterns relative to events such as work days, weekends,\nholidays, and special events. The layout algorithm has several format options\nand variations. It is implemented in the R package sugrrants.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 01:35:25 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Wang", "Earo", ""], ["Cook", "Dianne", ""], ["Hyndman", "Rob J", ""]]}, {"id": "1810.09682", "submitter": "Augustin Touron", "authors": "Augustin Touron (UP11), Thi Thu Huong Hoang (EDF), Sylvie Parey (EDF)", "title": "Bivariate modelling of precipitation and temperature using a\n  non-homogeneous hidden Markov model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to generate realistic synthetic times series of the bivariate process\nof daily mean temperature and precipitations, we introduce a non-homogeneous\nhidden Markov model. The non-homogeneity lies in periodic transition\nprobabilities between the hidden states, and time-dependent emission\ndistributions. This enables the model to account for the non-stationary\nbehaviour of weather variables. By carefully choosing the emission\ndistributions, it is also possible to model the dependance structure between\nthe two variables. The model is applied to several weather stations in Europe\nwith various climates, and we show that it is able to simulate realistic\nbivariate time series.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 06:44:42 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 09:30:05 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Touron", "Augustin", "", "UP11"], ["Hoang", "Thi Thu Huong", "", "EDF"], ["Parey", "Sylvie", "", "EDF"]]}, {"id": "1810.09753", "submitter": "Taras Lazariv", "authors": "Taras Lazariv and Christoph Lehmann", "title": "Goodness-of-Fit Tests for Large Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, data analysis in the world of Big Data is connected typically to\ndata mining, descriptive or exploratory statistics, e.~g.\\ cluster analysis,\nclassification or regression analysis. Aside these techniques there is a huge\narea of methods from inferential statistics that are rarely considered in\nconnection with Big Data. Nevertheless, inferential methods are also of use for\nBig Data analysis, especially for quantifying uncertainty. The article at hand\nwill provide some insights to methodological and technical issues referring\ninferential methods in the Big Data area in order to bring together Big Data\nand inferential statistics, as it comes along with its difficulties. We present\nan approach that allows testing goodness-of-fit without model assumptions and\nrelying on the empirical distribution. Especially, the method is able to\nutilize information from large datasets. Thereby, the approach is based on a\nclear theoretical background. We concentrate on the widely-used\nKolmogorov-Smirnov test that is applied for testing goodness-of-fit in\nstatistics. Our approach can be parallelized easily, which makes it applicable\nto distributed datasets particularly on a compute cluster. By this\ncontribution, we turn to an audience that is interested in the technical and\nmethodological backgrounds while implementing especially inferential\nstatistical methods with Big Data tools as e. g. Spark.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 10:03:38 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Lazariv", "Taras", ""], ["Lehmann", "Christoph", ""]]}, {"id": "1810.09781", "submitter": "Clement Lee", "authors": "Clement Lee and Darren J Wilkinson", "title": "A Social Network Analysis of Articles on Social Network Analysis", "comments": "47 pages, 22 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collection of articles on the statistical modelling and inference of social\nnetworks is analysed in a network fashion. The references of these articles are\nused to construct a citation network data set, which is almost a directed\nacyclic graph because only existing articles can be cited. A mixed membership\nstochastic block model is then applied to this data set to soft cluster the\narticles. The results obtained from a Gibbs sampler give us insights into the\ninfluence and the categorisation of these articles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:23:58 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 14:38:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Lee", "Clement", ""], ["Wilkinson", "Darren J", ""]]}, {"id": "1810.09782", "submitter": "Mario Chater", "authors": "Mario Chater, Luc Arrondel, Jean-Pascal Gayant, Jean-Fran\\c{c}ois\n  Laslier", "title": "Fixing Match-Fixing: Optimal schedules to promote competitiveness", "comments": "22 pages, 4 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last round of the FIFA World Cup group stage, games for which the\noutcome does not affect the selection of the qualified teams are played with\nlittle enthusiasm. Furthermore, a team that has already qualified may take into\naccount other factors, such as the opponents it will face in the next stage of\nthe competition so that, depending on the results in the other groups and the\nscheduling of the next stage, winning the game may not be in its best interest.\nEven more critically, there may be situations in which a simple draw will\nqualify both teams for the next stage of the competition. Any situation in\nwhich the two opposing teams do not play competitively is detrimental to the\nsport, and, above all, can lead to collusion and match-fixing opportunities. We\nhere develop a relatively general method of evaluating competitiveness and\napply it to the current format of the World Cup group stage. We then propose\nchanges to the current format in order to increase the stakes in the last round\nof games of the group stage, making games more exciting to watch and, at the\nsame time, reducing any collusion opportunities. We appeal to the same method\nto evaluate a \"groups of 3\" format which will be introduced in the 2026 World\nCup edition as well as a format similar to the one of the current Euro UEFA\nCup.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:25:09 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 16:11:02 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 09:13:51 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 15:11:48 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Chater", "Mario", ""], ["Arrondel", "Luc", ""], ["Gayant", "Jean-Pascal", ""], ["Laslier", "Jean-Fran\u00e7ois", ""]]}, {"id": "1810.09894", "submitter": "Alejandra Avalos-Pacheco", "authors": "Alejandra Avalos-Pacheco, David Rossell and Richard S. Savage", "title": "Heterogeneous large datasets integration using Bayesian factor\n  regression", "comments": "Main manuscript: 34 pages. Supplementary material: 12 pages. Typos\n  corrected, link to the R code added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two key challenges in modern statistical applications are the large amount of\ninformation recorded per individual, and that such data are often not collected\nall at once but in batches. These batch effects can be complex, causing\ndistortions in both mean and variance. We propose a novel sparse latent factor\nregression model to integrate such heterogeneous data. The model provides a\ntool for data exploration via dimensionality reduction while correcting for a\nrange of batch effects. We study the use of several sparse priors (local and\nnon-local) to learn the dimension of the latent factors. Our model is fitted in\na deterministic fashion by means of an EM algorithm for which we derive\nclosed-form updates, contributing a novel scalable algorithm for non-local\npriors of interest beyond the immediate scope of this paper. We present several\nexamples, with a focus on bioinformatics applications. Our results show an\nincrease in the accuracy of the dimensionality reduction, with non-local priors\nsubstantially improving the reconstruction of factor cardinality, as well as\nthe need to account for batch effects to obtain reliable results. Our model\nprovides a novel approach to latent factor regression that balances sparsity\nwith sensitivity and is highly computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 14:48:23 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 23:37:58 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 17:13:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Avalos-Pacheco", "Alejandra", ""], ["Rossell", "David", ""], ["Savage", "Richard S.", ""]]}, {"id": "1810.10036", "submitter": "Gwendolyn Eadie", "authors": "Gwendolyn Eadie and Mario Juri\\'c", "title": "The cumulative mass profile of the Milky Way as determined by globular\n  cluster kinematics from Gaia DR2", "comments": "accepted to ApJ, 12 pages, 8 figures, 1 table", "journal-ref": null, "doi": "10.3847/1538-4357/ab0f97", "report-no": null, "categories": "astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new mass estimates and cumulative mass profiles (CMPs) with\nBayesian credible regions for the Milky Way (MW) Galaxy, given the kinematic\ndata of globular clusters as provided by (1) the $\\textit{Gaia}$ DR2\ncollaboration and the HSTPROMO team, and (2) the new catalog in Vasiliev\n(2019). We use globular clusters beyond 15kpc to estimate the CMP of the MW,\nassuming a total gravitational potential model $\\Phi(r) =\n\\Phi_{\\circ}r^{-\\gamma}$, which approximates an NFW-type potential at large\ndistances when $\\gamma=0.5$. We compare the resulting CMPs given data sets (1)\nand (2), and find the results to be nearly identical. The median estimate for\nthe total mass is $M_{200}= 0.70 \\times 10^{12} M_{\\odot}$ and the $50\\%$\nBayesian credible interval is $(0.62, 0.81)\\times10^{12}M_{\\odot}$. However,\nbecause the Vasiliev catalog contains more complete data at large $r$, the MW\ntotal mass is slightly more constrained by these data. In this work, we also\nsupply instructions for how to create a CMP for the MW with Bayesian credible\nregions, given a model for $M(<r)$ and samples drawn from a posterior\ndistribution. With the CMP, we can report median estimates and $50\\%$ Bayesian\ncredible regions for the MW mass within any distance (e.g., $M(r=25\\text{kpc})=\n0.26~(0.20, 0.36)\\times10^{12}M_{\\odot}$, $M(r=50\\text{kpc})= 0.37~(0.29, 0.51)\n\\times10^{12}M_{\\odot}$, $M(r=100\\text{kpc}) = 0.53~(0.41, 0.74)\n\\times10^{12}M_{\\odot}$, etc.), making it easy to compare our results directly\nto other studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:30:54 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 00:24:58 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Eadie", "Gwendolyn", ""], ["Juri\u0107", "Mario", ""]]}, {"id": "1810.10138", "submitter": "Hansapani Rodrigo", "authors": "Hansapani Rodrigo, Chris Tsokos", "title": "Bayesian Modeling of Nonlinear Poisson Regression with Artificial Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being in the era of big data, modeling and prediction of count data have\nbecome significantly important in many fields including health, finance,\nsocial, etc. Although linear Poisson regression has been widely used to model\ncount and rate data, it might not be always suitable as it cannot capture some\ninherent variability within complex data. In this study, we introduce a\nprobabilistically driven nonlinear Poisson regression model with Bayesian\nartificial neural networks (ANN) to model count or rate data. This new\nnonlinear Poisson regression model developed with Bayesian ANN provides higher\nprediction accuracies over traditional Poisson or negative binomial regression\nmodels as revealed in our simulation and real data studies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 00:30:48 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Rodrigo", "Hansapani", ""], ["Tsokos", "Chris", ""]]}, {"id": "1810.10213", "submitter": "Marie-Pierre Etienne", "authors": "Th\\'eo Michelot, Marie-Pierre Etienne (IRMAR, LMA2), Pierre Gloaguen\n  (MIA-Paris)", "title": "The Langevin diffusion as a continuous-time model of animal movement and\n  habitat selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. The utilisation distribution describes the relative probability of use of\na spatial unit by an animal. It is natural to think of it as the long-term\nconsequence of the animal's short-term movement decisions: it is the\naccumulation of small displacements which, over time, gives rise to global\npatterns of space use. However, most utilisation distribution models either\nignore the underlying movement, assuming the independenceof observed locations,\nor are based on simplistic Brownian motion movement rules. 2. We introduce a\nnew continuous-time model of animal movement, based on the Langevin diffusion.\nThis stochastic process has an explicit stationary distribution, conceptually\nanalogous to the idea of the utilisation distribution, and thus provides an\nintuitive framework to integrate movement and space use. We model the\nstationary (utilisation) distribution with a resource selection function to\nlink the movement to spatial covariates, and allow inference into habitat\nselection. 3. Standard approximation techniques can be used to derive the\npseudo-likelihood of the Langevin diffusion movement model, and to estimate\nhabitat preference and movement parameters from tracking data. We investigate\nthe performance of the method on simulated data, and discuss its sensitivity to\nthe time scale of the sampling. We present an example of its application to\ntracking data of Stellar sea lions (Eumetopiasjubatus). 4. Due to its\ncontinuous-time formulation, this method can be applied to irregular telemetry\ndata. It provides a rigorous framework to estimate long-term habitat selection\nfrom correlated movement data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 06:53:50 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Michelot", "Th\u00e9o", "", "IRMAR, LMA2"], ["Etienne", "Marie-Pierre", "", "IRMAR, LMA2"], ["Gloaguen", "Pierre", "", "MIA-Paris"]]}, {"id": "1810.10488", "submitter": "Erica Ashe", "authors": "Erica L. Ashe, Niamh Cahill, Carling Hay, Nicole S. Khan, Andrew Kemp,\n  Simon Engelhart, Benjamin P. Horton, Andrew Parnell, Robert E. Kopp", "title": "Statistical modeling of rates and trends in Holocene relative sea level", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the spatio-temporal variability of relative sea level (RSL)\nand estimating local, regional, and global RSL trends requires statistical\nanalysis of RSL data. Formal statistical treatments, needed to account for the\nspatially and temporally sparse distribution of data and for geochronological\nand elevational uncertainties, have advanced considerably over the last decade.\nTime-series models have adopted more flexible and physically-informed\nspecifications with more rigorous quantification of uncertainties.\nSpatio-temporal models have evolved from simple regional averaging to\nframeworks that more richly represent the correlation structure of RSL across\nspace and time. More complex statistical approaches enable rigorous\nquantification of spatial and temporal variability, the combination of\ngeographically disparate data, and the separation of the RSL field into various\ncomponents associated with different driving processes. We review the range of\nstatistical modeling and analysis choices used in the literature, reformulating\nthem for ease of comparison in a common hierarchical statistical framework. The\nhierarchical framework separates each model into different levels, clearly\npartitioning measurement and inferential uncertainty from process variability.\nPlacing models in a hierarchical framework enables us to highlight both the\nsimilarities and differences among modeling and analysis choices. We illustrate\nthe implications of some modeling and analysis choices currently used in the\nliterature by comparing the results of their application to common datasets\nwithin a hierarchical framework. In light of the complex patterns of spatial\nand temporal variability exhibited by RSL, we recommend non-parametric\napproaches for modeling temporal and spatio-temporal RSL.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 16:51:28 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Ashe", "Erica L.", ""], ["Cahill", "Niamh", ""], ["Hay", "Carling", ""], ["Khan", "Nicole S.", ""], ["Kemp", "Andrew", ""], ["Engelhart", "Simon", ""], ["Horton", "Benjamin P.", ""], ["Parnell", "Andrew", ""], ["Kopp", "Robert E.", ""]]}, {"id": "1810.10533", "submitter": "Hari Prasanna Das", "authors": "Hari Prasanna Das, Ioannis C. Konstantakopoulos, Aummul Baneen\n  Manasawala, Tanya Veeravalli, Huihan Liu and Costas J. Spanos", "title": "Segmentation Analysis in Human Centric Cyber-Physical Systems using\n  Graphical Lasso", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.05142", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized gamification framework is introduced as a form of smart\ninfrastructure with potential to improve sustainability and energy efficiency\nby leveraging humans-in-the-loop strategy. The proposed framework enables a\nHuman-Centric Cyber-Physical System using an interface to allow building\nmanagers to interact with occupants. The interface is designed for occupant\nengagement-integration supporting learning of their preferences over resources\nin addition to understanding how preferences change as a function of external\nstimuli such as physical control, time or incentives. Towards intelligent and\nautonomous incentive design, a noble statistical learning algorithm performing\noccupants energy usage behavior segmentation is proposed. We apply the proposed\nalgorithm, Graphical Lasso, on energy resource usage data by the occupants to\nobtain feature correlations--dependencies. Segmentation analysis results in\ncharacteristic clusters demonstrating different energy usage behaviors. The\nfeatures--factors characterizing human decision-making are made explainable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 11:08:13 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 02:14:17 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Das", "Hari Prasanna", ""], ["Konstantakopoulos", "Ioannis C.", ""], ["Manasawala", "Aummul Baneen", ""], ["Veeravalli", "Tanya", ""], ["Liu", "Huihan", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1810.10669", "submitter": "Perry Williams", "authors": "Perry Williams, William Kendall, Mevin Hooten", "title": "Model Selection using Multi-Objective Optimization", "comments": "19 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choices in scientific research and management require balancing multiple,\noften competing objectives.Multiple-objective optimization (MOO) provides a\nunifying framework for solving multiple objective problems. Model selection is\na critical component to scientific inference and prediction and concerns\nbalancing the competing objectives of model fit and model complexity. The\ntradeoff between model fit and model complexity provides a basis for describing\nthe model-selection problem within the MOO framework. We discuss MOO and two\nstrategies for solving the MOO problem; modeling preferences pre-optimization\nand post-optimization. Most model selection methods are consistent with solving\nMOO problems via specification of preferences pre-optimization. We reconcile\nthese methods within the MOO framework. We also consider model selection using\npost-optimization specification of preferences. That is, by first identifying\nPareto optimal solutions, and then selecting among them. We demonstrate\nconcepts with an ecological application of model selection using avian species\nrichness data in the continental United States.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 00:53:16 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Williams", "Perry", ""], ["Kendall", "William", ""], ["Hooten", "Mevin", ""]]}, {"id": "1810.10705", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Alzheimer's Disease Neuroimaging Initiative", "title": "Alzheimer's Disease Prediction Using Longitudinal and Heterogeneous\n  Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent evidence has shown that structural magnetic resonance imaging (MRI) is\nan effective tool for Alzheimer's disease (AD) prediction and diagnosis. While\ntraditional MRI-based diagnosis uses images acquired at a single time point, a\nlongitudinal study is more sensitive and accurate in detecting early\npathological changes of the AD. Two main difficulties arise in longitudinal\nMRI-based diagnosis: (1) the inconsistent longitudinal scans among subjects\n(i.e., different scanning time and different total number of scans); (2) the\nheterogeneous progressions of high-dimensional regions of interest (ROIs) in\nMRI. In this work, we propose a novel feature selection and estimation method\nwhich can be applied to extract features from the heterogeneous longitudinal\nMRI. A key ingredient of our method is the combination of smoothing splines and\nthe $l_1$-penalty. We perform experiments on the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. The results corroborate the advantages\nof the proposed method for AD prediction in longitudinal studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 03:25:28 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Dai", "Xiaowu", ""], ["Initiative", "Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1810.10794", "submitter": "Dennis Lendrem", "authors": "B Clare Lendrem, Dennis W Lendrem, Arthur G Pratt, Najib Naamane,\n  Peter McMeekin, Wan-Fai Ng, Joy Allen, Michael Power, John D Isaacs", "title": "Between a ROC and a Hard Place: Using prevalence plots to understand the\n  likely real world performance of biomarkers in the clinic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Receiver Operating Characteristic (ROC) curve and the Area Under the\nCurve (AUC) of the ROC curve are widely used to compare the performance of\ndiagnostic and prognostic assays. The ROC curve has the advantage that it is\nindependent of disease prevalence. However, in this note we remind readers that\nthe performance of an assay upon translation to the clinic is critically\ndependent upon that very same prevalence. Without an understanding of\nprevalence in the test population, even robust bioassays with excellent ROC\ncharacteristics may perform poorly in the clinic. Instead, simple plots of\ncandidate assay performance as a function of prevalence rate give a more\nrealistic understanding of the likely real-world performance and a greater\nunderstanding of the likely impact of variation in that prevalence on\ntranslational performance in the clinic.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:12:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lendrem", "B Clare", ""], ["Lendrem", "Dennis W", ""], ["Pratt", "Arthur G", ""], ["Naamane", "Najib", ""], ["McMeekin", "Peter", ""], ["Ng", "Wan-Fai", ""], ["Allen", "Joy", ""], ["Power", "Michael", ""], ["Isaacs", "John D", ""]]}, {"id": "1810.10930", "submitter": "Th\\'eo Michelot", "authors": "Th\\'eo Michelot, Paul G. Blackwell, Simon Chamaill\\'e-Jammes, Jason\n  Matthiopoulos", "title": "Inference in MCMC step selection models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Habitat selection models are used in ecology to link the distribution of\nanimals to environmental covariates, and identify habitats that are important\nfor conservation. The most widely used models of this type, resource selection\nfunctions, assume independence between the observed locations of an animal.\nThis is unrealistic when location data display spatio-temporal autocorrelation.\nAlternatively, step selection functions embed habitat selection in a model of\nanimal movement, to account for the autocorrelation. However, inferences from\nstep selection functions depend on the movement model, and they cannot readily\nbe used to predict long-term space use. We recently suggested that a Markov\nchain Monte Carlo (MCMC) algorithm could define a step selection model with an\nexplicit stationary distribution: the target distribution. Here, we explain how\nthe likelihood of a MCMC step selection model is derived, and how maximum\nlikelihood estimation can be used for inference about parameters of movement\nand habitat selection. We describe the local Gibbs sampler, a rejection-free\nMCMC scheme designed to capture important features of real animal movement. The\nsampler can be used as the basis for a flexible class of movement models, and\nwe derive the likelihood function for several important special cases. In a\nsimulation study, we verify that maximum likelihood estimation can be used to\nrecover all model parameters. We illustrate the application of the method with\ndata from a plains zebra.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:30:17 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Michelot", "Th\u00e9o", ""], ["Blackwell", "Paul G.", ""], ["Chamaill\u00e9-Jammes", "Simon", ""], ["Matthiopoulos", "Jason", ""]]}, {"id": "1810.10936", "submitter": "Dennis Lendrem Dr", "authors": "Dennis W Lendrem, B Clare Lendrem, Arthur G Pratt, Jessica R Tarn,\n  Andrew Skelton, Kathryn James, Peter McMeekin, Matt Linsley, Colin Gillespie,\n  Heather Cordell, Wan-Fai Ng, John D Isaacs", "title": "Building Reality Checks into the Translational Pathway for Diagnostic\n  and Prognostic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a significant increase in the number of diagnostic and\nprognostic models published in the last decade. Testing such models in an\nindependent, external validation cohort gives some assurance the model will\ntransfer to a naturalistic, healthcare setting. Of 2,147 published models in\nthe PubMed database, we found just 120 included some kind of separate external\nvalidation cohort. Of these studies not all were sufficiently well documented\nto allow a judgement about whether that model was likely to transfer to other\ncentres, with other patients, treated by other clinicians, using data scored or\nanalysed by other laboratories. We offer a solution to better characterizing\nthe validation cohort and identify the key steps on the translational pathway\nfor diagnostic and prognostic models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:44:56 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lendrem", "Dennis W", ""], ["Lendrem", "B Clare", ""], ["Pratt", "Arthur G", ""], ["Tarn", "Jessica R", ""], ["Skelton", "Andrew", ""], ["James", "Kathryn", ""], ["McMeekin", "Peter", ""], ["Linsley", "Matt", ""], ["Gillespie", "Colin", ""], ["Cordell", "Heather", ""], ["Ng", "Wan-Fai", ""], ["Isaacs", "John D", ""]]}, {"id": "1810.11056", "submitter": "Pierre Masselot", "authors": "Pierre Masselot, Fateh Chebana, Taha B.M.J. Ouarda, Diane B\\'elanger,\n  Andr\\'e St-Hilaire, Pierre Gosselin", "title": "A new look at weather-related health impacts through functional\n  regression", "comments": null, "journal-ref": "Masselot, P., Chebana, F., Ouarda, T.B.M.J., B\\'elanger, D.,\n  St-Hilaire, A., Gosselin, P., 2018. A new look at weather-related health\n  impacts through functional regression. Scientific Reports 8, 15241", "doi": "10.1038/s41598-018-33626-1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge of climate change adaptation is to assess the effect of\nchanging weather on human health. In spite of an increasing literature on the\nweather-related health subject, many aspect of the relationship are not known,\nlimiting the predictive power of epidemiologic models. The present paper\nproposes new models to improve the performances of the currently used ones. The\nproposed models are based on functional data analysis (FDA), a statistical\nframework dealing with continuous curves instead of scalar time series. The\nmodels are applied to the temperature-related cardiovascular mortality issue in\nMontreal. By making use of the whole information available, the proposed models\nimprove the prediction of cardiovascular mortality according to temperature. In\naddition, results shed new lights on the relationship by quantifying\nphysiological adaptation effects. These results, not found with classical\nmodel, illustrate the potential of FDA approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:26:10 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Masselot", "Pierre", ""], ["Chebana", "Fateh", ""], ["Ouarda", "Taha B. M. J.", ""], ["B\u00e9langer", "Diane", ""], ["St-Hilaire", "Andr\u00e9", ""], ["Gosselin", "Pierre", ""]]}, {"id": "1810.11185", "submitter": "Timothy NeCamp", "authors": "Timothy NeCamp, Josh Gardner, Christopher Brooks", "title": "Beyond A/B Testing: Sequential Randomization for Developing\n  Interventions in Scaled Digital Learning Environments", "comments": null, "journal-ref": "2019, The 9th International Learning Analytics & Knowledge\n  Conference, Tempe, AZ, USA. ACM, New York, NY, USA", "doi": "10.1145/3303772.3303812", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments ensure robust causal inference that are critical to\neffective learning analytics research and practice. However, traditional\nrandomized experiments, like A/B tests, are limiting in large scale digital\nlearning environments. While traditional experiments can accurately compare two\ntreatment options, they are less able to inform how to adapt interventions to\ncontinually meet learners' diverse needs. In this work, we introduce a trial\ndesign for developing adaptive interventions in scaled digital learning\nenvironments -- the sequential randomized trial (SRT). With the goal of\nimproving learner experience and developing interventions that benefit all\nlearners at all times, SRTs inform how to sequence, time, and personalize\ninterventions. In this paper, we provide an overview of SRTs, and we illustrate\nthe advantages they hold compared to traditional experiments. We describe a\nnovel SRT run in a large scale data science MOOC. The trial results\ncontextualize how learner engagement can be addressed through inclusive\nculturally targeted reminder emails. We also provide practical advice for\nresearchers who aim to run their own SRTs to develop adaptive interventions in\nscaled digital learning environments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 04:15:51 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 02:46:46 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["NeCamp", "Timothy", ""], ["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1810.11385", "submitter": "Dan Li", "authors": "Dan Li, Dariush Fooladivanda and Sonia Martinez", "title": "Data-driven Variable Speed Limit Design for Highways via\n  Distributionally Robust Optimization", "comments": "10 pages, 2 figures, submitted to ECC 2019", "journal-ref": null, "doi": "10.23919/ECC.2019.8796026", "report-no": null, "categories": "math.OC cs.SY eess.SY math.DS math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an optimization problem (P) and a solution strategy to\ndesign variable-speed-limit controls for a highway that is subject to traffic\ncongestion and uncertain vehicle arrival and departure. By employing a finite\ndata-set of samples of the uncertain variables, we aim to find a data-driven\nsolution that has a guaranteed out-of-sample performance. In principle, such\nformulation leads to an intractable problem (P) as the distribution of the\nuncertainty variable is unknown. By adopting a distributionally robust\noptimization approach, this work presents a tractable reformulation of (P) and\nan efficient algorithm that provides a suboptimal solution that retains the\nout-of-sample performance guarantee. A simulation illustrates the effectiveness\nof this method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 15:30:26 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Dan", ""], ["Fooladivanda", "Dariush", ""], ["Martinez", "Sonia", ""]]}, {"id": "1810.11557", "submitter": "Simon Demers", "authors": "Simon Demers", "title": "The Duration of Optimal Stopping Problems", "comments": "37 pages, 2 figures, 4 tables. This version contains important\n  corrections and additional extensions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal stopping problems give rise to random distributions describing how\nmany applicants the decision-maker will sample or interview before choosing\none, a quantity sometimes referred to as the search time or process duration.\nThis research note surveys several variants of optimal stopping problems,\nextends earlier results in various directions, and shows how many interviews\nare expected to be conducted in various settings. The focus is on problems that\nrequire a decision-maker to choose a candidate from a pool of sequential\napplicants with no recall, in the vein of previously studied Cayley-Moser,\nSecretary and Sultan's Dowry problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 23:49:34 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 04:21:30 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 21:44:04 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Demers", "Simon", ""]]}, {"id": "1810.11776", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Stefan Bauer and Jonas Peters", "title": "Learning stable and predictive structures in kinetic systems: Benefits\n  of a causal approach", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1905688116", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning kinetic systems from data is one of the core challenges in many\nfields. Identifying stable models is essential for the generalization\ncapabilities of data-driven inference. We introduce a computationally efficient\nframework, called CausalKinetiX, that identifies structure from discrete time,\nnoisy observations, generated from heterogeneous experiments. The algorithm\nassumes the existence of an underlying, invariant kinetic model, a key\ncriterion for reproducible research. Results on both simulated and real-world\nexamples suggest that learning the structure of kinetic systems benefits from a\ncausal perspective. The identified variables and models allow for a concise\ndescription of the dynamics across multiple experimental settings and can be\nused for prediction in unseen experiments. We observe significant improvements\ncompared to well established approaches focusing solely on predictive\nperformance, especially for out-of-sample generalization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 07:47:54 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 12:07:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Pfister", "Niklas", ""], ["Bauer", "Stefan", ""], ["Peters", "Jonas", ""]]}, {"id": "1810.11891", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Pamela Ventola, James\n  S. Duncan", "title": "Prediction of severity and treatment outcome for ASD from fMRI", "comments": null, "journal-ref": "International Workshop on Predictive Intelligence In Medicine, pp\n  9-17, 2018, Springer", "doi": "10.1007/978-3-030-00320-3_2", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is a complex neurodevelopmental syndrome.\nEarly diagnosis and precise treatment are essential for ASD patients. Although\nresearchers have built many analytical models, there has been limited progress\nin accurate predictive models for early diagnosis. In this project, we aim to\nbuild an accurate model to predict treatment outcome and ASD severity from\nearly stage functional magnetic resonance imaging (fMRI) scans. The difficulty\nin building large databases of patients who have received specific treatments\nand the high dimensionality of medical image analysis problems are challenges\nin this work. We propose a generic and accurate two-level approach for\nhigh-dimensional regression problems in medical image analysis. First, we\nperform region-level feature selection using a predefined brain parcellation.\nBased on the assumption that voxels within one region in the brain have similar\nvalues, for each region we use the bootstrapped mean of voxels within it as a\nfeature. In this way, the dimension of data is reduced from number of voxels to\nnumber of regions. Then we detect predictive regions by various feature\nselection methods. Second, we extract voxels within selected regions, and\nperform voxel-level feature selection. To use this model in both linear and\nnon-linear cases with limited training examples, we apply two-level elastic net\nregression and random forest (RF) models respectively. To validate accuracy and\nrobustness of this approach, we perform experiments on both task-fMRI and\nresting state fMRI datasets. Furthermore, we visualize the influence of each\nregion, and show that the results match well with other findings.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 21:48:21 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhuang", "Juntang", ""], ["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1810.11900", "submitter": "Mason Youngblood", "authors": "Mason Youngblood", "title": "Cultural transmission modes of music sampling traditions remain stable\n  despite delocalization in the digital age", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0211860", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Music sampling is a common practice among hip-hop and electronic producers\nthat has played a critical role in the development of particular subgenres.\nArtists preferentially sample drum breaks, and previous studies have suggested\nthat these may be culturally transmitted. With the advent of digital sampling\ntechnologies and social media the modes of cultural transmission may have\nshifted, and music communities may have become decoupled from geography. The\naim of the current study was to determine whether drum breaks are culturally\ntransmitted through musical collaboration networks, and to identify the factors\ndriving the evolution of these networks. Using network-based diffusion analysis\nwe found strong evidence for the cultural transmission of drum breaks via\ncollaboration between artists, and identified several demographic variables\nthat bias transmission. Additionally, using network evolution methods we found\nevidence that the structure of the collaboration network is no longer biased by\ngeographic proximity after the year 2000, and that gender disparity has relaxed\nover the same period. Despite the delocalization of communities by the\ninternet, collaboration remains a key transmission mode of music sampling\ntraditions. The results of this study provide valuable insight into how\ndemographic biases shape cultural transmission in complex networks, and how the\nevolution of these networks has shifted in the digital age.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 22:57:09 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 01:24:25 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Youngblood", "Mason", ""]]}, {"id": "1810.12105", "submitter": "Nathaniel Beck", "authors": "Nathaniel Beck", "title": "Estimating grouped data models with a binary dependent variable and\n  fixed effect via logit vs OLS: the impact of dropped units", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.06505", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter deals with a very simple issue: if we have grouped data with a\nbinary dependent variable and want to include fixed effects (group specific\nintercepts) in the specification, is Ordinary Least Squares (OLS) in any way\nsuperior to a logit form because the OLS method \\emph{appears} to keep all\nobservations whereas the logit drops all groups which have either all zeros or\nall ones on the dependent variable? It is shown that OLS averages the estimates\nfor the all zero (and all one) groups, which by definition have all slope\ncoefficients of zero, with the slope coefficients for the groups with a mix of\nzeros and ones. Thus the correct comparison of OLS to logit is to only look at\ngroups with some variation in the dependent variable. Researchers using OLS are\nurged to report results both for all groups and for the subset of groups where\nthe dependent variable varies. The interpretation of the difference between\nthese two results depends upon assumptions which cannot be empirically\nassessed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 00:59:13 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Beck", "Nathaniel", ""]]}, {"id": "1810.12169", "submitter": "Marie Szafranski", "authors": "Florent Guinot (LaMME), Marie Szafranski (LaMME), Julien Chiquet\n  (MIA-Paris), Anouk Zancarini, Christine Le Signor, Christophe Mougel (IGEPP),\n  Christophe Ambroise (LaMME)", "title": "Fast Computation of Genome-Metagenome Interaction Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Association studies have been widely used to search for\nassociations between common genetic variants observations and a given\nphenotype. However, it is now generally accepted that genes and environment\nmust be examined jointly when estimating phenotypic variance. In this work we\nconsider two types of biological markers: genotypic markers, which characterize\nan observation in terms of inherited genetic information, and metagenomic\nmarker which are related to the environment. Both types of markers are\navailable in their millions and can be used to characterize any observation\nuniquely. Objective. Our focus is on detecting interactions between groups of\ngenetic and metagenomic markers in order to gain a better understanding of the\ncomplex relationship between environment and genome in the expression of a\ngiven phenotype. Contributions. We propose a novel approach for efficiently\ndetecting interactions between complementary datasets in a high-dimensional\nsetting with a reduced computational cost. The method, named SICOMORE, reduces\nthe dimension of the search space by selecting a subset of supervariables in\nthe two complementary datasets. These supervariables are given by a weighted\ngroup structure defined on sets of variables at different scales. A Lasso\nselection is then applied on each type of supervariable to obtain a subset of\npotential interactions that will be explored via linear model testing. Results.\nWe compare SICOMORE with other approaches in simulations, with varying sample\nsizes, noise, and numbers of true interactions. SICOMORE exhibits convincing\nresults in terms of recall, as well as competitive performances with respect to\nrunning time. The method is also used to detect interaction between genomic\nmarkers in Medicago truncatula and metagenomic markers in its rhizosphere\nbacterial community. Software availability. A R package is available, along\nwith its documentation and associated scripts, allowing the reader to reproduce\nthe results presented in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:57:02 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:41:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 16:19:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Guinot", "Florent", "", "LaMME"], ["Szafranski", "Marie", "", "LaMME"], ["Chiquet", "Julien", "", "MIA-Paris"], ["Zancarini", "Anouk", "", "IGEPP"], ["Signor", "Christine Le", "", "IGEPP"], ["Mougel", "Christophe", "", "IGEPP"], ["Ambroise", "Christophe", "", "LaMME"]]}, {"id": "1810.12177", "submitter": "Sebastien Marmin", "authors": "S\\'ebastien Marmin, Maurizio Filippone", "title": "Variational Calibration of Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian calibration of black-box computer models offers an established\nframework to obtain a posterior distribution over model parameters. Traditional\nBayesian calibration involves the emulation of the computer model and an\nadditive model discrepancy term using Gaussian processes; inference is then\ncarried out using MCMC. These choices pose computational and statistical\nchallenges and limitations, which we overcome by proposing the use of\napproximate Deep Gaussian processes and variational inference techniques. The\nresult is a practical and scalable framework for calibration, which obtains\ncompetitive performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:07:07 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Marmin", "S\u00e9bastien", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1810.12345", "submitter": "Carlos Henrique Gomes Ferreira", "authors": "Carlos H. G. Ferreira, Breno de Souza Matos and Jusssara M. Almeida", "title": "Analyzing Ideological Communities in Congressional Voting Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01129-1_16", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here study the behavior of political party members aiming at identifying\nhow ideological communities are created and evolve over time in diverse\n(fragmented and non-fragmented) party systems. Using public voting data of both\nBrazil and the US, we propose a methodology to identify and characterize\nideological communities, their member polarization, and how such communities\nevolve over time, covering a 15-year period. Our results reveal very distinct\npatterns across the two case studies, in terms of both structural and dynamic\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 18:49:04 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Ferreira", "Carlos H. G.", ""], ["Matos", "Breno de Souza", ""], ["Almeida", "Jusssara M.", ""]]}, {"id": "1810.12389", "submitter": "Thomas Nagler", "authors": "Wiebke S. J\\\"ager, Thomas Nagler, Claudia Czado, Robert T. McCall", "title": "A Statistical Simulation Method for Joint Time Series of Non-stationary\n  Hourly Wave Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistically simulated time series of wave parameters are required for many\ncoastal and offshore engineering applications, often at the resolution of\napproximately one hour. Various studies have relied on autoregressive\nmoving-average (ARMA) processes to simulate synthetic series of wave parameters\nin a Monte Carlo sense. However, accurately representing inter-series\ndependencies has remained a challenge. In particular, the relationship between\nwave height and period statistics is complex, due to the limiting steepness\ncondition. Here, we present a new simulation method for joint time series of\nsignificant wave height, mean zero-crossing periods and a directional regime\nvariable. The latter distinguishes between northern and southwestern waves. The\nmethod rests on several model components which include renewal processes,\nFourier series with random coefficients, ARMA processes, copulas and\nregime-switching. A particular feature is a data-driven estimate for a wave\nheight-dependent limiting wave steepness condition which is used to facilitate\ncopulabased dependence modeling. The method was developed for and applied to a\ndata set in the Southern North Sea. For this site, the method could simulate\ntime series with realistic annual cycles and inter-annual variability. In the\ntime series data, the bivariate distribution of significant wave height and\nmean zero-crossing period was well represented. An influence of the directional\nregime on the bivariate distribution could also be modeled. However, the\ninfluence was not as strong in simulated data as in observed data. Finally,\nsimulated series captured duration and inter-arrival time of storm events well.\nPotential applications for output of the simulation method range from the\nassessment of coastal risks or design of coastal structures to the planning and\nbudgeting of offshore operations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 20:18:52 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["J\u00e4ger", "Wiebke S.", ""], ["Nagler", "Thomas", ""], ["Czado", "Claudia", ""], ["McCall", "Robert T.", ""]]}, {"id": "1810.12398", "submitter": "Tauhid Zaman", "authors": "Nicolas Guenon des Mesnards, David Scott Hunter, Zakaria el Hjouji,\n  and Tauhid Zaman", "title": "Detecting Bots and Assessing Their Impact in Social Networks", "comments": "58 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks are often subject to influence campaigns by malicious\nactors through the use of automated accounts known as bots. We consider the\nproblem of detecting bots in online social networks and assessing their impact\non the opinions of individuals. We begin by analyzing the behavior of bots in\nsocial networks and identify that they exhibit heterophily, meaning they\ninteract with humans more than other bots. We use this property to develop a\ndetection algorithm based on the Ising model from statistical physics. The bots\nare identified by solving a minimum cut problem. We show that this Ising model\nalgorithm can identify bots with higher accuracy while utilizing much less data\nthan other state of the art methods.\n  We then develop a a function we call generalized harmonic influence\ncentrality to estimate the impact bots have on the opinions of users in social\nnetworks. This function is based on a generalized opinion dynamics model and\ncaptures how the activity level and network connectivity of the bots shift\nequilibrium opinions. To apply generalized harmonic influence centrality to\nreal social networks, we develop a deep neural network to measure the opinions\nof users based on their social network posts. Using this neural network, we\nthen calculate the generalized harmonic influence centrality of bots in\nmultiple real social networks. For some networks we find that a limited number\nof bots can cause non-trivial shifts in the population opinions. In other\nnetworks, we find that the bots have little impact. Overall we find that\ngeneralized harmonic influence centrality is a useful operational tool to\nmeasure the impact of bots in social networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 20:40:26 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 14:46:44 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 22:57:41 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 05:16:47 GMT"}, {"version": "v5", "created": "Wed, 16 Dec 2020 03:37:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Mesnards", "Nicolas Guenon des", ""], ["Hunter", "David Scott", ""], ["Hjouji", "Zakaria el", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1810.12401", "submitter": "Vitalii Makogin", "authors": "Denis Dresvyanskiy and Tatiana Karaseva and Sergei Mitrofanov and\n  Claudia Redenbach and Stefanie Schwaar and Vitalii Makogin and Evgeny\n  Spodarev", "title": "Application of Clustering Methods to Anomaly Detection in Fibrous Media", "comments": null, "journal-ref": null, "doi": "10.1088/1757-899X/537/2/022001", "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of anomaly detection in 3D images of fibre\nmaterials. The spatial Stochastic Expectation Maximisation algorithm and\nAdaptive Weights Clustering are applied to solve this problem. The initial 3D\ngrey scale image was divided into small cubes subject to clustering. For each\ncube clustering attributes values were calculated: mean local direction and\ndirectional entropy. Clustering is conducted according to the given attributes.\nThe proposed methods are tested on the simulated images and on real fibre\nmaterials.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 20:43:31 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dresvyanskiy", "Denis", ""], ["Karaseva", "Tatiana", ""], ["Mitrofanov", "Sergei", ""], ["Redenbach", "Claudia", ""], ["Schwaar", "Stefanie", ""], ["Makogin", "Vitalii", ""], ["Spodarev", "Evgeny", ""]]}, {"id": "1810.12430", "submitter": "Alberto Baccini", "authors": "Alberto Baccini, Lucio Barabesi, Giuseppe De Nicolao", "title": "On the agreement between bibliometrics and peer review: evidence from\n  the Italian research assessment exercises", "comments": "28 pages, 6 tables, 4 Figures. This version contains identical\n  results and maths. It adds an extended literature review, a deeper discussion\n  of findings, and 4 new figures illustrating results", "journal-ref": null, "doi": "10.1371/journal.pone.0242520", "report-no": null, "categories": "stat.AP physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper appraises the concordance between bibliometrics and peer review,\nby drawing evidence from the data of two experiments realized by the Italian\ngovernmental agency for research evaluation. The experiments were performed for\nvalidating the dual system of evaluation, consisting in the interchangeable use\nof bibliometyrics and peer review, adopted by the agency in the research\nassessment exercises. The two experiments were based on stratified random\nsamples of journal articles. Each article was scored by bibliometrics and by\npeer review. The degree of concordance between the two evaluations is then\ncomputed. The correct setting of the experiments is defined by developing the\ndesign-based estimation of the Cohen's kappa coefficient and some testing\nprocedures for assessing the homogeneity of missing proportions between strata.\nThe results of both experiments show that for each research areas of hard\nsciences, engineering and life sciences, the degree of agreement between\nbibliometrics and peer review is -- at most -- weak at an individual article\nlevel. Thus, the outcome of the experiments does not validate the use of the\ndual system of evaluation in the Italian research assessments. More in general,\nthe very weak concordance indicates that metrics should not replace peer review\nat the level of individual article. Hence, the use of the dual system of\nevaluation for reducing costs might introduce unknown biases in a research\nassessment exercise.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:06:43 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 06:38:48 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 14:32:39 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Baccini", "Alberto", ""], ["Barabesi", "Lucio", ""], ["De Nicolao", "Giuseppe", ""]]}, {"id": "1810.12499", "submitter": "Erin Peterson", "authors": "Catherine Leigh, Sevvandi Kandanaarachchi, James M. McGree, Rob J.\n  Hyndman, Omar Alsibai, Kerrie Mengersen, and Erin E. Peterson", "title": "Predicting Sediment and Nutrient Concentrations in Rivers Using High\n  Frequency Water Quality Surrogates", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0215503", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particular focus of water-quality monitoring is the concentrations of\nsediments and nutrients in rivers, constituents that can smother biota and\ncause eutrophication. However, the physical and economic constraints of manual\nsampling prohibit data collection at the frequency required to capture\nadequately the variation in concentrations through time. Here, we developed\nmodels to predict total suspended solids (TSS) and oxidized nitrogen (NOx)\nconcentrations based on high-frequency time series of turbidity, conductivity\nand river level data from low-cost in situ sensors in rivers flowing into the\nGreat Barrier Reef lagoon. We fit generalized least squares linear mixed\neffects models with a continuous first-order autoregressive correlation to data\ncollected traditionally by manual sampling for subsequent analysis in the\nlaboratory, then used these models to predict TSS or NOx from in situ sensor\nwater-quality surrogate data, at two freshwater sites and one estuarine site.\nThese models accounted for both temporal autocorrelation and unevenly\ntime-spaced observations in the data. Turbidity proved a useful surrogate of\nTSS, with high predictive ability at both freshwater and estuarine sites. NOx\nmodels had much poorer fits, even when additional covariates of conductivity\nand river level were included along with turbidity. Furthermore, the relative\ninfluence of covariates in the NOx models was not consistent across sites. Our\nfindings likely reflect the complexity of dissolved nutrient dynamics in\nrivers, which are influenced by multiple and interacting factors including\nphysical, chemical and biological processes, and the need for greater and\nbetter incorporation of spatial and temporal components within models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 02:50:19 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Leigh", "Catherine", ""], ["Kandanaarachchi", "Sevvandi", ""], ["McGree", "James M.", ""], ["Hyndman", "Rob J.", ""], ["Alsibai", "Omar", ""], ["Mengersen", "Kerrie", ""], ["Peterson", "Erin E.", ""]]}, {"id": "1810.13014", "submitter": "Andr\\'as Zempl\\'eni Dr", "authors": "Csilla Hajas and Andr\\'as Zempl\\'eni", "title": "Mathematical modelling European temperature data: spatial differences in\n  global warming", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows an analysis of the gridded European precipitation data. We\ncombine simple linear regression with data mining tools like clustering, and\nevaluate the strength of the results by the modern bootstrap methods. We have\nused the 0.5 grade-grid of daily temperatures for 65 years, created by the\nEuropean Climate Assessment. We have checked the stability of the results by\nchanging the starting point of the linear regression - this approach might be\nvaluable for climatologists in finding the \"best\" starting point for assessing\nthe global warming in Europe. Different bootstrap approaches were compared and\nit turned out that the dependent weighted bootstrap is the best for checking\nthe significance of the estimators.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 21:41:05 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hajas", "Csilla", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1810.13076", "submitter": "Erin Peterson", "authors": "Catherine Leigh, Omar Alsibai, Rob J. Hyndman, Sevvandi\n  Kandanaarachchi, Olivia C. King, James M. McGree, Catherine Neelamraju,\n  Jennifer Strauss, Priyanga Dilini Talagala, Ryan S. Turner, Kerrie Mengersen,\n  Erin E. Peterson", "title": "A framework for automated anomaly detection in high frequency\n  water-quality data from in situ sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  River water-quality monitoring is increasingly conducted using automated in\nsitu sensors, enabling timelier identification of unexpected values. However,\nanomalies caused by technical issues confound these data, while the volume and\nvelocity of data prevent manual detection. We present a framework for automated\nanomaly detection in high-frequency water-quality data from in situ sensors,\nusing turbidity, conductivity and river level data. After identifying end-user\nneeds and defining anomalies, we ranked their importance and selected suitable\ndetection methods. High priority anomalies included sudden isolated spikes and\nlevel shifts, most of which were classified correctly by regression-based\nmethods such as autoregressive integrated moving average models. However, using\nother water-quality variables as covariates reduced performance due to complex\nrelationships among variables. Classification of drift and periods of\nanomalously low or high variability improved when we applied replaced anomalous\nmeasurements with forecasts, but this inflated false positive rates.\nFeature-based methods also performed well on high priority anomalies, but were\nalso less proficient at detecting lower priority anomalies, resulting in high\nfalse negative rates. Unlike regression-based methods, all feature-based\nmethods produced low false positive rates, but did not and require training or\noptimization. Rule-based methods successfully detected impossible values and\nmissing observations. Thus, we recommend using a combination of methods to\nimprove anomaly detection performance, whilst minimizing false detection rates.\nFurthermore, our framework emphasizes the importance of communication between\nend-users and analysts for optimal outcomes with respect to both detection\nperformance and end-user needs. Our framework is applicable to other types of\nhigh frequency time-series data and anomaly detection applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 02:31:29 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 23:41:16 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Leigh", "Catherine", ""], ["Alsibai", "Omar", ""], ["Hyndman", "Rob J.", ""], ["Kandanaarachchi", "Sevvandi", ""], ["King", "Olivia C.", ""], ["McGree", "James M.", ""], ["Neelamraju", "Catherine", ""], ["Strauss", "Jennifer", ""], ["Talagala", "Priyanga Dilini", ""], ["Turner", "Ryan S.", ""], ["Mengersen", "Kerrie", ""], ["Peterson", "Erin E.", ""]]}]