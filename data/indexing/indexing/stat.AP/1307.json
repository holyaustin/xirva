[{"id": "1307.0199", "submitter": "Anthony Coolen", "authors": "Hans van Baardewijk, Hans Garmo, Mieke van Hemelrijck, Lars Holmberg\n  and Anthony CC Coolen", "title": "Generic solution of the heterogeneity-induced competing risk problem in\n  survival analysis", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most papers implicitly assume competing risks to be induced by residual\ncohort heterogeneity, i.e. heterogeneity that is not captured by the recorded\ncovariates. Based on this observation we develop a generic statistical\ndescription of competing risks that unifies the main schools of thought.\nAssuming heterogeneity-induced competing risks is much weaker than assuming\nrisk independence. However, we show that it still imposes sufficient\nconstraints to solve the competing risk problem, and derive exact formulae for\ndecontaminated primary risk hazard rates and cause-specific survival functions.\nThe canonical description is in terms of a cohort's covariate-constrained\nfunctional distribution of individual hazard rates of all risks. Assuming\nproportional hazards at the level of individuals leads to a natural\nparametrisation of this distribution, from which Cox regression, frailty and\nrandom effects models, and latent class models can all be recovered in special\nlimits, and which also generates parametrised cumulative incidence functions\n(the language of Fine and Gray).\n  We demonstrate with synthetic data how the generic method can uncover and map\na cohort's substructure, if such substructure exists, and remove\nheterogeneity-induced false protectivity and false exposure effects.\nApplication to real survival data from the ULSAM study, with prostate cancer as\nthe primary risk, is found to give plausible alternative explanations for\nprevious counter-intuitive inferences.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 11:43:34 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["van Baardewijk", "Hans", ""], ["Garmo", "Hans", ""], ["van Hemelrijck", "Mieke", ""], ["Holmberg", "Lars", ""], ["Coolen", "Anthony CC", ""]]}, {"id": "1307.0552", "submitter": "Leonardo Bennun LB", "authors": "Leonardo Aguero, Leonardo Bennun", "title": "Comment to : Uncertainty in the Multielemental Quantification by\n  Total-Reflection X-ray Fluorescence: Theoretical and Empirical Approximation", "comments": "We found a conceptual error in an overall procedure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an error in the uncertainty assessment of uncertainties\nof the Total-Reflection X-ray Fluorescence technique Ref [1]. A confusion,\nbetween the precision and accuracy of a measurement produced an incomplete\nevaluation of the uncertainties of the technique.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 22:49:54 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 17:01:05 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Aguero", "Leonardo", ""], ["Bennun", "Leonardo", ""]]}, {"id": "1307.0898", "submitter": "Leonardo Araujo", "authors": "Leonardo Carneiro Araujo and Tha\\\"is Crist\\'ofaro-Silva and Hani\n  Camille Yehia", "title": "Entropy of a Zipfian Distributed Lexicon", "comments": "4 figures, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the calculation of the entropy of a system with Zipfian\ndistribution and shows that a communication system tends to present an exponent\nvalue close to one, but still greater than one, so that it might maximize\nentropy and hold a feasible lexicon with an increasing size. This result is in\nagreement with what is observed in natural languages and with the balance\nbetween the speaker and listener communication efforts. On the other hand, the\nentropy of the communicating source is very sensitive to the exponent value as\nwell as the length of the observable data, making it a poor parameter to\ncharacterize the communication process.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 02:23:24 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Araujo", "Leonardo Carneiro", ""], ["Crist\u00f3faro-Silva", "Tha\u00efs", ""], ["Yehia", "Hani Camille", ""]]}, {"id": "1307.0915", "submitter": "Yar Muhamad Mr", "authors": "Yar M. Mughal, A. Krivoshei, P. Annus", "title": "Separation of cardiac and respiratory components from the electrical\n  bio-impedance signal using PCA and fast ICA", "comments": "4 pages, International Conference on Control, Engineering and\n  Information Technology (CEIT'13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an attempt to separate cardiac and respiratory signals from an\nelectrical bio-impedance (EBI) dataset. For this two well-known algorithms,\nnamely Principal Component Analysis (PCA) and Independent Component Analysis\n(ICA), were used to accomplish the task. The ability of the PCA and the ICA\nmethods first reduces the dimension and attempt to separate the useful\ncomponents of the EBI, the cardiac and respiratory ones accordingly. It was\ninvestigated with an assumption, that no motion artefacts are present. To carry\nout this procedure the two channel complex EBI measurements were provided using\nclassical Kelvin type four electrode configurations for the each complex\nchannel. Thus four real signals were used as inputs for the PCA and fast ICA.\nThe results showed, that neither PCA nor ICA nor combination of them can not\naccurately separate the components at least are used only two complex (four\nreal valued) input components.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 05:51:43 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Mughal", "Yar M.", ""], ["Krivoshei", "A.", ""], ["Annus", "P.", ""]]}, {"id": "1307.1067", "submitter": "Patric M\\\"uller", "authors": "Patric M\\\"uller and Sara van de Geer", "title": "The partial linear model in high dimensions", "comments": "48 pages, 16 figures, submitted to Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial linear models have been widely used as flexible method for modelling\nlinear components in conjunction with non-parametric ones. Despite the presence\nof the non-parametric part, the linear, parametric part can under certain\nconditions be estimated with parametric rate. In this paper, we consider a\nhigh-dimensional linear part. We show that it can be estimated with oracle\nrates, using the LASSO penalty for the linear part and a smoothness penalty for\nthe nonparametric part.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 16:27:05 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["M\u00fcller", "Patric", ""], ["van de Geer", "Sara", ""]]}, {"id": "1307.1164", "submitter": "Martin Lysy", "authors": "Martin Lysy and Natesh S. Pillai", "title": "Statistical Inference for Stochastic Differential Equations with Memory", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct a framework for doing statistical inference for\ndiscretely observed stochastic differential equations (SDEs) where the driving\nnoise has 'memory'. Classical SDE models for inference assume the driving noise\nto be Brownian motion, or \"white noise\", thus implying a Markov assumption. We\nfocus on the case when the driving noise is a fractional Brownian motion, which\nis a common continuous-time modeling device for capturing long-range memory.\nSince the likelihood is intractable, we proceed via data augmentation, adapting\na familiar discretization and missing data approach developed for the white\nnoise case. In addition to the other SDE parameters, we take the Hurst index to\nbe unknown and estimate it from the data. Posterior sampling is performed via a\nHybrid Monte Carlo algorithm on both the parameters and the missing data\nsimultaneously so as to improve mixing. We point out that, due to the\nlong-range correlations of the driving noise, careful discretization of the\nunderlying SDE is necessary for valid inference. Our approach can be adapted to\nother types of rough-path driving processes such as Gaussian \"colored\" noise.\nThe methodology is used to estimate the evolution of the memory parameter in US\nshort-term interest rates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 21:24:33 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Lysy", "Martin", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1307.1271", "submitter": "Jorge Ma\\~nana-Rodr\\'iguez", "authors": "Elea Gimenez-Toledo, Jorge Manana-Rodriguez and Emilio\n  Delgado-Lopez-Cozar", "title": "Quality indicators for scientific journals based on experts opinion", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results and further development of a survey sent to\n11,799 Spanish faculty members and researchers from various fields of the\nsocial sciences and the humanities, obtaining a total of 45.6% (5,368\nresponses) usable answers. Respondents were asked (a) to indicate the three\nmost important journals in their field and (b) to rate them on a 0-10 scale\naccording to their quality. The information obtained has been synthesized in\ntwo indicators which reflect the perceived quality of journals. Once the values\nwere obtained, the journals were categorized according to each indicator and\nthe ordinal positions were compared. Different profiles of journals are\nanalyzed in connection with experts opinion, such as regional orientation, and\nthe consensus among researchers is studied. Finally, the possibilities of\nextending the research and indicators to sets of international journals are\nexplored.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 11:06:28 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Gimenez-Toledo", "Elea", ""], ["Manana-Rodriguez", "Jorge", ""], ["Delgado-Lopez-Cozar", "Emilio", ""]]}, {"id": "1307.1380", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Uwe Aickelin, Tom Rodden", "title": "The Application of a Data Mining Framework to Energy Usage Profiling in\n  Domestic Residences using UK data", "comments": "Buildings Do Not Use Energy, People Do Research Student Conference,\n  Bath, UK, 2011. arXiv admin note: text overlap with arXiv:1307.1079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for defining representative load profiles for\ndomestic electricity users in the UK. It considers bottom up and clustering\nmethods and then details the research plans for implementing and improving\nexisting framework approaches based on the overall usage profile. The work\nfocuses on adapting and applying analysis framework approaches to UK energy\ndata in order to determine the effectiveness of creating a few (single figures)\narchetypical users with the intention of improving on the current methods of\ndetermining usage profiles. The work is currently in progress and the paper\ndetails initial results using data collected in Milton Keynes around 1990.\nVarious possible enhancements to the work are considered including a split\nbased on temperature to reflect the varying UK weather conditions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:45:09 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Dent", "Ian", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1307.1402", "submitter": "Xiangping Hu", "authors": "Xiangping Hu, Ingelin Steinsland, Daniel Simpson, Sara Martino and\n  H{\\aa}vard Rue", "title": "Spatial Modelling of Temperature and Humidity using Systems of\n  Stochastic Partial Differential Equations", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by constructing a weather simulator for precipitation.\nTemperature and humidity are two of the most important driving forces of\nprecipitation, and the strategy is to have a stochastic model for temperature\nand humidity, and use a deterministic model to go from these variables to\nprecipitation. Temperature and humidity are empirically positively correlated.\nGenerally speaking, if variables are empirically dependent, then multivariate\nmodels should be considered. In this work we model humidity and temperature in\nsouthern Norway. We want to construct bivariate Gaussian random fields (GRFs)\nbased on this dataset. The aim of our work is to use the bivariate GRFs to\ncapture both the dependence structure between humidity and temperature as well\nas their spatial dependencies. One important feature for the dataset is that\nthe humidity and temperature are not necessarily observed at the same\nlocations. Both univariate and bivariate spatial models are fitted and\ncompared. For modeling and inference the SPDE approach for univariate models\nand the systems of SPDEs approach for multivariate models have been used.\n  To evaluate the performance of the difference between the univariate and\nbivariate models, we compare predictive performance using some commonly used\nscoring rules: mean absolute error, mean-square error and continuous ranked\nprobability score. The results illustrate that we can capture strong positive\ncorrelation between the temperature and the humidity. Furthermore, the results\nalso agree with the physical or empirical knowledge. At the end, we conclude\nthat using the bivariate GRFs to model this dataset is superior to the approach\nwith independent univariate GRFs both when evaluating point predictions and for\nquantifying prediction uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 16:45:59 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 16:54:16 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Hu", "Xiangping", ""], ["Steinsland", "Ingelin", ""], ["Simpson", "Daniel", ""], ["Martino", "Sara", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1307.1411", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "Discovering Sequential Patterns in a UK General Practice Database", "comments": "2012 IEEE-EMBS International Conference on Biomedical and Health\n  Informatics, pp 960-963, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wealth of computerised medical information becoming readily available\npresents the opportunity to examine patterns of illnesses, therapies and\nresponses. These patterns may be able to predict illnesses that a patient is\nlikely to develop, allowing the implementation of preventative actions. In this\npaper sequential rule mining is applied to a General Practice database to find\nrules involving a patients age, gender and medical history. By incorporating\nthese rules into current health-care a patient can be highlighted as\nsusceptible to a future illness based on past or current illnesses, gender and\nyear of birth. This knowledge has the ability to greatly improve health-care\nand reduce health-care costs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 17:01:44 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1307.1524", "submitter": "Harpreet S. Dhillon", "authors": "Harpreet S. Dhillon, Ying Li, Pavan Nuggehalli, Zhouyue Pi, Jeffrey G.\n  Andrews", "title": "Fundamentals of Heterogeneous Cellular Networks with Energy Harvesting", "comments": "submitted to IEEE Transactions on Wireless Communications, July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new tractable model for K-tier heterogeneous cellular networks\n(HetNets), where each base station (BS) is powered solely by a self-contained\nenergy harvesting module. The BSs across tiers differ in terms of the energy\nharvesting rate, energy storage capacity, transmit power and deployment\ndensity. Since a BS may not always have enough energy, it may need to be kept\nOFF and allowed to recharge while nearby users are served by neighboring BSs\nthat are ON. We show that the fraction of time a k^{th} tier BS can be kept ON,\ntermed availability \\rho_k, is a fundamental metric of interest. Using tools\nfrom random walk theory, fixed point analysis and stochastic geometry, we\ncharacterize the set of K-tuples (\\rho_1, \\rho_2, ... \\rho_K), termed the\navailability region, that is achievable by general uncoordinated operational\nstrategies, where the decision to toggle the current ON/OFF state of a BS is\ntaken independently of the other BSs. If the availability vector corresponding\nto the optimal system performance, e.g., in terms of rate, lies in this\navailability region, there is no performance loss due to the presence of\nunreliable energy sources. As a part of our analysis, we model the temporal\ndynamics of the energy level at each BS as a birth-death process, derive the\nenergy utilization rate, and use hitting/stopping time analysis to prove that\nthere exists a fundamental limit on \\rho_k that cannot be surpassed by any\nuncoordinated strategy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 05:29:29 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Dhillon", "Harpreet S.", ""], ["Li", "Ying", ""], ["Nuggehalli", "Pavan", ""], ["Pi", "Zhouyue", ""], ["Andrews", "Jeffrey G.", ""]]}, {"id": "1307.1552", "submitter": "Alessio Farcomeni", "authors": "Alessio Farcomeni, Daria Scacciatelli", "title": "Heterogeneity and behavioral response in continuous time\n  capture-recapture, with application to street cannabis use in Italy", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS672 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2293-2314", "doi": "10.1214/13-AOAS672", "report-no": "IMS-AOAS-AOAS672", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general and flexible capture-recapture model in continuous time.\nOur model incorporates time-heterogeneity, observed and unobserved individual\nheterogeneity, and behavioral response to capture. Behavioral response can\npossibly have a delayed onset and a finite-time memory. Estimation of the\npopulation size is based on the conditional likelihood after use of the EM\nalgorithm. We develop an application to the estimation of the number of adult\ncannabinoid users in Italy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 09:05:26 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 13:50:19 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Farcomeni", "Alessio", ""], ["Scacciatelli", "Daria", ""]]}, {"id": "1307.1582", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "Chi-square goodness of fit tests for weighted histograms. Review and\n  improvements", "comments": "27 pages, 4 figures", "journal-ref": "Journal of Instrumentation 10 (2015) P05004", "doi": "10.1088/1748-0221/10/05/P05004", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted histograms are used for the estimation of probability density\nfunctions. Computer simulation is the main domain of application of this type\nof histogram. A review of chi-square goodness of fit tests for weighted\nhistograms is presented in this paper. Improvements are proposed to these tests\nthat have size more close to its nominal value. Numerical examples are\npresented in this paper for evaluation of tests and to demonstrate various\napplications of tests.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 11:21:05 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 11:24:43 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 16:47:58 GMT"}, {"version": "v4", "created": "Thu, 19 Feb 2015 14:40:37 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1307.2048", "submitter": "Gregor Wergen", "authors": "Gregor Wergen", "title": "Modeling record-breaking stock prices", "comments": "20 pages, 28 figures", "journal-ref": null, "doi": "10.1016/j.physa.2013.11.001", "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistics of record-breaking events in daily stock prices of\n366 stocks from the Standard and Poors 500 stock index. Both the record events\nin the daily stock prices themselves and the records in the daily returns are\ndiscussed. In both cases we try to describe the record statistics of the stock\ndata with simple theoretical models. The daily returns are compared to i.i.d.\nRV's and the stock prices are modeled using a biased random walk, for which the\nrecord statistics are known. These models agree partly with the behavior of the\nstock data, but we also identify several interesting deviations. Most\nimportantly, the number of records in the stocks appears to be systematically\ndecreased in comparison with the random walk model. Considering the\nautoregressive AR(1) process, we can predict the record statistics of the daily\nstock prices more accurately. We also compare the stock data with simulations\nof the record statistics of the more complicated GARCH(1,1) model, which, in\ncombination with the AR(1) model, gives the best agreement with the\nobservational data. To better understand our findings, we discuss the survival\nand first-passage times of stock prices on certain intervals and analyze the\ncorrelations between the individual record events. After recapitulating some\nrecent results for the record statistics of ensembles of N stocks, we also\npresent some new observations for the weekly distributions of record events.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 11:45:49 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Wergen", "Gregor", ""]]}, {"id": "1307.2129", "submitter": "Diego Fasoli", "authors": "D. Fasoli, O. Faugeras", "title": "Finite size effects in the correlation structure of stochastic neural\n  networks: analysis of different connectivity matrices and failure of the\n  mean-field theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the finite size effects in a stochastic network made up of rate\nneurons, for several kinds of recurrent connectivity matrices. This analysis is\nperformed by means of a perturbative expansion of the neural equations, where\nthe perturbative parameters are the intensities of the sources of randomness in\nthe system. In detail, these parameters are the variances of the background or\ninput noise, of the initial conditions and of the distribution of the synaptic\nweights. The technique developed in this article can be used to study systems\nwhich are invariant under the exchange of the neural indices and it allows us\nto quantify the correlation structure of the network, in terms of pairwise and\nhigher order correlations between the neurons. We also determine the relation\nbetween the correlation and the external input of the network, showing that\nstrong signals coming from the environment reduce significantly the amount of\ncorrelation between the neurons. Moreover we prove that in general the\nphenomenon of propagation of chaos does not occur, even in the thermodynamic\nlimit, due to the correlation structure of the 3 sources of randomness\nconsidered in the model. Furthermore, we show that the propagation of chaos\ndoes not depend only on the number of neurons in the network, but also and\nmainly on the number of incoming connections per neuron. To conclude, we prove\nthat for special values of the parameters of the system the neurons become\nperfectly correlated, a phenomenon that we have called stochastic\nsynchronization. These discoveries clearly prevent the use of the mean-field\ntheory in the description of the neural network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 15:33:13 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Fasoli", "D.", ""], ["Faugeras", "O.", ""]]}, {"id": "1307.2579", "submitter": "Jonathan Huang", "authors": "Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng,\n  Daphne Koller", "title": "Tuned Models of Peer Assessment in MOOCs", "comments": "Proceedings of The 6th International Conference on Educational Data\n  Mining (EDM 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive open online courses (MOOCs), peer grading serves as a critical\ntool for scaling the grading of complex, open-ended assignments to courses with\ntens or hundreds of thousands of students. But despite promising initial\ntrials, it does not always deliver accurate results compared to human experts.\nIn this paper, we develop algorithms for estimating and correcting for grader\nbiases and reliabilities, showing significant improvement in peer grading\naccuracy on real data with 63,199 peer grades from Coursera's HCI course\nofferings --- the largest peer grading networks analysed to date. We relate\ngrader biases and reliabilities to other student factors such as student\nengagement, performance as well as commenting style. We also show that our\nmodel can lead to more intelligent assignment of graders to gradees.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 20:03:51 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Piech", "Chris", ""], ["Huang", "Jonathan", ""], ["Chen", "Zhenghao", ""], ["Do", "Chuong", ""], ["Ng", "Andrew", ""], ["Koller", "Daphne", ""]]}, {"id": "1307.2674", "submitter": "Hongwei Li", "authors": "Hongwei Li, Bin Yu and Dengyong Zhou", "title": "Error Rate Bounds in Crowdsourcing Models", "comments": "13 pages, 3 figures, downloadable supplementary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is an effective tool for human-powered computation on many\ntasks challenging for computers. In this paper, we provide finite-sample\nexponential bounds on the error rate (in probability and in expectation) of\nhyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. The\nbounds can be applied to analyze many common prediction methods, including the\nmajority voting and weighted majority voting. These bound results could be\nuseful for controlling the error rate and designing better algorithms. We show\nthat the oracle Maximum A Posterior (MAP) rule approximately optimizes our\nupper bound on the mean error rate for any hyperplane binary labeling rule, and\npropose a simple data-driven weighted majority voting (WMV) rule (called\none-step WMV) that attempts to approximate the oracle MAP and has a provable\ntheoretical guarantee on the error rate. Moreover, we use simulated and real\ndata to demonstrate that the data-driven EM-MAP rule is a good approximation to\nthe oracle MAP rule, and to demonstrate that the mean error rate of the\ndata-driven EM-MAP rule is also bounded by the mean error rate bound of the\noracle MAP rule with estimated parameters plugging into the bound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 05:19:10 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Li", "Hongwei", ""], ["Yu", "Bin", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1307.2869", "submitter": "Gail Potter", "authors": "Gail E. Potter, Timo Smieszek, Kerstin Sailer", "title": "Modelling workplace contact networks: the effects of organizational\n  structure, architecture, and reporting errors on epidemic predictions", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face social contacts are potentially important transmission routes\nfor acute respiratory infections, and understanding the contact network can\nimprove our ability to predict, contain, and control epidemics. Although\nworkplaces are important settings for infectious disease transmission, few\nstudies have collected workplace contact data and estimated workplace contact\nnetworks. We use contact diaries, architectural distance measures, and\ninstitutional structures to estimate social contact networks within a Swiss\nresearch institute. Some contact reports were inconsistent, indicating\nreporting errors. We adjust for this with a latent variable model, jointly\nestimating the true (unobserved) network of contacts and duration-specific\nreporting probabilities. We find that contact probability decreases with\ndistance, and research group membership, role, and shared projects are strongly\npredictive of contact patterns. Estimated reporting probabilities were low only\nfor 0-5 minute contacts. Adjusting for reporting error changed the estimate of\nthe duration distribution, but did not change the estimates of covariate\neffects and had little effect on epidemic predictions. Our epidemic simulation\nstudy indicates that inclusion of network structure based on architectural and\norganizational structure data can improve the accuracy of epidemic forecasting\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 18:17:22 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 17:58:50 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 07:01:48 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Potter", "Gail E.", ""], ["Smieszek", "Timo", ""], ["Sailer", "Kerstin", ""]]}, {"id": "1307.2921", "submitter": "Daniel Lawson", "authors": "Daniel John Lawson and Neeraj Oak", "title": "Apparent strength conceals instability in a model for the collapse of\n  historical states", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0096523", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explanation for the political processes leading to the sudden collapse of\nempires and states would be useful for understanding both historical and\ncontemporary political events. We seek a general description of state collapse\nspanning eras and cultures, from small kingdoms to continental empires, drawing\non a suitably diverse range of historical sources. Our aim is to provide an\naccessible verbal hypothesis that bridges the gap between mathematical and\nsocial methodology. We use game-theory to determine whether factions within a\nstate will accept the political status quo, or wish to better their\ncircumstances through costly rebellion. In lieu of precise data we verify our\nmodel using sensitivity analysis. We find that a small amount of\ndissatisfaction is typically harmless, but can trigger sudden collapse when\nthere is a sufficient buildup of political inequality. Contrary to intuition, a\nstate is predicted to be least stable when its leadership is at the height of\nits political power and thus most able to exert its influence through external\nwarfare, lavish expense or autocratic decree.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 21:14:35 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Lawson", "Daniel John", ""], ["Oak", "Neeraj", ""]]}, {"id": "1307.3146", "submitter": "Alice Cleynen", "authors": "Alice Cleynen and St\\'ephane Robin", "title": "Comparing change-point locations of independent profiles with\n  application to gene annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the comparison of transcript boundaries from cells which\noriginated in different environments. The goal is to assess whether this\nphenomenon, called differential splicing, is used to modify the transcription\nof the genome in response to stress factors. We address this question by\ncomparing the change-points locations in the individual segmentation of each\nprofile, which correspond to the RNA-Seq data for a gene in one growth\ncondition. This requires the ability to evaluate the uncertainty of the\nchange-point positions, and the work of Rigaill et. al. (2011) provides an\nappropriate framework in such case. Building on their approach, we propose two\nmethods for the comparison of change-points, and illustrate our results on a\ndataset from the yeast specie. We show that the UTR boundaries are subject to\ndifferential splicing, while the intron boundaries are conserved in all\nprofiles. Our approach is implemented in an R package called EBS which is\navailable on the CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 15:44:37 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Cleynen", "Alice", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1307.3286", "submitter": "Djalel Eddine Meskaldji", "authors": "Djalel Eddine Meskaldji and Patric Hagmann and Jean-Philippe Thiran\n  and Stephan Morgenthaler", "title": "Two step multiple comparisons procedures for positively dependent data\n  with application to detecting differences in human brain network topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing positively dependent multiple hypotheses\nassuming that a prior information about the dependence structure is available.\nWe propose two-step multiple comparisons procedures that exploit the prior\ninformation of the dependence structure, without relying on strong assumptions.\nIn the first step, we group the tests into subsets where tests are supposed to\nbe positively dependent and in each of which we compute the standardized mean\nof the test scores. Given the subset mean scores or equivalently the subsets\np-values, we apply a first screening at a predefined threshold, which results\nin two types of subsets. Based on this typing, the original single test\np-values are modified such that they can be used in conjunction with any\nmultiple comparison procedure. We show by means of different simulation that\npower is gained with the proposed two-step methods, and compare it with\ntraditional multiple comparison procedures. As an illustration, our method is\napplied on real data comparing topological differences between two groups of\nhuman brain networks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:21:11 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Meskaldji", "Djalel Eddine", ""], ["Hagmann", "Patric", ""], ["Thiran", "Jean-Philippe", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "1307.3490", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "On-line Bayesian parameter estimation in general non-linear state-space\n  models: A tutorial and new results", "comments": "A condensed version of this article has been published in: Tulsyan,\n  A., Huang, B., Gopaluni, R.B., Forbes, J.F. \"On simultaneous on-line state\n  and parameter estimation in non-linear state-space models\". Journal of\n  Process Control, vol 23, no. 4, 2013", "journal-ref": "Journal of Process Control, vol 23, no. 4, 2013", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line estimation plays an important role in process control and monitoring.\nObtaining a theoretical solution to the simultaneous state-parameter estimation\nproblem for non-linear stochastic systems involves solving complex\nmulti-dimensional integrals that are not amenable to analytical solution. While\nbasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for\nsimultaneous estimation exist, it is well recognized that there is a need for\nmaking these on-line algorithms non-degenerate, fast and applicable to\nprocesses with missing measurements. To overcome the deficiencies in\ntraditional algorithms, this work proposes a Bayesian approach to on-line state\nand parameter estimation. Its extension to handle missing data in real-time is\nalso provided. The simultaneous estimation is performed by filtering an\nextended vector of states and parameters using an adaptive\nsequential-importance-resampling (SIR) filter with a kernel density estimation\nmethod. The approach uses an on-line optimization algorithm based on\nKullback-Leibler (KL) divergence to allow adaptation of the SIR filter for\ncombined state-parameter estimation. An optimal tuning rule to control the\nwidth of the kernel and the variance of the artificial noise added to the\nparameters is also proposed. The approach is illustrated through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:30:38 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.3495", "submitter": "James Scott", "authors": "James G. Scott, Ryan C. Kelly, Matthew A. Smith, Pengcheng Zhou, and\n  Robert E. Kass", "title": "False discovery rate regression: an application to neural synchrony\n  detection in primary visual cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches for multiple testing begin with the assumption that all tests\nin a given study should be combined into a global false-discovery-rate\nanalysis. But this may be inappropriate for many of today's large-scale\nscreening problems, where auxiliary information about each test is often\navailable, and where a combined analysis can lead to poorly calibrated error\nrates within different subsets of the experiment. To address this issue, we\nintroduce an approach called false-discovery-rate regression that directly uses\nthis auxiliary information to inform the outcome of each test. The method can\nbe motivated by a two-groups model in which covariates are allowed to influence\nthe local false discovery rate, or equivalently, the posterior probability that\na given observation is a signal. This poses many subtle issues at the interface\nbetween inference and computation, and we investigate several variations of the\noverall approach. Simulation evidence suggests that: (1) when covariate effects\nare present, FDR regression improves power for a fixed false-discovery rate;\nand (2) when covariate effects are absent, the method is robust, in the sense\nthat it does not lead to inflated error rates. We apply the method to neural\nrecordings from primary visual cortex. The goal is to detect pairs of neurons\nthat exhibit fine-time-scale interactions, in the sense that they fire together\nmore often than expected due to chance. Our method detects roughly 50% more\nsynchronous pairs versus a standard FDR-controlling analysis. The companion R\npackage FDRreg implements all methods described in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:55:33 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2013 03:30:47 GMT"}, {"version": "v3", "created": "Sun, 8 Jun 2014 22:27:29 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Scott", "James G.", ""], ["Kelly", "Ryan C.", ""], ["Smith", "Matthew A.", ""], ["Zhou", "Pengcheng", ""], ["Kass", "Robert E.", ""]]}, {"id": "1307.3544", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Yunghsiang S. Han, Swastik Brahma, Pramod K.\n  Varshney", "title": "Distributed Bayesian Detection with Byzantine Data", "comments": "32 pages, 4 figures, Submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2450191", "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.GT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of distributed Bayesian detection in\nthe presence of Byzantines in the network. It is assumed that a fraction of the\nnodes in the network are compromised and reprogrammed by an adversary to\ntransmit false information to the fusion center (FC) to degrade detection\nperformance. The problem of distributed detection is formulated as a binary\nhypothesis test at the FC based on 1-bit data sent by the sensors. The\nexpression for minimum attacking power required by the Byzantines to blind the\nFC is obtained. More specifically, we show that above a certain fraction of\nByzantine attackers in the network, the detection scheme becomes completely\nincapable of utilizing the sensor data for detection. We analyze the problem\nunder different attacking scenarios and derive results for different\nnon-asymptotic cases. It is found that existing asymptotics-based results do\nnot hold under several non-asymptotic scenarios. When the fraction of\nByzantines is not sufficient to blind the FC, we also provide closed form\nexpressions for the optimal attacking strategies for the Byzantines that most\ndegrade the detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 19:28:00 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:56:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Han", "Yunghsiang S.", ""], ["Brahma", "Swastik", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1307.3598", "submitter": "Paul McNicholas", "authors": "Irene Vrbik and Paul D. McNicholas", "title": "Fractionally-Supervised Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s00357-015-9188-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there are three species of classification: unsupervised,\nsupervised, and semi-supervised. Supervised and semi-supervised classification\ndiffer by whether or not weight is given to unlabelled observations in the\nclassification procedure. In unsupervised classification, or clustering, all\nobservations are unlabeled and hence full weight is given to unlabelled\nobservations. When some observations are unlabelled, it can be very difficult\nto \\textit{a~priori} choose the optimal level of supervision, and the\nconsequences of a sub-optimal choice can be non-trivial. A flexible\nfractionally-supervised approach to classification is introduced, where any\nlevel of supervision --- ranging from unsupervised to supervised --- can be\nattained. Our approach uses a weighted likelihood, wherein weights control the\nrelative role that labelled and unlabelled data have in building a classifier.\nA comparison between our approach and the traditional species is presented\nusing simulated and real data. Gaussian mixture models are used as a vehicle to\nillustrate our fractionally-supervised classification approach; however, it is\nbroadly applicable and variations on the postulated model can be easily made.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 00:41:37 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 16:15:30 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 05:29:13 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2015 18:04:19 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 18:16:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Vrbik", "Irene", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1307.4132", "submitter": "Roy Dong", "authors": "Roy Dong, Lillian J. Ratliff, Henrik Ohlsson, S. Shankar Sastry", "title": "Energy Disaggregation via Adaptive Filtering", "comments": "Submitted to 51st Annual Allerton Conference on Communication,\n  Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy disaggregation problem is recovering device level power\nconsumption signals from the aggregate power consumption signal for a building.\nWe show in this paper how the disaggregation problem can be reformulated as an\nadaptive filtering problem. This gives both a novel disaggregation algorithm\nand a better theoretical understanding for disaggregation. In particular, we\nshow how the disaggregation problem can be solved online using a filter bank\nand discuss its optimality.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 23:33:37 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Dong", "Roy", ""], ["Ratliff", "Lillian J.", ""], ["Ohlsson", "Henrik", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1307.4592", "submitter": "Pierre Weiss", "authors": "J\\'er\\^ome Fehrenbach, Pierre Weiss", "title": "Processing stationary noise: model and parameter selection in\n  variational methods", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive or multiplicative stationary noise recently became an important\nissue in applied fields such as microscopy or satellite imaging. Relatively few\nworks address the design of dedicated denoising methods compared to the usual\nwhite noise setting. We recently proposed a variational algorithm to tackle\nthis issue. In this paper, we analyze this problem from a statistical point of\nview and provide deterministic properties of the solutions of the associated\nvariational problems. In the first part of this work, we demonstrate that in\nmany practical problems, the noise can be assimilated to a colored Gaussian\nnoise. We provide a quantitative measure of the distance between a stationary\nprocess and the corresponding Gaussian process. In the second part, we focus on\nthe Gaussian setting and analyze denoising methods which consist of minimizing\nthe sum of a total variation term and an $l^2$ data fidelity term. While the\nconstrained formulation of this problem allows to easily tune the parameters,\nthe Lagrangian formulation can be solved more efficiently since the problem is\nstrongly convex. Our second contribution consists in providing analytical\nvalues of the regularization parameter in order to approximately satisfy\nMorozov's discrepancy principle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 12:14:52 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Fehrenbach", "J\u00e9r\u00f4me", ""], ["Weiss", "Pierre", ""]]}, {"id": "1307.5040", "submitter": "Sarod Yatawatta", "authors": "S. Kazemi and S. Yatawatta", "title": "Robust Radio Interferometric Calibration Using the t-Distribution", "comments": "MNRAS accepted", "journal-ref": null, "doi": "10.1093/mnras/stt1347", "report-no": null, "categories": "astro-ph.IM math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major stage of radio interferometric data processing is calibration or the\nestimation of systematic errors in the data and the correction for such errors.\nA stochastic error (noise) model is assumed, and in most cases, this underlying\nmodel is assumed to be Gaussian. However, outliers in the data due to\ninterference or due to errors in the sky model would have adverse effects on\nprocessing based on a Gaussian noise model. Most of the shortcomings of\ncalibration such as the loss in flux or coherence, and the appearance of\nspurious sources, could be attributed to the deviations of the underlying noise\nmodel. In this paper, we propose to improve the robustness of calibration by\nusing a noise model based on Student's t distribution. Student's t noise is a\nspecial case of Gaussian noise when the variance is unknown. Unlike Gaussian\nnoise model based calibration, traditional least squares minimization would not\ndirectly extend to a case when we have a Student's t noise model. Therefore, we\nuse a variant of the Expectation Maximization (EM) algorithm, called the\nExpectation-Conditional Maximization Either (ECME) algorithm when we have a\nStudent's t noise model and use the Levenberg-Marquardt algorithm in the\nmaximization step. We give simulation results to show the robustness of the\nproposed calibration method as opposed to traditional Gaussian noise model\nbased calibration, especially in preserving the flux of weaker sources that are\nnot included in the calibration model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 18:43:49 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Kazemi", "S.", ""], ["Yatawatta", "S.", ""]]}, {"id": "1307.5145", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh, Sachin Malik and Viplav K. Singh", "title": "An Improved Estimator In Systematic Sampling", "comments": "6 pages, 1 table", "journal-ref": "Jour. Of Scie. Res., 56, 177-182", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have adapted Bahl and Tuteja (1991) estimator in systematic\nsampling using auxiliary information. Using Bedi (1996) transformation an\nimproved estimator is also proposed under systematic sampling. The expressions\nof bias and mean square error up to the first order of approximation are\nderived. It has been shown that the proposed estimator performs better than\nmany existing estimators. A numerical example is included to support the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 07:36:38 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Singh", "Rajesh", ""], ["Malik", "Sachin", ""], ["Singh", "Viplav K.", ""]]}, {"id": "1307.5558", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Paul D. McNicholas and Ryan P. Browne", "title": "Mixtures of Common Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.43", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 19:18:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 15:56:34 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2013 21:28:57 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Murray", "Paula M.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1307.6021", "submitter": "Francisco Javier Rubio", "authors": "F. J. Rubio, E. O. Ogundimu, J. L. Hutton", "title": "On modelling asymmetric data using two-piece sinh-arcsinh distributions", "comments": "Previous title: Robust modelling using two-piece sinh-arcsinh\n  distributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the univariate two--piece sinh-arcsinh distribution, which\ncontains two shape parameters that separately control skewness and kurtosis. We\nshow that this new model can capture higher levels of asymmetry than the\noriginal sinh-arcsinh distribution (Jones and Pewsey, 2009), in terms of some\nasymmetry measures, while keeping flexibility of the tails and tractability. We\nillustrate the performance of the proposed model with real data, and compare it\nto appropriate alternatives. Although we focus on the study of the univariate\nversions of the proposed distributions, we point out some multivariate\nextensions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 11:16:28 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 18:54:39 GMT"}, {"version": "v3", "created": "Sat, 10 Jan 2015 10:28:16 GMT"}, {"version": "v4", "created": "Wed, 20 May 2015 08:58:13 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Rubio", "F. J.", ""], ["Ogundimu", "E. O.", ""], ["Hutton", "J. L.", ""]]}, {"id": "1307.6254", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "Error analysis in Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, B. Huang, R.B.\n  Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part II- Error Analysis. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, several methods based on sequential Monte Carlo\n(SMC) and Markov chain Monte Carlo (MCMC) have been proposed for Bayesian\nidentification of stochastic non-linear state-space models (SSMs). It is well\nknown that the performance of these simulation based identification methods\ndepends on the numerical approximations used in their design. We propose the\nuse of posterior Cram\\'er-Rao lower bound (PCRLB) as a mean square error (MSE)\nbound. Using PCRLB, a systematic procedure is developed to analyse the\nestimates delivered by Bayesian identification methods in terms of bias, MSE,\nand efficiency. The efficacy and utility of the proposed approach is\nillustrated through a numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 21:53:53 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6258", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Swanand R. Khare, Biao Huang, R. Bhushan Gopaluni and\n  J. Fraser Forbes", "title": "Input design for Bayesian identification of non-linear state-space\n  models", "comments": "This article has been published in: Tulsyan, A, S.R. Khare, B. Huang,\n  R.B. Gopaluni and J.F. Forbes (2013). Bayesian identification of non-linear\n  state-space models: Part I- Input design. In: Proceedings of the 10th IFAC\n  International Symposium on Dynamics and Control of Process Systems. Mumbai,\n  India", "journal-ref": "Proceedings of the 10th IFAC International Symposium on Dynamics\n  and Control of Process Systems. Mumbai, India, 2013", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for designing optimal inputs for on-line Bayesian\nidentification of stochastic non-linear state-space models. The proposed method\nrelies on minimization of the posterior Cram\\'er Rao lower bound derived for\nthe model parameters, with respect to the input sequence. To render the\noptimization problem computationally tractable, the inputs are parametrized as\na multi-dimensional Markov chain in the input space. The proposed approach is\nillustrated through a simulation example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 22:09:04 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Khare", "Swanand R.", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.6290", "submitter": "Qixin Wang", "authors": "Guanxi Zhuang", "title": "Neural Network Model of Pricing Health Care Insurance", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To pricing health insurance plan, statisticians use mathematical models to\npredict customers' future health condition. General Addictive Model (GAM) is a\nwide accepted method for this problem. However, it have several limitations. To\nsolve this problem, a new method named neural network model is implemented.\nCompare with GAM model, neural network provide a more accurate predicting\nresult.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 03:48:25 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Zhuang", "Guanxi", ""]]}, {"id": "1307.6366", "submitter": "Jonas Wallin", "authors": "David Bolin and Jonas Wallin", "title": "Non-Gaussian Mat\\'ern fields with an application to precipitation\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed non-Gaussian Mat\\'{e}rn random field models, generated\nthrough Stochastic Partial differential equations (SPDEs), are extended by\nconsidering the class of Generalized Hyperbolic processes as noise forcings.\nThe models are also extended to the standard geostatistical setting where\nirregularly spaced observations are modeled using measurement errors and\ncovariates. A maximum likelihood estimation technique based on the Monte Carlo\nExpectation Maximization (MCEM) algorithm is presented, and it is shown how the\nmodel can be used to do predictions at unobserved locations. Finally, an\napplication to precipitation data over the United States for two month in 1997\nis presented, and the performance of the non-Gaussian models is compared with\nstandard Gaussian and transformed Gaussian models through cross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 10:08:36 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "1307.6417", "submitter": "Andreas Mayr", "authors": "Andreas Mayr and Matthias Schmid", "title": "Boosting the concordance index for survival data - a unified framework\n  to derive and evaluate biomarker combinations", "comments": "revised manuscript - added simulation study, additional results", "journal-ref": "PloS ONE 2014, 9(1): e84483", "doi": "10.1371/journal.pone.0084483", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of molecular signatures for the prediction of time-to-event\noutcomes is a methodologically challenging task in bioinformatics and\nbiostatistics. Although there are numerous approaches for the derivation of\nmarker combinations and their evaluation, the underlying methodology often\nsuffers from the problem that different optimization criteria are mixed during\nthe feature selection, estimation and evaluation steps. This might result in\nmarker combinations that are only suboptimal regarding the evaluation criterion\nof interest. To address this issue, we propose a unified framework to derive\nand evaluate biomarker combinations. Our approach is based on the concordance\nindex for time-to-event data, which is a non-parametric measure to quantify the\ndiscrimatory power of a prediction rule. Specifically, we propose a\ncomponent-wise boosting algorithm that results in linear biomarker combinations\nthat are optimal with respect to a smoothed version of the concordance index.\nWe investigate the performance of our algorithm in a large-scale simulation\nstudy and in two molecular data sets for the prediction of survival in breast\ncancer patients. Our numerical results show that the new approach is not only\nmethodologically sound but can also lead to a higher discriminatory power than\ntraditional approaches for the derivation of gene signatures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 13:51:16 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 14:14:42 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Mayr", "Andreas", ""], ["Schmid", "Matthias", ""]]}, {"id": "1307.6539", "submitter": "Brian Macdonald", "authors": "Brian Macdonald, Christopher Weld, David C. Arney", "title": "Quantifying playmaking ability in hockey", "comments": "21 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often said that a sign of a great player is that he makes the players\naround him better. The player may or may not score much himself, but his\nteammates perform better when he plays. One way a hockey player can improve his\nor her teammates' performance is to create goal scoring opportunities.\nUnfortunately, in hockey goal scoring is relatively infrequent, and statistics\nlike assists can be unreliable as a measure of a player's playmaking ability.\nAssists also depend on playing time, power play usage, the strength of a\nplayer's linemates, and other factors. In this paper we develop a metric for\nquantifying playmaking ability that addresses these issues. Our playmaking\nmetric has two benefits over assists for which we can provide statistical\nevidence: it is more consistent than assists, and it is better than assists at\npredicting future assists. Quantifying player contributions using this measure\ncan assist decision-makers in identifying, acquiring, and integrating\nsuccessful playmakers into their lineups.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 19:31:54 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Macdonald", "Brian", ""], ["Weld", "Christopher", ""], ["Arney", "David C.", ""]]}, {"id": "1307.6837", "submitter": "Nicolas Chauffert", "authors": "Nicolas Chauffert (INRIA Saclay - Ile de France), Philippe Ciuciu\n  (INRIA Saclay - Ile de France), Jonas Kahn, Pierre Weiss (ITAV)", "title": "Travelling salesman-based variable density sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1302.1281", "journal-ref": "SampTA - 10th Conference International Conference on Sampling\n  Theory and Applications (2013) 509-512", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing theory indicates that selecting a few measurements\nindependently at random is a near optimal strategy to sense sparse or\ncompressible signals. This is infeasible in practice for many acquisition\ndevices that acquire sam- ples along continuous trajectories. Examples include\nmagnetic resonance imaging (MRI), radio-interferometry, mobile-robot sampling,\n... In this paper, we propose to generate continuous sampling trajectories by\ndrawing a small set of measurements independently and joining them using a\ntravelling salesman problem solver. Our contribution lies in the theoretical\nderivation of the appropriate probability density of the initial drawings.\nPreliminary simulation results show that this strategy is as efficient as\nindependent drawings while being implementable on real acquisition systems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 18:44:03 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Chauffert", "Nicolas", "", "INRIA Saclay - Ile de France"], ["Ciuciu", "Philippe", "", "INRIA Saclay - Ile de France"], ["Kahn", "Jonas", "", "ITAV"], ["Weiss", "Pierre", "", "ITAV"]]}, {"id": "1307.6889", "submitter": "Nicholas Magliocca", "authors": "N. R. Magliocca (1), E. C. Ellis (1), T. Oates (2) and M. Schmill (2)\n  ((1) Department of Geography and Environmental Systems, University of\n  Maryland, Baltimore County, Baltimore, Maryland, USA,(2) Department of\n  Computer Science and Electrical Engineering, University of Maryland,\n  Baltimore County, Baltimore, Maryland, USA)", "title": "Contextualizing the global relevance of local land change observations", "comments": "5 pages, 4 figures, white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand global changes in the Earth system, scientists must generalize\nglobally from observations made locally and regionally. In land change science\n(LCS), local field-based observations are costly and time consuming, and\ngenerally obtained by researchers working at disparate local and regional\ncase-study sites chosen for different reasons. As a result, global synthesis\nefforts in LCS tend to be based on non-statistical inferences subject to\ngeographic biases stemming from data limitations and fragmentation. Thus, a\nfundamental challenge is the production of generalized knowledge that links\nevidence of the causes and consequences of local land change to global patterns\nand vice versa. The GLOBE system was designed to meet this challenge. GLOBE\naims to transform global change science by enabling new scientific workflows\nbased on statistically robust, globally relevant integration of local and\nregional observations using an online social-computational and geovisualization\nsystem. Consistent with the goals of Digital Earth, GLOBE has the capability to\nassess the global relevance of local case-study findings within the context of\nover 50 global biophysical, land-use, climate, and socio-economic datasets. We\ndemonstrate the implementation of one such assessment - a representativeness\nanalysis - with a recently published meta-study of changes in swidden\nagriculture in tropical forests. The analysis provides a standardized indicator\nto judge the global representativeness of the trends reported in the\nmeta-study, and a geovisualization is presented that highlights areas for which\nsampling efforts can be reduced and those in need of further study. GLOBE will\nenable researchers and institutions to rapidly share, compare, and synthesize\nlocal and regional studies within the global context, as well as contributing\nto the larger goal of creating a Digital Earth.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 22:40:04 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Magliocca", "N. R.", ""], ["Ellis", "E. C.", ""], ["Oates", "T.", ""], ["Schmill", "M.", ""]]}, {"id": "1307.6960", "submitter": "Nicolas Chauffert", "authors": "Nicolas Chauffert (INRIA Saclay - Ile de France), Philippe Ciuciu\n  (INRIA Saclay - Ile de France), Pierre Weiss (ITAV), Fabrice Gamboa (UMR CNRS\n  5219)", "title": "From variable density sampling to continuous sampling using Markov\n  chains", "comments": null, "journal-ref": "SampTA - 10th Conference International Conference on Sampling\n  Theory and Applications (2013) 200-203", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its discovery over the last decade, Compressed Sensing (CS) has been\nsuccessfully applied to Magnetic Reso- nance Imaging (MRI). It has been shown\nto be a powerful way to reduce scanning time without sacrificing image quality.\nMR images are actually strongly compressible in a wavelet basis, the latter\nbeing largely incoherent with the k-space or spatial Fourier domain where\nacquisition is performed. Nevertheless, since its first application to MRI [1],\nthe theoretical justification of actual k-space sampling strategies is\nquestionable. Indeed, the vast majority of k-space sampling distributions have\nbeen heuris- tically designed (e.g., variable density) or driven by\nexperimental feasibility considerations (e.g., random radial or spiral sampling\nto achieve smoothness k-space trajectory). In this paper, we try to reconcile\nvery recent CS results with the MRI specificities (mag- netic field gradients)\nby enforcing the measurements, i.e. samples of k-space, to fit continuous\ntrajectories. To this end, we propose random walk continuous sampling based on\nMarkov chains and we compare the reconstruction quality of this scheme to the\nstate- of-the art.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 08:57:15 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Chauffert", "Nicolas", "", "INRIA Saclay - Ile de France"], ["Ciuciu", "Philippe", "", "INRIA Saclay - Ile de France"], ["Weiss", "Pierre", "", "ITAV"], ["Gamboa", "Fabrice", "", "UMR CNRS\n  5219"]]}, {"id": "1307.7126", "submitter": "Grigory Sokolov", "authors": "Aleksey S. Polunchenko, Grigory Sokolov and Alexander G. Tartakovsky", "title": "Optimal Design and Analysis of the Exponentially Weighted Moving Average\n  Chart for Exponential Data", "comments": "28 pages, 5 figures, accepted for publication in the Sri Lankan\n  Journal of Applied Statistics. arXiv admin note: text overlap with\n  arXiv:1202.2849. text overlap with arXiv:1202.2849", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal design of the Exponentially Weighted Moving Average (EWMA)\nchart by a proper choice of the smoothing factor and the initial value\n(headstart) of the decision statistic. The particular problem addressed is that\nof quickest detection of an abrupt change in the parameter of a discrete-time\nexponential model. Both pre- and post-change parameter values are assumed\nknown, but the change-point is not known. For this change-point detection\nscenario, we examine the performance of the conventional one-sided EWMA chart\nwith respect to two optimality criteria: Pollak's minimax criterion associated\nwith the maximal conditional expected delay to detection and Shiryaev's\nmulti-cyclic setup associated with the stationary expected delay to detection.\nUsing the integral-equations approach, we derive the exact closed-form formulae\nfor all of the required performance measures. Based on these formulae we find\nthe optimal smoothing factor and headstart by solving the corresponding two\nbivariate constraint optimization problems. Finally, the performance of the\noptimized EWMA chart is compared against that of the Shiryaev--Roberts--$r$\nprocedure in the minimax setting, and against that of the original\nShiryaev--Roberts procedure in the multi-cyclic setting. The main conclusion is\nthat the EWMA chart, when fully optimized, turns out to be a very competitive\nprocedure, with performance nearly indistinguishable from that of the\nknown-to-be-best Shiryaev--Roberts--$r$ and Shiryaev--Roberts procedures.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 19:11:35 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 20:11:11 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 20:30:44 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1307.7536", "submitter": "Mette Langaas", "authors": "Mette Langaas, {\\O}yvind Bakke", "title": "Robust Methods for Disease-Genotype Association in Genetic Association\n  Studies: Calculate P-values Using Exact Conditional Enumeration instead of\n  Asymptotic Approximations", "comments": null, "journal-ref": null, "doi": "10.1515/sagmb-2013-0084", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, detecting disease-genotype associations is a\nprimary goal. For most diseases, the underlying genetic model is unknown, and\nwe study seven robust test statistics for monotone association. For a given\ntest statistic, there are many ways to calculate a p-value, but in genetic\nassociation studies, calculations have predominantly been based on asymptotic\napproximations or on simulated permutations. We show that when the number of\npermutations tends to infinity, the permutation p-value approaches the exact\nconditional enumeration p-value, and further that calculating the latter\np-value is much more efficient than performing simulated permutations. We then\nanswer two research questions. (i) Which of the test statistics under study are\nthe most powerful for monotone genetic models? (ii) Based on test size, power,\nand computational considerations, should asymptotic approximations or exact\nconditional enumeration be used for calculating p-values? We have studied\ncase-control sample sizes with 500-5000 cases and 500-15000 controls, and\nsignificance levels from 5e-8 to 0.05, thus our results are applicable to\ngenetic association studies with only one genetic marker under study,\nintermediate follow-up studies, and genome wide association studies. We find\nthat if all monotone genetic models are of interest, the best performance is\nachieved for a test statistics based on the maximum over a range of\nCochrane-Armitage trend tests with different scores and for a constrained\nlikelihood ratio test. For significance levels below 0.05, asymptotic\napproximations may give a test size up to 20 times the nominal level, and\nshould therefore be used with caution. Further, calculating p-values based on\nexact conditional enumeration is a powerful, valid and computationally feasible\napproach, and we advocate its use in genetic association studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 10:58:54 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Langaas", "Mette", ""], ["Bakke", "\u00d8yvind", ""]]}, {"id": "1307.7667", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Jinlin Song", "title": "A Rejection Principle for Sequential Tests of Multiple Hypotheses\n  Controlling Familywise Error Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying approach to multiple testing procedures for sequential\n(or streaming) data by giving sufficient conditions for a sequential multiple\ntesting procedure to control the familywise error rate (FWER), extending to the\nsequential domain the work of Goeman and Solari (2010) who accomplished this\nfor fixed sample size procedures. Together we call these conditions the\n\"rejection principle for sequential tests,\" which we then apply to some\nexisting sequential multiple testing procedures to give simplified\nunderstanding of their FWER control. Next the principle is applied to derive\ntwo new sequential multiple testing procedures with provable FWER control, one\nfor testing hypotheses in order and another for closed testing. Examples of\nthese new procedures are given by applying them to a chromosome aberration data\nset and to finding the maximum safe dose of a treatment.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 18:05:00 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:58:58 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Bartroff", "Jay", ""], ["Song", "Jinlin", ""]]}, {"id": "1307.7860", "submitter": "Cathy Maugis", "authors": "Gilles Celeux, Marie-Laure Martin-Magniette, Cathy Maugis-Rabusseau\n  and Adrian E. Raftery", "title": "Comparing Model Selection and Regularization Approaches to Variable\n  Selection in Model-Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two major approaches to variable selection in clustering: model\nselection and regularization. Based on previous results, we select the method\nof Maugis et al. (2009b), which modified the method of Raftery and Dean (2006),\nas a current state of the art model selection method. We select the method of\nWitten and Tibshirani (2010) as a current state of the art regularization\nmethod. We compared the methods by simulation in terms of their accuracy in\nboth classification and variable selection. In the first simulation experiment\nall the variables were conditionally independent given cluster membership. We\nfound that variable selection (of either kind) yielded substantial gains in\nclassification accuracy when the clusters were well separated, but few gains\nwhen the clusters were close together. We found that the two variable selection\nmethods had comparable classification accuracy, but that the model selection\napproach had substantially better accuracy in selecting variables. In our\nsecond simulation experiment, there were correlations among the variables given\nthe cluster memberships. We found that the model selection approach was\nsubstantially more accurate in terms of both classification and variable\nselection than the regularization approach, and that both gave more accurate\nclassifications than $K$-means without variable selection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 08:02:02 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Celeux", "Gilles", ""], ["Martin-Magniette", "Marie-Laure", ""], ["Maugis-Rabusseau", "Cathy", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1307.7950", "submitter": "Xia Shen", "authors": "Xia Shen", "title": "Revealing the missing heritability via cross-validated genome-wide\n  association studies", "comments": "7 pages main text, 2 figures, 2 supplementary tables, 49\n  supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented here is a simple method for cross-validated genome-wide association\nstudies (cvGWAS). Focusing on phenotype prediction, the method is able to\nreveal a significant amount of missing heritability by properly selecting a\nsmall number of loci with implicit predictive ability. The results provide new\ninsights into the missing heritability problem and the underlying genetic\narchitecture of complex traits.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 12:50:37 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2013 13:27:58 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Shen", "Xia", ""]]}, {"id": "1307.8046", "submitter": "Andrea Rau", "authors": "Andrea Rau and Florence Jaffr\\'ezic and Gr\\'egory Nuel", "title": "Joint estimation of causal effects from observational and intervention\n  gene expression data", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Inference of gene regulatory networks from transcriptomic data\nhas been a wide research area in recent years. Proposed methods are mainly\nbased on the use of graphical Gaussian models for observational wild-type data\nand provide undirected graphs that are not able to accurately highlight the\ncausal relationships among genes. In the present work, we seek to improve\nestimation of causal effects among genes by jointly modeling observational\ntranscriptomic data with intervention data obtained by performing knock-outs or\nknock-downs on a subset of genes. By examining the impact of such expression\nperturbations on other genes, a more accurate reflection of regulatory\nrelationships may be obtained than through the use of wild-type data alone.\nResults: Using the framework of Gaussian Bayesian networks, we propose a Markov\nchain Monte Carlo algorithm with a Mallows model and an analytical likelihood\nmaximization to sample from the posterior distribution of causal node\norderings, and in turn, to estimate causal effects. The main advantage of the\nproposed algorithm over previously proposed methods is that it has the\nflexibility to accommodate any kind of intervention design, including partial\nor multiple knock-out experiments. Methods were compared on simulated data as\nwell as data from the DREAM 2007 challenge. Conclusions: The simulation study\nconfirmed the impossibility of estimating causal orderings of genes with\nobservation data only. The proposed algorithm was found, in most cases, to\nperform better than the previously proposed methods in terms of accuracy for\nthe estimation of causal effects. In addition, multiple knock-outs proved to\nbring valuable additional information compared to single knock-outs. The choice\nof optimal intervention design therefore appears to be a crucial aspect for\ncausal inference and an interesting challenge for future research.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 16:59:06 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Rau", "Andrea", ""], ["Jaffr\u00e9zic", "Florence", ""], ["Nuel", "Gr\u00e9gory", ""]]}, {"id": "1307.8229", "submitter": "Mengjie Chen", "authors": "Mengjie Chen, Chao Gao, Hongyu Zhao", "title": "Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By expressing prior distributions as general stochastic processes,\nnonparametric Bayesian methods provide a flexible way to incorporate prior\nknowledge and constrain the latent structure in statistical inference. The\nIndian buffet process (IBP) is such an example that can be used to define a\nprior distribution on infinite binary features, where the exchangeability among\nsubjects is assumed. The phylogenetic Indian buffet process (pIBP), a\nderivative of IBP, enables the modeling of non-exchangeability among subjects\nthrough a stochastic process on a rooted tree, which is similar to that used in\nphylogenetics, to describe relationships among the subjects. In this paper, we\nstudy the theoretical properties of IBP and pIBP under a binary factor model.\nWe establish the posterior contraction rates for both IBP and pIBP and\nsubstantiate the theoretical results through simulation studies. This is the\nfirst work addressing the frequentist property of the posterior behaviors of\nIBP and pIBP. We also demonstrated its practical usefulness by applying pIBP\nprior to a real data example arising in the field of cancer genomics where the\nexchangeability among subjects is violated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 05:56:20 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 20:02:07 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1307.8271", "submitter": "Nicy Sebastian", "authors": "Nicy Sebastian and Rudolf Gorenflo", "title": "A Fractional Generalization of the Poisson Processes and Some of its\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have provided a fractional generalization of the Poisson renewal processes\nby replacing the first time derivative in the relaxation equation of the\nsurvival probability by a fractional derivative of order $\\alpha ~(0 < \\alpha\n\\leq 1)$. A generalized Laplacian model associated with the Mittag-Leffler\ndistribution is examined. We also discuss some properties of this new model and\nits relevance to time series. Distribution of gliding sums, regression\nbehaviors and sample path properties are studied. Finally we introduce the\n$q$-Mittag-Leffler process associated with the $q$-Mittag-Leffler distribution.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 10:23:45 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Sebastian", "Nicy", ""], ["Gorenflo", "Rudolf", ""]]}, {"id": "1307.8308", "submitter": "Yanshan Shi", "authors": "Y. Shi, A. N. Gorban, T. Y. Yang", "title": "Is it possible to predict long-term success with k-NN? Case Study of\n  four market indices (FTSE100, DAX, HANGSENG, NASDAQ)", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1742-6596/490/1/012082", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This case study tests the possibility of prediction for \"success\" (or\n\"winner\") components of four stock & shares market indices in a time period of\nthree years from 02-Jul-2009 to 29-Jun-2012.We compare their performance ain\ntwo time frames: initial frame three months at the beginning\n(02/06/2009-30/09/2009) and the final three month frame\n(02/04/2012-29/06/2012). To label the components, average price ratio between\ntwo time frames in descending order is computed. The average price ratio is\ndefined as the ratio between the mean prices of the beginning and final time\nperiod. The \"winner\" components are referred to the top one third of total\ncomponents in the same order as average price ratio it means the mean price of\nfinal time period is relatively higher than the beginning time period. The\n\"loser\" components are referred to the last one third of total components in\nthe same order as they have higher mean prices of beginning time period. We\nanalyse, is there any information about the winner-looser separation in the\ninitial fragments of the daily closing prices log-returns time series. The\nLeave-One-Out Cross-Validation with k-NN algorithm is applied on the daily\nlog-return of components using a distance and proximity in the experiment. By\nlooking at the error analysis, it shows that for HANGSENG and DAX index, there\nare clear signs of possibility to evaluate the probability of long-term\nsuccess. The correlation distance matrix histograms and 2-D/3-D elastic maps\ngenerated from ViDaExpert show that the winner components are closer to each\nother and winner/loser components are separable on elastic maps for HANGSENG\nand DAX index while for the negative possibility indices, there is no sign of\nseparation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 12:56:35 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Shi", "Y.", ""], ["Gorban", "A. N.", ""], ["Yang", "T. Y.", ""]]}, {"id": "1307.8366", "submitter": "Avi Ma'ayan", "authors": "Neil R. Clark, Kevin Hu, Edward Y. Chen, Qioanan Duan, Avi Ma`ayan", "title": "Characteristic Direction Approach to Identify Differentially Expressed\n  Genes", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide gene expression profiles, as measured with microarrays or RNA-Seq\nexperiments, have revolutionized biological and biomedical research by\nproviding a quantitative measure of the entire mRNA transcriptome. Typically,\nresearchers set up experiments where control samples are compared to a\ntreatment condition, and using the t-test they identify differentially\nexpressed genes upon which further analysis and ultimately biological discovery\nfrom such experiments is based. Here we describe an alternative geometrical\napproach to identify differentially expressed genes. We show that this\nalternative method, called the Characteristic Direction, is capable of\nidentifying more relevant genes. We evaluate our approach in three case\nstudies. In the first two, we match transcription factor targets determined by\nChIP-seq profiling with differentially expressed genes after the same\ntranscription factor knockdown or over-expression in mammalian cells. In the\nthird case study, we evaluate the quality of enriched terms when comparing\nnormal epithelial cells with cancer stem cells. In conclusion, we demonstrate\nthat the Characteristic Direction approach is much better in calling the\nsignificantly differentially expressed genes and should replace the widely\ncurrently in used t-test method for this purpose. Implementations of the method\nin MATLAB, Python and Mathematica are available at:\nhttp://www.maayanlab.net/CD.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 16:01:21 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Clark", "Neil R.", ""], ["Hu", "Kevin", ""], ["Chen", "Edward Y.", ""], ["Duan", "Qioanan", ""], ["Ma`ayan", "Avi", ""]]}]