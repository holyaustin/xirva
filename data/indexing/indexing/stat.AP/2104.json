[{"id": "2104.00094", "submitter": "Benjamin Eltzner", "authors": "Henrik Wiechers, Benjamin Eltzner, Stephan F. Huckemann, Kanti V.\n  Mardia", "title": "Clustering Schemes on the Torus with Application to RNA Clashes", "comments": "8 pages, 4 figures, conference submission to GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular structures of RNA molecules reconstructed from X-ray\ncrystallography frequently contain errors. Motivated by this problem we examine\nclustering on a torus since RNA shapes can be described by dihedral angles. A\npreviously developed clustering method for torus data involves two tuning\nparameters and we assess clustering results for different parameter values in\nrelation to the problem of so-called RNA clashes. This clustering problem is\npart of the dynamically evolving field of statistics on manifolds. Statistical\nproblems on the torus highlight general challenges for statistics on manifolds.\nTherefore, the torus PCA and clustering methods we propose make an important\ncontribution to directional statistics and statistics on manifolds in general.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:25:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wiechers", "Henrik", ""], ["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan F.", ""], ["Mardia", "Kanti V.", ""]]}, {"id": "2104.00108", "submitter": "Jamie Yap", "authors": "Jamie Yap, John Dziak, Raju Maiti, Kevin Lynch, James R. McKay, Bibhas\n  Chakraborty, Inbal Nahum-Shani", "title": "Planning SMARTs: Sample size estimation for comparing dynamic treatment\n  regimens using longitudinal count outcomes with excess zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many health domains such as substance-use, outcomes are often counts with\nan excessive number of zeros (EZ) - count data having zero counts at a rate\nsignificantly higher than that expected of a standard count distribution (e.g.,\nPoisson). However, an important gap exists in sample size estimation\nmethodology for planning sequential multiple assignment randomized trials\n(SMARTs) for comparing dynamic treatment regimens (DTRs) using longitudinal\ncount data. DTRs, also known as treatment algorithms or adaptive interventions,\nmimic the individualized and evolving nature of patient care through the\nspecification of decision rules guiding the type, timing and modality of\ndelivery, and dosage of treatments to address the unique and changing needs of\nindividuals. To close this gap, we develop a Monte Carlo-based approach to\nsample size estimation. A SMART for engaging alcohol and cocaine-dependent\npatients in treatment is used as motivation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yap", "Jamie", ""], ["Dziak", "John", ""], ["Maiti", "Raju", ""], ["Lynch", "Kevin", ""], ["McKay", "James R.", ""], ["Chakraborty", "Bibhas", ""], ["Nahum-Shani", "Inbal", ""]]}, {"id": "2104.00262", "submitter": "Michael Grabinski", "authors": "Maike Torm\\\"ahlen, Galiya Klinkova and Michael Grabinski", "title": "Statistical significance revisited", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.20944/preprints202103.0398.v1", "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical significance measures the reliability of a result obtained from a\nrandom experiment. We investigate the number of repetitions needed for a\nstatistical result to have a certain significance. In the first step, we\nconsider binomially distributed variables in the example of medication testing\nwith fixed placebo efficacy, asking how many experiments are needed in order to\nachieve a significance of 95 %. In the next step, we take the probability\ndistribution of the placebo efficacy into account, which to the best of our\nknowledge has not been done so far. Depending on the specifics, we show that in\norder to obtain identical significance, it may be necessary to perform twice as\nmany experiments than in a setting where the placebo distribution is neglected.\nWe proceed by considering more general probability distributions and close with\ncomments on some erroneous assumptions on probability distributions which lead,\nfor instance, to a trivial explanation of the fat tail.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 05:38:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:35:20 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Torm\u00e4hlen", "Maike", ""], ["Klinkova", "Galiya", ""], ["Grabinski", "Michael", ""]]}, {"id": "2104.00333", "submitter": "Richard D. Gill", "authors": "Richard D. Gill", "title": "Repeated measurements with unintended feedback: The Dutch new herring\n  scandals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An econometric analysis of consumer research data which hit newspaper\nheadlines in the Netherlands illustrates almost everything that can go wrong\nwhen statistical models are fit to the superficial characteristics of a\ndata-set with no attention paid to the data generation mechanism. This paper is\ndedicated to Ornulf Borgan on the occasion of his virtual 65th birthday\ncelebrations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:31:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gill", "Richard D.", ""]]}, {"id": "2104.00454", "submitter": "Yi Zhao", "authors": "Yi Zhao, Bingkai Wang, Chin-Fu Liu, Andreia V. Faria, Michael I.\n  Miller, Brian S. Caffo, Xi Luo", "title": "Identifying brain hierarchical structures associated with Alzheimer's\n  disease using a regularized regression method with tree predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain segmentation at different levels is generally represented as\nhierarchical trees. Brain regional atrophy at specific levels was found to be\nmarginally associated with Alzheimer's disease outcomes. In this study, we\npropose an L1-type regularization for predictors that follow a hierarchical\ntree structure. Considering a tree as a directed acyclic graph, we interpret\nthe model parameters from a path analysis perspective. Under this concept, the\nproposed penalty regulates the total effect of each predictor on the outcome.\nWith regularity conditions, it is shown that under the proposed regularization,\nthe estimator of the model coefficient is consistent in L2-norm and the model\nselection is also consistent. By applying to a brain structural magnetic\nresonance imaging dataset acquired from the Alzheimer's Disease Neuroimaging\nInitiative, the proposed approach identifies brain regions where atrophy in\nthese regions demonstrates the declination in memory. With regularization on\nthe total effects, the findings suggest that the impact of atrophy on memory\ndeficits is localized from small brain regions but at various levels of brain\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:15:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhao", "Yi", ""], ["Wang", "Bingkai", ""], ["Liu", "Chin-Fu", ""], ["Faria", "Andreia V.", ""], ["Miller", "Michael I.", ""], ["Caffo", "Brian S.", ""], ["Luo", "Xi", ""]]}, {"id": "2104.00507", "submitter": "Przemyslaw Biecek", "authors": "Jakub Wi\\'sniewski, Przemys{\\l}aw Biecek", "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And\n  Mitigation", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning decision systems are getting omnipresent in our lives. From\ndating apps to rating loan seekers, algorithms affect both our well-being and\nfuture. Typically, however, these systems are not infallible. Moreover, complex\npredictive models are really eager to learn social biases present in historical\ndata that can lead to increasing discrimination. If we want to create models\nresponsibly then we need tools for in-depth validation of models also from the\nperspective of potential discrimination. This article introduces an R package\nfairmodels that helps to validate fairness and eliminate bias in classification\nmodels in an easy and flexible fashion. The fairmodels package offers a\nmodel-agnostic approach to bias detection, visualization and mitigation. The\nimplemented set of functions and fairness metrics enables model fairness\nvalidation from different perspectives. The package includes a series of\nmethods for bias mitigation that aim to diminish the discrimination in the\nmodel. The package is designed not only to examine a single model, but also to\nfacilitate comparisons between multiple models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:06:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wi\u015bniewski", "Jakub", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2104.00510", "submitter": "Karthik Bharath", "authors": "Shariq Mohammed, Karthik Bharath, Sebastian Kurtek, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "RADIOHEAD: Radiogenomic Analysis Incorporating Tumor Heterogeneity in\n  Imaging Through Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent technological advancements have enabled detailed investigation of\nassociations between the molecular architecture and tumor heterogeneity,\nthrough multi-source integration of radiological imaging and genomic\n(radiogenomic) data. In this paper, we integrate and harness radiogenomic data\nin patients with lower grade gliomas (LGG), a type of brain cancer, in order to\ndevelop a regression framework called RADIOHEAD (RADIOgenomic analysis\nincorporating tumor HEterogeneity in imAging through Densities) to identify\nradiogenomic associations. Imaging data is represented through voxel intensity\nprobability density functions of tumor sub-regions obtained from multimodal\nmagnetic resonance imaging, and genomic data through molecular signatures in\nthe form of pathway enrichment scores corresponding to their gene expression\nprofiles. Employing a Riemannian-geometric framework for principal component\nanalysis on the set of probability densities functions, we map each probability\ndensity to a vector of principal component scores, which are then included as\npredictors in a Bayesian regression model with the pathway enrichment scores as\nthe response. Variable selection compatible with the grouping structure amongst\nthe predictors induced through the tumor sub-regions is carried out under a\ngroup spike-and-slab prior. A Bayesian false discovery rate mechanism is then\nused to infer significant associations based on the posterior distribution of\nthe regression coefficients. Our analyses reveal several pathways relevant to\nLGG etiology (such as synaptic transmission, nerve impulse and neurotransmitter\npathways), to have significant associations with the corresponding\nimaging-based predictors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:08:13 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:37:40 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mohammed", "Shariq", ""], ["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2104.00530", "submitter": "Andrew Song", "authors": "Andrew H. Song, Bahareh Tolooshams, Demba Ba", "title": "Gaussian Process Convolutional Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional dictionary learning (CDL), the problem of estimating\nshift-invariant templates from data, is typically conducted in the absence of a\nprior/structure on the templates. In data-scarce or low signal-to-noise ratio\n(SNR) regimes, which have received little attention from the community, learned\ntemplates overfit the data and lack smoothness, which can affect the predictive\nperformance of downstream tasks. To address this limitation, we propose GPCDL,\na convolutional dictionary learning framework that enforces priors on templates\nusing Gaussian Processes (GPs). With the focus on smoothness, we show\ntheoretically that imposing a GP prior is equivalent to Wiener filtering the\nlearned templates, thereby suppressing high-frequency components and promoting\nsmoothness. We show that the algorithm is a simple extension of the classical\niteratively reweighted least squares, which allows the flexibility to\nexperiment with different smoothness assumptions. Through simulation, we show\nthat GPCDL learns smooth dictionaries with better accuracy than the\nunregularized alternative across a range of SNRs. Through an application to\nneural spiking data from rats, we show that learning templates by GPCDL results\nin a more accurate and visually-interpretable smooth dictionary, leading to\nsuperior predictive performance compared to non-regularized CDL, as well as\nparametric alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:40:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Song", "Andrew H.", ""], ["Tolooshams", "Bahareh", ""], ["Ba", "Demba", ""]]}, {"id": "2104.00546", "submitter": "Rob Johnson", "authors": "Rob Johnson, Chris Jackson, Anne Presanis, Sofia S. Villar, Daniela De\n  Angelis", "title": "Quantifying efficiency gains of innovative designs of two-arm vaccine\n  trials for COVID-19 using an epidemic simulation model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical trials of a vaccine during an epidemic face particular challenges,\nsuch as the pressure to identify an effective vaccine quickly to control the\nepidemic, and the effect that time-space-varying infection incidence has on the\npower of a trial. We illustrate how the operating characteristics of different\ntrial design elements may be evaluated using a network epidemic and trial\nsimulation model, based on COVID-19 and individually randomised two-arm trials\nwith a binary outcome. We show that \"ring\" recruitment strategies, prioritising\nparticipants at high risk of infection, can result in substantial improvement\nin terms of power, if sufficiently many contacts of observed cases are at high\nrisk. In addition, we introduce a novel method to make more efficient use of\nthe data from the earliest cases of infection observed in the trial, whose\ninfection may have been too early to be vaccine-preventable. Finally, we\ncompare several methods of response-adaptive randomisation, discussing their\nadvantages and disadvantages in this two-arm context and identifying particular\nadaptation strategies that preserve power and estimation properties, while\nslightly reducing the number of infections, given an effective vaccine.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:59:03 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:26:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Johnson", "Rob", ""], ["Jackson", "Chris", ""], ["Presanis", "Anne", ""], ["Villar", "Sofia S.", ""], ["De Angelis", "Daniela", ""]]}, {"id": "2104.00571", "submitter": "Flora Karathanasi", "authors": "Takvor H. Soukissian, Flora E. Karathanasi, Dimitrios K. Zaragkas", "title": "Exploiting offshore wind and solar resources in the Mediterranean using\n  ERA5 reanalysis data", "comments": "63 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial electricity production from marine renewable sources is becoming a\nnecessity at a global scale. Offshore wind and solar resources can be combined\nto reduce construction and maintenance costs. In this respect, the aim of this\nstudy is two-fold: i) analyse offshore wind and solar resource and their\nvariability in the Mediterranean Sea at the annual and seasonal scales based on\nthe recently published ERA5 reanalysis dataset, and; ii) perform a preliminary\nassessment of some important features of complementarity, synergy, and\navailability of the examined resources using an event-based probabilistic\napproach. A robust coefficient of variation is introduced to examine the\nvariability of each resource and a joint coefficient of variation is\nimplemented for the first time to evaluate the joint variability of offshore\nwind and solar potential. The association between the resources is examined by\nintroducing a robust measure of correlation, along with the Pearson's r and\nKendall's tau correlation coefficient and the corresponding results are\ncompared. Several metrics are used to examine the degree of complementarity\naffected by variability and intermittency issues. Areas with high potential and\nlow variability for both resources include the Aegean and Alboran seas, while\nsignificant synergy (over 52%) is identified in the gulfs of Lion, Gabes and\nSidra, Aegean Sea and northern Cyprus Isl. The advantage of combining these two\nresources is highlighted at selected locations in terms of the monthly energy\nproduction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:53:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Soukissian", "Takvor H.", ""], ["Karathanasi", "Flora E.", ""], ["Zaragkas", "Dimitrios K.", ""]]}, {"id": "2104.01107", "submitter": "Felix Ambellan", "authors": "Felix Ambellan, Stefan Zachow, Christoph von Tycowicz", "title": "Geodesic B-Score for Improved Assessment of Knee Osteoarthritis", "comments": "To be published in: Proc. International Conference on Information\n  Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional medical imaging enables detailed understanding of\nosteoarthritis structural status. However, there remains a vast need for\nautomatic, thus, reader-independent measures that provide reliable assessment\nof subject-specific clinical outcomes. To this end, we derive a consistent\ngeneralization of the recently proposed B-score to Riemannian shape spaces. We\nfurther present an algorithmic treatment yielding simple, yet efficient\ncomputations allowing for analysis of large shape populations with several\nthousand samples. Our intrinsic formulation exhibits improved discrimination\nability over its Euclidean counterpart, which we demonstrate for predictive\nvalidity on assessing risks of total knee replacement. This result highlights\nthe potential of the geodesic B-score to enable improved personalized\nassessment and stratification for interventions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:16:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ambellan", "Felix", ""], ["Zachow", "Stefan", ""], ["von Tycowicz", "Christoph", ""]]}, {"id": "2104.01133", "submitter": "Jonathan Cardoso-Silva", "authors": "Pedro Henrique da Costa Avelar, Luis C. Lamb, Sophia Tsoka, Jonathan\n  Cardoso-Silva", "title": "Weekly Bayesian modelling strategy to predict deaths by COVID-19: a\n  model and case study for the state of Santa Catarina, Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: The novel coronavirus pandemic has affected Brazil's Santa\nCatarina State (SC) severely. At the time of writing (24 March 2021), over\n764,000 cases and over 9,800 deaths by COVID-19 have been confirmed, hospitals\nwere fully occupied with local news reporting at least 397 people in the\nwaiting list for an ICU bed. In an attempt to better inform local policy\nmaking, we applied an existing Bayesian algorithm to model the spread of the\npandemic in the seven geographic macro-regions of the state. Here we propose\nchanges to extend the model and improve its forecasting capabilities.\n  Methods: Our four proposed variations of the original method allow accessing\ndata of daily reported infections and take into account under-reporting of\ncases more explicitly. Two of the proposed versions also attempt to model the\ndelay in test reporting. We simulated weekly forecasting of deaths from the\nperiod from 31/05/2020 until 31/01/2021. First week data were used as a\ncold-start to the algorithm, after which weekly calibrations of the model were\nable to converge in fewer iterations. Google Mobility data were used as\ncovariates to the model, as well as to estimate of the susceptible population\nat each simulated run.\n  Findings: The changes made the model significantly less reactive and more\nrapid in adapting to scenarios after a peak in deaths is observed. Assuming\nthat the cases are under-reported greatly benefited the model in its stability,\nand modelling retroactively-added data (due to the \"hot\" nature of the data\nused) had a negligible impact in performance.\n  Interpretation: Although not as reliable as death statistics, case\nstatistics, when modelled in conjunction with an overestimate parameter,\nprovide a good alternative for improving the forecasting of models, especially\nin long-range predictions and after the peak of an infection wave.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:18:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Avelar", "Pedro Henrique da Costa", ""], ["Lamb", "Luis C.", ""], ["Tsoka", "Sophia", ""], ["Cardoso-Silva", "Jonathan", ""]]}, {"id": "2104.01140", "submitter": "Giulio Giacomo Cantone", "authors": "Venera Tomaselli, Giulio Giacomo Cantone, Valeria Mazzeo", "title": "The polarising effect of Review Bomb", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study discusses the Review Bomb, a phenomenon consisting of a massive\nattack by groups of Internet users on a website that displays users' review on\nproducts. It gained attention, especially on websites that aggregate numerical\nratings. Although this phenomenon can be considered an example of online\nmisinformation, it differs from conventional spam review, which happens within\nlarger time spans. In particular, the Bomb occurs suddenly and for a short\ntime, because in this way it leverages the notorious problem of cold-start: if\nreviews are submitted by a lot of fresh new accounts, it makes hard to justify\npreventative measures. The present research work is focused on the case of The\nLast of Us Part II, a video game published by Sony, that was the target of the\nwidest phenomenon of Review Bomb, occurred in June 2020. By performing an\nobservational analysis of a linguistic corpus of English reviews and the\nfeatures of its users, this study confirms that the Bomb was an ideological\nattack aimed at breaking down the rating system of the platform Metacritic.\nEvidence supports that the bombing had the unintended consequence to induce a\nreaction from users, ending into a consistent polarisation of ratings towards\nextreme values. The results not only display the theory of polarity in online\nreviews, but them also provide insights for the research on the problem of\ncold-start detection of spam review. In particular, it illustrates the\nrelevance of detecting users discussing contextual elements instead of the\nproduct and users with anomalous features.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:39:23 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Tomaselli", "Venera", ""], ["Cantone", "Giulio Giacomo", ""], ["Mazzeo", "Valeria", ""]]}, {"id": "2104.01165", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena and Alex Petersen", "title": "Distributional data analysis with accelerometer data in a NHANES\n  database with nonparametric survey regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerometers enable an objective measurement of physical activity levels\namong groups of individuals in free-living environments, providing\nhigh-resolution detail about physical activity changes at different time\nscales. Current approaches used in the literature for analyzing such data\ntypically employ summary measures such as total inactivity time or\ncompositional metrics. However, at the conceptual level, these methods have the\npotential disadvantage of discarding important information from recorded data\nwhen calculating these summaries and metrics since these typically depend on\ncut-offs related to intensity exercise zones that are chosen subjectively or\neven arbitrarily. Much of the data collected in these studies follow complex\nsurvey designs, making application of standard statistical tools such as\nnon-parametric regression models inappropriate and the requirement of specific\nestimation procedures according to particular sampling-design is mandatory.\nWith functional data or other complex objects, barely literature exist that\nhandles complex sampling designs in the statistical analysis. This paper aims\ntwo-fold; first, we introduce a new functional representation of accelerometer\ndata of a distributional nature to build a complete individualized profile of\neach subject's physical activity levels. Second, using the NHANES accelerometer\ndata (2003-2006), we show the potential advantages of this new representation\nto predict patients' outcomes over $68$ years of age. A critical component in\nour statistical modeling is that we extend non-parametric functional models\nused: kernel smoother and kernel ridge regression, to handle the specific\neffect of complex sampling design in order to provide reliable conclusions\nabout the influence of physical activity in distinct analysis performed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 17:30:39 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Matabuena", "Marcos", ""], ["Petersen", "Alex", ""]]}, {"id": "2104.01344", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "A COVINDEX based on a GAM beta regression model with an application to\n  the COVID-19 pandemic in Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting changes in COVID-19 disease transmission over time is a key\nindicator of epidemic growth. Near real-time monitoring of the pandemic growth\nis crucial for policy makers and public health officials who need to make\ninformed decisions about whether to enforce lockdowns or allow certain\nactivities. The effective reproduction number Rt is the standard index used in\nmany countries for this goal. However, it is known that due to the delays\nbetween infection and case registration, its use for decision making is\nsomewhat limited. In this paper a near real-time COVINDEX is proposed for\nmonitoring the evolution of the pandemic. The index is computed from\npredictions obtained from a GAM beta regression for modelling the test positive\nrate as a function of time. The proposal is illustrated using data on COVID-19\npandemic in Italy and compared with Rt. A simple chart is also proposed for\nmonitoring local and national outbreaks by policy makers and public health\nofficials.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 08:53:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "2104.01346", "submitter": "Ruth Heller", "authors": "Ruth Heller and Abba Krieger and Saharon Rosset", "title": "Optimal multiple testing and design in clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A central goal in designing clinical trials is to find the test that\nmaximizes power (or equivalently minimizes required sample size) for finding a\ntrue research hypothesis subject to the constraint of type I error. When there\nis more than one test, such as in clinical trials with multiple endpoints, the\nissues of optimal design and optimal policies become more complex. In this\npaper we address the question of how such optimal tests should be defined and\nhow they can be found. We review different notions of power and how they relate\nto study goals, and also consider the requirements of type I error control and\nthe nature of the policies. This leads us to formulate the optimal policy\nproblem as an explicit optimization problem with objective and constraints\nwhich describe its specific desiderata. We describe a complete solution for\nderiving optimal policies for two hypotheses, which have desired monotonicity\nproperties, and are computationally simple. For some of the optimization\nformulations this yields optimal policies that are identical to existing\npolicies, such as Hommel's procedure or the procedure of Bittman et al. (2009),\nwhile for others it yields completely novel and more powerful policies than\nexisting ones. We demonstrate the nature of our novel policies and their\nimproved power extensively in simulation and on the APEX study (Cohen et al.,\n2016).\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 08:54:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heller", "Ruth", ""], ["Krieger", "Abba", ""], ["Rosset", "Saharon", ""]]}, {"id": "2104.01573", "submitter": "Maliheh Heidari", "authors": "Maliheh Heidari, Md Abu Manju, Pieta C.IJzerman-Boon and Edwin R. van\n  den Heuvel", "title": "D-optimal designs for the Mitscherlich non-linear regression function", "comments": "23 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitscherlich's function is a well-known three-parameter non-linear regression\nfunction that quantifies the relation between a stimulus or a time variable and\na response. Optimal designs for this function have been constructed only for\nnormally distributed responses with homoscedastic variances. In this paper, we\nconstruct D-optimal designs for discrete and continuous responses having their\ndistribution function in the exponential family. We also demonstrate the\nconnection with D-optimality for weighted linear regression.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:34:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heidari", "Maliheh", ""], ["Manju", "Md Abu", ""], ["IJzerman-Boon", "Pieta C.", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2104.01650", "submitter": "Adway Mitra", "authors": "Gaurav Suryawanshi, Varun Madhavan, Adway Mitra, Partha Pratim\n  Chakrabarti", "title": "City-scale Simulation of Covid-19 Pandemic and Intervention Policies\n  using Agent-based Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the Covid-19 pandemic, most governments across the world imposed\npolicies like lock-down of public spaces and restrictions on people's movements\nto minimize the spread of the virus through physical contact. However, such\npolicies have grave social and economic costs, and so it is important to\npre-assess their impacts. In this work we aim to visualize the dynamics of the\npandemic in a city under different intervention policies, by simulating the\nbehavior of the residents. We develop a very detailed agent-based model for a\ncity, including its residents, physical and social spaces like homes,\nmarketplaces, workplaces, schools/colleges etc. We parameterize our model for\nKolkata city in India using ward-level demographic and civic data. We\ndemonstrate that under appropriate choice of parameters, our model is able to\nreproduce the observed dynamics of the Covid-19 pandemic in Kolkata, and also\nindicate the counter-factual outcomes of alternative intervention policies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 17:30:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Suryawanshi", "Gaurav", ""], ["Madhavan", "Varun", ""], ["Mitra", "Adway", ""], ["Chakrabarti", "Partha Pratim", ""]]}, {"id": "2104.01679", "submitter": "Elham Bayat Mokhtari", "authors": "Elham Bayat Mokhtari, Benjamin Ridenhour", "title": "Filtering ASVs/OTUs via Mutual Information-Based Microbiome Network\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial communities are widely studied using high-throughput sequencing\ntechniques, such as 16S rRNA gene sequencing. These techniques have attracted\nbiologists as they offer powerful tools to explore microbial communities and\ninvestigate their patterns of diversity in biological and biomedical samples at\nremarkable resolution. However, the accuracy of these methods can negatively\naffected by the presence of contamination. Several studies have recognized that\ncontamination is a common problem in microbial studies and have offered\npromising computational and laboratory-based approaches to assess and remove\ncontaminants. Here we propose a novel strategy, MI-based (mutual information\nbased) filtering method, which uses information theoretic functionals and graph\ntheory to identify and remove contaminants. We applied MI-based filtering\nmethod to a mock community data set and evaluated the amount of information\nloss due to filtering taxa. We also compared our method to commonly practice\ntraditional filtering methods. In a mock community data set, MI-based filtering\napproach maintained the true bacteria in the community without significant loss\nof information. Our results indicate that MI-based filtering method effectively\nidentifies and removes contaminants in microbial communities and hence it can\nbe beneficial as a filtering method to microbiome studies. We believe our\nfiltering method has two advantages over traditional filtering methods. First,\nit does not required an arbitrary choice of threshold and second, it is able to\ndetect true taxa with low abundance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:54:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mokhtari", "Elham Bayat", ""], ["Ridenhour", "Benjamin", ""]]}, {"id": "2104.01822", "submitter": "Solon Karapanagiotis", "authors": "Solon Karapanagiotis, Umberto Benedetto, Sach Mukherjee, Paul D. W.\n  Kirk, Paul J. Newcombe", "title": "Tailored Bayes: a risk modelling framework under unequal\n  misclassification costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Risk prediction models are a crucial tool in healthcare. Risk prediction\nmodels with a binary outcome (i.e., binary classification models) are often\nconstructed using methodology which assumes the costs of different\nclassification errors are equal. In many healthcare applications this\nassumption is not valid, and the differences between misclassification costs\ncan be quite large. For instance, in a diagnostic setting, the cost of\nmisdiagnosing a person with a life-threatening disease as healthy may be larger\nthan the cost of misdiagnosing a healthy person as a patient. In this work, we\npresent Tailored Bayes (TB), a novel Bayesian inference framework which\n\"tailors\" model fitting to optimise predictive performance with respect to\nunbalanced misclassification costs. We use simulation studies to showcase when\nTB is expected to outperform standard Bayesian methods in the context of\nlogistic regression. We then apply TB to three real-world applications, a\ncardiac surgery, a breast cancer prognostication task and a breast cancer\ntumour classification task, and demonstrate the improvement in predictive\nperformance over standard methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 09:37:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Karapanagiotis", "Solon", ""], ["Benedetto", "Umberto", ""], ["Mukherjee", "Sach", ""], ["Kirk", "Paul D. W.", ""], ["Newcombe", "Paul J.", ""]]}, {"id": "2104.01869", "submitter": "Luciana Dalla Valle PhD", "authors": "Lauren Ansell and Luciana Dalla Valle", "title": "Social Media Integration of Flood Data: A Vine Copula-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floods are the most common and among the most severe natural disasters in\nmany countries around the world. As global warming continues to exacerbate sea\nlevel rise and extreme weather, governmental authorities and environmental\nagencies are facing the pressing need of timely and accurate evaluations and\npredictions of flood risks. Current flood forecasts are generally based on\nhistorical measurements of environmental variables at monitoring stations. In\nrecent years, in addition to traditional data sources, large amounts of\ninformation related to floods have been made available via social media.\nMembers of the public are constantly and promptly posting information and\nupdates on local environmental phenomena on social media platforms. Despite the\ngrowing interest of scholars towards the usage of online data during natural\ndisasters, the majority of studies focus exclusively on social media as a\nstand-alone data source, while its joint use with other type of information is\nstill unexplored. In this paper we propose to fill this gap by integrating\ntraditional historical information on floods with data extracted by Twitter and\nGoogle Trends. Our methodology is based on vine copulas, that allow us to\ncapture the dependence structure among the marginals, which are modelled via\nappropriate time series methods, in a very flexible way. We apply our\nmethodology to data related to three different coastal locations in the South\ncost of the UK. The results show that our approach, based on the integration of\nsocial media data, outperforms traditional methods, providing a more accurate\nevaluation and prediction of flood events.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:16:43 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ansell", "Lauren", ""], ["Valle", "Luciana Dalla", ""]]}, {"id": "2104.01902", "submitter": "Kendal Foster", "authors": "Kendal Foster (1) and Henrik Singmann (2 and 1) ((1) University of\n  Warwick, (2) University College London)", "title": "Another Approximation of the First-Passage Time Densities for the\n  Ratcliff Diffusion Decision Model", "comments": "51 pages, 10 figures. Source code can be found at\n  https://github.com/rtdists/fddm with supplemental materials in the folder\n  $\\texttt{paper_analysis}$. Submitted to The Journal of Mathematical\n  Psychology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for approximating the probability density function\n(PDF) of the first-passage times in the Ratcliff diffusion decision model\n(DDM). We implemented this approximation method in $\\texttt{C++}$ using the\n$\\texttt{R}$ package $\\texttt{Rcpp}$ to utilize the faster $\\texttt{C++}$\nlanguage while maintaining the $\\texttt{R}$ language interface. In addition to\nour novel approximation method, we also compiled all known approximation\nmethods for the DDM density function (with fixed and variable drift rate),\nincluding previously unused combinations of techniques found in the relevant\nliterature. We ported these approximation methods to $\\texttt{C++}$ and\noptimized them to run in this new language. Given an acceptable error tolerance\nin the value of the PDF approximation, we benchmarked all of these\napproximation methods to compare their speed against each other and also\nagainst commonly used $\\texttt{R}$ functions from the literature. The results\nof these tests show that our novel approximation method is not only orders of\nmagnitude faster than the current standards, but it is also faster than all of\nthe other approximation methods available even after translation and\noptimization to the faster $\\texttt{C++}$ language. All of these approximation\nmethods are bundled in the $\\texttt{fddm}$ package for the $\\texttt{R}$\nstatistical computing language; this package is available via CRAN, and the\nsource code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:29:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Foster", "Kendal", "", "2 and 1"], ["Singmann", "Henrik", "", "2 and 1"]]}, {"id": "2104.01944", "submitter": "Yuyang Xu", "authors": "Yuyang Xu, Zhonghua Liu, Jianfeng Yao", "title": "ERStruct: An Eigenvalue Ratio Approach to Inferring Population Structure\n  from Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of population structure from genetic data plays an important role\nin population and medical genetics studies. The traditional EIGENSTRAT method\nhas been widely used for computing and selecting top principal components that\ncapture population structure information (Price et al., 2006). With the\nadvancement and decreasing cost of sequencing technology, whole-genome\nsequencing data provide much richer information about the underlying population\nstructures. However, the EIGENSTRAT method was originally developed for\nanalyzing array-based genotype data and thus may not perform well on sequencing\ndata for two reasons. First, the number of genetic variants $p$ is much larger\nthan the sample size $n$ in sequencing data such that the sample-to-marker\nratio $n/p$ is nearly zero, violating the assumption of the Tracy-Widom test\nused in the EIGENSTRAT method. Second, the EIGENSTRAT method might not be able\nto handle the linkage disequilibrium (LD) well in sequencing data. To resolve\nthose two critical issues, we propose a new statistical method called ERStruct\nto estimate the number of latent sub-populations based on sequencing data. We\npropose to use the ratio of successive eigenvalues as a more robust testing\nstatistic, and then we approximate the null distribution of our proposed test\nstatistic using modern random matrix theory. Simulation studies found that our\nproposed ERStruct method has outperformed the traditional Tracy-Widom test on\nsequencing data. We further use two public data sets from the HapMap 3 and the\n1000 Genomes Projects to demonstrate the performance of our ERStruct method. We\nalso implement our ERStruct in a MATLAB toolbox which is now publicly available\non GitHub through https://github.com/bglvly/ERStruct.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:04:37 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:18:05 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 13:54:23 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 15:02:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Xu", "Yuyang", ""], ["Liu", "Zhonghua", ""], ["Yao", "Jianfeng", ""]]}, {"id": "2104.02056", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Yang Weng, Chin-woo Tan, Ram Rajagopal", "title": "Quick Line Outage Identification in Urban Distribution Grids via Smart\n  Meters", "comments": "12 pages, 12 figures. arXiv admin note: substantial text overlap with\n  arXiv:1811.05646", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing integration of distributed energy resources (DERs) in\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration in distribution grids,\ntraditional outage detection methods, which rely on customers report and smart\nmeters' last gasp signals, will have poor performance, because the renewable\ngenerators and storages and the mesh structure in urban distribution grids can\ncontinue supplying power after line outages. To address these challenges, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis with a theoretical guarantee. Specifically, we prove via power\nflow analysis that the dependency of time-series voltage measurements exhibits\nsignificant statistical changes after line outages. This makes the theory on\noptimal change-point detection suitable to identify line outages. However,\nexisting change point detection methods require post-outage voltage\ndistribution, which is unknown in distribution systems. Therefore, we design a\nmaximum likelihood estimator to directly learn the distribution parameters from\nvoltage data. We prove that the estimated parameters-based detection also\nachieves the optimal performance, making it extremely useful for fast\ndistribution grid outage identifications. Furthermore, since smart meters have\nbeen widely installed in distribution grids and advanced infrastructure (e.g.,\nPMU) has not widely been available, our approach only requires voltage\nmagnitude for quick outage identification. Simulation results show highly\naccurate outage identification in eight distribution grids with 14\nconfigurations with and without DERs using smart meter data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:10:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liao", "Yizheng", ""], ["Weng", "Yang", ""], ["Tan", "Chin-woo", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2104.02126", "submitter": "Qingyan Xiang", "authors": "Qingyan Xiang (1), Ronald J. Bosch (2), Judith J. Lok (3) ((1)\n  Department of Biostatistics, Boston University, (2) Center for Biostatistics\n  in AIDS Research, Harvard T.H. Chan School of Public Health, (3) Department\n  of Mathematics and Statistics, Boston University)", "title": "The survival-incorporated median versus the median in the survivors or\n  in the always-survivors: What are we measuring? And why?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many clinical studies evaluate the benefit of treatment based on both\nsurvival and other ordinal/continuous clinical outcomes, such as neurocognitive\nscores or quality-of-life scores. In these studies, there are situations when\nthe clinical outcomes are truncated by death, where subjects die before their\nclinical outcome is measured. Treating outcomes as \"missing\" or \"censored\" due\nto death can be misleading for treatment effect evaluation. We show that if we\nuse the median in the survivors or in the always-survivors to summarize\nclinical outcomes, we may conclude a trade-off exists between the probability\nof survival and good clinical outcomes, even in settings where both the\nprobability of survival and the probability of any good clinical outcome are\nbetter for one treatment. Therefore, we advocate not always treating death as a\nmechanism through which clinical outcomes are missing, but rather as part of\nthe outcome measure. To account for the survival status, we describe the\nsurvival-incorporated median as an alternative summary measure for outcomes in\nthe presence of death. The survival-incorporated median is the threshold such\nthat 50\\% of the population is alive with an outcome above that threshold. We\nuse conceptual examples to show that the survival-incorporated median provides\na simple and useful summary measure to inform clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:50:17 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 19:44:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xiang", "Qingyan", ""], ["Bosch", "Ronald J.", ""], ["Lok", "Judith J.", ""]]}, {"id": "2104.02237", "submitter": "Alan Mishler", "authors": "Alan Mishler, Rebecca Nugent", "title": "Clustering Students and Inferring Skill Set Profiles with Skill\n  Hierarchies", "comments": "4 pages, 3 figures. Originally presented at the Doctoral Consortium\n  of the 11th International Conference on Educational Data Mining, July, 2018,\n  Buffalo, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cognitive diagnosis models (CDMs) are a popular tool for assessing students'\nmastery of sets of skills. Given a set of $K$ skills tested on an assessment,\nstudents are classified into one of $2^K$ latent skill set profiles that\nrepresent whether they have mastered each skill or not. Traditional approaches\nto estimating these profiles are computationally intensive and become\ninfeasible on large datasets. Instead, proxy skill estimates can be generated\nfrom the observed responses and then clustered, and these clusters can be\nassigned to different profiles. Building on previous work, we consider how to\noptimally perform this clustering when not all $2^K$ profiles are possible,\ne.g. because of hierarchical relationships among the skills, and when not all\npossible profiles are present in the population. We compare hierarchical\nclustering and several k-means variants, including semisupervised clustering\nusing simulated student responses. The empty k-means algorithm paired with a\nnovel method for generating starting centers yields the best overall\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:00:48 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mishler", "Alan", ""], ["Nugent", "Rebecca", ""]]}, {"id": "2104.02289", "submitter": "Prateek Bansal", "authors": "Krishna Murthy Gurumurthy, Zili Li, Kara M. Kockelman, and Prateek\n  Bansal", "title": "Modelling Animal-Vehicle Collision Counts across Large Networks Using a\n  Bayesian Hierarchical Model with Time-Varying Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal-vehicle collisions (AVCs) are common around the world and result in\nconsiderable loss of animal and human life, as well as significant property\ndamage and regular insurance claims. Understanding their occurrence in relation\nto various contributing factors and being able to identify locations of high\nrisk are valuable to AVC prevention, yielding economic, social and\nenvironmental cost savings. However, many challenges exist in the study of AVC\ndatasets. These include seasonality of animal activity, unknown exposure (i.e.,\nthe number of animal crossings), very low AVC counts across most sections of\nextensive roadway networks, and computational burdens that come with discrete\nresponse analysis using large datasets. To overcome these challenges, a\nBayesian hierarchical model is proposed where the exposure is modeled with\nnonparametric Dirichlet process, and the number of segment-level AVCs is\nassumed to follow a Binomial distribution. A P\\'olya-Gamma augmented Gibbs\nsampler is derived to estimate the proposed model. By using the AVC data of\nmultiple years across about 100,000 segments of state-controlled highways in\nTexas, U.S., it is demonstrated that the model is scalable to large datasets,\nwith a preponderance of zeros and clear monthly seasonality in counts, while\nidentifying high-risk locations (for application of design treatments, like\nseparated animal crossings with fencing) and key explanatory factors based on\nsegment-specific factors (such as changes in speed limit) can be done within\nthe modelling framework, which provide useful information for policy-making\npurposes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:05:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 02:53:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gurumurthy", "Krishna Murthy", ""], ["Li", "Zili", ""], ["Kockelman", "Kara M.", ""], ["Bansal", "Prateek", ""]]}, {"id": "2104.02383", "submitter": "Augustin Kelava", "authors": "Augustin Kelava, Pascal Kilian, Judith Glaesser, Samuel Merk, Holger\n  Brandt", "title": "Forecasting intra-individual changes of affective states taking into\n  account inter-individual differences using intensive longitudinal data from a\n  university student drop out study in math", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The longitudinal process that leads to university student drop out in STEM\nsubjects can be described by referring to a) inter-individual differences\n(e.g., cognitive abilities) as well as b) intra-individual changes (e.g.,\naffective states), c) (unobserved) heterogeneity of trajectories, and d)\ntime-dependent variables. Large dynamic latent variable model frameworks for\nintensive longitudinal data (ILD) have been proposed which are (partially)\ncapable of simultaneously separating the complex data structures (e.g., DLCA;\nAsparouhov, Hamaker, & Muth\\'en, 2017; DSEM; Asparouhov, Hamaker, & Muth\\'en,\n2018; NDLC-SEM, Kelava & Brandt, 2019). From a methodological perspective,\nforecasting in dynamic frameworks allowing for real-time inferences on latent\nor observed variables based on ongoing data collection has not been an\nextensive research topic. From a practical perspective, there has been no\nempirical study on student drop out in math that integrates ILD, dynamic\nframeworks, and forecasting of critical states of the individuals allowing for\nreal-time interventions. In this paper, we show how Bayesian forecasting of\nmultivariate intra-individual variables and time-dependent class membership of\nindividuals (affective states) can be performed in these dynamic frameworks. To\nillustrate our approach, we use an empirical example where we apply forecasting\nmethodology to ILD from a large university student drop out study in math with\nmultivariate observations collected over 50 measurement occasions from multiple\nstudents (N = 122). More specifically, we forecast emotions and behavior\nrelated to drop out. This allows us to model (i) just-in-time interventions,\n(ii) detection of heterogeneity in trajectories, and (iii) prediction of\nemerging dynamic states (e.g. critical stress levels or pre-decisional states).\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:19:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kelava", "Augustin", ""], ["Kilian", "Pascal", ""], ["Glaesser", "Judith", ""], ["Merk", "Samuel", ""], ["Brandt", "Holger", ""]]}, {"id": "2104.02769", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu and Jung-Yi Joyce Lin and Jiayi Ji", "title": "Variable selection with missing data in both covariates and outcomes:\n  Imputation and machine learning", "comments": "29 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The missing data issue is ubiquitous in health studies. Variable selection in\nthe presence of both missing covariates and outcomes is an important\nstatistical research topic but has been less studied. Existing literature\nfocuses on parametric regression techniques that provide direct parameter\nestimates of the regression model. Flexible nonparametric machine learning\nmethods considerably mitigate the reliance on the parametric assumptions, but\ndo not provide as naturally defined variable importance measure as the\ncovariate effect native to parametric models. We investigate a general variable\nselection approach when both the covariates and outcomes can be missing at\nrandom and have general missing data patterns. This approach exploits the\nflexibility of machine learning modeling techniques and bootstrap imputation,\nwhich is amenable to nonparametric methods in which the covariate effects are\nnot directly available. We conduct expansive simulations investigating the\npractical operating characteristics of the proposed variable selection\napproach, when combined with four tree-based machine learning methods, XGBoost,\nRandom Forests, Bayesian Additive Regression Trees (BART) and Conditional\nRandom Forests, and two commonly used parametric methods, lasso and backward\nstepwise selection. Numeric results suggest that when combined with bootstrap\nimputation, XGBoost and BART have the overall best variable selection\nperformance with respect to the $F_1$ score and Type I error across various\nsettings. In general, there is no significant difference in the variable\nselection performance due to imputation methods. We further demonstrate the\nmethods via a case study of risk factors for 3-year incidence of metabolic\nsyndrome with data from the Study of Women's Health Across the Nation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:18:29 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 21:02:21 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hu", "Liangyuan", ""], ["Lin", "Jung-Yi Joyce", ""], ["Ji", "Jiayi", ""]]}, {"id": "2104.02785", "submitter": "Sara Zahedian", "authors": "Sara Zahedian, Kaveh Farokhi Sadabadi, Amir Nohekhan", "title": "Localization of Autonomous Vehicles: Proof of Concept for A Computer\n  Vision Approach", "comments": "2019 ITS America Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a visual-based localization method for autonomous\nvehicles (AVs) that operate in the absence of any complicated hardware system\nbut a single camera. Visual localization refers to techniques that aim to find\nthe location of an object based on visual information of its surrounding area.\nThe problem of localization has been of interest for many years. However,\nvisual localization is a relatively new subject in the literature of\ntransportation. Moreover, the inevitable application of this type of\nlocalization in the context of autonomous vehicles demands special attention\nfrom the transportation community to this problem. This study proposes a\ntwo-step localization method that requires a database of geotagged images and a\ncamera mounted on a vehicle that can take pictures while the car is moving. The\nfirst step which is image retrieval uses SIFT local feature descriptor to find\nan initial location for the vehicle using image matching. The next step is to\nutilize the Kalman filter to estimate a more accurate location for the vehicle\nas it is moving. All stages of the introduced method are implemented as a\ncomplete system using different Python libraries. The proposed system is tested\non the KITTI dataset and has shown an average accuracy of 2 meters in finding\nthe final location of the vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:09:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zahedian", "Sara", ""], ["Sadabadi", "Kaveh Farokhi", ""], ["Nohekhan", "Amir", ""]]}, {"id": "2104.02924", "submitter": "Rishideep Roy", "authors": "Soudeep Deb, Rishideep Roy, Shubhabrata Das", "title": "Modeling a sequence of multinomial data with randomly varying\n  probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sequence of variables having multinomial distribution with the\nnumber of trials corresponding to these variables being large and possibly\ndifferent. The multinomial probabilities of the categories are assumed to vary\nrandomly depending on batches. The proposed framework is interesting from the\nperspective of various applications in practice such as predicting the winner\nof an election, forecasting the market share of different brands etc. In this\nwork, first we derive sufficient conditions of asymptotic normality of the\nestimates of the multinomial cell probabilities, and corresponding suitable\ntransformations. Then, we consider a Bayesian setting to implement our model.\nWe consider hierarchical priors using multivariate normal and inverse Wishart\ndistributions, and establish the posterior consistency. Based on this result\nand following appropriate Gibbs sampling algorithms, we can infer about\naggregate data. The methodology is illustrated in detail with two real life\napplications, in the contexts of political election and sales forecasting.\nAdditional insights of effectiveness are also derived through a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:39:56 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Deb", "Soudeep", ""], ["Roy", "Rishideep", ""], ["Das", "Shubhabrata", ""]]}, {"id": "2104.03065", "submitter": "Marcelo Medeiros", "authors": "Marcelo C. Medeiros, Henrique F. Pires", "title": "The Proper Use of Google Trends in Forecasting Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is widely known that Google Trends have become one of the most popular\nfree tools used by forecasters both in academics and in the private and public\nsectors. There are many papers, from several different fields, concluding that\nGoogle Trends improve forecasts' accuracy. However, what seems to be widely\nunknown, is that each sample of Google search data is different from the other,\neven if you set the same search term, data and location. This means that it is\npossible to find arbitrary conclusions merely by chance. This paper aims to\nshow why and when it can become a problem and how to overcome this obstacle.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:33:51 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:15:57 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 13:09:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Medeiros", "Marcelo C.", ""], ["Pires", "Henrique F.", ""]]}, {"id": "2104.03067", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Quantifying incentive (in)compatibility: a case study from sports", "comments": "19 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incentive compatibility is usually considered a binary concept in the\nacademic literature. Our paper aims to present a method for quantifying the\nviolation of strategy-proofness in the case of sports tournaments, in\nparticular, through the example of the European Qualifiers for the 2022 FIFA\nWorld Cup. Even though that competition is known to be vulnerable to\nmanipulation since certain teams might be interested in the success of other\nteams, the extent of the problem has remained unexplored until now. Based on\ncomputer simulations, the strategic behaviour of the contestants is found to\nhave non-negligible sporting effects. While strategy-proofness cannot be\nguaranteed without fundamentally changing the tournament design, we can\nsubstantially mitigate this threat by adding a carefully chosen set of draw\nrestrictions, which offers a justifiable and transparent solution to improve\nfairness. Sports governing bodies are encouraged to take our findings into\naccount.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:34:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2104.03167", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross, Sonja Petrovi\\'c, Despina Stasi", "title": "Random graphs with node and block effects: models, goodness-of-fit\n  tests, and applications to biological networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many popular models from the networks literature can be viewed through a\ncommon lens. We describe it here and call the class of models log-linear ERGMs.\nIt includes degree-based models, stochastic blockmodels, and combinations of\nthese. Given the interest in combined node and block effects in network\nformation mechanisms, we introduce a general directed relative of the\ndegree-corrected stochastic blockmodel: an exponential family model we call\n$p_1$-SBM. It is a generalization of several well-known variants of the\nblockmodel.\n  We study the problem of testing model fit for the log-linear ERGM class.\n  The model fitting approach we take, through the use of quick estimation\nalgorithms borrowed from the contingency table literature and effective\nsampling methods rooted in graph theory and algebraic statistics, results in an\nexact test whose $p$-value can be approximated efficiently in networks of\nmoderate sizes.\n  We showcase the performance of the method on two data sets from biology: the\nconnectome of \\emph{C. elegans} and the interactome of \\emph{Arabidopsis\nthaliana}. These two networks, a neuronal network and a protein-protein\ninteraction network, have been popular examples in the network science\nliterature, but a model-based approach to studying them has been missing thus\nfar.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:56:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gross", "Elizabeth", ""], ["Petrovi\u0107", "Sonja", ""], ["Stasi", "Despina", ""]]}, {"id": "2104.03394", "submitter": "Osama Almalik", "authors": "Osama Almalik and Edwin R. van den Heuvel", "title": "A bivariate likelihood approach for estimation of a pooled continuous\n  effect size from a heteroscedastic meta-analysis study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The DerSimonian-Laird (DL) weighted average method has been widely used for\nestimation of a pooled effect size from an aggregated data meta-analysis study.\nIt is mainly criticized for its underestimation of the standard error of the\npooled effect size in the presence of heterogeneous study effect sizes. The\nuncertainty in the estimation of the between-study variance is not accounted\nfor in the calculation of this standard error. Due to this negative property,\nmany alternative estimation approaches have been proposed in literature. One\napproach was developed by Hardy and Thompson (HT), who implemented a profile\nlikelihood approach instead of the moment-based approach of DL. Others have\nfurther extended the likelihood approach and proposed higher-order likelihood\ninferences (e.g., Bartlett-type corrections). Likelihood-based methods better\naddress the uncertainty in estimating the between-study variance than the DL\nmethod, but all these methods assume that the within-study standard deviations\nare known and equal to the observed standard error of the study effect sizes.\nHere we treat the observed standard errors as estimators for the within-study\nvariability and we propose a bivariate likelihood approach that jointly\nestimates the pooled effect size, the between-study variance, and the\npotentially heteroscedastic within-study variances. We study the performance of\nthe proposed method by means of simulation, and compare it to DL, HT, and the\nhigher-order likelihood methods. Our proposed approach appears to be less\nsensitive to the number of studies, and less biased in case of\nheteroscedasticity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:01:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Almalik", "Osama", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2104.03395", "submitter": "Michel Montoril PhD", "authors": "Michel H. Montoril, Leandro T. Correia, Helio S. Migon", "title": "Dynamic Weights in Gaussian Mixture Models: A Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we consider a Gaussian mixture model where the mixture weight\nbehaves as an unknown function of time. To estimate the mixture weight\nfunction, we develop a Bayesian nonlinear dynamic approach for polynomial\nmodels. Two estimation methods that can be extended to other situations are\nconsidered. One of them, called here component-wise Metropolis-Hastings, is\nmore general and can be used for any situation where the observation and state\nequations are nonlinearly connected. The other method tends to be faster but\nmust be applied specifically to binary data (by using a probit link function).\nThis kind of Gaussian mixture model is capable of successfully capturing the\nfeatures of the data, as observed in numerical studies. It can be useful in\nstudies such as clustering, change-point and process control. We apply the\nproposed method an array Comparative Genomic Hybridization (aCGH) dataset from\nglioblastoma cancer studies, where we illustrate the ability of the new method\nto detect chromosome aberrations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:05:03 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 00:02:35 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 03:00:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Montoril", "Michel H.", ""], ["Correia", "Leandro T.", ""], ["Migon", "Helio S.", ""]]}, {"id": "2104.03545", "submitter": "Kevin Kuo", "authors": "Kevin Kuo and Ronald Richman", "title": "Embeddings and Attention in Predictive Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore in depth how categorical data can be processed with embeddings in\nthe context of claim severity modeling. We develop several models that range in\ncomplexity from simple neural networks to state-of-the-art attention based\narchitectures that utilize embeddings. We illustrate the utility of learned\nembeddings from neural networks as pretrained features in generalized linear\nmodels, and discuss methods for visualizing and interpreting embeddings.\nFinally, we explore how attention based models can contextually augment\nembeddings, leading to enhanced predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:54:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kuo", "Kevin", ""], ["Richman", "Ronald", ""]]}, {"id": "2104.03613", "submitter": "Luca Biggio", "authors": "Luca Biggio, Alexander Wieland, Manuel Arias Chao, Iason Kastanis,\n  Olga Fink", "title": "Uncertainty-aware Remaining Useful Life predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Remaining Useful Life (RUL) estimation is the problem of inferring how long a\ncertain industrial asset can be expected to operate within its defined\nspecifications. Deploying successful RUL prediction methods in real-life\napplications is a prerequisite for the design of intelligent maintenance\nstrategies with the potential of drastically reducing maintenance costs and\nmachine downtimes. In light of their superior performance in a wide range of\nengineering fields, Machine Learning (ML) algorithms are natural candidates to\ntackle the challenges involved in the design of intelligent maintenance\nsystems. In particular, given the potentially catastrophic consequences or\nsubstantial costs associated with maintenance decisions that are either too\nlate or too early, it is desirable that ML algorithms provide uncertainty\nestimates alongside their predictions. However, standard data-driven methods\nused for uncertainty estimation in RUL problems do not scale well to large\ndatasets or are not sufficiently expressive to model the high-dimensional\nmapping from raw sensor data to RUL estimates. In this work, we consider Deep\nGaussian Processes (DGPs) as possible solutions to the aforementioned\nlimitations. We perform a thorough evaluation and comparison of several\nvariants of DGPs applied to RUL predictions. The performance of the algorithms\nis evaluated on the N-CMAPSS (New Commercial Modular Aero-Propulsion System\nSimulation) dataset from NASA for aircraft engines. The results show that the\nproposed methods are able to provide very accurate RUL predictions along with\nsensible uncertainty estimates, providing more reliable solutions for\n(safety-critical) real-life industrial applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:50:44 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Biggio", "Luca", ""], ["Wieland", "Alexander", ""], ["Chao", "Manuel Arias", ""], ["Kastanis", "Iason", ""], ["Fink", "Olga", ""]]}, {"id": "2104.03671", "submitter": "Fran Llopis-Cardona", "authors": "Fran Llopis-Cardona, Carmen Armero and Gabriel Sanf\\'elix-Gimeno", "title": "Reflection on modern methods: competing risks versus multi-state models", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival competing risks models are very useful for studying the incidence of\ndiseases whose occurrence competes with other possible diseases or health\nconditions. These models perform properly when working with terminal events,\nsuch as death, that imply the conclusion of the corresponding study. But they\ndo not allow the treatment of scenarios with non-terminal competing events that\nmay occur sequentially. Multi-state models are complex survival models. They\nfocus on pathways defined by the temporal and sequential occurrence of numerous\nevents of interest and thus they are suitable for connecting competing\nnon-terminal events as well as to manage other survival scenarios with higher\ncomplexity. We discuss competing risks within the framework of multi-state\nmodels and clarify the usefulness of both models for analysing epidemiological\ndata. We highlight the power of multi-state models through a real-world study\nof recurrent hip fracture from Bayesian inferential methodology.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:39:22 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Llopis-Cardona", "Fran", ""], ["Armero", "Carmen", ""], ["Sanf\u00e9lix-Gimeno", "Gabriel", ""]]}, {"id": "2104.03735", "submitter": "Ashirwad Barnwal", "authors": "Ashirwad Barnwal, Pranamesh Chakraborty, Anuj Sharma, Luis\n  Riera-Garcia, Koray Ozcan, Sayedomidreza Davami, Soumik Sarkar, Matthew\n  Rizzo, and Jennifer Merickel", "title": "Sugar and Stops in Drivers with Insulin-Dependent Type 1 Diabetes", "comments": "16 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes is a major public health challenge worldwide. Abnormal physiology in\ndiabetes, particularly hypoglycemia, can cause driver impairments that affect\nsafe driving. While diabetes driver safety has been previously researched, few\nstudies link real-time physiologic changes in drivers with diabetes to\nobjective real-world driver safety, particularly at high-risk areas like\nintersections. To address this, we investigated the role of acute physiologic\nchanges in drivers with type 1 diabetes mellitus (T1DM) on safe stopping at\nstop intersections. 18 T1DM drivers (21-52 years, mean = 31.2 years) and 14\ncontrols (21-55 years, mean = 33.4 years) participated in a 4-week naturalistic\ndriving study. At induction, each participant's vehicle was fitted with a\ncamera and sensor system to collect driving data. Video was processed with\ncomputer vision algorithms detecting traffic elements. Stop intersections were\ngeolocated with clustering methods, state intersection databases, and manual\nreview. Videos showing driver stop intersection approaches were extracted and\nmanually reviewed to classify stopping behavior (full, rolling, and no stop)\nand intersection traffic characteristics. Mixed-effects logistic regression\nmodels determined how diabetes driver stopping safety (safe vs. unsafe stop)\nwas affected by 1) disease and 2) at-risk, acute physiology (hypo- and\nhyperglycemia). Diabetes drivers who were acutely hyperglycemic had 2.37\nincreased odds of unsafe stopping (95% CI: 1.26-4.47, p = 0.008) compared to\nthose with normal physiology. Acute hypoglycemia did not associate with unsafe\nstopping (p = 0.537), however the lower frequency of hypoglycemia (vs.\nhyperglycemia) warrants a larger sample of drivers to investigate this effect.\nCritically, presence of diabetes alone did not associate with unsafe stopping,\nunderscoring the need to evaluate driver physiology in licensing guidelines.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:39:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Barnwal", "Ashirwad", ""], ["Chakraborty", "Pranamesh", ""], ["Sharma", "Anuj", ""], ["Riera-Garcia", "Luis", ""], ["Ozcan", "Koray", ""], ["Davami", "Sayedomidreza", ""], ["Sarkar", "Soumik", ""], ["Rizzo", "Matthew", ""], ["Merickel", "Jennifer", ""]]}, {"id": "2104.03743", "submitter": "Wei Xing", "authors": "Wei W. Xing, Akeel A. Shah, Peng Wang, Shandian Zhe Qian Fu, and\n  Robert. M. Kirby", "title": "Residual Gaussian Process: A Tractable Nonparametric Bayesian Emulator\n  for Multi-fidelity Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenges in multi-fidelity modeling relate to accuracy, uncertainty\nestimation and high-dimensionality. A novel additive structure is introduced in\nwhich the highest fidelity solution is written as a sum of the lowest fidelity\nsolution and residuals between the solutions at successive fidelity levels,\nwith Gaussian process priors placed over the low fidelity solution and each of\nthe residuals. The resulting model is equipped with a closed-form solution for\nthe predictive posterior, making it applicable to advanced, high-dimensional\ntasks that require uncertainty estimation. Its advantages are demonstrated on\nunivariate benchmarks and on three challenging multivariate problems. It is\nshown how active learning can be used to enhance the model, especially with a\nlimited computational budget. Furthermore, error bounds are derived for the\nmean prediction in the univariate case.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:57:46 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Xing", "Wei W.", ""], ["Shah", "Akeel A.", ""], ["Wang", "Peng", ""], ["Fu", "Shandian Zhe Qian", ""], ["Kirby", "Robert. M.", ""]]}, {"id": "2104.03757", "submitter": "Livia Paranhos", "authors": "Livia Paranhos", "title": "Predicting Inflation with Neural Networks", "comments": "47 pages, 9 figures. References to other arXiv articles:\n  arXiv:2008.12477, arXiv:1502.03167", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper applies neural network models to forecast inflation. The use of a\nparticular recurrent neural network, the long-short term memory model, or LSTM,\nthat summarizes macroeconomic information into common components is a major\ncontribution of the paper. Results from an exercise with US data indicate that\nthe estimated neural nets usually present better forecasting performance than\nstandard benchmarks, especially at long horizons. The LSTM in particular is\nfound to outperform the traditional feed-forward network at long horizons,\nsuggesting an advantage of the recurrent model in capturing the long-term trend\nof inflation. This finding can be rationalized by the so called long memory of\nthe LSTM that incorporates relatively old information in the forecast as long\nas accuracy is improved, while economizing in the number of estimated\nparameters. Interestingly, the neural nets containing macroeconomic information\ncapture well the features of inflation during and after the Great Recession,\npossibly indicating a role for nonlinearities and macro information in this\nepisode. The estimated common components used in the forecast seem able to\ncapture the business cycle dynamics, as well as information on prices.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:19:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Paranhos", "Livia", ""]]}, {"id": "2104.03792", "submitter": "Ritwik Bhattacharya", "authors": "Ritwik Bhattacharya and Narayanaswamy Balakrishnan", "title": "A MCMC-type simple probabilistic approach for determining optimal\n  progressive censoring schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present here a simple probabilistic approach for determining an optimal\nprogressive censoring scheme by defining a probability structure on the set of\nfeasible solutions. Given an initial solution, the new updated solution is\ncomputed within the probabilistic structure. This approach will be especially\nuseful when the cardinality of the set of feasible solutions is large. The\nvalidation of the proposed approach is demonstrated by comparing the optimal\nscheme with these obtained by exhaustive numerical search.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:15:47 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bhattacharya", "Ritwik", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2104.04033", "submitter": "Curtis Storlie", "authors": "Curtis B. Storlie, Ricardo L. Rojas, Gabriel O. Demuth, Benjamin D.\n  Pollock, Patrick W. Johnson, Patrick M. Wilson, Ethan P. Heinzen, Hongfang\n  Liu, Rickey E. Carter, Sean C. Dowdy, Shannon M. Dunlay, Elizabeth B.\n  Habermann, Daryl J. Kor, Matthew R. Neville, Andrew H. Limper, Katherine H.\n  Noe, Mohamad Bydon, Pablo Moreno Franco, Priya Sampathkumar, Nilay D. Shah\n  and Henry H. Ting", "title": "A Hierarchical Bayesian Model for Stochastic Spatiotemporal SIR Modeling\n  and Prediction of COVID-19 Cases and Hospitalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most COVID-19 predictive modeling efforts use statistical or mathematical\nmodels to predict national- and state-level COVID-19 cases or deaths in the\nfuture. These approaches assume parameters such as reproduction time, test\npositivity rate, hospitalization rate, and social intervention effectiveness\n(masking, distancing, and mobility) are constant. However, the one certainty\nwith the COVID-19 pandemic is that these parameters change over time, as well\nas vary across counties and states. In fact, the rate of spread over region,\nhospitalization rate, hospital length of stay and mortality rate, the\nproportion of the population that is susceptible, test positivity rate, and\nsocial behaviors can all change significantly over time. Thus, the\nquantification of uncertainty becomes critical in making meaningful and\naccurate forecasts of the future. Bayesian approaches are a natural way to\nfully represent this uncertainty in mathematical models and have become\nparticularly popular in physics and engineering models. The explicit\nintegration time varying parameters and uncertainty quantification into a\nhierarchical Bayesian forecast model differentiates the Mayo COVID-19 model\nfrom other forecasting models. By accounting for all sources of uncertainty in\nboth parameter estimation as well as future trends with a Bayesian approach,\nthe Mayo COVID-19 model accurately forecasts future cases and hospitalizations,\nas well as the degree of uncertainty. This approach has been remarkably\naccurate and a linchpin in Mayo Clinic's response to managing the COVID-19\npandemic. The model accurately predicted timing and extent of the summer and\nfall surges at Mayo Clinic sites, allowing hospital leadership to manage\nresources effectively to provide a successful pandemic response. This model has\nalso proven to be very useful to the state of Minnesota to help guide difficult\npolicy decisions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:09:13 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Storlie", "Curtis B.", ""], ["Rojas", "Ricardo L.", ""], ["Demuth", "Gabriel O.", ""], ["Pollock", "Benjamin D.", ""], ["Johnson", "Patrick W.", ""], ["Wilson", "Patrick M.", ""], ["Heinzen", "Ethan P.", ""], ["Liu", "Hongfang", ""], ["Carter", "Rickey E.", ""], ["Dowdy", "Sean C.", ""], ["Dunlay", "Shannon M.", ""], ["Habermann", "Elizabeth B.", ""], ["Kor", "Daryl J.", ""], ["Neville", "Matthew R.", ""], ["Limper", "Andrew H.", ""], ["Noe", "Katherine H.", ""], ["Bydon", "Mohamad", ""], ["Franco", "Pablo Moreno", ""], ["Sampathkumar", "Priya", ""], ["Shah", "Nilay D.", ""], ["Ting", "Henry H.", ""]]}, {"id": "2104.04133", "submitter": "Dominic Robson", "authors": "Dominic T Robson, Andreas CW Baas, Alessia Annibale", "title": "A combined model of aggregation, fragmentation, and exchange processes:\n  insights from analytical calculations", "comments": "19 pages, 5 figures. To be published in Journal of Statistical\n  Mechanics: Theory and Experiment (JSTAT)", "journal-ref": "J. Stat. Mech. (2021) 053203", "doi": "10.1088/1742-5468/abfa1d", "report-no": null, "categories": "physics.soc-ph physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mean-field framework for the study of systems of interacting\nparticles sharing a conserved quantity. The work generalises and unites the\nexisting fields of asset-exchange models, often applied to socio-economic\nsystems, and aggregation-fragmentation models, typically used in modelling the\ndynamics of clusters. An initial model includes only two-body collisions, which\nis then extended to include many-body collisions and spontaneous fragmentation.\nWe derive self-consistency equations for the steady-state distribution, which\ncan be solved using a population dynamics algorithm, as well as a full solution\nfor the time evolution of the moments, corroborated with numerical simulations.\nThe generality of the model makes it applicable to many problems and allows for\nthe study of systems exhibiting more complex interactions that those typically\nconsidered. The work is relevant to the modelling of barchan dune fields in\nwhich interactions between the bedforms and spontaneous fragmentation due to\nchanges in the wind are thought to lead to size-selection. Our work could also\nbe applied in finding wealth distributions when agents can both combine assets\nas well as split into multiple subsidiaries.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:01:12 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Robson", "Dominic T", ""], ["Baas", "Andreas CW", ""], ["Annibale", "Alessia", ""]]}, {"id": "2104.04144", "submitter": "Alfredo Carrillo MSc", "authors": "Alfredo Carrillo, Luis F. Cant\\'u and Alejandro Noriega", "title": "Individual Explanations in Machine Learning Models: A Survey for\n  Practitioners", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the use of sophisticated statistical models that influence\ndecisions in domains of high societal relevance is on the rise. Although these\nmodels can often bring substantial improvements in the accuracy and efficiency\nof organizations, many governments, institutions, and companies are reluctant\nto their adoption as their output is often difficult to explain in\nhuman-interpretable ways. Hence, these models are often regarded as\nblack-boxes, in the sense that their internal mechanisms can be opaque to human\naudit. In real-world applications, particularly in domains where decisions can\nhave a sensitive impact--e.g., criminal justice, estimating credit scores,\ninsurance risk, health risks, etc.--model interpretability is desired.\nRecently, the academic literature has proposed a substantial amount of methods\nfor providing interpretable explanations to machine learning models. This\nsurvey reviews the most relevant and novel methods that form the\nstate-of-the-art for addressing the particular problem of explaining individual\ninstances in machine learning. It seeks to provide a succinct review that can\nguide data science and machine learning practitioners in the search for\nappropriate methods to their problem domain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:46:34 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 02:46:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carrillo", "Alfredo", ""], ["Cant\u00fa", "Luis F.", ""], ["Noriega", "Alejandro", ""]]}, {"id": "2104.04148", "submitter": "Alfredo Carrillo MSc", "authors": "Alfredo Carrillo, Luis F. Cant\\'u, Luis Tejerina and Alejandro Noriega", "title": "Individual Explanations in Machine Learning Models: A Case Study on\n  Poverty Estimation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning methods are being increasingly applied in sensitive societal\ncontexts, where decisions impact human lives. Hence it has become necessary to\nbuild capabilities for providing easily-interpretable explanations of models'\npredictions. Recently in academic literature, a vast number of explanations\nmethods have been proposed. Unfortunately, to our knowledge, little has been\ndocumented about the challenges machine learning practitioners most often face\nwhen applying them in real-world scenarios. For example, a typical procedure\nsuch as feature engineering can make some methodologies no longer applicable.\nThe present case study has two main objectives. First, to expose these\nchallenges and how they affect the use of relevant and novel explanations\nmethods. And second, to present a set of strategies that mitigate such\nchallenges, as faced when implementing explanation methods in a relevant\napplication domain -- poverty estimation and its use for prioritizing access to\nsocial policies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:54:58 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:06:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carrillo", "Alfredo", ""], ["Cant\u00fa", "Luis F.", ""], ["Tejerina", "Luis", ""], ["Noriega", "Alejandro", ""]]}, {"id": "2104.04157", "submitter": "Nicola Rennie", "authors": "Nicola Rennie, Catherine Cleophas, Adam M. Sykulski, Florian Dost", "title": "Detecting outlying demand in multi-leg bookings for transportation\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network effects complicate demand forecasting in general, and outlier\ndetection in particular. For example, in transportation networks, sudden\nincreases in demand for a specific destination will not only affect the legs\narriving at that destination, but also connected legs nearby in the network.\nNetwork effects are particularly relevant when transport service providers,\nsuch as railway or coach companies, offer many multi-leg itineraries. In this\npaper, we present a novel method for generating automated outlier alerts, to\nsupport analysts in adjusting demand forecasts accordingly for reliable\nplanning. To create such alerts, we propose a two-step method for detecting\noutlying demand from transportation network bookings. The first step clusters\nnetwork legs to appropriately partition and pool booking patterns. The second\nstep identifies outliers within each cluster to create a ranked alert list of\naffected legs. We show that this method outperforms analyses that independently\nconsider each leg in a network, especially in highly-connected networks where\nmost passengers book multi-leg itineraries. We illustrate the applicability on\nempirical data obtained from Deutsche Bahn and with a detailed simulation\nstudy. The latter demonstrates the robustness of the approach and quantifies\nthe potential revenue benefits of adjusting for outlying demand in networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 13:40:45 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Rennie", "Nicola", ""], ["Cleophas", "Catherine", ""], ["Sykulski", "Adam M.", ""], ["Dost", "Florian", ""]]}, {"id": "2104.04432", "submitter": "Yajuan Si", "authors": "Yajuan Si, Roderick J. A. Little, Ya Mo and Nell Sedransk", "title": "A Case Study of Nonresponse Bias Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonresponse bias is a widely prevalent problem for data collections. We\ndevelop a ten-step exemplar to guide nonresponse bias analysis (NRBA) in\ncross-sectional studies, and apply these steps to the Early Childhood\nLongitudinal Study, Kindergarten Class of 2010-11. A key step is the\nconstruction of indices of nonresponse bias based on proxy pattern-mixture\nmodels for survey variables of interest. A novel feature is to characterize the\nstrength of evidence about nonresponse bias contained in these indices, based\non the strength of relationship of the characteristics in the nonresponse\nadjustment with the key survey variables. Our NRBA incorporates missing at\nrandom and missing not at random mechanisms, and all analyses can be done\nstraightforwardly with standard statistical software.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:30:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Si", "Yajuan", ""], ["Little", "Roderick J. A.", ""], ["Mo", "Ya", ""], ["Sedransk", "Nell", ""]]}, {"id": "2104.04435", "submitter": "Yajuan Si", "authors": "Len Covello, Andrew Gelman, Yajuan Si and Siquan Wang", "title": "Routine Hospital-based SARS-CoV-2 Testing Outperforms State-based Data\n  in Predicting Clinical Burden", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the COVID-19 pandemic, government policy and healthcare\nimplementation responses have been guided by reported positivity rates and\ncounts of positive cases in the community. The selection bias of these data\ncalls into question their validity as measures of the actual viral incidence in\nthe community and as predictors of clinical burden. In the absence of any\nsuccessful public or academic campaign for comprehensive or random testing, we\nhave developed a proxy method for synthetic random sampling, based on viral RNA\ntesting of patients who present for elective procedures within a hospital\nsystem. We present here an approach under multilevel regression and\npoststratification (MRP) to collecting and analyzing data on viral exposure\namong patients in a hospital system and performing statistical adjustment that\nhas been made publicly available to estimate true viral incidence and trends in\nthe community. We apply our MRP method to track viral behavior in a mixed\nurban-suburban-rural setting in Indiana. This method can be easily implemented\nin a wide variety of hospital settings. Finally, we provide evidence that this\nmodel predicts the clinical burden of SARS-CoV-2 earlier and more accurately\nthan currently accepted metrics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:35:41 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Covello", "Len", ""], ["Gelman", "Andrew", ""], ["Si", "Yajuan", ""], ["Wang", "Siquan", ""]]}, {"id": "2104.04546", "submitter": "Maria A. Zuluaga", "authors": "Laura M. Ferrari, Guy Abi Hanna, Paolo Volpe, Esma Ismailova,\n  Fran\\c{c}ois Bremond, Maria A. Zuluaga", "title": "One-class Autoencoder Approach for Optimal Electrode Set-up\n  Identification in Wearable EEG Event Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A limiting factor towards the wide routine use of wearables devices for\ncontinuous healthcare monitoring is their cumbersome and obtrusive nature. This\nis particularly true for electroencephalography (EEG) recordings, which require\nthe placement of multiple electrodes in contact with the scalp. In this work,\nwe propose to identify the optimal wearable EEG electrode set-up, in terms of\nminimal number of electrodes, comfortable location and performance, for\nEEG-based event detection and monitoring. By relying on the demonstrated power\nof autoencoder (AE) networks to learn latent representations from\nhigh-dimensional data, our proposed strategy trains an AE architecture in a\none-class classification setup with different electrode set-ups as input data.\nThe resulting models are assessed using the F-score and the best set-up is\nchosen according to the established optimal criteria. Using alpha wave\ndetection as use case, we demonstrate that the proposed method allows to detect\nan alpha state from an optimal set-up consisting of electrodes in the forehead\nand behind the ear, with an average F-score of 0.78. Our results suggest that a\nlearning-based approach can be used to enable the design and implementation of\noptimized wearable devices for real-life healthcare monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:17:22 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:02:50 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:16:45 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ferrari", "Laura M.", ""], ["Hanna", "Guy Abi", ""], ["Volpe", "Paolo", ""], ["Ismailova", "Esma", ""], ["Bremond", "Fran\u00e7ois", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2104.04605", "submitter": "Thomas House", "authors": "Thomas House, Lorenzo Pellis, Koen B. Pouwels, Sebastian Bacon,\n  Arturas Eidukas, Kaveh Jahanshahi, Rosalind M. Eggo, A. Sarah Walker", "title": "Inferring Risks of Coronavirus Transmission from Community Household\n  Data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The response of many governments to the COVID-19 pandemic has involved\nmeasures to control within- and between-household transmission, providing\nmotivation to improve understanding of the absolute and relative risks in these\ncontexts. Here, we perform exploratory, residual-based, and\ntransmission-dynamic household analysis of the Office for National Statistics\n(ONS) COVID-19 Infection Survey (CIS) data from 26 April 2020 to 8 March 2021\nin England. This provides evidence for: (i) temporally varying rates of\nintroduction of infection into households broadly following the trajectory of\nthe overall epidemic; (ii) Susceptible-Infectious Transmission Probabilities\n(SITPs) of within-household transmission in the 15-35% range; (iii) the\nemergence of the B.1.1.7 variant, being around 50% more infectious within\nhouseholds; (iv) significantly (in the range 25-300%) more risk of bringing\ninfection into the household for workers in patient-facing roles; (v) increased\nrisk for secondary school-age children of bringing the infection into the\nhousehold when schools are open (in the range 64-235%); (vi) increased risk for\nprimary school-age children of bringing the infection into the household when\nschools were open in late autumn 2020 (around 40%).\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:12:20 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 09:32:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["House", "Thomas", ""], ["Pellis", "Lorenzo", ""], ["Pouwels", "Koen B.", ""], ["Bacon", "Sebastian", ""], ["Eidukas", "Arturas", ""], ["Jahanshahi", "Kaveh", ""], ["Eggo", "Rosalind M.", ""], ["Walker", "A. Sarah", ""]]}, {"id": "2104.05076", "submitter": "Daoji Li", "authors": "Ruipeng Dong, Daoji Li, Zemin Zheng", "title": "Parallel integrative learning for large-scale multi-response regression\n  with incomplete outcomes", "comments": "32 pages", "journal-ref": "Computational Statistics and Data Analysis, 2021", "doi": "10.1016/j.csda.2021.107243", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-task learning is increasingly used to investigate the association\nstructure between multiple responses and a single set of predictor variables in\nmany applications. In the era of big data, the coexistence of incomplete\noutcomes, large number of responses, and high dimensionality in predictors\nposes unprecedented challenges in estimation, prediction, and computation. In\nthis paper, we propose a scalable and computationally efficient procedure,\ncalled PEER, for large-scale multi-response regression with incomplete\noutcomes, where both the numbers of responses and predictors can be\nhigh-dimensional. Motivated by sparse factor regression, we convert the\nmulti-response regression into a set of univariate-response regressions, which\ncan be efficiently implemented in parallel. Under some mild regularity\nconditions, we show that PEER enjoys nice sampling properties including\nconsistency in estimation, prediction, and variable selection. Extensive\nsimulation studies show that our proposal compares favorably with several\nexisting methods in estimation accuracy, variable selection, and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:01:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dong", "Ruipeng", ""], ["Li", "Daoji", ""], ["Zheng", "Zemin", ""]]}, {"id": "2104.05184", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "A smoothed and probabilistic PARAFAC model with covariates", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and clustering of multivariate time-series data attract growing\ninterest in immunological and clinical studies. In such applications,\nresearchers are interested in clustering subjects based on potentially\nhigh-dimensional longitudinal features, and in investigating how clinical\ncovariates may affect the clustering results. These studies are often\nchallenging due to high dimensionality, as well as the sparse and irregular\nnature of sample collection along the time dimension. We propose a smoothed\nprobabilistic PARAFAC model with covariates (SPACO) to tackle these two\nproblems while utilizing auxiliary covariates of interest. We provide intensive\nsimulations to test different aspects of SPACO and demonstrate its use on\nimmunological data sets from two recent cohorts of SARs-CoV-2 patients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:38:33 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 23:54:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "2104.05192", "submitter": "Yutao Liu", "authors": "Yutao Liu, Andrew Gelman, Qixuan Chen", "title": "Inference from Non-Random Samples Using Bayesian Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference from non-random samples in data-rich settings where\nhigh-dimensional auxiliary information is available both in the sample and the\ntarget population, with survey inference being a special case. We propose a\nregularized prediction approach that predicts the outcomes in the population\nusing a large number of auxiliary variables such that the ignorability\nassumption is reasonable while the Bayesian framework is straightforward for\nquantification of uncertainty. Besides the auxiliary variables, inspired by\nLittle & An (2004), we also extend the approach by estimating the propensity\nscore for a unit to be included in the sample and also including it as a\npredictor in the machine learning models. We show through simulation studies\nthat the regularized predictions using soft Bayesian additive regression trees\nyield valid inference for the population means and coverage rates close to the\nnominal levels. We demonstrate the application of the proposed methods using\ntwo different real data applications, one in a survey and one in an\nepidemiology study.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:08:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Yutao", ""], ["Gelman", "Andrew", ""], ["Chen", "Qixuan", ""]]}, {"id": "2104.05204", "submitter": "Tianxiang Zhan", "authors": "Tianxiang Zhan, Fuyuan Xiao", "title": "A Fast Evidential Approach for Stock Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of evidence theory, the confidence functions of\ndifferent information can be combined into a combined confidence function to\nsolve uncertain problems. The Dempster combination rule is a classic method of\nfusing different information. This paper proposes a similar confidence function\nfor the time point in the time series. The Dempster combination rule can be\nused to fuse the growth rate of the last time point, and finally a relatively\naccurate forecast data can be obtained. Stock price forecasting is a concern of\neconomics. The stock price data is large in volume, and more accurate forecasts\nare required at the same time. The classic methods of time series, such as\nARIMA, cannot balance forecasting efficiency and forecasting accuracy at the\nsame time. In this paper, the fusion method of evidence theory is applied to\nstock price prediction. Evidence theory deals with the uncertainty of stock\nprice prediction and improves the accuracy of prediction. At the same time, the\nfusion method of evidence theory has low time complexity and fast prediction\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:58:51 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 08:49:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhan", "Tianxiang", ""], ["Xiao", "Fuyuan", ""]]}, {"id": "2104.05292", "submitter": "Mikkel Meyer Andersen", "authors": "Mikkel Meyer Andersen, S{\\o}ren H{\\o}jsgaard", "title": "Computer Algebra in R with caracas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The capability of R to do symbolic mathematics is enhanced by the caracas\npackage. This package uses the Python computer algebra library SymPy as a\nback-end but caracas is tightly integrated in the R environment, thereby\nenabling the R user with symbolic mathematics within R. Key components of the\ncaracas package are illustrated in this paper. Examples are taken from\nstatistics and mathematics. The caracas package integrates well with e.g.\nRmarkdown, and as such creation of scientific reports and teaching is\nsupported.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:52:50 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 10:41:54 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 06:55:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Andersen", "Mikkel Meyer", ""], ["H\u00f8jsgaard", "S\u00f8ren", ""]]}, {"id": "2104.05449", "submitter": "Hon Keung Tony Ng", "authors": "James U. Gleaton, David Han, James D. Lynch, Hon Keung Tony Ng and\n  Fabrizio Ruggeri", "title": "Current Overview of Statistical Fiber Bundles Model and Its Application\n  to Physics-based Reliability Analysis of Thin-film Dielectrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a critical overview of statistical fiber bundles\nmodels. We discuss relevant aspects, like assumptions and consequences stemming\nfrom models in the literature and propose new ones. This is accomplished by\nconcentrating on both the physical and statistical aspects of a specific\nload-sharing example, the breakdown (BD) for circuits of capacitors and related\ndielectrics. For series and parallel/series circuits (series/parallel\nreliability systems) of ordinary capacitors, the load-sharing rules are derived\nfrom the electrical laws. This with the BD formalism is then used to obtain the\nBD distribution of the circuit. The BD distribution and Gibbs measure are given\nfor a series circuit and the size effects are illustrated for simulations of\nseries and parallel/series circuits. This is related to the finite weakest link\nadjustments for the BD distribution that arise in large series/parallel\nreliability load-sharing systems, such as dielectric BD, from their extreme\nvalue approximations.\n  An elementary but in-depth discussion of the physical aspects of SiO$_2$ and\nHfO$_2$ dielectrics and cell models is given. This is used to study a\nload-sharing cell model for the BD of HfO$_2$ dielectrics and the BD formalism.\nThe latter study is based on an analysis of Kim and Lee (2004)'s data for such\ndielectrics. Here, several BD distributions are compared in the analysis and\nproportional hazard regression models are used to study the BD formalism. In\naddition, some areas of open research are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:48:04 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:15:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Gleaton", "James U.", ""], ["Han", "David", ""], ["Lynch", "James D.", ""], ["Ng", "Hon Keung Tony", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "2104.05513", "submitter": "Larry Han", "authors": "Larry Han, Xuan Wang, Tianxi Cai", "title": "On the Evaluation of Surrogate Markers in Real World Data Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shortcomings of randomized clinical trials are pronounced in urgent health\ncrises, when rapid identification of effective treatments is critical.\nLeveraging short-term surrogates in real-world data (RWD) can guide\npolicymakers evaluating new treatments. In this paper, we develop novel\nestimators for the proportion of treatment effect (PTE) on the true outcome\nexplained by a surrogate in RWD settings. We propose inverse probability\nweighted and doubly robust (DR) estimators of an optimal transformation of the\nsurrogate and PTE by semi-nonparametrically modeling the relationship between\nthe true outcome and surrogate given baseline covariates. We show that our\nestimators are consistent and asymptotically normal, and the DR estimator is\nconsistent when either the propensity score model or outcome regression model\nis correctly specified. We compare our proposed estimators to existing\nestimators and show a reduction in bias and gains in efficiency through\nsimulations. We illustrate the utility of our method in obtaining an\ninterpretable PTE by conducting a cross-trial comparison of two biologic\ntherapies for ulcerative colitis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:44:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Han", "Larry", ""], ["Wang", "Xuan", ""], ["Cai", "Tianxi", ""]]}, {"id": "2104.05560", "submitter": "Tommy Nyberg", "authors": "Tommy Nyberg (1), Katherine A. Twohig (2), Ross J. Harris (3), Shaun\n  R. Seaman (1), Joe Flannagan (2), Hester Allen (2), Andre Charlett (3),\n  Daniela De Angelis (1 and 3), Gavin Dabrera (2), Anne M. Presanis (1) ((1)\n  MRC Biostatistics Unit, University of Cambridge, Cambridge, United Kingdom,\n  (2) COVID-19 National Epidemiology Cell, Public Health England, London,\n  United Kingdom, (3) National Infection Service, Public Health England,\n  London, United Kingdom)", "title": "Hospitalisation risk for COVID-19 patients infected with SARS-CoV-2\n  variant B.1.1.7: cohort analysis", "comments": "29 pages, 9 figures", "journal-ref": "BMJ 2021;373:n1412", "doi": "10.1136/bmj.n1412", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To evaluate the relationship between coronavirus disease 2019\n(COVID-19) diagnosis with SARS-CoV-2 variant B.1.1.7 (also known as Variant of\nConcern 202012/01) and the risk of hospitalisation compared to diagnosis with\nwildtype SARS-CoV-2 variants.\n  Design: Retrospective cohort, analysed using stratified Cox regression.\n  Setting: Community-based SARS-CoV-2 testing in England, individually linked\nwith hospitalisation data.\n  Participants: 839,278 laboratory-confirmed COVID-19 patients, of whom 36,233\nhad been hospitalised within 14 days, tested between 23rd November 2020 and\n31st January 2021 and analysed at a laboratory with an available TaqPath assay\nthat enables assessment of S-gene target failure (SGTF). SGTF is a proxy test\nfor the B.1.1.7 variant. Patient data were stratified by age, sex, ethnicity,\ndeprivation, region of residence, and date of positive test.\n  Main outcome measures: Hospitalisation between 1 and 14 days after the first\npositive SARS-CoV-2 test.\n  Results: 27,710 of 592,409 SGTF patients (4.7%) and 8,523 of 246,869 non-SGTF\npatients (3.5%) had been hospitalised within 1-14 days. The stratum-adjusted\nhazard ratio (HR) of hospitalisation was 1.52 (95% confidence interval [CI]\n1.47 to 1.57) for COVID-19 patients infected with SGTF variants, compared to\nthose infected with non-SGTF variants. The effect was modified by age\n(P<0.001), with HRs of 0.93-1.21 for SGTF compared to non-SGTF patients below\nage 20 years, 1.29 in those aged 20-29, and 1.45-1.65 in age groups 30 years or\nolder.\n  Conclusions: The results suggest that the risk of hospitalisation is higher\nfor individuals infected with the B.1.1.7 variant compared to wildtype\nSARS-CoV-2, likely reflecting a more severe disease. The higher severity may be\nspecific to adults above the age of 30.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:29:20 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 18:49:26 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 00:53:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Nyberg", "Tommy", "", "1 and 3"], ["Twohig", "Katherine A.", "", "1 and 3"], ["Harris", "Ross J.", "", "1 and 3"], ["Seaman", "Shaun R.", "", "1 and 3"], ["Flannagan", "Joe", "", "1 and 3"], ["Allen", "Hester", "", "1 and 3"], ["Charlett", "Andre", "", "1 and 3"], ["De Angelis", "Daniela", "", "1 and 3"], ["Dabrera", "Gavin", ""], ["Presanis", "Anne M.", ""]]}, {"id": "2104.05751", "submitter": "Jorge Sicacha-Parada", "authors": "Jorge Sicacha-Parada and Diego Pavon-Jordan and Ingelin Steinsland and\n  Roel May and B{\\aa}rd Stokke and Ingar Jostein {\\O}ien", "title": "A spatial modeling framework for monitoring surveys with different\n  sampling protocols with a case study for bird populations in mid-Scandinavia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying species abundance is the basis for spatial ecology and\nbiodiversity conservation. Abundance data are mostly collected through\nprofessional surveys as part of monitoring programs, often at a national level.\nThese surveys rarely follow the same sampling protocol in different countries,\nwhich represents a challenge for producing abundance maps based on the\ninformation available for more than one country. We here present a novel\nsolution for this methodological challenge with a case study concerning bird\nabundance in mid-Scandinavia. We use data from bird monitoring programs in\nNorway and Sweden. Each census collects abundance data following two different\nsampling protocols that each contain two different sampling methods. We propose\na modeling framework that assumes a common Gaussian Random Field driving both\nthe observed and true abundance with either a linear or a relaxed linear\nassociation between them. Thus, the models in this framework can integrate all\nsources of information involving count of organisms to produce one estimate for\nthe expected abundance, its uncertainty and the covariate effects. Bayesian\ninference is performed using INLA and the SPDE approach for spatial modeling.\nWe also present the results of a simulation study based on the census data from\nmid-Scandinavia to assess the performance of the models under misspecification.\nFinally, maps of the total expected abundance of the bird species present in\nour study region in mid-Scandinavia were produced. We found that the framework\nallows for consistent integration of data from surveys with different sampling\nprotocols. Further, the simulation study showed that models with a relaxed\nlinear specification are less sensitive to misspecification, compared to the\nmodel that assumes linear association between counts. Relaxed linear\nspecifications improved goodness-of-fit, but not the predictive power of the\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:24:37 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sicacha-Parada", "Jorge", ""], ["Pavon-Jordan", "Diego", ""], ["Steinsland", "Ingelin", ""], ["May", "Roel", ""], ["Stokke", "B\u00e5rd", ""], ["\u00d8ien", "Ingar Jostein", ""]]}, {"id": "2104.05915", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Mahir Jain, Manavendra Maharana, Pavel N. Krivitsky", "title": "Revisiting Bayesian Autoencoders with MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders gained popularity in the deep learning revolution given their\nability to compress data and provide dimensionality reduction. Although\nprominent deep learning methods have been used to enhance autoencoders, the\nneed to provide robust uncertainty quantification remains a challenge. This has\nbeen addressed with variational autoencoders so far. Bayesian inference via\nMCMC methods have faced limitations but recent advances with parallel computing\nand advanced proposal schemes that incorporate gradients have opened routes\nless travelled. In this paper, we present Bayesian autoencoders powered MCMC\nsampling implemented using parallel computing and Langevin gradient proposal\nscheme. Our proposed Bayesian autoencoder provides similar performance accuracy\nwhen compared to related methods from the literature, with the additional\nfeature of robust uncertainty quantification in compressed datasets. This\nmotivates further application of the Bayesian autoencoder framework for other\ndeep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:23:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Mahir", ""], ["Maharana", "Manavendra", ""], ["Krivitsky", "Pavel N.", ""]]}, {"id": "2104.06169", "submitter": "Chao Zhang", "authors": "Samson Lasaulce, Chao Zhang, Vineeth Varma, and Irinel Constantin\n  Morarescu", "title": "Analysis of the tradeoff between health and economic impacts of the\n  Covid-19 epidemic", "comments": null, "journal-ref": "Frontiers in Public Health, 2021", "doi": "10.3389/fpubh.2021.620770", "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Various measures have been taken in different countries to mitigate the\nCovid-19 epidemic. But, throughout the world, many citizens don't understand\nwell how these measures are taken and even question the decisions taken by\ntheir government. Should the measures be more (or less) restrictive? Are they\ntaken for a too long (or too short) period of time? To provide some\nquantitative elements of response to these questions, we consider the\nwell-known SEIR model for the Covid-19 epidemic propagation and propose a\npragmatic model of the government decision-making operation. Although simple\nand obviously improvable, the proposed model allows us to study the tradeoff\nbetween health and economic aspects in a pragmatic and insightful way. Assuming\na given number of phases for the epidemic and a desired tradeoff between health\nand economic aspects, it is then possible to determine the optimal duration of\neach phase and the optimal severity level for each of them. The numerical\nanalysis is performed for the case of France but the adopted approach can be\napplied to any country. One of the takeaway messages of this analysis is that\nbeing able to implement the optimal 4-phase epidemic management strategy in\nFrance would have led to 1.05 million infected people and a GDP loss of 231\nbillion euro instead of 6.88 million of infected and a loss of 241 billion\neuro. This indicates that, seen from the proposed model perspective, the\neffectively implemented epidemic management strategy is good economically,\nwhereas substantial improvements might have been obtained in terms of health\nimpact. Our analysis indicates that the lockdown/severe phase should have been\nmore severe but shorter, and the adjustment phase occurred earlier. Due to the\nnatural tendency of people to deviate from the official rules, updating\nmeasures every month over the whole epidemic episode seems to be more\nappropriate.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:46:04 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lasaulce", "Samson", ""], ["Zhang", "Chao", ""], ["Varma", "Vineeth", ""], ["Morarescu", "Irinel Constantin", ""]]}, {"id": "2104.06199", "submitter": "Giuseppe Calafiore", "authors": "Giuseppe Calafiore, Giulia Fracastoro", "title": "COVID-19 case data for Italy stratified by age class", "comments": "six pages, one table, three figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dataset described in this paper contains daily data about COVID-19 cases\nthat occurred in Italy over the period from Jan. 28, 2020 to March 20, 2021,\ndivided into ten age classes of the population, the first class being 0-9\nyears, the tenth class being 90 years and over. The dataset contains eight\ncolumns, namely: date (day), age class, number of new cases, number of newly\nhospitalized patients, number of patients entering intensive care, number of\ndeceased patients, number of recovered patients, number of active infected\npatients. This data has been officially released for research purposes by the\nItalian authority for COVID-19 epidemiologic surveillance (Istituto Superiore\ndi Sanit\\`a - ISS), upon formal request by the authors, in accordance with the\nOrdonnance of the Chief of the Civil Protection Department n. 691 dated Aug. 4\n2020. A separate file contains the numerosity of the population in each age\nclass, according to the National Institute of Statistics (ISTAT) data of the\nresident population of Italy as of Jan. 2020. This data has potential use, for\ninstance, in epidemiologic studies of the effects of the COVID-19 contagion in\nItaly, in mortality analysis by age class, and in the development and testing\nof dynamical models of the contagion.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:51:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Calafiore", "Giuseppe", ""], ["Fracastoro", "Giulia", ""]]}, {"id": "2104.06299", "submitter": "Lucy Prior", "authors": "Lucy Prior, John Jerrim, Dave Thomson, George Leckie", "title": "A review and evaluation of secondary school accountability in England:\n  Statistical strengths, weaknesses, and challenges for 'Progress 8'", "comments": "67 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  School performance measures are published annually in England to hold schools\nto account and to support parental school choice. This article reviews and\nevaluates the Progress 8 secondary school accountability system for\nstate-funded schools. We assess the statistical strengths and weaknesses of\nProgress 8 relating to: choice of pupil outcome attainment measures; potential\nadjustments for pupil input attainment and background characteristics;\ndecisions around which schools and pupils are excluded from the measure;\npresentation of Progress 8 to users, choice of statistical model, and\ncalculation of statistical uncertainty; and issues related to the volatility of\nschool performance over time, including scope for reporting multi-year\naverages. We then discuss challenges for Progress 8 raised by the COVID-19\npandemic. Six simple recommendations follow to improve Progress 8 and school\naccountability in England.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:45:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Prior", "Lucy", ""], ["Jerrim", "John", ""], ["Thomson", "Dave", ""], ["Leckie", "George", ""]]}, {"id": "2104.06571", "submitter": "Sherri Rose", "authors": "Ani Eloyan and Sherri Rose", "title": "Considerations Across Three Cultures: Parametric Regressions,\n  Interpretable Algorithms, and Complex Algorithms", "comments": "7 pages, forthcoming in Observational Studies", "journal-ref": "Observational Studies (2021); 7(1):191-196.\n  https://muse.jhu.edu/article/799734", "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of Leo Breiman's thesis from \"Statistical Modeling:\nThe Two Cultures\" to include a bifurcation of algorithmic modeling, focusing on\nparametric regressions, interpretable algorithms, and complex (possibly\nexplainable) algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:07:54 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Eloyan", "Ani", ""], ["Rose", "Sherri", ""]]}, {"id": "2104.07029", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Mean-Squared Accuracy of Good-Turing Estimator", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.31351.44960/1", "report-no": null, "categories": "stat.ML cs.CR cs.IT cs.LG math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brilliant method due to Good and Turing allows for estimating objects not\noccurring in a sample. The problem, known under names \"sample coverage\" or\n\"missing mass\" goes back to their cryptographic work during WWII, but over\nyears has found has many applications, including language modeling, inference\nin ecology and estimation of distribution properties. This work characterizes\nthe maximal mean-squared error of the Good-Turing estimator, for any sample\n\\emph{and} alphabet size.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:23:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2104.07113", "submitter": "Bingkai Wang", "authors": "Bingkai Wang, Brian S. Caffo, Xi Luo, Chin-Fu Liu, Andreia V. Faria,\n  Michael I. Miller, Yi Zhao", "title": "Regularized regression on compositional trees with application to MRI\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A compositional tree refers to a tree structure on a set of random variables\nwhere each random variable is a node and composition occurs at each non-leaf\nnode of the tree. As a generalization of compositional data, compositional\ntrees handle more complex relationships among random variables and appear in\nmany disciplines, such as brain imaging, genomics and finance. We consider the\nproblem of sparse regression on data that are associated with a compositional\ntree and propose a transformation-free tree-based regularized regression method\nfor component selection. The regularization penalty is designed based on the\ntree structure and encourages a sparse tree representation. We prove that our\nproposed estimator for regression coefficients is both consistent and model\nselection consistent. In the simulation study, our method shows higher accuracy\nthan competing methods under different scenarios. By analyzing a brain imaging\ndata set from studies of Alzheimer's disease, our method identifies meaningful\nassociations between memory declination and volume of brain regions that are\nconsistent with current understanding.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:27:37 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 18:42:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Bingkai", ""], ["Caffo", "Brian S.", ""], ["Luo", "Xi", ""], ["Liu", "Chin-Fu", ""], ["Faria", "Andreia V.", ""], ["Miller", "Michael I.", ""], ["Zhao", "Yi", ""]]}, {"id": "2104.07146", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko, Ronald Kriemann, Vladimir Berikov", "title": "Identification of unknown parameters and prediction with hierarchical\n  matrices", "comments": "16 pages, 5 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of massive datasets very often implies expensive linear\nalgebra operations with large dense matrices. Typical tasks are an estimation\nof unknown parameters of the underlying statistical model and prediction of\nmissing values. We developed the H-MLE procedure, which solves these tasks. The\nunknown parameters can be estimated by maximizing the joint Gaussian\nlog-likelihood function, which depends on a covariance matrix. To decrease high\ncomputational cost, we approximate the covariance matrix in the hierarchical\n(H-) matrix format. The H-matrix technique allows us to work with inhomogeneous\ncovariance matrices and almost arbitrary locations. Especially, H-matrices can\nbe applied in cases when the matrices under consideration are dense and\nunstructured.\n  For validation purposes, we implemented three machine learning methods: the\nk-nearest neighbors (kNN), random forest, and deep neural network. The best\nresults (for the given datasets) were obtained by the kNN method with three or\nseven neighbors depending on the dataset. The results computed with the H-MLE\nmethod were compared with the results obtained by the kNN method.\n  The developed H-matrix code and all datasets are freely available online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:13:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Litvinenko", "Alexander", ""], ["Kriemann", "Ronald", ""], ["Berikov", "Vladimir", ""]]}, {"id": "2104.07172", "submitter": "Carlos Erwin Rodr\\'iguez Dr.", "authors": "Carlos E. Rodr\\'iguez and Rams\\'es H. Mena", "title": "COVID-19 Clinical footprint to infer about mortality", "comments": "23 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information of 1.6 million patients identified as SARS-CoV-2 positive in\nMexico is used to understand the relationship between comorbidities, symptoms,\nhospitalizations and deaths due to the COVID-19 disease. Using the presence or\nabsence of these latter variables a clinical footprint for each patient is\ncreated. The risk, expected mortality and the prediction of death outcomes,\namong other relevant quantities, are obtained and analyzed by means of a\nmultivariate Bernoulli distribution. The proposal considers all possible\nfootprint combinations resulting in a robust model suitable for Bayesian\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:17:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rodr\u00edguez", "Carlos E.", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "2104.07575", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Amanda Fern\\'andez-Fontelo, Alejandra Caba\\~na,\n  Argimiro Arratia and Pedro Puig", "title": "Bayesian Synthetic Likelihood Estimation for Underreported\n  Non-Stationary Time Series: Covid-19 Incidence in Spain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The problem of dealing with misreported data is very common in a wide range\nof contexts for different reasons. The current situation caused by the Covid-19\nworldwide pandemic is a clear example, where the data provided by official\nsources were not always reliable due to data collection issues and to the high\nproportion of asymptomatic cases. In this work, we explore the performance of\nBayesian Synthetic Likelihood to estimate the parameters of a model capable of\ndealing with misreported information and to reconstruct the most likely\nevolution of the phenomenon. The performance of the proposed methodology is\nevaluated through a comprehensive simulation study and illustrated by\nreconstructing the weekly Covid-19 incidence in each Spanish Autonomous\nCommunity in 2020.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:29:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Caba\u00f1a", "Alejandra", ""], ["Arratia", "Argimiro", ""], ["Puig", "Pedro", ""]]}, {"id": "2104.07617", "submitter": "Yusuke Narita", "authors": "Yusuke Narita and Ayumi Sudo", "title": "Curse of Democracy: Evidence from 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Countries with more democratic political regimes experienced greater GDP loss\nand more deaths from Covid-19 in 2020. Using five different instrumental\nvariable strategies, we find that democracy is a major cause of the wealth and\nhealth losses. This impact is global and is not driven by China and the US\nalone. A key channel for democracy's negative impact is weaker and narrower\ncontainment policies at the beginning of the outbreak, \\textit{not} the speed\nof introducing policies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:26:36 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Narita", "Yusuke", ""], ["Sudo", "Ayumi", ""]]}, {"id": "2104.07654", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, Pamela Ventola, and\n  James S. Duncan", "title": "Demographic-Guided Attention in Recurrent Neural Networks for Modeling\n  Neuropathophysiological Heterogeneity", "comments": "MLMI 2020 (MICCAI Workshop)", "journal-ref": null, "doi": "10.1007/978-3-030-59861-7_37", "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous presentation of a neurological disorder suggests potential\ndifferences in the underlying pathophysiological changes that occur in the\nbrain. We propose to model heterogeneous patterns of functional network\ndifferences using a demographic-guided attention (DGA) mechanism for recurrent\nneural network models for prediction from functional magnetic resonance imaging\n(fMRI) time-series data. The context computed from the DGA head is used to help\nfocus on the appropriate functional networks based on individual demographic\ninformation. We demonstrate improved classification on 3 subsets of the ABIDE I\ndataset used in published studies that have previously produced\nstate-of-the-art results, evaluating performance under a leave-one-site-out\ncross-validation framework for better generalizeability to new data. Finally,\nwe provide examples of interpreting functional network differences based on\nindividual demographic variables.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:58:36 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "2104.07839", "submitter": "Lutfi Mardianto", "authors": "Endah R.M. Putri, Lutfi Mardianto, Amirul Hakam, Chairul Imron, Hadi\n  Susanto", "title": "Removing non-smoothness in solving Black-Scholes equation using a\n  perturbation method", "comments": null, "journal-ref": null, "doi": "10.1016/j.physleta.2021.127367", "report-no": null, "categories": "q-fin.MF math.AP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-Scholes equation as one of the most celebrated mathematical models has\nan explicit analytical solution known as the Black-Scholes formula. Later\nvariations of the equation, such as fractional or nonlinear Black-Scholes\nequations, do not have a closed form expression for the corresponding formula.\nIn that case, one will need asymptotic expansions, including homotopy\nperturbation method, to give an approximate analytical solution. However, the\nsolution is non-smooth at a special point. We modify the method by {first}\nperforming variable transformations that push the point to infinity. As a test\nbed, we apply the method to the solvable Black-Scholes equation, where\nexcellent agreement with the exact solution is obtained. We also extend our\nstudy to multi-asset basket and quanto options by reducing the cases to\nsingle-asset ones. Additionally we provide a novel analytical solution of the\nsingle-asset quanto option that is simple and different from the existing\nexpression.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:08:29 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 07:31:50 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Putri", "Endah R. M.", ""], ["Mardianto", "Lutfi", ""], ["Hakam", "Amirul", ""], ["Imron", "Chairul", ""], ["Susanto", "Hadi", ""]]}, {"id": "2104.07843", "submitter": "L\\'eo Belzile", "authors": "L\\'eo R. Belzile, Anthony C. Davison, Jutta Gampe, Holger Rootz\\'en,\n  Dmitrii Zholud", "title": "Is there a cap on longevity? A statistical review", "comments": "30 pages, including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is sustained and widespread interest in understanding the limit, if\nany, to the human lifespan. Apart from its intrinsic and biological interest,\nchanges in survival in old age have implications for the sustainability of\nsocial security systems. A central question is whether the endpoint of the\nunderlying lifetime distribution is finite. Recent analyses of data on the\noldest human lifetimes have led to competing claims about survival and to some\ncontroversy, due in part to incorrect statistical analysis. This paper\ndiscusses the particularities of such data, outlines correct ways of handling\nthem and presents suitable models and methods for their analysis. We provide a\ncritical assessment of some earlier work and illustrate the ideas through\nreanalysis of semi-supercentenarian lifetime data. Our analysis suggests that\nremaining life-length after age 109 is exponentially distributed, and that any\nupper limit lies well beyond the highest lifetime yet reliably recorded. Lower\nlimits to 95% confidence intervals for the human lifespan are around 130 years,\nand point estimates typically indicate no upper limit at all.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:30:59 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Belzile", "L\u00e9o R.", ""], ["Davison", "Anthony C.", ""], ["Gampe", "Jutta", ""], ["Rootz\u00e9n", "Holger", ""], ["Zholud", "Dmitrii", ""]]}, {"id": "2104.07938", "submitter": "Jens Rauch", "authors": "Jens Rauch, Iyiola E. Olatunji and Megha Khosla", "title": "Achieving differential privacy for $k$-nearest neighbors based outlier\n  detection by data partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When applying outlier detection in settings where data is sensitive,\nmechanisms which guarantee the privacy of the underlying data are needed. The\n$k$-nearest neighbors ($k$-NN) algorithm is a simple and one of the most\neffective methods for outlier detection. So far, there have been no attempts\nmade to develop a differentially private ($\\epsilon$-DP) approach for $k$-NN\nbased outlier detection. Existing approaches often relax the notion of\n$\\epsilon$-DP and employ other methods than $k$-NN. We propose a method for\n$k$-NN based outlier detection by separating the procedure into a fitting step\non reference inlier data and then apply the outlier classifier to new data. We\nachieve $\\epsilon$-DP for both the fitting algorithm and the outlier classifier\nwith respect to the reference data by partitioning the dataset into a uniform\ngrid, which yields low global sensitivity. Our approach yields nearly optimal\nperformance on real-world data with varying dimensions when compared to the\nnon-private versions of $k$-NN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:35:26 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rauch", "Jens", ""], ["Olatunji", "Iyiola E.", ""], ["Khosla", "Megha", ""]]}, {"id": "2104.08183", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz and Richard Bowden", "title": "Shadow-Mapping for Unsupervised Neural Causal Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal across most scientific fields is the discovery of causal\nstructures underling a set of observations. Unfortunately, causal discovery\nmethods which are based on correlation or mutual information can often fail to\nidentify causal links in systems which exhibit dynamic relationships. Such\ndynamic systems (including the famous coupled logistic map) exhibit `mirage'\ncorrelations which appear and disappear depending on the observation window.\nThis means not only that correlation is not causation but, perhaps\ncounter-intuitively, that causation may occur without correlation. In this\npaper we describe Neural Shadow-Mapping, a neural network based method which\nembeds high-dimensional video data into a low-dimensional shadow\nrepresentation, for subsequent estimation of causal links. We demonstrate its\nperformance at discovering causal links from video-representations of dynamic\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:50:03 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 12:58:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2104.08344", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Elizabeth C.Lange, Elizabeth A.Archie, Fernando A.Campos,\n  Susan C.Alberts, Fan Li", "title": "Causal Mediation Analysis for Longitudinal Mediators and Survival\n  Outcomes", "comments": "30 pages, 6 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2007.01796", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis studies how the treatment effect of an exposure on\noutcomes is mediated through intermediate variables. Although many applications\ninvolve longitudinal data, the existing methods are not directly applicable to\nsettings where the mediators are measured on irregular time grids. In this\npaper, we propose a causal mediation method that accommodates longitudinal\nmediators on arbitrary time grids and survival outcomes simultaneously. We take\na functional data analysis perspective and view longitudinal mediators as\nrealizations of underlying smooth stochastic processes. We define causal\nestimands of direct and indirect effects accordingly and provide corresponding\nidentification assumptions. We employ a functional principal component analysis\napproach to estimate the mediator process, and propose a Cox hazard model for\nthe survival outcome that flexibly adjusts the mediator process. We then derive\na g-computation formula to express the causal estimands using the model\ncoefficients. The proposed method is applied to a longitudinal data set from\nthe Amboseli Baboon Research Project to investigate the causal relationships\nbetween early adversity, adult physiological stress responses, and survival\namong wild female baboons. We find that adversity experienced in early life has\na significant direct effect on females' life expectancy and survival\nprobability, but find little evidence that these effects were mediated by\nmarkers of the stress response in adulthood. We further developed a sensitivity\nanalysis method to assess the impact of potential violation to the key\nassumption of sequential ignorability.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:49:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zeng", "Shuxi", ""], ["Lange", "Elizabeth C.", ""], ["Archie", "Elizabeth A.", ""], ["Campos", "Fernando A.", ""], ["Alberts", "Susan C.", ""], ["Li", "Fan", ""]]}, {"id": "2104.08402", "submitter": "Emil Alfred Edgar Mendoza", "authors": "Emil Mendoza, Fabian Dunker, Marco Reale", "title": "Regularized Maximum Likelihood Estimation for the Random Coefficients\n  Model", "comments": "23 Pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The random coefficients model $Y_i={\\beta_0}_i+{\\beta_1}_i\n{X_1}_i+{\\beta_2}_i {X_2}_i+\\ldots+{\\beta_d}_i {X_d}_i$, with $\\mathbf{X}_i$,\n$Y_i$, $\\mathbf{\\beta}_i$ i.i.d, and $\\mathbf{\\beta}_i$ independent of $X_i$ is\noften used to capture unobserved heterogeneity in a population. We propose a\nquasi-maximum likelihood method to estimate the joint density distribution of\nthe random coefficient model. This method implicitly involves the inversion of\nthe Radon transformation in order to reconstruct the joint distribution, and\nhence is an inverse problem. Nonparametric estimation for the joint density of\n$\\mathbf{\\beta}_i=({\\beta_0}_i,\\ldots, {\\beta_d}_i)$ based on kernel methods or\nFourier inversion have been proposed in recent years. Most of these methods\nassume a heavy tailed design density $f_\\mathbf{X}$. To add stability to the\nsolution, we apply regularization methods. We analyze the convergence of the\nmethod without assuming heavy tails for $f_\\mathbf{X}$ and illustrate\nperformance by applying the method on simulated and real data. To add stability\nto the solution, we apply a Tikhonov-type regularization method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:09:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mendoza", "Emil", ""], ["Dunker", "Fabian", ""], ["Reale", "Marco", ""]]}, {"id": "2104.08509", "submitter": "Andreu Arderiu", "authors": "Andreu Arderiu and Rapha\\\"el de Fondeville", "title": "Influence of Vaporfly shoe on sub-2 hour marathon and other top running\n  performances", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, Eliud Chipgoke ran a sub-two hour marathon in an unofficial race\nwearing last-generation shoes. The legitimacy of this feat was naturally\nquestioned due to unusual racing conditions and suspicions of technological\ndoping. In this work, we assess the likelihood of a sub-two hour marathon in an\nofficial race, and the potential influence of Vaporfly shoes, by studying the\nevolution of running top performances from 2001 to 2019 for distances ranging\nfrom 10k to marathon. The analysis is performed using extreme value theory, a\nfield of statistics dealing with analysis of records. We find a significant\nevidence of technological doping with a 12% increase of the probability that a\nnew world record for marathon-man discipline is set in 2021. However, results\nsuggest that achieving a sub-two hour marathon in an official event in 2021 is\nstill very unlikely, and exceeds 10% probability only by 2025.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 10:42:43 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Arderiu", "Andreu", ""], ["de Fondeville", "Rapha\u00ebl", ""]]}, {"id": "2104.08626", "submitter": "Taufik Abrao PhD", "authors": "Alex Mussi and Taufik Abr\\~ao", "title": "Mixed Gibbs Sampling Detector in High-Order Modulation Large-Scale MIMO\n  Systems", "comments": "28 pages, 7 figures, 3 tables and 3 pseudo-codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neighborhood restricted Mixed Gibbs Sampling (MGS) based approach is\nproposed for low-complexity high-order modulation large-scale Multiple-Input\nMultiple-Output (LS-MIMO) detection. The proposed LS-MIMO detector applies a\nneighborhood limitation (NL) on the noisy solution from the MGS at a distance d\n- thus, named d-simplified MGS (d-sMGS) - in order to mitigate its impact,\nwhich can be harmful when a high order modulation is considered. Numerical\nsimulation results considering 64-QAM demonstrated that the proposed detection\nmethod can substantially improve the MGS algorithm convergence, whereas no\nextra computational complexity per iteration is required. The proposed\nd-sMGS-based detector suitable for high-order modulation LS-MIMO further\nexhibits improved performance vs. complexity tradeoff when the system loading\nis high, i.e., when K >= 0.75. N. Also, with increasing the number of\ndimensions, i.e., increasing the number of antennas and/or modulation order, a\nsmaller restriction of 2-sMGS was shown to be a more interesting choice than\n1-sMGS.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:17:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mussi", "Alex", ""], ["Abr\u00e3o", "Taufik", ""]]}, {"id": "2104.08846", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison", "title": "Tutorial on logistic-regression calibration and fusion: Converting a\n  score to a likelihood ratio", "comments": "26 pages, 11 figures", "journal-ref": "Australian Journal of Forensic Sciences, 45, 173-197 (2013)", "doi": "10.1080/00450618.2012.733025", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Logistic-regression calibration and fusion are potential steps in the\ncalculation of forensic likelihood ratios. The present paper provides a\ntutorial on logistic-regression calibration and fusion at a practical\nconceptual level with minimal mathematical complexity. A score is\nlog-likelihood-ratio like in that it indicates the degree of similarity of a\npair of samples while taking into consideration their typicality with respect\nto a model of the relevant population. A higher-valued score provides more\nsupport for the same-origin hypothesis over the different-origin hypothesis\nthan does a lower-valued score; however, the absolute values of scores are not\ninterpretable as log likelihood ratios. Logistic-regression calibration is a\nprocedure for converting scores to log likelihood ratios, and\nlogistic-regression fusion is a procedure for converting parallel sets of\nscores from multiple forensic-comparison systems to log likelihood ratios.\nLogistic-regression calibration and fusion were developed for automatic speaker\nrecognition and are popular in forensic voice comparison. They can also be\napplied in other branches of forensic science, a fingerprint/fingermark example\nis provided.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:55:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""]]}, {"id": "2104.09067", "submitter": "Sumito Kurata", "authors": "Yohta Yamanaka, Sumito Kurata, Keisuke Yano, Fumiyasu Komaki, Takahiro\n  Shiina, Aitaro Kato", "title": "Structured regularization based local earthquake tomography for the\n  adaptation to velocity discontinuities", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a local earthquake tomography method that applies a\nstructured regularization technique to determine sharp changes in the Earth's\nseismic velocity structure with travel time data of direct waves. Our approach\nfocuses on the ability to better image two common features that are observed\nthe Earth's seismic velocity structure: velocity jumps that correspond to\nmaterial boundaries, such as the Conrad and Moho discontinuities, and gradual\nvelocity changes that are associated with the pressure and temperature\ndistributions in the crust and mantle. We employ different penalty terms in the\nvertical and horizontal directions to refine the imaging process. We utilize a\nvertical-direction (depth) penalty term that takes the form of the l1-sum of\nthe l2-norm of the second-order differences of the horizontal units in the\nvertical direction. This penalty is intended to represent sharp velocity jumps\ndue to discontinuities by creating a piecewise linear depth profile of the\naverage velocity structure. We set a horizontal-direction penalty term on the\nbasis of the l2-norm to express gradual velocity tendencies in the horizontal\ndirection. We use a synthetic dataset to demonstrate that our method provides\nsignificant improvements over the estimated velocity structures from\nconventional methods by obtaining stable estimates of both the velocity jumps\nand gradual velocity changes. We also demonstrate that our proposed method is\nrelatively robust against variations in the amplitude of the velocity jump,\ninitial velocity model, and the number of observed travel times. Furthermore,\nwe present a considerable potential for detecting a velocity discontinuity\nusing the observed travel times from only a small number of direct-wave\nobservations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:21:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yamanaka", "Yohta", ""], ["Kurata", "Sumito", ""], ["Yano", "Keisuke", ""], ["Komaki", "Fumiyasu", ""], ["Shiina", "Takahiro", ""], ["Kato", "Aitaro", ""]]}, {"id": "2104.09237", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz, Yohsuke Miyamoto, Luke Bornn, Maurice Smith", "title": "Inverse Bayesian Optimization: Learning Human Search Strategies in a\n  Sequential Optimization Task", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a popular algorithm for sequential optimization of a\nlatent objective function when sampling from the objective is costly. The\nsearch path of the algorithm is governed by the acquisition function, which\ndefines the agent's search strategy. Conceptually, the acquisition function\ncharacterizes how the optimizer balances exploration and exploitation when\nsearching for the optimum of the latent objective. In this paper, we explore\nthe inverse problem of Bayesian optimization; we seek to estimate the agent's\nlatent acquisition function based on observed search paths. We introduce a\nprobabilistic solution framework for the inverse problem which provides a\nprincipled framework to quantify both the variability with which the agent\nperforms the optimization task as well as the uncertainty around their\nestimated acquisition function.\n  We illustrate our methods by analyzing human behavior from an experiment\nwhich was designed to force subjects to balance exploration and exploitation in\nsearch of an invisible target location. We find that while most subjects\ndemonstrate clear trends in their search behavior, there is significant\nvariation around these trends from round to round. A wide range of search\nstrategies are exhibited across the subjects in our study, but upper confidence\nbound acquisition functions offer the best fit for the majority of subjects.\nFinally, some subjects do not map well to any of the acquisition functions we\ninitially consider; these subjects tend to exhibit exploration preferences\nbeyond that of standard acquisition functions to capture. Guided by the model\ndiscrepancies, we augment the candidate acquisition functions to yield a\nsuperior fit to the human behavior in this task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:40:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Miyamoto", "Yohsuke", ""], ["Bornn", "Luke", ""], ["Smith", "Maurice", ""]]}, {"id": "2104.09270", "submitter": "Andrew Duncan", "authors": "Andrew J Duncan, Aaron Reeves, George J Gunn, Roger W Humphry", "title": "Quantifying changes in the British cattle movement network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Cattle Tracing System database is an online recording system for cattle\nbirths, deaths and between--herd movements in the United Kingdom. Although it\nhas been thoroughly examined, the most recently reported movement analysis is\nfrom 2009. This article uses the database to construct weighted directed\nmonthly movement networks for two distinct periods of time, 2004--2006 and\n2015--2017, to quantify by how much the underlying structure of the network has\nchanged. Substantial changes in network structure may influence policy--makers\ndirectly or may influence models built upon the network data, and these in turn\ncould impact policy--makers and their assessment of risk. Four general network\nmeasures are used (total number of nodes with movements, movements, births and\ndeaths), in conjunction with network metrics to describe each monthly network.\nTwo updates of the database were examined to determine by how much the movement\ndata stored for a particular time period had been cleansed between updates.\nStatistical models show that there is a statistically significant effect of the\ntime period (2004--2006 vs 2015--2017) in the values of all network measures\nand six of nine network metrics. Changes in the sizes of both the Giant and\nWeakly Strongly Connected components predict reductions in the upper and lower\nbounds of the maximum epidemic size. Examination of the updates of the database\nshow that there are differences in records between updates and therefore\nevidence of historical data changing between updates. Accurate modelling of\ndisease spread through a network requires representative descriptions of the\nnetwork. The authors recommend that where possible the most recent available\ndata always be used for network modelling and that methods of network\nprediction be examined to mitigate for the time required for data to become\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:15:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Duncan", "Andrew J", ""], ["Reeves", "Aaron", ""], ["Gunn", "George J", ""], ["Humphry", "Roger W", ""]]}, {"id": "2104.09358", "submitter": "Arun Kuchibhotla", "authors": "Arun K. Kuchibhotla, Richard A. Berk", "title": "Nested Conformal Prediction Sets for Classification with Applications to\n  Probation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessments to help inform criminal justice decisions have been used in\nthe United States since the 1920s. Over the past several years, statistical\nlearning risk algorithms have been introduced amid much controversy about\nfairness, transparency and accuracy. In this paper, we focus on accuracy for a\nlarge department of probation and parole that is considering a major revision\nof its current, statistical learning risk methods. Because the content of each\noffender's supervision is substantially shaped by a forecast of subsequent\nconduct, forecasts have real consequences. Here we consider the probability\nthat risk forecasts are correct. We augment standard statistical learning\nestimates of forecasting uncertainty (i.e., confusion tables) with uncertainty\nestimates from nested conformal prediction sets. In a demonstration of concept\nusing data from the department of probation and parole, we show that the\nstandard uncertainty measures and uncertainty measures from nested conformal\nprediction sets can differ dramatically in concept and output. We also provide\na modification of nested conformal called the localized conformal method to\nmatch confusion tables more closely when possible. A strong case can be made\nfavoring the nested and localized conformal approach. As best we can tell, our\nformulation of such comparisons and consequent recommendations is novel.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:42:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Berk", "Richard A.", ""]]}, {"id": "2104.09452", "submitter": "Vincent Pisztora", "authors": "Vincent Pisztora, Yanglan Ou, Xiaolei Huang, Francesca Chiaromonte,\n  Jia Li", "title": "Epsilon Consistent Mixup: An Adaptive Consistency-Interpolation Tradeoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose $\\epsilon$-Consistent Mixup ($\\epsilon$mu).\n$\\epsilon$mu is a data-based structural regularization technique that combines\nMixup's linear interpolation with consistency regularization in the Mixup\ndirection, by compelling a simple adaptive tradeoff between the two. This\nlearnable combination of consistency and interpolation induces a more flexible\nstructure on the evolution of the response across the feature space and is\nshown to improve semi-supervised classification accuracy on the SVHN and\nCIFAR10 benchmark datasets, yielding the largest gains in the most challenging\nlow label-availability scenarios. Empirical studies comparing $\\epsilon$mu and\nMixup are presented and provide insight into the mechanisms behind\n$\\epsilon$mu's effectiveness. In particular, $\\epsilon$mu is found to produce\nmore accurate synthetic labels and more confident predictions than Mixup.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:10:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pisztora", "Vincent", ""], ["Ou", "Yanglan", ""], ["Huang", "Xiaolei", ""], ["Chiaromonte", "Francesca", ""], ["Li", "Jia", ""]]}, {"id": "2104.09609", "submitter": "Julien Berger", "authors": "Julien Berger, Thibaut Colinart, Bruna R. Loiola, Helcio R.B. Orlande", "title": "Parameter estimation and model selection for water sorption in a wood\n  fibre material", "comments": null, "journal-ref": "Wood Sci Technol 54, 1423-1446 (2020)", "doi": "10.1007/s00226-020-01206-0", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sorption curve is an essential feature for the modelling of heat and mass\ntransfer in porous building materials. Several models have been proposed in the\nliterature to represent the amount of moisture content in the material\naccording to the water activity (or capillary pressure) level. These models are\nbased on analytical expressions and few parameters that need to be estimated by\ninverse analysis. This article investigates the reliability of eight models\nthrough the accuracy of the estimated parameters. For this, experimental data\nfor a wood fibre material are generated with special attention to the stop\ncriterion to capture long time kinetic constants. Among five sets of\nmeasurements, the best estimate is computed. The reliability of the models is\nthen discussed. After proving the theoretical identifiability of the unknown\nparameters for each model, the primary identifiability is analysed. It\nevaluates whether the parameters influence on the model output is sufficient to\nproceed the parameter estimation with accuracy. For this, a continuous\nderivative-based approach is adopted. Seven models have a low primary\nidentifiability for at least one parameter. Indeed, when estimating the unknown\nparameters using the experimental observations, the parameters with low primary\nidentifiability exhibit large uncertainties. Finally, an Approximation Bayesian\nComputation algorithm is used to simultaneously select the best model and\nestimate the parameters that best represent the experimental data. The\nthermodynamic and \\textsc{Feng}--\\textsc{Xing} models, together with a proposed\nmodel in this work, were the best ones selected by this algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:13:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Berger", "Julien", ""], ["Colinart", "Thibaut", ""], ["Loiola", "Bruna R.", ""], ["Orlande", "Helcio R. B.", ""]]}, {"id": "2104.09645", "submitter": "Arun Chandrasekhar", "authors": "Abhijit Banerjee, Arun G. Chandrasekhar, Suresh Dalpath, Esther Duflo,\n  John Floretta, Matthew O. Jackson, Harini Kannan, Francine Loza, Anirudh\n  Sankar, Anna Schrimpf, Maheshwor Shrestha", "title": "Selecting the Most Effective Nudge: Evidence from a Large-Scale\n  Experiment on Immunization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluate a large-scale set of interventions to increase demand for\nimmunization in Haryana, India. The policies under consideration include the\ntwo most frequently discussed tools--reminders and incentives--as well as an\nintervention inspired by the networks literature. We cross-randomize whether\n(a) individuals receive SMS reminders about upcoming vaccination drives; (b)\nindividuals receive incentives for vaccinating their children; (c) influential\nindividuals (information hubs, trusted individuals, or both) are asked to act\nas \"ambassadors\" receiving regular reminders to spread the word about\nimmunization in their community. By taking into account different versions (or\n\"dosages\") of each intervention, we obtain 75 unique policy combinations. We\ndevelop a new statistical technique--a smart pooling and pruning procedure--for\nfinding a best policy from a large set, which also determines which policies\nare effective and the effect of the best policy. We proceed in two steps.\nFirst, we use a LASSO technique to collapse the data: we pool dosages of the\nsame treatment if the data cannot reject that they had the same impact, and\nprune policies deemed ineffective. Second, using the remaining (pooled)\npolicies, we estimate the effect of the best policy, accounting for the\nwinner's curse. The key outcomes are (i) the number of measles immunizations\nand (ii) the number of immunizations per dollar spent. The policy that has the\nlargest impact (information hubs, SMS reminders, incentives that increase with\neach immunization) increases the number of immunizations by 44% relative to the\nstatus quo. The most cost-effective policy (information hubs, SMS reminders, no\nincentives) increases the number of immunizations per dollar by 9.1%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:13:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Banerjee", "Abhijit", ""], ["Chandrasekhar", "Arun G.", ""], ["Dalpath", "Suresh", ""], ["Duflo", "Esther", ""], ["Floretta", "John", ""], ["Jackson", "Matthew O.", ""], ["Kannan", "Harini", ""], ["Loza", "Francine", ""], ["Sankar", "Anirudh", ""], ["Schrimpf", "Anna", ""], ["Shrestha", "Maheshwor", ""]]}, {"id": "2104.09697", "submitter": "David Ellenberger", "authors": "David Ellenberger and Michael Siebert", "title": "Introducing the Partitioned Equivalence Test: Artificial Intelligence in\n  Automatic Passenger Counting Validation", "comments": "23 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic passenger counting (APC) in public transport has been introduced in\nthe 1970s and has been rapidly emerging in recent years. APC systems, like all\nother measurement devices, are susceptible to error, which is treated as random\nnoise and is required to not exceed certain bounds. The demand for very low\nerrors is especially fueld by applications like revenue sharing, which is in\nthe billions, annually. As a result, both the requirements as well as the costs\nheavily increased. In this work, we address the latter problem and present a\nsolution to increase the efficiency of initial or recurrent (e.g. yearly or\nmore frequent) APC validation. Our new approach, the partitioned equivalence\ntest, is an extension to this widely used statistic hypothesis test and\nguarantees the same bounded, low user risk while reducing effort. This can be\nused to either cut costs or to extend validation without cost increase. It\ninvolves a pre-classification step, which itsself can be arbitrary, so we\nevaluated several use cases: entirely manual and algorithmic, artificial\nintelligence assisted workflows. For former, by restructuring the evaluation of\nmanual counts, our new statistical test can be used as a drop-in replacement\nfor existing test procedures. The largest savings, however, result from latter\nalgorithmic use cases: Due to the user risk being as bounded as in the original\nequivalence test, no additional requirements are introduced. Algorithms are\nallowed to be failable and thus, our test does not require the availability of\ngeneral artificial intelligence. All in all, automatic passenger counting as\nwell as the equivalence test itself can both benefit from our new extension.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:15:56 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 22:43:14 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 10:18:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ellenberger", "David", ""], ["Siebert", "Michael", ""]]}, {"id": "2104.09730", "submitter": "Joshua Warren", "authors": "Joshua L. Warren, Howard H. Chang, Lauren K. Warren, Matthew J.\n  Strickland, Lyndsey A. Darrow, James A. Mulholland", "title": "Critical Window Variable Selection for Mixtures: Estimating the Impact\n  of Multiple Air Pollutants on Stillbirth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the role of time-varying pollution mixtures on human health is\ncritical as people are simultaneously exposed to multiple pollutants during\ntheir lives. For vulnerable sub-populations who have well-defined exposure\nperiods (e.g., pregnant women), questions regarding critical windows of\nexposure to these mixtures are important for mitigating harm. We extend\nCritical Window Variable Selection (CWVS) to the multipollutant setting by\nintroducing CWVS for Mixtures (CWVSmix), a hierarchical Bayesian method that\ncombines smoothed variable selection and temporally correlated weight\nparameters to (i) identify critical windows of exposure to mixtures of\ntime-varying pollutants, (ii) estimate the time-varying relative importance of\neach individual pollutant and their first order interactions within the\nmixture, and (iii) quantify the impact of the mixtures on health. Through\nsimulation, we show that CWVSmix offers the best balance of performance in each\nof these categories in comparison to competing methods. Using these approaches,\nwe investigate the impact of exposure to multiple ambient air pollutants on the\nrisk of stillbirth in New Jersey, 2005-2014. We find consistent elevated risk\nin gestational weeks 2, 16-17, and 20 for non-Hispanic Black mothers, with\npollution mixtures dominated by ammonium (weeks 2, 17, 20), nitrate (weeks 2,\n17), nitrogen oxides (weeks 2, 16), PM2.5 (week 2), and sulfate (week 20). The\nmethod is available in the R package CWVSmix.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 02:56:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Warren", "Joshua L.", ""], ["Chang", "Howard H.", ""], ["Warren", "Lauren K.", ""], ["Strickland", "Matthew J.", ""], ["Darrow", "Lyndsey A.", ""], ["Mulholland", "James A.", ""]]}, {"id": "2104.09879", "submitter": "Hibiki Kaibuchi", "authors": "Hibiki Kaibuchi, Yoshinori Kawasaki and Gilles Stupfler", "title": "GARCH-UGH: A bias-reduced approach for dynamic extreme Value-at-Risk\n  estimation in financial time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Value-at-Risk (VaR) is a widely used instrument in financial risk\nmanagement. The question of estimating the VaR of loss return distributions at\nextreme levels is an important question in financial applications, both from\noperational and regulatory perspectives; in particular, the dynamic estimation\nof extreme VaR given the recent past has received substantial attention. We\npropose here a two-step bias-reduced estimation methodology called GARCH-UGH\n(Unbiased Gomes-de Haan), whereby financial returns are first filtered using an\nAR-GARCH model, and then a bias-reduced estimator of extreme quantiles is\napplied to the standardized residuals to estimate one-step ahead dynamic\nextreme VaR. Our results indicate that the GARCH-UGH estimates are more\naccurate than those obtained by combining conventional AR-GARCH filtering and\nextreme value estimates from the perspective of in-sample and out-of-sample\nbacktestings of historical daily returns on several financial time series.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:19:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kaibuchi", "Hibiki", ""], ["Kawasaki", "Yoshinori", ""], ["Stupfler", "Gilles", ""]]}, {"id": "2104.10029", "submitter": "Dongnan Liu", "authors": "Yang Ma, Chaoyi Zhang, Mariano Cabezas, Yang Song, Zihao Tang, Dongnan\n  Liu, Weidong Cai, Michael Barnett, Chenyu Wang", "title": "Multiple Sclerosis Lesion Analysis in Brain Magnetic Resonance Images:\n  Techniques and Clinical Applications", "comments": "12 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple sclerosis (MS) is a chronic inflammatory and degenerative disease of\nthe central nervous system, characterized by the appearance of focal lesions in\nthe white and gray matter that topographically correlate with an individual\npatient's neurological symptoms and signs. Magnetic resonance imaging (MRI)\nprovides detailed in-vivo structural information, permitting the quantification\nand categorization of MS lesions that critically inform disease management.\nTraditionally, MS lesions have been manually annotated on 2D MRI slices, a\nprocess that is inefficient and prone to inter-/intra-observer errors.\nRecently, automated statistical imaging analysis techniques have been proposed\nto extract and segment MS lesions based on MRI voxel intensity. However, their\neffectiveness is limited by the heterogeneity of both MRI data acquisition\ntechniques and the appearance of MS lesions. By learning complex lesion\nrepresentations directly from images, deep learning techniques have achieved\nremarkable breakthroughs in the MS lesion segmentation task. Here, we provide a\ncomprehensive review of state-of-the-art automatic statistical and\ndeep-learning MS segmentation methods and discuss current and future clinical\napplications. Further, we review technical strategies, such as domain\nadaptation, to enhance MS lesion segmentation in real-world clinical settings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:08:51 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ma", "Yang", ""], ["Zhang", "Chaoyi", ""], ["Cabezas", "Mariano", ""], ["Song", "Yang", ""], ["Tang", "Zihao", ""], ["Liu", "Dongnan", ""], ["Cai", "Weidong", ""], ["Barnett", "Michael", ""], ["Wang", "Chenyu", ""]]}, {"id": "2104.10041", "submitter": "Elvis Cui", "authors": "Elvis Cui, Dongyuan Song, Weng Kee Wong", "title": "Particle swarm optimization in constrained maximum likelihood estimation\n  a case study", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of paper is to apply two types of particle swarm optimization, global\nbest andlocal best PSO to a constrained maximum likelihood estimation problem\nin pseudotime anal-ysis, a sub-field in bioinformatics. The results have shown\nthat particle swarm optimizationis extremely useful and efficient when the\noptimization problem is non-differentiable and non-convex so that analytical\nsolution can not be derived and gradient-based methods can not beapplied.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 07:32:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cui", "Elvis", ""], ["Song", "Dongyuan", ""], ["Wong", "Weng Kee", ""]]}, {"id": "2104.10070", "submitter": "Natalie Klein", "authors": "Natalie Klein, Joshua H. Siegle, Tobias Teichert, Robert E. Kass", "title": "Cross-population coupling of neural activity based on Gaussian process\n  current source densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because LFPs arise from multiple sources in different spatial locations, they\ndo not easily reveal coordinated activity across neural populations on a\ntrial-to-trial basis. As we show here, however, once disparate source signals\nare decoupled, their trial-to-trial fluctuations become more accessible, and\ncross-population correlations become more apparent. To decouple sources we\nintroduce a general framework for estimation of current source densities\n(CSDs). In this framework, the set of LFPs result from noise being added to the\ntransform of the CSD by a biophysical forward model, while the CSD is\nconsidered to be the sum of a zero-mean, stationary, spatiotemporal Gaussian\nprocess, having fast and slow components, and a mean function, which is the sum\nof multiple time-varying functions distributed across space, each varying\nacross trials. We derived biophysical forward models relevant to the data we\nanalyzed. In simulation studies this approach improved identification of source\nsignals compared to existing CSD estimation methods. Using data recorded from\nprimate auditory cortex, we analyzed trial-to-trial fluctuations in both\nsteady-state and task-evoked signals. We found cortical layer-specific phase\ncoupling between two probes and showed that the same analysis applied directly\nto LFPs did not recover these patterns. We also found task-evoked CSDs to be\ncorrelated across probes, at specific cortical depths. Using data from\nNeuropixels probes in mouse visual areas, we again found evidence for\ndepth-specific phase coupling of areas V1 and LM based on the CSDs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:46:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Klein", "Natalie", ""], ["Siegle", "Joshua H.", ""], ["Teichert", "Tobias", ""], ["Kass", "Robert E.", ""]]}, {"id": "2104.10125", "submitter": "Alexander Bond", "authors": "A. J. Bond, C. B. Beggs", "title": "Bisecting for selecting: using a Laplacian eigenmaps clustering approach\n  to create the new European football Super League", "comments": "24 pages, 9 Figures, 3 Tables, 1 Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use European football performance data to select teams to form the\nproposed European football Super League, using only unsupervised techniques. We\nfirst used random forest regression to select important variables predicting\ngoal difference, which we used to calculate the Euclidian distances between\nteams. Creating a Laplacian eigenmap, we bisected the Fielder vector to\nidentify the five major European football leagues' natural clusters. Our\nresults showed how an unsupervised approach could successfully identify four\nclusters based on five basic performance metrics: shots, shots on target, shots\nconceded, possession, and pass success. The top two clusters identify those\nteams who dominate their respective leagues and are the best candidates to\ncreate the most competitive elite super league.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:12:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bond", "A. J.", ""], ["Beggs", "C. B.", ""]]}, {"id": "2104.10150", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Bayesian subset selection and variable importance for interpretable\n  prediction and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subset selection is a valuable tool for interpretable learning, scientific\ndiscovery, and data compression. However, classical subset selection is often\neschewed due to selection instability, computational bottlenecks, and lack of\npost-selection inference. We address these challenges from a Bayesian\nperspective. Given any Bayesian predictive model $\\mathcal{M}$, we elicit\npredictively-competitive subsets using linear decision analysis. The approach\nis customizable for (local) prediction or classification and provides\ninterpretable summaries of $\\mathcal{M}$. A key quantity is the acceptable\nfamily of subsets, which leverages the predictive distribution from\n$\\mathcal{M}$ to identify subsets that offer nearly-optimal prediction. The\nacceptable family spawns new (co-) variable importance metrics based on whether\nvariables (co-) appear in all, some, or no acceptable subsets. Crucially, the\nlinear coefficients for any subset inherit regularization and predictive\nuncertainty quantification via $\\mathcal{M}$. The proposed approach exhibits\nexcellent prediction, interval estimation, and variable selection for simulated\ndata, including $p=400 > n$. These tools are applied to a large education\ndataset with highly correlated covariates, where the acceptable family is\nespecially useful. Our analysis provides unique insights into the combination\nof environmental, socioeconomic, and demographic factors that predict\neducational outcomes, and features highly competitive prediction with\nremarkable stability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:48:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2104.10240", "submitter": "Mohsen Rezapour", "authors": "Bahareh Afhami, Mohsen Rezapour, Mohsen Madadi, and Vahed Maroufy", "title": "Portfolio Selection under Multivariate Merton Model with Correlated Jump\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio selection in the periodic investment of securities modeled by a\nmultivariate Merton model with dependent jumps is considered. The optimization\nframework is designed to maximize expected terminal wealth when portfolio risk\nis measured by the Condition-Value-at-Risk ($CVaR$). Solving the portfolio\noptimization problem by Monte Carlo simulation often requires intensive and\ntime-consuming computation; hence a faster and more efficient portfolio\noptimization method based on closed-form comonotonic bounds for the risk\nmeasure $CVaR$ of the terminal wealth is proposed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 20:36:23 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Afhami", "Bahareh", ""], ["Rezapour", "Mohsen", ""], ["Madadi", "Mohsen", ""], ["Maroufy", "Vahed", ""]]}, {"id": "2104.10298", "submitter": "Olivia Bernstein", "authors": "Olivia M. Bernstein, Brian G. Vegetabile, Christian R. Salazar, Joshua\n  D. Grill, and Daniel L. Gillen", "title": "Adjustment for Biased Sampling Using NHANES Derived Propensity Weights", "comments": "20 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Consent-to-Contact (C2C) registry at the University of California, Irvine\ncollects data from community participants to aid in the recruitment to clinical\nresearch studies. Self-selection into the C2C likely leads to bias due in part\nto enrollees having more years of education relative to the US general\npopulation. Salazar et al. (2020) recently used the C2C to examine associations\nof race/ethnicity with participant willingness to be contacted about research\nstudies. To address questions about generalizability of estimated associations\nwe estimate propensity for self-selection into the convenience sample weights\nusing data from the National Health and Nutrition Examination Survey (NHANES).\nWe create a combined dataset of C2C and NHANES subjects and compare different\napproaches (logistic regression, covariate balancing propensity score, entropy\nbalancing, and random forest) for estimating the probability of membership in\nC2C relative to NHANES. We propose methods to estimate the variance of\nparameter estimates that account for uncertainty that arises from estimating\npropensity weights. Simulation studies explore the impact of propensity weight\nestimation on uncertainty. We demonstrate the approach by repeating the\nanalysis by Salazar et al. with the deduced propensity weights for the C2C\nsubjects and contrast the results of the two analyses. This method can be\nimplemented using our estweight package in R available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 01:10:18 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bernstein", "Olivia M.", ""], ["Vegetabile", "Brian G.", ""], ["Salazar", "Christian R.", ""], ["Grill", "Joshua D.", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "2104.10440", "submitter": "Lucy Downey Mrs", "authors": "Lucy Downey, Achille Fonzone, Grigorios Fountas and Torran Semple", "title": "Impact of COVID-19 on travel behaviour, transport, lifestyles and\n  location choices in Scotland", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In March 2020, the UK and Scottish Governments imposed a lockdown restricting\neveryday life activities to only the most essential. These Governmental\nmeasures together with individual choices to refrain from travelling during the\nCOVID-19 pandemic have had a profound effect on transport related activity. In\nthe current investigation an online questionnaire was distributed to 994\nScottish residents in order to identify travel habits, attitudes and\npreferences during the different phases of the COVID-19 pandemic outbreak and\nanticipated travel habits after the pandemic. Quota constraints were enforced\nfor age, gender and household income to ensure the sample was representative of\nthe Scottish population as a whole. Perceptions of risk, trust in information\nsources and compliance with COVID-19 regulations were determined together with\nchanges in levels of life satisfaction and modal choice following the onset of\nCOVID-19. In addition, survey responses were used to identify anticipated\ntravel mode use in the future. Consideration was also given to the effects of\nCOVID-19 on transport related lifestyle issues such as working from home,\nonline shopping and the expectations of moving residences in the future. As\npart of the analysis, statistical models were developed to provide an insight\ninto both the relationships between the levels of non compliance with COVID-19\nregulations and demographic variables and the respondent attributes which might\naffect future public transport usage. In general, the study confirmed\nsignificant reductions in traffic activity, among respondents during the\nCOVID-19 pandemic associated with walking, driving a car and either using a bus\nor train. The respondents also indicated that they anticipated they would\ncontinue to make less use of buses and trains at the end of the pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:19:08 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Downey", "Lucy", ""], ["Fonzone", "Achille", ""], ["Fountas", "Grigorios", ""], ["Semple", "Torran", ""]]}, {"id": "2104.10554", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Wenbin Lu, Rui Song", "title": "Calibrated Optimal Decision Making with Multiple Data Sources and\n  Limited Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal decision-making problem in a primary sample of\ninterest with multiple auxiliary sources available. The outcome of interest is\nlimited in the sense that it is only observed in the primary sample. In\nreality, such multiple data sources may belong to different populations and\nthus cannot be combined directly. This paper proposes a novel calibrated\noptimal decision rule (CODR) to address the limited outcome, by leveraging the\nshared pattern in multiple data sources. Under a mild and testable assumption\nthat the conditional means of intermediate outcomes in different samples are\nequal given baseline covariates and the treatment information, we can show that\nthe calibrated mean outcome of interest under the CODR is unbiased and more\nefficient than using the primary sample solely. Extensive experiments on\nsimulated datasets demonstrate empirical validity and improvement of the\nproposed CODR, followed by a real application on the MIMIC-III as the primary\nsample with auxiliary data from eICU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:24:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2104.10573", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Rui Song, Wenbin Lu", "title": "GEAR: On Optimal Decision Making with Auxiliary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized optimal decision making, finding the optimal decision rule (ODR)\nbased on individual characteristics, has attracted increasing attention\nrecently in many fields, such as education, economics, and medicine. Current\nODR methods usually require the primary outcome of interest in samples for\nassessing treatment effects, namely the experimental sample. However, in many\nstudies, treatments may have a long-term effect, and as such the primary\noutcome of interest cannot be observed in the experimental sample due to the\nlimited duration of experiments, which makes the estimation of ODR impossible.\nThis paper is inspired to address this challenge by making use of an auxiliary\nsample to facilitate the estimation of ODR in the experimental sample. We\npropose an auGmented inverse propensity weighted Experimental and Auxiliary\nsample-based decision Rule (GEAR) by maximizing the augmented inverse\npropensity weighted value estimator over a class of decision rules using the\nexperimental sample, with the primary outcome being imputed based on the\nauxiliary sample. The asymptotic properties of the proposed GEAR estimators and\ntheir associated value estimators are established. Simulation studies are\nconducted to demonstrate its empirical validity with a real AIDS application.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:59:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Song", "Rui", ""], ["Lu", "Wenbin", ""]]}, {"id": "2104.10720", "submitter": "Rachel Scarlett", "authors": "Rachel D. Scarlett, Mangala Subramaniam, Sara K. McMillan, Anastasia\n  T. Ingermann, Sandra M. Clinton", "title": "Stormwater on the Margins: Influence of Race, Gender, and Education on\n  Willingness to Participate in Stormwater Management", "comments": "Accepted Manuscript 43 pages, 2 figures, 3 tables", "journal-ref": "Journal of Environmental Management 290 (2021) 112552", "doi": "10.1016/j.jenvman.2021.112552", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stormwater has immense impacts on urban flooding and water quality, leaving\nthe marginalized and the impoverished disproportionately impacted by and\nvulnerable to stormwater hazards. However, the environmental health concerns of\nsocially and economically marginalized individuals are largely underestimated.\nThrough regression analysis of data from three longitudinal surveys, this\narticle examines if and how an individual's race, gender, and education level\nhelp predict one's concern about and willingness to participate in stormwater\nmanagement. We found that people of color, women, and less-educated respondents\nhad a greater willingness to participate in stormwater management than White,\nmale, and more-educated respondents, and their concern about local stormwater\nhazards drove their willingness to participate. Our analysis suggests that\nphysical exposure and high vulnerability to stormwater hazards may shape an\nindividual's concern about and willingness to participate in stormwater\nmanagement.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:42:04 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 20:59:58 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Scarlett", "Rachel D.", ""], ["Subramaniam", "Mangala", ""], ["McMillan", "Sara K.", ""], ["Ingermann", "Anastasia T.", ""], ["Clinton", "Sandra M.", ""]]}, {"id": "2104.10735", "submitter": "Robert Bassett", "authors": "Robert Bassett, Jacob Foster, Kay L. Gemba, Paul Leary, Kevin B. Smith", "title": "The Maximal Eigengap Estimator for Acoustic Vector-Sensor Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the maximal eigengap estimator for finding the\ndirection of arrival of a wideband acoustic signal using a single\nvector-sensor. We show that in this setting narrowband cross-spectral density\nmatrices can be combined in an optimal weighting that approximately maximizes\nsignal-to-noise ratio across a wide frequency band. The signal subspace\nresulting from this optimal combination of narrowband power matrices defines\nthe maximal eigengap estimator. We discuss the advantages of the maximal\neigengap estimator over competing methods, and demonstrate its utility in a\nreal-data application using signals collected in 2019 from an acoustic\nvector-sensor deployed in the Monterey Bay.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:43:23 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 21:52:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bassett", "Robert", ""], ["Foster", "Jacob", ""], ["Gemba", "Kay L.", ""], ["Leary", "Paul", ""], ["Smith", "Kevin B.", ""]]}, {"id": "2104.10784", "submitter": "Alejandro Schuler", "authors": "Alejandro Schuler", "title": "Designing efficient randomized trials: power and sample size calculation\n  when using semiparametric efficient estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trials enroll a large number of subjects in order to attain power, making\nthem expensive and time-consuming. Sample size calculations are often performed\nwith the assumption of an unadjusted analysis, even if the trial analysis plan\nspecifies a more efficient estimator (e.g. ANCOVA). This leads to conservative\nestimates of required sample sizes and an opportunity for savings. Here we show\nthat a relatively simple formula can be used to estimate the power of any\ntwo-arm, single-timepoint trial analyzed with a semiparametric efficient\nestimator, regardless of the domain of the outcome or kind of treatment effect\n(e.g. odds ratio, mean difference). Since an efficient estimator attains the\nminimum possible asymptotic variance, this allows for the design of trials that\nare as small as possible while still attaining design power and control of type\nI error. The required sample size calculation is parsimonious and requires the\nanalyst to provide only a small number of population parameters. We verify in\nsimulation that the large-sample properties of trials designed this way attain\ntheir nominal values. Lastly, we demonstrate how to use this formula in the\n\"design\" (and subsequent reanalysis) of a real clinical trial and show that\nfewer subjects are required to attain the same design power when a\nsemiparametric efficient estimator is accounted for at the design stage.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:33:32 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 21:19:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Schuler", "Alejandro", ""]]}, {"id": "2104.10878", "submitter": "Samuel W.K. Wong", "authors": "Geoffrey McGregor, Jennifer Tippett, Andy T.S. Wan, Mengxiao Wang,\n  Samuel W.K. Wong", "title": "Assessing the effectiveness of regional physical distancing measures of\n  COVID-19 in rural regions of British Columbia", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effects of physical distancing measures for the spread of\nCOVID-19 in regional areas within British Columbia, using the reported cases of\nthe five provincial Health Authorities. Building on the Bayesian\nepidemiological model of Anderson et al. (2020), we propose a hierarchical\nBayesian model with time-varying regional parameters to account for the\nrelative reduction in contact due to physical distancing and increased testing\nfrom March to December of 2020. In the absence of COVID-19 variants and\nvaccinations during this period, we examine the regionalized basic reproduction\nnumber, modelled prevalence, fraction of normal contacts, proportion of\nanticipated cases, and we observed significant differences between the\nprovincial-wide and regional models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:01:04 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["McGregor", "Geoffrey", ""], ["Tippett", "Jennifer", ""], ["Wan", "Andy T. S.", ""], ["Wang", "Mengxiao", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "2104.10906", "submitter": "Francisco Javier Rubio", "authors": "Danilo Alvares and Francisco Javier Rubio", "title": "A tractable Bayesian joint model for longitudinal and survival data", "comments": "To appear in Statistics in Medicine. Software available at\n  https://github.com/daniloalvares/Tractable-BJM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a numerically tractable formulation of Bayesian joint models for\nlongitudinal and survival data. The longitudinal process is modelled using\ngeneralised linear mixed models, while the survival process is modelled using a\nparametric general hazard structure. The two processes are linked by sharing\nfixed and random effects, separating the effects that play a role at the time\nscale from those that affect the hazard scale. This strategy allows for the\ninclusion of non-linear and time-dependent effects while avoiding the need for\nnumerical integration, which facilitates the implementation of the proposed\njoint model. We explore the use of flexible parametric distributions for\nmodelling the baseline hazard function which can capture the basic shapes of\ninterest in practice. We discuss prior elicitation based on the interpretation\nof the parameters. We present an extensive simulation study, where we analyse\nthe inferential properties of the proposed models, and illustrate the trade-off\nbetween flexibility, sample size, and censoring. We also apply our proposal to\ntwo real data applications in order to demonstrate the adaptability of our\nformulation both in univariate time-to-event data and in a competing risks\nframework. The methodology is implemented in rstan.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:36:40 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Alvares", "Danilo", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "2104.10979", "submitter": "Thomas Pinder", "authors": "Thomas Pinder, Michael Hollaway, Christopher Nemeth, Paul J. Young,\n  David Leslie", "title": "A Probabilistic Assessment of the COVID-19 Lockdown on Air Quality in\n  the UK", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In March 2020 the United Kingdom (UK) entered a nationwide lockdown period\ndue to the Covid-19 pandemic. As a result, levels of nitrogen dioxide (NO2) in\nthe atmosphere dropped. In this work, we use 550,134 NO2 data points from 237\nstations in the UK to build a spatiotemporal Gaussian process capable of\npredicting NO2 levels across the entire UK. We integrate several covariate\ndatasets to enhance the model's ability to capture the complex spatiotemporal\ndynamics of NO2. Our numerical analyses show that, within two weeks of a UK\nlockdown being imposed, UK NO2 levels dropped 36.8%. Further, we show that as a\ndirect result of lockdown NO2 levels were 29-38% lower than what they would\nhave been had no lockdown occurred. In accompaniment to these numerical\nresults, we provide a software framework that allows practitioners to easily\nand efficiently fit similar models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:23:42 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pinder", "Thomas", ""], ["Hollaway", "Michael", ""], ["Nemeth", "Christopher", ""], ["Young", "Paul J.", ""], ["Leslie", "David", ""]]}, {"id": "2104.10996", "submitter": "Lukun Zheng", "authors": "Lukun Zheng, Yuhang Jiang", "title": "Combining dissimilarity measure for the study of evolution in scientific\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of scientific fields has been attracting much attention in\nrecent years. One of the key issues in evolution of scientific field is to\nquantify the dissimilarity between two collections of scientific publications\nin literature. Many existing works study the evolution based on one or two\ndissimilarity measures, despite the fact that there are many different\ndissimilarity measures. Finding the appropriate dissimilarity measures among\nsuch a collection of choices is of fundamental importance to the study of\nscientific evolution. In this article, we develop a new measure of the\nevolution combining twelve keyword-based temporal dissimilarities of the\nscientific fields using the method of principal component analysis. To\ndemonstrate the usage of this new measure, we chose four scientific fields:\nenvironmental studies, information science & library science, mechanical\ninformatics, and religion. A database consisting of 274453 bibliographic\nrecords in these four chosen fields from 1991 to 2019 are built. The results\nshow that all these four scientific fields share an overall decreasing trend in\nevolution from 1991 to 2019 and different fields exhibits different evolution\npatterns during different time periods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 11:34:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zheng", "Lukun", ""], ["Jiang", "Yuhang", ""]]}, {"id": "2104.11009", "submitter": "Udit Bhatia", "authors": "Pravin Bhasme, Jenil Vagadiya, Udit Bhatia", "title": "Enhancing predictive skills in physically-consistent way: Physics\n  Informed Machine Learning for Hydrological Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current modeling approaches for hydrological modeling often rely on either\nphysics-based or data-science methods, including Machine Learning (ML)\nalgorithms. While physics-based models tend to rigid structure resulting in\nunrealistic parameter values in certain instances, ML algorithms establish the\ninput-output relationship while ignoring the constraints imposed by well-known\nphysical processes. While there is a notion that the physics model enables\nbetter process understanding and ML algorithms exhibit better predictive\nskills, scientific knowledge that does not add to predictive ability may be\ndeceptive. Hence, there is a need for a hybrid modeling approach to couple ML\nalgorithms and physics-based models in a synergistic manner. Here we develop a\nPhysics Informed Machine Learning (PIML) model that combines the process\nunderstanding of conceptual hydrological model with predictive abilities of\nstate-of-the-art ML models. We apply the proposed model to predict the monthly\ntime series of the target (streamflow) and intermediate variables (actual\nevapotranspiration) in the Narmada river basin in India. Our results show the\ncapability of the PIML model to outperform a purely conceptual model ($abcd$\nmodel) and ML algorithms while ensuring the physical consistency in outputs\nvalidated through water balance analysis. The systematic approach for combining\nconceptual model structure with ML algorithms could be used to improve the\npredictive accuracy of crucial hydrological processes important for flood risk\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:13:42 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bhasme", "Pravin", ""], ["Vagadiya", "Jenil", ""], ["Bhatia", "Udit", ""]]}, {"id": "2104.11355", "submitter": "Salil Koner", "authors": "Salil Koner, So Young Park, Ana-Maria Staicu", "title": "PROFIT: Projection-based Test in Longitudinal Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, a dependent functional response is observed for\neach subject over repeated time, leading to longitudinal functional data. In\nthis paper, we propose a novel statistical procedure to test whether the mean\nfunction varies over time. Our approach relies on reducing the dimension of the\nresponse using data-driven orthogonal projections and it employs a\nlikelihood-based hypothesis testing. We investigate the methodology\ntheoretically and discuss a computationally efficient implementation. The\nproposed test maintains the type I error rate, and shows excellent power to\ndetect departures from the null hypothesis in finite sample simulation studies.\nWe apply our method to the longitudinal diffusion tensor imaging study of\nmultiple sclerosis (MS) patients to formally assess whether the brain's health\ntissue, as summarized by fractional anisotropy (FA) profile, degrades over time\nduring the study period.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:03:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Koner", "Salil", ""], ["Park", "So Young", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "2104.11461", "submitter": "Darren Shannon", "authors": "Darren Shannon, Grigorios Fountas", "title": "Extending the Heston Model to Forecast Motor Vehicle Collision Rates", "comments": "25 pages (excl. Ref, Appendices). 11 figures, 7 tables, 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.CP q-fin.MF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an alternative approach to the forecasting of motor vehicle\ncollision rates. We adopt an oft-used tool in mathematical finance, the Heston\nStochastic Volatility model, to forecast the short-term and long-term evolution\nof motor vehicle collision rates. We incorporate a number of extensions to the\nHeston model to make it fit for modelling motor vehicle collision rates. We\nincorporate the temporally-unstable and non-deterministic nature of collision\nrate fluctuations, and introduce a parameter to account for periods of\naccelerated safety. We also adjust estimates to account for the seasonality of\ncollision patterns. Using these parameters, we perform a short-term forecast of\ncollision rates and explore a number of plausible scenarios using long-term\nforecasts. The short-term forecast shows a close affinity with realised rates\n(over 95% accuracy), and outperforms forecasting models currently used in road\nsafety research (Vasicek, SARIMA, SARIMA-GARCH). The long-term scenarios\nsuggest that modest targets to reduce collision rates (1.83% annually) and\ntargets to reduce the fluctuations of month-to-month collision rates (by half)\ncould have significant benefits for road safety. The median forecast in this\nscenario suggests a 50% fall in collision rates, with 75% of simulations\nsuggesting that an effective change in collision rates is observed before 2044.\nThe main benefit the model provides is eschewing the necessity for setting\nunreasonable safety targets that are often missed. Instead, the model presents\nthe effects that modest and achievable targets can have on road safety over the\nlong run, while incorporating random variability. Examining the parameters that\nunderlie expected collision rates will aid policymakers in determining the\neffectiveness of implemented policies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:18:29 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 11:36:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shannon", "Darren", ""], ["Fountas", "Grigorios", ""]]}, {"id": "2104.11492", "submitter": "Andrea Sottosanti", "authors": "Andrea Sottosanti, Mauro Bernardi, Alessandra R. Brazzale, Alex\n  Geringer-Sameth, David C. Stenning, Roberto Trotta, David A. van Dyk", "title": "Identification of high-energy astrophysical point sources via\n  hierarchical Bayesian nonparametric clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.HE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The light we receive from distant astrophysical objects carries information\nabout their origins and the physical mechanisms that power them. The study of\nthese signals, however, is complicated by the fact that observations are often\na mixture of the light emitted by multiple localized sources situated in a\nspatially-varying background. A general algorithm to achieve robust and\naccurate source identification in this case remains an open question in\nastrophysics.\n  This paper focuses on high-energy light (such as X-rays and gamma-rays), for\nwhich observatories can detect individual photons (quanta of light), measuring\ntheir incoming direction, arrival time, and energy. Our proposed Bayesian\nmethodology uses both the spatial and energy information to identify point\nsources, that is, separate them from the spatially-varying background, to\nestimate their number, and to compute the posterior probabilities that each\nphoton originated from each identified source. This is accomplished via a\nDirichlet process mixture while the background is simultaneously reconstructed\nvia a flexible Bayesian nonparametric model based on B-splines. Our proposed\nmethod is validated with a suite of simulation studies and illustrated with an\napplication to a complex region of the sky observed by the \\emph{Fermi}\nGamma-ray Space Telescope.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:29:36 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 07:40:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sottosanti", "Andrea", ""], ["Bernardi", "Mauro", ""], ["Brazzale", "Alessandra R.", ""], ["Geringer-Sameth", "Alex", ""], ["Stenning", "David C.", ""], ["Trotta", "Roberto", ""], ["van Dyk", "David A.", ""]]}, {"id": "2104.11525", "submitter": "Peter Kevei", "authors": "Anita Bogdanov and P\\'eter Kevei and M\\'at\\'e Szalai and Dezs\\H{o}\n  Virok", "title": "Stochastic modeling of in vitro bactericidal potency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a Galton--Watson model for the growth of a bacterial population in\nthe presence of antibiotics. We assume that bacterial cells either die or\nduplicate, and the corresponding probabilities depend on the concentration of\nthe antibiotic. Assuming that the mean offspring number is given by $m(c) = 2 /\n(1 + \\alpha c^\\beta)$ for some $\\alpha, \\beta$, where $c$ stands for the\nantibiotic concentration we obtain weakly consistent, asymptotically normal\nestimator both for $(\\alpha, \\beta)$ and for the minimal inhibitory\nconcentration (MIC), a relevant parameter in pharmacology. We apply our method\nto real data, where \\emph{Chlamydia trachomatis} bacteria was treated by\nazithromycin and ciprofloxacin. For the measurements of \\emph{Chlamydia} growth\nquantitative PCR technique was used. The 2-parameter model fits remarkably well\nto the biological data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:22:02 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bogdanov", "Anita", ""], ["Kevei", "P\u00e9ter", ""], ["Szalai", "M\u00e1t\u00e9", ""], ["Virok", "Dezs\u0151", ""]]}, {"id": "2104.11526", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel and Norbert Hilger", "title": "Heterogeneous item populations across individuals: Consequences for the\n  factor model, item inter-correlations, and scale validity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper is devoted to the consequences of blind random selection of items\nfrom different item populations that might be based on completely uncorrelated\nfactors for item inter-correlations and corresponding factor loadings. Based on\nthe model of essentially parallel measurements, we explore the consequences of\npresenting items from different populations across individuals and items from\nidentical populations within each individual for the factor model and item\ninter-correlations in the total population of individuals. Moreover, we explore\nthe consequences of presenting items from different as well as identical item\npopulations across and within individuals. We show that correlations can be\nsubstantial in the total population of individuals even when -- in\nsubpopulations of individuals -- items are drawn from populations with\nuncorrelated factors. In order to address this challenge for the validity of a\nscale, we propose a method that helps to detect whether item inter-correlations\nresult from different item populations in different subpopulations of\nindividuals and evaluate the method by means of a simulation study. Based on\nthe analytical results and on results from a simulation study, we provide\nrecommendations for the detection of subpopulations of individuals responding\nto items from different item populations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:31:18 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Hilger", "Norbert", ""]]}, {"id": "2104.11531", "submitter": "Jouni Kuha", "authors": "Jouni Kuha, Siliang Zhang and Fiona Steele", "title": "Latent variable models for multivariate dyadic data with zero inflation:\n  Analysis of intergenerational exchanges of family support", "comments": "25 pages, 1 figure and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the help and support that is exchanged between family members\nof different generations is of increasing importance, with research questions\nin sociology and social policy focusing on both predictors of the levels of\nhelp given and received, and on reciprocity between them. We propose general\nlatent variable models for analysing such data, when helping tendencies in each\ndirection are measured by multiple binary indicators of specific types of help.\nThe model combines two continuous latent variables, which represent the helping\ntendencies, with two binary latent class variables which allow for high\nproportions of responses where no help of any kind is given or received. This\ndefines a multivariate version of a zero inflation model. The main part of the\nmodels is estimated using MCMC methods, with a bespoke data augmentation\nalgorithm. We apply the models to analyse exchanges of help between adult\nindividuals and their non-coresident parents, using survey data from the UK\nHousehold Longitudinal Study.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:47:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kuha", "Jouni", ""], ["Zhang", "Siliang", ""], ["Steele", "Fiona", ""]]}, {"id": "2104.11566", "submitter": "Wolfgang Rauch", "authors": "Rezgar Arabzadeh, Daniel Martin Gruenbacher, Heribert Insam, Norbert\n  Kreuzinger, Rudolf Markt and Wolfgang Rauch", "title": "Data filtering methods for SARS-CoV-2 wastewater surveillance", "comments": "22 pages 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the case of SARS-CoV-2 pandemic management, wastewater-based epidemiology\naims to derive information on the infection dynamics by monitoring virus\nconcentrations in the wastewater. However, due to the intrinsic random\nfluctuations of the viral signal in the wastewater (due to e.g., dilution;\ntransport and fate processes in sewer system; variation in the number of\npersons discharging; variations in virus excretion and water consumption per\nday) the subsequent prevalence analysis may result in misleading conclusions.\nIt is thus helpful to apply data filtering techniques to reduce the noise in\nthe signal. In this paper we investigate 13 smoothing algorithms applied to the\nvirus signals monitored in four wastewater treatment plants in Austria. The\nparameters of the algorithms have been defined by an optimization procedure\naiming for performance metrics. The results are further investigated by means\nof a cluster analysis. While all algorithms are in principle applicable,\nSPLINE, Generalized Additive Model and Friedman Super Smoother are recognized\nas superior methods in this context (with the latter two having a tendency to\nover-smoothing). A first analysis of the resulting datasets indicates the\ninfluence of catchment size for wastewater-based epidemiology as smaller\ncommunities both reveal a signal threshold before any relation with infection\ndynamics is visible and also a higher sensitivity towards infection clusters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:46:56 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Arabzadeh", "Rezgar", ""], ["Gruenbacher", "Daniel Martin", ""], ["Insam", "Heribert", ""], ["Kreuzinger", "Norbert", ""], ["Markt", "Rudolf", ""], ["Rauch", "Wolfgang", ""]]}, {"id": "2104.11594", "submitter": "Mohsen Rezapour", "authors": "Bahareh Afhami, Mohsen Rezapour, Mohsen Madadi, Vahed Maroufy", "title": "Dynamic investment portfolio optimization using a Multivariate Merton\n  Model with Correlated Jump Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the optimization of a dynamic investment\nportfolio when the securities which follow a multivariate Merton model with\ndependent jumps are periodically invested and proceed by approximating the\nCondition-Value-at-Risk (CVaR) by comonotonic bounds and maximize the expected\nterminal wealth. Numerical studies as well as applications of our results to\nreal datasets are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:08:31 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Afhami", "Bahareh", ""], ["Rezapour", "Mohsen", ""], ["Madadi", "Mohsen", ""], ["Maroufy", "Vahed", ""]]}, {"id": "2104.11683", "submitter": "Wauter Bosma", "authors": "Wauter Bosma, Sander Dalm, Erwin van Eijk, Rachid el Harchaoui, Edwin\n  Rijgersberg, Hannah Tereza Tops, Alle Veenstra, Rolf Ypma", "title": "Establishing phone-pair co-usage by comparing mobility patterns", "comments": null, "journal-ref": "Science & Justice 2 (2020)", "doi": "10.1016/j.scijus.2019.10.005", "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In forensic investigations it is often of value to establish whether two\nphones were used by the same person during a given time period. We present a\nmethod that uses time and location of cell tower registrations of mobile phones\nto assess the strength of evidence that any pair of phones were used by the\nsame person. The method is transparent as it uses logistic regression to\ndiscriminate between the hypotheses of same and different user, and a standard\nkernel density estimation to quantify the weight of evidence in terms of a\nlikelihood ratio. We further add to previous theoretical work by training and\nvalidating our method on real world data, paving the way for application in\npractice. The method shows good performance under different modeling choices\nand robustness under lower quantity or quality of data. We discuss practical\nusage in court.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:21:38 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 05:56:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bosma", "Wauter", ""], ["Dalm", "Sander", ""], ["van Eijk", "Erwin", ""], ["Harchaoui", "Rachid el", ""], ["Rijgersberg", "Edwin", ""], ["Tops", "Hannah Tereza", ""], ["Veenstra", "Alle", ""], ["Ypma", "Rolf", ""]]}, {"id": "2104.11702", "submitter": "Ryan Dew", "authors": "Ryan Dew, Yuhao Fan", "title": "A Gaussian Process Model of Cross-Category Dynamics in Brand Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding individual customers' sensitivities to prices, promotions,\nbrand, and other aspects of the marketing mix is fundamental to a wide swath of\nmarketing problems, including targeting and pricing. Companies that operate\nacross many product categories have a unique opportunity, insofar as they can\nuse purchasing data from one category to augment their insights in another.\nSuch cross-category insights are especially crucial in situations where\npurchasing data may be rich in one category, and scarce in another. An\nimportant aspect of how consumers behave across categories is dynamics:\npreferences are not stable over time, and changes in individual-level\npreference parameters in one category may be indicative of changes in other\ncategories, especially if those changes are driven by external factors. Yet,\ndespite the rich history of modeling cross-category preferences, the marketing\nliterature lacks a framework that flexibly accounts for \\textit{correlated\ndynamics}, or the cross-category interlinkages of individual-level sensitivity\ndynamics. In this work, we propose such a framework, leveraging\nindividual-level, latent, multi-output Gaussian processes to build a\nnonparametric Bayesian choice model that allows information sharing of\npreference parameters across customers, time, and categories. We apply our\nmodel to grocery purchase data, and show that our model detects interesting\ndynamics of customers' price sensitivities across multiple categories.\nManagerially, we show that capturing correlated dynamics yields substantial\npredictive gains, relative to benchmarks. Moreover, we find that capturing\ncorrelated dynamics can have implications for understanding changes in\nconsumers preferences over time, and developing targeted marketing strategies\nbased on those dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:43:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dew", "Ryan", ""], ["Fan", "Yuhao", ""]]}, {"id": "2104.11722", "submitter": "Filippo Carone Fabiani", "authors": "Filippo Carone Fabiani", "title": "Asymptotic Incidence Rate estimation of SARS-COVID-19 via a Polya\n  process scheme: a comparative analysis in Italy and European Countries", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During an ongoing epidemic, especially in the case of a new agent, data are\npartial and sparse, also affected by external factors as for example climatic\neffects or preparedness and response capability of healthcare structures.\nDespite that we showed how, under some universality assumptions, it is possible\nto extract strategic insights by modelling the pandemic trough a probabilistic\nPolya urn scheme. In the Polya framework, we provide both the distribution of\ninfected cases and the asymptotic estimation of the incidence rate, showing\nthat data are consistent with a general underlying process at different scales.\nUsing European confirmed cases and diagnostic test data on COVID-19 , we\nprovided an extensive comparison among European countries and between Europe\nand Italy at regional scale, for both the two big waves of infection. We\nglobally estimated an incidence rate in accordance with previous studies. On\nthe other hand, this quantity could play a crucial role as a proxy variable for\nan unbiased estimation of the real incidence rate, including symptomatic and\nasymptomatic cases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:16:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Fabiani", "Filippo Carone", ""]]}, {"id": "2104.11726", "submitter": "Maria Tresita Paul V", "authors": "Maria Tresita Paul V., N. Uma Devi", "title": "Managing mental & psychological wellbeing amidst COVID-19 pandemic:\n  Positive psychology interventions", "comments": "10 Pages, 3 Figures, 3 Tables", "journal-ref": "The American Journal of Humanities and Social Sciences Research,\n  2021, Vol. 4, Issue 3, Pages 121-131", "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 pandemic has shaken the roots of healthcare facilities worldwide,\nwith the US being one of the most affected countries irrespective of being a\nsuperpower. Along with the current pandemic, COVID-19 can cause a secondary\ncrisis of mental health pandemic if left unignored. Various studies from past\nepidemics, financial turmoil and pandemic, especially SARS and MERS, have shown\na steep increase in mental and psychological issues like depression, low\nquality of life, self-harm and suicidal tendencies among general populations.\nThe most venerable being the individuals infected and cured due to social\ndiscrimination. The government is taking steps to contain and prevent further\ninfections of COVID-19. However, the mental and psychological wellbeing of\npeople is still left ignored in developing countries like India. There is a\nsignificant gap in India concerning mental and psychological health still being\nstigmatized and considered 'non-existent'. This study's effort is to highlight\nthe importance of mental and psychological health and to suggest interventions\nbased on positive psychology literature. These interventions can support the\nwellbeing of people acting as a psychological first aid. Keywords: COVID-19,\nCoronavirus, Pandemic, Mental wellbeing, Psychological Wellbeing, Positive\nPsychology Interventions.\n  KEYWORDS - COVID-19, Coronavirus, Pandemic, Wellbeing, Positive Psychology,\nInterventions, PPI.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:22:05 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 03:32:24 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 04:50:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["V.", "Maria Tresita Paul", ""], ["Devi", "N. Uma", ""]]}, {"id": "2104.12055", "submitter": "Md Easin Hasan", "authors": "Fahad B. Mostafa and Md Easin Hasan", "title": "Machine Learning Approaches for Binary Classification to Discover Liver\n  Diseases using Clinical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a medical diagnosis, health professionals use different kinds of\npathological ways to make a decision for medical reports in terms of patients\nmedical condition. In the modern era, because of the advantage of computers and\ntechnologies, one can collect data and visualize many hidden outcomes from\nthem. Statistical machine learning algorithms based on specific problems can\nassist one to make decisions. Machine learning data driven algorithms can be\nused to validate existing methods and help researchers to suggest potential new\ndecisions. In this paper, multiple imputation by chained equations was applied\nto deal with missing data, and Principal Component Analysis to reduce the\ndimensionality. To reveal significant findings, data visualizations were\nimplemented. We presented and compared many binary classifier machine learning\nalgorithms (Artificial Neural Network, Random Forest, Support Vector Machine)\nwhich were used to classify blood donors and non-blood donors with hepatitis,\nfibrosis and cirrhosis diseases. From the data published in UCI-MLR [1], all\nmentioned techniques were applied to find one better method to classify blood\ndonors and non-blood donors (hepatitis, fibrosis, and cirrhosis) that can help\nhealth professionals in a laboratory to make better decisions. Our proposed\nML-method showed better accuracy score (e.g. 98.23% for SVM). Thus, it improved\nthe quality of classification.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 04:10:19 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:34:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mostafa", "Fahad B.", ""], ["Hasan", "Md Easin", ""]]}, {"id": "2104.12231", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Leon A. Gatys, Joseph Futoma, Emily B. Fox", "title": "Model-based metrics: Sample-efficient estimates of predictive model\n  subpopulation performance", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models $-$ now commonly developed to screen, diagnose, or\npredict health conditions $-$ are evaluated with a variety of performance\nmetrics. An important first step in assessing the practical utility of a model\nis to evaluate its average performance over an entire population of interest.\nIn many settings, it is also critical that the model makes good predictions\nwithin predefined subpopulations. For instance, showing that a model is fair or\nequitable requires evaluating the model's performance in different demographic\nsubgroups. However, subpopulation performance metrics are typically computed\nusing only data from that subgroup, resulting in higher variance estimates for\nsmaller groups. We devise a procedure to measure subpopulation performance that\ncan be more sample-efficient than the typical subsample estimates. We propose\nusing an evaluation model $-$ a model that describes the conditional\ndistribution of the predictive model score $-$ to form model-based metric (MBM)\nestimates. Our procedure incorporates model checking and validation, and we\npropose a computationally efficient approximation of the traditional\nnonparametric bootstrap to form confidence intervals. We evaluate MBMs on two\nmain tasks: a semi-synthetic setting where ground truth metrics are available\nand a real-world hospital readmission prediction task. We find that MBMs\nconsistently produce more accurate and lower variance estimates of model\nperformance for small subpopulations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:06:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Miller", "Andrew C.", ""], ["Gatys", "Leon A.", ""], ["Futoma", "Joseph", ""], ["Fox", "Emily B.", ""]]}, {"id": "2104.12431", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas, Han Lin Shang, Zaher Mundher Yaseen", "title": "A functional autoregressive model based on exogenous hydrometeorological\n  variables for river flow prediction", "comments": "42 pages, 13 figures, to appear at the Journal of Hydrology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, a functional time series model was introduced to predict\nfuture realizations of river flow time series. The proposed model was\nconstructed based on a functional time series's correlated lags and the\nessential exogenous climate variables. Rainfall, temperature, and evaporation\nvariables were hypothesized to have substantial functionality in river flow\nsimulation. Because an actual time series model is unspecified and the input\nvariables' significance for the learning process is unknown in practice, it was\nemployed a variable selection procedure to determine only the significant\nvariables for the model. A nonparametric bootstrap model was also proposed to\ninvestigate predictions' uncertainty and construct pointwise prediction\nintervals for the river flow curve time series. Historical datasets at three\nmeteorological stations (Mosul, Baghdad, and Kut) located in the semi-arid\nregion, Iraq, were used for model development. The prediction performance of\nthe proposed model was validated against existing functional and traditional\ntime series models. The numerical analyses revealed that the proposed model\nprovides competitive or even better performance than the benchmark models.\nAlso, the incorporated exogenous climate variables have substantially improved\nthe modeling predictability performance. Overall, the proposed model indicated\na reliable methodology for modeling river flow within the semi-arid region.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:42:00 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""], ["Yaseen", "Zaher Mundher", ""]]}, {"id": "2104.12492", "submitter": "Varun Ramamohan", "authors": "Mohd Shoaib and Varun Ramamohan", "title": "Simulation Modelling and Analysis of Primary Health Centre Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present discrete-event simulation models of the operations of primary\nhealth centres (PHCs) in the Indian context. Our PHC simulation models\nincorporate four types of patients seeking medical care: outpatients,\ninpatients, childbirth cases, and patients seeking antenatal care. A generic\nmodelling approach was adopted to develop simulation models of PHC operations.\nThis involved developing an archetype PHC simulation, which was then adapted to\nrepresent two other PHC configurations, differing in numbers of resources and\ntypes of services provided, encountered during PHC visits. A model representing\na benchmark configuration conforming to government-mandated operational\nguidelines, with demand estimated from disease burden data and service times\ncloser to international estimates (higher than observed), was also developed.\nSimulation outcomes for the three observed configurations indicate negligible\npatient waiting times and low resource utilisation values at observed patient\ndemand estimates. However, simulation outcomes for the benchmark configuration\nindicated significantly higher resource utilisation. Simulation experiments to\nevaluate the effect of potential changes in operational patterns on reducing\nthe utilisation of stressed resources for the benchmark case were performed.\nOur analysis also motivated the development of simple analytical approximations\nof the average utilisation of a server in a queueing system with\ncharacteristics similar to the PHC doctor/patient system. Our study represents\nthe first step in an ongoing effort to establish the computational\ninfrastructure required to analyse public health operations in India, and can\nprovide researchers in other settings with hierarchical health systems a\ntemplate for the development of simulation models of their primary healthcare\nfacilities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:03:45 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 19:36:07 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Shoaib", "Mohd", ""], ["Ramamohan", "Varun", ""]]}, {"id": "2104.12516", "submitter": "Mark Green", "authors": "Mark Green", "title": "Evaluating the performance of personal, social, health-related,\n  biomarker and genetic data for predicting an individuals future health using\n  machine learning: A longitudinal analysis", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As we gain access to a greater depth and range of health-related information\nabout individuals, three questions arise: (1) Can we build better models to\npredict individual-level risk of ill health? (2) How much data do we need to\neffectively predict ill health? (3) Are new methods required to process the\nadded complexity that new forms of data bring? The aim of the study is to apply\na machine learning approach to identify the relative contribution of personal,\nsocial, health-related, biomarker and genetic data as predictors of future\nhealth in individuals. Using longitudinal data from 6830 individuals in the UK\nfrom Understanding Society (2010-12 to 2015-17), the study compares the\npredictive performance of five types of measures: personal (e.g. age, sex),\nsocial (e.g. occupation, education), health-related (e.g. body weight, grip\nstrength), biomarker (e.g. cholesterol, hormones) and genetic single nucleotide\npolymorphisms (SNPs). The predicted outcome variable was limiting long-term\nillness one and five years from baseline. Two machine learning approaches were\nused to build predictive models: deep learning via neural networks and XGBoost\n(gradient boosting decision trees). Model fit was compared to traditional\nlogistic regression models. Results found that health-related measures had the\nstrongest prediction of future health status, with genetic data performing\npoorly. Machine learning models only offered marginal improvements in model\naccuracy when compared to logistic regression models, but also performed well\non other metrics e.g. neural networks were best on AUC and XGBoost on\nprecision. The study suggests that increasing complexity of data and methods\ndoes not necessarily translate to improved understanding of the determinants of\nhealth or performance of predictive models of ill health.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:31:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Green", "Mark", ""]]}, {"id": "2104.12555", "submitter": "Siruo Wang", "authors": "Siruo Wang, Leah R. Jager, Kai Kammers, Aboozar Hadavand, Jeffrey T.\n  Leek", "title": "Linking open-source code commits and MOOC grades to evaluate massive\n  online open peer review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Massive Open Online Courses (MOOCs) have been used by students as a low-cost\nand low-touch educational credential in a variety of fields. Understanding the\ngrading mechanisms behind these course assignments is important for evaluating\nMOOC credentials. A common approach to grading free-response assignments is\nmassive scale peer-review, especially used for assignments that are not easy to\ngrade programmatically. It is difficult to assess these approaches since the\nresponses typically require human evaluation. Here we link data from public\ncode repositories on GitHub and course grades for a large massive-online open\ncourse to study the dynamics of massive scale peer review. This has important\nimplications for understanding the dynamics of difficult to grade assignments.\nSince the research was not hypothesis-driven, we described the results in an\nexploratory framework. We find three distinct clusters of repeated peer-review\nsubmissions and use these clusters to study how grades change in response to\nchanges in code submissions. Our exploration also leads to an important\nobservation that massive scale peer-review scores are highly variable,\nincrease, on average, with repeated submissions, and changes in scores are not\nclosely tied to the code changes that form the basis for the re-submissions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:27:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Siruo", ""], ["Jager", "Leah R.", ""], ["Kammers", "Kai", ""], ["Hadavand", "Aboozar", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "2104.12639", "submitter": "Imke Mayer", "authors": "Imke Mayer, Julie Josse, Traumabase Group", "title": "Transporting treatment effects with incomplete attributes", "comments": "preprint, 47 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The simultaneous availability of experimental and observational data to\nestimate a treatment effect is both an opportunity and a statistical challenge:\nCombining the information gathered from both data is a promising avenue to\nbuild upon the internal validity of randomized controlled trials (RCTs) and a\ngreater external validity of observational data, but it raises methodological\nissues, especially due to different sampling designs inducing distributional\nshifts. We focus on the aim of transporting a causal effect estimated on an RCT\nonto a target population described by a set of covariates. Available methods\nsuch as inverse propensity weighting are not designed to handle missing values,\nwhich are however common in both data. In addition to coupling the assumptions\nfor causal identifiability and for the missing values mechanism and to defining\nappropriate strategies, one has to consider the specific structure of the data\nwith two sources and treatment and outcome only available in the RCT. We study\ndifferent approaches and their underlying assumptions on the data generating\nprocesses and distribution of missing values and suggest several adapted\nmethods, in particular multiple imputation strategies. These methods are\nassessed in an extensive simulation study and practical guidelines are provided\nfor different scenarios. This work is motivated by the analysis of a large\nregistry of over 20,000 major trauma patients and a multi-centered RCT studying\nthe effect of tranexamic acid administration on mortality. The analysis\nillustrates how the missing values handling can impact the conclusion about the\neffect transported from the RCT to the target population.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:09:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mayer", "Imke", ""], ["Josse", "Julie", ""], ["Group", "Traumabase", ""]]}, {"id": "2104.12672", "submitter": "Yiqiao Yin", "authors": "Shaw-Hwa Lo, Yiqiao Yin", "title": "A Novel Interaction-based Methodology Towards Explainable AI with Better\n  Understanding of Pneumonia Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the field of eXplainable AI (XAI), robust \"blackbox\" algorithms such as\nConvolutional Neural Networks (CNNs) are known for making high prediction\nperformance. However, the ability to explain and interpret these algorithms\nstill require innovation in the understanding of influential and, more\nimportantly, explainable features that directly or indirectly impact the\nperformance of predictivity. A number of methods existing in literature focus\non visualization techniques but the concepts of explainability and\ninterpretability still require rigorous definition. In view of the above needs,\nthis paper proposes an interaction-based methodology -- Influence Score\n(I-score) -- to screen out the noisy and non-informative variables in the\nimages hence it nourishes an environment with explainable and interpretable\nfeatures that are directly associated to feature predictivity. We apply the\nproposed method on a real world application in Pneumonia Chest X-ray Image data\nset and produced state-of-the-art results. We demonstrate how to apply the\nproposed approach for more general big data problems by improving the\nexplainability and interpretability without sacrificing the prediction\nperformance. The contribution of this paper opens a novel angle that moves the\ncommunity closer to the future pipelines of XAI problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 23:02:43 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:57:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:29:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lo", "Shaw-Hwa", ""], ["Yin", "Yiqiao", ""]]}, {"id": "2104.12769", "submitter": "William Ruth", "authors": "William Ruth and Richard Lockhart", "title": "Network Analysis of SFU Course Registrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamics of disease infection via shared classes at Simon\nFraser University, a medium-sized school in Western Canada. Specifically, we\nuse simulation to examine the impact of keeping classes above a certain size\nonline, while those below that size are allowed to meet in person. We use\nsimple models for infection and recovery, as well as multiple regimes for\ninfectiousness and class size threshold. Graph theoretic properties of the\nstudent-class enrollment network are also computed, and one such statistic is\nused to model an important output of our simulations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 21:26:16 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 22:30:08 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ruth", "William", ""], ["Lockhart", "Richard", ""]]}, {"id": "2104.12852", "submitter": "Christopher Blier-Wong", "authors": "Christopher Blier-Wong and H\\'el\\`ene Cossette and Luc Lamontagne and\n  Etienne Marceau", "title": "Geographic ratemaking with spatial embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial data is a rich source of information for actuarial applications:\nknowledge of a risk's location could improve an insurance company's ratemaking,\nreserving or risk management processes. Insurance companies with high exposures\nin a territory typically have a competitive advantage since they may use\nhistorical losses in a region to model spatial risk non-parametrically. Relying\non geographic losses is problematic for areas where past loss data is\nunavailable. This paper presents a method based on data (instead of smoothing\nhistorical insurance claim losses) to construct a geographic ratemaking model.\nIn particular, we construct spatial features within a complex representation\nmodel, then use the features as inputs to a simpler predictive model (like a\ngeneralized linear model). Our approach generates predictions with smaller bias\nand smaller variance than other spatial interpolation models such as bivariate\nsplines in most situations. This method also enables us to generate rates in\nterritories with no historical experience.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:09:45 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Blier-Wong", "Christopher", ""], ["Cossette", "H\u00e9l\u00e8ne", ""], ["Lamontagne", "Luc", ""], ["Marceau", "Etienne", ""]]}, {"id": "2104.12919", "submitter": "Xu Wu", "authors": "Xu Wu, Ziyu Xie, Farah Alsafadi, Tomasz Kozlowski", "title": "A Comprehensive Survey of Inverse Uncertainty Quantification of Physical\n  Model Parameters in Nuclear System Thermal-Hydraulics Codes", "comments": "76 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty Quantification (UQ) is an essential step in computational model\nvalidation because assessment of the model accuracy requires a concrete,\nquantifiable measure of uncertainty in the model predictions. The concept of UQ\nin the nuclear community generally means forward UQ (FUQ), in which the\ninformation flow is from the inputs to the outputs. Inverse UQ (IUQ), in which\nthe information flow is from the model outputs and experimental data to the\ninputs, is an equally important component of UQ but has been significantly\nunderrated until recently. FUQ requires knowledge in the input uncertainties\nwhich has been specified by expert opinion or user self-evaluation. IUQ is\ndefined as the process to inversely quantify the input uncertainties based on\nexperimental data. This review paper aims to provide a comprehensive and\ncomparative discussion of the major aspects of the IUQ methodologies that have\nbeen used on the physical models in system thermal-hydraulics codes. IUQ\nmethods can be categorized by three main groups: frequentist (deterministic),\nBayesian (probabilistic), and empirical (design-of-experiments). We used eight\nmetrics to evaluate an IUQ method, including solidity, complexity,\naccessibility, independence, flexibility, comprehensiveness, transparency, and\ntractability. Twelve IUQ methods are reviewed, compared, and evaluated based on\nthese eight metrics. Such comparative evaluation will provide a good guidance\nfor users to select a proper IUQ method based on the IUQ problem under\ninvestigation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:23:56 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wu", "Xu", ""], ["Xie", "Ziyu", ""], ["Alsafadi", "Farah", ""], ["Kozlowski", "Tomasz", ""]]}, {"id": "2104.13081", "submitter": "Thorsten Dickhaus", "authors": "Anh-Tuan Hoang and Thorsten Dickhaus", "title": "Combining independent p-values in replicability analysis: A comparative\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Given a family of null hypotheses $H_{1},\\ldots,H_{s}$, we are interested in\nthe hypothesis $H_{s}^{\\gamma}$ that at most $\\gamma-1$ of these null\nhypotheses are false. Assuming that the corresponding $p$-values are\nindependent, we are investigating combined $p$-values that are valid for\ntesting $H_{s}^{\\gamma}$. In various settings in which $H_{s}^{\\gamma}$ is\nfalse, we determine which combined $p$-value works well in which setting. Via\nsimulations, we find that the Stouffer method works well if the null $p$-values\nare uniformly distributed and the signal strength is low, and the Fisher method\nworks better if the null $p$-values are conservative, i.e. stochastically\nlarger than the uniform distribution. The minimum method works well if the\nevidence for the rejection of $H_{s}^{\\gamma}$ is focused on only a few\nnon-null $p$-values, especially if the null $p$-values are conservative.\nMethods that incorporate the combination of $e$-values work well if the null\nhypotheses $H_{1},\\ldots,H_{s}$ are simple.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 09:56:42 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Hoang", "Anh-Tuan", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "2104.13201", "submitter": "Maximilian Croci", "authors": "Maximilian L. Croci, Ushnish Sengupta, Matthew P. Juniper", "title": "Online parameter inference for the simulation of a Bunsen flame using\n  heteroscedastic Bayesian neural network ensembles", "comments": "6 pages, 3 figures", "journal-ref": "ICLR 2021 Deep Learning for Simulation Workshop", "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a Bayesian data-driven machine learning method for the\nonline inference of the parameters of a G-equation model of a ducted, premixed\nflame. Heteroscedastic Bayesian neural network ensembles are trained on a\nlibrary of 1.7 million flame fronts simulated in LSGEN2D, a G-equation solver,\nto learn the Bayesian posterior distribution of the model parameters given\nobservations. The ensembles are then used to infer the parameters of Bunsen\nflame experiments so that the dynamics of these can be simulated in LSGEN2D.\nThis allows the surface area variation of the flame edge, a proxy for the heat\nrelease rate, to be calculated. The proposed method provides cheap and online\nparameter and uncertainty estimates matching results obtained with the ensemble\nKalman filter, at less computational cost. This enables fast and reliable\nsimulation of the combustion process.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:47:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Croci", "Maximilian L.", ""], ["Sengupta", "Ushnish", ""], ["Juniper", "Matthew P.", ""]]}, {"id": "2104.13246", "submitter": "Fran\\c{c}ois Waldner", "authors": "Michele Meroni, Fran\\c{c}ois Waldner, Lorenzo Seguini, Herv\\'e\n  Kerdiles, Felix Rembold", "title": "Yield forecasting with machine learning and small data: what gains for\n  grains?", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting crop yields is important for food security, in particular to\npredict where crop production is likely to drop. Climate records and\nremotely-sensed data have become instrumental sources of data for crop yield\nforecasting systems. Similarly, machine learning methods are increasingly used\nto process big Earth observation data. However, access to data necessary to\ntrain such algorithms is often limited in food-insecure countries. Here, we\nevaluate the performance of machine learning algorithms and small data to\nforecast yield on a monthly basis between the start and the end of the growing\nseason. To do so, we developed a robust and automated machine-learning pipeline\nwhich selects the best features and model for prediction. Taking Algeria as\ncase study, we predicted national yields for barley, soft wheat and durum wheat\nwith an accuracy of 0.16-0.2 t/ha (13-14 % of mean yield) within the season.\nThe best machine-learning models always outperformed simple benchmark models.\nThis was confirmed in low-yielding years, which is particularly relevant for\nearly warning. Nonetheless, the differences in accuracy between machine\nlearning and benchmark models were not always of practical significance.\nBesides, the benchmark models outperformed up to 60% of the machine learning\nmodels that were tested, which stresses the importance of proper model\ncalibration and selection. For crop yield forecasting, like for many\napplication domains, machine learning has delivered significant improvement in\npredictive power. Nonetheless, superiority over simple benchmarks is often\nfully achieved after extensive calibration, especially when dealing with small\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:57:41 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 07:14:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Meroni", "Michele", ""], ["Waldner", "Fran\u00e7ois", ""], ["Seguini", "Lorenzo", ""], ["Kerdiles", "Herv\u00e9", ""], ["Rembold", "Felix", ""]]}, {"id": "2104.13588", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami and Tomoko Matsui", "title": "Improved log-Gaussian approximation for over-dispersed Poisson\n  regression: application to spatial analysis of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of open data, Poisson and other count regression models are\nincreasingly important. Provided this, we develop a closed-form inference for\nan over-dispersed Poisson regression, especially for (over-dispersed) Bayesian\nPoisson wherein the exact inference is unobtainable. The approach is derived\nvia mode-based log-Gaussian approximation. Unlike closed-form alternatives, it\nremains accurate even for zero-inflated count data. Besides, our approach has\nno arbitrary parameter that must be determined a priori. Monte Carlo\nexperiments demonstrate that the estimation error of the proposed method is a\nconsiderably smaller estimation error than the closed-form alternatives and as\nsmall as the usual Poisson regressions. We obtained similar results in the case\nof Poisson additive mixed modeling considering spatial or group effects. The\ndeveloped method was applied for analyzing COVID-19 data in Japan. This result\nsuggests that influences of pedestrian density, age, and other factors on the\nnumber of cases change over periods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:43:04 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:24:04 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murakami", "Daisuke", ""], ["Matsui", "Tomoko", ""]]}, {"id": "2104.13705", "submitter": "Suchandan Kayal", "authors": "Suchandan Kayal", "title": "Failure extropy, dynamic failure extropy and their weighted versions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extropy was introduced as a dual complement of the Shannon entropy. In this\ninvestigation, we consider failure extropy and its dynamic version. Various\nbasic properties of these measures are presented. It is shown that the dynamic\nfailure extropy characterizes the distribution function uniquely. We also\nconsider weighted versions of these measures. Several virtues of the weighted\nmeasures are explored. Finally, nonparametric estimators are introduced based\non the empirical distribution function.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:05:37 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kayal", "Suchandan", ""]]}, {"id": "2104.13747", "submitter": "Fabian Stephany", "authors": "Fabian Stephany, Hanno Lorenz", "title": "The Future of Employment Revisited: How Model Selection Determines\n  Automation Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The uniqueness of human labour is at question in times of smart technologies.\nThe 250 years-old discussion on technological unemployment reawakens.\nProminently, Frey and Osborne (2017) estimated that half of US employment will\nbe automated by algorithms within the next 20 years. Other follow-up studies\nconclude that only a small fraction of workers will be replaced by digital\ntechnologies. The main contribution of our work is to show that the diversity\nof previous findings regarding the degree of job automation is, to a large\nextent, driven by model selection and not by controlling for personal\ncharacteristics or tasks. For our case study, we consult experts in machine\nlearning and industry professionals on the susceptibility to digital\ntechnologies in the Austrian labour market. Our results indicate that, while\nclerical computer-based routine jobs are likely to change in the next decade,\nprofessional activities, such as the processing of complex information, are\nless prone to digital change.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:27:49 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Stephany", "Fabian", ""], ["Lorenz", "Hanno", ""]]}, {"id": "2104.13865", "submitter": "Jiarui Liu", "authors": "Jiarui Liu", "title": "Sequential Search Models: A Pairwise Maximum Rank Approach", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sequential search models that (1) incorporate unobserved\nproduct quality, which can be correlated with endogenous observable\ncharacteristics (such as price) and endogenous search cost variables (such as\nproduct rankings in online search intermediaries); and (2) do not require\nresearchers to know the true distribution of the match value between consumers\nand products. A likelihood approach to estimate such models gives biased\nresults. Therefore, I propose a new estimator -- pairwise maximum rank (PMR)\nestimator -- for both preference and search cost parameters. I show that the\nPMR estimator is consistent using only data on consumers' search order among\none pair of products rather than data on consumers' full consideration set or\nfinal purchase. Additionally, we can use the PMR estimator to test for the true\nmatch value distribution in the data. In the empirical application, I apply the\nPMR estimator to quantify the effect of rankings in Expedia hotel search using\ntwo samples of the data set, to which consumers are randomly assigned. I find\nthe position effect to be \\$0.11-\\$0.36, and the effect estimated using the\nsample with randomly generated rankings is close to the effect estimated using\nthe sample with endogenous rankings. Moreover, I find that the true match value\ndistribution in the data is unlikely to be N(0,1). Likelihood estimation\nignoring endogeneity gives an upward bias of at least \\$1.17; misspecification\nof match value distribution as N(0,1) gives an upward bias of at least \\$2.99.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:19:56 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Liu", "Jiarui", ""]]}, {"id": "2104.13957", "submitter": "Xi Jiang", "authors": "Xi Jiang, Qiwei Li and Guanghua Xiao", "title": "BOOST-Ising: Bayesian Modeling of Spatial Transcriptomics Data via Ising\n  Model", "comments": "Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent technology breakthrough in spatial molecular profiling has enabled the\ncomprehensive molecular characterizations of single cells while preserving\nspatial information. It provides new opportunities to delineate how cells from\ndifferent origins form tissues with distinctive structures and functions. One\nimmediate question in analysis of spatial molecular profiling data is how to\nidentify spatially variable genes. Most of the current methods build upon the\ngeostatistical model with a Gaussian process that relies on selecting ad hoc\nkernels to account for spatial expression patterns. To overcome this potential\nchallenge and capture more types of spatial patterns, we introduce a Bayesian\napproach to identify spatially variable genes via Ising model. The key idea is\nto use the energy interaction parameter of the Ising model to characterize\nspatial expression patterns. We use auxiliary variable Markov chain Monte Carlo\nalgorithms to sample from the posterior distribution with an intractable\nnormalizing constant in the Ising model. Simulation results show that our\nenergy-based modeling approach led to higher accuracy in detecting spatially\nvariable genes than those kernel-based methods. Applying our method to two real\nspatial transcriptomics datasets, we discovered novel spatial patterns that\nshed light on the biological mechanisms. The proposed method presents a new\nperspective for analyzing spatial transcriptomics data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:27:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 00:37:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Jiang", "Xi", ""], ["Li", "Qiwei", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2104.14033", "submitter": "Anirbit Mukherjee", "authors": "Anirbit Mukherjee", "title": "A Study of the Mathematics of Deep Learning", "comments": "(A) Our PAC-Bayes risk bounds on neural nets given in Section 6 here\n  does not yet occur in any other file on arXiv. (B) In our paper\n  arXiv:/2005.04211, there is a significantly improved version of Section 3.3\n  of this thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"Deep Learning\"/\"Deep Neural Nets\" is a technological marvel that is now\nincreasingly deployed at the cutting-edge of artificial intelligence tasks.\nThis dramatic success of deep learning in the last few years has been hinged on\nan enormous amount of heuristics and it has turned out to be a serious\nmathematical challenge to be able to rigorously explain them. In this thesis,\nsubmitted to the Department of Applied Mathematics and Statistics, Johns\nHopkins University we take several steps towards building strong theoretical\nfoundations for these new paradigms of deep-learning. In chapter 2 we show new\ncircuit complexity theorems for deep neural functions and prove classification\ntheorems about these function spaces which in turn lead to exact algorithms for\nempirical risk minimization for depth 2 ReLU nets. We also motivate a measure\nof complexity of neural functions to constructively establish the existence of\nhigh-complexity neural functions. In chapter 3 we give the first algorithm\nwhich can train a ReLU gate in the realizable setting in linear time in an\nalmost distribution free set up. In chapter 4 we give rigorous proofs towards\nexplaining the phenomenon of autoencoders being able to do sparse-coding. In\nchapter 5 we give the first-of-its-kind proofs of convergence for stochastic\nand deterministic versions of the widely used adaptive gradient deep-learning\nalgorithms, RMSProp and ADAM. This chapter also includes a detailed empirical\nstudy on autoencoders of the hyper-parameter values at which modern algorithms\nhave a significant advantage over classical acceleration based methods. In the\nlast chapter 6 we give new and improved PAC-Bayesian bounds for the risk of\nstochastic neural nets. This chapter also includes an experimental\ninvestigation revealing new geometric properties of the paths in weight space\nthat are traced out by the net during the training.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:05:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Mukherjee", "Anirbit", ""]]}, {"id": "2104.14036", "submitter": "Benjamin Haibe-Kains", "authors": "Petr Smirnov, Ian Smith, Zhaleh Safikhani, Wail Ba-alawi, Farnoosh\n  Khodakarami, Eva Lin, Yihong Yu, Scott Martin, Janosch Ortmann, Tero\n  Aittokallio, Marc Hafner, Benjamin Haibe-Kains", "title": "Evaluation of statistical approaches for association testing in noisy\n  drug screening data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying associations among biological variables is a major challenge in\nmodern quantitative biological research, particularly given the systemic and\nstatistical noise endemic to biological systems. Drug sensitivity data has\nproven to be a particularly challenging field for identifying associations to\ninform patient treatment. To address this, we introduce two semi-parametric\nvariations on the commonly used Concordance Index: the robust Concordance Index\nand the kernelized Concordance Index (rCI, kCI), which incorporate measurements\nabout the noise distribution from the data. We demonstrate that common\nstatistical tests applied to the concordance index and its variations fail to\ncontrol for false positives, and introduce efficient implementations to compute\np-values using adaptive permutation testing. We then evaluate the statistical\npower of these coefficients under simulation and compare with Pearson and\nSpearman correlation coefficients. Finally, we evaluate the various statistics\nin matching drugs across pharmacogenomic datasets. We observe that the rCI and\nkCI are better powered than the concordance index in simulation and show some\nimprovement on real data. Surprisingly, we observe that the Pearson correlation\nwas the most robust to measurement noise among the different metrics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:23:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Smirnov", "Petr", ""], ["Smith", "Ian", ""], ["Safikhani", "Zhaleh", ""], ["Ba-alawi", "Wail", ""], ["Khodakarami", "Farnoosh", ""], ["Lin", "Eva", ""], ["Yu", "Yihong", ""], ["Martin", "Scott", ""], ["Ortmann", "Janosch", ""], ["Aittokallio", "Tero", ""], ["Hafner", "Marc", ""], ["Haibe-Kains", "Benjamin", ""]]}, {"id": "2104.14054", "submitter": "David Frazier", "authors": "David T. Frazier, Ruben Loaiza-Maya, Gael M. Martin and Bonsoo Koo", "title": "Loss-Based Variational Bayes Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for Bayesian prediction that caters for models with a\nlarge number of parameters and is robust to model misspecification. Given a\nclass of high-dimensional (but parametric) predictive models, this new approach\nconstructs a posterior predictive using a variational approximation to a\nloss-based, or Gibbs, posterior that is directly focused on predictive\naccuracy. The theoretical behavior of the new prediction approach is analyzed\nand a form of optimality demonstrated. Applications to both simulated and\nempirical data using high-dimensional Bayesian neural network and\nautoregressive mixture models demonstrate that the approach provides more\naccurate results than various alternatives, including misspecified\nlikelihood-based predictions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:36:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Frazier", "David T.", ""], ["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""], ["Koo", "Bonsoo", ""]]}, {"id": "2104.14204", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski, Florian Ziel", "title": "Optimal bidding on hourly and quarter-hourly day-ahead electricity price\n  auctions: trading large volumes of power with market impact and transaction\n  costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.MF q-fin.PM q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity exchanges offer several trading possibilities for market\nparticipants: starting with futures products through the spot market consisting\nof the auction and continuous part, and ending with the balancing market. This\nvariety of choice creates a new question for traders - when to trade to\nmaximize the gain. This problem is not trivial especially for trading larger\nvolumes as the market participants should also consider their own price impact.\nThe following paper raises this issue considering two markets: the hourly EPEX\nDay-Ahead Auction and the quarter-hourly EPEX Intraday Auction. We consider a\nrealistic setting which includes a forecasting study and a suitable evaluation.\nFor a meaningful optimization many price scenarios are considered that we\nobtain using bootstrap with models that are well-known and researched in the\nelectricity price forecasting literature. The own market impact is predicted by\nmimicking the demand or supply shift in the respectful auction curves. A number\nof trading strategies is considered, e.g. minimization of the trading costs,\nrisk neutral or risk averse agents. Additionally, we provide theoretical\nresults for risk neutral agents. Especially we show when the optimal trading\npath coincides with the solution that minimizes transaction costs. The\napplication study is conducted using the German market data, but the presented\nmethods can be easily utilized with other two auction-based markets. They could\nbe also generalized to other market types, what is discussed in the paper as\nwell. The empirical results show that market participants could increase their\ngains significantly compared to simple benchmark strategies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:52:18 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "2104.14243", "submitter": "Jonathan Rathjens", "authors": "Jonathan Rathjens, Arthur Kolbe, J\\\"urgen H\\\"olzer, Katja Ickstadt and\n  Nadja Klein", "title": "Bivariate Analysis of Birth Weight and Gestational Age Depending on\n  Environmental Exposures: Bayesian Distributional Regression with Copulas", "comments": "25 pages, 7 figures (some of them composed from several pdf files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we analyze perinatal data with birth weight (BW) as\nprimarily interesting response variable. Gestational age (GA) is usually an\nimportant covariate and included in polynomial form. However, in opposition to\nthis univariate regression, bivariate modeling of BW and GA is recommended to\ndistinguish effects on each, on both, and between them. Rather than a\nparametric bivariate distribution, we apply conditional copula regression,\nwhere marginal distributions of BW and GA (not necessarily of the same form)\ncan be estimated independently, and where the dependence structure is modeled\nconditional on the covariates separately from these marginals. In the resulting\ndistributional regression models, all parameters of the two marginals and the\ncopula parameter are observation-specific. Besides biometric and obstetric\ninformation, data on drinking water contamination and maternal smoking are\nincluded as environmental covariates. While the Gaussian distribution is\nsuitable for BW, the skewed GA data are better modeled by the three-parametric\nDagum distribution. The Clayton copula performs better than the Gumbel and the\nsymmetric Gaussian copula, indicating lower tail dependence (stronger\ndependence when both variables are low), although this non-linear dependence\nbetween BW and GA is surprisingly weak and only influenced by Cesarean section.\nA non-linear trend of BW on GA is detected by a classical univariate model that\nis polynomial with respect to the effect of GA. Linear effects on BW mean are\nsimilar in both models, while our distributional copula regression also reveals\ncovariates' effects on all other parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:20:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Rathjens", "Jonathan", ""], ["Kolbe", "Arthur", ""], ["H\u00f6lzer", "J\u00fcrgen", ""], ["Ickstadt", "Katja", ""], ["Klein", "Nadja", ""]]}, {"id": "2104.14281", "submitter": "Yongzhen Wang", "authors": "Yongzhen Wang, Xiaozhong Liu, Katy B\\\"orner, Jun Lin, Yingnan Ju,\n  Changlong Sun, Luo Si", "title": "Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle\n  Choices: New Insights into Chronic Disease Prevention Literacy", "comments": "47 pages with SI, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous internet access is reshaping the way we live, but it is\naccompanied by unprecedented challenges in preventing chronic diseases planted\nby long exposure to unhealthy lifestyles. This paper proposes leveraging online\nshopping behaviors as a proxy for personal lifestyle choices to improve chronic\ndisease prevention literacy targeted for times when e-commerce user experience\nhas been assimilated into most people's daily lives. Here, retrospective\nlongitudinal query logs and purchase records from millions of online shoppers\nwere accessed, constructing a broad spectrum of lifestyle features covering\nassorted product categories and buyer personas. Using the lifestyle-related\ninformation preceding their first purchases of prescription drugs, we could\ndetermine associations between online shoppers' past lifestyle choices and\nwhether they suffered from a particular chronic disease or not. Novel lifestyle\nrisk factors were discovered in two exemplars -- depression and diabetes, most\nof which showed cognitive congruence with existing healthcare knowledge.\nFurther, such empirical findings could be adopted to locate online shoppers at\nhigh risk of these chronic diseases with fair accuracy, closely matching the\nperformance of screening surveys benchmarked against medical diagnosis.\nUnobtrusive chronic disease surveillance via e-commerce sites may soon meet\nconsenting individuals in the digital space they already inhabit.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 12:05:16 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 00:54:23 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 12:08:12 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Yongzhen", ""], ["Liu", "Xiaozhong", ""], ["B\u00f6rner", "Katy", ""], ["Lin", "Jun", ""], ["Ju", "Yingnan", ""], ["Sun", "Changlong", ""], ["Si", "Luo", ""]]}, {"id": "2104.14291", "submitter": "Aaron Fisher", "authors": "Aaron Fisher", "title": "Optimizing Rescoring Rules with Interpretable Representations of\n  Long-Term Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing temporal data (e.g., wearable device data) requires a decision\nabout how to combine information from the recent and distant past. In the\ncontext of classifying sleep status from actigraphy, Webster's rescoring rules\noffer one popular solution based on the long-term patterns in the output of a\nmoving-window model. Unfortunately, the question of how to optimize rescoring\nrules for any given setting has remained unsolved. To address this problem and\nexpand the possible use cases of rescoring rules, we propose rephrasing these\nrules in terms of epoch-specific features. Our features take two general forms:\n(1) the time lag between now and the most recent [or closest upcoming] bout of\ntime spent in a given state, and (2) the length of the most recent [or closest\nupcoming] bout of time spent in a given state. Given any initial moving window\nmodel, these features can be defined recursively, allowing for straightforward\noptimization of rescoring rules. Joint optimization of the moving window model\nand the subsequent rescoring rules can also be implemented using gradient-based\noptimization software, such as Tensorflow. Beyond binary classification\nproblems (e.g., sleep-wake), the same approach can be applied to summarize\nlong-term patterns for multi-state classification problems (e.g., sitting,\nwalking, or stair climbing). We find that optimized rescoring rules improve the\nperformance of sleep-wake classifiers, achieving accuracy comparable to that of\ncertain neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:30:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Fisher", "Aaron", ""]]}, {"id": "2104.14483", "submitter": "Michael Crowther", "authors": "Jonathan Broomfield, Caroline E. Weibull, Michael J. Crowther", "title": "Assessing and relaxing the Markov assumption in the illness-death model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-state survival analysis considers several potential events of interest\nalong a disease pathway. Such analyses are crucial to model complex patient\ntrajectories and are increasingly being used in epidemiological and health\neconomic settings. Multi-state models often make the Markov assumption, whereby\nan individual's future trajectory is dependent only upon their present state,\nnot their past. In reality, there may be transitional dependence upon either\nprevious events and/or more than one timescale, for example time since entry to\nthe current or previous state(s). The aim of this study was to develop an\nillness-death Weibull model allowing for multiple timescales to impact the\nfuture risk of death. Following this, we evaluated the performance of the\nmultiple timescale model against a Markov illness-death model in a set of\nplausible simulation scenarios when the Markov assumption was violated. Guided\nby a study in breast cancer, data were simulated from Weibull baseline\ndistributions, with hazard functions dependent on single and multiple\ntimescales. Markov and non-Markov models were fitted to account for/ignore the\nunderlying data structure. Ignoring the presence of multiple timescales led to\nbias in underlying transition rates between states and associated covariate\neffects, while transition probabilities and lengths of stay were fairly\nrobustly estimated. Further work may be needed to evaluate different estimands\nor more complex multi-state models. Software implementations in Stata are also\ndescribed for simulating and estimating multiple timescale multi-state models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:47:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Broomfield", "Jonathan", ""], ["Weibull", "Caroline E.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2104.14695", "submitter": "Tae Kim", "authors": "Tae Hyun Kim, Dan Nicolae", "title": "Dynamic Gene Coexpression Analysis with Correlation Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many transcriptomic studies, the correlation of genes might fluctuate with\nquantitative factors such as genetic ancestry. We propose a method that models\nthe covariance between two variables to vary against a continuous covariate.\nFor the bivariate case, the proposed score test statistic is computationally\nsimple and robust to model misspecification of the covariance term.\nSubsequently, the method is expanded to test relationships between one highly\nconnected gene, such as a transcription factor, and several other genes for a\nmore global investigation of the dynamic of the coexpression network.\nSimulations show that the proposed method has higher statistical power than\nalternatives, can be used in more diverse scenarios, and is computationally\ncheaper. We apply this method to African American subjects from GTEx to analyze\nthe dynamic behavior of their gene coexpression against genetic ancestry and to\nidentify transcription factors whose coexpression with their target genes\nchange with the genetic ancestry. The proposed method can be applied to a wide\narray of problems that require covariance modeling.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:17:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kim", "Tae Hyun", ""], ["Nicolae", "Dan", ""]]}, {"id": "2104.14725", "submitter": "Nan Shen", "authors": "Nan Shen, B\\'arbara Gonz\\'alez", "title": "Bayesian Information Criterion for Linear Mixed-effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Bayesian information criterion (BIC) in the model selection\nprocedure is under the assumption that the observations are independent and\nidentically distributed (i.i.d.). However, in practice, we do not always have\ni.i.d. samples. For example, clustered observations tend to be more similar\nwithin the same group, and longitudinal data is collected by measuring the same\nsubject repeatedly. In these scenarios, the assumption in BIC is not satisfied.\nThe concept of effective sample size is brought up and improved BIC is defined\nby replacing the sample size in the original BIC expression with the effective\nsample size, which will give us a better theoretical foundation in the\ncircumstance that mixed-effects models involve. Numerical experiment results\nare also given by comparing the performance of our new BIC with other widely\nused BICs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:16:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Shen", "Nan", ""], ["Gonz\u00e1lez", "B\u00e1rbara", ""]]}, {"id": "2104.14813", "submitter": "Davide Ferrari", "authors": "Davide Ferrari, Steven Stillman, Mirco Tonin", "title": "Does Covid-19 Mass Testing Work? The Importance of Accounting for the\n  Epidemic Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mass antigen testing has been proposed as a possible cost-effective tool to\ncontain the Covid-19 pandemic. We test the impact of a voluntary mass testing\ncampaign implemented in the Italian region of South Tyrol on the spread of the\nvirus in the following months. We do so by using an innovative empirical\napproach which embeds a semi-parametric growth model - where Covid-19\ntransmission dynamics are allowed to vary across regions and to be impacted by\nthe implementation of the mass testing campaign - into a synthetic control\nframework which creates an appropriate control group of other Italian regions.\nWe find that the mass test campaign decreased the growth rate of Covid-19 by\n39% which corresponds to a reduction in the total additional cases of 18%, 30%\nand 56% within ten, twenty and forty days from the intervention date,\nrespectively. Our results suggest that mass testing campaigns are useful\ninstruments for mitigating the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:58:24 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ferrari", "Davide", ""], ["Stillman", "Steven", ""], ["Tonin", "Mirco", ""]]}, {"id": "2104.14910", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and \\'Agnes Baran", "title": "Calibration of wind speed ensemble forecasts for power generation", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades wind power became the second largest energy source in the\nEU covering 16% of its electricity demand. However, due to its volatility,\naccurate short range wind power predictions are required for successful\nintegration of wind energy into the electrical grid. Accurate predictions of\nwind power require accurate hub height wind speed forecasts, where the state of\nthe art method is the probabilistic approach based on ensemble forecasts\nobtained from multiple runs of numerical weather prediction models.\nNonetheless, ensemble forecasts are often uncalibrated and might also be\nbiased, thus require some form of post-processing to improve their predictive\nperformance. We propose a novel flexible machine learning approach for\ncalibrating wind speed ensemble forecasts, which results in a truncated normal\npredictive distribution. In a case study based on 100m wind speed forecasts\nproduced by the operational ensemble prediction system of the Hungarian\nMeteorological Service, the forecast skill of this method is compared with the\npredictive performance of three different ensemble model output statistics\napproaches and the raw ensemble forecasts. We show that compared with the raw\nensemble, post-processing always improves the calibration of probabilistic and\naccuracy of point forecasts and from the four competing methods the novel\nmachine learning based approach results in the best overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:18:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 10:12:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Baran", "\u00c1gnes", ""]]}, {"id": "2104.14923", "submitter": "Helen Barnett", "authors": "Helen Yvette Barnett, Matthew George, Donia Skanji, Gaelle\n  Saint-Hilary, Thomas Jaki, Pavel Mozgunov", "title": "A Comparison of Model-Free Phase I Dose Escalation Designs for\n  Dual-Agent Combination Therapies", "comments": "29 pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly common for therapies in oncology to be given in\ncombination. In some cases, patients can benefit from the interaction between\ntwo drugs, although often at the risk of higher toxicity. A large number of\ndesigns to conduct phase I trials in this setting are available, where the\nobjective is to select the maximum tolerated dose combination (MTC). Recently,\na number of model-free (also called model-assisted) designs have provoked\ninterest, providing several practical advantages over the more conventional\napproaches of rule-based or model-based designs. In this paper, we demonstrate\na novel calibration procedure for model-free designs to determine their most\ndesirable parameters. Under the calibration procedure, we compare the behaviour\nof model-free designs to a model-based approach in a comprehensive simulation\nstudy, covering a number of clinically plausible scenarios. It is found that\nmodel-free designs are competitive with the model-based design in terms of the\nproportion of correct selections of the MTC. However, there are a number of\nscenarios in which model-free designs offer a safer alternative. This is also\nillustrated in the application of the designs to a case study using data from a\nphase I oncology trial.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:40:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Barnett", "Helen Yvette", ""], ["George", "Matthew", ""], ["Skanji", "Donia", ""], ["Saint-Hilary", "Gaelle", ""], ["Jaki", "Thomas", ""], ["Mozgunov", "Pavel", ""]]}, {"id": "2104.14987", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Marc Goodfellow", "title": "Emulating computationally expensive dynamical simulators using Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Gaussian process (GP)-based methodology is proposed to emulate\ncomputationally expensive dynamical computer models or simulators. The method\nrelies on emulating the short-time numerical flow map of the model. The flow\nmap returns the solution of a dynamic system at an arbitrary time for a given\ninitial condition. The prediction of the flow map is performed via a GP whose\nkernel is estimated using random Fourier features. This gives a distribution\nover the flow map such that each realisation serves as an approximation to the\nflow map. A realisation is then employed in an iterative manner to perform\none-step ahead predictions and forecast the whole time series. Repeating this\nprocedure with multiple draws from the emulated flow map provides a probability\ndistribution over the time series. The mean and variance of that distribution\nare used as the model output prediction and a measure of the associated\nuncertainty, respectively. The proposed method is used to emulate several\ndynamic non-linear simulators including the well-known Lorenz attractor and van\nder Pol oscillator. The results show that our approach has a high prediction\nperformance in emulating such systems with an accurate representation of the\nprediction uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:45:44 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:56:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Goodfellow", "Marc", ""]]}, {"id": "2104.15028", "submitter": "Claude Renaux", "authors": "Claude Renaux, Peter B\\\"uhlmann", "title": "Efficient Multiple Testing Adjustment for Hierarchical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical inference in (generalized) regression problems is powerful for\nfinding significant groups or even single covariates, especially in\nhigh-dimensional settings where identifiability of the entire regression\nparameter vector may be ill-posed. The general method proceeds in a fully\ndata-driven and adaptive way from large to small groups or singletons of\ncovariates, depending on the signal strength and the correlation structure of\nthe design matrix. We propose a novel hierarchical multiple testing adjustment\nthat can be used in combination with any significance test for a group of\ncovariates to perform hierarchical inference. Our adjustment passes on the\nsignificance level of certain hypotheses that could not be rejected and is\nshown to guarantee strong control of the familywise error rate. Our method is\nat least as powerful as a so-called depth-wise hierarchical Bonferroni\nadjustment. It provides a substantial gain in power over other previously\nproposed inheritance hierarchical procedures if the underlying alternative\nhypotheses occur sparsely along a few branches in the tree-structured\nhierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:30:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Renaux", "Claude", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2104.15043", "submitter": "Marios Kondakis", "authors": "Marios Kondakis, Nikolaos Demiris, Ioannis Ntzoufras and Nikos E.\n  Papanikolaou", "title": "Inference and model determination for Temperature-Driven non-linear\n  Ecological Models", "comments": "48 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with a contemporary Bayesian approach to the effect\nof temperature on developmental rates. We develop statistical methods using\nrecent computational tools to model four commonly used ecological non-linear\nmathematical curves that describe arthropods' developmental rates. Such models\naddress the effect of temperature fluctuations on the developmental rate of\narthropods. In addition to the widely used Gaussian distributional assumption,\nwe also explore Inverse Gamma--based alternatives, which naturally accommodate\nadaptive variance fluctuation with temperature. Moreover, to overcome the\nassociated parameter indeterminacy in the case of no development, we suggest\nthe Zero Inflated Inverse Gamma model. The ecological models are compared\ngraphically via posterior predictive plots and quantitatively via Marginal\nlikelihood estimates and Information criteria values. Inference is performed\nusing the Stan software and we investigate the statistical and computational\nefficiency of its Hamiltonian Monte Carlo and Variational Inference methods. We\nexplore model uncertainty and use Bayesian Model Averaging framework for robust\nestimation of the key ecological parameters\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:11:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kondakis", "Marios", ""], ["Demiris", "Nikolaos", ""], ["Ntzoufras", "Ioannis", ""], ["Papanikolaou", "Nikos E.", ""]]}, {"id": "2104.15086", "submitter": "Helen Barnett", "authors": "Helen Barnett, Oliver Boix, Dimintris Kontos and Thomas Jaki", "title": "Dose Finding Studies for Therapies with Late-Onset Toxicities: A\n  Comparison Study of Designs", "comments": "34 Pages, 5 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An objective of phase I dose-finding trials is to find the maximum tolerated\ndose; the dose with a particular risk of toxicity. Frequently, this risk is\nassessed across the first cycle of therapy. However, in oncology, a course of\ntreatment frequently consists of multiple cycles of therapy. In many cases, the\noverall risk of toxicity for a given treatment is not fully encapsulated by\nobservations from the first cycle, and hence it is advantageous to include\ntoxicity outcomes from later cycles in phase I trials. Extending the follow up\nperiod in a trial naturally extends the total length of the trial which is\nundesirable. We present a comparison of eight methods that incorporate late\nonset toxicities whilst not extensively extending the trial length. We conduct\nsimulation studies over a number of scenarios and in two settings; the first\nsetting with minimal stopping rules and the second setting with a full set of\nstandard stopping rules expected in such a dose finding study. We find that the\nmodel-based approaches in general outperform the model-assisted approaches,\nwith an Interval Censored approach and a modified version of the Time-to-Event\nContinual Reassessment Method giving the most promising overall performance in\nterms of correct selections and trial length. Further recommendations are made\nfor the implementation of such methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:07:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Barnett", "Helen", ""], ["Boix", "Oliver", ""], ["Kontos", "Dimintris", ""], ["Jaki", "Thomas", ""]]}, {"id": "2104.15087", "submitter": "Wanrudee Skulpakdee", "authors": "Wanrudee Skulpakdee and Mongkol Hunkrajok", "title": "Models Based on Exponential Interarrival Times for Single-Unusual-Event\n  Count Data", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At least one unusual event appears in some count datasets. It will lead to a\nmore concentrated (or dispersed) distribution than the Poisson, the gamma, the\nWeibull, and the Conway-Maxwell-Poisson (CMP) can accommodate. These well-known\ncount models are based on the equal rates of interarrival times between\nsuccessive events. Under the assumption of unequal rates (one unusual event)\nand independent exponential interarrival times, a new class of parametric\nmodels for single-unusual-event (SUE) count data is proposed. These two models\nare applied to two empirical applications, the number of births and the number\nof bids, and yield considerably better results to the above well-known count\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:11:39 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:52:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Skulpakdee", "Wanrudee", ""], ["Hunkrajok", "Mongkol", ""]]}]