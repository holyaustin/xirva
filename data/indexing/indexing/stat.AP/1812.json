[{"id": "1812.00022", "submitter": "Laina Mercer", "authors": "Laina D. Mercer, Fred Lu, and Joshua L. Proctor", "title": "Sub-national levels and trends in contraceptive prevalence, unmet need,\n  and demand for family planning in Nigeria with survey uncertainty", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambitious global goals have been established to provide universal access to\naffordable modern contraceptive methods. The UN's sustainable development goal\n3.7.1 proposes satisfying the demand for family planning (FP) services by\nincreasing the proportion of women of reproductive age using modern methods. To\nmeasure progress toward such goals in populous countries like Nigeria, it's\nessential to characterize the current levels and trends of FP indicators such\nas unmet need and modern contraceptive prevalence rates (mCPR). Moreover, the\nsubstantial heterogeneity across Nigeria and scale of programmatic\nimplementation requires a sub-national resolution of these FP indicators.\nHowever, significant challenges face estimating FP indicators sub-nationally in\nNigeria. In this article, we develop a robust, data-driven model to utilize all\navailable surveys to estimate the levels and trends of FP indicators in\nNigerian states for all women and by age-parity demographic subgroups. We\nestimate that overall rates and trends of mCPR and unmet need have remained low\nin Nigeria: the average annual rate of change for mCPR by state is 0.5%\n(0.4%,0.6%) from 2012-2017. Unmet need by age-parity demographic groups varied\nsignificantly across Nigeria; parous women express much higher rates of unmet\nneed than nulliparous women. Our hierarchical Bayesian model incorporates data\nfrom a diverse set of survey instruments, accounts for survey uncertainty,\nleverages spatio-temporal smoothing, and produces probabilistic estimates with\nuncertainty intervals. Our flexible modeling framework directly informs\nprogrammatic decision-making by identifying age-parity-state subgroups with\nlarge rates of unmet need, highlights conflicting trends across survey\ninstruments, and holistically interprets direct survey estimates.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 19:06:42 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Mercer", "Laina D.", ""], ["Lu", "Fred", ""], ["Proctor", "Joshua L.", ""]]}, {"id": "1812.00055", "submitter": "Yili Hong", "authors": "Lu Lu and I-Chen Lee and Yili Hong", "title": "Bayesian Sequential Design Based on Dual Objectives for Accelerated Life\n  Tests", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional accelerated life test plans are typically based on optimizing the\nC-optimality for minimizing the variance of an interested quantile of the\nlifetime distribution. The traditional methods rely on some specified planning\nvalues for the model parameters, which are usually unknown prior to the actual\ntests. The ambiguity of the specified parameters can lead to suboptimal designs\nfor optimizing the intended reliability performance. In this paper, we propose\na sequential design strategy for life test plans based on considering dual\nobjectives. In the early stage of the sequential experiment, we suggest to\nallocate more design locations based on optimizing the D-optimality to quickly\ngain precision in the estimated model parameters. In the later stage of the\nexperiment, we can allocate more samples based on optimizing the C-optimality\nto maximize the precision of the estimated quantile of the lifetime\ndistribution. We compare the proposed sequential design strategy with existing\ntest plans considering only a single criterion and illustrate the new method\nwith an example on fatigue testing of polymer composites.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 20:54:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lu", "Lu", ""], ["Lee", "I-Chen", ""], ["Hong", "Yili", ""]]}, {"id": "1812.00080", "submitter": "Ionut-Gabriel Farcas", "authors": "Ionut-Gabriel Farcas, Tobias G\\\"orler, Hans-Joachim Bungartz, Frank\n  Jenko and Tobias Neckel", "title": "Sensitivity-driven adaptive sparse stochastic approximations in plasma\n  microinstability analysis", "comments": "34 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA math.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying uncertainty in predictive simulations for real-world problems is\nof paramount importance - and far from trivial, mainly due to the large number\nof stochastic parameters and significant computational requirements. Adaptive\nsparse grid approximations are an established approach to overcome these\nchallenges. However, standard adaptivity is based on global information, thus\nproperties such as lower intrinsic stochastic dimensionality or anisotropic\ncoupling of the input directions, which are common in practical applications,\nare not fully exploited. We propose a novel structure-exploiting\ndimension-adaptive sparse grid approximation methodology using Sobol'\ndecompositions in each subspace to introduce a sensitivity scoring system to\ndrive the adaptive process. By employing local sensitivity information, we\nexplore and exploit the anisotropic coupling of the stochastic inputs as well\nas the lower intrinsic stochastic dimensionality. The proposed approach is\ngeneric, i.e., it can be formulated in terms of arbitrary approximation\noperators and point sets. In particular, we consider sparse grid interpolation\nand pseudo-spectral projection constructed on (L)-Leja sequences. The power and\nusefulness of the proposed method is demonstrated by applying it to the\nanalysis of gyrokinetic microinstabilities in fusion plasmas, one of the key\nscientific problems in plasma physics and fusion research. In this context, it\nis shown that a 12D parameter space can be scanned very efficiently, gaining\nmore than an order of magnitude in computational cost over the standard\nadaptive approach. Moreover, it allows for the uncertainty propagation and\nsensitivity analysis in higher-dimensional plasma microturbulence problems,\nwhich would be almost impossible to tackle with standard screening approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 22:41:34 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:28:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Farcas", "Ionut-Gabriel", ""], ["G\u00f6rler", "Tobias", ""], ["Bungartz", "Hans-Joachim", ""], ["Jenko", "Frank", ""], ["Neckel", "Tobias", ""]]}, {"id": "1812.00096", "submitter": "Han Lin Shang", "authors": "Han Lin Shang, Yang Yang, Fearghal Kearney", "title": "Intraday forecasts of a volatility index: Functional time series methods\n  with dynamic updating", "comments": "29 pages, 5 figures, To appear at the Annals of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a forward-looking measure of future equity market volatility, the VIX\nindex has gained immense popularity in recent years to become a key measure of\nrisk for market analysts and academics. We consider discrete reported intraday\nVIX tick values as realisations of a collection of curves observed sequentially\non equally spaced and dense grids over time and utilise functional data\nanalysis techniques to produce one-day-ahead forecasts of these curves. The\nproposed method facilitates the investigation of dynamic changes in the index\nover very short time intervals as showcased using the 15-second high-frequency\nVIX index values. With the help of dynamic updating techniques, our point and\ninterval forecasts are shown to enjoy improved accuracy over conventional time\nseries models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:39:46 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Shang", "Han Lin", ""], ["Yang", "Yang", ""], ["Kearney", "Fearghal", ""]]}, {"id": "1812.00215", "submitter": "Bo Zhang", "authors": "Bo Zhang and Dylan Small", "title": "A calibrated sensitivity analysis for matched observational studies with\n  application to the effect of second-hand smoke exposure on blood lead levels\n  in U.S. children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matched observational studies are commonly used to study treatment effects in\nnon-randomized data. After matching for observed confounders, there could\nremain bias from unobserved confounders. A standard way to address this problem\nis to do a sensitivity analysis. A sensitivity analysis asks how sensitive the\nresult is to a hypothesized unmeasured confounder U. One method, known as\nsimultaneous sensitivity analysis, has two sensitivity parameters: one relating\nU to treatment assignment and the other to response. This method assumes that\nin each matched set, U is distributed to make the bias worst. This approach has\ntwo concerning features. First, this worst case distribution of U in each\nmatched set does not correspond to a realistic distribution of U in the\npopulation. Second, sensitivity parameters are in absolute scales which are\nhard to compare to observed covariates. We address these concerns by\nintroducing a method that endows U with a probability distribution in the\npopulation and calibrates the unmeasured confounder to the observed covariates.\nWe compare our method to simultaneous sensitivity analysis in simulations and\nin a study of the effect of second-hand smoke exposure on blood lead levels in\nU.S. children.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 15:25:44 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 20:44:28 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 23:45:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Bo", ""], ["Small", "Dylan", ""]]}, {"id": "1812.00250", "submitter": "Wenge Guo", "authors": "Zhiying Qiu, Li Yu, Wenge Guo", "title": "A Family-based Graphical Approach for Testing Hierarchically Ordered\n  Families of Hypotheses", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.23109.29929", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications of clinical trials, tested hypotheses are often grouped as\nmultiple hierarchically ordered families. To test such structured hypotheses,\nvarious gatekeeping strategies have been developed in the literature, such as\nseries gatekeeping, parallel gatekeeping, tree-structured gatekeeping\nstrategies, etc. However, these gatekeeping strategies are often either\nnon-intuitive or less flexible when addressing increasingly complex logical\nrelationships among families of hypotheses. In order to overcome the issue, in\nthis paper, we develop a new family-based graphical approach, which can easily\nderive and visualize different gatekeeping strategies. In the proposed\napproach, a directed and weighted graph is used to represent the generated\ngatekeeping strategy where each node corresponds to a family of hypotheses and\ntwo simple updating rules are used for updating the critical value of each\nfamily and the transition coefficient between any two families. Theoretically,\nwe show that the proposed graphical approach strongly controls the overall\nfamilywise error rate at a pre-specified level. Through some case studies and a\nreal clinical example, we demonstrate simplicity and flexibility of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:38:14 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Qiu", "Zhiying", ""], ["Yu", "Li", ""], ["Guo", "Wenge", ""]]}, {"id": "1812.00251", "submitter": "Oscar Alberto Quijano Xacur", "authors": "Oscar Alberto Quijano Xacur", "title": "The Unifed Distribution", "comments": "arXiv admin note: text overlap with arXiv:1710.08553", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new distribution with support on (0,1) called unifed. It can\nbe used as the response distribution for a GLM and it is suitable for data\naggregation. We make a comparison to the beta regression. A link to an R\npackage for working with the unifed is provided.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:38:14 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 02:23:14 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Xacur", "Oscar Alberto Quijano", ""]]}, {"id": "1812.00269", "submitter": "Matthias Fischer", "authors": "Matthias M. Fischer", "title": "Quantifying the uncertainty of variance partitioning estimates of\n  ecological datasets", "comments": "19 pages, 7 figures", "journal-ref": "Environmental and Ecological Statistics (2019), Volume 26, Issue\n  4, pp 351-366", "doi": "10.1007/s10651-019-00431-6", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important objective of experimental biology is the quantification of the\nrelationship between predictor and response variables, a statistical analysis\noften termed variance partitioning (VP). In this paper, a series of simulations\nis presented, aiming to generate quantitative estimates of the expected\nstatistical uncertainty of VP analyses. We demonstrate scenarios with\nconsiderable uncertainty in VP estimates, such that it significantly reduces\nthe statistical reliability of the obtained results. Especially when a\npredictor variable of a dataset shows a low between-group variance, VP\nestimates may show a high margin of error. This becomes particularly important\nwhen the respective predictor variable only explains a small fraction of the\noverall variance, or the number of replicates is particularly small. Moreover,\nit is demonstrated that the expected error of VP estimates of a dataset can be\napproximated by bootstrap resampling, giving researchers a tool for the\nquantification of the uncertainty associated with an arbitrary VP analysis. The\napplicability of this method is demonstrated by a re-analysis of the Oribatid\nmite dataset introduced by Borcard and Legendre in 1994 and the Barro Colorado\nIsland tree count dataset by Condit and colleagues. We believe that this study\nmay encourage biologists to approach routine statistical analyses such as VP\nmore critically, and report the error associated with them more frequently.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 21:40:40 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 15:56:50 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Fischer", "Matthias M.", ""]]}, {"id": "1812.00318", "submitter": "Richard Scalzo", "authors": "Richard Scalzo, David Kohn, Hugo Olierook, Gregory Houseman, Rohitash\n  Chandra, Mark Girolami, and Sally Cripps", "title": "Efficiency and robustness in Monte Carlo sampling of 3-D geophysical\n  inversions with Obsidian v0.1.2: Setting up for success", "comments": "17 pages, 5 figures, 1 table; submitted to Geoscientific Model\n  Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rigorous quantification of uncertainty in geophysical inversions is a\nchallenging problem. Inversions are often ill-posed and the likelihood surface\nmay be multi-modal; properties of any single mode become inadequate uncertainty\nmeasures, and sampling methods become inefficient for irregular posteriors or\nhigh-dimensional parameter spaces. We explore the influences of different\nchoices made by the practitioner on the efficiency and accuracy of Bayesian\ngeophysical inversion methods that rely on Markov chain Monte Carlo sampling to\nassess uncertainty, using a multi-sensor inversion of the three-dimensional\nstructure and composition of a region in the Cooper Basin of South Australia as\na case study. The inversion is performed using an updated version of the\nObsidian distributed inversion software. We find that the posterior for this\ninversion has complex local covariance structure, hindering the efficiency of\nadaptive sampling methods that adjust the proposal based on the chain history.\nWithin the context of a parallel-tempered Markov chain Monte Carlo scheme for\nexploring high-dimensional multi-modal posteriors, a preconditioned\nCrank-Nicholson proposal outperforms more conventional forms of random walk.\nAspects of the problem setup, such as priors on petrophysics or on 3-D\ngeological structure, affect the shape and separation of posterior modes,\ninfluencing sampling performance as well as the inversion results. Use of\nuninformative priors on sensor noise can improve inversion results by enabling\noptimal weighting among multiple sensors even if noise levels are uncertain.\nEfficiency could be further increased by using posterior gradient information\nwithin proposals, which Obsidian does not currently support, but which could be\nemulated using posterior surrogates.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 03:58:33 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Scalzo", "Richard", ""], ["Kohn", "David", ""], ["Olierook", "Hugo", ""], ["Houseman", "Gregory", ""], ["Chandra", "Rohitash", ""], ["Girolami", "Mark", ""], ["Cripps", "Sally", ""]]}, {"id": "1812.00553", "submitter": "Xinyue Li", "authors": "Xinyue Li, Yunting Zhang, Fan Jiang, Hongyu Zhao", "title": "A Hidden Markov Model Based Unsupervised Algorithm for Sleep/Wake\n  Identification Using Actigraphy", "comments": "19 pages, 4 tables, 3 figures The old dataset has been replaced with\n  a new dataset to better evaluate the method by validating against PSG. Thus,\n  the evaluation metrics have all been changed. A major part of the manuscript\n  has been revised", "journal-ref": null, "doi": "10.1080/07420528.2020.1754848", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actigraphy is widely used in sleep studies but lacks a universal unsupervised\nalgorithm for sleep/wake identification. In this study, we proposed a Hidden\nMarkov Model (HMM) based unsupervised algorithm that can automatically and\neffectively infer sleep/wake states. It is an individualized data-driven\napproach that analyzes actigraphy from each individual respectively to learn\nactivity characteristics and further separate sleep and wake states. We used\nActiwatch and polysomnography (PSG) data from 43 individuals in the\nMulti-Ethnic Study of Atherosclerosis to evaluate the performance of our\nmethod. Epoch-by-epoch comparisons were made between our HMM algorithm and that\nembedded in the Actiwatch software (AS). The percent agreement between HMM and\nPSG was 85.7%, and that between AS and PSG was 84.7%. Positive predictive\nvalues for sleep epochs were 85.6% and 84.6% for HMM and AS, respectively, and\n95.5% and 85.6% for wake epochs. Both methods have similar performance and tend\nto overestimate sleep and underestimate wake compared to PSG. Our HMM approach\nis able to quantify the variability in activity counts that allow us to\ndifferentiate relatively active and sedentary individuals: individuals with\nhigher estimated variabilities tend to show more frequent sedentary behaviors.\nIn conclusion, our unsupervised data-driven HMM algorithm achieves slightly\nbetter performance compared to the commonly used algorithm in the Actiwatch\nsoftware. HMM can help expand the application of actigraphy in large-scale\nstudies and in cases where intrusive PSG is hard to acquire or unavailable. In\naddition, the estimated HMM parameters can characterize individual activity\npatterns that can be utilized for further analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:56:39 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 08:13:32 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Li", "Xinyue", ""], ["Zhang", "Yunting", ""], ["Jiang", "Fan", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1812.00561", "submitter": "Jong Hee Park", "authors": "Sooahn Shin, Hyein Yang, and Jong Hee Park", "title": "Twists and Turns in the US-North Korea Dialogue: Key Figure Dynamic\n  Network Analysis using News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for analyzing a dynamic network of key\nfigures in the U.S.-North Korea relations during the first two quarters of\n2018. Our method constructs key figure networks from U.S. news articles on\nNorth Korean issues by taking co-occurrence of people's names in an article as\na domain-relevant social link. We call a group of people that co-occur\nrepeatedly in the same domain (news articles on North Korean issues in our\ncase) \"key figures\" and their social networks \"key figure networks.\" We analyze\nblock-structure changes of key figure networks in the U.S.-North Korea\nrelations using a Bayesian hidden Markov multilinear tensor model. The results\nof our analysis show that block structure changes in the key figure network in\nthe U.S.-North Korea relations predict important game-changing moments in the\nU.S.-North Korea relations in the first two quarters of 2018.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 05:22:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Shin", "Sooahn", ""], ["Yang", "Hyein", ""], ["Park", "Jong Hee", ""]]}, {"id": "1812.00661", "submitter": "Lukas Weber", "authors": "Lukas M. Weber, Wouter Saelens, Robrecht Cannoodt, Charlotte Soneson,\n  Alexander Hapfelmeier, Paul P. Gardner, Anne-Laure Boulesteix, Yvan Saeys,\n  Mark D. Robinson", "title": "Essential guidelines for computational method benchmarking", "comments": "Minor updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In computational biology and other sciences, researchers are frequently faced\nwith a choice between several computational methods for performing data\nanalyses. Benchmarking studies aim to rigorously compare the performance of\ndifferent methods using well-characterized benchmark datasets, to determine the\nstrengths of each method or to provide recommendations regarding suitable\nchoices of methods for an analysis. However, benchmarking studies must be\ncarefully designed and implemented to provide accurate, unbiased, and\ninformative results. Here, we summarize key practical guidelines and\nrecommendations for performing high-quality benchmarking analyses, based on our\nexperiences in computational biology.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:03:26 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:48:53 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 17:57:53 GMT"}, {"version": "v4", "created": "Mon, 3 Jun 2019 13:30:00 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Weber", "Lukas M.", ""], ["Saelens", "Wouter", ""], ["Cannoodt", "Robrecht", ""], ["Soneson", "Charlotte", ""], ["Hapfelmeier", "Alexander", ""], ["Gardner", "Paul P.", ""], ["Boulesteix", "Anne-Laure", ""], ["Saeys", "Yvan", ""], ["Robinson", "Mark D.", ""]]}, {"id": "1812.00822", "submitter": "Fabian Guignard", "authors": "Fabian Guignard, Dasaraden Mauree, Michele Lovallo, Mikhail Kanevski,\n  Luciano Telesca", "title": "Fisher-Shannon complexity analysis of high-frequency urban wind speed\n  time series", "comments": "10 pages, 7 figures", "journal-ref": "Entropy 2019, 21(1), 47", "doi": "10.3390/e21010047", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  1Hz wind time series recorded at different levels (from 1.5 to 25.5 meters)\nin an urban area are investigated by using the Fisher-Shannon (FS) analysis. FS\nanalysis is a well known method to get insight of the complex behavior of\nnonlinear systems, by quantifying the order/disorder properties of time series.\nOur findings reveal that the FS complexity, defined as the product between the\nFisher Information Measure and the Shannon entropy power, decreases with the\nheight of the anemometer from the ground, suggesting a height-dependent\nvariability in the order/disorder features of the high frequency wind speed\nmeasured in urban layouts. Furthermore, the correlation between the FS\ncomplexity of wind speed and the daily variance of the ambient temperature\nshows similar decrease with the height of the wind sensor. Such correlation is\nlarger for the lower anemometers, indicating that ambient temperature is an\nimportant forcing of the wind speed variability in the vicinity of the ground.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:11:37 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Guignard", "Fabian", ""], ["Mauree", "Dasaraden", ""], ["Lovallo", "Michele", ""], ["Kanevski", "Mikhail", ""], ["Telesca", "Luciano", ""]]}, {"id": "1812.00887", "submitter": "Donghui Yan", "authors": "Donghui Yan, Timothy W. Randolph, Jian Zou and Peng Gong", "title": "Incorporating Deep Features in the Analysis of Tissue Microarray Images", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue microarray (TMA) images have been used increasingly often in cancer\nstudies and the validation of biomarkers. TACOMA---a cutting-edge automatic\nscoring algorithm for TMA images---is comparable to pathologists in terms of\naccuracy and repeatability. Here we consider how this algorithm may be further\nimproved. Inspired by the recent success of deep learning, we propose to\nincorporate representations learnable through computation. We explore\nrepresentations of a group nature through unsupervised learning, e.g.,\nhierarchical clustering and recursive space partition. Information carried by\nclustering or spatial partitioning may be more concrete than the labels when\nthe data are heterogeneous, or could help when the labels are noisy. The use of\nsuch information could be viewed as regularization in model fitting. It is\nmotivated by major challenges in TMA image scoring---heterogeneity and label\nnoise, and the cluster assumption in semi-supervised learning. Using this\ninformation on TMA images of breast cancer, we have reduced the error rate of\nTACOMA by about 6%. Further simulations on synthetic data provide insights on\nwhen such representations would likely help. Although we focus on TMAs,\nlearnable representations of this type are expected to be applicable in other\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 04:18:17 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Yan", "Donghui", ""], ["Randolph", "Timothy W.", ""], ["Zou", "Jian", ""], ["Gong", "Peng", ""]]}, {"id": "1812.01087", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, David Beymer, Ehsan Dehghan", "title": "Disease Detection in Weakly Annotated Volumetric Medical Images using a\n  Convolutional LSTM Network", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216 Medical Imaging Meets NeurIPS Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/19", "categories": "cs.CV cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a solution for learning disease signatures from weakly, yet easily\nobtainable, annotated volumetric medical imaging data by analyzing 3D volumes\nas a sequence of 2D images. We demonstrate the performance of our solution in\nthe detection of emphysema in lung cancer screening low-dose CT images. Our\napproach utilizes convolutional long short-term memory (LSTM) to \"scan\"\nsequentially through an imaging volume for the presence of disease in a portion\nof scanned region. This framework allowed effective learning given only\nvolumetric images and binary disease labels, thus enabling training from a\nlarge dataset of 6,631 un-annotated image volumes from 4,486 patients. When\nevaluated in a testing set of 2,163 volumes from 2,163 patients, our model\ndistinguished emphysema with area under the receiver operating characteristic\ncurve (AUC) of .83. This approach was found to outperform 2D convolutional\nneural networks (CNN) implemented with various multiple-instance learning\nschemes (AUC=0.69-0.76) and a 3D CNN (AUC=.77).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:32:28 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Braman", "Nathaniel", ""], ["Beymer", "David", ""], ["Dehghan", "Ehsan", ""]]}, {"id": "1812.01292", "submitter": "Chintan Amrit", "authors": "Chintan Amrit and Joanne ter Maat", "title": "Understanding Information Centrality Metric: A Simulation Approach", "comments": "I think I have made a mistake in the correlations in the analysis on\n  page 6 and in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the central people in information flow networks is essential to\nunderstanding how people communicate and coordinate as well as who controls the\ninformation flows in the network. However, the appropriate usage of centrality\nmetrics depends on an understanding of the type of network flow. Networks can\nvary in the way node-to-node transmission takes place, or in the way a course\nthrough the network is taken, thereby leading to different types of information\nflow processes. When metrics are used for an inappropriate flow process, the\nresult of the metric can be misleading and often incorrect. In this paper we\ncreate a simulation of the flow of information in a network, and then we\ninvestigate the relation of information centrality as well as other network\ncentralities, like betweenness, closeness and eigenvector along with the\noutcome of simulations with information flowing through walks rather than\npaths, trails or geodesics. We find that Information Centrality is more similar\nto Eigenvector and Degree centrality than to Closeness centrality as postulated\nby previous literature. We also find an interesting pattern emerge from the\ninter metric correlations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 09:31:23 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 11:20:29 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Amrit", "Chintan", ""], ["ter Maat", "Joanne", ""]]}, {"id": "1812.01499", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento, Andr\\'e Tarrinho, Pedro C\\^amara, Vera\n  Costa", "title": "A System for Efficient Communication between Patients and Pharmacies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying human-technology interaction systems, researchers thrive to\nachieve intuitiveness and facilitate the people's life through a thoughtful and\nin-depth study of several components of the application system that supports\nsome particular business communication with customers. Particularly in the\nhealthcare field, some requirements such as clarity, transparency, efficiency,\nand speed in transmitting information to patients and or healthcare\nprofessionals might mean an important increase in the well-being of the patient\nand productivity of the healthcare professional. In this work, the authors\nstudy the difficulties patients frequently have when communicating with\npharmacists. In addition to a statistical study of a survey conducted with more\nthan two hundred frequent pharmacy customers, we propose an IT solution for\nbetter communication between patients and pharmacists.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:01:43 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Tarrinho", "Andr\u00e9", ""], ["C\u00e2mara", "Pedro", ""], ["Costa", "Vera", ""]]}, {"id": "1812.01767", "submitter": "Qingsong Wen", "authors": "Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, Huan Xu, Shenghuo\n  Zhu", "title": "RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time\n  Series", "comments": "Accepted to the thirty-third AAAI Conference on Artificial\n  Intelligence (AAAI 2019), 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing complex time series into trend, seasonality, and remainder\ncomponents is an important task to facilitate time series anomaly detection and\nforecasting. Although numerous methods have been proposed, there are still many\ntime series characteristics exhibiting in real-world data which are not\naddressed properly, including 1) ability to handle seasonality fluctuation and\nshift, and abrupt change in trend and reminder; 2) robustness on data with\nanomalies; 3) applicability on time series with long seasonality period. In the\npaper, we propose a novel and generic time series decomposition algorithm to\naddress these challenges. Specifically, we extract the trend component robustly\nby solving a regression problem using the least absolute deviations loss with\nsparse regularization. Based on the extracted trend, we apply the the non-local\nseasonal filtering to extract the seasonality component. This process is\nrepeated until accurate decomposition is obtained. Experiments on different\nsynthetic and real-world time series datasets demonstrate that our method\noutperforms existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:01:52 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Wen", "Qingsong", ""], ["Gao", "Jingkun", ""], ["Song", "Xiaomin", ""], ["Sun", "Liang", ""], ["Xu", "Huan", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1812.01788", "submitter": "Yu Song", "authors": "Yu Song, David Noyce", "title": "Effects of Transit Signal Priority on Traffic Safety: Interrupted Time\n  Series Analysis of Portland, Oregon, Implementations", "comments": "Published in Accident Analysis & Prevention", "journal-ref": "Song, Y. & Noyce, D. 2019. Effects of Transit Signal Priority on\n  Traffic Safety: Interrupted Time Series Analysis of Portland, Oregon,\n  Implementations. Accident Analysis & Prevention, 123, 291-302", "doi": "10.1016/j.aap.2018.12.001", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transit signal priority (TSP) has been implemented to transit systems in many\ncities of the United States. In evaluating TSP systems, more attention has been\ngiven to its operational effects than to its safety effects. Existing studies\nassessing safety effects of TSP reported mixed results, indicating that the\nsafety effects of TSP vary in different contexts. In this study, TSP\nimplementations in Portland, Oregon, were assessed using interrupted time\nseries analysis (ITSA) on month-to-month changes in number of crashes from\nJanuary 1995 to December 2010. Single-group and controlled ITSA were conducted\nfor all crashes, property-damage-only crashes, fatal and injury crashes,\npedestrian-involved crashes, and bike-involved crashes. Evaluation of the\npost-intervention period (2003 to 2010) showed a reduction in all crashes on\nstreet sections with TSP (-4.5 percent), comparing with the counterfactual\nestimations based on the control group data. The reduction in\nproperty-damage-only crashes (-10.0 percent) contributed the most to the\noverall reduction. Fatal and injury crashes leveled out after TSP\nimplementation but did not change significantly comparing with the control\ngroup. Pedestrian and bike-involved crashes were found to increase in the\npost-intervention period with TSP, comparing with the control group. Potential\nreasons to these TSP effects on traffic safety were discussed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 02:31:26 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 03:26:24 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Song", "Yu", ""], ["Noyce", "David", ""]]}, {"id": "1812.02021", "submitter": "Harsha Honnappa", "authors": "Wenbo Zhang, Harsha Honnappa, Satish V. Ukkusuri", "title": "Modeling Urban Taxi Services with e-hailings: A Queueing Network\n  Approach", "comments": "Submitted to ISTTT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of e-hailing taxis has significantly altered urban transportation\nand resulted in a competitive taxi market with both traditional street-hailing\nand e-hailing taxis. The new mobility services provide similar door-to-door\nrides as the traditional one and there is competition across these various\nservices. In this study, we propose an innovative queueing network model for\nthe competitive taxi market and capture the interactions not only within the\ntaxi market but also between the taxi market and urban road system.\n  An example is designed based on data from New York City. Numerical results\nshow that the proposed modeling structure, together with the corresponding\nstationary limits, can capture dynamics within high demand areas using multiple\ndata sources. Overall, this study shows how the queueing network approach can\nmeasure both the taxi and urban road system performance at an aggregate level.\nThe model can be used to estimate not only the waiting/searching time during\npassenger-vehicle matching but also the delays in the urban road network.\nFurthermore, the model can be generalized to study the control and management\nof taxi markets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 14:44:22 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Wenbo", ""], ["Honnappa", "Harsha", ""], ["Ukkusuri", "Satish V.", ""]]}, {"id": "1812.02061", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Othman Kadmiri (LMB), Bruno\n  Saussereau (LMB)", "title": "Estimation of multivariate asymmetric power GARCH models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely accepted that volatility models have to incorporate the\nso-called leverage effect in order to to model the dynamics of daily financial\nreturns.We suggest a new class of multivariate power transformed asymmetric\nmodels. It includes several functional forms of multivariate GARCH models which\nare of great interest in financial modeling and time series literature. We\nprovide an explicit necessary and sufficient condition to establish the strict\nstationarity of the model. We derive the asymptotic properties of the\nquasi-maximum likelihood estimator of the parameters. These properties are\nestablished both when the power of the transformation is known or is unknown.\nThe asymptotic results are illustrated by Monte Carlo experiments. An\napplication to real financial data is also proposed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 15:53:57 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:18:46 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Kadmiri", "Othman", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1812.02127", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos", "title": "Information geometry for approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to explore the basic Approximate Bayesian\nComputation (ABC) algorithm via the lens of information theory. ABC is a widely\nused algorithm in cases where the likelihood of the data is hard to work with\nor intractable, but one can simulate from it. We use relative entropy ideas to\nanalyze the behavior of the algorithm as a function of the threshold parameter\nand of the size of the data. Relative entropy here is data driven as it depends\non the values of the observed statistics. Relative entropy also allows us to\nexplore the effect of the distance metric and sets up a mathematical framework\nfor sensitivity analysis allowing to find important directions which could lead\nto lower computational cost of the algorithm for the same level of accuracy. In\naddition, we also investigate the bias of the estimators for generic\nobservables as a function of both the threshold parameters and the size of the\ndata. Our analysis provides error bounds on performance for positive tolerances\nand finite sample sizes. Simulation studies complement and illustrate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:30:03 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:35:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1812.02222", "submitter": "Emma Pierson", "authors": "Bo Liu, Shuyang Shi, Yongshang Wu, Daniel Thomas, Laura Symul, Emma\n  Pierson, Jure Leskovec", "title": "Predicting pregnancy using large-scale data from a women's health\n  tracking mobile application", "comments": "Accepted at WWW 2019 (Health on the Web short paper track); an\n  earlier version of this paper was presented at the 2018 NeurIPS ML4H Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting pregnancy has been a fundamental problem in women's health for\nmore than 50 years. Previous datasets have been collected via carefully curated\nmedical studies, but the recent growth of women's health tracking mobile apps\noffers potential for reaching a much broader population. However, the\nfeasibility of predicting pregnancy from mobile health tracking data is\nunclear. Here we develop four models -- a logistic regression model, and 3 LSTM\nmodels -- to predict a woman's probability of becoming pregnant using data from\na women's health tracking app, Clue by BioWink GmbH. Evaluating our models on a\ndataset of 79 million logs from 65,276 women with ground truth pregnancy test\ndata, we show that our predicted pregnancy probabilities meaningfully stratify\nwomen: women in the top 10% of predicted probabilities have a 89% chance of\nbecoming pregnant over 6 menstrual cycles, as compared to a 27% chance for\nwomen in the bottom 10%. We develop a technique for extracting interpretable\ntime trends from our deep learning models, and show these trends are consistent\nwith previous fertility research. Our findings illustrate the potential that\nwomen's health tracking data offers for predicting pregnancy on a broader\npopulation; we conclude by discussing the steps needed to fulfill this\npotential.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 20:58:14 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 21:34:11 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Liu", "Bo", ""], ["Shi", "Shuyang", ""], ["Wu", "Yongshang", ""], ["Thomas", "Daniel", ""], ["Symul", "Laura", ""], ["Pierson", "Emma", ""], ["Leskovec", "Jure", ""]]}, {"id": "1812.02433", "submitter": "Alex Lenkoski", "authors": "Gunnhildur H. Steinbakk, Alex Lenkoski, Ragnar Bang Huseby, Anders\n  L{\\o}land, and Tor Arne {\\O}ig{\\aa}rd", "title": "Using published bid/ask curves to error dress spot electricity price\n  forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasts of electricity spot prices are essential to the daily\noperational and planning decisions made by power producers and distributors.\nTypically, point forecasts of these quantities suffice, particularly in the\nNord Pool market where the large quantity of hydro power leads to price\nstability. However, when situations become irregular, deviations on the price\nscale can often be extreme and difficult to pinpoint precisely, which is a\nresult of the highly varying marginal costs of generating facilities at the\nedges of the load curve. In these situations it is useful to supplant a point\nforecast of price with a distributional forecast, in particular one whose tails\nare adaptive to the current production regime. This work outlines a methodology\nfor leveraging published bid/ask information from the Nord Pool market to\nconstruct such adaptive predictive distributions. Our methodology is a\nnon-standard application of the concept of error-dressing, which couples a\nfeature driven error distribution in volume space with a non-linear\ntransformation via the published bid/ask curves to obtain highly non-symmetric,\nadaptive price distributions. Using data from the Nord Pool market, we show\nthat our method outperforms more standard forms of distributional modeling. We\nfurther show how such distributions can be used to render `warning systems'\nthat issue reliable probabilities of prices exceeding various important\nthresholds.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 10:09:35 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Steinbakk", "Gunnhildur H.", ""], ["Lenkoski", "Alex", ""], ["Huseby", "Ragnar Bang", ""], ["L\u00f8land", "Anders", ""], ["\u00d8ig\u00e5rd", "Tor Arne", ""]]}, {"id": "1812.02522", "submitter": "Peter O. Fedichev", "authors": "Evgeny Getmantsev, Boris Zhurov, Timothy V. Pyrkov, and Peter O.\n  Fedichev", "title": "A novel health risk model based on intraday physical activity time\n  series collected by smartphones", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compiled a demo application and collected a motion database of more than\n10,000 smartphone users to produce a health risk model trained on physical\nactivity streams. We turned to adversarial domain adaptation and employed the\nUK Biobank dataset of motion data, augmented by a rich set of clinical\ninformation as the source domain to train the model using a deep residual\nconvolutional neuron network (ResNet). The model risk score is a biomarker of\nageing, since it was predictive of lifespan and healthspan (as defined by the\nonset of specified diseases), and was elevated in groups associated with\nlife-shortening lifestyles, such as smoking. We ascertained the target domain\nperformance in a smaller cohort of the mobile application that included users\nwho were willing to share answers to a short questionnaire related to their\ndisease and smoking status. We thus conclude that the proposed pipeline\ncombining deep convolutional and Domain Adversarial neuron networks (DANN) is a\npowerful tool for disease risk and lifestyle-associated hazard assessment from\nmobile motion sensors that are transferable across devices and populations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:46:15 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Getmantsev", "Evgeny", ""], ["Zhurov", "Boris", ""], ["Pyrkov", "Timothy V.", ""], ["Fedichev", "Peter O.", ""]]}, {"id": "1812.02598", "submitter": "Danilo Bzdok", "authors": "Hao-Ting Wang, Jonathan Smallwood, Janaina Mourao-Miranda, Cedric\n  Huchuan Xia, Theodore D. Satterthwaite, Danielle S. Bassett, Danilo Bzdok", "title": "Finding the needle in high-dimensional haystack: A tutorial on canonical\n  correlation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the beginning of the 21st century, the size, breadth, and granularity\nof data in biology and medicine has grown rapidly. In the example of\nneuroscience, studies with thousands of subjects are becoming more common,\nwhich provide extensive phenotyping on the behavioral, neural, and genomic\nlevel with hundreds of variables. The complexity of such big data repositories\noffer new opportunities and pose new challenges to investigate brain,\ncognition, and disease. Canonical correlation analysis (CCA) is a prototypical\nfamily of methods for wrestling with and harvesting insight from such rich\ndatasets. This doubly-multivariate tool can simultaneously consider two\nvariable sets from different modalities to uncover essential hidden\nassociations. Our primer discusses the rationale, promises, and pitfalls of CCA\nin biomedicine.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:17:06 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Wang", "Hao-Ting", ""], ["Smallwood", "Jonathan", ""], ["Mourao-Miranda", "Janaina", ""], ["Xia", "Cedric Huchuan", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""], ["Bzdok", "Danilo", ""]]}, {"id": "1812.02687", "submitter": "Daria Rukina", "authors": "Daria Rukina", "title": "Adaptive multicenter designs for continuous response clinical trials in\n  the presence of an unknown sensitive subgroup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial effectiveness of drugs is of importance to the pharmaceutical\nindustry. Randomized controlled trials (RCTs) assuming the existence of a\nsubgroup sensitive to the treatment are already used. These designs, however,\nare available only if there is a known marker for identifying subjects in the\nsubgroup. In this paper we investigate a model in which the response in the\ntreatment group $Z^T$ has a two-component mixture density $(1-p)\\mathcal\nN(\\mu^C, \\sigma^2)+p\\mathcal N(\\mu^T, \\sigma^2)$ representing the treatment\nresponses of \\emph{placebo responders} and \\emph{drug responders}. The\ntreatment-specific effect is $\\mu = \\frac{\\mu^T-\\mu^C}{\\sigma}$ and $p$ is the\nprevalence of the drug responders in the population. Other patients in the\ntreatment group react as if they had received a placebo.\n  We develop one- and two-stage RCT designs that are able to detect a sensitive\nsubgroup based solely on the responses. We also extend them to a multicenter\nRCTs using Hochberg's step-up procedure. We avoid extensive simulations and use\nsimple and quick numerical optimization methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:07:15 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Rukina", "Daria", ""]]}, {"id": "1812.02824", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Anne S. Kiremidjian, Ram Rajagopal, Chin-Hsuing Loh", "title": "Structural Damage Detection and Localization with Unknown Post-Damage\n  Feature Distribution Using Sequential Change-Point Detection Method", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high structural deficient rate poses serious risks to the operation of\nmany bridges and buildings. To prevent critical damage and structural collapse,\na quick structural health diagnosis tool is needed during normal operation or\nimmediately after extreme events. In structural health monitoring (SHM), many\nexisting works will have limited performance in the quick damage identification\nprocess because 1) the damage event needs to be identified with short delay and\n2) the post-damage information is usually unavailable. To address these\ndrawbacks, we propose a new damage detection and localization approach based on\nstochastic time series analysis. Specifically, the damage sensitive features\nare extracted from vibration signals and follow different distributions before\nand after a damage event. Hence, we use the optimal change point detection\ntheory to find damage occurrence time. As the existing change point detectors\nrequire the post-damage feature distribution, which is unavailable in SHM, we\npropose a maximum likelihood method to learn the distribution parameters from\nthe time-series data. The proposed damage detection using estimated parameters\nalso achieves the optimal performance. Also, we utilize the detection results\nto find damage location without any further computation. Validation results\nshow highly accurate damage identification in American Society of Civil\nEngineers benchmark structure and two shake table experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 06:32:22 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Liao", "Yizheng", ""], ["Kiremidjian", "Anne S.", ""], ["Rajagopal", "Ram", ""], ["Loh", "Chin-Hsuing", ""]]}, {"id": "1812.02829", "submitter": "Katrina Devick", "authors": "Katrina L. Devick, Linda Valeri, Jarvis Chen, Alejandro Jara,\n  Marie-Ab\\`ele Bind, and Brent A. Coull", "title": "The Role of Body Mass Index at Diagnosis on Black-White Disparities in\n  Colorectal Cancer Survival: A Density Regression Mediation Approach", "comments": "15 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of racial/ethnic inequalities in health is important to reduce the\nuneven burden of disease. In the case of colorectal cancer (CRC), disparities\nin survival among non-Hispanic Whites and Blacks are well documented, and\nmechanisms leading to these disparities need to be studied formally. It has\nalso been established that body mass index (BMI) is a risk factor for\ndeveloping CRC, and recent literature shows BMI at diagnosis of CRC is\nassociated with survival. Since BMI varies by racial/ethnic group, a question\nthat arises is whether disparities in BMI is partially responsible for observed\nracial/ethnic disparities in CRC survival. This paper presents new methodology\nto quantify the impact of the hypothetical intervention that matches the BMI\ndistribution in the Black population to a potentially complex distributional\nform observed in the White population on racial/ethnic disparities in survival.\nWe perform a simulation that shows our proposed Bayesian density regression\napproach performs as well as or better than current methodology allowing for a\nshift in the mean of the distribution only, and that standard practice of\ncategorizing BMI leads to large biases. When applied to motivating data from\nthe Cancer Care Outcomes Research and Surveillance (CanCORS) Consortium, our\napproach suggests the proposed intervention is potentially beneficial for\nelderly and low income Black patients, yet harmful for young and high income\nBlack populations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:21:27 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Devick", "Katrina L.", ""], ["Valeri", "Linda", ""], ["Chen", "Jarvis", ""], ["Jara", "Alejandro", ""], ["Bind", "Marie-Ab\u00e8le", ""], ["Coull", "Brent A.", ""]]}, {"id": "1812.03258", "submitter": "Amir Karami", "authors": "Amir Karami, Aida Elkouri", "title": "Political Popularity Analysis in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularity is a critical success factor for a politician and her/his party to\nwin in elections and implement their plans. Finding the reasons behind the\npopularity can provide a stable political movement. This research attempts to\nmeasure popularity in Twitter using a mixed method. In recent years, Twitter\ndata has provided an excellent opportunity for exploring public opinions by\nanalyzing a large number of tweets. This study has collected and examined 4.5\nmillion tweets related to a US politician, Senator Bernie Sanders. This study\ninvestigated eight economic reasons behind the senator's popularity in Twitter.\nThis research has benefits for politicians, informatics experts, and\npolicymakers to explore public opinion. The collected data will also be\navailable for further investigation.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 03:17:29 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Karami", "Amir", ""], ["Elkouri", "Aida", ""]]}, {"id": "1812.03260", "submitter": "Amir Karami", "authors": "George Shaw and Amir Karami", "title": "An Exploratory Study of (#)Exercise in the Twittersphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media analytics allows us to extract, analyze, and establish semantic\nfrom user-generated contents in social media platforms. This study utilized a\nmixed method including a three-step process of data collection, topic modeling,\nand data annotation for recognizing exercise related patterns. Based on the\nfindings, 86% of the detected topics were identified as meaningful topics after\nconducting the data annotation process. The most discussed exercise-related\ntopics were physical activity (18.7%), lifestyle behaviors (6.6%), and dieting\n(4%). The results from our experiment indicate that the exploratory data\nanalysis is a practical approach to summarizing the various characteristics of\ntext data for different health and medical applications.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 03:21:26 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Shaw", "George", ""], ["Karami", "Amir", ""]]}, {"id": "1812.03311", "submitter": "Nil Kamal Hazra", "authors": "Maxim Finkelstein, Nil Kamal Hazra", "title": "Generalization of the pairwise stochastic precedence order to the\n  sequence of random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a new stochastic ordering for the sequence of independent random\nvariables. It generalizes the stochastic precedence order that is defined for\ntwo random variables to the case $n>2$. All conventional stochastic orders are\ntransitive, whereas the stochastic precedence order is not. Therefore, a new\napproach to compare the sequence of random variables had to be developed that\nresulted in the notion of the sequential precedence order. A sufficient\ncondition for this order is derived and some examples are considered.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 12:35:39 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Finkelstein", "Maxim", ""], ["Hazra", "Nil Kamal", ""]]}, {"id": "1812.03643", "submitter": "An Zeng", "authors": "An Zeng, Zhesi Shen, Jianlin Zhou, Ying Fan, Zengru Di, Yougui Wang,\n  H. Eugene Stanley, Shlomo Havlin", "title": "Increasing trend of scientists to switch between topics", "comments": "37 pages, 21 figures", "journal-ref": null, "doi": "10.1038/s41467-019-11401-8", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the publication records of individual scientists, aiming to\nquantify the topic switching dynamics of scientists and its influence. For each\nscientist, the relations among her publications are characterized via shared\nreferences. We find that the co-citing network of the papers of a scientist\nexhibits a clear community structure where each major community represents a\nresearch topic. Our analysis suggests that scientists tend to have a narrow\ndistribution of the number of topics. However, researchers nowadays switch more\nfrequently between topics than those in the early days. We also find that high\nswitching probability in early career (<12y) is associated with low overall\nproductivity, while it is correlated with high overall productivity in latter\ncareer. Interestingly, the average citation per paper, however, is in all\ncareer stages negatively correlated with the switching probability. We propose\na model with exploitation and exploration mechanisms that can explain the main\nobserved features.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 06:34:44 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zeng", "An", ""], ["Shen", "Zhesi", ""], ["Zhou", "Jianlin", ""], ["Fan", "Ying", ""], ["Di", "Zengru", ""], ["Wang", "Yougui", ""], ["Stanley", "H. Eugene", ""], ["Havlin", "Shlomo", ""]]}, {"id": "1812.03685", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "An extended trivariate vine copula mixed model for meta-analysis of\n  diagnostic studies in the presence of non-evaluable outcomes", "comments": "arXiv admin note: text overlap with arXiv:1506.03920,\n  arXiv:1502.07505", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper proposed an extended trivariate generalized linear mixed model\n(TGLMM) for synthesis of diagnostic test accuracy studies in the presence of\nnon-evaluable index test results. Inspired by the aforementioned model we\npropose an extended trivariate vine copula mixed model that includes the TGLMM\nas special case, but can also operate on the original scale of sensitivity,\nspecificity, and disease prevalence. The performance of the proposed vine\ncopula mixed model is examined by extensive simulation studies in comparison\nwith the TGLMM. Simulation studies showed that the TGLMM overestimates the\nmeta-analytic estimates of sensitivity, specificity, and prevalence when the\nunivariate random effects are misspecified. The vine copula mixed model gives\nnearly unbiased estimates of test accuracy indices and disease prevalence. Our\ngeneral methodology is illustrated by meta-analysing coronary CT angiography\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 09:16:35 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 08:20:30 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 18:48:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1812.03715", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek and Pietro Li\\`o", "title": "Modelling trait dependent speciation with Approximate Bayesian\n  Computation", "comments": null, "journal-ref": "Acta Physica Polonica B Proceedings Supplement, 12(1):25-47, 2019", "doi": "10.5506/APhysPolBSupp.12.25", "report-no": null, "categories": "q-bio.PE cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogeny is the field of modelling the temporal discrete dynamics of\nspeciation. Complex models can nowadays be studied using the Approximate\nBayesian Computation approach which avoids likelihood calculations. The field's\nprogression is hampered by the lack of robust software to estimate the numerous\nparameters of the speciation process. In this work we present an R package,\npcmabc, based on Approximate Bayesian Computations, that implements three novel\nphylogenetic algorithms for trait-dependent speciation modelling. Our\nphylogenetic comparative methodology takes into account both the simulated\ntraits and phylogeny, attempting to estimate the parameters of the processes\ngenerating the phenotype and the trait. The user is not restricted to a\npredefined set of models and can specify a variety of evolutionary and\nbranching models. We illustrate the software with a simulation-reestimation\nstudy focused around the branching Ornstein-Uhlenbeck process, where the\nbranching rate depends non-linearly on the value of the driving\nOrnstein-Uhlenbeck process. Included in this work is a tutorial on how to use\nthe software.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 10:17:12 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1812.03899", "submitter": "Chad M. Topaz", "authors": "Chad M. Topaz and Bernhard Klingenberg and Daniel Turek and Brianna\n  Heggeseth and Pamela E. Harris and Julie C. Blackwood and C. Ondine Chavoya\n  and Steven Nelson and Kevin M. Murphy", "title": "Diversity of Artists in Major U.S. Museums", "comments": "15 pages, 2 figures, minor revisions of and enhancements to text", "journal-ref": null, "doi": "10.1371/journal.pone.0212852", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U.S. art museum sector is grappling with diversity. While previous work\nhas investigated the demographic diversity of museum staffs and visitors, the\ndiversity of artists in their collections has remained unreported. We conduct\nthe first large-scale study of artist diversity in museums. By scraping the\npublic online catalogs of 18 major U.S. museums, deploying a sample of 10,000\nartist records comprising over 9,000 unique artists to crowdsourcing, and\nanalyzing 45,000 responses, we infer artist genders, ethnicities, geographic\norigins, and birth decades. Our results are threefold. First, we provide\nestimates of gender and ethnic diversity at each museum, and overall, we find\nthat 85% of artists are white and 87% are men. Second, we identify museums that\nare outliers, having significantly higher or lower representation of certain\ndemographic groups than the rest of the pool. Third, we find that the\nrelationship between museum collection mission and artist diversity is weak,\nsuggesting that a museum wishing to increase diversity might do so without\nchanging its emphases on specific time periods and regions. Our methodology can\nbe used to broadly and efficiently assess diversity in other fields.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 16:11:50 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 10:08:08 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 09:03:19 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Topaz", "Chad M.", ""], ["Klingenberg", "Bernhard", ""], ["Turek", "Daniel", ""], ["Heggeseth", "Brianna", ""], ["Harris", "Pamela E.", ""], ["Blackwood", "Julie C.", ""], ["Chavoya", "C. Ondine", ""], ["Nelson", "Steven", ""], ["Murphy", "Kevin M.", ""]]}, {"id": "1812.04103", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Na Zou, Dinggang Shen, Shuiwang Ji", "title": "Non-local U-Net for Biomedical Image Segmentation", "comments": "In Proceedings of the 34th AAAI Conference on Artificial Intelligence\n  (AAAI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown its great promise in various biomedical image\nsegmentation tasks. Existing models are typically based on U-Net and rely on an\nencoder-decoder architecture with stacked local operators to aggregate\nlong-range information gradually. However, only using the local operators\nlimits the efficiency and effectiveness. In this work, we propose the non-local\nU-Nets, which are equipped with flexible global aggregation blocks, for\nbiomedical image segmentation. These blocks can be inserted into U-Net as\nsize-preserving processes, as well as down-sampling and up-sampling layers. We\nperform thorough experiments on the 3D multimodality isointense infant brain MR\nimage segmentation task to evaluate the non-local U-Nets. Results show that our\nproposed models achieve top performances with fewer parameters and faster\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 21:28:55 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 21:00:45 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wang", "Zhengyang", ""], ["Zou", "Na", ""], ["Shen", "Dinggang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1812.04345", "submitter": "Philipp Bach", "authors": "Philipp Bach, Victor Chernozhukov, Martin Spindler", "title": "Closing the U.S. gender wage gap requires understanding its\n  heterogeneity", "comments": "Main text: 8 pages, 3 figures; Supplementary Material available\n  online", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016, the majority of full-time employed women in the U.S. earned\nsignificantly less than comparable men. The extent to which women were affected\nby gender inequality in earnings, however, depended greatly on socio-economic\ncharacteristics, such as marital status or educational attainment. In this\npaper, we analyzed data from the 2016 American Community Survey using a\nhigh-dimensional wage regression and applying double lasso to quantify\nheterogeneity in the gender wage gap. We found that the gap varied\nsubstantially across women and was driven primarily by marital status, having\nchildren at home, race, occupation, industry, and educational attainment. We\nrecommend that policy makers use these insights to design policies that will\nreduce discrimination and unequal pay more effectively.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:05:26 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:18:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bach", "Philipp", ""], ["Chernozhukov", "Victor", ""], ["Spindler", "Martin", ""]]}, {"id": "1812.04369", "submitter": "Shuang Xu", "authors": "Shuang Xu and Chun-Xia Zhang and Pei Wang and Jiangshe Zhang", "title": "Variational Bayesian Weighted Complex Network Reconstruction", "comments": null, "journal-ref": "Information Sciences, vol. 521, pp. 291-306, 2020", "doi": "10.1016/j.ins.2020.02.050", "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network reconstruction is a hot topic in many fields. Currently, the\nmost popular data-driven reconstruction framework is based on lasso. However,\nit is found that, in the presence of noise, lasso loses efficiency for weighted\nnetworks. This paper builds a new framework to cope with this problem. The key\nidea is to employ a series of linear regression problems to model the\nrelationship between network nodes, and then to use an efficient variational\nBayesian algorithm to infer the unknown coefficients. The numerical experiments\nconducted on both synthetic and real data demonstrate that the new method\noutperforms lasso with regard to both reconstruction accuracy and running\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:53:31 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 08:20:33 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 03:08:37 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xu", "Shuang", ""], ["Zhang", "Chun-Xia", ""], ["Wang", "Pei", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "1812.04567", "submitter": "Raoul Wadhwa", "authors": "Raoul R. Wadhwa, Andrew Dhawan, Drew F.K. Williamson, Jacob G. Scott", "title": "A flat persistence diagram for improved visualization of persistent\n  homology", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization in the emerging field of topological data analysis has\nprogressed from persistence barcodes and persistence diagrams to display of\ntwo-parameter persistent homology. Although persistence barcodes and diagrams\nhave permitted insight into the geometry underlying complex datasets,\nvisualization of even single-parameter persistent homology has significant room\nfor improvement. Here, we propose a modification to the conventional\npersistence diagram - the flat persistence diagram - that more efficiently\ndisplays information relevant to persistent homology and simultaneously\ncorrects for visual bias present in the former. Flat persistence diagrams\ndisplay equivalent information as their predecessor, while providing\nresearchers with an intuitive horizontal reference axis in contrast to the\nusual diagonal reference line. Reducing visual bias through the use of\nappropriate graphical displays not only provides more accurate, but also deeper\ninsights into the topology that underlies complex datasets. Introducing flat\npersistence diagrams into widespread use would bring researchers one step\ncloser to practical application of topological data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 17:43:36 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 11:03:46 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wadhwa", "Raoul R.", ""], ["Dhawan", "Andrew", ""], ["Williamson", "Drew F. K.", ""], ["Scott", "Jacob G.", ""]]}, {"id": "1812.04680", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Arnab Maity", "title": "A Score Based Test for Functional Linear Concurrent Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for testing the null hypothesis of no effect of a\ncovariate on the response in the context of functional linear concurrent\nregression. We establish an equivalent random effects formulation of our\nfunctional regression model under which our testing problem reduces to testing\nfor zero variance component for random effects. For this purpose, we use a\none-sided score test approach, which is an extension of the classical score\ntest. We provide theoretical justification as to why our testing procedure has\nthe right levels (asymptotically) under null using standard assumptions. Using\nnumerical simulations, we show that our testing method has the desired type I\nerror rate and gives higher power compared to a bootstrapped F test currently\nexisting in the literature. Our model and testing procedure are shown to give\ngood performances even when the data is sparsely observed, and the covariate is\ncontaminated with noise. Applications of the proposed testing method are\ndemonstrated on gait study and a dietary calcium absorption data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 20:35:51 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 19:02:11 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ghosal", "Rahul", ""], ["Maity", "Arnab", ""]]}, {"id": "1812.04933", "submitter": "Sumit Kumar", "authors": "Harsh Tripathi, Abhimanyu Singh Yadav, Mahendra Saha, Sumit Kumar", "title": "Generalized inverse xgamma distribution: A non-monotone hazard rate\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a generalized inverse xgamma distribution (GIXGD) has been\nintroduced as the generalized version of the inverse xgamma distribution. The\nproposed model exhibits the pattern of non-monotone hazard rate and belongs to\nfamily of positively skewed models. The explicit expressions of some\ndistributional properties, such as, moments, inverse moments, conditional\nmoments, mean deviation, quantile function have been derived. The maximum\nlikelihood estimation procedure has been used to estimate the unknown model\nparameters as well as survival characteristics of GIXGD. The practical\napplicability of the proposed model has been illustrated through a survival\ndata of guinea pigs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 14:22:43 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Tripathi", "Harsh", ""], ["Yadav", "Abhimanyu Singh", ""], ["Saha", "Mahendra", ""], ["Kumar", "Sumit", ""]]}, {"id": "1812.05170", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz and Luke Bornn", "title": "Markov Decision Processes with Dynamic Transition Probabilities: An\n  Analysis of Shooting Strategies in Basketball", "comments": "32 pages, 9 Figures, code available at\n  https://github.com/nsandholtz/nba_replay", "journal-ref": "Ann. Appl. Stat. 14(3), (2020) 1122-1145", "doi": "10.1214/20-AOAS1348", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we model basketball plays as episodes from team-specific\nnon-stationary Markov decision processes (MDPs) with shot clock dependent\ntransition probabilities. Bayesian hierarchical models are employed in the\nmodeling and parametrization of the transition probabilities to borrow strength\nacross players and through time. To enable computational feasibility, we\ncombine lineup-specific MDPs into team-average MDPs using a novel transition\nweighting scheme. Specifically, we derive the dynamics of the team-average\nprocess such that the expected transition count for an arbitrary state-pair is\nequal to the weighted sum of the expected counts of the separate\nlineup-specific MDPs.\n  We then utilize these non-stationary MDPs in the creation of a basketball\nplay simulator with uncertainty propagated via posterior samples of the model\ncomponents. After calibration, we simulate seasons both on-policy and under\naltered policies and explore the net changes in efficiency and production under\nthe alternate policies. Additionally, we discuss the game-theoretic\nramifications of testing alternative decision policies.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 21:48:53 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 00:52:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Bornn", "Luke", ""]]}, {"id": "1812.05224", "submitter": "Tong Wang", "authors": "Yunyi Li and Tong Wang", "title": "Next Hit Predictor - Self-exciting Risk Modeling for Predicting Next\n  Locations of Serial Crimes", "comments": null, "journal-ref": "AI for Social Good Workshop NIPS2018", "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to predict the location of the next crime in a crime series,\nbased on the identified previous offenses in the series. We build a predictive\nmodel called Next Hit Predictor (NHP) that finds the most likely location of\nthe next serial crime via a carefully designed risk model. The risk model\nfollows the paradigm of a self-exciting point process which consists of a\nbackground crime risk and triggered risks stimulated by previous offenses in\nthe series. Thus, NHP creates a risk map for a crime series at hand. To train\nthe risk model, we formulate a convex learning objective that considers\npairwise rankings of locations and use stochastic gradient descent to learn the\noptimal parameters. Next Hit Predictor incorporates both spatial-temporal\nfeatures and geographical characteristics of prior crime locations in the\nseries. Next Hit Predictor has demonstrated promising results on decades' worth\nof serial crime data collected by the Crime Analysis Unit of the Cambridge\nPolice Department in Massachusetts, USA.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 01:57:26 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Li", "Yunyi", ""], ["Wang", "Tong", ""]]}, {"id": "1812.05529", "submitter": "Matthew Parno", "authors": "Matthew Parno, Devin O'Connor, Matthew Smith", "title": "High dimensional inference for the structural health monitoring of lock\n  gates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locks and dams are critical pieces of inland waterways. However, many\ncomponents of existing locks have been in operation past their designed\nlifetime. To ensure safe and cost effective operations, it is therefore\nimportant to monitor the structural health of locks. To support lock gate\nmonitoring, this work considers a high dimensional Bayesian inference problem\nthat combines noisy real time strain observations with a detailed finite\nelement model. To solve this problem, we develop a new technique that combines\nKarhunen-Lo\\`eve decompositions, stochastic differential equation\nrepresentations of Gaussian processes, and Kalman smoothing that scales\nlinearly with the number of observations and could be used for near real-time\nmonitoring. We use quasi-periodic Gaussian processes to model thermal\ninfluences on the strain and infer spatially distributed boundary conditions in\nthe model, which are also characterized with Gaussian process prior\ndistributions. The power of this approach is demonstrated on a small synthetic\nexample and then with real observations of Mississippi River Lock 27, which is\nlocated near St. Louis, MO USA. The results show that our approach is able to\nprobabilistically characterize the posterior distribution over nearly 1.4\nmillion parameters in under an hour on a standard desktop computer.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:23:38 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Parno", "Matthew", ""], ["O'Connor", "Devin", ""], ["Smith", "Matthew", ""]]}, {"id": "1812.05691", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Kathy Li, Haoran Zhang, Scott W. Linderman, Raul\n  Rabadan, David M. Blei, Chris H. Wiggins", "title": "Dose-response modeling in high-throughput cancer drug screenings: An\n  end-to-end approach", "comments": "Added biomarker discovery testing section, among other revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized cancer treatments based on the molecular profile of a patient's\ntumor are an emerging and exciting class of treatments in oncology. As genomic\ntumor profiling is becoming more common, targeted treatments to specific\nmolecular alterations are gaining traction. To discover new potential\ntherapeutics that may apply to broad classes of tumors matching some molecular\npattern, experimentalists and pharmacologists rely on high-throughput, in-vitro\nscreens of many compounds against many different cell lines. We propose a\nhierarchical Bayesian model of how cancer cell lines respond to drugs in these\nexperiments and develop a method for fitting the model to real-world\nhigh-throughput screening data. Through a case study, the model is shown to\ncapture nontrivial associations between molecular features and drug response,\nsuch as requiring both wild type TP53 and overexpression of MDM2 to be\nsensitive to Nutlin-3(a). In quantitative benchmarks, the model outperforms a\nstandard approach in biology, with ~20% lower predictive error on held out\ndata. When combined with a conditional randomization testing procedure, the\nmodel discovers biomarkers of therapeutic response that recapitulate known\nbiology and suggest new avenues for investigation. All code for the paper is\npublicly available at https://github.com/tansey/deep-dose-response.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 21:08:27 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:55:02 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tansey", "Wesley", ""], ["Li", "Kathy", ""], ["Zhang", "Haoran", ""], ["Linderman", "Scott W.", ""], ["Rabadan", "Raul", ""], ["Blei", "David M.", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1812.05903", "submitter": "Jarod Lee", "authors": "Jarod Y. L. Lee and Craig Anderson and Wai T. Hung and Hon Hwang and\n  Louise M. Ryan", "title": "Detecting Faltering Growth in Children via Minimum Random Slopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A child is considered to have faltered growth when increases in their height\nor weight starts to decline relative to a suitable comparison population.\nHowever, there is currently a lack of consensus on both the choice of\nanthropometric indexes for characterizing growth over time and the operational\ndefinition of faltering. Cole's classic conditional standard deviation scores\nis a popular metric but can be problematic, since it only utilizes two data\npoints and relies on having complete data. In the existing literature,\narbitrary thresholds are often used to define faltering, which may not be\nappropriate for all populations. In this article, we propose to assess\nfaltering via minimum random slopes (MRS) derived from a piecewise linear mixed\nmodel. When used in conjunction with mixture model-based classification, MRS\nprovides a viable method for identifying children that have faltered, without\nbeing dependent upon arbitrary standards. We illustrate our work via a\nsimulation study and apply it to a case study based on a birth cohort within\nthe Healthy Birth, Growth and Development knowledge integration (HBGDki)\nproject funded by the Bill and Melinda Gates Foundation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 13:20:48 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Lee", "Jarod Y. L.", ""], ["Anderson", "Craig", ""], ["Hung", "Wai T.", ""], ["Hwang", "Hon", ""], ["Ryan", "Louise M.", ""]]}, {"id": "1812.05908", "submitter": "Vladimir Batagelj", "authors": "Daria Maltseva and Vladimir Batagelj", "title": "Social Network Analysis: Bibliographic Network Analysis of the Field and\n  its Evolution / Part 1. Basic Statistics and Citation Network Analysis", "comments": null, "journal-ref": "Maltseva, D., Batagelj, V.: Social network analysis as a field of\n  invasions: bibliographic approach to study SNA development. Scientometrics,\n  121(2019)2, 1085-1128", "doi": "10.1007/s11192-019-03193-x", "report-no": null, "categories": "physics.soc-ph cs.SI math.HO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the results of the study on the development of\nsocial network analysis (SNA) discipline and its evolution over time, using the\nanalysis of bibliographic networks. The dataset consists of articles from the\nWeb of Science Clarivate Analytics database and those published in the main\njournals in the field (70,000+ publications), created by searching for the key\nword \"social network*.\" From the collected data, we constructed several\nnetworks (citation and two-mode, linking publications with authors, keywords\nand journals). Analyzing the obtained networks, we evaluated the trends in the\nfield`s growth, noted the most cited works, created a list of authors and\njournals with the largest amount of works, and extracted the most often used\nkeywords in the SNA field. Next, using the Search path count approach, we\nextracted the main path, key-route paths and link islands in the citation\nnetwork. Based on the probabilistic flow node values, we identified the most\nimportant articles. Our results show that authors from the social sciences, who\nwere most active through the whole history of the field development,\nexperienced the \"invasion\" of physicists from 2000's. However, starting from\nthe 2010's, a new very active group of animal social network analysis has\nemerged.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 13:28:51 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Maltseva", "Daria", ""], ["Batagelj", "Vladimir", ""]]}, {"id": "1812.05954", "submitter": "Lucilio Cordero-Grande", "authors": "Lucilio Cordero-Grande and Daan Christiaens and Jana Hutter and\n  Anthony N. Price and Joseph V. Hajnal", "title": "Complex diffusion-weighted image estimation via matrix recovery under\n  general noise models", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a patch-based singular value shrinkage method for diffusion\nmagnetic resonance image estimation targeted at low signal to noise ratio and\naccelerated acquisitions. It operates on the complex data resulting from a\nsensitivity encoding reconstruction, where asymptotically optimal signal\nrecovery guarantees can be attained by modeling the noise propagation in the\nreconstruction and subsequently simulating or calculating the limit singular\nvalue spectrum. Simple strategies are presented to deal with phase\ninconsistencies and optimize patch construction. The pertinence of our\ncontributions is quantitatively validated on synthetic data, an in vivo adult\nexample, and challenging neonatal and fetal cohorts. Our methodology is\ncompared with related approaches, which generally operate on magnitude-only\ndata and use data-based noise level estimation and singular value truncation.\nVisual examples are provided to illustrate effectiveness in generating denoised\nand debiased diffusion estimates with well preserved spatial and diffusion\ndetail.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:25:50 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 16:47:56 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Cordero-Grande", "Lucilio", ""], ["Christiaens", "Daan", ""], ["Hutter", "Jana", ""], ["Price", "Anthony N.", ""], ["Hajnal", "Joseph V.", ""]]}, {"id": "1812.06038", "submitter": "James Bagrow", "authors": "Daniel Berenberg and James P. Bagrow", "title": "Inferring the size of the causal universe: features and fusion of causal\n  attribution networks", "comments": "15 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cause-and-effect reasoning, the attribution of effects to causes, is one of\nthe most powerful and unique skills humans possess. Multiple surveys are\nmapping out causal attributions as networks, but it is unclear how well these\nefforts can be combined. Further, the total size of the collective causal\nattribution network held by humans is currently unknown, making it challenging\nto assess the progress of these surveys. Here we study three causal attribution\nnetworks to determine how well they can be combined into a single network.\nCombining these networks requires dealing with ambiguous nodes, as nodes\nrepresent written descriptions of causes and effects and different descriptions\nmay exist for the same concept. We introduce NetFUSES, a method for combining\nnetworks with ambiguous nodes. Crucially, treating the different causal\nattributions networks as independent samples allows us to use their overlap to\nestimate the total size of the collective causal attribution network. We find\nthat existing surveys capture 5.77% $\\pm$ 0.781% of the $\\approx$293 000 causes\nand effects estimated to exist, and 0.198% $\\pm$ 0.174% of the $\\approx$10 200\n000 attributed cause-effect relationships.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:28:20 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Berenberg", "Daniel", ""], ["Bagrow", "James P.", ""]]}, {"id": "1812.06078", "submitter": "Hamzeh Torabi", "authors": "Hossein Nadeb, Hamzeh Torabi and Ali Dolati", "title": "Stochastic comparisons between the extreme claim amounts from two\n  heterogeneous portfolios in the case of transmuted-G model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{\\lambda_1}, \\ldots , X_{\\lambda_n}$ be independent non-negative\nrandom variables belong to the transmuted-G model and let $Y_i=I_{p_i}\nX_{\\lambda_i}$, $i=1,\\ldots,n$, where $I_{p_1}, \\ldots, I_{p_n}$ are\nindependent Bernoulli random variables independent of $X_{\\lambda_i}$'s, with\n${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial sciences, $Y_i$\ncorresponds to the claim amount in a portfolio of risks. In this paper we\ncompare the smallest and the largest claim amounts of two sets of independent\nportfolios belonging to the transmuted-G model, in the sense of usual\nstochastic order, hazard rate order and dispersive order, when the variables in\none set have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and the variables in\nthe other set have the parameters $\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$. For\nillustration we apply the results to the transmuted-G exponential and the\ntransmuted-G Weibull models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:57:39 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Nadeb", "Hossein", ""], ["Torabi", "Hamzeh", ""], ["Dolati", "Ali", ""]]}, {"id": "1812.06115", "submitter": "Jiraphan Suntornchost", "authors": "Partha Lahiri and Jiraphan Suntornchost", "title": "A General Bayesian Approach to Meet Different Inferential Goals in\n  Poverty Research for Small Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poverty mapping that displays spatial distribution of various poverty indices\nis most useful to policymakers and researchers when they are disaggregated into\nsmall geographic units, such as cities, municipalities or other administrative\npartitions of a country. Typically, national household surveys that contain\nwelfare variables such as income and expenditures provide limited or no data\nfor small areas. It is well-known that while direct survey-weighted estimates\nare quite reliable for national or large geographical areas they are unreliable\nfor small geographic areas. If the objective is to find areas with extreme\npoverty, these direct estimates will often select small areas due to the high\nvariabilities in the estimates. Empirical best prediction and Bayesian methods\nhave been proposed to improve on the direct point estimates. However, these\nestimates are not appropriate for different inferential purposes. For example,\nfor identifying areas with extreme poverty, these estimates would often select\nareas with large sample sizes. In this paper, using databases used by the\nChilean Ministry for their Small Area Estimation production, we illustrate how\nappropriate Bayesian methodology can be developed to address different\ninferential problems.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:31:38 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lahiri", "Partha", ""], ["Suntornchost", "Jiraphan", ""]]}, {"id": "1812.06157", "submitter": "Mathieu Pigeon", "authors": "Jean-Philippe Boucher and Mathieu Pigeon", "title": "A Claim Score for Dynamic Claim Counts Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a claim score based on the Bonus-Malus approach proposed by [7].\nWe compare the fit and predictive ability of this new model with various models\nfor of panel count data. In particular, we study in more details a new dynamic\nmodel based on the Harvey-Fernand\\`es (HF) approach, which gives different\nweight to the claims according to their date of occurrence. We show that the HF\nmodel has serious shortcomings that limit its use in practice. In contrast, the\nBonus-Malus model does not have these defects. Instead, it has several\ninteresting properties: interpretability, computational advantages and ease of\nuse in practice. We believe that the flexibility of this new model means that\nit could be used in many other actuarial contexts. Based on a real database, we\nshow that the proposed model generates the best fit and one of the best\npredictive capabilities among the other models tested.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:39:08 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Boucher", "Jean-Philippe", ""], ["Pigeon", "Mathieu", ""]]}, {"id": "1812.06166", "submitter": "Hamzeh Torabi", "authors": "Hossein Nadeb, Hamzeh Torabi, Ali Dolati", "title": "Ordering the smallest claim amounts from two sets of interdependent\n  heterogeneous portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be a set of dependent and\nnon-negative random variables share a survival copula and let $Y_i=\nI_{p_i}X_{\\lambda_i}$, $i=1,\\ldots,n$, where $I_{p_1},\\ldots,I_{p_n}$ be\nindependent Bernoulli random variables independent of $X_{\\lambda_i}$'s, with\n${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial sciences, $Y_i$\ncorresponds to the claim amount in a portfolio of risks. This paper considers\ncomparing the smallest claim amounts from two sets of interdependent\nportfolios, in the sense of usual and likelihood ratio orders, when the\nvariables in one set have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and\n$p_1,\\ldots,p_n$ and the variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. Also, we present\nsome bounds for survival function of the smallest claim amount in a portfolio.\nTo illustrate validity of the results, we serve some applicable models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:08:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Nadeb", "Hossein", ""], ["Torabi", "Hamzeh", ""], ["Dolati", "Ali", ""]]}, {"id": "1812.06167", "submitter": "Ben Boukai", "authors": "Ben Boukai and Yue Zhang", "title": "Recycled Least Squares Estimation in Nonlinear Regression", "comments": "19 pages with 4 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a resampling scheme for parameters estimates in nonlinear\nregression models. We provide an estimation procedure which recycles, via\nrandom weighting, the relevant parameters estimates to construct consistent\nestimates of the sampling distribution of the various estimates. We establish\nthe asymptotic normality of the resampled estimates and demonstrate the\napplicability of the recycling approach in a small simulation study and via\nexample.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:12:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Boukai", "Ben", ""], ["Zhang", "Yue", ""]]}, {"id": "1812.06175", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Alisa Kolesnikova, Stefan Lessmann, Tiejun Ma,\n  Ming-Chien Sung, Johnnie E.V. Johnson", "title": "Can Deep Learning Predict Risky Retail Investors? A Case Study in\n  Financial Risk Behavior Forecasting", "comments": "Within the \"equal\" contribution, Yaodong Yang contributed the core\n  deep learning algorithm along with its experimental results, and the first\n  draft of the manuscript (including Figure 1,2,3,4,7,8,9,11, and Table 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper examines the potential of deep learning to support decisions in\nfinancial risk management. We develop a deep learning model for predicting\nwhether individual spread traders secure profits from future trades. This task\nembodies typical modeling challenges faced in risk and behavior forecasting.\nConventional machine learning requires data that is representative of the\nfeature-target relationship and relies on the often costly development,\nmaintenance, and revision of handcrafted features. Consequently, modeling\nhighly variable, heterogeneous patterns such as trader behavior is challenging.\nDeep learning promises a remedy. Learning hierarchical distributed\nrepresentations of the data in an automatic manner (e.g. risk taking behavior),\nit uncovers generative features that determine the target (e.g., trader's\nprofitability), avoids manual feature engineering, and is more robust toward\nchange (e.g. dynamic market conditions). The results of employing a deep\nnetwork for operational risk forecasting confirm the feature learning\ncapability of deep learning, provide guidance on designing a suitable network\narchitecture and demonstrate the superiority of deep learning over machine\nlearning and rule-based benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:31:41 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 16:31:04 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 22:59:23 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yang", "Yaodong", ""], ["Kolesnikova", "Alisa", ""], ["Lessmann", "Stefan", ""], ["Ma", "Tiejun", ""], ["Sung", "Ming-Chien", ""], ["Johnson", "Johnnie E. V.", ""]]}, {"id": "1812.06205", "submitter": "Yizheng Liao", "authors": "Yizheng Liao and Ram Rajagopal", "title": "Sequential Multiple Structural Damage Detection and Localization: A\n  Distributed Approach", "comments": "38 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As essential components of the modern urban system, the health conditions of\ncivil structures are the foundation of urban system sustainability and need to\nbe continuously monitored. In Structural Health Monitoring (SHM), many existing\nworks will have limited performance in the sequential damage diagnosis process\nbecause 1) the damage events needs to be reported with short delay, 2) multiple\ndamage locations have to be identified simultaneously, and 3) the computational\ncomplexity is intractable in large-scale wireless sensor networks (WSNs). To\naddress these drawbacks, we propose a new damage identification approach that\nutilizes the time-series of damage sensitive features extracted from multiple\nsensors' measurements and the optimal change point detection theory to find\ndamage occurrence time and identify the number of damage locations. As the\nexisting change point detection methods require to centralize the sensor data,\nwhich is impracticable in many applications, we use the probabilistic graphical\nmodel to formulate WSNs and the targeting structure and propose a distributed\nalgorithm for structural damage identification. Validation results show highly\naccurate damage identification in a shake table experiment and American Society\nof Civil Engineers benchmark structure. Also, we demonstrate that the detection\ndelay is reduced significantly by utilizing multiple sensors' data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 00:13:09 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Liao", "Yizheng", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1812.06361", "submitter": "Kellie Ottoboni", "authors": "Kellie Ottoboni, Matthew Bernhard, J. Alex Halderman, Ronald L.\n  Rivest, Philip B. Stark", "title": "Bernoulli Ballot Polling: A Manifest Improvement for Risk-Limiting\n  Audits", "comments": "Accepted for Voting'19 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method and software for ballot-polling risk-limiting audits\n(RLAs) based on Bernoulli sampling: ballots are included in the sample with\nprobability $p$, independently. Bernoulli sampling has several advantages: (1)\nit does not require a ballot manifest; (2) it can be conducted independently at\ndifferent locations, rather than requiring a central authority to select the\nsample from the whole population of cast ballots or requiring stratified\nsampling; (3) it can start in polling places on election night, before margins\nare known. If the reported margins for the 2016 U.S. Presidential election are\ncorrect, a Bernoulli ballot-polling audit with a risk limit of 5% and a\nsampling rate of $p_0 = 1\\%$ would have had at least a 99% probability of\nconfirming the outcome in 42 states. (The other states were more likely to have\nneeded to examine additional ballots.) Logistical and security advantages that\nauditing in the polling place affords may outweigh the cost of examining more\nballots than some other methods might require.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 21:57:23 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Bernhard", "Matthew", ""], ["Halderman", "J. Alex", ""], ["Rivest", "Ronald L.", ""], ["Stark", "Philip B.", ""]]}, {"id": "1812.06575", "submitter": "Xiao Wu", "authors": "Xiao Wu, Fabrizia Mealli, Marianthi-Anna Kioumourtzoglou, Francesca\n  Dominici, Danielle Braun", "title": "Matching on Generalized Propensity Scores with Continuous Exposures", "comments": "We create an R package, GPSmacthing, available at\n  https://github.com/wxwx1993/GPSmatching, to implement the proposed matching\n  approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of statistical methods and interpretability of statistical\nanalysis from observational studies are central concerns in many applied\nfields, and robust causal inference methods have the potential to mitigate\nthese concerns. When estimating the causal effects of continuous exposure in\nobservational studies, generalized propensity scores (GPS) have been used to\nadjust for confounding bias. Existing GPS methods, either relying on weighting\nor regression, have certain limitations: a) they require a correctly specified\noutcome model; b) they are sensitive to extreme values of the estimated GPS; c)\nassessing covariate balance when using these approaches is not straightforward.\nMatching, a class of popular causal inference methods with binary treatments,\nhas not been extended to the continuous exposure setting, disregarding its many\nattractive features on method robustness and interpretability. In this paper,\nwe propose an innovative approach for GPS caliper matching in settings with\ncontinuous exposures. We first introduce an assumption of identifiability,\ncalled local weak unconfoundedness, that is less stringent than what is\ncurrently proposed in the literature. Under this assumption and mild smoothness\nconditions, we provide theoretical guarantees that our proposed matching\nestimators attain consistency and asymptotic normality. In simulations, our\nproposed matching estimator outperforms existing methods under settings of\nmodel misspecification and/or in the presence of extreme values of the\nestimated GPS in terms of bias reduction, root mean squared error, and overall\nachieves excellent covariate balance. We utilize the largest-to-date Medicare\nclaims data for the entire US from 2000 to 2016 to construct a continuous\ncausal exposure-response curve for long-term exposure to fine particles\n($PM_{2.5}$) on mortality.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:45:20 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 17:21:40 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 21:03:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wu", "Xiao", ""], ["Mealli", "Fabrizia", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Dominici", "Francesca", ""], ["Braun", "Danielle", ""]]}, {"id": "1812.06749", "submitter": "Carlos Lima Azevedo", "authors": "Joana Cavadas, Carlos Lima Azevedo, Haneen Farah, Ana Ferreira", "title": "Road safety of passing maneuvers: a bivariate extreme value theory\n  approach under non-stationary conditions", "comments": "30 pages, 6 figures, 6 tables, presented at Road Safety and\n  Simulation conference 2017 (http://rss2017.org/) and currently under review\n  at APP", "journal-ref": "Accident Analysis & Prevention Volume 134, January 2020, 105315", "doi": "10.1016/j.aap.2019.105315", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observed accidents have been the main resource for road safety analysis over\nthe past decades. Although such reliance seems quite straightforward, the rare\nnature of these events has made safety difficult to assess, especially for new\nand innovative traffic treatments. Surrogate measures of safety have allowed to\nstep away from traditional safety performance functions and analyze safety\nperformance without relying on accident records. In recent years, the use of\nextreme value theory (EV) models in combination with surrogate safety measures\nto estimate accident probabilities has gained popularity within the safety\ncommunity. In this paper we extend existing efforts on EV for accident\nprobability estimation for two dependent surrogate measures. Using detailed\ntrajectory data from a driving simulator, we model the joint probability of\nhead-on and rear-end collisions in passing maneuvers. In our estimation we\naccount for driver specific characteristics and road infrastructure variables.\nWe show that accounting for these factors improve the head-on collision\nprobability estimation. This work highlights the importance of considering\ndriver and road heterogeneity in evaluating related safety events, of relevance\nto interventions both for in-vehicle and infrastructure-based solutions. Such\nfeatures are essential to keep up with the expectations from surrogate safety\nmeasures for the integrated analysis of accident phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 13:23:50 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cavadas", "Joana", ""], ["Azevedo", "Carlos Lima", ""], ["Farah", "Haneen", ""], ["Ferreira", "Ana", ""]]}, {"id": "1812.06948", "submitter": "Paulo Serra", "authors": "Paulo Serra, Tatyana Krivobokova, Francisco Rosales", "title": "Adaptive Non-parametric Estimation of Mean and Autocovariance in\n  Regression with Dependent Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully automatic non-parametric approach to simultaneous\nestimation of mean and autocovariance functions in regression with dependent\nerrors. Our empirical Bayesian approach is adaptive, numerically efficient and\nallows for the construction of confidence sets for the regression function.\nConsistency of the estimators is shown and small sample performance is\ndemonstrated in simulations and real data analysis. The method is implemented\nin the R package eBsc that accompanies the paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:39:07 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Serra", "Paulo", ""], ["Krivobokova", "Tatyana", ""], ["Rosales", "Francisco", ""]]}, {"id": "1812.07062", "submitter": "Edith Osorio De La Rosa PhD", "authors": "Edith Osorio de la Rosa, Guillermo Becerra Nu\\~nez, Alfredo Omar\n  Palafox Roca and Ren\\'e Ledesma-Alonso", "title": "An empiric-stochastic approach, based on normalization parameters, to\n  simulate solar irradiance", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data acquisition of solar radiation in a locality is essential for the\ndevelopment of efficient designs of systems, whose operation is based on solar\nenergy. This paper presents a methodology to estimate solar irradiance using an\nempiric-stochastic approach, which consists of the computation of normalization\nparameters from solar irradiance data. For this study, solar irradiance data\nwas collected with a weather station during a year. Post-treatment included a\ntrimmed moving average, to smooth the data, the performance a fitting procedure\nusing a simple model, to recover normalization parameters, and the estimation\nof a probability density map by means of a kernel density estimation method.\nThe normalization parameters and the probability density map allowed us to\nbuild an empiric-stochastic methodology that generates an estimate of the solar\nirradiance. In order to validate our method, simulated solar irradiance has\nbeen used to compute the theoretical generation of solar power, which in turn\nhas been compared to experimental data, retrieved from a commercial\nphotovoltaic system. Since the simulation results show a good agreement has\nbeen with the experimental data, this simple methodology can estimate the solar\npower production and may help consumers to design and test a photovoltaic\nsystem before installation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 21:25:50 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["de la Rosa", "Edith Osorio", ""], ["Nu\u00f1ez", "Guillermo Becerra", ""], ["Roca", "Alfredo Omar Palafox", ""], ["Ledesma-Alonso", "Ren\u00e9", ""]]}, {"id": "1812.07295", "submitter": "Matteo Grigoletto", "authors": "Luisa Bisaglia and Matteo Grigoletto", "title": "A new time-varying model for forecasting long-memory series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new class of long-memory models with time-varying\nfractional parameter. In particular, the dynamics of the long-memory\ncoefficient, $d$, is specified through a stochastic recurrence equation driven\nby the score of the predictive likelihood, as suggested by Creal et al. (2013)\nand Harvey (2013). We demonstrate the validity of the proposed model by a Monte\nCarlo experiment and an application to two real time series.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 11:03:11 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Bisaglia", "Luisa", ""], ["Grigoletto", "Matteo", ""]]}, {"id": "1812.07455", "submitter": "William Denault", "authors": "William Denault, H\\r{a}kon K. Gjessing, Julius Juodakis, Bo Jacobsson,\n  Astanand Jugessur", "title": "Wavelet Screaming: a novel approach to analyzing GWAS data", "comments": "Presented at the American Society of Human Genetic, San Diego 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an alternative method for genome-wide association studies (GWAS)\nthat is more powerful than the regular GWAS method for locus detection. The\nregular GWAS method suffers from a substantial multiple-testing burden because\nof the millions of single nucleotide polymorphisms (SNPs) being tested\nsimultaneously. Furthermore, it does not consider the functional genetic effect\non the response variable; i.e., it ignores more complex joint effects of nearby\nSNPs within a region. Our proposed method screens the entire genome for\nassociations using a sequential sliding-window approach based on wavelets. A\nsequence of SNPs represents a genetic signal, and for every screened region, we\ntransform the genetic signal into the wavelet space. We then estimate the\nproportion of wavelet coefficients associated with the phenotype at different\nscales. The significance of a region is assessed via simulations, taking\nadvantage of a recent result on Bayes factor distributions. Our new approach\nreduces the number of independent tests to be performed. Moreover, we show via\nsimulations that the Wavelet Screaming method provides a substantial gain in\npower compared to the classic GWAS modeling when faced with more complex\nsignals than just single-SNP associations. To demonstrate feasibility, we\nre-analyze data from the large Norwegian HARVEST cohort. Keywords: Bayes\nfactors, GWAS, SNP, Multiple testing, Polygenic\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 18:18:07 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Denault", "William", ""], ["Gjessing", "H\u00e5kon K.", ""], ["Juodakis", "Julius", ""], ["Jacobsson", "Bo", ""], ["Jugessur", "Astanand", ""]]}, {"id": "1812.07691", "submitter": "Jingjing Zou", "authors": "Jingjing Zou, David J. Lederer, Daniel Rabinowitz", "title": "Efficiency in Lung Transplant Allocation Strategies", "comments": "36 pages of main text, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently in the United States, lung transplantations are allocated to\ncandidates according to the candidates' Lung Allocation Score (LAS). The LAS is\nan ad-hoc ranking system for patients' priorities of transplantation. The goal\nof this study is to develop a framework for improving patients' life expectancy\nover the LAS based on a comprehensive modeling of the lung transplantation\nwaiting list. Patients and organs are modeled as arriving according to Poisson\nprocesses, patients' health status evolving a waiting time inhomogeneous Markov\nprocess until death or transplantation, with organ recipient's expected\npost-transplant residual life depending on waiting time and health status at\ntransplantation. Under allocation rules satisfying minimal fairness\nrequirements, the long-term average expected life converges, and its limit is a\nnatural standard for comparing allocation strategies. Via the\nHamilton-Jacobi-Bellman equations, upper bounds for the limiting average\nexpected life are derived as a function of organ availability. Corresponding to\neach upper bound is an allocable set of (time, state) pairs at which patients\nwould be optimally transplanted. The allocable set expands monotonically as\norgan availability increases, which motivates the development of an allocation\nstrategy that leads to long-term expected life close to the upper bound.\nSimulation studies are conducted with model parameters estimated from national\nlung transplantation data. Results suggest that compared to the LAS, the\nproposed allocation strategy could provide a 7.7% increase in average total\nlife. We further extended the results to the the allocation and matching of\nmultiple organ types.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:22:42 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 03:53:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zou", "Jingjing", ""], ["Lederer", "David J.", ""], ["Rabinowitz", "Daniel", ""]]}, {"id": "1812.07694", "submitter": "Alexander Petersen", "authors": "Alexander Petersen and Hans-Georg M\\\"uller", "title": "Wasserstein Covariance for Multiple Random Densities", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common feature of methods for analyzing samples of probability density\nfunctions is that they respect the geometry inherent to the space of densities.\nOnce a metric is specified for this space, the Fr\\'echet mean is typically used\nto quantify and visualize the average density from the sample. For\none-dimensional densities, the Wasserstein metric is popular due to its\ntheoretical appeal and interpretive value as an optimal transport metric,\nleading to the Wasserstein-Fr\\'echet mean or barycenter as the mean density. We\nextend the existing methodology for samples of densities in two key directions.\nFirst, motivated by applications in neuroimaging, we consider dependent density\ndata, where a $p$-vector of univariate random densities is observed for each\nsampling unit. Second, we introduce a Wasserstein covariance measure and\npropose intuitively appealing estimators for both fixed and diverging $p$,\nwhere the latter corresponds to continuously-indexed densities. We also give\ntheory demonstrating consistency and asymptotic normality, while accounting for\nerrors introduced in the unavoidable preparatory density estimation step. The\nutility of the Wasserstein covariance matrix is demonstrated through\napplications to functional connectivity in the brain using functional magnetic\nresonance imaging data and to the secular evolution of mortality for various\ncountries.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:30:30 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Petersen", "Alexander", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1812.07696", "submitter": "Xiyue Liao", "authors": "Xiyue Liao and Mary C. Meyer", "title": "cgam: An R Package for the Constrained Generalized Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cgam package contains routines to fit the generalized additive model\nwhere the components may be modeled with shape and smoothness assumptions. The\nmain routine is cgam and nineteen symbolic routines are provided to indicate\nthe relationship between the response and each predictor, which satisfies\nconstraints such as monotonicity, convexity, their combinations, tree, and\numbrella orderings. The user may specify constrained splines to fit the\ncomponents for continuous predictors, and various types of orderings for the\nordinal predictors. In addition, the user may specify parametrically modeled\ncovariates. The set over which the likelihood is maximized is a polyhedral\nconvex cone, and a least-squares solution is obtained by projecting the data\nvector onto the cone. For generalized models, the fit is obtained through\niteratively re-weighted cone projections. The cone information criterion is\nprovided and may be used to compare fits for combinations of variables and\nshapes. In addition, the routine wps implements monotone regression in two\ndimensions using warped-plane splines, without an additivity assumption. The\ngraphical routine plotpersp will plot an estimated mean surface for a selected\npair of predictors, given an object of either cgam or wps. This package is now\navailable from the Comprehensive R Archive Network at\nhttp://CRAN.R-project.org/package=cgam.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:35:35 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Liao", "Xiyue", ""], ["Meyer", "Mary C.", ""]]}, {"id": "1812.07701", "submitter": "Kai Li", "authors": "Zhanfeng Wang and Kai Li and Jian Qing Shi", "title": "A robust estimation for the extended t-process regression model", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation and variable selection procedure are developed for the\nextended t-process regression model with functional data. Statistical\nproperties such as consistency of estimators and predictions are obtained.\nNumerical studies show that the proposed method performs well.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:53:27 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Li", "Kai", ""], ["Shi", "Jian Qing", ""]]}, {"id": "1812.07704", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis E. Nieto-Barajas", "title": "Bayesian regression with spatio-temporal varying coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the impact of climate variables on morbidity of some diseases in\nMexico, we propose a spatio-temporal varying coefficients regression model. For\nthat we introduce a new spatio-temporal dependent process prior, in a Bayesian\ncontext, with identically distributed normal marginal distributions and joint\nmultivariate normal distribution. We study its properties and characterise the\ndependence induced. Our results show that the effect of climate variables, on\nthe incidence of specific diseases, is not constant across space and time and\nour proposed model is able to capture and quantify those changes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 00:03:00 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 17:56:13 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 19:09:32 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Nieto-Barajas", "Luis E.", ""]]}, {"id": "1812.07722", "submitter": "Sung-En Chiu", "authors": "Sung-En Chiu, Nancy Ronquillo and Tara Javidi", "title": "Active Learning and CSI Acquisition for mmWave Initial Alignment", "comments": "This paper appears in: IEEE Journal on Selected Areas in\n  Communications On page(s): 1-16 Print ISSN: 0733-8716 Online ISSN: 1558-0008", "journal-ref": null, "doi": "10.1109/JSAC.2019.2933967", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter wave (mmWave) communication with large antenna arrays is a\npromising technique to enable extremely high data rates due to the large\navailable bandwidth in mmWave frequency bands. In addition, given the knowledge\nof an optimal directional beamforming vector, large antenna arrays have been\nshown to overcome both the severe signal attenuation in mmWave as well as the\ninterference problem. However, fundamental limits on achievable learning rate\nof an optimal beamforming vector remain.\n  This paper considers the problem of adaptive and sequential optimization of\nthe beamforming vectors during the initial access phase of communication. With\na single-path channel model, the problem is reduced to actively learning the\nAngle-of-Arrival (AoA) of the signal sent from the user to the Base Station\n(BS). Drawing on the recent results in the design of a hierarchical beamforming\ncodebook [1], sequential measurement dependent noisy search strategies [2], and\nactive learning from an imperfect labeler [3], an adaptive and sequential\nalignment algorithm is proposed.\n  An upper bound on the expected search time of the proposed algorithm is\nderived via Extrinsic Jensen-Shannon Divergence. which demonstrates that the\nsearch time of the proposed algorithm asymptotically matches the performance of\nthe noiseless bisection search up to a constant factor. Furthermore, the upper\nbound shows that the acquired AoA error probability decays exponentially fast\nwith the search time with an exponent that is a decreasing function of the\nacquisition rate.\n  Numerically, the proposed algorithm is compared with prior work where a\nsignificant improvement of the system communication rate is observed. Most\nnotably, in the relevant regime of low (-10dB to 5dB) raw SNR, this establishes\nthe first practically viable solution for initial access and, hence, the first\ndemonstration of stand-alone mmWave communication\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 01:14:29 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 23:50:38 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 22:15:38 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 01:21:21 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Chiu", "Sung-En", ""], ["Ronquillo", "Nancy", ""], ["Javidi", "Tara", ""]]}, {"id": "1812.07805", "submitter": "Zheng Chen", "authors": "Zheng Chen, Yong Zhang, Yue Shang, Xiaohua Hu", "title": "Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression\n  Model for Online Reviews", "comments": null, "journal-ref": "Asian Conference on Machine Learning. 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new HDP based online review rating regression model\nnamed Topic-Sentiment-Preference Regression Analysis (TSPRA). TSPRA combines\ntopics (i.e. product aspects), word sentiment and user preference as regression\nfactors, and is able to perform topic clustering, review rating prediction,\nsentiment analysis and what we invent as \"critical aspect\" analysis altogether\nin one framework. TSPRA extends sentiment approaches by integrating the key\nconcept \"user preference\" in collaborative filtering (CF) models into\nconsideration, while it is distinct from current CF models by decoupling \"user\npreference\" and \"sentiment\" as independent factors. Our experiments conducted\non 22 Amazon datasets show overwhelming better performance in rating\npredication against a state-of-art model FLAME (2015) in terms of error,\nPearson's Correlation and number of inverted pairs. For sentiment analysis, we\ncompare the derived word sentiments against a public sentiment resource\nSenticNet3 and our sentiment estimations clearly make more sense in the context\nof online reviews. Last, as a result of the de-correlation of \"user preference\"\nfrom \"sentiment\", TSPRA is able to evaluate a new concept \"critical aspects\",\ndefined as the product aspects seriously concerned by users but negatively\ncommented in reviews. Improvement to such \"critical aspects\" could be most\neffective to enhance user experience.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:33:31 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Zheng", ""], ["Zhang", "Yong", ""], ["Shang", "Yue", ""], ["Hu", "Xiaohua", ""]]}, {"id": "1812.07886", "submitter": "Philip Jonathan", "authors": "Emma Ross, Ole Christian Astrup, Elzbieta Bitner-Gregersen, Nigel\n  Bunn, Graham Feld, Ben Gouldby, Arne Huseby, Ye Liu, David Randell, Erik\n  Vanem, Philip Jonathan", "title": "On environmental contours for marine and coastal design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental contours are used in structural reliability analysis of marine\nand coastal structures as an approximate means to locate the boundary of the\ndistribution of environmental variables, and hence sets of environmental\nconditions giving rise to extreme structural loads and responses. Outline\nguidance concerning the application of environmental contour methods is given\nin recent design guidelines from many organisations. However there is lack of\nclarity concerning the differences between approaches to environmental contour\nestimation reported in the literature, and regarding the relationship between\nthe environmental contour, corresponding to some return period, and the extreme\nstructural response for the same period. Hence there is uncertainty about\nprecisely when environmental contours should be used, and how they should be\nused well. This article seeks to provide some assistance in understanding the\nfundamental issues regarding environmental contours and their use in structural\nreliability analysis. Approaches to estimating the joint distribution of\nenvironmental variables, and to estimating environmental contours based on that\ndistribution, are described. Simple software for estimation of the joint\ndistribution, and hence environmental contours, is illustrated (and is freely\navailable from the authors). Extra assumptions required to relate the\ncharacteristics of environmental contour to structural failure are outlined.\nAlternative response-based methods not requiring environmental contours are\nsummarised. The results of an informal survey of the metocean user community\nregarding environmental contours are presented. Finally, recommendations about\nwhen and how environmental contour methods should be used are made.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 11:31:42 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Ross", "Emma", ""], ["Astrup", "Ole Christian", ""], ["Bitner-Gregersen", "Elzbieta", ""], ["Bunn", "Nigel", ""], ["Feld", "Graham", ""], ["Gouldby", "Ben", ""], ["Huseby", "Arne", ""], ["Liu", "Ye", ""], ["Randell", "David", ""], ["Vanem", "Erik", ""], ["Jonathan", "Philip", ""]]}, {"id": "1812.07955", "submitter": "Benedikt M. P\u00c3\u00b6tscher", "authors": "Hannes Leeb, Benedikt M. P\\\"otscher, and Danijel Kivaranovic", "title": "Discussion on \"Model Confidence Bounds for Variable Selection\" by Yang\n  Li, Yuetian Luo, Davide Ferrari, Xiaonan Hu, and Yichen Qin", "comments": null, "journal-ref": "Biometrics 75 (2019), 407-410", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on the article mentioned in the title.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:10:37 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:32:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Leeb", "Hannes", ""], ["P\u00f6tscher", "Benedikt M.", ""], ["Kivaranovic", "Danijel", ""]]}, {"id": "1812.07963", "submitter": "Pauline Barmby", "authors": "P. Barmby", "title": "Astronomical observations: a guide for allied researchers", "comments": "Final version for Open Journal of Astrophysics; correct minor typos\n  from previous version", "journal-ref": null, "doi": "10.21105/astro.1812.07963", "report-no": null, "categories": "physics.ed-ph astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observational astrophysics uses sophisticated technology to collect and\nmeasure electromagnetic and other radiation from beyond the Earth. Modern\nobservatories produce large, complex datasets and extracting the maximum\npossible information from them requires the expertise of specialists in many\nfields beyond physics and astronomy, from civil engineers to statisticians and\nsoftware engineers. This article introduces the essentials of professional\nastronomical observations to colleagues in allied fields, to provide context\nand relevant background for both facility construction and data analysis. It\ncovers the path of electromagnetic radiation through telescopes, optics,\ndetectors, and instruments, its transformation through processing into\nmeasurements and information, and the use of that information to improve our\nunderstanding of the physics of the cosmos and its history.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:35:28 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 20:38:09 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 19:33:57 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Barmby", "P.", ""]]}, {"id": "1812.08071", "submitter": "Gianluca Martelloni", "authors": "Gianluca Martelloni, Francesca Di Patti, Ugo Bardi", "title": "Pattern Analysis of World Conflicts over the past 600 years", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the database prepared by Brecke (Brecke 2011) for violent\nconflict, covering some 600 years of human history. After normalizing the data\nfor the global human population, we find that the number of casualties tends to\nfollow a power law over the whole data series for the period considered, with\nno evidence of periodicity. We also observe that the number of conflicts, again\nnormalized for the human population, show a decreasing trend as a function of\ntime. Our result agree with previous analyses on this subject and tend to\nsupport the idea that war is a statistical phenomenon related to the network\nstructure of the human society.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:48:06 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 14:11:01 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 16:07:40 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 17:29:28 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Martelloni", "Gianluca", ""], ["Di Patti", "Francesca", ""], ["Bardi", "Ugo", ""]]}, {"id": "1812.08147", "submitter": "Kevin Lin", "authors": "Kevin Z. Lin and Han Liu and Kathryn Roeder", "title": "Covariance-based sample selection for heterogeneous data: Applications\n  to gene expression and autism risk gene detection", "comments": "45 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk for autism can be influenced by genetic mutations in hundreds of genes.\nBased on findings showing that genes with highly correlated gene expressions\nare functionally interrelated, \"guilt by association\" methods such as DAWN have\nbeen developed to identify these autism risk genes. Previous research analyzes\nthe BrainSpan dataset, which contains gene expression of brain tissues from\nvarying regions and developmental periods. Since the spatiotemporal properties\nof brain tissue is known to affect the gene expression's covariance, previous\nresearch have focused only on a specific subset of samples to avoid the issue\nof heterogeneity. This leads to a potential loss of power when detecting risk\ngenes. In this article, we develop a new method called COBS (COvariance-Based\nsample Selection) to find a larger and more homogeneous subset of samples that\nshare the same population covariance matrix for the downstream DAWN analysis.\nTo demonstrate COBS's effectiveness, we utilize genetic risk scores from two\nsequential data freezes obtained in 2014 and 2019. We show COBS improves DAWN's\nability to predict risk genes detected in the newer data freeze when utilizing\nthe risk scores of the older data freeze as input.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 18:43:49 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 23:24:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lin", "Kevin Z.", ""], ["Liu", "Han", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1812.08520", "submitter": "Serge Iovleff", "authors": "Serge Iovleff (MODAL,LPP), Seydou Syllla, Cheikh Loucoubar", "title": "Block clustering of Binary Data with Gaussian Co-variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous grouping of rows and columns is an important technique that\nis increasingly used in large-scale data analysis. In this paper, we present a\nnovel co-clustering method using co-variables in its construction. It is based\non a latent block model taking into account the problem of grouping variables\nand clustering individuals by integrating information given by sets of\nco-variables. Numerical experiments on simulated data sets and an application\non real genetic data highlight the interest of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 12:32:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Iovleff", "Serge", "", "MODAL,LPP"], ["Syllla", "Seydou", ""], ["Loucoubar", "Cheikh", ""]]}, {"id": "1812.08591", "submitter": "Gerard Keogh Dr.", "authors": "Gerard Keogh", "title": "A Gravity Model Analysis of Irish Merchandise Goods Exports under Brexit", "comments": "36 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effect of a Hard Brexit on Irish Exports using the PPML\nGravity Model and Irish Exports data at the micro level. We find a hard Brexit\ncould reduce Irish national income by over 9 billion euro annually and the\neffect would be sustained mst in th etraditional sectors of Agriculture and\nFood production of the Irish economy\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 14:22:03 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Keogh", "Gerard", ""]]}, {"id": "1812.08855", "submitter": "Yaoyuan Vincent Tan", "authors": "Yaoyuan Vincent Tan, Carol A.C. Flannagan, Lindsay R. Pool, and\n  Michael R. Elliott", "title": "Accounting for selection bias due to death in estimating the effect of\n  wealth shock on cognition for the Health and Retirement Study", "comments": "43 pages, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Health and Retirement Study is a longitudinal study of US adults enrolled\nat age 50 and older. We were interested in investigating the effect of a sudden\nlarge decline in wealth on the cognitive score of subjects. Our analysis was\ncomplicated by the lack of randomization, confounding by indication, and a\nsubstantial fraction of the sample and population will die during follow-up\nleading to some of our outcomes being censored. Common methods to handle these\nproblems for example marginal structural models, may not be appropriate because\nit upweights subjects who are more likely to die to obtain a population that\nover time resembles that would have been obtained in the absence of death. We\npropose a refined approach by comparing the treatment effect among subjects who\nwould survive under both sets of treatment regimes being considered. We do so\nby viewing this as a large missing data problem and impute the survival status\nand outcomes of the counterfactual. To improve the robustness of our\nimputation, we used a modified version of the penalized spline of propensity\nmethods in treatment comparisons approach. We found that our proposed method\nworked well in various simulation scenarios and our data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:38:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Tan", "Yaoyuan Vincent", ""], ["Flannagan", "Carol A. C.", ""], ["Pool", "Lindsay R.", ""], ["Elliott", "Michael R.", ""]]}, {"id": "1812.08858", "submitter": "Haoxiang Yang", "authors": "Haoxiang Yang, Yue Hu, David P. Morton", "title": "Analyzing Client Behavior in a Syringe Exchange Program", "comments": null, "journal-ref": "IISE Transactions on Healthcare Systems Engineering, 10:2, 142-157\n  (2020)", "doi": "10.1080/24725579.2019.1702125", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple syringe exchange programs serve the Chicago metropolitan area,\nproviding support for drug users to help prevent infectious diseases. Using\ndata from one program over a ten-year period, we study the behavior of its\nclients, focusing on the temporal process governing their visits to service\nlocations and on their demographics. We construct a phase-type distribution\nwith an affine relationship between model parameters and features of an\nindividual client. The phase-type distribution governs inter-arrival times\nbetween reoccurring visits of each client and is informed by characteristics of\na client including age, gender, ethnicity, and more. The inter-arrival time\nmodel is a sub-model in a simulation that we construct for the larger system,\nwhich allows us to provide a personalized prediction regarding the client's\ntime-to-return to a service location so that better intervention decisions can\nbe made with the help of simulation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 06:17:57 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 18:52:17 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Yang", "Haoxiang", ""], ["Hu", "Yue", ""], ["Morton", "David P.", ""]]}, {"id": "1812.08863", "submitter": "William Valdar", "authors": "Alan B. Lenarcic, John D. Calaway, Fernando Pardo-Manuel de Villena,\n  William Valdar", "title": "Bayesian Manifold-Constrained-Prior Model for an Experiment to Locate\n  Xce", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an analysis for a novel experiment intended to locate the genetic\nlocus Xce (X-chromosome controlling element), which biases the stochastic\nprocess of X-inactivation in the mouse. X-inactivation bias is a phenomenon\nwhere cells in the embryo randomly choose one parental chromosome to\ninactivate, but show an average bias towards one parental strain. Measurement\nof allele-specific gene-expression through pyrosequencing was conducted on\nmouse crosses of an uncharacterized parent with known carriers. Our Bayesian\nanalysis is suitable for this adaptive experimental design, accounting for the\nbiases and differences in precision among genes. Model identifiability is\nfacilitated by priors constrained to a manifold. We show that reparameterized\nslice-sampling can suitably tackle a general class of constrained priors. We\ndemonstrate a physical model, based upon a \"weighted-coin\" hypothesis, that\npredicts X-inactivation ratios in untested crosses. This model suggests that\nXce alleles differ due to a process known as copy number variation, where\nstronger Xce alleles are shorter sequences.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:49:00 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Lenarcic", "Alan B.", ""], ["Calaway", "John D.", ""], ["de Villena", "Fernando Pardo-Manuel", ""], ["Valdar", "William", ""]]}, {"id": "1812.09061", "submitter": "Tiejun Tong", "authors": "Jiandong Shi and Aimin Wu and Tiejun Tong", "title": "A new paradox in random-effects meta-analysis", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is an important tool for combining results from multiple\nstudies and has been widely used in evidence-based medicine for several\ndecades. This paper reports, for the first time, an interesting and valuable\nparadox in random-effects meta-analysis that is likely to occur when the number\nof studies is small and/or the heterogeneity is large. With the incredible\nparadox, we hence advocate meta-analysts to be extremely cautious when\ninterpreting the final results from the random-effects meta-analysis. And more\nimportantly, with the unexpected dilemma in making decisions, the new paradox\nhas raised an open question whether the current random-effects model is\nreasonable and tenable for meta-analysis, or it needs to be abandoned or\nfurther improved to some extent.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:39:37 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 11:04:25 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Shi", "Jiandong", ""], ["Wu", "Aimin", ""], ["Tong", "Tiejun", ""]]}, {"id": "1812.09081", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski and Florian Ziel", "title": "Econometric modelling and forecasting of intraday electricity prices", "comments": "Accepted for publication in the Journal of Commodity Markets", "journal-ref": null, "doi": "10.1016/j.jcomm.2019.100107", "report-no": null, "categories": "q-fin.ST econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper, we analyse the ID$_3$-Price in the German Intraday\nContinuous electricity market using an econometric time series model. A\nmultivariate approach is conducted for hourly and quarter-hourly products\nseparately. We estimate the model using lasso and elastic net techniques and\nperform an out-of-sample, very short-term forecasting study. The model's\nperformance is compared with benchmark models and is discussed in detail.\nForecasting results provide new insights to the German Intraday Continuous\nelectricity market regarding its efficiency and to the ID$_3$-Price behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 12:36:07 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 13:28:52 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "1812.09157", "submitter": "Jasmine Petry", "authors": "J. P\\'etry, B. De Boeck, N. Sebaihi, M. Coenegrachts, T. Caebergs, M.\n  Dobre", "title": "Uncertainty evalutation through data modelling for dimensional nanoscale\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck in nanoparticle measurements is the lack of comparability.\nComparability of measurement results is obtained by metrological traceability,\nwhich is obtained by calibration. In the present work the calibration of\ndimensional nanoparticle measurements is performed through the construction of\na calibration curve by comparison of measured reference standards to their\ncertified value. Subsequently, a general approach is proposed to perform a\nmeasurement uncertainty evaluation for a measured quantity when no\ncomprehensive physical model is available, by statistically modelling\nappropriately selected measurement data. The experimental data is collected so\nthat the influence of relevant parameters can be assessed by fitting a mixed\nmodel to the data. Furthermore, this model allows to generate a probability\ndensity function (PDF) for the concerned measured quantity. Applying this\nmethodology to dimensional nanoparticle measurements leads to a PDF for a\nmeasured dimensional quantity of the nanoparticles. A PDF for the measurand,\nwhich is the certified counterpart of that measured dimensional quantity, can\nthen be extracted by reporting a PDF for the measured dimensional quantity on\nthe calibration curve. The PDF for the measurand grasps its total measurement\nuncertainty. Working in a fully Bayesian framework is natural due to the\ninstrinsic caracter of the quantity of interest: the distribution of size\nrather than the size of one single particle. The developed methodology is\napplied to the particular case where dimensional nanoparticle measurements are\nperformed using an atomic force microscope (AFM). The reference standards used\nto build a calibration curve are nano-gratings with step heights covering the\napplication range of the calibration curve.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:48:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["P\u00e9try", "J.", ""], ["De Boeck", "B.", ""], ["Sebaihi", "N.", ""], ["Coenegrachts", "M.", ""], ["Caebergs", "T.", ""], ["Dobre", "M.", ""]]}, {"id": "1812.09178", "submitter": "Yuanzhi Huang", "authors": "Yuanzhi Huang, Eamonn Ahearne, Szymon Baron, Andrew Parnell", "title": "An Evaluation of Methods for Real-Time Anomaly Detection using Force\n  Measurements from the Turning Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examined the use of three conventional anomaly detection methods and\nassess their potential for on-line tool wear monitoring. Through efficient data\nprocessing and transformation of the algorithm proposed here, in a real-time\nenvironment, these methods were tested for fast evaluation of cutting tools on\nCNC machines. The three-dimensional force data streams we used were extracted\nfrom a turning experiment of 21 runs for which a tool was run until it\ngenerally satisfied an end-of-life criterion. Our real-time anomaly detection\nalgorithm was scored and optimised according to how precisely it can predict\nthe progressive wear of the tool flank. Most of our tool wear predictions were\naccurate and reliable as illustrated in our off-line simulation results.\nParticularly when the multivariate analysis was applied, the algorithm we\ndevelop was found to be very robust across different scenarios and against\nparameter changes. It shall be reasonably easy to apply our approach elsewhere\nfor real-time tool wear analytics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 17:15:18 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Huang", "Yuanzhi", ""], ["Ahearne", "Eamonn", ""], ["Baron", "Szymon", ""], ["Parnell", "Andrew", ""]]}, {"id": "1812.09181", "submitter": "Alexis Tantet", "authors": "Alexis Tantet (LMD, X-DEP-MECA), Silvia Concettini, Claudia\n  d'Ambrosio, Dimitri Thomopulos, Peter Tankov, Marc St\\'efanon (LMD), Philippe\n  Drobinski (SA), Jordi Badosa (LMD), Anna Cr\\'eti (X-DEP-ECO), Dimitri\n  Thomopulos", "title": "e4clim 1.0 : The Energy for CLimate Integrated Model: Description and\n  Application to Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an open-source Python software integrating flexibility needs from\nVariable Renewable Energies (VREs) in the development of regional energy mixes.\nIt provides a flexible and extensible tool to researchers/engineers, and for\neducation/outreach. It aims at evaluating and optimizing energy deployment\nstrategies with high shares of VRE; assessing the impact of new technologies\nand of climate variability; conducting sensitivity studies. Specifically, to\nlimit the algorithm's complexity, we avoid solving a full-mix cost-minimization\nproblem by taking the mean and variance of the renewable production-demand\nratio as proxies to balance services. Second, observations of VRE technologies\nbeing typically too short or nonexistent, the hourly demand and production are\nestimated from climate time-series and fitted to available observations. We\nillustrate e4clim's potential with an optimal recommissioning-study of the 2015\nItalian PV-wind mix testing different climate-data sources and strategies and\nassessing the impact of climate variability and the robustness of the results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:11:14 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 09:08:10 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 09:37:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Tantet", "Alexis", "", "LMD, X-DEP-MECA"], ["Concettini", "Silvia", "", "LMD"], ["d'Ambrosio", "Claudia", "", "LMD"], ["Thomopulos", "Dimitri", "", "LMD"], ["Tankov", "Peter", "", "LMD"], ["St\u00e9fanon", "Marc", "", "LMD"], ["Drobinski", "Philippe", "", "SA"], ["Badosa", "Jordi", "", "LMD"], ["Cr\u00e9ti", "Anna", "", "X-DEP-ECO"], ["Thomopulos", "Dimitri", ""]]}, {"id": "1812.09230", "submitter": "Elvira Di Nardo Prof.", "authors": "Davide Ricossa, Enrico Baccaglini, Elvira Di Nardo, Emilia Parodi,\n  Riccardo Scopigno", "title": "Automatic cry analysis and classification for infant pain assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of pain management relies on the choice and the correct use\nof suitable pain assessment tools. In the case of newborns, some of the most\ncommon tools are human-based and observational, thus affected by subjectivity\nand methodological problems. Therefore, in the last years there has been an\nincreasing interest in developing an automatic machine-based pain assessment\ntool.\n  This research is a preliminary investigation towards the inclusion of a\nscoring system for the vocal expression of the infant into an automatic tool.\nTo this aim we present a method to compute three correlated indicators which\nmeasure three distress-related features of the cry: duration, dysphonantion and\nfundamental frequency of the first cry. In particular, we propose a new method\nto measure the dysphonantion of the cry via spectral entropy analysis,\nresulting in an indicator that identifies three well separated levels of\ndistress in the vocal expression. These levels provide a classification that is\nhighly correlated with the human-based assessment of the cry.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:20:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Ricossa", "Davide", ""], ["Baccaglini", "Enrico", ""], ["Di Nardo", "Elvira", ""], ["Parodi", "Emilia", ""], ["Scopigno", "Riccardo", ""]]}, {"id": "1812.09424", "submitter": "Yuan-chin Ivan Chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Distributed sequential method for analyzing massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyse a very large data set containing lengthy variables, we adopt a\nsequential estimation idea and propose a parallel divide-and-conquer method. We\nconduct several conventional sequential estimation procedures separately, and\nproperly integrate their results while maintaining the desired statistical\nproperties. Additionally, using a criterion from the statistical experiment\ndesign, we adopt an adaptive sample selection, together with an adaptive\nshrinkage estimation method, to simultaneously accelerate the estimation\nprocedure and identify the effective variables. We confirm the cogency of our\nmethods through theoretical justifications and numerical results derived from\nsynthesized data sets. We then apply the proposed method to three real data\nsets, including those pertaining to appliance energy use and particulate matter\nconcentration.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 00:54:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1812.09464", "submitter": "Kunjin Chen", "authors": "Kunjin Chen, Jun Hu, Yu Zhang, Zhanqing Yu, Jinliang He", "title": "Fault Location in Power Distribution Systems via Deep Graph\n  Convolutional Networks", "comments": "Accepcted by IEEE Journal on Selected Areas in Communication", "journal-ref": null, "doi": "10.1109/JSAC.2019.2951964", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel graph convolutional network (GCN) framework for\nfault location in power distribution networks. The proposed approach integrates\nmultiple measurements at different buses while taking system topology into\naccount. The effectiveness of the GCN model is corroborated by the IEEE 123 bus\nbenchmark system. Simulation results show that the GCN model significantly\noutperforms other widely-used machine learning schemes with very high fault\nlocation accuracy. In addition, the proposed approach is robust to measurement\nnoise and data loss errors. Data visualization results of two competing neural\nnetworks are presented to explore the mechanism of GCN's superior performance.\nA data augmentation procedure is proposed to increase the robustness of the\nmodel under various levels of noise and data loss errors. Further experiments\nshow that the model can adapt to topology changes of distribution networks and\nperform well with a limited number of measured buses.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 06:31:04 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 02:25:07 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chen", "Kunjin", ""], ["Hu", "Jun", ""], ["Zhang", "Yu", ""], ["Yu", "Zhanqing", ""], ["He", "Jinliang", ""]]}, {"id": "1812.09590", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Bayesian Propagation of Record Linkage Uncertainty into Population Size\n  Estimation of Human Rights Violations", "comments": "Published as part of the Special Issue in memory of Stephen E.\n  Fienberg (https://projecteuclid.org/euclid.aoas/1532743462#toc), in\n  https://projecteuclid.org/euclid.aoas/1532743484 at the Annals of Applied\n  Statistics\n  (https://www.imstat.org/journals-and-publications/annals-of-applied-statistics/)\n  by the Institute of Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2018, Volume 12, Number 2, 1013-1038", "doi": "10.1214/18-AOAS1178", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-systems or capture-recapture estimation are common techniques for\npopulation size estimation, particularly in the quantitative study of human\nrights violations. These methods rely on multiple samples from the population,\nalong with the information of which individuals appear in which samples. The\ngoal of record linkage techniques is to identify unique individuals across\nsamples based on the information collected on them. Linkage decisions are\nsubject to uncertainty when such information contains errors and missingness,\nand when different individuals have very similar characteristics. Uncertainty\nin the linkage should be propagated into the stage of population size\nestimation. We propose an approach called linkage-averaging to propagate\nlinkage uncertainty, as quantified by some Bayesian record linkage\nmethodologies, into a subsequent stage of population size estimation.\nLinkage-averaging is a two-stage approach in which the results from the record\nlinkage stage are fed into the population size estimation stage. We show that\nunder some conditions the results of this approach correspond to those of a\nproper Bayesian joint model for both record linkage and population size\nestimation. The two-stage nature of linkage-averaging allows us to combine\ndifferent record linkage models with different capture-recapture models, which\nfacilitates model exploration. We present a case study from the Salvadoran\ncivil war, where we are interested in estimating the total number of civilian\nkillings using lists of witnesses' reports collected by different\norganizations. These lists contain duplicates, typographical and spelling\nerrors, missingness, and other inaccuracies that lead to uncertainty in the\nlinkage. We show how linkage-averaging can be used for transferring the\nuncertainty in the linkage of these lists into different models for population\nsize estimation.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 19:38:17 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1812.09654", "submitter": "Shuang Jiang", "authors": "Shuang Jiang, Guanghua Xiao, Andrew Y. Koh, Qiwei Li, Xiaowei Zhan", "title": "A Bayesian Zero-Inflated Negative Binomial Regression Model for the\n  Integrative Analysis of Microbiome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbiome `omics approaches can reveal intriguing relationships between the\nhuman microbiome and certain disease states. Along with the identification of\nspecific bacteria taxa associated with diseases, recent scientific advancements\nprovide mounting evidence that metabolism, genetics and environmental factors\ncan all modulate these microbial effects. However, the current methods for\nintegrating microbiome data and other covariates are severely lacking. Hence,\nwe present an integrative Bayesian zero-inflated negative binomial regression\nmodel that can both distinguish differentially abundant taxa with distinct\nphenotypes and quantify covariate-taxa effects. Our model demonstrates good\nperformance using simulated data. Furthermore, we successfully integrated\nmicrobiome taxonomies and metabolomics in two real microbiome datasets to\nprovide biologically interpretable findings. In all, we proposed a novel\nintegrative Bayesian regression model that features bacterial differential\nabundance analysis and microbiome-covariate effects quantifications, which\nmakes it suitable for general microbiome studies.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 04:03:55 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 01:30:44 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Jiang", "Shuang", ""], ["Xiao", "Guanghua", ""], ["Koh", "Andrew Y.", ""], ["Li", "Qiwei", ""], ["Zhan", "Xiaowei", ""]]}, {"id": "1812.09729", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "A Note on the Bayesian Approach to Sliding Window Detector Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a Bayesian methodology has been introduced, enabling the\nconstruction of sliding window detectors with the constant false alarm rate\nproperty. The approach introduces a Bayesian predictive inference approach,\nwhere under the assumption of no target, a predictive density of the cell under\ntest, conditioned on the clutter range profile, is produced. The probability of\nfalse alarm can then be produced by integrating this density. As a result of\nthis, for a given clutter model, the Bayesian constant false alarm rate\ndetector is produced. This note outlines how this approach can be extended, to\nallow the construction of alternative Bayesian decision rules, based upon more\nuseful measures of the clutter level.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 15:25:46 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1812.09758", "submitter": "Paul McNicholas", "authors": "Forrest Paton and Paul D. McNicholas", "title": "Detecting British Columbia Coastal Rainfall Patterns by Clustering\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is a statistical framework where data are assumed to\nfollow some functional form. This method of analysis is commonly applied to\ntime series data, where time, measured continuously or in discrete intervals,\nserves as the location for a function's value. Gaussian processes are a\ngeneralization of the multivariate normal distribution to function space and,\nin this paper, they are used to shed light on coastal rainfall patterns in\nBritish Columbia (BC). Specifically, this work addressed the question over how\none should carry out an exploratory cluster analysis for the BC, or any\nsimilar, coastal rainfall data. An approach is developed for clustering\nmultiple processes observed on a comparable interval, based on how similar\ntheir underlying covariance kernel is. This approach provides interesting\ninsights into the BC data, and these insights can be framed in terms of El\nNi\\~{n}o and La Ni\\~{n}a; however, the result is not simply one cluster\nrepresenting El Ni\\~{n}o years and another for La Ni\\~{n}a years. From one\nperspective, the results show that clustering annual rainfall can potentially\nbe used to identify extreme weather patterns.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 19:15:36 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 22:54:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Paton", "Forrest", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1812.09786", "submitter": "Ben Moews", "authors": "Ben Moews, Rafael S. de Souza, Emille E. O. Ishida, Alex I. Malz,\n  Caroline Heneka, Ricardo Vilalta, Joe Zuntz (for the COIN Collaboration)", "title": "Stress testing the dark energy equation of state imprint on supernova\n  data", "comments": "14 pages, 9 figures", "journal-ref": "Phys. Rev. D 99, 123529 (2019)", "doi": "10.1103/PhysRevD.99.123529", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work determines the degree to which a standard Lambda-CDM analysis based\non type Ia supernovae can identify deviations from a cosmological constant in\nthe form of a redshift-dependent dark energy equation of state w(z). We\nintroduce and apply a novel random curve generator to simulate instances of\nw(z) from constraint families with increasing distinction from a cosmological\nconstant. After producing a series of mock catalogs of binned type Ia\nsupernovae corresponding to each w(z) curve, we perform a standard Lambda-CDM\nanalysis to estimate the corresponding posterior densities of the absolute\nmagnitude of type Ia supernovae, the present-day matter density, and the\nequation of state parameter. Using the Kullback-Leibler divergence between\nposterior densities as a difference measure, we demonstrate that a standard\ntype Ia supernova cosmology analysis has limited sensitivity to extensive\nredshift dependencies of the dark energy equation of state. In addition, we\nreport that larger redshift-dependent departures from a cosmological constant\ndo not necessarily manifest easier-detectable incompatibilities with the\nLambda-CDM model. Our results suggest that physics beyond the standard model\nmay simply be hidden in plain sight.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 22:26:18 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 01:06:18 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Moews", "Ben", "", "for the COIN Collaboration"], ["de Souza", "Rafael S.", "", "for the COIN Collaboration"], ["Ishida", "Emille E. O.", "", "for the COIN Collaboration"], ["Malz", "Alex I.", "", "for the COIN Collaboration"], ["Heneka", "Caroline", "", "for the COIN Collaboration"], ["Vilalta", "Ricardo", "", "for the COIN Collaboration"], ["Zuntz", "Joe", "", "for the COIN Collaboration"]]}, {"id": "1812.09821", "submitter": "Adam Spannaus", "authors": "Adam Spannaus and Vasileios Maroulas and David J. Keffer and Kody J.\n  H. Law", "title": "Bayesian Point Set Registration", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point set registration involves identifying a smooth invertible\ntransformation between corresponding points in two point sets, one of which may\nbe smaller than the other and possibly corrupted by observation noise. This\nproblem is traditionally decomposed into two separate optimization problems:\n(i) assignment or correspondence, and (ii) identification of the optimal\ntransformation between the ordered point sets. In this work, we propose an\napproach solving both problems simultaneously. In particular, a coherent\nBayesian formulation of the problem results in a marginal posterior\ndistribution on the transformation, which is explored within a Markov chain\nMonte Carlo scheme. Motivated by Atomic Probe Tomography (APT), in the context\nof structure inference for high entropy alloys (HEA), we focus on the\nregistration of noisy sparse observations of rigid transformations of a known\nreference configuration.Lastly, we test our method on synthetic data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 03:50:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Spannaus", "Adam", ""], ["Maroulas", "Vasileios", ""], ["Keffer", "David J.", ""], ["Law", "Kody J. H.", ""]]}, {"id": "1812.10107", "submitter": "Seung Jun Shin", "authors": "Seung Jun Shin, Yichao Wu, Ning Hao", "title": "A backward procedure for change-point detection with applications to\n  copy number variation detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point detection regains much attention recently for analyzing array or\nsequencing data for copy number variation (CNV) detection. In such\napplications, the true signals are typically very short and buried in the long\ndata sequence, which makes it challenging to identify the variations\nefficiently and accurately. In this article, we propose a new change-point\ndetection method, a backward procedure, which is not only fast and simple\nenough to exploit high-dimensional data but also performs very well for\ndetecting short signals. Although motivated by CNV detection, the backward\nprocedure is generally applicable to assorted change-point problems that arise\nin a variety of scientific applications. It is illustrated by both simulated\nand real CNV data that the backward detection has clear advantages over other\ncompeting methods especially when the true signal is short.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 13:49:47 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 23:56:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shin", "Seung Jun", ""], ["Wu", "Yichao", ""], ["Hao", "Ning", ""]]}, {"id": "1812.10236", "submitter": "Eric Fox", "authors": "Eric W. Fox, Jay M. Ver Hoef, and Anthony R. Olsen", "title": "Comparing Spatial Regression to Random Forests for Large Environmental\n  Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental data may be \"large\" due to number of records, number of\ncovariates, or both. Random forests has a reputation for good predictive\nperformance when using many covariates with nonlinear relationships, whereas\nspatial regression, when using reduced rank methods, has a reputation for good\npredictive performance when using many records that are spatially\nautocorrelated. In this study, we compare these two techniques using a data set\ncontaining the macroinvertebrate multimetric index (MMI) at 1859 stream sites\nwith over 200 landscape covariates. A primary application is mapping MMI\npredictions and prediction errors at 1.1 million perennial stream reaches\nacross the conterminous United States. For the spatial regression model, we\ndevelop a novel transformation procedure that estimates Box-Cox transformations\nto linearize covariate relationships and handles possibly zero-inflated\ncovariates. We find that the spatial regression model with transformations, and\na subsequent selection of significant covariates, has cross-validation\nperformance slightly better than random forests. We also find that prediction\ninterval coverage is close to nominal for each method, but that spatial\nregression prediction intervals tend to be narrower and have less variability\nthan quantile regression forest prediction intervals. A simulation study is\nused to generalize results and clarify advantages of each modeling approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:55:56 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Fox", "Eric W.", ""], ["Hoef", "Jay M. Ver", ""], ["Olsen", "Anthony R.", ""]]}, {"id": "1812.10404", "submitter": "Sebastian Vollmer", "authors": "Sebastian Vollmer, Bilal A. Mateen, Gergo Bohner, Franz J Kir\\'aly,\n  Rayid Ghani, Pall Jonsson, Sarah Cumbers, Adrian Jonas, Katherine S.L.\n  McAllister, Puja Myles, David Granger, Mark Birse, Richard Branson, Karel GM\n  Moons, Gary S Collins, John P.A. Ioannidis, Chris Holmes, Harry Hemingway", "title": "Machine learning and AI research for Patient Benefit: 20 Critical\n  Questions on Transparency, Replicability, Ethics and Effectiveness", "comments": "25 pages, 2 boxes, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML), artificial intelligence (AI) and other modern\nstatistical methods are providing new opportunities to operationalize\npreviously untapped and rapidly growing sources of data for patient benefit.\nWhilst there is a lot of promising research currently being undertaken, the\nliterature as a whole lacks: transparency; clear reporting to facilitate\nreplicability; exploration for potential ethical concerns; and, clear\ndemonstrations of effectiveness. There are many reasons for why these issues\nexist, but one of the most important that we provide a preliminary solution for\nhere is the current lack of ML/AI- specific best practice guidance. Although\nthere is no consensus on what best practice looks in this field, we believe\nthat interdisciplinary groups pursuing research and impact projects in the\nML/AI for health domain would benefit from answering a series of questions\nbased on the important issues that exist when undertaking work of this nature.\nHere we present 20 questions that span the entire project life cycle, from\ninception, data analysis, and model evaluation, to implementation, as a means\nto facilitate project planning and post-hoc (structured) independent\nevaluation. By beginning to answer these questions in different settings, we\ncan start to understand what constitutes a good answer, and we expect that the\nresulting discussion will be central to developing an international consensus\nframework for transparent, replicable, ethical and effective research in\nartificial intelligence (AI-TREE) for health.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 18:11:20 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Vollmer", "Sebastian", ""], ["Mateen", "Bilal A.", ""], ["Bohner", "Gergo", ""], ["Kir\u00e1ly", "Franz J", ""], ["Ghani", "Rayid", ""], ["Jonsson", "Pall", ""], ["Cumbers", "Sarah", ""], ["Jonas", "Adrian", ""], ["McAllister", "Katherine S. L.", ""], ["Myles", "Puja", ""], ["Granger", "David", ""], ["Birse", "Mark", ""], ["Branson", "Richard", ""], ["Moons", "Karel GM", ""], ["Collins", "Gary S", ""], ["Ioannidis", "John P. A.", ""], ["Holmes", "Chris", ""], ["Hemingway", "Harry", ""]]}, {"id": "1812.10800", "submitter": "Nicholas Seewald", "authors": "Nicholas J. Seewald, Shawna N. Smith, Andy Jinseok Lee, Predrag\n  Klasnja, Susan A. Murphy", "title": "Practical Considerations for Data Collection and Management in Mobile\n  Health Micro-randomized Trials", "comments": "Author accepted manuscript", "journal-ref": null, "doi": "10.1007/s12561-018-09228-w", "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in leveraging the prevalence of mobile technology\nto improve health by delivering momentary, contextualized interventions to\nindividuals' smartphones. A just-in-time adaptive intervention (JITAI) adjusts\nto an individual's changing state and/or context to provide the right\ntreatment, at the right time, in the right place. Micro-randomized trials\n(MRTs) allow for the collection of data which aid in the construction of an\noptimized JITAI by sequentially randomizing participants to different treatment\noptions at each of many decision points throughout the study. Often, this data\nis collected passively using a mobile phone. To assess the causal effect of\ntreatment on a near-term outcome, care must be taken when designing the data\ncollection system to ensure it is of appropriately high quality. Here, we make\nseveral recommendations for collecting and managing data from an MRT. We\nprovide advice on selecting which features to collect and when, choosing\nbetween \"agents\" to implement randomization, identifying sources of missing\ndata, and overcoming other novel challenges. The recommendations are informed\nby our experience with HeartSteps, an MRT designed to test the effects of an\nintervention aimed at increasing physical activity in sedentary adults. We also\nprovide a checklist which can be used in designing a data collection system so\nthat scientists can focus more on their questions of interest, and less on\ncleaning data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:23:23 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Seewald", "Nicholas J.", ""], ["Smith", "Shawna N.", ""], ["Lee", "Andy Jinseok", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1812.10857", "submitter": "Lili Zhang", "authors": "Lili Zhang, Herman Ray, Jennifer Priestley and Soon Tan", "title": "A Descriptive Study of Variable Discretization and Cost-Sensitive\n  Logistic Regression on Imbalanced Credit Data", "comments": "Journal of Applied Statistics (2019)", "journal-ref": null, "doi": "10.1080/02664763.2019.1643829", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training classification models on imbalanced data tends to result in bias\ntowards the majority class. In this paper, we demonstrate how variable\ndiscretization and cost-sensitive logistic regression help mitigate this bias\non an imbalanced credit scoring dataset, and further show the application of\nthe variable discretization technique on the data from other domains,\ndemonstrating its potential as a generic technique for classifying imbalanced\ndata beyond credit socring. The performance measurements include ROC curves,\nArea under ROC Curve (AUC), Type I Error, Type II Error, accuracy, and F1\nscore. The results show that proper variable discretization and cost-sensitive\nlogistic regression with the best class weights can reduce the model bias\nand/or variance. From the perspective of the algorithm, cost-sensitive logistic\nregression is beneficial for increasing the value of predictors even if they\nare not in their optimized forms while maintaining monotonicity. From the\nperspective of predictors, the variable discretization performs better than\ncost-sensitive logistic regression, provides more reasonable coefficient\nestimates for predictors which have nonlinear relationships against their\nempirical logit, and is robust to penalty weights on misclassifications of\nevents and non-events determined by their apriori proportions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 01:10:13 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 14:20:37 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhang", "Lili", ""], ["Ray", "Herman", ""], ["Priestley", "Jennifer", ""], ["Tan", "Soon", ""]]}, {"id": "1812.11030", "submitter": "Viviana Vargas", "authors": "Viviana Lorena Vargas and Sinesio Pesco", "title": "Vector Field-based Simulation of Tree-Like Non-Stationary Geostatistical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new non-stationary multiple point geostatistical algorithm\ncalled vector field-based simulation is proposed. The motivation behind this\nwork is the modeling of a certain structures that exhibit directional features\nwith branching, like a tree, as can be frequently found in fan deltas or\nturbidity channels. From an image construction approach, the main idea of this\nwork is that instead of using the training image as a source of patterns, it\nmay be used to create a new object called a training vector field (TVF). This\nobject assigns a vector to each point in the reservoir within the training\nimage. The vector represents the direction in which the reservoir develops. The\nTVF is defined as an approximation of the tangent line at each point in the\ncontour curve of the reservoir. This vector field has a great potential to\nbetter capture the non-stationary nature of the training image since the vector\nnot only gives information about the point where it was defined but naturally\ncaptures the local trend near that point.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 17:45:05 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Vargas", "Viviana Lorena", ""], ["Pesco", "Sinesio", ""]]}, {"id": "1812.11335", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (GdR MASCOT-NUM), Amandine Marrel (CEA)", "title": "Advanced methodology for uncertainty propagation in computer experiments\n  with large number of inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of the estimation of safety margins in nuclear accident\nanalysis, a quantitative assessment of the uncertainties tainting the results\nof computer simulations is essential. Accurate uncertainty propagation\n(estimation of high probabilities or quantiles) and quantitative sensitivity\nanalysis may call for several thousand of code simulations. Complex computer\ncodes, as the ones used in thermal-hydraulic accident scenario simulations, are\noften too cpu-time expensive to be directly used to perform these studies. A\nsolution consists in replacing the computer model by a cpu inexpensive\nmathematical function, called a metamodel, built from a reduced number of code\nsimulations. However, in case of high dimensional experiments (with typically\nseveral tens of inputs), the metamodel building process remains difficult. To\nface this limitation, we propose a methodology which combines several advanced\nstatistical tools: initial space-filling design, screening to identify the\nnon-influential inputs, Gaussian process (Gp) metamodel building with the group\nof influential inputs as explanatory variables. The residual effect of the\ngroup of non-influential inputs is captured by another Gp metamodel. Then, the\nresulting joint Gp metamodel is used to accurately estimate Sobol' sensitivity\nindices and high quantiles (here $95\\%$-quantile).The efficiency of the\nmethodology to deal with a large number of inputs and reduce the calculation\nbudget is illustrated on a thermal-hydraulic calculation case simulating with\nthe CATHARE2 code a Loss Of Coolant Accident scenario in a Pressurized Water\nReactor. A predictive Gp metamodel is built with only a few hundred of code\nsimulations and allows the calculation of the Sobol' sensitivity indices. This\nGp also provides a more accurate estimation of the 95%-quantile and associated\nconfidence interval than the empirical approach, at equal calculation budget.\nMoreover, on this test case, the joint Gp approach outperforms the simple Gp.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 10:59:55 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Iooss", "Bertrand", "", "GdR MASCOT-NUM"], ["Marrel", "Amandine", "", "CEA"]]}, {"id": "1812.11704", "submitter": "Arnab Hazra", "authors": "Arnab Hazra, Brian J. Reich, Ana-Maria Staicu", "title": "A multivariate spatial skew-t process for joint modeling of extreme\n  precipitation indexes", "comments": "23 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study trends in extreme precipitation across US over the years 1951-2017,\nwe consider 10 climate indexes that represent extreme precipitation, such as\nannual maximum of daily precipitation, annual maximum of consecutive 5-day\naverage precipitation, which exhibit spatial correlation as well as mutual\ndependence. We consider the gridded data, produced by the CLIMDEX project\n(http://www.climdex.org/gewocs.html), constructed using daily precipitation\ndata. In this paper, we propose a multivariate spatial skew-t process for joint\nmodeling of extreme precipitation indexes and discuss its theoretical\nproperties. The model framework allows Bayesian inference while maintaining a\ncomputational time that is competitive with common multivariate geostatistical\napproaches. In a numerical study, we find that the proposed model outperforms\nmultivariate spatial Gaussian processes, multivariate spatial t-processes\nincluding their univariate alternatives in terms of various model selection\ncriteria. We apply the proposed model to estimate the average decadal change in\nthe extreme precipitation indexes throughout the United States and find several\nsignificant local changes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 06:19:15 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Hazra", "Arnab", ""], ["Reich", "Brian J.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1812.11829", "submitter": "Paul McNicholas", "authors": "Nikola Pocuca, Petar Jevtic, Paul D. McNicholas, and Tatjana Miljkovic", "title": "Modeling Frequency and Severity of Claims with the Zero-Inflated\n  Generalized Cluster-Weighted Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two important extensions to cluster-weighted models\n(CWMs). First, we extend CWMs to have generalized cluster-weighted models\n(GCWMs) by allowing modeling of non-Gaussian distribution of the continuous\ncovariates, as they frequently occur in insurance practice. Secondly, we\nintroduce a zero-inflated extension of GCWM (ZI-GCWM) for modeling insurance\nclaims data with excess zeros coming from heterogenous sources. Additionally,\nwe give two expectation-optimization (EM) algorithms for parameter estimation\ngiven the proposed models. An appropriate simulation study shows that, for\nvarious settings and in contrast to the existing mixture-based approaches, both\nextended models perform well. Finally, a real data set based on French\nauto-mobile policies is used to illustrate the application of the proposed\nextensions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:06:31 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Pocuca", "Nikola", ""], ["Jevtic", "Petar", ""], ["McNicholas", "Paul D.", ""], ["Miljkovic", "Tatjana", ""]]}]