[{"id": "1605.00021", "submitter": "Philipp Boersch-Supan", "authors": "Philipp H Boersch-Supan, Sadie J Ryan, Leah R Johnson", "title": "deBInfer: Bayesian inference for dynamical models of biological systems\n  in R", "comments": null, "journal-ref": "Methods in Ecology and Evolution 8 (2017) 511-518", "doi": "10.1111/2041-210X.12679", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. Understanding the mechanisms underlying biological systems, and\nultimately, predicting their behaviours in a changing environment requires\novercoming the gap between mathematical models and experimental or\nobservational data. Differential equations (DEs) are commonly used to model the\ntemporal evolution of biological systems, but statistical methods for comparing\nDE models to data and for parameter inference are relatively poorly developed.\nThis is especially problematic in the context of biological systems where\nobservations are often noisy and only a small number of time points may be\navailable. 2. The Bayesian approach offers a coherent framework for parameter\ninference that can account for multiple sources of uncertainty, while making\nuse of prior information. It offers a rigorous methodology for parameter\ninference, as well as modelling the link between unobservable model states and\nparameters, and observable quantities. 3. We present deBInfer, a package for\nthe statistical computing environment R, implementing a Bayesian framework for\nparameter inference in DEs. deBInfer provides templates for the DE model, the\nobservation model and data likelihood, and the model parameters and their prior\ndistributions. A Markov chain Monte Carlo (MCMC) procedure processes these\ninputs to estimate the posterior distributions of the parameters and any\nderived quantities, including the model trajectories. Further functionality is\nprovided to facilitate MCMC diagnostics, the visualisation of the posterior\ndistributions of model parameters and trajectories, and the use of compiled DE\nmodels for improved computational performance. 4. The templating approach makes\ndeBInfer applicable to a wide range of DE models. We demonstrate its\napplication to ordinary and delay DE models for population ecology.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 20:41:31 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 03:24:21 GMT"}, {"version": "v3", "created": "Sat, 15 Oct 2016 13:48:45 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Boersch-Supan", "Philipp H", ""], ["Ryan", "Sadie J", ""], ["Johnson", "Leah R", ""]]}, {"id": "1605.00040", "submitter": "Anderson Ara", "authors": "Francisco Louzada and Anderson Ara", "title": "MWStat: A Modulated Web-Based Statistical System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the development of a modulated web based statistical\nsystem, hereafter MWStat, which shifts the statistical paradigm of analyzing\ndata into a real time structure. The MWStat system is useful for both online\nstorage data and questionnaires analysis, as well as to provide real time\ndisposal of results from analysis related to several statistical methodologies\nin a customizable fashion. Overall, it can be seem as a useful technical\nsolution that can be applied to a large range of statistical applications,\nwhich needs of a scheme of devolution of real time results, accessible to\nanyone with internet access. We display here the step-by-step instructions for\nimplementing the system. The structure is accessible, built with an easily\ninterpretable language and it can be strategically applied to online\nstatistical applications. We rely on the relationship of several free\nlanguages, namely, PHP, R, MySQL database and an Apache HTTP server, and on the\nuse of software tools such as phpMyAdmin. We expose three didactical examples\nof the MWStat system on institutional evaluation, statistical quality control\nand multivariate analysis. The methodology is also illustrated in a real\nexample on institutional evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 23:07:20 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Louzada", "Francisco", ""], ["Ara", "Anderson", ""]]}, {"id": "1605.00104", "submitter": "Michael Chipeta Mr", "authors": "Michael G. Chipeta, Dianne J. Terlouw, Kamija S. Phiri, Peter J.\n  Diggle", "title": "Inhibitory geostatistical designs for spatial prediction taking account\n  of uncertain covariance structure", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of choosing spatial sampling designs for investigating unobserved\nspatial phenomenon S arises in many contexts, for example in identifying\nhouseholds to select for a prevalence survey to study disease burden and\nheterogeneity in a study region D. We studied randomised inhibitory spatial\nsampling designs to address the problem of spatial prediction whilst taking\naccount of the need to estimate covariance structure. Two specific classes of\ndesign are inhibitory designs and inhibitory designs plus close pairs. In an\ninhibitory design, any pair of sample locations must be separated by at least\nan inhibition distance {$\\delta$}. In an inhibitory plus close pairs design, n\n- k sample locations in an inhibitory design with inhibition distance\n{$\\delta$} are augmented by k locations each positioned close to one of the\nrandomly selected n - k locations in the inhibitory design, uniformly\ndistributed within a disc of radius {$\\zeta$}. We present simulation results\nfor the Matern class of covariance structures. When the nugget variance is\nnon-negligible, inhibitory plus close pairs designs demonstrate improved\npredictive efficiency over designs without close pairs. We illustrate how these\nfindings can be applied to the design of a rolling Malaria Indicator Survey\nthat forms part of an ongoing large-scale, five-year malaria transmission\nreduction project in Malawi.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 13:07:15 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Chipeta", "Michael G.", ""], ["Terlouw", "Dianne J.", ""], ["Phiri", "Kamija S.", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1605.00155", "submitter": "Chad Hazlett", "authors": "Chad Hazlett", "title": "Kernel Balancing: A flexible non-parametric weighting procedure for\n  estimating causal effects", "comments": "Work originally included in PhD Thesis, May 2014, MIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of unobserved confounders, matching and weighting methods are\nwidely used to estimate causal quantities including the Average Treatment\nEffect on the Treated (ATT). Unfortunately, these methods do not necessarily\nachieve their goal of making the multivariate distribution of covariates for\nthe control group identical to that of the treated, leaving some (potentially\nmultivariate) functions of the covariates with different means between the two\ngroups. When these \"imbalanced\" functions influence the non-treatment potential\noutcome, the conditioning on observed covariates fails, and ATT estimates may\nbe biased. Kernel balancing, introduced here, targets a weaker requirement for\nunbiased ATT estimation, specifically, that the expected non-treatment\npotential outcome for the treatment and control groups are equal. The\nconditional expectation of the non-treatment potential outcome is assumed to\nfall in the space of functions associated with a choice of kernel, implying a\nset of basis functions in which this regression surface is linear. Weights are\nthen chosen on the control units such that the treated and control group have\nequal means on these basis functions. As a result, the expectation of the\nnon-treatment potential outcome must also be equal for the treated and control\ngroups after weighting, allowing unbiased ATT estimation by subsequent\ndifference in means or an outcome model using these weights. Moreover, the\nweights produced are (1) precisely those that equalize a particular\nkernel-based approximation of the multivariate distribution of covariates for\nthe treated and control, and (2) equivalent to a form of stabilized inverse\npropensity score weighting, though it does not require assuming any model of\nthe treatment assignment mechanism. An R package, KBAL, is provided to\nimplement this approach.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 19:49:20 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Hazlett", "Chad", ""]]}, {"id": "1605.00230", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania and Nima Nonejad", "title": "Density Forecasts and the Leverage Effect: Some Evidence from\n  Observation and Parameter-Driven Volatility Models", "comments": "36 pages, 2 figures, 20 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leverage effect refers to the well-established relationship between\nreturns and volatility. When returns fall, volatility increases. We examine the\nrole of the leverage effect with regards to generating density forecasts of\nequity returns using well-known observation and parameter-driven volatility\nmodels. These models differ in their assumptions regarding: The parametric\nspecification, the evolution of the conditional volatility process and how the\nleverage effect is accounted for. The ability of a model to generate accurate\ndensity forecasts when the leverage effect is incorporated or not as well as a\ncomparison between different model-types is carried out using a large number of\nfinancial time-series. We find that, models with the leverage effect generally\ngenerate more accurate density forecasts compared to their no-leverage\ncounterparts. Moreover, we also find that our choice with regards to how to\nmodel the leverage effect and the conditional log-volatility process is\nimportant in generating accurate density forecasts\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 10:16:29 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 18:52:46 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Catania", "Leopoldo", ""], ["Nonejad", "Nima", ""]]}, {"id": "1605.00249", "submitter": "Kevin Kunzmann", "authors": "Kevin Kunzmann and Meinhard Kieser", "title": "Optimal adaptive two-stage designs for single-arm trial with binary\n  endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the number of patients exposed to potentially harmful drugs in\nearly onco logical trials is a major concern during planning. Adaptive designs\naccount for the inherent uncertainty about the true effect size by determining\nthe final sample size within an ongoing trial after an interim look at the\ndata. We formulate the problem of finding adaptive designs which minimize\nexpected sample size under the null hypothesis for single-arm trials with\nbinary outcome as an integer linear program. This representation can be used to\nidentify optimal adaptive designs which improve previous designs in two ways:\nFirstly, designs can be found exhibiting lower expected sample size under the\nnull hypothesis than those provided by previous algorithms. Secondly, we\nexplain how integer programming techniques can be exploited to remove\npathologies of the optimal and previous solutions arising from the discrete\nnature of the underlying statistics. The resulting designs are both efficient\nin terms of expected sample size under the null hypothesis and well\ninterpretable.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 13:14:49 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Kunzmann", "Kevin", ""], ["Kieser", "Meinhard", ""]]}, {"id": "1605.00282", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Elizabeth Hou, Alfred O. Hero", "title": "Online Diversion Detection in Nuclear Fuel Cycles via Multimodal\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nuclear fuel cycles, an enrichment facility typically provides low\nenriched uranium (LEU) to a number of customers. We consider monitoring an\nenrichment facility to timely detect a possible diversion of highly enriched\nuranium (HEU). To increase the the detection accuracy it is important to\nefficiently use the available information diversity. In this work, it is\nassumed that the shipment times and the power consumption of the enrichment\nfacility are observed for each shipment of enriched uranium. We propose to\ninitially learn the statistical patterns of the enrichment facility through the\nbimodal observations in a training period, that is known to be free of\ndiversions. Then, for the goal of timely diversion detection, we propose to use\nan online detection algorithm which sequentially compares each set of new\nobservations in the test period, which possibly includes diversions, to the\nlearned patterns, and raises a diversion alarm when a significant statistical\ndeviation is detected. The efficacy of the proposed method is shown by\ncomparing its detection performance to those of the traditional detection\nmethods in the Statistics literature.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 17:46:16 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 02:13:07 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Hou", "Elizabeth", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1605.00328", "submitter": "Jessica Tran", "authors": "Ritvik Kharkar, Jessica Tran, Charles Z. Marshak", "title": "Core Course Analysis for Undergraduate Students in Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop statistical tools to understand core courses at the\nuniversity level. Traditionally, professors and administrators label courses as\n\"core\" when the courses contain foundational material. Such courses are often\nrequired to complete a major, and, in some cases, allocated additional\neducational resources. We identify two key attributes which we expect core\ncourses to have. Namely, we expect core courses to be highly correlated with\nand highly impactful on a student's overall mathematics GPA. We use two\nstatistical procedures to measure the strength of these attributes across\ncourses. The first of these procedures fashions a metric out of standard\ncorrelation measures. The second utilizes sparse regression. We apply these\nmethods on student data coming from the University of California, Los Angeles\n(UCLA) department of mathematics to compare core and non-core coursework.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 01:23:37 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Kharkar", "Ritvik", ""], ["Tran", "Jessica", ""], ["Marshak", "Charles Z.", ""]]}, {"id": "1605.00391", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Arthur Gretton, Bernhard Sch\\\"olkopf, Moritz\n  Grosse-Wentrup", "title": "Recovery of non-linear cause-effect relationships from linearly mixed\n  neuroimaging data", "comments": "arXiv admin note: text overlap with arXiv:1512.01255", "journal-ref": "Pattern Recognition in Neuroimaging (PRNI), International Workshop\n  on, 1-4, 2016", "doi": "10.1109/PRNI.2016.7552331", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables. However, often only linear combinations of variables\nconstitute meaningful causal variables. For example, recovering the signal of a\ncortical source from electroencephalography requires a well-tuned combination\nof signals recorded at multiple electrodes. We recently introduced the MERLiN\n(Mixture Effect Recovery in Linear Networks) algorithm that is able to recover,\nfrom an observed linear mixture, a causal variable that is a linear effect of\nanother given variable. Here we relax the assumption of this cause-effect\nrelationship being linear and present an extended algorithm that can pick up\nnon-linear cause-effect relationships. Thus, the main contribution is an\nalgorithm (and ready to use code) that has broader applicability and allows for\na richer model class. Furthermore, a comparative analysis indicates that the\nassumption of linear cause-effect relationships is not restrictive in analysing\nelectroencephalographic data.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 08:45:59 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 20:01:00 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1605.00779", "submitter": "Sipan Aslan", "authors": "Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun", "title": "Temporal Clustering of Time Series via Threshold Autoregressive Models:\n  Application to Commodity Prices", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aimed to find temporal clusters for several commodity prices using\nthe threshold non-linear autoregressive model. It is expected that the process\nof determining the commodity groups that are time-dependent will advance the\ncurrent knowledge about the dynamics of co-moving and coherent prices, and can\nserve as a basis for multivariate time series analyses. The clustering of\ncommodity prices was examined using the proposed clustering approach based on\ntime series models to incorporate the time varying properties of price series\ninto the clustering scheme. Accordingly, the primary aim in this study was\ngrouping time series according to the similarity between their Data Generating\nMechanisms (DGMs) rather than comparing pattern similarities in the time series\ntraces. The approximation to the DGM of each series was accomplished using\nthreshold autoregressive models, which are recognized for their ability to\nrepresent nonlinear features in time series, such as abrupt changes,\ntime-irreversibility and regime-shifting behavior. Through the use of the\nproposed approach, one can determine and monitor the set of co-moving time\nseries variables across the time dimension. Furthermore, generating a time\nvarying commodity price index and sub-indexes can become possible.\nConsequently, we conducted a simulation study to assess the effectiveness of\nthe proposed clustering approach and the results are presented for both the\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 08:13:58 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Aslan", "Sipan", ""], ["Yozgatligil", "Ceylan", ""], ["Iyigun", "Cem", ""]]}, {"id": "1605.00814", "submitter": "Marta Blangiardo", "authors": "Yingbo Wang, Sylvia Richardson, Anna Hansell, Marta Blangiardo", "title": "Using Ecological Propensity Score to Adjust for Missing Confounders in\n  Small Area Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area ecological studies are commonly used in epidemiology to assess the\nimpact of area level risk factors on health outcomes when data are only\navailable in an aggregated form. However the resulting estimates are often\nbiased due to unmeasured confounders, which typically are not available from\nthe standard administrative registries used for these studies. Extra\ninformation on confounders can be provided through external datasets such as\nsurveys or cohorts, where the data are available at the individual level rather\nthan at the area level; however such data typically lack the geographical\ncoverage of administrative registries. We develop a framework of analysis which\ncombines ecological and individual level data from different sources to provide\nan adjusted estimate of area level risk factors which is less biased. Our\nmethod (i) summarises all available individual level confounders into an area\nlevel scalar variable, which we call ecological propensity score (EPS), (ii)\nimplements a hierarchical structured approach to predict the values of EPS\nwhenever they are missing, (iii) includes the estimated and predicted EPS into\nthe ecological regression linking the risk factors to the health outcome.\nThrough a simulation study we show that integrating individual level data into\nsmall area analyses via EPS is a promising method to reduce the bias intrinsic\nin ecological studies due to unmeasured confounders; we also apply the method\nto a real case study to evaluate the effect of air pollution on coronary heart\ndisease hospital admissions in Greater London.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 09:40:24 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 12:09:09 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Wang", "Yingbo", ""], ["Richardson", "Sylvia", ""], ["Hansell", "Anna", ""], ["Blangiardo", "Marta", ""]]}, {"id": "1605.01038", "submitter": "Florian Gerber", "authors": "Florian Gerber, Reinhard Furrer, Gabriela Schaepman-Strub, Rogier de\n  Jong, Michael E. Schaepman", "title": "Predicting missing values in spatio-temporal satellite data", "comments": "35 pages", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Volume 55,\n  Issue 5, 2841-2853, 2018", "doi": "10.1109/TGRS.2017.2785240", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remotely sensed data are sparse, which means that data have missing values,\nfor instance due to cloud cover. This is problematic for applications and\nsignal processing algorithms that require complete data sets. To address the\nsparse data issue, we present a new gap-fill algorithm. The proposed method\npredicts each missing value separately based on data points in a\nspatio-temporal neighborhood around the missing data point. The computational\nworkload can be distributed among several computers, making the method suitable\nfor large datasets. The prediction of the missing values and the estimation of\nthe corresponding prediction uncertainties are based on sorting procedures and\nquantile regression. The algorithm was applied to MODIS NDVI data from Alaska\nand tested with realistic cloud cover scenarios featuring up to 50% missing\ndata. Validation against established software showed that the proposed method\nhas a good performance in terms of the root mean squared prediction error. The\nprocedure is implemented and available in the open-source R package gapfill. We\ndemonstrate the software performance with a real data example and show how it\ncan be tailored to specific data. Due to the flexible software design, users\ncan control and redesign major parts of the procedure with little effort. This\nmakes it an interesting tool for gap-filling satellite data and for the future\ndevelopment of gap-fill procedures.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 19:32:17 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Gerber", "Florian", ""], ["Furrer", "Reinhard", ""], ["Schaepman-Strub", "Gabriela", ""], ["de Jong", "Rogier", ""], ["Schaepman", "Michael E.", ""]]}, {"id": "1605.01311", "submitter": "Christian Kleiber", "authors": "Christian Kleiber and Achim Zeileis", "title": "Visualizing Count Data Regressions Using Rootograms", "comments": "19 pages, 7 figures", "journal-ref": "The American Statistician, 2016, Vol. 70, No. 3, 296-303", "doi": "10.1080/00031305.2016.1173590", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rootogram is a graphical tool associated with the work of J. W. Tukey\nthat was originally used for assessing goodness of fit of univariate\ndistributions. Here we extend the rootogram to regression models and show that\nthis is particularly useful for diagnosing and treating issues such as\noverdispersion and/or excess zeros in count data models. We also introduce a\nweighted version of the rootogram that can be applied out of sample or to\n(weighted) subsets of the data, e.g., in finite mixture models. An empirical\nillustration revisiting a well-known data set from ethology is included, for\nwhich a negative binomial hurdle model is employed. Supplementary materials\nproviding two further illustrations are available online: the first, using data\nfrom public health, employs a two-component finite mixture of negative binomial\nmodels, the second, using data from finance, involves underdispersion. An R\nimplementation of our tools is available in the R package countreg. It also\ncontains the data and replication code.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 15:16:38 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Kleiber", "Christian", ""], ["Zeileis", "Achim", ""]]}, {"id": "1605.01421", "submitter": "Stefan Widgren", "authors": "Stefan Widgren, Pavol Bauer, Robin Eriksson, Stefan Engblom", "title": "SimInf: An R package for Data-driven Stochastic Disease Spread\n  Simulations", "comments": "The manual has been updated to the latest version of SimInf (v6.0.0).\n  41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the R package SimInf which provides an efficient and very flexible\nframework to conduct data-driven epidemiological modeling in realistic large\nscale disease spread simulations. The framework integrates infection dynamics\nin subpopulations as continuous-time Markov chains using the Gillespie\nstochastic simulation algorithm and incorporates available data such as births,\ndeaths and movements as scheduled events at predefined time-points. Using C\ncode for the numerical solvers and OpenMP to divide work over multiple\nprocessors ensures high performance when simulating a sample outcome. One of\nour design goal was to make SimInf extendable and enable usage of the numerical\nsolvers from other R extension packages in order to facilitate complex\nepidemiological research. In this paper, we provide a technical description of\nthe framework and demonstrate its use on some basic examples. We also discuss\nhow to specify and extend the framework with user-defined models.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 20:16:20 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 19:04:00 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 16:56:46 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Widgren", "Stefan", ""], ["Bauer", "Pavol", ""], ["Eriksson", "Robin", ""], ["Engblom", "Stefan", ""]]}, {"id": "1605.01526", "submitter": "Alberto Carrassi", "authors": "Alberto Carrassi, Marc Bocquet, Alexis Hannart and Michael Ghil", "title": "Estimating model evidence using data assimilation", "comments": null, "journal-ref": null, "doi": "10.1002/qj.2972", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the field of data assimilation (DA) from a Bayesian perspective and\nshow that, in addition to its by now common application to state estimation, DA\nmay be used for model selection. An important special case of the latter is the\ndiscrimination between a factual model --- which corresponds, to the best of\nthe modeler's knowledge, to the situation in the actual world in which a\nsequence of events has occurred --- and a counterfactual model, in which a\nparticular forcing or process might be absent or just quantitatively different\nfrom the actual world. Three different ensemble-DA methods are reviewed for\nthis purpose: the ensemble Kalman filter (EnKF), the ensemble four-dimensional\nvariational smoother (En-4D-Var), and the iterative ensemble Kalman smoother\n(IEnKS). An original contextual formulation of model evidence (CME) is\nintroduced. It is shown how to apply these three methods to compute CME, using\nthe approximated time-dependent probability distribution functions (pdfs) each\nof them provide in the process of state estimation. The theoretical formulae so\nderived are applied to two simplified nonlinear and chaotic models: (i) the\nLorenz three-variable convection (L63) model, and (ii) the Lorenz 40-variable\nmid-latitude atmospheric dynamics model (L95). The numerical results of these\nthree DA-based methods and those of an integration based on importance sampling\nare compared. It is found that better CME estimates are obtained by using DA,\nand the IEnKS method appears to be best among the DA methods. Differences among\nthe performance of the three DA-based methods are discussed as a function of\nmodel properties. Finally, the methodology is implemented for parameter\nestimation and for event attribution.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 08:21:17 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 18:34:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Carrassi", "Alberto", ""], ["Bocquet", "Marc", ""], ["Hannart", "Alexis", ""], ["Ghil", "Michael", ""]]}, {"id": "1605.01598", "submitter": "Eric Schulz", "authors": "Eric Schulz, Maarten Speekenbrink and Bj\\\"orn Meder", "title": "Simple trees in complex forests: Growing Take The Best by Approximate\n  Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can heuristic strategies emerge from smaller building blocks? We propose\nApproximate Bayesian Computation as a computational solution to this problem.\nAs a first proof of concept, we demonstrate how a heuristic decision strategy\nsuch as Take The Best (TTB) can be learned from smaller, probabilistically\nupdated building blocks. Based on a self-reinforcing sampling scheme, different\nbuilding blocks are combined and, over time, tree-like non-compensatory\nheuristics emerge. This new algorithm, coined Approximately Bayesian Computed\nTake The Best (ABC-TTB), is able to recover a data set that was generated by\nTTB, leads to sensible inferences about cue importance and cue directions, can\noutperform traditional TTB, and allows to trade-off performance and\ncomputational effort explicitly.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 13:59:37 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 14:41:13 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Schulz", "Eric", ""], ["Speekenbrink", "Maarten", ""], ["Meder", "Bj\u00f6rn", ""]]}, {"id": "1605.01889", "submitter": "Francisco Javier Rubio", "authors": "F. J. Rubio and K. Yu", "title": "Flexible objective Bayesian linear regression with applications in\n  survival analysis", "comments": null, "journal-ref": "Journal of Applied Statistics (2016)", "doi": "10.1080/02664763.2016.1182138", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study objective Bayesian inference for linear regression models with\nresidual errors distributed according to the class of two-piece scale mixtures\nof normal distributions. These models allow for capturing departures from the\nusual assumption of normality of the errors in terms of heavy tails, asymmetry,\nand certain types of heteroscedasticity. We propose a general noninformative,\nscale-invariant, prior structure and provide sufficient conditions for the\npropriety of the posterior distribution of the model parameters, which cover\ncases when the response variables are censored. These results allow us to apply\nthe proposed models in the context of survival analysis. This paper represents\nan extension to the Bayesian framework of the models proposed in Rubio and Hong\n(2015). We present a simulation study that shows good frequentist properties of\nthe posterior credible intervals as well as point estimators associated to the\nproposed priors. We illustrate the performance of these models with real data\nin the context of survival analysis of cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 11:01:32 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Rubio", "F. J.", ""], ["Yu", "K.", ""]]}, {"id": "1605.02082", "submitter": "Amy Willis", "authors": "Amy Willis, John Bunge, and Thea Whitman", "title": "Improved detection of changes in species richness in high-diversity\n  microbial communities", "comments": "arXiv admin note: text overlap with arXiv:1506.05710", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput sequencing (HTS) continues to expand our understanding of\nmicrobial communities, despite insufficient sequencing depths to detect all\nrare taxa. These low abundance taxa are not accounted for in existing methods\nfor detecting changes in species richness. We address this with a new\nhierarchical model that permits rigorous testing for both heterogeneity and\nbiodiversity changes, and simultaneously improves Type I & II error rates\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 20:17:08 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Willis", "Amy", ""], ["Bunge", "John", ""], ["Whitman", "Thea", ""]]}, {"id": "1605.02231", "submitter": "Hudson Golino", "authors": "Hudson F. Golino and Sacha Epskamp", "title": "Exploratory graph analysis: a new approach for estimating the number of\n  dimensions in psychological research", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0174035", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The estimation of the correct number of dimensions is a long-standing problem\nin psychometrics. Several methods have been proposed, such as parallel analysis\n(PA), Kaiser-Guttman's eigenvalue-greaterthan-one rule, multiple average\npartial procedure (MAP), the maximum-likelihood approaches that use fit indexes\nas BIC and EBIC and the less used and studied approach called very simple\nstructure (VSS). In the present paper a new approach to estimate the number of\ndimensions will be introduced and compared via simulation to the traditional\ntechniques pointed above. The approach proposed in the current paper is called\nexploratory graph analysis (EGA), since it is based on the graphical lasso with\nthe regularization parameter specified using EBIC. The number of dimensions is\nverified using the walktrap, a random walk algorithm used to identify\ncommunities in networks. In total, 32,000 data sets were simulated to fit known\nfactor structures, with the data sets varying across different criteria: number\nof factors (2 and 4), number of items (5 and 10), sample size (100, 500, 1000\nand 5000) and correlation between factors (orthogonal, .20, .50 and .70),\nresulting in 64 different conditions. For each condition, 500 data sets were\nsimulated using lavaan. The result shows that the EGA performs comparable to\nparallel analysis, EBIC, eBIC and to KaiserGuttman rule in a number of\nsituations, especially when the number of factors was two. However, EGA was the\nonly technique able to correctly estimate the number of dimensions in the\nfour-factor structure when the correlation between factors were .7, showing an\naccuracy of 100% for a sample size of 5,000 observations. Finally, the EGA was\nused to estimate the number of factors in a real dataset, in order to compare\nits performance with the other six techniques tested in the simulation study.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 18:41:46 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 16:02:18 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 17:20:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Golino", "Hudson F.", ""], ["Epskamp", "Sacha", ""]]}, {"id": "1605.02234", "submitter": "Farouk Nathoo", "authors": "Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance and\n  Farouk S. Nathoo", "title": "A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recent advances in technology for brain imaging and\nhigh-throughput genotyping have motivated studies examining the influence of\ngenetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have\ndeveloped an approach for the analysis of imaging genomic studies using\npenalized multi-task regression with regularization based on a novel group\n$l_{2,1}$-norm penalty which encourages structured sparsity at both the gene\nlevel and SNP level. While incorporating a number of useful features, the\nproposed method only furnishes a point estimate of the regression coefficients;\ntechniques for conducting statistical inference are not provided. A new\nBayesian method is proposed here to overcome this limitation.\n  Results: We develop a Bayesian hierarchical modeling formulation where the\nposterior mode corresponds to the estimator proposed by Wang et al.\n(Bioinformatics, 2012), and an approach that allows for full posterior\ninference including the construction of interval estimates for the regression\nparameters. We show that the proposed hierarchical model can be expressed as a\nthree-level Gaussian scale mixture and this representation facilitates the use\nof a Gibbs sampling algorithm for posterior simulation. Simulation studies\ndemonstrate that the interval estimates obtained using our approach achieve\nadequate coverage probabilities that outperform those obtained from the\nnonparametric bootstrap. Our proposed methodology is applied to the analysis of\nneuroimaging and genetic data collected as part of the Alzheimer's Disease\nNeuroimaging Initiative (ADNI), and this analysis of the ADNI cohort\ndemonstrates clearly the value added of incorporating interval estimation\nbeyond only point estimation when relating SNPs to brain imaging\nendophenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 19:16:53 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:46:27 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Greenlaw", "Keelin", ""], ["Szefer", "Elena", ""], ["Graham", "Jinko", ""], ["Lesperance", "Mary", ""], ["Nathoo", "Farouk S.", ""]]}, {"id": "1605.02351", "submitter": "Boris Hejblum", "authors": "Denis Agniel and Boris P Hejblum", "title": "Variance component score test for time-course gene set analysis of\n  longitudinal RNA-seq data", "comments": "23 pages, 6 figures, typo corrections & acceptance acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As gene expression measurement technology is shifting from microarrays to\nsequencing, the statistical tools available for their analysis must be adapted\nsince RNA-seq data are measured as counts. Recently, it has been proposed to\ntackle the count nature of these data by modeling log-count reads per million\nas continuous variables, using nonparametric regression to account for their\ninherent heteroscedasticity. Adopting such a framework, we propose tcgsaseq, a\nprincipled, model-free and efficient top-down method for detecting longitudinal\nchanges in RNA-seq gene sets. Considering gene sets defined a priori, tcgsaseq\nidentifies those whose expression vary over time, based on an original variance\ncomponent score test accounting for both covariates and heteroscedasticity\nwithout assuming any specific parametric distribution for the transformed\ncounts. We demonstrate that despite the presence of a nonparametric component,\nour test statistic has a simple form and limiting distribution, and both may be\ncomputed quickly. A permutation version of the test is additionally proposed\nfor very small sample sizes. Applied to both simulated data and two real\ndatasets, the proposed method is shown to exhibit very good statistical\nproperties, with an increase in stability and power when compared to state of\nthe art methods ROAST, edgeR and DESeq2, which can fail to control the type I\nerror under certain realistic settings. We have made the method available for\nthe community in the R package tcgsaseq.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 19:21:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 20:01:35 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 04:20:31 GMT"}, {"version": "v4", "created": "Thu, 5 Jan 2017 11:48:37 GMT"}, {"version": "v5", "created": "Fri, 6 Jan 2017 14:47:20 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Agniel", "Denis", ""], ["Hejblum", "Boris P", ""]]}, {"id": "1605.02418", "submitter": "Pritam Ranjan", "authors": "Sujay Mukhoti, Pritam Ranjan", "title": "Mean-correction and Higher Order Moments for a Stochastic Volatility\n  Model with Correlated Errors", "comments": "15 pages; 5 figures, submitted to IJSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an efficient stock market, the log-returns and their time-dependent\nvariances are often jointly modelled by stochastic volatility models (SVMs).\nMany SVMs assume that errors in log-return and latent volatility process are\nuncorrelated, which is unrealistic. It turns out that if a non-zero correlation\nis included in the SVM (e.g., Shephard (2005)), then the expected log-return at\ntime t conditional on the past returns is non-zero, which is not a desirable\nfeature of an efficient stock market. In this paper, we propose a\nmean-correction for such an SVM for discrete-time returns with non-zero\ncorrelation. We also find closed form analytical expressions for higher moments\nof log-return and its lead-lag correlations with the volatility process. We\ncompare the performance of the proposed and classical SVMs on S&P 500 index\nreturns obtained from NYSE.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 05:01:35 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Mukhoti", "Sujay", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1605.02540", "submitter": "Fabrice Rossi", "authors": "Marco Corneli (SAMM), Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Exact ICL maximization in a non-stationary temporal extension of the\n  stochastic block model for dynamic networks", "comments": null, "journal-ref": "Neurocomputing, Elsevier, 2016, Advances in artificial neural\n  networks, machine learning and computational intelligence - Selected papers\n  from the 23rd European Symposium on Artificial Neural Networks (ESANN 2015),\n  192, pp.81-91", "doi": "10.1016/j.neucom.2016.02.031", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a flexible probabilistic tool that can be\nused to model interactions between clusters of nodes in a network. However, it\ndoes not account for interactions of time varying intensity between clusters.\nThe extension of the SBM developed in this paper addresses this shortcoming\nthrough a temporal partition: assuming interactions between nodes are recorded\non fixed-length time intervals, the inference procedure associated with the\nmodel we propose allows to cluster simultaneously the nodes of the network and\nthe time intervals. The number of clusters of nodes and of time intervals, as\nwell as the memberships to clusters, are obtained by maximizing an exact\nintegrated complete-data likelihood, relying on a greedy search approach.\nExperiments on simulated and real data are carried out in order to assess the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:44:24 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 09:37:27 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Corneli", "Marco", "", "SAMM"], ["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1605.02560", "submitter": "Zi Wang", "authors": "Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana", "title": "Studying the brain from adolescence to adulthood through sparse\n  multi-view matrix factorisations", "comments": "Submitted to the 6th International Workshop on Pattern Recognition in\n  Neuroimaging (PRNI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Men and women differ in specific cognitive abilities and in the expression of\nseveral neuropsychiatric conditions. Such findings could be attributed to sex\nhormones, brain differences, as well as a number of environmental variables.\nExisting research on identifying sex-related differences in brain structure\nhave predominantly used cross-sectional studies to investigate, for instance,\ndifferences in average gray matter volumes (GMVs). In this article we explore\nthe potential of a recently proposed multi-view matrix factorisation (MVMF)\nmethodology to study structural brain changes in men and women that occur from\nadolescence to adulthood. MVMF is a multivariate variance decomposition\ntechnique that extends principal component analysis to \"multi-view\" datasets,\ni.e. where multiple and related groups of observations are available. In this\napplication, each view represents a different age group. MVMF identifies latent\nfactors explaining shared and age-specific contributions to the observed\noverall variability in GMVs over time. These latent factors can be used to\nproduce low-dimensional visualisations of the data that emphasise age-specific\neffects once the shared effects have been accounted for. The analysis of two\ndatasets consisting of individuals born prematurely as well as healthy controls\nprovides evidence to suggest that the separation between males and females\nbecomes increasingly larger as the brain transitions from adolescence to\nadulthood. We report on specific brain regions associated to these variance\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 12:40:22 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Wang", "Zi", ""], ["Karolis", "Vyacheslav", ""], ["Nosarti", "Chiara", ""], ["Montana", "Giovanni", ""]]}, {"id": "1605.02561", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (LNE, L2S, GdR MASCOT-NUM), Julien Bect (L2S, GdR\n  MASCOT-NUM), S\\'everine Demeyer (LNE), Nicolas Fischer (LNE), Emmanuel\n  Vazquez (L2S, GdR MASCOT-NUM)", "title": "Gaussian process modeling for stochastic multi-fidelity simulators, with\n  application to fire safety", "comments": null, "journal-ref": "48{\\`e}mes Journ{\\'e}es de Statistique de la SFdS (JdS 2016), May\n  2016, Montpellier, France", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the possibility of evacuating a building in case of a fire, a\nstandard method consists in simulating the propagation of fire, using finite\ndifference methods and takes into account the random behavior of the fire, so\nthat the result of a simulation is non-deterministic. The mesh fineness tunes\nthe quality of the numerical model, and its computational cost. Depending on\nthe mesh fineness, one simulation can last anywhere from a few minutes to\nseveral weeks. In this article, we focus on predicting the behavior of the fire\nsimulator at fine meshes, using cheaper results, at coarser meshes. In the\nliterature of the design and analysis of computer experiments, such a problem\nis referred to as multi-fidelity prediction. Our contribution is to extend to\nthe case of stochastic simulators the Bayesian multi-fidelity model proposed by\nPicheny and Ginsbourger (2013) and Tuo et al. (2014).\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 12:41:27 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE, L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR\n  MASCOT-NUM"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "1605.03280", "submitter": "Neelesh Upadhye Dr", "authors": "Rakshith Jagannath and Neelesh S Upadhye", "title": "The LASSO Estimator: Distributional Properties", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least absolute shrinkage and selection operator (LASSO) is a popular\ntechnique for simultaneous estimation and model selection. There have been a\nlot of studies on the large sample asymptotic distributional properties of the\nLASSO estimator, but it is also well-known that the asymptotic results can give\na wrong picture of the LASSO estimator's actual finite-sample behavior. The\nfinite sample distribution of the LASSO estimator has been previously studied\nfor the special case of orthogonal models. The aim in this work is to\ngeneralize the finite sample distribution properties of LASSO estimator for a\nreal and linear measurement model in Gaussian noise.\n  In this work, we derive an expression for the finite sample characteristic\nfunction of the LASSO estimator, we then use the Fourier slice theorem to\nobtain an approximate expression for the marginal probability density functions\nof the one-dimensional components of a linear transformation of the LASSO\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 04:47:59 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 04:43:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Jagannath", "Rakshith", ""], ["Upadhye", "Neelesh S", ""]]}, {"id": "1605.03325", "submitter": "Luca Barbaglia", "authors": "Ines Wilms, Luca Barbaglia and Christophe Croux", "title": "Multi-class Vector AutoRegressive Models for Multi-store Sales Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retailers use the Vector AutoRegressive (VAR) model as a standard tool to\nestimate the effects of prices, promotions and sales in one product category on\nthe sales of another product category. Besides, these price, promotion and\nsales data are available for not just one store, but a whole chain of stores.\nWe propose to study cross-category effects using a multi-class VAR model: we\njointly estimate cross-category effects for several distinct but related VAR\nmodels, one for each store. Our methodology encourages effects to be similar\nacross stores, while still allowing for small differences between stores to\naccount for store heterogeneity. Moreover, our estimator is sparse: unimportant\neffects are estimated as exactly zero, which facilitates the interpretation of\nthe results. A simulation study shows that the proposed multi-class estimator\nimproves estimation accuracy by borrowing strength across classes. Finally, we\nprovide three visual tools showing (i) the clustering of stores on identical\ncross-category effects, (ii) the networks of product categories and (iii) the\nsimilarity matrices of shared cross-category effects across stores.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:32:13 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Wilms", "Ines", ""], ["Barbaglia", "Luca", ""], ["Croux", "Christophe", ""]]}, {"id": "1605.03486", "submitter": "Alexander Tybl", "authors": "Alexander J. Tybl", "title": "An Overview of Spatial Econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers an expository overview of the field of spatial\neconometrics. It first justifies the necessity of special statistical\nprocedures for the analysis of spatial data and then proceeds to describe the\nfundamentals of these procedures. In particular, this paper covers three\ncrucial techniques for building models with spatial data. First, we discuss how\nto create a spatial weights matrix based on the distances between each data\npoint in a dataset. Next, we describe the conventional methods to formally\ndetect spatial autocorrelation, both global and local. Finally, we outline the\nchief components of a spatial autoregressive model, noting the circumstances\nunder which it would be appropriate to incorporate each component into a model.\nThis paper seeks to offer a concise introduction to spatial econometrics that\nwill be accessible to interested individuals with a background in statistics or\neconometrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:52:25 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Tybl", "Alexander J.", ""]]}, {"id": "1605.03508", "submitter": "Paul Sharkey", "authors": "Paul Sharkey, Jonathan A. Tawn", "title": "A Poisson process reparameterisation for Bayesian inference for extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common approach to modelling extreme values is to consider the excesses\nabove a high threshold as realisations of a non-homogeneous Poisson process.\nWhile this method offers the advantage of modelling using threshold-invariant\nextreme value parameters, the dependence between these parameters makes\nestimation more difficult. We present a novel approach for Bayesian estimation\nof the Poisson process model parameters by reparameterising in terms of a\ntuning parameter $m$. This paper presents a method for choosing the optimal\nvalue of m that near-orthogonalises the parameters, which is achieved by\nminimising the correlation between the asymptotic posterior distribution of the\nparameters. This choice of m ensures more rapid convergence and efficient\nsampling from the joint posterior distribution using Markov Chain Monte Carlo\nmethods. Samples from the parameterisation of interest are then obtained by a\nsimple transform. Results are presented in the cases of identically and\nnon-identically distributed models for extreme rainfall in Cumbria, UK.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 16:51:00 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 17:39:13 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Sharkey", "Paul", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1605.03620", "submitter": "Mianzhi Wang", "authors": "Mianzhi Wang and Arye Nehorai", "title": "Coarrays, MUSIC, and the Cram\\'er Rao Bound", "comments": "Revised Corollary 2. Added Fig. 6", "journal-ref": null, "doi": "10.1109/TSP.2016.2626255", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear arrays, such as co-prime arrays and nested arrays, have the\nattractive capability of providing enhanced degrees of freedom. By exploiting\nthe coarray structure, an augmented sample covariance matrix can be constructed\nand MUSIC (MUtiple SIgnal Classification) can be applied to identify more\nsources than the number of sensors. While such a MUSIC algorithm works quite\nwell, its performance has not been theoretically analyzed. In this paper, we\nderive a simplified asymptotic mean square error (MSE) expression for the MUSIC\nalgorithm applied to the coarray model, which is applicable even if the source\nnumber exceeds the sensor number. We show that the directly augmented sample\ncovariance matrix and the spatial smoothed sample covariance matrix yield the\nsame asymptotic MSE for MUSIC. We also show that when there are more sources\nthan the number of sensors, the MSE converges to a positive value instead of\nzero when the signal-to-noise ratio (SNR) goes to infinity. This finding\nexplains the \"saturation\" behavior of the coarray-based MUSIC algorithms in the\nhigh SNR region observed in previous studies. Finally, we derive the\nCram\\'er-Rao bound (CRB) for sparse linear arrays, and conduct a numerical\nstudy of the statistical efficiency of the coarray-based estimator.\nExperimental results verify theoretical derivations and reveal the complex\nefficiency pattern of coarray-based MUSIC algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 21:03:54 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 22:03:54 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Wang", "Mianzhi", ""], ["Nehorai", "Arye", ""]]}, {"id": "1605.03626", "submitter": "Gordon Berman", "authors": "Gordon J. Berman, William Bialek, and Joshua W. Shaevitz", "title": "Predictability and hierarchy in Drosophila behavior", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1607601113", "report-no": null, "categories": "physics.bio-ph cs.IT math.IT q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Even the simplest of animals exhibit behavioral sequences with complex\ntemporal dynamics. Prominent amongst the proposed organizing principles for\nthese dynamics has been the idea of a hierarchy, wherein the movements an\nanimal makes can be understood as a set of nested sub-clusters. Although this\ntype of organization holds potential advantages in terms of motion control and\nneural circuitry, measurements demonstrating this for an animal's entire\nbehavioral repertoire have been limited in scope and temporal complexity. Here,\nwe use a recently developed unsupervised technique to discover and track the\noccurrence of all stereotyped behaviors performed by fruit flies moving in a\nshallow arena. Calculating the optimally predictive representation of the fly's\nfuture behaviors, we show that fly behavior exhibits multiple time scales and\nis organized into a hierarchical structure that is indicative of its underlying\nbehavioral programs and its changing internal states.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 21:47:29 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Berman", "Gordon J.", ""], ["Bialek", "William", ""], ["Shaevitz", "Joshua W.", ""]]}, {"id": "1605.03660", "submitter": "Anna Seigal", "authors": "Anna Seigal, Portia Mira, Bernd Sturmfels, Miriam Barlow", "title": "Does Antibiotic Resistance Evolve in Hospitals?", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nosocomial outbreaks of bacteria are well-documented. Based on these\nincidents, and the heavy usage of antibiotics in hospitals, it has been assumed\nthat antibiotic resistance evolves in hospital environments. To test this\nassumption, we studied resistance phenotypes of bacteria collected from patient\nisolates at a community hospital over a 2.5-year period. A graphical model\nanalysis shows no association between resistance and patient information other\nthan time of arrival. This allows us to focus on time course data.\n  We introduce a Hospital Transmission Model, based on negative binomial delay.\nOur main contribution is a statistical hypothesis test called the Nosocomial\nEvolution of Resistance Detector (NERD). It calculates the significance of\nresistance trends occurring in a hospital. It can inform hospital staff about\nthe effects of various practices and interventions, can help detect clonal\noutbreaks, and is available as an R-package.\n  We applied the NERD method to each of the 16 antibiotics in the study via 16\nhypothesis tests. For 13 of the antibiotics, we found that the hospital\nenvironment had no significant effect upon the evolution of resistance; the\nhospital is merely a piece of the larger picture. The p-values obtained for the\nother three antibiotics (Cefepime, Ceftazidime and Gentamicin) indicate that\nparticular care should be taken in hospital practices with these antibiotics.\nOne of the three, Ceftazidime, was significant after accounting for multiple\nhypotheses, indicating a trend of decreased resistance for this drug.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 02:58:39 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 17:23:55 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 21:03:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Seigal", "Anna", ""], ["Mira", "Portia", ""], ["Sturmfels", "Bernd", ""], ["Barlow", "Miriam", ""]]}, {"id": "1605.03702", "submitter": "Nor Aishah Muhammad", "authors": "Nor Aishah Muhammad, Peng Wang, Yonghui Li, Branka Vucetic", "title": "Analytical Model for Outdoor Millimeter Wave Channels using\n  Geometry-Based Stochastic Approach", "comments": "Accepted to appear in IEEE Transactions on Vehicular Technology", "journal-ref": null, "doi": "10.1109/TVT.2016.2566644", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The severe bandwidth shortage in conventional microwave bands has spurred the\nexploration of the millimeter wave (MMW) spectrum for the next revolution in\nwireless communications. However, there is still lack of proper channel\nmodeling for the MMW wireless propagation, especially in the case of outdoor\nenvironments. In this paper, we develop a geometry-based stochastic channel\nmodel to statistically characterize the effect of all the first-order\nreflection paths between the transmitter and receiver. These first-order\nreflections are generated by the single-bounce of signals reflected from the\nwalls of randomly distributed buildings. Based on this geometric model, a\nclosed-form expression for the power delay profile (PDP) contributed by all the\nfirst-order reflection paths is obtained and then used to evaluate their impact\non the MMW outdoor propagation characteristics. Numerical results are provided\nto validate the accuracy of the proposed model under various channel parameter\nsettings. The findings in this paper provide a promising step towards more\ncomplex and practical MMW propagation channel modeling.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:21:16 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Muhammad", "Nor Aishah", ""], ["Wang", "Peng", ""], ["Li", "Yonghui", ""], ["Vucetic", "Branka", ""]]}, {"id": "1605.03868", "submitter": "Bhaswar Bhattacharya", "authors": "Kwonsang Lee, Bhaswar B. Bhattacharya, Jing Qin, and Dylan S. Small", "title": "A Nonparametric Likelihood Approach for Inference in Instrumental\n  Variable Models", "comments": "Major changes. Updated BL method. New theorems and data analysis\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods allow for inference about the treatment effect\nby controlling for unmeasured confounding in randomized experiments with\nnoncompliance. However, many studies do not consider the observed compliance\nbehavior in the testing procedure, which can lead to a loss of power. In this\npaper, we propose a novel nonparametric likelihood approach, referred to as the\nbinomial likelihood (BL) method, that incorporates information on compliance\nbehavior while overcoming several limitations of previous techniques and\nutilizing the advantages of likelihood methods. Our proposed method produces\nproper estimates of the counterfactual distribution functions by maximizing the\nbinomial likelihood over the space of distribution functions. Using this we\npropose two versions of a binomial likelihood ratio test for the null\nhypothesis of no treatment effect. We show that both versions are more powerful\nto detect any distributional change than existing methods in finite sample\ncases, and are asymptotically equivalent to the two-sample Anderson-Darling\ntest. We also develop an efficient algorithm for computing our estimates, and\napply the binomial likelihood method to a study of the effect of Medicaid\ncoverage on mental health using the Oregon Health Insurance Experiment.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:55:51 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:30:18 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 00:19:26 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Lee", "Kwonsang", ""], ["Bhattacharya", "Bhaswar B.", ""], ["Qin", "Jing", ""], ["Small", "Dylan S.", ""]]}, {"id": "1605.03872", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Dylan S. Small, Jesse Y. Hsu, Jeffrey H. Silber and Paul\n  R. Rosenbaum", "title": "Discovering Effect Modification in an Observational Study of Surgical\n  Mortality at Hospitals with Superior Nursing", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is effect modification if the magnitude or stability of a treatment\neffect varies systematically with the level of an observed covariate. A larger\nor more stable treatment effect is typically less sensitive to bias from\nunmeasured covariates, so it is important to recognize effect modification when\nit is present. We illustrate a recent proposal for conducting a sensitivity\nanalysis that empirically discovers effect modification by exploratory methods,\nbut controls the family-wise error rate in discovered groups. The example\nconcerns a study of mortality and use of the intensive care unit in 23,715\nmatched pairs of two Medicare patients, one of whom underwent surgery at a\nhospital identified for superior nursing, the other at a conventional hospital.\nThe pairs were matched exactly for 130 four-digit ICD-9 surgical procedure\ncodes and balanced 172 observed covariates. The pairs were then split into five\ngroups of pairs by CART in its effort to locate effect modification. The\nevidence of a beneficial effect of magnet hospitals on mortality is least\nsensitive to unmeasured biases in a large group of patients undergoing rather\nserious surgical procedures, but in the absence of other life-threatening\nconditions, such as a comorbidity of congestive heart failure or an emergency\nadmission leading to surgery.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:06:38 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 22:32:48 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""], ["Hsu", "Jesse Y.", ""], ["Silber", "Jeffrey H.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1605.03992", "submitter": "Brian Segal", "authors": "Brian Segal, Thomas Braun, Michael Elliott, Hui Jiang", "title": "Fast Approximation of Small p-values in Permutation Tests by\n  Partitioning the Permutations", "comments": "64 pages, 34 figures, 12 tables including appendices (22 pages, 8\n  figures, 1 table not including appendices)", "journal-ref": "Biometrics. 74 (2018) 196-206", "doi": "10.1111/biom.12731", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in genetics and other life sciences commonly use permutation\ntests to evaluate differences between groups. Permutation tests have desirable\nproperties, including exactness if data are exchangeable, and are applicable\neven when the distribution of the test statistic is analytically intractable.\nHowever, permutation tests can be computationally intensive. We propose both an\nasymptotic approximation and a resampling algorithm for quickly estimating\nsmall permutation p-values (e.g. $<10^{-6}$) for the difference and ratio of\nmeans in two-sample tests. Our methods are based on the distribution of test\nstatistics within and across partitions of the permutations, which we define.\nIn this article, we present our methods and demonstrate their use through\nsimulations and an application to cancer genomic data. Through simulations, we\nfind that our resampling algorithm is more computationally efficient than\nanother leading alternative, particularly for extremely small p-values (e.g.\n$<10^{-30}$). Through application to cancer genomic data, we find that our\nmethods can successfully identify up- and down-regulated genes. While we focus\non the difference and ratio of means, we speculate that our approaches may work\nin other settings.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 21:13:03 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 22:46:32 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Segal", "Brian", ""], ["Braun", "Thomas", ""], ["Elliott", "Michael", ""], ["Jiang", "Hui", ""]]}, {"id": "1605.04574", "submitter": "Neal Master", "authors": "Neal Master, David Scheinker, Nicholas Bambos", "title": "Predicting Pediatric Surgical Durations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective management of operating room resources relies on accurate\npredictions of surgical case durations. This prediction problem is known to be\nparticularly difficult in pediatric hospitals due to the extreme variation in\npediatric patient populations. We propose a novel metric for measuring accuracy\nof predictions which captures key issues relevant to hospital operations. With\nthis metric in mind we propose several tree-based prediction models. Some are\nautomated (they do not require input from surgeons) while others are\nsemi-automated (they do require input from surgeons). We see that many of our\nautomated methods generally outperform currently used algorithms and even\nachieve the same performance as surgeons. Our semi-automated methods can\noutperform surgeons by a significant margin. We gain insights into the\npredictive value of different features and suggest avenues of future work.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 16:22:32 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 15:31:47 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Master", "Neal", ""], ["Scheinker", "David", ""], ["Bambos", "Nicholas", ""]]}, {"id": "1605.05382", "submitter": "Michelle Anzarut Ms", "authors": "Michelle Anzarut and Ramses H. Mena", "title": "A Harris process to model stochastic volatility", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tractable non-independent increment process which provides a\nhigh modeling flexibility. The process lies on an extension of the so-called\nHarris chains to continuous time being stationary and Feller. We exhibit\nconstructions, properties, and inference methods for the process. Afterwards,\nwe use the process to propose a stochastic volatility model with an arbitrary\nbut fixed invariant distribution, which can be tailored to fit different\napplied scenarios. We study the model performance through simulation while\nillustrating its use in practice with empirical work. The model proves to be an\ninteresting competitor to a number of short-range stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 22:15:25 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Anzarut", "Michelle", ""], ["Mena", "Ramses H.", ""]]}, {"id": "1605.05397", "submitter": "Geoff Boeing", "authors": "Geoff Boeing and Paul Waddell", "title": "New Insights into Rental Housing Markets across the United States: Web\n  Scraping and Analyzing Craigslist Rental Listings", "comments": "20 pages, 9 figures, Journal of Planning Education and Research.\n  2016. Online first", "journal-ref": null, "doi": "10.1177/0739456X16664789", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current sources of data on rental housing - such as the census or commercial\ndatabases that focus on large apartment complexes - do not reflect recent\nmarket activity or the full scope of the U.S. rental market. To address this\ngap, we collected, cleaned, analyzed, mapped, and visualized 11 million\nCraigslist rental housing listings. The data reveal fine-grained spatial and\ntemporal patterns within and across metropolitan housing markets in the U.S. We\nfind some metropolitan areas have only single-digit percentages of listings\nbelow fair market rent. Nontraditional sources of volunteered geographic\ninformation offer planners real-time, local-scale estimates of rent and housing\ncharacteristics currently lacking in alternative sources, such as census data.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 23:29:58 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 00:02:15 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 03:31:20 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 06:09:52 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Boeing", "Geoff", ""], ["Waddell", "Paul", ""]]}, {"id": "1605.05476", "submitter": "Sylvain Robert", "authors": "Sylvain Robert and Hans R. K\\\"unsch", "title": "Localizing the Ensemble Kalman Particle Filter", "comments": null, "journal-ref": null, "doi": "10.1080/16000870.2017.1282016", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods such as the Ensemble Kalman Filter (EnKF) are widely used\nfor data assimilation in large-scale geophysical applications, as for example\nin numerical weather prediction (NWP). There is a growing interest for physical\nmodels with higher and higher resolution, which brings new challenges for data\nassimilation techniques because of the presence of non-linear and non-Gaussian\nfeatures that are not adequately treated by the EnKF. We propose two new\nlocalized algorithms based on the Ensemble Kalman Particle Filter (EnKPF), a\nhybrid method combining the EnKF and the Particle Filter (PF) in a way that\nmaintains scalability and sample diversity. Localization is a key element of\nthe success of EnKFs in practice, but it is much more challenging to apply to\nPFs. The algorithms that we introduce in the present paper provide a compromise\nbetween the EnKF and the PF while avoiding some of the problems of localization\nfor pure PFs. Numerical experiments with a simplified model of cumulus\nconvection based on a modified shallow water equation show that the proposed\nalgorithms perform better than the local EnKF. In particular, the PF nature of\nthe method allows to capture non-Gaussian characteristics of the estimated\nfields such as the location of wet and dry areas.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 08:22:16 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 16:03:13 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Robert", "Sylvain", ""], ["K\u00fcnsch", "Hans R.", ""]]}, {"id": "1605.05588", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi", "title": "A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire\n  Systems", "comments": "It had to be noted that the assumption was made that all sensors have\n  access to all observations and state estimate vectors. In addition, the\n  summations in the DAQKF Algorithm are on all sensors, not just the\n  neighbouring sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of automated flight control and management systems have made\npossible aircraft designs that sacrifice arodynamic stability in order to\nincorporate stealth technology intro their shape, operate more efficiently, and\nare highly maneuverable. Therefore, modern flight management systems are\nreliant on multiple redundant sensors to monitor and control the rotations of\nthe aircraft. To this end, a novel distributed quaternion Kalman filtering\nalgorithm is developed for tracking the rotation and orientation of an aircraft\nin the three-dimensional space. The algorithm is developed to distribute\ncomputation among the sensors in a manner that forces them to consent to a\nunique solution while being robust to sensor and link failure, a desirable\ncharacteristic for flight management systems. In addition, the underlying\nquaternion-valued state space model allows to avoid problems associated with\ngimbal lock. The performance of the developed algorithm is verified through\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 19:48:53 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 08:06:46 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Talebi", "Sayed Pouria", ""]]}, {"id": "1605.05779", "submitter": "So Young Park", "authors": "So Young Park, Cai Li, Santa-Maria Mendoza, Eric van Heugten, and\n  Ana-Maria Staicu", "title": "Conditional analysis for mixed covariates, with application to feed\n  intake of lactating sows", "comments": "30 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel modeling framework to study the effect of covariates of\nvarious types on the conditional distribution of the response. The methodology\naccommodates flexible model structure, allows for joint estimation of the\nquantiles at all levels, and involves a computationally efficient estimation\nalgorithm. Extensive numerical investigation confirms good performance of the\nproposed method. The methodology is motivated by and applied to a lactating sow\nstudy, where the primary interest is to understand how the dynamic change of\nminute-by-minute temperature in the farrowing rooms within a day (functional\ncovariate) is associated with low quantiles of feed intake of lactating sows,\nwhile accounting for other sow-specific information (vector covariate).\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 22:31:48 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 05:55:32 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Park", "So Young", ""], ["Li", "Cai", ""], ["Mendoza", "Santa-Maria", ""], ["van Heugten", "Eric", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1605.05784", "submitter": "Avleen Bijral", "authors": "Avleen S. Bijral, Richard Johnston, Juan Lavista Ferres", "title": "Predicting Unemployment Claims Using Regional and Exogenous Signals: A\n  Sparse Modeling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply a time series based Vector Auto Regressive (VAR)\napproach to the problem of predicting unemployment insurance claims in\ndifferent census regions of the United States. Unemployment insurance claims\ndata, reported weekly, are a leading indicator of the US unemployment rate.\nGathering weekly unemployment claims and aggregating by region, we model\ncorrelation between the different census regions. Additionally, we explore the\nuse of external variables such as Bing search query volumes and URL site clicks\nrelated to unemployment claims. To prevent any spurious predictors from\nappearing in the model we use sparse model based regularization. Preliminary\nresults indicate that our approach is promising and in ongoing work we are\nextending the approach to a larger set of predictors and a longer data range.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 23:55:30 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Bijral", "Avleen S.", ""], ["Johnston", "Richard", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "1605.05808", "submitter": "Earnest Akofor", "authors": "Earnest Akofor", "title": "Optimal Inference for Distributed Detection", "comments": "Doctoral thesis (Minor typos corrected in Theorem 2.6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed detection, there does not exist an automatic way of generating\noptimal decision strategies for non-affine decision functions. Consequently, in\na detection problem based on a non-affine decision function, establishing\noptimality of a given decision strategy, such as a generalized likelihood ratio\ntest, is often difficult or even impossible.\n  In this thesis we develop a novel detection network optimization technique\nthat can be used to determine necessary and sufficient conditions for\noptimality in distributed detection for which the underlying objective function\nis monotonic and convex in probabilistic decision strategies. Our developed\napproach leverages on basic concepts of optimization and statistical inference\nwhich are provided in sufficient detail. These basic concepts are combined to\nform the basis of an optimal inference technique for signal detection.\n  We prove a central theorem that characterizes optimality in a variety of\ndistributed detection architectures. We discuss three applications of this\nresult in distributed signal detection. These applications include interactive\ndistributed detection, optimal tandem fusion architecture, and distributed\ndetection by acyclic graph networks. In the conclusion we indicate several\nfuture research directions, which include possible generalizations of our\noptimization method and new research problems arising from each of the three\napplications considered.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 04:51:59 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 13:28:21 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 22:16:29 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Akofor", "Earnest", ""]]}, {"id": "1605.05893", "submitter": "Vukasin Jovic", "authors": "Luca Di Persio, Vukasin Jovic", "title": "Jump Diffusion and {\\alpha}-Stable Techniques for the Markov Switching\n  Approach to Financial Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a detailed comparison between a Markov Switching Jump Diffusion\nModel and a Markov Switching {\\alpha}-Stable Distribution Model with respect to\nthe analysis of non-stationary data. We show that the jump diffusion model is\nextremely robust, flexible and accurate in fitting of financial time series. A\nthorough computational study involving the two models being applied to real\ndata, namely, the S&P500 index, is provided. The study shows that the\njump-diffusion model solves the over-smoothing issue stated in (Di Persio and\nFrigo, 2016), while the {\\alpha}-stable distribution approach is a good\ncompromise between computational effort and performance in the estimate of\nimplied volatility, which is a major problem widely underlined in the dedicated\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 11:31:31 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Di Persio", "Luca", ""], ["Jovic", "Vukasin", ""]]}, {"id": "1605.05910", "submitter": "Swati Chandna", "authors": "Swati Chandna, Andrew T. Walden", "title": "A Frequency Domain Test for Propriety of Complex-Valued Vector Time\n  Series", "comments": "13 pages, 3 figures. Methodology (stat.ME), Applications (stat.AP)", "journal-ref": null, "doi": "10.1109/TSP.2016.2639459", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a frequency domain approach to test the hypothesis that a\ncomplex-valued vector time series is proper, i.e., for testing whether the\nvector time series is uncorrelated with its complex conjugate. If the\nhypothesis is rejected, frequency bands causing the rejection will be\nidentified and might usefully be related to known properties of the physical\nprocesses. The test needs the associated spectral matrix which can be estimated\nby multitaper methods using, say, $K$ tapers. Standard asymptotic distributions\nfor the test statistic are of no use since they would require $K \\rightarrow\n\\infty,$ but, as $K$ increases so does resolution bandwidth which causes\nspectral blurring. In many analyses $K$ is necessarily kept small, and hence\nour efforts are directed at practical and accurate methodology for hypothesis\ntesting for small $K.$ Our generalized likelihood ratio statistic combined with\nexact cumulant matching gives very accurate rejection percentages and\noutperforms other methods. We also prove that the statistic on which the test\nis based is comprised of canonical coherencies arising from our complex-valued\nvector time series.Our methodology is demonstrated on ocean current data\ncollected at different depths in the Labrador Sea. Overall this work extends\nresults on propriety testing for complex-valued vectors to the complex-valued\nvector time series setting.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:12:33 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Chandna", "Swati", ""], ["Walden", "Andrew T.", ""]]}, {"id": "1605.06193", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Ori Davidov, Shyamal D. Peddada", "title": "Analysis of High Dimensional Compositional Data Containing Structural\n  Zeros with Applications to Microbiome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the recent interest in the analysis of high dimen-\nsional microbiome data. A key feature of this data is the presence of\n`structural zeros' which are microbes missing from an observation vector due to\nan underlying biological process and not due to error in measurement. Typical\nnotions of missingness are insufficient to model these structural zeros. We\ndefine a general framework which allows for structural zeros in the model and\npropose methods of estimating sparse high dimensional covariance and precision\nmatrices under this setup. We establish error bounds in the spectral and\nfrobenius norms for the proposed esti- mators and empirically support them with\na simulation study. We also apply the proposed methodology to the global human\ngut microbiome data of Yatsunenko (2012).\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 01:39:41 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Kaul", "Abhishek", ""], ["Davidov", "Ori", ""], ["Peddada", "Shyamal D.", ""]]}, {"id": "1605.06459", "submitter": "Paul Slater", "authors": "Paul B. Slater", "title": "Two-Qubit Separability Probabilities as Joint Functions of the Bloch\n  Radii of the Qubit Subsystems", "comments": "24 pages, 17 figures--revised--to appear in International Journal of\n  Quantum Information", "journal-ref": "International Journal of Quantum Information Volume No. 14, Issue\n  No. 08, December, 2016", "doi": "10.1142/S0219749916500428", "report-no": null, "categories": "quant-ph math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detect a certain pattern of behavior of separability probabilities\n$p(r_A,r_B)$ for two-qubit systems endowed with Hilbert-Schmidt, and more\ngenerally, random induced measures, where $r_A$ and $r_B$ are the Bloch radii\n($0 \\leq r_A,r_B \\leq 1$) of the qubit reduced states ($A,B$). We observe a\nrelative repulsion of radii effect, that is $p(r_A,r_A) < p(r_A,1-r_A)$, except\nfor rather narrow \"crossover\" intervals $[\\tilde{r}_A,\\frac{1}{2}]$. Among the\nseven specific cases we study are, firstly, the \"toy\" seven-dimensional\n$X$-states model and, then, the fifteen-dimensional two-qubit states obtained\nby tracing over the pure states in $4 \\times K$-dimensions, for $K=3, 4, 5$,\nwith $K=4$ corresponding to Hilbert-Schmidt (flat/Euclidean) measure. We also\nexamine the real (two-rebit) $K=4$, the $X$-states $K=5$, and Bures (minimal\nmonotone)--for which no nontrivial crossover behavior is observed--instances.\nIn the two $X$-states cases, we derive analytical results, for $K=3, 4$, we\npropose formulas that well-fit our numerical results, and for the other\nscenarios, rely presently upon large numerical analyses. The separability\nprobability crossover regions found expand in length (lower $\\tilde{r}_A$) as\n$K$ increases. This report continues our efforts (arXiv:1506.08739) to extend\nthe recent work of Milz and Strunz (J. Phys. A: 48 [2015] 035306) from a\nunivariate ($r_A$) framework---in which they found separability probabilities\nto hold constant with $r_A$---to a bivariate ($r_A,r_B$) one. We also analyze\nthe two-qutrit and qubit-qutrit counterparts reported in arXiv:1512.07210 in\nthis context, and study two-qubit separability probabilities of the form\n$p(r_A,\\frac{1}{2})$.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 18:04:19 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 19:12:12 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 15:30:55 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Slater", "Paul B.", ""]]}, {"id": "1605.06482", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn, Asahi Ushio, Teruo Nakatsuma", "title": "Volatility Forecasts Using Nonlinear Leverage Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leverage effect-- the correlation between an asset's return and its\nvolatility-- has played a key role in forecasting and understanding volatility\nand risk. While it is a long standing consensus that leverage effects exist and\nimprove forecasts, empirical evidence paradoxically do not show that most\nindividual stocks exhibit this phenomena, mischaracterizing risk and therefore\nleading to poor predictive performance. We examine this paradox, with the goal\nto improve density forecasts, by relaxing the assumption of linearity in the\nleverage effect. Nonlinear generalizations of the leverage effect are proposed\nwithin the Bayesian stochastic volatility framework in order to capture\nflexible leverage structures, where small fluctuations in prices have a\ndifferent effect from large shocks. Efficient Bayesian sequential computation\nis developed and implemented to estimate this effect in a practical, on-line\nmanner. Examining 615 stocks that comprise the S\\&P500 and Nikkei 225, we find\nthat relaxing the linear assumption to our proposed nonlinear leverage effect\nfunction improves predictive performances for 89\\% of all stocks compared to\nthe conventional model assumption.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 19:35:35 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 07:42:49 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 08:34:43 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 04:47:54 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Ushio", "Asahi", ""], ["Nakatsuma", "Teruo", ""]]}, {"id": "1605.06779", "submitter": "Yafeng Cheng", "authors": "Yafeng Cheng, Jian Qing Shi, Janet Eyre", "title": "Nonlinear Mixed-effects Scalar-on-function Models and Variable Selection\n  for Kinematic Upper Limb Movement Data", "comments": "31 pages, 9 figures. Submitted to Annuals of applied statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper arises from collaborative research the aim of which was to model\nclinical assessments of upper limb function after stroke using 3D kinematic\ndata. We present a new nonlinear mixed-effects scalar-on-function regression\nmodel with a Gaussian process prior focusing on variable selection from large\nnumber of candidates including both scalar and function variables. A novel\nvariable selection algorithm has been developed, namely functional least angle\nregression (fLARS). As they are essential for this algorithm, we studied the\nrepresentation of functional variables with different methods and the\ncorrelation between a scalar and a group of mixed scalar and functional\nvariables. We also propose two new stopping rules for practical usage.\n  This algorithm is able to do variable selection when the number of variables\nis larger than the sample size. It is efficient and accurate for both variable\nselection and parameter estimation. Our comprehensive simulation study showed\nthat the method is superior to other existing variable selection methods. When\nthe algorithm was applied to the analysis of the 3D kinetic movement data the\nuse of the non linear random-effects model and the function variables\nsignificantly improved the prediction accuracy for the clinical assessment.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 12:23:47 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Cheng", "Yafeng", ""], ["Shi", "Jian Qing", ""], ["Eyre", "Janet", ""]]}, {"id": "1605.06898", "submitter": "Ian Barnett", "authors": "Ian Barnett, Tarun Khanna, and Jukka-Pekka Onnela", "title": "Social and Spatial Clustering of People at Humanity's Largest Gathering", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0156794", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macroscopic behavior of scientific and societal systems results from the\naggregation of microscopic behaviors of their constituent elements, but\nconnecting the macroscopic with the microscopic in human behavior has\ntraditionally been difficult. Manifestations of homophily, the notion that\nindividuals tend to interact with others who resemble them, have been observed\nin many small and intermediate size settings. However, whether this behavior\ntranslates to truly macroscopic levels, and what its consequences may be,\nremains unknown. Here, we use call detail records (CDRs) to examine the\npopulation dynamics and manifestations of social and spatial homophily at a\nmacroscopic level among the residents of 23 states of India at the Kumbh Mela,\na 3-month-long Hindu festival. We estimate that the festival was attended by 61\nmillion people, making it the largest gathering in the history of humanity.\nWhile we find strong overall evidence for both types of homophily for residents\nof different states, participants from low-representation states show\nconsiderably stronger propensity for both social and spatial homophily than\nthose from high-representation states. These manifestations of homophily are\namplified on crowded days, such as the peak day of the festival, which we\nestimate was attended by 25 million people. Our findings confirm that\nhomophily, which here likely arises from social influence, permeates all scales\nof human behavior.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 05:19:09 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Barnett", "Ian", ""], ["Khanna", "Tarun", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1605.07072", "submitter": "Christian Mueller", "authors": "Christian L. M\\\"uller, Richard Bonneau, Zachary Kurtz", "title": "Generalized Stability Approach for Regularized Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting regularization parameters in penalized high-dimensional graphical\nmodels in a principled, data-driven, and computationally efficient manner\ncontinues to be one of the key challenges in high-dimensional statistics. We\npresent substantial computational gains and conceptual generalizations of the\nStability Approach to Regularization Selection (StARS), a state-of-the-art\ngraphical model selection scheme. Using properties of the Poisson-Binomial\ndistribution and convex non-asymptotic distributional modeling we propose lower\nand upper bounds on the StARS graph regularization path which results in\ngreatly reduced computational cost without compromising regularization\nselection. We also generalize the StARS criterion from single edge to induced\nsubgraph (graphlet) stability. We show that simultaneously requiring edge and\ngraphlet stability leads to superior graph recovery performance independent of\ngraph topology. These novel insights render Gaussian graphical model selection\na routine task on standard multi-core computers.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:08:46 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["M\u00fcller", "Christian L.", ""], ["Bonneau", "Richard", ""], ["Kurtz", "Zachary", ""]]}, {"id": "1605.07242", "submitter": "Natesh Pillai", "authors": "Joseph J. Lee, Laura Forastiere, Luke Miratrix, Natesh S. Pillai", "title": "More Powerful Multiple Testing in Randomized Experiments with\n  Non-Compliance", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two common concerns raised in analyses of randomized experiments are (i)\nappropriately handling issues of non-compliance, and (ii) appropriately\nadjusting for multiple tests (e.g., on multiple outcomes or subgroups).\nAlthough simple intention-to-treat (ITT) and Bonferroni methods are valid in\nterms of type I error, they can each lead to a substantial loss of power; when\nemploying both simultaneously, the total loss may be severe. Alternatives exist\nto address each concern. Here we propose an analysis method for experiments\ninvolving both features that merges posterior predictive $p$-values for\ncomplier causal effects with randomization-based multiple comparisons\nadjustments; the results are valid familywise tests that are doubly\nadvantageous: more powerful than both those based on standard ITT statistics\nand those using traditional multiple comparison adjustments. The operating\ncharacteristics and advantages of our method are demonstrated through a series\nof simulated experiments and an analysis of the United States Job Training\nPartnership Act (JTPA) Study, where our methods lead to different conclusions\nregarding the significance of estimated JTPA effects.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 00:22:15 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lee", "Joseph J.", ""], ["Forastiere", "Laura", ""], ["Miratrix", "Luke", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1605.07264", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson", "title": "Trajectory probability hypothesis density filter", "comments": "Published in the Proceedings of the 21st International Conference on\n  Information Fusion (FUSION)", "journal-ref": null, "doi": "10.23919/ICIF.2018.8455270", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the probability hypothesis density (PHD) filter for sets\nof trajectories: the trajectory probability density (TPHD) filter. The TPHD\nfilter is capable of estimating trajectories in a principled way without\nrequiring to evaluate all measurement-to-target association hypotheses. The\nTPHD filter is based on recursively obtaining the best Poisson approximation to\nthe multitrajectory filtering density in the sense of minimising the\nKullback-Leibler divergence. We also propose a Gaussian mixture implementation\nof the TPHD recursion. Finally, we include simulation results to show the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:19:11 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 14:05:43 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1605.07266", "submitter": "Pengcheng Zhou", "authors": "Pengcheng Zhou, Shanna L. Resendez, Jose Rodriguez-Romaguera, Jessica\n  C. Jimenez, Shay Q. Neufeld, Garret D. Stuber, Rene Hen, Mazen A. Kheirbek,\n  Bernardo L. Sabatini, Robert E. Kass, Liam Paninski", "title": "Efficient and accurate extraction of in vivo calcium signals from\n  microendoscopic video data", "comments": "The image has been compressed for meeting arXiv requirements. A pdf\n  version with higher resolution image can be downloaded here\n  https://zhoupc.github.io/data/zhou2016.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vivo calcium imaging through microscopes has enabled deep brain imaging of\npreviously inaccessible neuronal populations within the brains of freely moving\nsubjects. However, microendoscopic data suffer from high levels of background\nfluorescence as well as an increased potential for overlapping neuronal\nsignals. Previous methods fail in identifying neurons and demixing their\ntemporal activity because the cellular signals are often submerged in the large\nfluctuating background. Here we develop an efficient method to extract cellular\nsignals with minimal influence from the background. We model the background\nwith two realistic components: (1) one models the constant baseline and slow\ntrends of each pixel, and (2) the other models the fast fluctuations from\nout-of-focus signals and is therefore constrained to have low spatial-frequency\nstructure. This decomposition avoids cellular signals being absorbed into the\nbackground term. After subtracting the background approximated with this model,\nwe use Constrained Nonnegative Matrix Factorization (CNMF, Pnevmatikakis et al.\n(2016)) to better demix neural signals and get their denoised and deconvolved\ntemporal activity. We validate our method on simulated and experimental data,\nwhere it shows fast, reliable, and high quality signal extraction under a wide\nvariety of imaging parameters.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:35:09 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 20:22:54 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Zhou", "Pengcheng", ""], ["Resendez", "Shanna L.", ""], ["Rodriguez-Romaguera", "Jose", ""], ["Jimenez", "Jessica C.", ""], ["Neufeld", "Shay Q.", ""], ["Stuber", "Garret D.", ""], ["Hen", "Rene", ""], ["Kheirbek", "Mazen A.", ""], ["Sabatini", "Bernardo L.", ""], ["Kass", "Robert E.", ""], ["Paninski", "Liam", ""]]}, {"id": "1605.07284", "submitter": "Jiangfan Zhang", "authors": "Jiangfan Zhang, Rick S. Blum, Lance Kaplan, and Xuanxuan Lu", "title": "Functional Forms of Optimum Spoofing Attacks for Vector Parameter\n  Estimation in Quantized Sensor Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2626258", "report-no": null, "categories": "cs.IT cs.CR math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of an unknown deterministic vector from quantized sensor data is\nconsidered in the presence of spoofing attacks which alter the data presented\nto several sensors. Contrary to previous work, a generalized attack model is\nemployed which manipulates the data using transformations with arbitrary\nfunctional forms determined by some attack parameters whose values are unknown\nto the attacked system. For the first time, necessary and sufficient conditions\nare provided under which the transformations provide a guaranteed attack\nperformance in terms of Cramer-Rao Bound (CRB) regardless of the processing the\nestimation system employs, thus defining a highly desirable attack.\nInterestingly, these conditions imply that, for any such attack when the\nattacked sensors can be perfectly identified by the estimation system, either\nthe Fisher Information Matrix (FIM) for jointly estimating the desired and\nattack parameters is singular or that the attacked system is unable to improve\nthe CRB for the desired vector parameter through this joint estimation even\nthough the joint FIM is nonsingular. It is shown that it is always possible to\nconstruct such a highly desirable attack by properly employing a sufficiently\nlarge dimension attack vector parameter relative to the number of quantization\nlevels employed, which was not observed previously. To illustrate the theory in\na concrete way, we also provide some numerical results which corroborate that\nunder the highly desirable attack, attacked data is not useful in reducing the\nCRB.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 04:35:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:47:38 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Zhang", "Jiangfan", ""], ["Blum", "Rick S.", ""], ["Kaplan", "Lance", ""], ["Lu", "Xuanxuan", ""]]}, {"id": "1605.07439", "submitter": "Marko Laine", "authors": "Virpi Junttila and Marko Laine", "title": "Bayesian Principal Component Regression model with spatial effects for\n  forest inventory under small field sample size", "comments": null, "journal-ref": null, "doi": "10.1016/j.rse.2017.01.035", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing observations are extensively used for analysis of\nenvironmental variables. These variables often exhibit spatial correlation,\nwhich has to be accounted for in the calibration models used in predictions,\neither by direct modelling of the dependencies or by allowing for spatially\ncorrelated stochastic effects. Another feature in many remote sensing\ninstruments is that the derived predictor variables are highly correlated,\nwhich can lead to unnecessary model over-training and at worst, singularities\nin the estimates. Both of these affect the prediction accuracy, especially when\nthe training set for model calibration is small. To overcome these modelling\nchallenges, we present a general model calibration procedure for remotely\nsensed data and apply it to airborne laser scanning data for forest inventory.\nWe use a linear regression model that accounts for multicollinearity in the\npredictors by principal components and Bayesian regularization. It has a\nspatial random effect component for the spatial correlations that are not\nexplained by a simple linear model. An efficient Markov chain Monte Carlo\nsampling scheme is used to account for the uncertainty in all the model\nparameters. We tested the proposed model against several alternatives and it\noutperformed the other linear calibration models, especially when there were\nspatial effects, multicollinearity and the training set size was small.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 13:18:06 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 06:21:07 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 15:05:24 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Junttila", "Virpi", ""], ["Laine", "Marko", ""]]}, {"id": "1605.07511", "submitter": "Mijung Park", "authors": "Mijung Park, Max Welling", "title": "A note on privacy preserving iteratively reweighted least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iteratively reweighted least squares (IRLS) is a widely-used method in\nmachine learning to estimate the parameters in the generalised linear models.\nIn particular, IRLS for L1 minimisation under the linear model provides a\nclosed-form solution in each step, which is a simple multiplication between the\ninverse of the weighted second moment matrix and the weighted first moment\nvector. When dealing with privacy sensitive data, however, developing a privacy\npreserving IRLS algorithm faces two challenges. First, due to the inversion of\nthe second moment matrix, the usual sensitivity analysis in differential\nprivacy incorporating a single datapoint perturbation gets complicated and\noften requires unrealistic assumptions. Second, due to its iterative nature, a\nsignificant cumulative privacy loss occurs. However, adding a high level of\nnoise to compensate for the privacy loss hinders from getting accurate\nestimates. Here, we develop a practical algorithm that overcomes these\nchallenges and outputs privatised and accurate IRLS solutions. In our method,\nwe analyse the sensitivity of each moments separately and treat the matrix\ninversion and multiplication as a post-processing step, which simplifies the\nsensitivity analysis. Furthermore, we apply the {\\it{concentrated differential\nprivacy}} formalism, a more relaxed version of differential privacy, which\nrequires adding a significantly less amount of noise for the same level of\nprivacy guarantee, compared to the conventional and advanced compositions of\ndifferentially private mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 15:50:26 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Park", "Mijung", ""], ["Welling", "Max", ""]]}, {"id": "1605.07663", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee and Dylan S. Small", "title": "Estimating the Malaria Attributable Fever Fraction Accounting for\n  Parasites Being Killed by Fever and Measurement Error", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a parasitic disease that is a major health problem in many\ntropical regions. The most characteristic symptom of malaria is fever. The\nfraction of fevers that are attributable to malaria, the malaria attributable\nfever fraction (MAFF), is an important public health measure for assessing the\neffect of malaria control programs and other purposes. Estimating the MAFF is\nnot straightforward because there is no gold standard diagnosis of a malaria\nattributable fever; an individual can have malaria parasites in her blood and a\nfever, but the individual may have developed partial immunity that allows her\nto tolerate the parasites and the fever is being caused by another infection.\nWe define the MAFF using the potential outcome framework for causal inference\nand show what assumptions underlie current estimation methods. Current\nestimation methods rely on an assumption that the parasite density is correctly\nmeasured. However, this assumption does not generally hold because (i) fever\nkills some parasites and (ii) the measurement of parasite density has\nmeasurement error. In the presence of these problems, we show current\nestimation methods do not perform well. We propose a novel maximum likelihood\nestimation method based on exponential family g-modeling. Under the assumption\nthat the measurement error mechanism and the magnitude of the fever killing\neffect are known, we show that our proposed method provides approximately\nunbiased estimates of the MAFF in simulation studies. A sensitivity analysis\ncan be used to assess the impact of different magnitudes of fever killing and\ndifferent measurement error mechanisms. We apply our proposed method to\nestimate the MAFF in Kilombero, Tanzania.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 21:12:53 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""]]}, {"id": "1605.07854", "submitter": "Lukas Martig", "authors": "Lukas Martig, J\\\"urg H\\\"usler", "title": "Asymptotic normality of the likelihood moment estimators for a\n  stationary linear process with heavy-tailed innovations", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of estimators for the parameters of the Generalized Pareto\ndistribution, the approximating distribution for excesses over a high\nthreshold, have been proposed, always assuming the underlying data to be\nindependent. We recently proved that the likelihood moment estimators are\nconsistent estimators for the parameters of the Generalized Pareto distribution\nfor the case where the underlying data arises from a (stationary) linear\nprocess with heavy-tailed innovations. In this paper we derive the bivariate\nasymptotic normality under some additional assumptions and give an explicit\nexample on how to check these conditions by using asymptotic expansions.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:31:31 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Martig", "Lukas", ""], ["H\u00fcsler", "J\u00fcrg", ""]]}, {"id": "1605.07924", "submitter": "Theodore  Kypraios", "authors": "Jessica E. Stockdale, Theodore Kypraios, Philip D. O'Neill", "title": "Modelling and Bayesian analysis of the Abakaliki Smallpox Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Abakaliki smallpox data have appeared numerous times in the\nepidemic modelling literature, but in almost all cases only a specific subset\nof the data is considered. There is one previous analysis of the full data set,\nbut this relies on approximation methods to derive a likelihood. The data\nthemselves continue to be of interest due to concerns about the possible\nre-emergence of smallpox as a bioterrorism weapon. We present the first full\nBayesian analysis using data-augmentation Markov chain Monte Carlo methods\nwhich avoid the need for likelihood approximations. Results include estimates\nof basic model parameters as well as reproduction numbers and the likely path\nof infection. Model assessment is carried out using simulation-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 15:11:25 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Stockdale", "Jessica E.", ""], ["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1605.08078", "submitter": "Simon Tindemans", "authors": "James R. Schofield, Simon H. Tindemans, Goran Strbac", "title": "A baseline-free method to identify responsive customers on dynamic\n  time-of-use tariffs", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time-of-use tariffs incentivise changes in electricity consumption.\nThis paper presents a non-parametric method to retrospectively analyse\nconsumption data and quantify the significance of a customer's observed\nresponse to a dynamic price signal without constructing a baseline demand\nmodel. If data from a control group is available, this can be used to infer\ncustomer responsiveness - individually and collectively - on an absolute scale.\nThe results are illustrated using data from the Low Carbon London project,\nwhich included the UK's first dynamic time-of-use pricing trial.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 21:15:30 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Schofield", "James R.", ""], ["Tindemans", "Simon H.", ""], ["Strbac", "Goran", ""]]}, {"id": "1605.08369", "submitter": "Matthew Shum", "authors": "Nicholas Buchholz, Haiqing Xu, Matthew Shum", "title": "Semiparametric Estimation of Dynamic Discrete-Choice Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of dynamic discrete choice models in a\nsemiparametric setting, in which the per-period utility functions are known up\nto a finite number of parameters, but the distribution of utility shocks is\nleft unspecified. This semiparametric setup differs from most of the existing\nidentification and estimation literature for dynamic discrete choice mod- els.\nTo show identification we derive and exploit a new Bellman-like recursive\nrepresentation for the unknown quantile function of the utility shocks. Our\nestimators are straightforward to compute; some are simple and require no\niteration, and resemble classic estimators from the literature on\nsemiparametric regression and average derivative estimation. Monte Carlo\nsimulations demonstrate that our estimator performs well in small samples. To\nhighlight features of this estimator, we estimate a structural model of dynamic\nlabor supply for New York City taxicab drivers.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:19:56 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Buchholz", "Nicholas", ""], ["Xu", "Haiqing", ""], ["Shum", "Matthew", ""]]}, {"id": "1605.08499", "submitter": "James Miller", "authors": "K. Miller, P. Huggins, A. Dubrawski, S. Labov, K. Nelson", "title": "Evaluation of Coded Aperture Radiation Detectors using a Bayesian\n  Approach", "comments": "Submission abstract for poster presented at nuclear science symposium\n  (NSS) 2015", "journal-ref": null, "doi": "10.1016/j.nima.2016.09.027", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the utility of coded aperture (CA) for roadside radiation\nthreat detection applications. With coded aperture, information in the form of\nphoton quantity is traded for directional information. Whether and in what\nscenarios this trade-off is beneficial is the focus of this study. We quantify\nthe impact of a masking approach by comparing performance with an unmasked\napproach in terms of both detection and localization of a roadside nuclear\nthreat. We measure performance over many instances of a drive-by scenario via\nMonte Carlo simulation based on empirical observations.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 03:16:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Miller", "K.", ""], ["Huggins", "P.", ""], ["Dubrawski", "A.", ""], ["Labov", "S.", ""], ["Nelson", "K.", ""]]}, {"id": "1605.08740", "submitter": "Elizabeth Lee", "authors": "Elizabeth C. Lee, Jason M. Asher, Sandra Goldlust, John D. Kraemer,\n  Andrew B. Lawson, and Shweta Bansal", "title": "Mind the scales: Harnessing spatial big data for infectious disease\n  surveillance and inference", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial big data have the \"velocity,\" \"volume,\" and \"variety\" of big data\nsources and additional geographic information about the record. Digital data\nsources, such as medical claims, mobile phone call data records, and geo-tagged\ntweets, have entered infectious disease epidemiology as novel sources of data\nto complement traditional infectious disease surveillance. In this work, we\nprovide examples of how spatial big data have been used thus far in\nepidemiological analyses and describe opportunities for these sources to\nimprove public health coordination and disease mitigation strategies. In\naddition, we consider the technical, practical, and ethical challenges with the\nuse of spatial big data in infectious disease surveillance and inference.\nFinally, we discuss the implications of the rising use of spatial big data in\nepidemiology to health risk communications, across-scale public health\ncoordination, and public health policy recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:17:20 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 02:39:04 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 20:31:56 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Lee", "Elizabeth C.", ""], ["Asher", "Jason M.", ""], ["Goldlust", "Sandra", ""], ["Kraemer", "John D.", ""], ["Lawson", "Andrew B.", ""], ["Bansal", "Shweta", ""]]}, {"id": "1605.08753", "submitter": "Gaurav Sood", "authors": "Gaurav Sood and Derek Willis", "title": "Fairly Random: The Impact of Winning the Toss on the Probability of\n  Winning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a competitive sport, every little thing matters. Yet, many sports leave\nsome large levers out of the reach of the teams, and in the hands of fate. In\ncricket, world's second most popular sport by some measures, one such\nlever---the toss---has been subject to much recent attention. Using a large\nnovel dataset of 44,224 cricket matches, we estimate the impact of winning the\ntoss on the probability of winning. The data suggest that winning the toss\nincreases the chance of winning by a small ($\\sim$ 2.8\\%) but significant\nmargin. The advantage varies heftily and systematically, by how closely matched\nthe competing teams are, and by playing conditions---tautologically, winning\nthe toss in conditions where the toss grants a greater advantage, for e.g., in\nday and night matches, has a larger impact on the probability of winning.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:46:13 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Sood", "Gaurav", ""], ["Willis", "Derek", ""]]}, {"id": "1605.08759", "submitter": "Jonathan Azose", "authors": "Jonathan J. Azose and Adrian E. Raftery", "title": "Estimating Large Correlation Matrices for International Migration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations is the major organization producing and regularly updating\nprobabilistic population projections for all countries. International migration\nis a critical component of such projections, and between-country correlations\nare important for forecasts of regional aggregates. However, there are 200\ncountries and only 12 data points, each one corresponding to a five-year time\nperiod. Thus a $200 \\times 200$ correlation matrix must be estimated on the\nbasis of 12 data points. Using Pearson correlations produces many spurious\ncorrelations. We propose a maximum a posteriori estimator for the correlation\nmatrix with an interpretable informative prior distribution. The prior serves\nto regularize the correlation matrix, shrinking a priori untrustworthy elements\ntowards zero. Our estimated correlation structure improves projections of net\nmigration for regional aggregates, producing narrower projections of migration\nfor Africa as a whole and wider projections for Europe. A simulation study\nconfirms that our estimator outperforms both the Pearson correlation matrix and\na simple shrinkage estimator when estimating a sparse correlation matrix.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 19:18:21 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Azose", "Jonathan J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1605.08860", "submitter": "Xueou Wang", "authors": "Xueou Wang, David J. Nott, C.C. Drovandi, Kerrie Mengersen, and\n  Michael Evans", "title": "Using history matching for prior choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It can be important in Bayesian analyses of complex models to construct\ninformative prior distributions which reflect knowledge external to the data at\nhand. Nevertheless, how much prior information an analyst can elicit from an\nexpert will be limited due to constraints of time, cost and other factors. This\npaper develops effective numerical methods for exploring reasonable choices of\na prior distribution from a parametric class, when prior information is\nspecified in the form of some limited constraints on prior predictive\ndistributions, and where these prior predictive distributions are analytically\nintractable. The methods developed may be thought of as a novel application of\nthe ideas of history matching, a technique developed in the literature on\nassessment of computer models. We illustrate the approach in the context of\nlogistic regression and sparse signal shrinkage prior distributions for\nhigh-dimensional linear models.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 08:12:26 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 08:38:04 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 05:37:48 GMT"}, {"version": "v4", "created": "Thu, 9 Nov 2017 06:22:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Wang", "Xueou", ""], ["Nott", "David J.", ""], ["Drovandi", "C. C.", ""], ["Mengersen", "Kerrie", ""], ["Evans", "Michael", ""]]}, {"id": "1605.09107", "submitter": "Arthur Guillaumin", "authors": "Arthur P. Guillaumin and Adam M. Sykulski and Sofia C. Olhede and\n  Jeffrey J. Early and Jonathan M. Lilly", "title": "Analysis of nonstationary modulated time series with applications to\n  oceanographic flow measurements", "comments": "31 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ao-ph physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of univariate nonstationary time series models, using\nthe framework of modulated time series, which is appropriate for the analysis\nof rapidly-evolving time series as well as time series observations with\nmissing data. We extend our techniques to a class of bivariate time series that\nare isotropic. Exact inference is often not computationally viable for time\nseries analysis, and so we propose an estimation method based on the\nWhittle-likelihood, a commonly adopted pseudo-likelihood. Our inference\nprocedure is shown to be consistent under standard assumptions, as well as\nhaving considerably lower computational cost than exact likelihood in general.\nWe show the utility of this framework for the analysis of drifting instruments,\nan analysis that is key to characterising global ocean circulation and\ntherefore also for decadal to century-scale climate understanding.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 05:12:02 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 11:40:31 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Guillaumin", "Arthur P.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Early", "Jeffrey J.", ""], ["Lilly", "Jonathan M.", ""]]}, {"id": "1605.09171", "submitter": "Guillaume Basse", "authors": "Guillaume W. Basse, Hossein Azari Soufiani and Diane Lambert", "title": "Randomization and The Pernicious Effects of Limited Budgets on Auction\n  Experiments", "comments": "Appeared in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Buyers (e.g., advertisers) often have limited financial and processing\nresources, and so their participation in auctions is throttled. Changes to\nauctions may affect bids or throttling and any change may affect what winners\npay. This paper shows that if an A/B experiment affects only bids, then the\nobserved treatment effect is unbiased when all the bidders in an auction are\nrandomly assigned to A or B but it can be severely biased otherwise, even in\nthe absence of throttling. Experiments that affect throttling algorithms can\nalso be badly biased, but the bias can be substantially reduced if the budget\nfor each advertiser in the experiment is allocated to separate pots for the A\nand B arms of the experiment.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 10:52:19 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Soufiani", "Hossein Azari", ""], ["Lambert", "Diane", ""]]}, {"id": "1605.09484", "submitter": "Man Chung Fung", "authors": "Man Chung Fung, Gareth W. Peters, Pavel V. Shevchenko", "title": "A unified approach to mortality modelling using state-space framework:\n  characterisation, identification, estimation and forecasting", "comments": "46 pages", "journal-ref": "Annals of Actuarial Science 11 (2), pp. 343-389, 2017", "doi": "10.1017/S1748499517000069", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores and develops alternative statistical representations and\nestimation approaches for dynamic mortality models. The framework we adopt is\nto reinterpret popular mortality models such as the Lee-Carter class of models\nin a general state-space modelling methodology, which allows modelling,\nestimation and forecasting of mortality under a unified framework. Furthermore,\nwe propose an alternative class of model identification constraints which is\nmore suited to statistical inference in filtering and parameter estimation\nsettings based on maximization of the marginalized likelihood or in Bayesian\ninference. We then develop a novel class of Bayesian state-space models which\nincorporate apriori beliefs about the mortality model characteristics as well\nas for more flexible and appropriate assumptions relating to heteroscedasticity\nthat present in observed mortality data. We show that multiple period and\ncohort effect can be cast under a state-space structure. To study long term\nmortality dynamics, we introduce stochastic volatility to the period effect.\nThe estimation of the resulting stochastic volatility model of mortality is\nperformed using a recent class of Monte Carlo procedure specifically designed\nfor state and parameter estimation in Bayesian state-space models, known as the\nclass of particle Markov chain Monte Carlo methods. We illustrate the framework\nwe have developed using Danish male mortality data, and show that incorporating\nheteroscedasticity and stochastic volatility markedly improves model fit\ndespite an increase of model complexity. Forecasting properties of the enhanced\nmodels are examined with long term and short term calibration periods on the\nreconstruction of life tables.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 03:51:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Fung", "Man Chung", ""], ["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1605.09503", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, Mark Thomas, Holger Teismann, Sujay Mukhoti", "title": "Inverse problem for time-series valued computer model via scalarization", "comments": "23 pages (submitted to OJS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an expensive to evaluate computer simulator, even the estimate of the\noverall surface can be a challenging problem. In this paper, we focus on the\nestimation of the inverse solution, i.e., to find the set(s) of input\ncombinations of the simulator that generates (or gives good approximation of) a\npre-determined simulator output. Ranjan et al. (2008) proposed an expected\nimprovement criterion under a sequential design framework for the inverse\nproblem with a scalar valued simulator. In this paper, we focus on the inverse\nproblem for a time-series valued simulator. We have used a few simulated and\ntwo real examples for performance comparison.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 06:00:24 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 06:08:23 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ranjan", "Pritam", ""], ["Thomas", "Mark", ""], ["Teismann", "Holger", ""], ["Mukhoti", "Sujay", ""]]}, {"id": "1605.09511", "submitter": "Paul Smaldino", "authors": "Paul E. Smaldino and Richard McElreath", "title": "The Natural Selection of Bad Science", "comments": "41 pages, 7 figures", "journal-ref": "Royal Society Open Science, 3: 160384 (2016)", "doi": "10.1098/rsos.160384", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor research design and data analysis encourage false-positive findings.\nSuch poor methods persist despite perennial calls for improvement, suggesting\nthat they result from something more than just misunderstanding. The\npersistence of poor methods results partly from incentives that favor them,\nleading to the natural selection of bad science. This dynamic requires no\nconscious strategizing---no deliberate cheating nor loafing---by scientists,\nonly that publication is a principle factor for career advancement. Some\nnormative methods of analysis have almost certainly been selected to further\npublication instead of discovery. In order to improve the culture of science, a\nshift must be made away from correcting misunderstandings and towards rewarding\nunderstanding. We support this argument with empirical evidence and\ncomputational modeling. We first present a 60-year meta-analysis of statistical\npower in the behavioral sciences and show that power has not improved despite\nrepeated demonstrations of the necessity of increasing power. To demonstrate\nthe logical consequences of structural incentives, we then present a dynamic\nmodel of scientific communities in which competing laboratories investigate\nnovel or previously published hypotheses using culturally transmitted research\nmethods. As in the real world, successful labs produce more \"progeny\", such\nthat their methods are more often copied and their students are more likely to\nstart labs of their own. Selection for high output leads to poorer methods and\nincreasingly high false discovery rates. We additionally show that replication\nslows but does not stop the process of methodological deterioration. Improving\nthe quality of research requires change at the institutional level.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 07:25:37 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Smaldino", "Paul E.", ""], ["McElreath", "Richard", ""]]}, {"id": "1605.09535", "submitter": "Peter Klimek", "authors": "Peter Klimek, Silke Aichberger, Stefan Thurner", "title": "Disentangling genetic and environmental risk factors for individual\n  diseases from multiplex comorbidity networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most disorders are caused by a combination of multiple genetic and/or\nenvironmental factors. If two diseases are caused by the same molecular\nmechanism, they tend to co-occur in patients. Here we provide a quantitative\nmethod to disentangle how much genetic or environmental risk factors contribute\nto the pathogenesis of 358 individual diseases, respectively. We pool data on\ngenetic, pathway-based, and toxicogenomic disease-causing mechanisms with\ndisease co-occurrence data obtained from almost two million patients. From this\ndata we construct a multilayer network where nodes represent disorders that are\nconnected by links that either represent phenotypic comorbidity of the patients\nor the involvement of a certain molecular mechanism. From the similarity of\nphenotypic and mechanism-based networks for each disorder we derive measure\nthat allows us to quantify the relative importance of various molecular\nmechanisms for a given disease. We find that most diseases are dominated by\ngenetic risk factors, while environmental influences prevail for disorders such\nas depressions, cancers, or dermatitis. Almost never we find that more than one\ntype of mechanisms is involved in the pathogenesis of diseases.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 09:05:07 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Klimek", "Peter", ""], ["Aichberger", "Silke", ""], ["Thurner", "Stefan", ""]]}, {"id": "1605.09652", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti, Indrani Mukherjee and Monojit Das", "title": "Some estimators of the PMF and CDF of the Logarithmic Series\n  Distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.06308", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the different methods of estimation of the probability\nmass function (PMF) and the cumulative distribution function (CDF) for the\nLogarithmic Series distribution. Following estimation methods are considered:\nuniformly minimum variance unbiased estimator (UMVUE), maximum likelihood\nestimator (MLE), percentile estimator (PCE), least square estimator (LSE),\nweighted least square estimator (WLSE). Monte Carlo simulations are performed\nto compare the performances of the proposed methods of estimation.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:00:31 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Mukherjee", "Indrani", ""], ["Das", "Monojit", ""]]}]