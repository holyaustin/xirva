[{"id": "0811.0182", "submitter": "William Shaw", "authors": "William T. Shaw", "title": "A model of returns for the post-credit-crunch reality: Hybrid Brownian\n  motion with price feedback", "comments": "3 figures; Aug 30: Further corrections and citations; Aug 09 update:\n  links to integrals of exponential Brownian motion; more exact solutions;\n  relations to other work. previous update: Simple VaR, more exact solutions,\n  relation to Legendre equation. In Update: Corrections; better history; more\n  exact solutions; discussion of market states", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The market events of 2007-2009 have reinvigorated the search for realistic\nreturn models that capture greater likelihoods of extreme movements. In this\npaper we model the medium-term log-return dynamics in a market with both\nfundamental and technical traders. This is based on a Poisson trade arrival\nmodel with variable size orders. With simplifications we are led to a hybrid\nSDE mixing both arithmetic and geometric Brownian motions, whose solution is\ngiven by a class of integrals of exponentials of one Brownian motion against\nanother, in forms considered by Yor and collaborators. The reduction of the\nhybrid SDE to a single Brownian motion leads to an SDE of the form considered\nby Nagahara, which is a type of \"Pearson diffusion\", or equivalently a\nhyperbolic OU SDE. Various dynamics and equilibria are possible depending on\nthe balance of trades. Under mean-reverting circumstances we arrive naturally\nat an equilibrium fat-tailed return distribution with a Student or Pearson Type\nIV form. Under less restrictive assumptions richer dynamics are possible,\nincluding bimodal structures. The phenomenon of variance explosion is\nidentified that gives rise to much larger price movements that might have a\npriori been expected, so that \"$25\\sigma$\" events are significantly more\nprobable. We exhibit simple example solutions of the Fokker-Planck equation\nthat shows how such variance explosion can hide beneath a standard Gaussian\nfacade. These are elementary members of an extended class of distributions with\na rich and varied structure, capable of describing a wide range of market\nbehaviours. Several approaches to the density function are possible, and an\nexample of the computation of a hyperbolic VaR is given. The model also\nsuggests generalizations of the Bougerol identity.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 18:01:10 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2008 11:23:12 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2009 12:53:39 GMT"}, {"version": "v4", "created": "Tue, 18 Aug 2009 12:51:00 GMT"}, {"version": "v5", "created": "Sun, 30 Aug 2009 09:03:43 GMT"}], "update_date": "2009-08-31", "authors_parsed": [["Shaw", "William T.", ""]]}, {"id": "0811.0237", "submitter": "Jean-Baptiste Rouquier", "authors": "Pablo Jensen (LET, Phys-ENS, IXXI), Jean-Baptiste Rouquier (IXXI,\n  LIP), Yves Croissant (LET)", "title": "Testing bibliometric indicators by their prediction of scientists\n  promotions", "comments": null, "journal-ref": "Scientometrics3:78,2008", "doi": "10.1007/s11192-007-2014-3", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a method to obtain robust quantitative bibliometric\nindicators for several thousand scientists. This allows us to study the\ndependence of bibliometric indicators (such as number of publications, number\nof citations, Hirsch index...) on the age, position, etc. of CNRS scientists.\nOur data suggests that the normalized h index (h divided by the career length)\nis not constant for scientists with the same productivity but differents ages.\nWe also compare the predictions of several bibliometric indicators on the\npromotions of about 600 CNRS researchers. Contrary to previous publications,\nour study encompasses most disciplines, and shows that no single indicator is\nthe best predictor for all disciplines. Overall, however, the Hirsch index h\nprovides the least bad correlations, followed by the number of papers\npublished. It is important to realize however that even h is able to recover\nonly half of the actual promotions. The number of citations or the mean number\nof citations per paper are definitely not good predictors of promotion.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2008 08:11:32 GMT"}], "update_date": "2008-11-26", "authors_parsed": [["Jensen", "Pablo", "", "LET, Phys-ENS, IXXI"], ["Rouquier", "Jean-Baptiste", "", "IXXI,\n  LIP"], ["Croissant", "Yves", "", "LET"]]}, {"id": "0811.0717", "submitter": "Patricia Gautier", "authors": "Eric San Juan (INIST), Ivana Roche (INIST)", "title": "Visualization of association graphs for assisting the interpretation of\n  classifications", "comments": "International workshop on Webometrics, informetrics and\n  scientometrics. Seventh collnet meeting, Nancy : France (2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query on the PASCAL database maintained by the INIST, we design user\ninterfaces to visualize and browse two types of graphs extracted from\nabstracts: 1) the graph of all associations between authors (co-author graph),\n2) the graph of strong associations between authors and terms automatically\nextracted from abstracts and grouped using linguistic variations. We adapt for\nthis purpose the TermWatch system that comprises a term extractor, a relation\nidentifier which yields the terminological network and a clustering module. The\nresults are output on two interfaces: a graphic one mapping the clusters in a\n2D space and a terminological hypertext network allowing the user to\ninteractively explore results and return to source texts.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 12:58:38 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Juan", "Eric San", "", "INIST"], ["Roche", "Ivana", "", "INIST"]]}, {"id": "0811.0719", "submitter": "Patricia Gautier", "authors": "Xavier Polanco (INIST), Ivana Roche (INIST), Dominique Besagni (INIST)", "title": "Web Usage Analysis: New Science Indicators and Co-usage", "comments": null, "journal-ref": "S\\'eminaire VSST 2006, Lille : France (2006)", "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of statistical analysis of the science and technical information\n(STI) in the Web context is produced. We propose a set of indicators about Web\nusers, visualized bibliographic records, and e-commercial transactions. In\naddition, we introduce two Web usage factors. Finally, we give an overview of\nthe co-usage analysis. For these tasks, we introduce a computer based system,\ncalled Miri@d, which produces descriptive statistical information about the Web\nusers' searching behaviour, and what is effectively used from a free access\ndigital bibliographical database. The system is conceived as a server of\nstatistical data which are carried out beforehand, and as an interactive server\nfor online statistical work. The results will be made available to analysts,\nwho can use this descriptive statistical information as raw data for their\nindicator design tasks, and as input for multivariate data analysis, clustering\nanalysis, and mapping. Managers also can exploit the results in order to\nimprove management and decision-making.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 13:00:52 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Polanco", "Xavier", "", "INIST"], ["Roche", "Ivana", "", "INIST"], ["Besagni", "Dominique", "", "INIST"]]}, {"id": "0811.1477", "submitter": "Susan Holmes", "authors": "Persi Diaconis, Sharad Goel, Susan Holmes", "title": "Horseshoes in multidimensional scaling and local kernel methods", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS165 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 777-807", "doi": "10.1214/08-AOAS165", "report-no": "IMS-AOAS-AOAS165", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multidimensional scaling (MDS) is a method for visualizing\nhigh-dimensional point clouds by mapping to low-dimensional Euclidean space.\nThis mapping is defined in terms of eigenfunctions of a matrix of interpoint\ndissimilarities. In this paper we analyze in detail multidimensional scaling\napplied to a specific dataset: the 2005 United States House of Representatives\nroll call votes. Certain MDS and kernel projections output ``horseshoes'' that\nare characteristic of dimensionality reduction techniques. We show that, in\ngeneral, a latent ordering of the data gives rise to these patterns when one\nonly has local information. That is, when only the interpoint distances for\nnearby points are known accurately. Our results provide a rigorous set of\nresults and insight into manifold learning in the special case where the\nmanifold is a curve.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2008 14:23:23 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Diaconis", "Persi", ""], ["Goel", "Sharad", ""], ["Holmes", "Susan", ""]]}, {"id": "0811.1561", "submitter": "Michel Fliess", "authors": "Michel Fliess (LIX, INRIA Saclay - Ile de France), C\\'edric Join\n  (INRIA Saclay - Ile de France, CRAN)", "title": "Time Series Technical Analysis via New Fast Estimation Methods: A\n  Preliminary Study in Mathematical Finance", "comments": null, "journal-ref": "IAR-ACD08 (23rd IAR Workshop on Advanced Control and Diagnosis)\n  (2008)", "doi": null, "report-no": null, "categories": "stat.AP math.AC math.OC math.ST q-fin.CP q-fin.TR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New fast estimation methods stemming from control theory lead to a fresh look\nat time series, which bears some resemblance to \"technical analysis\". The\nresults are applied to a typical object of financial engineering, namely the\nforecast of foreign exchange rates, via a \"model-free\" setting, i.e., via\nrepeated identifications of low order linear difference equations on sliding\nshort time windows. Several convincing computer simulations, including the\nprediction of the position and of the volatility with respect to the forecasted\ntrendline, are provided. $\\mathcal{Z}$-transform and differential algebra are\nthe main mathematical tools.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2008 19:58:38 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2008 10:38:59 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Fliess", "Michel", "", "LIX, INRIA Saclay - Ile de France"], ["Join", "C\u00e9dric", "", "INRIA Saclay - Ile de France, CRAN"]]}, {"id": "0811.1606", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina, Fred L. Mannering, Andrew P. Tarko", "title": "Markov switching negative binomial models: an application to vehicle\n  accident frequencies", "comments": "This is a preprint, the final paper is available on Accident Analysis\n  and Prevention website (the preprint has 21 pages, 3 tables, 2 figures,\n  accepted for publication in Accident Analysis and Prevention)", "journal-ref": "Accident Analysis and Prevention, 2009, Volume 41, Issue 2, pages\n  217-226", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two-state Markov switching models are proposed to study\naccident frequencies. These models assume that there are two unobserved states\nof roadway safety, and that roadway entities (roadway segments) can switch\nbetween these states over time. The states are distinct, in the sense that in\nthe different states accident frequencies are generated by separate counting\nprocesses (by separate Poisson or negative binomial processes). To demonstrate\nthe applicability of the approach presented herein, two-state Markov switching\nnegative binomial models are estimated using five-year accident frequencies on\nIndiana interstate highway segments. Bayesian inference methods and Markov\nChain Monte Carlo (MCMC) simulations are used for model estimation. The\nestimated Markov switching models result in a superior statistical fit relative\nto the standard (single-state) negative binomial model. It is found that the\nmore frequent state is safer and it is correlated with better weather\nconditions. The less frequent state is found to be less safe and to be\ncorrelated with adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 00:28:55 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2008 20:09:36 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""], ["Tarko", "Andrew P.", ""]]}, {"id": "0811.1640", "submitter": "Donald B. Rubin", "authors": "Donald B. Rubin", "title": "For objective causal inference, design trumps analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS187 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 808-840", "doi": "10.1214/08-AOAS187", "report-no": "IMS-AOAS-AOAS187", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For obtaining causal inferences that are objective, and therefore have the\nbest chance of revealing scientific truths, carefully designed and executed\nrandomized experiments are generally considered to be the gold standard.\nObservational studies, in contrast, are generally fraught with problems that\ncompromise any claim for objectivity of the resulting causal inferences. The\nthesis here is that observational studies have to be carefully designed to\napproximate randomized experiments, in particular, without examining any final\noutcome data. Often a candidate data set will have to be rejected as inadequate\nbecause of lack of data on key covariates, or because of lack of overlap in the\ndistributions of key covariates between treatment and control groups, often\nrevealed by careful propensity score analyses. Sometimes the template for the\napproximating randomized experiment will have to be altered, and the use of\nprincipal stratification can be helpful in doing this. These issues are\ndiscussed and illustrated using the framework of potential outcomes to define\ncausal effects, which greatly clarifies critical issues.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 07:31:41 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Rubin", "Donald B.", ""]]}, {"id": "0811.1645", "submitter": "Hemant Ishwaran", "authors": "Hemant Ishwaran, Udaya B. Kogalur, Eugene H. Blackstone, Michael S.\n  Lauer", "title": "Random survival forests", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS169 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 841-860", "doi": "10.1214/08-AOAS169", "report-no": "IMS-AOAS-AOAS169", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce random survival forests, a random forests method for the\nanalysis of right-censored survival data. New survival splitting rules for\ngrowing survival trees are introduced, as is a new missing data algorithm for\nimputing missing data. A conservation-of-events principle for survival forests\nis introduced and used to define ensemble mortality, a simple interpretable\nmeasure of mortality that can be used as a predicted outcome. Several\nillustrative examples are given, including a case study of the prognostic\nimplications of body mass for individuals with coronary artery disease.\nComputations for all examples were implemented using the freely available\nR-software package, randomSurvivalForest.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 08:05:28 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Ishwaran", "Hemant", ""], ["Kogalur", "Udaya B.", ""], ["Blackstone", "Eugene H.", ""], ["Lauer", "Michael S.", ""]]}, {"id": "0811.1663", "submitter": "Louis Lyons", "authors": "Louis Lyons", "title": "Open statistical issues in particle physics", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS163 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 887-915", "doi": "10.1214/08-AOAS163", "report-no": "IMS-AOAS-AOAS163", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical issues arise in the analysis of Particle Physics\nexperiments. We give a brief introduction to Particle Physics, before\ndescribing the techniques used by Particle Physicists for dealing with\nstatistical problems, and also some of the open statistical questions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 09:56:54 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Lyons", "Louis", ""]]}, {"id": "0811.1679", "submitter": "Jerome H. Friedman", "authors": "Jerome H. Friedman, Bogdan E. Popescu", "title": "Predictive learning via rule ensembles", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS148 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 916-954", "doi": "10.1214/07-AOAS148", "report-no": "IMS-AOAS-AOAS148", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General regression and classification models are constructed as linear\ncombinations of simple rules derived from the data. Each rule consists of a\nconjunction of a small number of simple statements concerning the values of\nindividual input variables. These rule ensembles are shown to produce\npredictive accuracy comparable to the best methods. However, their principal\nadvantage lies in interpretation. Because of its simple form, each rule is easy\nto understand, as is its influence on individual predictions, selected subsets\nof predictions, or globally over the entire space of joint input variable\nvalues. Similarly, the degree of relevance of the respective input variables\ncan be assessed globally, locally in different regions of the input space, or\nat individual prediction points. Techniques are presented for automatically\nidentifying those variables that are involved in interactions with other\nvariables, the strength and degree of those interactions, as well as the\nidentities of the other variables with which they interact. Graphical\nrepresentations are used to visualize both main and interaction effects.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 10:43:07 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Friedman", "Jerome H.", ""], ["Popescu", "Bogdan E.", ""]]}, {"id": "0811.1686", "submitter": "L. Fraser Jackson", "authors": "L. Fraser Jackson, Alistair G. Gray, Stephen E. Fienberg", "title": "Sequential category aggregation and partitioning approaches for\n  multi-way contingency tables based on survey and census data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS175 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 955-981", "doi": "10.1214/08-AOAS175", "report-no": "IMS-AOAS-AOAS175", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large contingency tables arise in many contexts but especially in the\ncollection of survey and census data by government statistical agencies.\nBecause the vast majority of the variables in this context have a large number\nof categories, agencies and users need a systematic way of constructing tables\nwhich are summaries of such contingency tables. We propose such an approach in\nthis paper by finding members of a class of restricted log-linear models which\nmaximize the likelihood of the data and use this to find a parsimonious means\nof representing the table. In contrast with more standard approaches for model\nsearch in hierarchical log-linear models (HLLM), our procedure systematically\nreduces the number of categories of the variables. Through a series of\nexamples, we illustrate the extent to which it can preserve the interaction\nstructure found with HLLMs and be used as a data simplification procedure prior\nto HLL modeling. A feature of the procedure is that it can easily be applied to\nmany tables with millions of cells, providing a new way of summarizing large\ndata sets in many disciplines. The focus is on information and description\nrather than statistical testing. The procedure may treat each variable in the\ntable in different ways, preserving full detail, treating it as fully nominal,\nor preserving ordinality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 11:15:42 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Jackson", "L. Fraser", ""], ["Gray", "Alistair G.", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "0811.1697", "submitter": "Philip B. Stark", "authors": "Philip B. Stark", "title": "A Sharper discrepancy measure for post-election audits", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS171 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 982-985", "doi": "10.1214/08-AOAS171", "report-no": "IMS-AOAS-AOAS171", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-election audits use the discrepancy between machine counts and a hand\ntally of votes in a random sample of precincts to infer whether error affected\nthe electoral outcome. The maximum relative overstatement of pairwise margins\n(MRO) quantifies that discrepancy. The electoral outcome a full hand tally\nshows must agree with the apparent outcome if the MRO is less than 1. This\ncondition is sharper than previous ones when there are more than two candidates\nor when voters may vote for more than one candidate. For the 2006 U.S. Senate\nrace in Minnesota, a test using MRO gives a $P$-value of 4.05% for the\nhypothesis that a full hand tally would find a different winner, less than half\nthe value Stark [Ann. Appl. Statist. 2 (2008) 550--581] finds.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 12:22:39 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Stark", "Philip B.", ""]]}, {"id": "0811.1700", "submitter": "Daniela M. Witten", "authors": "Daniela M. Witten, Robert Tibshirani", "title": "Testing significance of features by lassoed principal components", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS182 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 986-1012", "doi": "10.1214/08-AOAS182", "report-no": "IMS-AOAS-AOAS182", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing the significance of features in\nhigh-dimensional settings. In particular, we test for differentially-expressed\ngenes in a microarray experiment. We wish to identify genes that are associated\nwith some type of outcome, such as survival time or cancer type. We propose a\nnew procedure, called Lassoed Principal Components (LPC), that builds upon\nexisting methods and can provide a sizable improvement. For instance, in the\ncase of two-class data, a standard (albeit simple) approach might be to compute\na two-sample $t$-statistic for each gene. The LPC method involves projecting\nthese conventional gene scores onto the eigenvectors of the gene expression\ndata covariance matrix and then applying an $L_1$ penalty in order to de-noise\nthe resulting projections. We present a theoretical framework under which LPC\nis the logical choice for identifying significant genes, and we show that LPC\ncan provide a marked reduction in false discovery rates over the conventional\nmethods on both real and simulated data. Moreover, this flexible procedure can\nbe applied to a variety of types of data and can be used to improve many\nexisting methods for the identification of significant features.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 12:50:26 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Witten", "Daniela M.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0811.1705", "submitter": "Mary C. Meyer", "authors": "Mary C. Meyer", "title": "Inference using shape-restricted regression splines", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS167 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 1013-1033", "doi": "10.1214/08-AOAS167", "report-no": "IMS-AOAS-AOAS167", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression splines are smooth, flexible, and parsimonious nonparametric\nfunction estimators. They are known to be sensitive to knot number and\nplacement, but if assumptions such as monotonicity or convexity may be imposed\non the regression function, the shape-restricted regression splines are robust\nto knot choices. Monotone regression splines were introduced by Ramsay\n[Statist. Sci. 3 (1998) 425--461], but were limited to quadratic and lower\norder. In this paper an algorithm for the cubic monotone case is proposed, and\nthe method is extended to convex constraints and variants such as\nincreasing-concave. The restricted versions have smaller squared error loss\nthan the unrestricted splines, although they have the same convergence rates.\nThe relatively small degrees of freedom of the model and the insensitivity of\nthe fits to the knot choices allow for practical inference methods; the\ncomputational efficiency allows for back-fitting of additive models. Tests of\nconstant versus increasing and linear versus convex regression function, when\nimplemented with shape-restricted regression splines, have higher power than\nthe standard version using ordinary shape-restricted regression.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 13:31:10 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Meyer", "Mary C.", ""]]}, {"id": "0811.1831", "submitter": "Beth Ann Griffin", "authors": "Beth Ann Griffin, Daniel F. McCaffrey, Andrew R. Morral", "title": "An application of principal stratification to control for\n  institutionalization at follow-up in studies of substance abuse treatment\n  programs", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS179 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 1034-1055", "doi": "10.1214/08-AOAS179", "report-no": "IMS-AOAS-AOAS179", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participants in longitudinal studies on the effects of drug treatment and\ncriminal justice system interventions are at high risk for institutionalization\n(e.g., spending time in an environment where their freedom to use drugs, commit\ncrimes, or engage in risky behavior may be circumscribed). Methods used for\nestimating treatment effects in the presence of institutionalization during\nfollow-up can be highly sensitive to assumptions that are unlikely to be met in\napplications and thus likely to yield misleading inferences. In this paper we\nconsider the use of principal stratification to control for\ninstitutionalization at follow-up. Principal stratification has been suggested\nfor similar problems where outcomes are unobservable for samples of study\nparticipants because of dropout, death or other forms of censoring. The method\nidentifies principal strata within which causal effects are well defined and\npotentially estimable. We extend the method of principal stratification to\nmodel institutionalization at follow-up and estimate the effect of residential\nsubstance abuse treatment versus outpatient services in a large scale study of\nadolescent substance abuse treatment programs. Additionally, we discuss\npractical issues in applying the principal stratification model to data. We\nshow via simulation studies that the model can only recover true effects\nprovided the data meet strenuous demands and that there must be caution taken\nwhen implementing principal stratification as a technique to control for\npost-treatment confounders such as institutionalization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2008 07:11:20 GMT"}], "update_date": "2008-11-13", "authors_parsed": [["Griffin", "Beth Ann", ""], ["McCaffrey", "Daniel F.", ""], ["Morral", "Andrew R.", ""]]}, {"id": "0811.1842", "submitter": "Steven D. Mark", "authors": "Steven D. Mark", "title": "A General formulation for standardization of rates as a method to\n  control confounding by measured and unmeasured disease risk factors", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS170 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 3, 1103-1122", "doi": "10.1214/08-AOAS170", "report-no": "IMS-AOAS-AOAS170", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardization, a common approach for controlling confounding in\npopulation-studies or data from disease registries, is defined to be a weighted\naverage of stratum specific rates. Typically, discussions on the construction\nof a particular standardized rate regard the strata as fixed, and focus on the\nconsiderations that affect the specification of weights. Each year the data\nfrom the SEER cancer registries are analyzed using a weighting procedure\nreferred to as ``direct standardization for age.'' To evaluate the performance\nof direct standardization, we define a general class of standardization\noperators. We regard a particular standardized rate to be the output of an\noperator and a given data set. Based on the functional form of the operators,\nwe define a subclass of standardization operators that controls for confounding\nby measured risk factors. Using the fundamental disease probability paradigm\nfor inference, we establish the conclusions that can be drawn from year-to-year\ncontrasts of standardized rates produced by these operators in the presence of\nunmeasured cancer risk factors. These conclusions take the form of falsifying\nspecific assumptions about the conditional probabilities of disease given all\nthe risk factors (both measured and unmeasured), and the conditional\nprobabilities of the unmeasured risk factors given the measured risk factors.\nWe show the one-to-one correspondence between these falsifications and the\ninferences made from the contrasts of directly standardized rates reported each\nyear in the Annual Report to the Nation on the Status of Cancer.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2008 09:05:03 GMT"}], "update_date": "2008-11-13", "authors_parsed": [["Mark", "Steven D.", ""]]}, {"id": "0811.2691", "submitter": "Johannes Siven", "authors": "Johannes Vitalis Siven, Jeffrey Todd Lins, Anna Szymkowiak-Have", "title": "Value-at-Risk Computation by Fourier Inversion with Explicit Error\n  Bounds", "comments": null, "journal-ref": "Finance Research Letters, Elsevier, vol. 6(2), pages 95-105, June\n  2009", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The value-at-risk of a delta-gamma approximated derivatives portfolio can be\ncomputed by numerical integration of the characteristic function. However,\nwhile the choice of parameters in any numerical integration scheme is\nparamount, in practice it often relies on ad hoc procedures of trial and error.\nFor normal and multivariate $t$-distributed risk factors, we show how to\ncalculate the necessary parameters for one particular integration scheme as a\nfunction of the data (the distribution of risk factors, and delta and gamma)\n\\emph{in order to satisfy a given error tolerance}. This allows for\nimplementation in a fully automated risk management system. We also demonstrate\nin simulations that the method is significantly faster than the Monte Carlo\nmethod, for a given error tolerance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2008 12:54:43 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Siven", "Johannes Vitalis", ""], ["Lins", "Jeffrey Todd", ""], ["Szymkowiak-Have", "Anna", ""]]}, {"id": "0811.3122", "submitter": "Johannes Siven", "authors": "Johannes Vitalis Siven, Jeffrey Todd Lins, Jonas Lundbek Hansen", "title": "A multiscale view on inverse statistics and gain/loss asymmetry in\n  financial time series", "comments": null, "journal-ref": "J. Stat. Mech. (2009) P02004", "doi": "10.1088/1742-5468/2009/02/P02004", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have studied the first passage time of financial time series and\nobserved that the smallest time interval needed for a stock index to move a\ngiven distance is typically shorter for negative than for positive price\nmovements. The same is not observed for the index constituents, the individual\nstocks. We use the discrete wavelet transform to illustrate that this is a long\nrather than short time scale phenomenon -- if enough low frequency content of\nthe price process is removed, the asymmetry disappears. We also propose a new\nmodel, which explain the asymmetry by prolonged, correlated down movements of\nindividual stocks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 14:13:47 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Siven", "Johannes Vitalis", ""], ["Lins", "Jeffrey Todd", ""], ["Hansen", "Jonas Lundbek", ""]]}, {"id": "0811.3639", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina and Fred L. Mannering", "title": "Zero-state Markov switching count-data models: an empirical assessment", "comments": "19 pages, 2 figures, 2 tables, the final version can be found in\n  Accident Analysis and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a two-state Markov switching count-data model is proposed as\nan alternative to zero-inflated models to account for the preponderance of\nzeros sometimes observed in transportation count data, such as the number of\naccidents occurring on a roadway segment over some period of time. For this\naccident-frequency case, zero-inflated models assume the existence of two\nstates: one of the states is a zero-accident count state, in which accident\nprobabilities are so low that they cannot be statistically distinguished from\nzero, and the other state is a normal count state, in which counts can be\nnon-negative integers that are generated by some counting process, for example,\na Poisson or negative binomial. In contrast to zero-inflated models, Markov\nswitching models allow specific roadway segments to switch between the two\nstates over time. An important advantage of this Markov switching approach is\nthat it allows for the direct statistical estimation of the specific\nroadway-segment state (i.e., zero or count state) whereas traditional\nzero-inflated models do not. To demonstrate the applicability of this approach,\na two-state Markov switching negative binomial model (estimated with Bayesian\ninference) and standard zero-inflated negative binomial models are estimated\nusing five-year accident frequencies on Indiana interstate highway segments. It\nis shown that the Markov switching model is a viable alternative and results in\na superior statistical fit relative to the zero-inflated models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 21:42:23 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2009 17:28:59 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0811.3644", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina, Fred L. Mannering", "title": "Markov switching multinomial logit model: an application to accident\n  injury severities", "comments": "24 pages, 1 figure, 3 tables", "journal-ref": "Accident Analysis and Prevention, 2009, volume 41, issue 4, pages\n  829-838", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, two-state Markov switching multinomial logit models are\nproposed for statistical modeling of accident injury severities. These models\nassume Markov switching in time between two unobserved states of roadway\nsafety. The states are distinct, in the sense that in different states accident\nseverity outcomes are generated by separate multinomial logit processes. To\ndemonstrate the applicability of the approach presented herein, two-state\nMarkov switching multinomial logit models are estimated for severity outcomes\nof accidents occurring on Indiana roads over a four-year time interval.\nBayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are\nused for model estimation. The estimated Markov switching models result in a\nsuperior statistical fit relative to the standard (single-state) multinomial\nlogit models. It is found that the more frequent state of roadway safety is\ncorrelated with better weather conditions. The less frequent state is found to\nbe correlated with adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 22:05:53 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0811.4447", "submitter": "Nancy Zhang", "authors": "Hock Peng Chan, Nancy R. Zhang, and Louis H.Y. Chen", "title": "Importance Sampling of Word Patterns in DNA and Protein Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods can provide accurate p-value estimates of word counting\ntest statistics and are easy to implement. They are especially attractive when\nan asymptotic theory is absent or when either the search sequence or the word\npattern is too short for the application of asymptotic formulae. Naive direct\nMonte Carlo is undesirable for the estimation of small probabilities because\nthe associated rare events of interest are seldom generated. We propose instead\nefficient importance sampling algorithms that use controlled insertion of the\ndesired word patterns on randomly generated sequences. The implementation is\nillustrated on word patterns of biological interest: Palindromes and inverted\nrepeats, patterns arising from position specific weight matrices and\nco-occurrences of pairs of motifs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 22:36:19 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Chan", "Hock Peng", ""], ["Zhang", "Nancy R.", ""], ["Chen", "Louis H. Y.", ""]]}]