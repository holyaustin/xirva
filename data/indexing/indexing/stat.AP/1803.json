[{"id": "1803.00006", "submitter": "Peter Cameron", "authors": "R. A. Bailey and Peter J. Cameron", "title": "Multi-part balanced incomplete-block designs", "comments": null, "journal-ref": "Statistical Papers 60 (2019), 55-76", "doi": "10.1007/s00362-018-01071-x", "report-no": null, "categories": "stat.AP math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider designs for cancer trials which allow each medical centre to\ntreat only a limited number of cancer types with only a limited number of\ndrugs. We specify desirable properties of these designs, and prove some\nconsequences. Then we give several different constructions. Finally we\ngeneralise this to three or more types of object, such as biomarkers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:33:09 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:49:16 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bailey", "R. A.", ""], ["Cameron", "Peter J.", ""]]}, {"id": "1803.00053", "submitter": "Saptarshi Das", "authors": "Valentina Bono, Saptarshi Das, Wasifa Jamal, Koushik Maharatna", "title": "Hybrid Wavelet and EMD/ICA Approach for Artifact Suppression in\n  Pervasive EEG", "comments": "45 pages, 47 figures", "journal-ref": "Journal of Neuroscience Methods (Elsevier), Volume 267, 15 July\n  2016, Pages 89-107", "doi": "10.1016/j.jneumeth.2016.04.006", "report-no": null, "categories": "physics.med-ph cs.CE physics.bio-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalogram (EEG) signals are often corrupted with unintended\nartifacts which need to be removed for extracting meaningful clinical\ninformation from them. Typically a priori knowledge of the nature of the\nartifacts is needed for such purpose. Artifact contamination of EEG is even\nmore prominent for pervasive EEG systems where the subjects are free to move\nand thereby introducing a wide variety of motion-related artifacts. This makes\nhard to get a priori knowledge about their characteristics rendering\nconventional artifact removal techniques often ineffective. In this paper, we\nexplore the performance of two hybrid artifact removal algorithms: Wavelet\npacket transform followed by Independent Component Analysis (WPTICA) and\nWavelet Packet Transform followed by Empirical Mode Decomposition (WPTEMD) in\npervasive EEG recording scenario, assuming existence of no a priori knowledge\nabout the artifacts and compare their performance with two existing artifact\nremoval algorithms. Artifact cleaning performance has been measured using Root\nMean Square Error (RMSE) and Artifact to Signal Ratio (ASR) - an index similar\nto traditional Signal to Noise Ratio (SNR), and also by observing normalized\npower distribution topography over the scalp. Comparison has been made first\nusing semi-simulated signals and then with real experimentally acquired EEG\ndata with commercially available 19-channel pervasive EEG system Enobio\ncorrupted by eight types of artifact. Our explorations show that WPTEMD\nconsistently gives best artifact cleaning performance not only in\nsemi-simulated scenario but also in the case of real EEG data containing\nartifacts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 13:33:35 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bono", "Valentina", ""], ["Das", "Saptarshi", ""], ["Jamal", "Wasifa", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1803.00082", "submitter": "Anastasia Brovkin", "authors": "Lea Waller, Anastasia Brovkin, Lena Dorfschmidt, Danilo Bzdok, Henrik\n  Walter, Johann Daniel Kruschwitz", "title": "GraphVar 2.0: A user-friendly toolbox for machine learning on functional\n  connectivity measures", "comments": "MATLAB, GUI, Toolbox, 11 pages, first two authors contributed\n  equally, , uncorrected proof Journal of Neuroscience Methods, Accepted 1 July\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We previously presented GraphVar as a user-friendly MATLAB\ntoolbox for comprehensive graph analyses of functional brain connectivity. Here\nwe introduce a comprehensive extension of the toolbox allowing users to\nseamlessly explore easily customizable decoding models across functional\nconnectivity measures as well as additional features.\n  New Method: GraphVar 2.0 provides machine learning (ML) model construction,\nvalidation and exploration. Machine learning can be performed across any\ncombination of network measures and additional variables, allowing for a\nflexibility in neuroimaging applications.\n  Results: In addition to previously integrated functionalities, such as\nnetwork construction and graph-theoretical analyses of brain connectivity with\na high-speed general linear model (GLM), users can now perform customizable ML\nacross connectivity matrices, network metrics and additionally imported\nvariables. The new extension also provides parametric and nonparametric testing\nof classifier and regressor performance, data export, figure generation and\nhigh quality export.\n  Comparison with existing methods: Compared to other existing toolboxes,\nGraphVar 2.0 offers (1) comprehensive customization, (2) an all-in-one user\nfriendly interface, (3) customizable model design and manual hyperparameter\nentry, (4) interactive results exploration and data export, (5) automated\ncueing for modelling multiple outcome variables within the same session, (6) an\neasy to follow introductory review.\n  Conclusions: GraphVar 2.0 allows comprehensive, user-friendly exploration of\nencoding (GLM) and decoding (ML) modelling approaches on functional\nconnectivity measures making big data neuroscience readily accessible to a\nbroader audience of neuroimaging investigators.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:58:51 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 09:36:43 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Waller", "Lea", ""], ["Brovkin", "Anastasia", ""], ["Dorfschmidt", "Lena", ""], ["Bzdok", "Danilo", ""], ["Walter", "Henrik", ""], ["Kruschwitz", "Johann Daniel", ""]]}, {"id": "1803.00113", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier, Andrew C. Miller, David Schlegel, Ryan P. Adams, Jon\n  D. McAuliffe, and Prabhat", "title": "Approximate Inference for Constructing Astronomical Catalogs from Images", "comments": "accepted to the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, fully generative model for constructing astronomical\ncatalogs from optical telescope image sets. Each pixel intensity is treated as\na random variable with parameters that depend on the latent properties of stars\nand galaxies. These latent properties are themselves modeled as random. We\ncompare two procedures for posterior inference. One procedure is based on\nMarkov chain Monte Carlo (MCMC) while the other is based on variational\ninference (VI). The MCMC procedure excels at quantifying uncertainty, while the\nVI procedure is 1000 times faster. On a supercomputer, the VI procedure\nefficiently uses 665,000 CPU cores to construct an astronomical catalog from 50\nterabytes of images in 14.6 minutes, demonstrating the scaling characteristics\nnecessary to construct catalogs for upcoming astronomical surveys.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 22:15:48 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 19:35:56 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 03:23:29 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Regier", "Jeffrey", ""], ["Miller", "Andrew C.", ""], ["Schlegel", "David", ""], ["Adams", "Ryan P.", ""], ["McAuliffe", "Jon D.", ""], ["Prabhat", "", ""]]}, {"id": "1803.00125", "submitter": "Enrique del Castillo", "authors": "Enrique del Castillo, Adam Meyers, Peng Chen", "title": "A social Network Analysis of the Operations Research/Industrial\n  Engineering Faculty Hiring Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the U.S. Operations Research/Industrial-Systems Engineering (ORIE)\nfaculty hiring network, consisting of 1,179 faculty origin and destination data\ntogether with attribute data from 83 ORIE departments. A social network\nanalysis of faculty hires can reveal important patterns in an academic field,\nsuch as the existence of a hierarchy or sociological aspects such as the\npresence of communities of departments. We first statistically test for the\nexistence of a linear hierarchy in the network and for its steepness. We find a\nnear linear hierarchical order of the departments, proposing a new index for\nhiring networks, which we contrast with other indicators of hierarchy,\nincluding published rankings. A single index is not capable to capture the full\nstructure of a complex network, however, so we next fit a latent exponential\nrandom graph model (ERGM) to the network, which is able to reproduce its main\nobserved characteristics: high incidence of self-hiring, skewed out-degree\ndistribution, low density and clustering. Finally, we use the latent variables\nin the ERGM to simplify the network to one where faculty hires take place among\nthree groups of departments. We contrast our findings with those reported for\nother related disciplines, Computer Science and Business.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 22:55:25 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 22:57:34 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["del Castillo", "Enrique", ""], ["Meyers", "Adam", ""], ["Chen", "Peng", ""]]}, {"id": "1803.00200", "submitter": "Chun Li", "authors": "Bryan E. Shepherd, Qi Liu, Valentine Wanga, Chun Li", "title": "Probability-Scale Residuals in HIV/AIDS Research: Diagnostics and\n  Inference", "comments": "28 pages, 4 figures, 2 tables. Book chapter in Quantitative Methods\n  for HIV/AIDS Research, 2017, edited by Chan C, Hudgens MG, Chow SC", "journal-ref": "Quantitative Methods for HIV/AIDS Research, 2017, edited by Chan\n  C, Hudgens MG, Chow SC", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability-scale residual (PSR) is well defined across a wide variety of\nvariable types and models, making it useful for studies of HIV/AIDS. In this\nmanuscript, we highlight some of the properties of the PSR and illustrate its\napplication with HIV data. As a residual, it can be useful for model\ndiagnostics; we demonstrate its use with ordered categorical data and\nsemiparametric transformation models. The PSR can also be used to construct\ntests of residual correlation. In fact, partial Spearman's rank correlation\nbetween $X$ and $Y$ while adjusting for covariates $Z$ can be constructed as\nthe correlation between PSRs from models of $Y$ on $Z$ and of $X$ on $Z$. The\ncovariance of PSRs is also useful in some settings. We apply these methods to a\nvariety of HIV datasets including 1) a study examining risk factors for more\nsevere forms of cervical lesions among 145 women living with HIV in Zambia, 2)\na study investigating the association between 21 metabolomic biomarkers among\n70 HIV-positive patients in the southeastern United States, and 3) a genome\nwide association study investigating the association between single nucleotide\npolymorphisms and tenofovir clearance among 501 HIV-positive persons\nparticipating in a multi-site randomized clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 03:58:52 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Shepherd", "Bryan E.", ""], ["Liu", "Qi", ""], ["Wanga", "Valentine", ""], ["Li", "Chun", ""]]}, {"id": "1803.00374", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\'e and Angela Montanari", "title": "A bootstrap test to detect prominent Granger-causalities across\n  frequencies", "comments": null, "journal-ref": null, "doi": "10.1007/s10614-021-10112-x", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger-causality in the frequency domain is an emerging tool to analyze the\ncausal relationship between two time series. We propose a bootstrap test on\nunconditional and conditional Granger-causality spectra, as well as on their\ndifference, to catch particularly prominent causality cycles in relative terms.\nIn particular, we consider a stochastic process derived applying independently\nthe stationary bootstrap to the original series. Our null hypothesis is that\neach causality or causality difference is equal to the median across\nfrequencies computed on that process. In this way, we are able to disambiguate\ncausalities which depart significantly from the median one obtained ignoring\nthe causality structure. Our test shows power one as the process tends to\nnon-stationarity, thus being more conservative than parametric alternatives. As\nan example, we infer about the relationship between money stock and GDP in the\nEuro Area via our approach, considering inflation, unemployment and interest\nrates as conditioning variables. We point out that during the period 1999-2017\nthe money stock aggregate M1 had a significant impact on economic output at all\nfrequencies, while the opposite relationship is significant only at high\nfrequencies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 14:17:07 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 21:35:59 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Farn\u00e9", "Matteo", ""], ["Montanari", "Angela", ""]]}, {"id": "1803.00464", "submitter": "Alexandre Boumezoued", "authors": "Fabrice Balland, Alexandre Boumezoued, Laurent Devineau, Marine\n  Habart, Tom Popa", "title": "Mortality data reliability in an internal model", "comments": null, "journal-ref": "Ann. actuar. sci. 14 (2020) 420-444", "doi": "10.1017/S1748499520000081", "report-no": null, "categories": "q-fin.RM math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the impact of some mortality data anomalies on an\ninternal model capturing longevity risk in the Solvency 2 framework. In\nparticular, we are concerned with abnormal cohort effects such as those for\ngenerations 1919 and 1920, for which the period tables provided by the Human\nMortality Database show particularly low and high mortality rates respectively.\nTo provide corrected tables for the three countries of interest here (France,\nItaly and West Germany), we use the approach developed by Boumezoued (2016) for\ncountries for which the method applies (France and Italy), and provide an\nextension of the method for West Germany as monthly fertility histories are not\nsufficient to cover the generations of interest. These mortality tables are\ncrucial inputs to stochastic mortality models forecasting future scenarios,\nfrom which the extreme 0,5% longevity improvement can be extracted, allowing\nfor the calculation of the Solvency Capital Requirement (SCR). More precisely,\nto assess the impact of such anomalies in the Solvency II framework, we use a\nsimplified internal model based on three usual stochastic models to project\nmortality rates in the future combined with a closure table methodology for\nolder ages. Correcting this bias obviously improves the data quality of the\nmortality inputs, which is of paramount importance today, and slightly\ndecreases the capital requirement. Overall, the longevity risk assessment\nremains stable, as well as the selection of the stochastic mortality model. As\na collateral gain of this data quality improvement, the more regular estimated\nparameters allow for new insights and a refined assessment regarding longevity\nrisk.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:46:04 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Balland", "Fabrice", ""], ["Boumezoued", "Alexandre", ""], ["Devineau", "Laurent", ""], ["Habart", "Marine", ""], ["Popa", "Tom", ""]]}, {"id": "1803.00513", "submitter": "Ixavier Higgins", "authors": "Ixavier A. Higgins, Suprateek Kundu and Ying Guo", "title": "Integrative Bayesian Analysis of Brain Functional Networks Incorporating\n  Anatomical Knowledge", "comments": "27 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been increased interest in fusing multimodal imaging to\nbetter understand brain organization. Specifically, accounting for knowledge of\nanatomical pathways connecting brain regions should lead to desirable outcomes\nsuch as increased accuracy in functional brain network estimates and greater\nreproducibility of topological features across scanning sessions. Despite the\nclear merits, major challenges persist in integrative analyses including an\nincomplete understanding of the structure-function relationship and\ninaccuracies in mapping anatomical structures due to deficiencies in existing\nimaging technology. Clearly advanced network modeling tools are needed to\nappropriately incorporate anatomical structure in constructing brain functional\nnetworks. We propose a hierarchical Bayesian Gaussian graphical modeling\napproach that estimates the functional networks via sparse precision matrices\nwhose degree of edge-specific shrinkage is informed by anatomical structure and\nan independent baseline component. The approach flexibly identifies functional\nconnections supported by structural connectivity knowledge. This enables robust\nbrain network estimation even in the presence of mis-specified anatomical\nknowledge, while accommodating heterogeneity in the structure-function\nrelationship. We implement the approach via an efficient optimization algorithm\nyielding maximum a posteriori estimates. Extensive numerical studies reveal the\nclear advantages of our approach over competing methods in accurately\nestimating brain functional connectivity, even when the anatomical knowledge is\nmis-specified. An application of the approach to the Philadelphia\nNeurodevelopmental Cohort (PNC) study reveals gender based connectivity\ndifferences across multiple age groups, and higher reproducibility in the\nestimation of network metrics compared to alternative methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:23:58 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Higgins", "Ixavier A.", ""], ["Kundu", "Suprateek", ""], ["Guo", "Ying", ""]]}, {"id": "1803.00609", "submitter": "Alberto Abadie", "authors": "Alberto Abadie", "title": "On Statistical Non-Significance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significance tests are probably the most extended form of inference in\nempirical research, and significance is often interpreted as providing greater\ninformational content than non-significance. In this article we show, however,\nthat rejection of a point null often carries very little information, while\nfailure to reject may be highly informative. This is particularly true in\nempirical contexts where data sets are large and where there are rarely reasons\nto put substantial prior probability on a point null. Our results challenge the\nusual practice of conferring point null rejections a higher level of scientific\nsignificance than non-rejections. In consequence, we advocate a visible\nreporting and discussion of non-significant results in empirical practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 20:15:28 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Abadie", "Alberto", ""]]}, {"id": "1803.00698", "submitter": "Kellie Ottoboni", "authors": "Mark Lindeman, Neal McBurnett, Kellie Ottoboni, Philip B. Stark", "title": "Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorado conducted risk-limiting tabulation audits (RLAs) across the state in\n2017, including both ballot-level comparison audits and ballot-polling audits.\nThose audits only covered contests restricted to a single county; methods to\nefficiently audit contests that cross county boundaries and combine ballot\npolling and ballot-level comparisons have not been available.\n  Colorado's current audit software (RLATool) needs to be improved to audit\nthese contests that cross county lines and to audit small contests efficiently.\n  This paper addresses these needs. It presents extremely simple but\ninefficient methods, more efficient methods that combine ballot polling and\nballot-level comparisons using stratified samples, and methods that combine\nballot-level comparison and variable-size batch comparison audits in a way that\ndoes not require stratified sampling.\n  We conclude with some recommendations, and illustrate our recommended method\nusing examples that compare them to existing approaches. Exemplar open-source\ncode and interactive Jupyter notebooks are provided that implement the methods\nand allow further exploration.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:46:37 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Lindeman", "Mark", ""], ["McBurnett", "Neal", ""], ["Ottoboni", "Kellie", ""], ["Stark", "Philip B.", ""]]}, {"id": "1803.00967", "submitter": "Zi Wang", "authors": "Zi Wang and Caelan Reed Garrett and Leslie Pack Kaelbling and Tom\\'as\n  Lozano-P\\'erez", "title": "Active model learning and diverse action sampling for task and motion\n  planning", "comments": "Proceedings of the 2018 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS), Madrid, Spain.\n  https://www.youtube.com/playlist?list=PLoWhBFPMfSzDbc8CYelsbHZa1d3uz-W_c", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to augment the basic abilities of a robot by\nlearning to use new sensorimotor primitives to enable the solution of complex\nlong-horizon problems. Solving long-horizon problems in complex domains\nrequires flexible generative planning that can combine primitive abilities in\nnovel combinations to solve problems as they arise in the world. In order to\nplan to combine primitive actions, we must have models of the preconditions and\neffects of those actions: under what circumstances will executing this\nprimitive achieve some particular effect in the world?\n  We use, and develop novel improvements on, state-of-the-art methods for\nactive learning and sampling. We use Gaussian process methods for learning the\nconditions of operator effectiveness from small numbers of expensive training\nexamples collected by experimentation on a robot. We develop adaptive sampling\nmethods for generating diverse elements of continuous sets (such as robot\nconfigurations and object poses) during planning for solving a new task, so\nthat planning is as efficient as possible. We demonstrate these methods in an\nintegrated system, combining newly learned models with an efficient\ncontinuous-space robot task and motion planner to learn to solve long horizon\nproblems more efficiently than was previously possible.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 17:40:18 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 16:08:00 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Zi", ""], ["Garrett", "Caelan Reed", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1803.01159", "submitter": "Guodong Du", "authors": "Guodong Du, Liang Yuan, Kong Joo Shin, Shunsuke Managi", "title": "Enhancement of land-use change modeling using convolutional neural\n  networks and convolutional denoising autoencoders", "comments": "26 pages, 8 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neighborhood effect is a key driving factor for the land-use change (LUC)\nprocess. This study applies convolutional neural networks (CNN) to capture\nneighborhood characteristics from satellite images and to enhance the\nperformance of LUC modeling. We develop a hybrid CNN model (conv-net) to\npredict the LU transition probability by combining satellite images and\ngeographical features. A spatial weight layer is designed to incorporate the\ndistance-decay characteristics of neighborhood effect into conv-net. As an\nalternative model, we also develop a hybrid convolutional denoising autoencoder\nand multi-layer perceptron model (CDAE-net), which specifically learns latent\nrepresentations from satellite images and denoises the image data. Finally, a\nDINAMICA-based cellular automata (CA) model simulates the LU pattern. The\nresults show that the convolutional-based models improve the modeling\nperformances compared with a model that accepts only the geographical features.\nOverall, conv-net outperforms CDAE-net in terms of LUC predictive performance.\nNonetheless, CDAE-net performs better when the data are noisy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 13:28:51 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Du", "Guodong", ""], ["Yuan", "Liang", ""], ["Shin", "Kong Joo", ""], ["Managi", "Shunsuke", ""]]}, {"id": "1803.01203", "submitter": "Shaobo Han", "authors": "Shaobo Han and David B. Dunson", "title": "Multiresolution Tensor Decomposition for Multiple Spatial Passing\n  Networks", "comments": "34 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by soccer positional passing networks collected\nacross multiple games. We refer to these data as replicated spatial passing\nnetworks---to accurately model such data it is necessary to take into account\nthe spatial positions of the passer and receiver for each passing event. This\nspatial registration and replicates that occur across games represent key\ndifferences with usual social network data. As a key step before investigating\nhow the passing dynamics influence team performance, we focus on developing\nmethods for summarizing different team's passing strategies. Our proposed\napproach relies on a novel multiresolution data representation framework and\nPoisson nonnegative block term decomposition model, which automatically\nproduces coarse-to-fine low-rank network motifs. The proposed methods are\napplied to detailed passing record data collected from the 2014 FIFA World Cup.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 16:57:48 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Han", "Shaobo", ""], ["Dunson", "David B.", ""]]}, {"id": "1803.01223", "submitter": "Donald Richards", "authors": "Ishan Muzumdar and Donald Richards", "title": "Long-Term Implications of the Revenue Transfer Methodology in the\n  Affordable Care Act", "comments": "11 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Affordable Care Act introduced a revenue transfer formula that requires\ninsurance plans with generally healthier enrollees to pay funds into a revenue\ntransfer pool for to reimburse plans with generally less healthy enrollees. For\na given plan, the issue arises of whether the plan will be a payer into or a\nreceiver from the pool in a chosen future year. To examine that issue, we\nanalyze data from The Actuary Magazine on transfer payments for 2014-2015, and\nwe infer strong evidence of a statistical relationship between year-to-year\ntransfer payments. We also apply to the data a Markov transition model to study\nannual changes in the payer-receiver statuses of insurance plans. We estimate\nthat the limiting conditional probability that an insurance plan will pay into\nthe pool, given that the plan had paid into the pool in 2014, is 55.6 percent.\nFurther, that limiting probability is attained quickly because the conditional\nprobability that an insurance plan will pay into the pool in 2024, given that\nthe plan had paid into the pool in 2014, is estimated to be 55.7 percent. We\nalso find the revenue transfer system to have the disturbing feature that once\na plan enters the \"state\" of paying into the pool then it will stay in that\nstate for an average period of 4.87 years; moreover, once a plan has received\nfunds from the pool then it will stay in that state for an average period of\n3.89 years.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 19:54:20 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 04:00:10 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 04:31:46 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Muzumdar", "Ishan", ""], ["Richards", "Donald", ""]]}, {"id": "1803.01327", "submitter": "Tsuyoshi Kunihama", "authors": "Tsuyoshi Kunihama, Zehang Richard Li, Samuel J. Clark, Tyler H.\n  McCormick", "title": "Bayesian factor models for probabilistic cause of death assessment with\n  verbal autopsies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of deaths by cause provides crucial information for public\nhealth planning, response, and evaluation. About 60% of deaths globally are not\nregistered or given a cause, limiting our ability to understand disease\nepidemiology. Verbal autopsy (VA) surveys are increasingly used in such\nsettings to collect information on the signs, symptoms, and medical history of\npeople who have recently died. This article develops a novel Bayesian method\nfor estimation of population distributions of deaths by cause using verbal\nautopsy data. The proposed approach is based on a multivariate probit model\nwhere associations among items in questionnaires are flexibly induced by latent\nfactors. Using the Population Health Metrics Research Consortium labeled data\nthat include both VA and medically certified causes of death, we assess\nperformance of the proposed method. Further, we estimate important\nquestionnaire items that are highly associated with causes of death. This\nframework provides insights that will simplify future data collection.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 09:44:31 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:49:22 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kunihama", "Tsuyoshi", ""], ["Li", "Zehang Richard", ""], ["Clark", "Samuel J.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1803.01328", "submitter": "Mingyuan Zhou", "authors": "Hao Zhang, Bo Chen, Dandan Guo, Mingyuan Zhou", "title": "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train an inference network jointly with a deep generative topic model,\nmaking it both scalable to big corpora and fast in out-of-sample prediction, we\ndevelop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet\nallocation, which infers posterior samples via a hybrid of stochastic-gradient\nMCMC and autoencoding variational Bayes. The generative network of WHAI has a\nhierarchy of gamma distributions, while the inference network of WHAI is a\nWeibull upward-downward variational autoencoder, which integrates a\ndeterministic-upward deep neural network, and a stochastic-downward deep\ngenerative model based on a hierarchy of Weibull distributions. The Weibull\ndistribution can be used to well approximate a gamma distribution with an\nanalytic Kullback-Leibler divergence, and has a simple reparameterization via\nthe uniform noise, which help efficiently compute the gradients of the evidence\nlower bound with respect to the parameters of the inference network. The\neffectiveness and efficiency of WHAI are illustrated with experiments on big\ncorpora.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 09:53:59 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 14:57:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Bo", ""], ["Guo", "Dandan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1803.01496", "submitter": "Kurnia Susvitasari", "authors": "Kurnia Susvitasari", "title": "Stochastic Model of SIR Epidemic Modelling", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Threshold theorem is probably the most important development of mathematical\nepidemic modelling. Unfortunately, some models may not behave according to the\nthreshold. In this paper, we will focus on the final outcome of SIR model with\ndemography. The behaviour of the model approached by deteministic and\nstochastic models will be introduced, mainly using simulations. Furthermore, we\nwill also investigate the dynamic of susceptibles in population in absence of\ninfective. We have successfully showed that both deterministic and stochastic\nmodels performed similar results when $R_0 \\leq 1$. That is, the disease-free\nstage in the epidemic. But when $R_0 > 1$, the deterministic and stochastic\napproaches had different interpretations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 04:59:15 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Susvitasari", "Kurnia", ""]]}, {"id": "1803.01688", "submitter": "Gianpaolo Scalia Tomba", "authors": "Tom Britton and Gianpaolo Scalia Tomba", "title": "Estimation in emerging epidemics: biases and remedies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analysing new emerging infectious disease outbreaks one typically has\nobservational data over a limited period of time and several parameters to\nestimate, such as growth rate, R0, serial or generation interval distribution,\nlatent and incubation times or case fatality rates. Also parameters describing\nthe temporal relations between appearance of symptoms, notification, death and\nrecovery/discharge will be of interest. These parameters form the basis for\npredicting the future outbreak, planning preventive measures and monitoring the\nprogress of the disease. We study the problem of making inference during the\nemerging phase of an outbreak and point out potential sources of bias related\nto contact tracing, replacing generation times by serial intervals, multiple\npotential infectors or truncation effects amplified by exponential growth.\nThese biases directly affect the estimation of e.g. the generation time\ndistribution and the case fatality rate, but can then propagate to other\nestimates, e.g. of R0 and growth rate. Many of the traditionally used\nestimation methods in disease epidemiology may suffer from these biases when\napplied to the emerging disease outbreak situation. We show how to avoid these\nbiases based on proper statistical modelling. We illustrate the theory by\nnumerical examples and simulations based on the recent 2014-15 Ebola outbreak\nto quantify possible estimation biases, which may be up to 20% underestimation\nof R0, if the epidemic growth rate is fitted to observed data or, conversely,\nup to 62% overestimation of the growth rate if the correct R0 is used in\nconjunction with the Euler-Lotka equation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:45:23 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Britton", "Tom", ""], ["Tomba", "Gianpaolo Scalia", ""]]}, {"id": "1803.01710", "submitter": "Hau-tieng Wu", "authors": "Gi-Ren Liu, Yu-Lun Lo, John Malik, Yuan-Chung Sheu, Hau-tieng Wu", "title": "Diffuse to fuse EEG spectra -- intrinsic geometry of sleep dynamics for\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for sleep dynamics visualization and automatic\nannotation by applying diffusion geometry based sensor fusion algorithm to fuse\nspectral information from two electroencephalograms (EEG). The diffusion\ngeometry approach helps organize the nonlinear dynamical structure hidden in\nthe EEG signal. The visualization is achieved by the nonlinear dimension\nreduction capability of the chosen diffusion geometry algorithms. For the\nautomatic annotation purpose, the {support vector machine} is trained to\npredict the sleep stage. The prediction performance is validated on a publicly\navailable benchmark database, Physionet Sleep-EDF [extended] SC$^*$ {(SC =\nSleep Cassette)} and ST$^*$ {(ST = Sleep Telemetry)}, with the\nleave-one-subject-out cross validation. When we have a single EEG channel\n(Fpz-Cz), the overall accuracy, macro F1 and Cohen's kappa achieve\n$82.72\\%$,$75.91\\%$ and $76.1\\%$ respectively in Sleep-EDF SC$^*$ and\n$78.63\\%$, $73.58\\%$ and $69.48\\%$ in Sleep-EDF ST$^*$. This performance is\ncompatible {with} the state-of-the-art results. When we have two EEG channels\n(Fpz-Cz and Pz-Oz), the overall accuracy, macro F1 and Cohen's kappa achieve\n$84.44\\%$,$78.25\\%$ and $78.36\\%$ respectively in Sleep-EDF SC$^*$ and\n$79.05\\%$, $74.73\\%$ and $70.31\\%$ in Sleep-EDF ST$^*$. The results suggest the\npotential of the proposed algorithm in practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 22:26:05 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 01:42:26 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Liu", "Gi-Ren", ""], ["Lo", "Yu-Lun", ""], ["Malik", "John", ""], ["Sheu", "Yuan-Chung", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1803.02024", "submitter": "Peng Ding", "authors": "Fan Yang, Peng Ding", "title": "Using Survival Information in Truncation by Death Problems Without the\n  Monotonicity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some randomized clinical trials, patients may die before the measurements\nof their outcomes. Even though randomization generates comparable treatment and\ncontrol groups, the remaining survivors often differ significantly in\nbackground variables that are prognostic to the outcomes. This is called the\ntruncation by death problem. Under the potential outcomes framework, the only\nwell-defined causal effect on the outcome is within the subgroup of patients\nwho would always survive under both treatment and control. Because the\ndefinition of the subgroup depends on the potential values of the survival\nstatus that could not be observed jointly, without making strong parametric\nassumptions, we cannot identify the causal effect of interest and consequently\ncan only obtain bounds of it. Unfortunately, however, many bounds are too wide\nto be useful. We propose to use detailed survival information before and after\nthe measurements of the outcomes to sharpen the bounds of the subgroup causal\neffect. Because survival times contain useful information about the final\noutcome, carefully utilizing them could improve statistical inference without\nimposing strong parametric assumptions. Moreover, we propose to use a copula\nmodel to relax the commonly-invoked but often doubtful monotonicity assumption\nthat the treatment extends the survival time for all patients.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 05:55:34 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Yang", "Fan", ""], ["Ding", "Peng", ""]]}, {"id": "1803.02174", "submitter": "Philipp Arras", "authors": "Philipp Arras, Jakob Knollm\\\"uller, Henrik Junklewitz and Torsten A.\n  En{\\ss}lin", "title": "Radio Imaging With Information Field Theory", "comments": "5 pages, 3 figures", "journal-ref": "2018 26th European Signal Processing Conference (EUSIPCO)", "doi": "10.23919/EUSIPCO.2018.8553533", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data from radio interferometers provide a substantial challenge for\nstatisticians. It is incomplete, noise-dominated and originates from a\nnon-trivial measurement process. The signal is not only corrupted by imperfect\nmeasurement devices but also from effects like fluctuations in the ionosphere\nthat act as a distortion screen. In this paper we focus on the imaging part of\ndata reduction in radio astronomy and present RESOLVE, a Bayesian imaging\nalgorithm for radio interferometry in its new incarnation. It is formulated in\nthe language of information field theory. Solely by algorithmic advances the\ninference could be sped up significantly and behaves noticeably more stable\nnow. This is one more step towards a fully user-friendly version of RESOLVE\nwhich can be applied routinely by astronomers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 13:52:06 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Arras", "Philipp", ""], ["Knollm\u00fcller", "Jakob", ""], ["Junklewitz", "Henrik", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1803.02402", "submitter": "Jonathan Yefenof", "authors": "Jonathan Yefenof, Yair Goldberg, Jennifer Wiler, Avishai Mandelbaum\n  and Ya'acov Ritov", "title": "Self-reporting and screening: Data with current-status and censored\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider survival data that combine three types of observations:\nuncensored, right-censored, and left-censored. Such data arises from screening\na medical condition, in situations where self-detection arises naturally. Our\ngoal is to estimate the failure-time distribution, based on these three\nobservation types. We propose a novel methodology for distribution estimation\nusing both parametric and nonparametric techniques. We then evaluate the\nperformance of these estimators via simulated data. Finally, as a case study,\nwe estimate the patience of patients who arrive at an emergency department and\nwait for treatment. Three categories of patients are observed: those who leave\nthe system and announce it, and thus their patience time is observed; those who\nget service and thus their patience time is right-censored by the waiting time;\nand those who leave the system without announcing it. For the third category,\nthe patients' absence is revealed only when they are called to service, which\nis after they have already left; formally, their patience time is\nleft-censored. Other applications of our proposed methodology are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:47:26 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Yefenof", "Jonathan", ""], ["Goldberg", "Yair", ""], ["Wiler", "Jennifer", ""], ["Mandelbaum", "Avishai", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1803.02433", "submitter": "Mohsen Kamrani", "authors": "Mohsen Kamrani, Ramin Arvin, Asad J. Khattak", "title": "Extracting useful information from connected vehicle data: An empirical\n  study of driving volatility measures and crash frequency at intersections", "comments": "Forthcoming, submitted for publication in Transportation Research\n  Record: Journal of the Transportation Research Board", "journal-ref": "Transportation Research Record: Journal of the Transportation\n  Research Board (2018)", "doi": "10.1177/0361198118773869", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of high-frequency connected and automated vehicle data,\nanalysts have become able to extract useful information from them. To this end,\nthe concept of \"driving volatility\" is defined and explored as deviation from\nthe norm. Several measures of dispersion and variation can be computed in\ndifferent ways using vehicles' instantaneous speed, acceleration, and jerk\nobserved at intersections. This study explores different measures of\nvolatility, representing newly available surrogate measures of safety, by\ncombining data from the Michigan Safety Pilot Deployment of connected vehicles\nwith crash and inventory data at several intersections. The intersection data\nwas error-checked and verified for accuracy. Then, for each intersection, 37\ndifferent measures of volatility were calculated. These volatilities were then\nused to explain crash frequencies at intersection by estimating fixed and\nrandom parameter Poisson regression models. Results show that an increase in\nthree measures of driving volatility are positively associated with higher\nintersection crash frequency, controlling for exposure variables and geometric\nfeatures. More intersection crashes were associated with higher percentages of\nvehicle data points (speed & acceleration) lying beyond threshold-bands. These\nbands were created using mean plus two standard deviations. Furthermore, a\nhigher magnitude of time-varying stochastic volatility of vehicle speeds when\nthey pass through the intersection is associated with higher crash frequencies.\nThese measures can be used to locate intersections with high driving\nvolatilities, i.e., hot-spots where crashes are waiting to happen. Therefore, a\ndeeper analysis of these intersections can be undertaken and proactive safety\ncountermeasures considered at high volatility locations to enhance safety.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 21:46:18 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 17:02:24 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Kamrani", "Mohsen", ""], ["Arvin", "Ramin", ""], ["Khattak", "Asad J.", ""]]}, {"id": "1803.02527", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Paul de Figueiredo,\n  Sing-Hoi Sze, Mingyuan Zhou, and Xiaoning Qian", "title": "Differential Expression Analysis of Dynamical Sequencing Count Data with\n  a Gamma Markov Chain", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing (NGS) to profile temporal changes in living\nsystems is gaining more attention for deriving better insights into the\nunderlying biological mechanisms compared to traditional static sequencing\nexperiments. Nonetheless, the majority of existing statistical tools for\nanalyzing NGS data lack the capability of exploiting the richer information\nembedded in temporal data. Several recent tools have been developed to analyze\nsuch data but they typically impose strict model assumptions, such as\nsmoothness on gene expression dynamic changes. To capture a broader range of\ngene expression dynamic patterns, we develop the gamma Markov negative binomial\n(GMNB) model that integrates a gamma Markov chain into a negative binomial\ndistribution model, allowing flexible temporal variation in NGS count data.\nUsing Bayes factors, GMNB enables more powerful temporal gene differential\nexpression analysis across different phenotypes or treatment conditions. In\naddition, it naturally handles the heterogeneity of sequencing depth in\ndifferent samples, removing the need for ad-hoc normalization. Efficient Gibbs\nsampling inference of the GMNB model parameters is achieved by exploiting novel\ndata augmentation techniques. Extensive experiments on both simulated and\nreal-world RNA-seq data show that GMNB outperforms existing methods in both\nreceiver operating characteristic (ROC) and precision-recall (PR) curves of\ndifferential expression analysis results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 05:31:55 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Dadaneh", "Siamak Zamani", ""], ["de Figueiredo", "Paul", ""], ["Sze", "Sing-Hoi", ""], ["Zhou", "Mingyuan", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1803.02626", "submitter": "PierGianLuca Porta Mana", "authors": "PierGianLuca Porta Mana, Claudia Bachmann, Abigail Morrison", "title": "Inferring health conditions from fMRI-graph data", "comments": "V1: 35 pages, 5 figures, 2 tables. V2: 36 pages, 5 figures, 2 tables;\n  partially rewritten all sections and added references. V3: Rewritten\n  introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated classification methods for disease diagnosis are currently in the\nlimelight, especially for imaging data. Classification does not fully meet a\nclinician's needs, however: in order to combine the results of multiple tests\nand decide on a course of treatment, a clinician needs the likelihood of a\ngiven health condition rather than binary classification yielded by such\nmethods. We illustrate how likelihoods can be derived step by step from first\nprinciples and approximations, and how they can be assessed and selected,\nillustrating our approach using fMRI data from a publicly available data set\ncontaining schizophrenic and healthy control subjects. We start from the basic\nassumption of partial exchangeability, and then the notion of sufficient\nstatistics and the \"method of translation\" (Edgeworth, 1898) combined with\nconjugate priors. This method can be used to construct a likelihood that can be\nused to compare different data-reduction algorithms. Despite the\nsimplifications and possibly unrealistic assumptions used to illustrate the\nmethod, we obtain classification results comparable to previous, more realistic\nstudies about schizophrenia, whilst yielding likelihoods that can naturally be\ncombined with the results of other diagnostic tests.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:58:46 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 14:42:08 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 10:48:02 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Mana", "PierGianLuca Porta", ""], ["Bachmann", "Claudia", ""], ["Morrison", "Abigail", ""]]}, {"id": "1803.02704", "submitter": "Felix Bestehorn", "authors": "Felix Bestehorn and Maike Bestehorn and Markus Bestehorn and Christian\n  Kirches", "title": "A deterministic balancing score algorithm to avoid common pitfalls of\n  propensity score matching", "comments": "25 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching (PSM) is the de-facto standard for estimating\ncausal effects in observational studies. We show that PSM and its\nimplementations are susceptible to several major drawbacks and illustrate these\nfindings using a case study with $17,427$ patients. We derive four formal\nproperties an optimal statistical matching algorithm should meet, and propose\nDeterministic Balancing Score exact Matching (DBSeM) which meets the\naforementioned properties for an exact matching. Furthermore, we investigate\none of the main problems of PSM, that is that common PSM results in one valid\nset of matched pairs or a bootstrapped PSM in a selection of possible valid\nsets of matched pairs. For exact matchings we provide the mathematical proof,\nthat DBSeM, as a result, delivers the expected value of all valid sets of\nmatched pairs for the investigated dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:08:08 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 06:52:13 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 15:43:47 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 15:06:06 GMT"}, {"version": "v5", "created": "Fri, 17 May 2019 12:53:28 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Bestehorn", "Felix", ""], ["Bestehorn", "Maike", ""], ["Bestehorn", "Markus", ""], ["Kirches", "Christian", ""]]}, {"id": "1803.02707", "submitter": "Michael Lebacher", "authors": "Michael Lebacher, Paul W. Thurner and G\\\"oran Kauermann", "title": "A Dynamic Separable Network Model with Actor Heterogeneity: An\n  Application to Global Weapons Transfers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to extend the separable temporal exponential random\ngraph model (STERGM) to account for time-varying network- and actor-specific\neffects. Our application case is the network of international major\nconventional weapons transfers, based on data from the Stockholm International\nPeace Research Institute (SIPRI). The application is particularly suitable\nsince it allows to distinguish the potentially differing driving forces for\ncreating new trade relationships and for the endurance of existing ones. In\naccordance with political economy models we expect security- and\nnetwork-related covariates to be most important for the formation of transfers,\nwhereas repeated transfers should prevalently be determined by the receivers'\nmarket size and military spending. Our proposed modelling approach corroborates\nthe hypothesis and quantifies the corresponding effects. Additionally, we\nsubject the time-varying heterogeneity effects to a functional principal\ncomponent analysis. This serves as exploratory tool and allows to identify\ncountries that stand out by exceptional increases or decreases of their\ntendency to import and export weapons.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:10:53 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 13:02:42 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 11:45:21 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 11:27:34 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Lebacher", "Michael", ""], ["Thurner", "Paul W.", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1803.02916", "submitter": "Lauri Mustonen", "authors": "Lauri Mustonen, Xiangxi Gao, Asteroide Santana, Rebecca Mitchell, Ymir\n  Vigfusson and Lars Ruthotto", "title": "A Bayesian framework for molecular strain identification from mixed\n  diagnostic samples", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": "10.1088/1361-6420/aad7cd", "report-no": null, "categories": "stat.AP math.NA q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a mathematical formulation and develop a computational framework\nfor identifying multiple strains of microorganisms from mixed samples of DNA.\nOur method is applicable in public health domains where efficient\nidentification of pathogens is paramount, e.g., for the monitoring of disease\noutbreaks. We formulate strain identification as an inverse problem that aims\nat simultaneously estimating a binary matrix (encoding presence or absence of\nmutations in each strain) and a real-valued vector (representing the mixture of\nstrains) such that their product is approximately equal to the measured data\nvector. The problem at hand has a similar structure to blind deconvolution,\nexcept for the presence of binary constraints, which we enforce in our\napproach. Following a Bayesian approach, we derive a posterior density. We\npresent two computational methods for solving the non-convex maximum a\nposteriori estimation problem. The first one is a local optimization method\nthat is made efficient and scalable by decoupling the problem into smaller\nindependent subproblems, whereas the second one yields a global minimizer by\nconverting the problem into a convex mixed-integer quadratic programming\nproblem. The decoupling approach also provides an efficient way to integrate\nover the posterior. This provides useful information about the ambiguity of the\nunderdetermined problem and, thus, the uncertainty associated with numerical\nsolutions. We evaluate the potential and limitations of our framework in silico\nusing synthetic and experimental data with available ground truths.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 23:40:12 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 00:31:49 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Mustonen", "Lauri", ""], ["Gao", "Xiangxi", ""], ["Santana", "Asteroide", ""], ["Mitchell", "Rebecca", ""], ["Vigfusson", "Ymir", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1803.03019", "submitter": "Amelia Sim\\'o", "authors": "Sonia Barahona, Pablo Centella, Ximo Gual-Arnau, Maria Victoria\n  Ib\\'a\\~nez and Amelia Sim\\'o", "title": "Generalized Linear Models for Geometrical Current predictors. An\n  application to predict garment fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to model an ordinal response variable in terms of\nvector-valued functional data included on a vector-valued RKHS. In particular,\nwe focus on the vector-valued RKHS obtained when a geometrical object (body) is\ncharacterized by a current and on the ordinal regression model. A common way to\nsolve this problem in functional data analysis is to express the data in the\northonormal basis given by decomposition of the covariance operator. But our\ndata present very important differences with respect to the usual functional\ndata setting. On the one hand, they are vector-valued functions, and on the\nother, they are functions in an RKHS with a previously defined norm. We propose\nto use three different bases: the orthonormal basis given by the kernel that\ndefines the RKHS, a basis obtained from decomposition of the integral operator\ndefined using the covariance function, and a third basis that combines the\nprevious two. The three approaches are compared and applied to an interesting\nproblem: building a model to predict the fit of children's garment sizes, based\non a 3D database of the Spanish child population.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 09:57:00 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 20:43:56 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Barahona", "Sonia", ""], ["Centella", "Pablo", ""], ["Gual-Arnau", "Ximo", ""], ["Ib\u00e1\u00f1ez", "Maria Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1803.03166", "submitter": "Aurelie Fischer", "authors": "Aur\\'elie Fischer (1), Mathilde Mougeot (1) ((1) LPSM UMR 8001)", "title": "Aggregation using input-output trade-off", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new learning strategy based on a seminal idea\nof Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for\ncombining several classifiers, relying on a consensus notion. In many\naggregation methods, the prediction for a new observation x is computed by\nbuilding a linear or convex combination over a collection of basic estimators\nr1(x),. .. , rm(x) previously calibrated using a training data set.\nMojirsheibani proposes to compute the prediction associated to a new\nobservation by combining selected outputs of the training examples. The output\nof a training example is selected if some kind of consensus is observed: the\npredictions computed for the training example with the different machines have\nto be \"similar\" to the prediction for the new observation. This approach has\nbeen recently extended to the context of regression in Biau et al. (2016). In\nthe original scheme, the agreement condition is actually required to hold for\nall individual estimators, which appears inadequate if there is one bad initial\nestimator. In practice, a few disagreements are allowed ; for establishing the\ntheoretical results, the proportion of estimators satisfying the condition is\nrequired to tend to 1. In this paper, we propose an alternative procedure,\nmixing the previous consensus ideas on the predictions with the Euclidean\ndistance computed between entries. This may be seen as an alternative approach\nallowing to reduce the effect of a possibly bad estimator in the initial list,\nusing a constraint on the inputs. We prove the consistency of our strategy in\nclassification and in regression. We also provide some numerical experiments on\nsimulated and real data to illustrate the benefits of this new aggregation\nmethod. On the whole, our practical study shows that our method may perform\nmuch better than the original combination technique, and, in particular,\nexhibit far less variance. We also show on simulated examples that this\nprocedure mixing inputs and outputs is still robust to high dimensional inputs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 15:43:34 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Fischer", "Aur\u00e9lie", "", "LPSM UMR 8001"], ["Mougeot", "Mathilde", "", "LPSM UMR 8001"]]}, {"id": "1803.03211", "submitter": "Weiqiang Zhu", "authors": "Weiqiang Zhu and Gregory C. Beroza", "title": "PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking\n  Method", "comments": null, "journal-ref": null, "doi": "10.1093/gji/ggy423", "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of seismic sensors grows, it is becoming increasingly difficult\nfor analysts to pick seismic phases manually and comprehensively, yet such\nefforts are fundamental to earthquake monitoring. Despite years of improvements\nin automatic phase picking, it is difficult to match the performance of\nexperienced analysts. A more subtle issue is that different seismic analysts\nmay pick phases differently, which can introduce bias into earthquake\nlocations. We present a deep-neural-network-based arrival-time picking method\ncalled \"PhaseNet\" that picks the arrival times of both P and S waves. Deep\nneural networks have recently made rapid progress in feature learning, and with\nsufficient training, have achieved super-human performance in many\napplications. PhaseNet uses three-component seismic waveforms as input and\ngenerates probability distributions of P arrivals, S arrivals, and noise as\noutput. We engineer PhaseNet such that peaks in probability provide accurate\narrival times for both P and S waves, and have the potential to increase the\nnumber of S-wave observations dramatically over what is currently available.\nThis will enable both improved locations and improved shear wave velocity\nmodels. PhaseNet is trained on the prodigious available data set provided by\nanalyst-labeled P and S arrival times from the Northern California Earthquake\nData Center. The dataset we use contains more than seven million waveform\nsamples extracted from over thirty years of earthquake recordings. We\ndemonstrate that PhaseNet achieves much higher picking accuracy and recall rate\nthan existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 17:28:21 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Zhu", "Weiqiang", ""], ["Beroza", "Gregory C.", ""]]}, {"id": "1803.03328", "submitter": "Yuwei Liao", "authors": "Yuwei Liao, Deovrat Kakde, Arin Chaudhuri, Hansi Jiang, Carol Sadek\n  and Seunghyun Kong", "title": "A new bandwidth selection criterion for using SVDD to analyze\n  hyperspectral data", "comments": null, "journal-ref": null, "doi": "10.1117/12.2314964", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for hyperspectral image classification that uses\nsupport vector data description (SVDD) with the Gaussian kernel function. SVDD\nhas been a popular machine learning technique for single-class classification,\nbut selecting the proper Gaussian kernel bandwidth to achieve the best\nclassification performance is always a challenging problem. This paper proposes\na new automatic, unsupervised Gaussian kernel bandwidth selection approach\nwhich is used with a multiclass SVDD classification scheme. The performance of\nthe multiclass SVDD classification scheme is evaluated on three frequently used\nhyperspectral data sets, and preliminary results show that the proposed method\ncan achieve better performance than published results on these data sets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 23:02:16 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 17:17:17 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 18:29:36 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 17:30:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liao", "Yuwei", ""], ["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""], ["Jiang", "Hansi", ""], ["Sadek", "Carol", ""], ["Kong", "Seunghyun", ""]]}, {"id": "1803.03364", "submitter": "Konstantin Zuev M", "authors": "Keegan Mendonca, Vasileios E. Kontosakos, Athanasios A. Pantelous, and\n  Konstantin M. Zuev", "title": "Efficient Pricing of Barrier Options on High Volatility Assets using\n  Subset Simulation", "comments": "41 pages, 9 figures, 3 tables, available at SSRN:\n  https://ssrn.com/abstract=3132336", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barrier options are one of the most widely traded exotic options on stock\nexchanges. In this paper, we develop a new stochastic simulation method for\npricing barrier options and estimating the corresponding execution\nprobabilities. We show that the proposed method always outperforms the standard\nMonte Carlo approach and becomes substantially more efficient when the\nunderlying asset has high volatility, while it performs better than multilevel\nMonte Carlo for special cases of barrier options and underlying assets. These\ntheoretical findings are confirmed by numerous simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:49:08 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 00:44:53 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Mendonca", "Keegan", ""], ["Kontosakos", "Vasileios E.", ""], ["Pantelous", "Athanasios A.", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "1803.03373", "submitter": "Yang Shi", "authors": "Yang Shi, Mengqiao Wang, Weiping Shi, Ji-Hyun Lee, Huining Kang and\n  Hui Jiang", "title": "Accurate and Efficient Estimation of Small P-values with the\n  Cross-Entropy Method: Applications in Genomic Data Analysis", "comments": "34 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small $p$-values are often required to be accurately estimated in large scale\ngenomic studies for the adjustment of multiple hypothesis tests and the ranking\nof genomic features based on their statistical significance. For those\ncomplicated test statistics whose cumulative distribution functions are\nanalytically intractable, existing methods usually do not work well with small\n$p$-values due to lack of accuracy or computational restrictions. We propose a\ngeneral approach for accurately and efficiently calculating small $p$-values\nfor a broad range of complicated test statistics based on the principle of the\ncross-entropy method and Markov chain Monte Carlo sampling techniques. We\nevaluate the performance of the proposed algorithm through simulations and\ndemonstrate its application to three real examples in genomic studies. The\nresults show that our approach can accurately evaluate small to extremely small\n$p$-values (e.g. $10^{-6}$ to $10^{-100}$). The proposed algorithm is helpful\nto the improvement of existing test procedures and the development of new test\nprocedures in genomic studies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 03:33:12 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 14:44:15 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Shi", "Yang", ""], ["Wang", "Mengqiao", ""], ["Shi", "Weiping", ""], ["Lee", "Ji-Hyun", ""], ["Kang", "Huining", ""], ["Jiang", "Hui", ""]]}, {"id": "1803.03491", "submitter": "Hussain Kazmi", "authors": "Hussain Kazmi, Johan Suykens, Johan Driesen", "title": "Valuing knowledge, information and agency in Multi-agent Reinforcement\n  Learning: a case study in smart buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing energy efficiency in buildings can reduce costs and emissions\nsubstantially. Historically, this has been treated as a local, or single-agent,\noptimization problem. However, many buildings utilize the same types of thermal\nequipment e.g. electric heaters and hot water vessels. During operation,\noccupants in these buildings interact with the equipment differently thereby\ndriving them to diverse regions in the state-space. Reinforcement learning\nagents can learn from these interactions, recorded as sensor data, to optimize\nthe overall energy efficiency. However, if these agents operate individually at\na household level, they can not exploit the replicated structure in the\nproblem. In this paper, we demonstrate that this problem can indeed benefit\nfrom multi-agent collaboration by making use of targeted exploration of the\nstate-space allowing for better generalization. We also investigate trade-offs\nbetween integrating human knowledge and additional sensors. Results show that\nsavings of over 40% are possible with collaborative multi-agent systems making\nuse of either expert knowledge or additional sensors with no loss of occupant\ncomfort. We find that such multi-agent systems comfortably outperform\ncomparable single agent systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 12:48:03 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Kazmi", "Hussain", ""], ["Suykens", "Johan", ""], ["Driesen", "Johan", ""]]}, {"id": "1803.03497", "submitter": "Fabricio Murai", "authors": "Francisco Galuppo Azevedo, Bruno Demattos Nogueira, Fabricio Murai,\n  Ana Paula Couto da Silva", "title": "Modelos de Resposta para Experimentos Randomizados em Redes Sociais de\n  Larga Escala", "comments": "15 pages, in Portuguese, 2 figures, submitted to SBC WPerformance\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B tests are randomized experiments frequently used by companies that offer\nservices on the Web for assessing the impact of new features. During an\nexperiment, each user is randomly redirected to one of two versions of the\nwebsite, called treatments. Several response models were proposed to describe\nthe behavior of a user in a social network website, where the treatment\nassigned to her neighbors must be taken into account. However, there is no\nconsensus as to which model should be applied to a given dataset. In this work,\nwe propose a new response model, derive theoretical limits for the estimation\nerror of several models, and obtain empirical results for cases where the\nresponse model was misspecified.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 13:15:20 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Azevedo", "Francisco Galuppo", ""], ["Nogueira", "Bruno Demattos", ""], ["Murai", "Fabricio", ""], ["da Silva", "Ana Paula Couto", ""]]}, {"id": "1803.03536", "submitter": "Michael Lebacher", "authors": "Michael Lebacher and G\\\"oran Kauermann", "title": "Exploring Dependence Structures in the International Arms Trade Network", "comments": "arXiv admin note: text overlap with arXiv:1803.02707", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we analyse dependence structures among international trade flows\nof major conventional weapons from 1952 to 2016. We employ a Network\nDisturbance Model commonly used in inferential network analysis and spatial\neconometrics. The dependence structure is represented by pre-defined weight\nmatrices that allow for relating the arms trade flows from the network of\ninternational arms exchange. Several different weight matrices are compared by\nmeans of the AIC in order to select the best dependence structure. It turns out\nthat the dependence structure among the arms trade flows is rather complex and\ncan be represented by a specification that, simply speaking, relates each arms\ntrade flow to all exports and imports of the sending and the receiving state.\nBy controlling for explanatory variables we are able to show the influence of\npolitical and economic variables on the volume traded.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 07:14:35 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lebacher", "Michael", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1803.03858", "submitter": "Sara Algeri", "authors": "Sara Algeri and David A. van Dyk", "title": "Testing One Hypothesis Multiple Times: The Multidimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of new rare signals in data, the detection of a sudden\nchange in a trend, and the selection of competing models, are among the most\nchallenging problems in statistical practice. These challenges can be tackled\nusing a test of hypothesis where a nuisance parameter is present only under the\nalternative, and a computationally efficient solution can be obtained by the\n\"Testing One Hypothesis Multiple times\" (TOHM) method. In the one-dimensional\nsetting, a fine discretization of the space of the non-identifiable parameter\nis specified, and a global p-value is obtained by approximating the\ndistribution of the supremum of the resulting stochastic process. In this\npaper, we propose a computationally efficient inferential tool to perform TOHM\nin the multidimensional setting. Here, the approximations of interest typically\ninvolve the expected Euler Characteristics (EC) of the excursion set of the\nunderlying random field. We introduce a simple algorithm to compute the EC in\nmultiple dimensions and for arbitrary large significance levels. This leads to\nan highly generalizable computational tool to perform inference under\nnon-standard regularity conditions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 19:49:26 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 17:39:19 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Algeri", "Sara", ""], ["van Dyk", "David A.", ""]]}, {"id": "1803.03987", "submitter": "Apostolos Gkatzionis", "authors": "Apostolos Gkatzionis and Stephen Burgess", "title": "Contextualizing selection bias in Mendelian randomization: how bad is it\n  likely to be?", "comments": "28 pages, 1 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection bias affects Mendelian randomization investigations when selection\ninto the study sample depends on a collider between the genetic variant and\nconfounders of the risk factor-outcome association. However, the relative\nimportance of selection bias for Mendelian randomization compared to other\npotential biases is unclear. We performed an extensive simulation study to\nassess the impact of selection bias on a typical Mendelian randomization\ninvestigation. Selection bias had a severe impact on bias and Type 1 error\nrates in our simulation study, but only when selection effects were large. For\nmoderate effects of the risk factor on selection, bias was generally small and\nType 1 error rate inflation was not considerable. The magnitude of bias was\nalso affected by the strength of confounder-risk factor and confounder-outcome\nassociations, the structure of the causal diagram and selection frequency. The\nuse of inverse probability weighting ameliorated bias when the selection model\nwas correctly specified, but increased bias when selection bias was moderate\nand the model was misspecified. Finally, we investigated whether selection bias\nmay explain a recently reported finding that lipoprotein(a) is not a causal\nrisk factor for cardiovascular mortality in individuals with previous coronary\nheart disease.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 17:00:51 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gkatzionis", "Apostolos", ""], ["Burgess", "Stephen", ""]]}, {"id": "1803.04024", "submitter": "Brandon Milholland", "authors": "Brandon Milholland, Xiao Dong and Jan Vijg", "title": "The shortness of human life constitutes its limit", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we affirm our earlier findings of evidence for a limit to\nhuman lifespan. In particular, we assess the analyses in extreme value theory\n(EVT) performed by Rootz\\'en and Zholud. We find that their criticisms of our\nwork are unfounded and that their analyses are contradicted by several other\npapers using EVT. Furthermore, we find that even if we completely accept the\nconclusions about late-life human mortality reached by Rootz\\'en and Zholud,\ntheir results do not actually contradict the findings presented in our original\npaper: whether unbounded or not, human lifespan is unlikely to greatly exceed\n120 years, and the improbability of longer survival---whether it is exactly\nzero or merely astronomically small---acts as a de facto limit. In order to\neliminate the confusion surrounding the issue, we propose the adoption of the\nterm \"limit\" to denote the age at which the chance of survival is exactly zero\nand the term \"effective limit\" to denote the age at which the change of\nsurvival falls below a given threshold. Once this distinction is made, it can\nbe demonstrated that the final result of Rootz\\'en and Zholud is essentially a\nrecapitulation of the main conclusion of our paper. Ultimately, much of the\ncontroversy surrounding the issue of a limit to human lifespan can be avoided\nby carefully reading the literature and applying statistics to practical human\nscales.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 19:38:41 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 01:37:50 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Milholland", "Brandon", ""], ["Dong", "Xiao", ""], ["Vijg", "Jan", ""]]}, {"id": "1803.04075", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Kernel estimation of the instantaneous frequency", "comments": null, "journal-ref": "I.E.E.E. Trans. Signal Processing 42, pp. 2644-2649 (1994)", "doi": "10.1109/78.324730", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider kernel estimators of the instantaneous frequency of a slowly\nevolving sinusoid in white noise. The expected estimation error consists of two\nterms. The systematic bias error grows as the kernel halfwidth increases while\nthe random error decreases. For a non-modulated signal, $g(t)$, the kernel\nhalfwidth which minimizes the expected error scales as$h \\sim \\left[{ \\sigma^2\n\\over\n  N| \\partial_t^2 g^{}|^2 } \\right]^{1/ 5}$, where %$A^{(\\ell)}$ is the\ncoherent signal at frequency, $f_{\\ell}$, $\\sigma^2$ is the noise variance and\n$N$ is the number of measurements per unit time. We show that estimating the\ninstantaneous frequency corresponds to estimating the first derivative of a\nmodulated signal, $A(t)\\exp(i\\phi(t))$. For instantaneous frequency estimation,\nthe halfwidth which minimizes the expected error is larger: $h_{1,3} \\sim\n\\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 }\n\\right]^{1/ 7}$. Since the optimal halfwidths depend on derivatives of the\nunknown function, we initially estimate these derivatives prior to estimating\nthe actual signal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:43:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.04077", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Statistical tests for evaluating an earthquake prediction method", "comments": null, "journal-ref": "Geophysical Research Letters 23, pp. 1407-1409 (1996)", "doi": "10.1029/96GL00476", "report-no": null, "categories": "stat.ME physics.data-an physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of including postcursors in the null hypothesis test is discussed.\nUnequal prediction probabilities can be included in the null hypothesis test\nusing a generalization of the central limit theorem. A test for determining the\nenhancement factor over random chance is given. The seismic earthquake signal\nmay preferentially precede earthquakes even if the VAN methodology fails to\nforecast the earthquakes. We formulate a statistical test for this possibility.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:49:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.04241", "submitter": "Fatemeh Nasiri", "authors": "Fatemeh Nasiri, Oscar Acosta-Tamayo", "title": "A Review of Mixed-Effect Modeling in the Longitudinal Studies Using\n  Medical Images of Patients", "comments": "Two pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review paper, some applications of the mixed effect modeling in\nmedial image processing and longitudinal analysis is studied. For this purpose,\na general structure is extracted from some of the researches in the literature.\nThis structure includes a number of essential elements, each of which having a\nfew design choices, namely 1) tracked features, 2) models mathematical\nexpression and random effects and finally 3) response prediction. Two research\nstudy examples in Alzheimers disease and prostate tomography are also briefly\nintroduced to further discuss the above design choices.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 09:58:58 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Nasiri", "Fatemeh", ""], ["Acosta-Tamayo", "Oscar", ""]]}, {"id": "1803.04353", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Partial Identifiability of Restricted Latent Class Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models have wide applications in social and biological sciences.\nIn many applications, pre-specified restrictions are imposed on the parameter\nspace of latent class models, through a design matrix, to reflect\npractitioners' assumptions about how the observed responses depend on subjects'\nlatent traits. Though widely used in various fields, such restricted latent\nclass models suffer from non-identifiability due to their discreteness nature\nand complex structure of restrictions. This work addresses the fundamental\nidentifiability issue of restricted latent class models by developing a general\nframework for strict and partial identifiability of the model parameters. Under\ncorrect model specification, the developed identifiability conditions only\ndepend on the design matrix and are easily checkable, which provide useful\npractical guidelines for designing statistically valid diagnostic tests.\nFurthermore, the new theoretical framework is applied to establish, for the\nfirst time, identifiability of several designs from cognitive diagnosis\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:23:08 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 05:06:45 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 01:20:42 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1803.04478", "submitter": "David Lattanzi", "authors": "Achyuthan Jootoo, David Lattanzi", "title": "Bridge type classification: supervised learning on a modified NBI\n  dataset", "comments": "Preprint of paper published in ASCE Journal of Computing\n  (https://ascelibrary.org/doi/full/10.1061/(ASCE)CP.1943-5487.0000712). 6\n  figures and 8 tables, provided at end of document", "journal-ref": "Journal of Computing in Civil Engineering 31.6 (2017): 04017063", "doi": "10.1061/(ASCE)CP.1943-5487.0000712", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key phase in the bridge design process is the selection of the structural\nsystem. Due to budget and time constraints, engineers typically rely on\nengineering judgment and prior experience when selecting a structural system,\noften considering a limited range of design alternatives. The objective of this\nstudy was to explore the suitability of supervised machine learning as a\npreliminary design aid that provides guidance to engineers with regards to the\nstatistically optimal bridge type to choose, ultimately improving the\nlikelihood of optimized design, design standardization, and reduced maintenance\ncosts. In order to devise this supervised learning system, data for over\n600,000 bridges from the National Bridge Inventory database were analyzed. Key\nattributes for determining the bridge structure type were identified through\nthree feature selection techniques. Potentially useful attributes like seismic\nintensity and historic data on the cost of materials (steel and concrete) were\nthen added from the US Geological Survey (USGS) database and Engineering News\nRecord. Decision tree, Bayes network and Support Vector Machines were used for\npredicting the bridge design type. Due to state-to-state variations in material\navailability, material costs, and design codes, supervised learning models\nbased on the complete data set did not yield favorable results. Supervised\nlearning models were then trained and tested using 10-fold cross validation on\ndata for each state. Inclusion of seismic data improved the model performance\nnoticeably. The data was then resampled to reduce the bias of the models\ntowards more common design types, and the supervised learning models thus\nconstructed showed further improvements in performance. The average recall and\nprecision for the state models was 88.6% and 88.0% using Decision Trees, 84.0%\nand 83.7% using Bayesian Networks, and 80.8% and 75.6% using SVM.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 16:33:30 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Jootoo", "Achyuthan", ""], ["Lattanzi", "David", ""]]}, {"id": "1803.04481", "submitter": "David Kohn", "authors": "David Kohn, Nick Glozier, Ian B. Hickie, Hugh Durrant-Whyte, Sally\n  Cripps", "title": "Irreproducibility; Nothing is More Predictable", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing ease of data capture and storage has led to a corresponding\nincrease in the choice of data, the type of analysis performed on that data,\nand the complexity of the analysis performed. The main contribution of this\npaper is to show that the subjective choice of data and analysis methodology\nsubstantially impacts the identification of factors and outcomes of\nobservational studies. This subjective variability of inference is at the heart\nof recent discussions around irreproducibility in scientific research. To\ndemonstrate this subjective variability, data is taken from an existing study,\nwhere interest centres on understanding the factors associated with a young\nadult's propensity to fall into the category of `not in employment, education\nor training' (NEET). A fully probabilistic analysis is performed, set in a\nBayesian framework and implemented using Reversible Jump Markov chain Monte\nCarlo (RJMCMC). The results show that different techniques lead to different\ninference but that models consisting of different factors often have the same\npredictive performance, whether the analysis is frequentist or Bayesian, making\ninference problematic. We demonstrate how the use of prior distributions in\nBayesian techniques can be used to as a tool for assessing a factor's\nimportance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:35:42 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Kohn", "David", ""], ["Glozier", "Nick", ""], ["Hickie", "Ian B.", ""], ["Durrant-Whyte", "Hugh", ""], ["Cripps", "Sally", ""]]}, {"id": "1803.04558", "submitter": "German A. Schnaidt Grez", "authors": "German A. Schnaidt Grez and Brani Vidakovic", "title": "Empirical Wavelet-based Estimation for Non-linear Additive Regression\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression models are actively researched in the statistical field\nbecause of their usefulness in the analysis of responses determined by\nnon-linear relationships with multivariate predictors. In this kind of\nstatistical models, the response depends linearly on unknown functions of\npredictor variables and typically, the goal of the analysis is to make\ninference about these functions.\n  In this paper, we consider the problem of Additive Regression with random\ndesigns from a novel viewpoint: we propose an estimator based on an orthogonal\nprojection onto a multiresolution space using empirical wavelet coefficients\nthat are fully data driven. In this setting, we derive a mean-square consistent\nestimator based on periodic wavelets on the interval $[0,1]$. For construction\nof the estimator, we assume that the joint distribution of predictors is\nnon-zero and bounded on its support; We also assume that the functions belong\nto a Sobolev space and integrate to zero over the $[0,1]$ interval, which\nguarantees model identifiability and convergence of the proposed method.\nMoreover, we provide the $\\mathbb{L}_{2}$ risk analysis of the estimator and\nderive its convergence rate.\n  Theoretically, we show that this approach achieves good convergence rates\nwhen the dimensionality of the problem is relatively low and the set of unknown\nfunctions is sufficiently smooth. In this approach, the results are obtained\nwithout the assumption of an equispaced design, a condition that is typically\nassumed in most wavelet-based procedures.\n  Finally, we show practical results obtained from simulated data,\ndemonstrating the potential applicability of our method in the problem of\nadditive regression models with random designs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:33:27 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Grez", "German A. Schnaidt", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1803.04640", "submitter": "Jie Hu", "authors": "Wei Liang, Yuxiao Yang, Yusi Fang, Zhongying Zhao, Jie Hu", "title": "Bayesian Detection of Abnormal ADS in Mutant Caenorhabditis elegans\n  Embryos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell division timing is critical for cell fate specification and\nmorphogenesis during embryogenesis. How division timings are regulated among\ncells during development is poorly understood. Here we focus on the comparison\nof asynchrony of division between sister cells (ADS) between wild-type and\nmutant individuals of Caenorhabditis elegans. Since the replicate number of\nmutant individuals of each mutated gene, usually one, is far smaller than that\nof wild-type, direct comparison of two distributions of ADS between wild-type\nand mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the\nother hand, we find that sometimes ADS is correlated with the life span of\ncorresponding mother cell in wild-type. Hence, we apply a semiparametric\nBayesian quantile regression method to estimate the 95% confidence interval\ncurve of ADS with respect to life span of mother cell of wild-type individuals.\nThen, mutant-type ADSs outside the corresponding confidence interval are\nselected out as abnormal one with a significance level of 0.05. Simulation\nstudy demonstrates the accuracy of our method and Gene Enrichment Analysis\nvalidates the results of real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 06:08:19 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Liang", "Wei", ""], ["Yang", "Yuxiao", ""], ["Fang", "Yusi", ""], ["Zhao", "Zhongying", ""], ["Hu", "Jie", ""]]}, {"id": "1803.04669", "submitter": "Faranak Golestaneh", "authors": "Faranak Golestaneh, Pierre Pinson and Hoay Beng Gooi", "title": "Polyhedral Predictive Regions For Power System Applications", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/TPWRS.2018.2861705", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial improvement in the development of forecasting approaches,\nconditional and dynamic uncertainty estimates ought to be accommodated in\ndecision-making in power system operation and market, in order to yield either\ncost-optimal decisions in expectation, or decision with probabilistic\nguarantees. The representation of uncertainty serves as an interface between\nforecasting and decision-making problems, with different approaches handling\nvarious objects and their parameterization as input. Following substantial\ndevelopments based on scenario-based stochastic methods, robust and\nchance-constrained optimization approaches have gained increasing attention.\nThese often rely on polyhedra as a representation of the convex envelope of\nuncertainty. In the work, we aim to bridge the gap between the probabilistic\nforecasting literature and such optimization approaches by generating forecasts\nin the form of polyhedra with probabilistic guarantees. For that, we see\npolyhedra as parameterized objects under alternative definitions (under $L_1$\nand $L_\\infty$ norms), the parameters of which may be modelled and predicted.\nWe additionally discuss assessing the predictive skill of such multivariate\nprobabilistic forecasts. An application and related empirical investigation\nresults allow us to verify probabilistic calibration and predictive skills of\nour polyhedra.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 08:01:44 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Golestaneh", "Faranak", ""], ["Pinson", "Pierre", ""], ["Gooi", "Hoay Beng", ""]]}, {"id": "1803.04676", "submitter": "Faranak Golestaneh", "authors": "Faranak Golestaneh and Hoay Beng Gooi", "title": "Multivariate Prediction Intervals for Photovoltaic Power Generation", "comments": "5 pages, ISGT Asia Conference, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current literature in probabilistic forecasting is focused on quantifying\nthe uncertainty of each random variable individually. This leads to the failure\nin informing about interdependence structure of uncertainty at different\nlocations and/or different lead times. When there is a positive or negative\nassociation between a number of random variables, the prediction regions for\nthem should be reflected by multivariate or joint uncertainty sets. The\nexisting literature is very primitive in the area of multivariate uncertainty\nsets modeling. In this paper, uncertainty regions are generated in the form of\nmultivariate prediction intervals. We will examine the performance of Gaussian\nand R-Vine copulas in characterizing the correlated behavior of PV power\ngenerations at successive lead-times. Copulas are compared based on\ngoodness-of-fit metrics as well as skill scores. A framework is elaborated to\ngenerate multivariate prediction intervals out of the scenarios generated from\nGaussian and R-vine multivariate densities. The resultant multivariate\nprediction intervals are evaluated based on their calibration and sharpness.\nThe approaches are tested on a real-world dataset including PV power\nmeasurements and weather forecasts. This paper provides a series of useful\nanalyses and comparative results for multivariate uncertainty modeling of PV\npower that can serve as a basis for future works in the area.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 08:15:14 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Golestaneh", "Faranak", ""], ["Gooi", "Hoay Beng", ""]]}, {"id": "1803.04799", "submitter": "Gregory S. Warrington", "authors": "Gregory S. Warrington", "title": "Introduction to the declination function for gerrymanders", "comments": "8 pages, 11 figures, python code, introduction to work in\n  arXiv:1705.09393", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The declination is a quantitative method for identifying possible partisan\ngerrymanders by analyzing vote distributions. In this expository note we\nexplain and motivate the definition of the declination. The minimal computer\ncode required for computing the declination is included. We end by computing\nits value on several recent elections.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 13:42:35 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Warrington", "Gregory S.", ""]]}, {"id": "1803.04853", "submitter": "Vivien Goepp", "authors": "Vivien Goepp (MAP5 - UMR 8145, UPD5, UPD5 Math\\'ematiques\n  Informatique), Jean-Christophe Thalabard (MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\\'edecine), Gr\\'egory Nuel (LPMA), Olivier Bouaziz (MAP5 - UMR 8145, UPD5,\n  UPD5 Math\\'ematiques Informatique, IUT - Paris Descartes)", "title": "Regularized Bidimensional Estimation of the Hazard Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological or demographic studies, with variable age at onset, a\ntypical quantity of interest is the incidence of a disease (for example the\ncancer incidence). In these studies, the individuals are usually highly\nheterogeneous in terms of dates of birth (the cohort) and with respect to the\ncalendar time (the period) and appropriate estimation methods are needed. In\nthis article a new estimation method is presented which extends classical\nage-period-cohort analysis by allowing interactions between age, period and\ncohort effects. This paper introduces a bidimensional regularized estimate of\nthe hazard rate where a penalty is introduced on the likelihood of the model.\nThis penalty can be designed either to smooth the hazard rate or to enforce\nconsecutive values of the hazard to be equal, leading to a parsimonious\nrepresentation of the hazard rate. In the latter case, we make use of an\niterative penalized likelihood scheme to approximate the L0 norm, which makes\nthe computation tractable. The method is evaluated on simulated data and\napplied on breast cancer survival data from the SEER program.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:49:54 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 16:53:17 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 11:58:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Goepp", "Vivien", "", "MAP5 - UMR 8145, UPD5, UPD5 Math\u00e9matiques\n  Informatique"], ["Thalabard", "Jean-Christophe", "", "MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\u00e9decine"], ["Nuel", "Gr\u00e9gory", "", "LPMA"], ["Bouaziz", "Olivier", "", "MAP5 - UMR 8145, UPD5,\n  UPD5 Math\u00e9matiques Informatique, IUT - Paris Descartes"]]}, {"id": "1803.05074", "submitter": "Behram Wali", "authors": "Behram Wali, Asad J. Khattak, Jim Waters, Deo Chimba, Xiaobing Li", "title": "Development of Safety Performance Functions: Incorporating Unobserved\n  Heterogeneity and Functional Form Analysis", "comments": "Accepted for Publication in Transportation Research Record: Journal\n  of the Transportation Research Board", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve transportation safety, this study applies Highway Safety Manual\n(HSM) procedures to roadways while accounting for unobserved heterogeneity and\nexploring alternative functional forms for Safety Performance Functions (SPFs).\nSpecifically, several functional forms are considered in Poisson and\nPoisson-gamma modeling frameworks. Using five years (2011-2015) of crash,\ntraffic, and road inventory data for two-way, two-lane roads in Tennessee,\nfixed- and random-parameter count data models are calibrated. The models\naccount for important methodological concerns of unobserved heterogeneity and\nomitted variable bias. With a validation dataset, the calibrated and\nuncalibrated HSM SPFs and eight new Tennessee-specific SPFs are compared for\nprediction accuracy. The results show that the statewide calibration factor is\n2.48, suggesting rural two-lane, two-way road segment crashes are at least 1.48\ntimes greater than what HSM SPF predicts. Significant variation in four\ndifferent regions in Tennessee is observed with calibration factors ranging\nbetween 2.02 and 2.77. Among all the SPFs considered, fully specified\nTennessee-specific random parameter Poisson SPF outperformed all competing SPFs\nin predicting out-of-sample crashes on these road segments. The best-fit random\nparameter SPF specification for crash frequency includes the following\nvariables: annual average daily traffic, segment length, shoulder width, lane\nwidth, speed limit, and the presence of passing lanes. Significant\nheterogeneity is observed in the effects of traffic exposure-related variables\non crash frequency. The study shows how heterogeneity-based models can be\nspecified and used by practitioners for obtaining accurate crash predictions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 23:31:48 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad J.", ""], ["Waters", "Jim", ""], ["Chimba", "Deo", ""], ["Li", "Xiaobing", ""]]}, {"id": "1803.05127", "submitter": "Xinzhi Han", "authors": "Xinzhi Han, Sen Lei", "title": "Feature Selection and Model Comparison on Microsoft Learning-to-Rank\n  Data Sets", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid advance of the Internet, search engines (e.g., Google, Bing,\nYahoo!) are used by billions of users for each day. The main function of a\nsearch engine is to locate the most relevant webpages corresponding to what the\nuser requests. This report focuses on the core problem of information\nretrieval: how to learn the relevance between a document (very often webpage)\nand a query given by user. Our analysis consists of two parts: 1) we use\nstandard statistical methods to select important features among 137 candidates\ngiven by information retrieval researchers from Microsoft. We find that not all\nthe features are useful, and give interpretations on the top-selected features;\n2) we give baselines on prediction over the real-world dataset MSLR-WEB by\nusing various learning algorithms. We find that models of boosting trees,\nrandom forest in general achieve the best performance of prediction. This\nagrees with the mainstream opinion in information retrieval community that\ntree-based algorithms outperform the other candidates for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 03:57:55 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Han", "Xinzhi", ""], ["Lei", "Sen", ""]]}, {"id": "1803.05297", "submitter": "Philip Gerrish", "authors": "Philip Gerrish, Benjamin Zepeda, Theophilus Okosun, Irene A Gerrish,\n  and Rosemary Joyce", "title": "A quantitative analysis of the 2017 Honduran election and the argument\n  used to defend its outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Honduran incumbent president and his administration recently declared\nvictory in an election riddled with irregularities and indicators of fraud.\nPerhaps most curious, however, was a numerical anomaly: the primary challenger\ncarried a very significant lead of five percentage points more than half way\nthrough the election but was ultimately defeated by the incumbent. The\nincumbent (Hernandez) offered a plausible explanation for the surprising\nturnaround in the ballots: his popularity is greater in remote areas of the\ncountry but votes from remote areas were not counted until later in the\nelection. Here, we mathematically formalize this argument, which we will call\nthe Hernandez conjecture, and employ the resulting formulae together with\ngeodemographic data from Honduras to quantitatively assess the conjectures\nveracity. When the departamentos were analyzed individually, three\nsparsely-populated departamentos (of 18 total) showed small but non-negligible\nprobability of the conjectures veracity; however, when the country was analyzed\nas a whole, the overall probability of the conjectures veracity was calculated\nto be less than 0.0001 under a wide range of different assumptions. Results of\nour three-pronged analysis, taken together, indicate a negligible probability\nof a fair win by the incumbent.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 14:18:13 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 12:50:54 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Gerrish", "Philip", ""], ["Zepeda", "Benjamin", ""], ["Okosun", "Theophilus", ""], ["Gerrish", "Irene A", ""], ["Joyce", "Rosemary", ""]]}, {"id": "1803.05398", "submitter": "Nikolay Vitanov k", "authors": "Nikolay K. Vitanov, Roumen Borissov", "title": "Model of a motion of substance in a channel of a network consisting of\n  two arms", "comments": "14 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1709.01833", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of the motion of substance in a channel of a network for\nthe case of channel having two arms. Stationary regime of the flow of the\nsubstance is considered. Analytical relationships for the distribution of the\nsubstance in the nodes of the arms of the channel are obtained. The obtained\nresults are discussed from the point of view of technological applications of\nthe model (e.g., motion of substances such as water in complex technological\nfacilities).\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:53:12 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Vitanov", "Nikolay K.", ""], ["Borissov", "Roumen", ""]]}, {"id": "1803.05484", "submitter": "I-Sheng Yang", "authors": "I-Sheng Yang", "title": "Removing Skill Bias from Gaming Statistics", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"The chance to win given a certain move\" is an easily obtainable quantity\nfrom data and often quoted in gaming statistics. It is also the fundamental\nquantity that reinforcement learning AI bases on. Unfortunately, this\nconditional probability can be misleading. Unless all players are equally\nskilled, this number does not tell us the intrinsic value of such move. That is\nbecause conditioning on one good move also inevitably selects a subset of\nbetter players. They tend to make other good moves, which also contribute to\nthe extra winning chance. We present a simple toy model to quantify this \"skill\nbias\" effect, and then propose a general method to remove it. Our method is\nmodular, generalizable, and also only requires easily obtainable quantities\nfrom data. In particular, it gets the same answer independent of whether the\ndata comes from a group of good or bad players. This may help us to eventually\nbreak free from the conventional wisdom of \"learning from the experts\" and\navoid the Group Thinking pitfall.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 19:21:41 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Yang", "I-Sheng", ""]]}, {"id": "1803.05659", "submitter": "Daniele Di Gennaro", "authors": "Marusca De Castris, Daniele Di Gennaro", "title": "Does agricultural subsidies foster Italian southern farms? A Spatial\n  Quantile Regression Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, public policies become a central pillar in\nsupporting and stabilising agricultural sector. In 1962, EU policy-makers\ndeveloped the so-called Common Agricultural Policy (CAP) to ensure\ncompetitiveness and a common market organisation for agricultural products,\nwhile 2003 reform decouple the CAP from the production to focus only on income\nstabilization and the sustainability of agricultural sector. Notwithstanding\nfarmers are highly dependent to public support, literature on the role played\nby the CAP in fostering agricultural performances is still scarce and\nfragmented. Actual CAP policies increases performance differentials between\nNorthern Central EU countries and peripheral regions. This paper aims to\nevaluate the effectiveness of CAP in stimulate performances by focusing on\nItalian lagged Regions. Moreover, agricultural sector is deeply rooted in\nplace-based production processes. In this sense, economic analysis which omit\nthe presence of spatial dependence produce biased estimates of the\nperformances. Therefore, this paper, using data on subsidies and economic\nresults of farms from the RICA dataset which is part of the Farm Accountancy\nData Network (FADN), proposes a spatial Augmented Cobb-Douglas Production\nFunction to evaluate the effects of subsidies on farm's performances. The major\ninnovation in this paper is the implementation of a micro-founded quantile\nversion of a spatial lag model to examine how the impact of the subsidies may\nvary across the conditional distribution of agricultural performances. Results\nshow an increasing shape which switch from negative to positive at the median\nand becomes statistical significant for higher quantiles. Additionally, spatial\nautocorrelation parameter is positive and significant across all the\nconditional distribution, suggesting the presence of significant spatial\nspillovers in agricultural performances.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:32:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["De Castris", "Marusca", ""], ["Di Gennaro", "Daniele", ""]]}, {"id": "1803.05664", "submitter": "David R\\\"ugamer", "authors": "Benjamin S\\\"afken, David R\\\"ugamer, Thomas Kneib and Sonja Greven", "title": "Conditional Model Selection in Mixed-Effects Models with cAIC4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection in mixed models based on the conditional distribution is\nappropriate for many practical applications and has been a focus of recent\nstatistical research. In this paper we introduce the R-package cAIC4 that\nallows for the computation of the conditional Akaike Information Criterion\n(cAIC). Computation of the conditional AIC needs to take into account the\nuncertainty of the random effects variance and is therefore not\nstraightforward. We introduce a fast and stable implementation for the\ncalculation of the cAIC for linear mixed models estimated with lme4 and\nadditive mixed models estimated with gamm4 . Furthermore, cAIC4 offers a\nstepwise function that allows for a fully automated stepwise selection scheme\nfor mixed models based on the conditional AIC. Examples of many possible\napplications are presented to illustrate the practical impact and easy handling\nof the package.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:57:21 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 15:21:02 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["S\u00e4fken", "Benjamin", ""], ["R\u00fcgamer", "David", ""], ["Kneib", "Thomas", ""], ["Greven", "Sonja", ""]]}, {"id": "1803.05673", "submitter": "Marius \\\"Otting", "authors": "Marius \\\"Otting, Roland Langrock, Christian Deutscher and Vianey\n  Leos-Barajas", "title": "The Hot Hand in Professional Darts", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12527", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hot hand hypothesis in professional darts in a near-ideal\nsetting with minimal to no interaction between players. Considering almost one\nyear of tournament data, corresponding to 167,492 dart throws in total, we use\nstate-space models to investigate serial dependence in throwing performance. In\nour models, a latent state process serves as a proxy for a player's underlying\nability, and we use autoregressive processes to model how this process evolves\nover time. We find a strong but short-lived serial dependence in the latent\nstate process, thus providing evidence for the existence of the hot hand.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 10:18:57 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 13:20:17 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["\u00d6tting", "Marius", ""], ["Langrock", "Roland", ""], ["Deutscher", "Christian", ""], ["Leos-Barajas", "Vianey", ""]]}, {"id": "1803.05757", "submitter": "Jessica Barrett", "authors": "Jessica K. Barrett, Raphael Huille, Richard Parker, Yuichiro Yano and\n  Michael Griswold", "title": "Estimating the association between blood pressure variability and\n  cardiovascular disease: An application using the ARIC Study", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.1002/sim.8074", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The association between visit-to-visit systolic blood pressure variability\nand cardiovascular events has recently received a lot of attention in the\ncardiovascular literature. But blood pressure variability is usually estimated\non a person-by-person basis, and is therefore subject to considerable\nmeasurement error. We demonstrate that hazard ratios estimated using this\napproach are subject to bias due to regression dilution and we propose\nalternative methods to reduce this bias: a two-stage method and a joint model.\nFor the two-stage method, in stage one repeated measurements are modelled using\na mixed effects model with a random component on the residual standard\ndeviation. The mixed effects model is used to estimate the blood pressure\nstandard deviation for each individual, which in stage two is used as a\ncovariate in a time-to-event model. For the joint model, the mixed effects\nsub-model and time-to-event sub-model are fitted simultaneously using shared\nrandom effects. We illustrate the methods using data from the Atherosclerosis\nRisk in Communities (ARIC) study.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:09:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 21:18:34 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Barrett", "Jessica K.", ""], ["Huille", "Raphael", ""], ["Parker", "Richard", ""], ["Yano", "Yuichiro", ""], ["Griswold", "Michael", ""]]}, {"id": "1803.05835", "submitter": "Francesco Finazzi", "authors": "Francesco Finazzi, Alessandro Fass\\`o, Fabio Madonna, Ilia Negri,\n  Bomin Sun and Marco Rosoldi", "title": "Statistical harmonization and uncertainty assessment in the comparison\n  of satellite and radiosonde climate variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite product validation is key to ensure the delivery of quality\nproducts for climate and weather applications. To do this, a fundamental step\nis the comparison with other instruments, such us radiosonde. This is specially\ntrue for Essential Climate Variables such as temperature and humidity. Thanks\nto a functional data representation, this paper uses a likelihood based\napproach which exploits the measurement uncertainties in a natural way. In\nparticular the comparison of temperature and humdity radiosonde measurements\ncollected within RAOB network and the corresponding atmospheric profiles\nderived from IASI interferometers aboard of Metop-A and Metop-B satellites is\ndeveloped with the aim of understanding the vertical smoothing mismatch\nuncertainty. Moreover, conventional RAOB functional data representation is\nassessed by means of a comparison with radiosonde reference measurements given\nby GRUAN network, which provides high resolution fully traceable radiosouding\nprofiles. In this way the uncertainty related to coarse vertical resolution, or\nsparseness, of conventional RAOB is assessed. It has been found that the\nuncertainty of vertical smoothing mismatch averaged along the profile is 0.50 K\nfor temperature and 0.16 g/kg for water vapour mixing ratio. Moreover the\nuncertainty related to RAOB sparseness, averaged along the profile is 0.29 K\nfor temperature and 0.13 g/kg for water vapour mixing ratio.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:14:52 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Finazzi", "Francesco", ""], ["Fass\u00f2", "Alessandro", ""], ["Madonna", "Fabio", ""], ["Negri", "Ilia", ""], ["Sun", "Bomin", ""], ["Rosoldi", "Marco", ""]]}, {"id": "1803.05874", "submitter": "Jingchen Hu", "authors": "Joerg Drechsler, Jingchen Hu", "title": "Synthesizing geocodes to facilitate access to detailed geographical\n  information in large scale administrative data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether generating synthetic data can be a viable strategy for\nproviding access to detailed geocoding information for external researchers,\nwithout compromising the confidentiality of the units included in the database.\nOur work was motivated by a recent project at the Institute for Employment\nResearch (IAB) in Germany that linked exact geocodes to the Integrated\nEmployment Biographies (IEB), a large administrative database containing\nseveral million records. We evaluate the performance of three synthesizers\nregarding the trade-off between preserving analytical validity and limiting\ndisclosure risks: One synthesizer employs Dirichlet Process mixtures of\nproducts of multinomials (DPMPM), while the other two use different versions of\nClassification and Regression Trees (CART). In terms of preserving analytical\nvalidity, our proposed synthesis strategy for geocodes based on categorical\nCART models outperforms the other two. If the risks of the synthetic data\ngenerated by the categorical CART synthesizer are deemed too high, we\ndemonstrate that synthesizing additional variables is the preferred strategy to\naddress the risk-utility trade-off in practice, compared to limiting the size\nof the regression trees or relying on the strategy of providing geographical\ninformation only on an aggregated level. We also propose strategies for making\nthe synthesizers scalable for large files, present analytical validity measures\nand disclosure risk measures for the generated data, and provide general\nrecommendations for statistical agencies considering the synthetic data\napproach for disseminating detailed geographical information.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:21:25 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:30:52 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 20:51:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Drechsler", "Joerg", ""], ["Hu", "Jingchen", ""]]}, {"id": "1803.06053", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Paul Gustafson", "title": "The world of research has gone berserk: modeling the consequences of\n  requiring \"greater statistical stringency\" for scientific publication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to growing concern about the reliability and reproducibility of\npublished science, researchers have proposed adopting measures of greater\nstatistical stringency, including suggestions to require larger sample sizes\nand to lower the highly criticized p<0.05 significance threshold. While pros\nand cons are vigorously debated, there has been little to no modeling of how\nadopting these measures might affect what type of science is published. In this\npaper, we develop a novel optimality model that, given current incentives to\npublish, predicts a researcher's most rational use of resources in terms of the\nnumber of studies to undertake, the statistical power to devote to each study,\nand the desirable pre-study odds to pursue. We then develop a methodology that\nallows one to estimate the reliability of published research by considering a\ndistribution of preferred research strategies. Using this approach, we\ninvestigate the merits of adopting measures of `greater statistical stringency'\nwith the goal of informing the ongoing debate.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:43:58 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 00:07:12 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Campbell", "Harlan", ""], ["Gustafson", "Paul", ""]]}, {"id": "1803.06109", "submitter": "Jesper Martinsson", "authors": "Jesper Martinsson and Silje Gustafsson", "title": "Modeling the effects of telephone nursing on healthcare utilization", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijmedinf.2018.02.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Telephone nursing is the first line of contact for many\ncare-seekers and aims at optimizing the performance of the healthcare system by\nsupporting and guiding patients to the correct level of care and reduce the\namount of unscheduled visits. Good statistical models that describe the effects\nof telephone nursing are important in order to study its impact on healthcare\nresources and evaluate changes in telephone nursing procedures.\n  Objective: To develop a valid model that captures the complex relationships\nbetween the nurse's recommendations, the patients' intended actions and the\npatients' health seeking behavior. Using the model to estimate the effects of\ntelephone nursing on patient behavior, healthcare utilization, and infer\npotential cost savings.\n  Methods: Bayesian ordinal regression modeling of data from randomly selected\npatients that received telephone nursing. Inference is based on Markov Chain\nMonte Carlo methods, model selection using the Watanabe-Akaike Information\nCriteria, and model validation using posterior predictive checks on standard\ndiscrepancy measures.\n  Results and Conclusions: We present a robust Bayesian ordinal regression\nmodel that predicts 76% of the patients' healthcare utilization after telephone\nnursing and we found no evidence of model deficiencies. The model reveals a\nrisk reducing behavior and the effect of the telephone nursing recommendation\nis 7 times higher than the effect of the patient's intended action prior to\nconsultation if the recommendation is the highest level of care. But the effect\nof the nurse's recommendation is lower, or even non-existing, if the\nrecommendation is self-care. Telephone nursing was found to have a constricting\neffect on healthcare utilization, however, the compliance to nurse's\nrecommendation is closely tied to perceptions of risk, emphasizing the\nimportance to address caller's needs of reassurance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 08:40:46 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Martinsson", "Jesper", ""], ["Gustafsson", "Silje", ""]]}, {"id": "1803.06186", "submitter": "Lionel Hertzog", "authors": "Lionel R. Hertzog", "title": "How robust are Structural Equation Models to model miss-specification? A\n  simulation study", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structural Equation Models (SEMs) are routinely used in the analysis of\nempirical data by researchers from different scientific fields such as\npsychologists or economists. In some fields, such as in ecology, SEMs have only\nstarted recently to attract attention and thanks to dedicated software packages\nthe use of SEMs has steadily increased. Yet, common analysis practices in such\nfields that might be transposed from other statistical techniques such as model\nacceptance or rejection based on p-value screening might be poorly fitted for\nSEMs especially when these models are used to confirm or reject hypotheses.\n  In this simulation study, SEMs were fitted via two commonly used R packages:\nlavaan and piecewiseSEM. Five different data-generation scenarios were\nexplored: (i) random, (ii) exact, (iii) shuffled, (iv) underspecified and (v)\noverspecified. In addition, sample size and model complexity were also varied\nto explore their impact on various global and local model fitness indices.\n  The results showed that not one single model index should be used to decide\non model fitness but rather a combination of different model fitness indices is\nneeded. The global chi-square test for lavaan or the Fisher's C statistic for\npiecewiseSEM were, in isolation, poor indicators of model fitness. In addition,\nthe simulations showed that to achieve sufficient power to detect individual\neffects, adequate sample sizes are required. Finally, BIC showed good capacity\nto select models closer to the truth especially for more complex models.\n  I provide, based on these results, a flowchart indicating how information\nfrom different metrics may be combined to reveal model strength and weaknesses.\nResearchers in scientific fields with little experience in SEMs, such as in\necology, should consider and accept these limitations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:17:35 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:10:11 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 13:33:35 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Hertzog", "Lionel R.", ""]]}, {"id": "1803.06206", "submitter": "Yili Hong", "authors": "Yili Hong and Man Zhang and William Q. Meeker", "title": "Big Data and Reliability Applications: The Complexity Dimension", "comments": "28 pages, 7 figures", "journal-ref": "Journal of Quality Technology, 2018", "doi": "10.1080/00224065.2018.1438007", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data features not only large volumes of data but also data with\ncomplicated structures. Complexity imposes unique challenges in big data\nanalytics. Meeker and Hong (2014, Quality Engineering, pp. 102-116) provided an\nextensive discussion of the opportunities and challenges in big data and\nreliability, and described engineering systems that can generate big data that\ncan be used in reliability analysis. Meeker and Hong (2014) focused on large\nscale system operating and environment data (i.e., high-frequency multivariate\ntime series data), and provided examples on how to link such data as covariates\nto traditional reliability responses such as time to failure, time to\nrecurrence of events, and degradation measurements. This paper intends to\nextend that discussion by focusing on how to use data with complicated\nstructures to do reliability analysis. Such data types include high-dimensional\nsensor data, functional curve data, and image streams. We first provide a\nreview of recent development in those directions, and then we provide a\ndiscussion on how analytical methods can be developed to tackle the challenging\naspects that arise from the complexity feature of big data in reliability\napplications. The use of modern statistical methods such as variable selection,\nfunctional data analysis, scalar-on-image regression, spatio-temporal data\nmodels, and machine learning techniques will also be discussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 13:08:25 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Hong", "Yili", ""], ["Zhang", "Man", ""], ["Meeker", "William Q.", ""]]}, {"id": "1803.06249", "submitter": "Yi Yu", "authors": "Haeran Cho and Yi Yu", "title": "Link prediction for interdisciplinary collaboration via co-authorship\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the Publication and Research (PURE) data set of University of\nBristol collected between $2008$ and $2013$. Using the existing co-authorship\nnetwork and academic information thereof, we propose a new link prediction\nmethodology, with the specific aim of identifying potential interdisciplinary\ncollaboration in a university-wide collaboration network.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:34:53 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Cho", "Haeran", ""], ["Yu", "Yi", ""]]}, {"id": "1803.06258", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu, Benjamin Paul Chamberlain", "title": "Online Controlled Experiments for Personalised e-Commerce Strategies:\n  Design, Challenges, and Pitfalls", "comments": "Not peer-reviewed but retained for historic interest. Removed an\n  erroneous statement on Welch's t-test assumptions in Section 3.2. 9 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online controlled experiments are the primary tool for measuring the causal\nimpact of product changes in digital businesses. It is increasingly common for\ndigital products and services to interact with customers in a personalised way.\nUsing online controlled experiments to optimise personalised interaction\nstrategies is challenging because the usual assumption of statistically\nequivalent user groups is violated. Additionally, challenges are introduced by\nusers qualifying for strategies based on dynamic, stochastic attributes.\nTraditional A/B tests can salvage statistical equivalence by pre-allocating\nusers to control and exposed groups, but this dilutes the experimental metrics\nand reduces the test power. We present a stacked incrementality test framework\nthat addresses problems with running online experiments for personalised user\nstrategies. We derive bounds that show that our framework is superior to the\nbest simple A/B test given enough users and that this condition is easily met\nfor large scale online experiments. In addition, we provide a test power\ncalculator and describe a selection of pitfalls and lessons learnt from our\nexperience using it.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:59:06 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 14:09:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1803.06287", "submitter": "Ranjan Maitra", "authors": "Karl T. Pazdernik and Ranjan Maitra and Douglas Nychka and Stephen\n  Sain", "title": "Reduced Basis Kriging for Big Spatial Fields", "comments": "Sankhya, Series A, accepted for publication", "journal-ref": "Sankhya, Series A, 80:2:280--300, 2018", "doi": "10.1007/s13171-018-0129-7", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, a common method for prediction over a Gaussian random\nfield (GRF) is maximum likelihood estimation combined with kriging. For massive\ndata sets, kriging is computationally intensive, both in terms of CPU time and\nmemory, and so fixed rank kriging has been proposed as a solution. The method\nhowever still involves operations on large matrices, so we develop an\nalteration to this method by utilizing the approximations made in fixed rank\nkriging combined with restricted maximum likelihood estimation and sparse\nmatrix methodology. Experiments show that our methodology can provide\nadditional gains in computational efficiency over fixed-rank kriging without\nloss of accuracy in prediction. The methodology is applied to climate data\narchived by the United States National Climate Data Center, with very good\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 03:37:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 12:11:14 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pazdernik", "Karl T.", ""], ["Maitra", "Ranjan", ""], ["Nychka", "Douglas", ""], ["Sain", "Stephen", ""]]}, {"id": "1803.06304", "submitter": "Hoang Vuong", "authors": "Quan-Hoang Vuong, Manh-Tung Ho, Viet-Phuong La, Dam Van Nhue, Bui\n  Quang Khiem, Nghiem Phu Kien Cuong, Thu-Trang Vuong, Manh-Toan Ho, Hong-Kong\n  T. Nguyen, Viet-Ha Nguyen, Hiep-Hung Pham, Nancy K. Napier", "title": "\"Cultural additivity\" and how the values and norms of Confucianism,\n  Buddhism, and Taoism co-exist, interact, and influence Vietnamese society: A\n  Bayesian analysis of long-standing folktales, using R and Stan", "comments": "8 figures, 35 pages", "journal-ref": null, "doi": null, "report-no": "WUH-ISR 1801", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every year, the Vietnamese people reportedly burned about 50,000 tons of joss\npapers, which took the form of not only bank notes, but iPhones, cars, clothes,\neven housekeepers, in hope of pleasing the dead. The practice was mistakenly\nattributed to traditional Buddhist teachings but originated in fact from China,\nwhich most Vietnamese were not aware of. In other aspects of life, there were\nmany similar examples of Vietnamese so ready and comfortable with adding new\nnorms, values, and beliefs, even contradictory ones, to their culture. This\nphenomenon, dubbed \"cultural additivity\", prompted us to study the\nco-existence, interaction, and influences among core values and norms of the\nThree Teachings--Confucianism, Buddhism, and Taoism--as shown through\nVietnamese folktales. By applying Bayesian logistic regression, we evaluated\nthe possibility of whether the key message of a story was dominated by a\nreligion (dependent variables), as affected by the appearance of values and\nanti-values pertaining to the Three Teachings in the story (independent\nvariables).\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 08:57:05 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Vuong", "Quan-Hoang", ""], ["Ho", "Manh-Tung", ""], ["La", "Viet-Phuong", ""], ["Van Nhue", "Dam", ""], ["Khiem", "Bui Quang", ""], ["Cuong", "Nghiem Phu Kien", ""], ["Vuong", "Thu-Trang", ""], ["Ho", "Manh-Toan", ""], ["Nguyen", "Hong-Kong T.", ""], ["Nguyen", "Viet-Ha", ""], ["Pham", "Hiep-Hung", ""], ["Napier", "Nancy K.", ""]]}, {"id": "1803.06336", "submitter": "Alex Deng", "authors": "Alex Deng, Ulf Knoblich, Jiannan Lu", "title": "Applying the Delta method in metric analytics: A practical guide with\n  novel ideas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, the information technology industry has adopted a\ndata-driven culture, relying on online metrics to measure and monitor business\nperformance. Under the setting of big data, the majority of such metrics\napproximately follow normal distributions, opening up potential opportunities\nto model them directly without extra model assumptions and solve big data\nproblems via closed-form formulas using distributed algorithms at a fraction of\nthe cost of simulation-based procedures like bootstrap. However, certain\nattributes of the metrics, such as their corresponding data generating\nprocesses and aggregation levels, pose numerous challenges for constructing\ntrustworthy estimation and inference procedures. Motivated by four real-life\nexamples in metric development and analytics for large-scale A/B testing, we\nprovide a practical guide to applying the Delta method, one of the most\nimportant tools from the classic statistics literature, to address the\naforementioned challenges. We emphasize the central role of the Delta method in\nmetric analytics by highlighting both its classic and novel applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:38:51 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 17:15:30 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 16:46:12 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 18:02:01 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Deng", "Alex", ""], ["Knoblich", "Ulf", ""], ["Lu", "Jiannan", ""]]}, {"id": "1803.06344", "submitter": "Andr\\'e Gensler", "authors": "Andr\\'e Gensler, Bernhard Sick", "title": "A Multi-Scheme Ensemble Using Coopetitive Soft-Gating With Application\n  to Power Forecasting for Renewable Energy Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel ensemble technique with a multi-scheme\nweighting based on a technique called coopetitive soft gating. This technique\ncombines both, ensemble member competition and cooperation, in order to\nmaximize the overall forecasting accuracy of the ensemble. The proposed\nalgorithm combines the ideas of multiple ensemble paradigms (power forecasting\nmodel ensemble, weather forecasting model ensemble, and lagged ensemble) in a\nhierarchical structure. The technique is designed to be used in a flexible\nmanner on single and multiple weather forecasting models, and for a variety of\nlead times. We compare the technique to other power forecasting models and\nensemble techniques with a flexible number of weather forecasting models, which\ncan have the same, or varying forecasting horizons. It is shown that the model\nis able to outperform those models on a number of publicly available data sets.\nThe article closes with a discussion of properties of the proposed model which\nare relevant in its application.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:23:37 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gensler", "Andr\u00e9", ""], ["Sick", "Bernhard", ""]]}, {"id": "1803.06393", "submitter": "Li Zeng", "authors": "Li Zeng, Joshua L. Warren, Hongyu Zhao", "title": "Phylogeny-based tumor subclone identification using a Bayesian feature\n  allocation model", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor cells acquire different genetic alterations during the course of\nevolution in cancer patients. As a result of competition and selection, only a\nfew subgroups of cells with distinct genotypes survive. These subgroups of\ncells are often referred to as subclones. In recent years, many statistical and\ncomputational methods have been developed to identify tumor subclones, leading\nto biologically significant discoveries and shedding light on tumor\nprogression, metastasis, drug resistance and other processes. However, most\nexisting methods are either not able to infer the phylogenetic structure among\nsubclones, or not able to incorporate copy number variations (CNV). In this\narticle, we propose SIFA (tumor Subclone Identification by Feature Allocation),\na Bayesian model which takes into account both CNV and tumor phylogeny\nstructure to infer tumor subclones. We compare the performance of SIFA with two\nother commonly used methods using simulation studies with varying sequencing\ndepth, evolutionary tree size, and tree complexity. SIFA consistently yields\nbetter results in terms of Rand Index and cellularity estimation accuracy. The\nusefulness of SIFA is also demonstrated through its application to whole genome\nsequencing (WGS) samples from four patients in a breast cancer study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 20:39:04 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zeng", "Li", ""], ["Warren", "Joshua L.", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1803.06452", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Alexander Sidorenko, Norton Bretz, David J. Thomson", "title": "Spectral Estimation of Plasma Fluctuations II: Nonstationary Analysis of\n  ELM Spectra", "comments": "Figures missing", "journal-ref": "Physics of Plasmas, Volume 1, Issue 3, March 1994, pp.501-514", "doi": "10.1063/1.870939", "report-no": null, "categories": "physics.plasm-ph eess.AS physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several analysis methods for nonstationary fluctuations are described and\napplied to the edge localized mode (ELM) instabilities of limiter H-mode\nplasmas. The microwave scattering diagnostic observes poloidal $k_{\\theta}$\nvalues of 3.3 cm$^{-1}$, averaged over a 20 cm region at the plasma edge.A\nshort autoregressive filter enhances the nonstationary component of the plasma\nfluctuations by removing much of the background level of stationary\nfluctuations. Between ELMs, the spectrum predominantly consists of broad-banded\n300-700 kHz fluctuations propagating in the electron diamagnetic drift\ndirection, indicating the presence of a negative electric field near the plasma\nedge. The time-frequency spectrogram is computed with the multiple taper\ntechnique. By using the singular value decomposition of the spectrogram, it is\nshown that the spectrum during the ELM is broader and more symmetric than that\nof the stationary spectrum. The ELM period and the evolution of the spectrum\nbetween ELMs varies from discharge to discharge. For the discharge under\nconsideration which has distinct ELMs with a 1 msec period, the spectrum has a\nmaximum in the electron drift direction which relaxes to a near constant value\n%its characteristic shape in the first half millisecond after the end of the\nELM and then grows slowly. In contrast, the level of the fluctuations in the\nion drift direction increases exponentially by a factor of eight in the five\nmilliseconds~after the ELM. High frequency precursors are found which occur one\nmillisecond before the ELMs and propagate in the ion drift direction. These\nprecursors are very short ($\\sim 10 \\mu$secs), coherent bursts, and they\npredict the occurrence of an ELM with a high success rate.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:56:11 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 02:47:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "Alexander", ""], ["Bretz", "Norton", ""], ["Thomson", "David J.", ""]]}, {"id": "1803.06518", "submitter": "Eric Chi", "authors": "Eric C. Chi and Brian R. Gaines and Will Wei Sun and Hua Zhou and Jian\n  Yang", "title": "Provable Convex Co-clustering of Tensors", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research, 21(214):1-58, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:15:28 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:53:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chi", "Eric C.", ""], ["Gaines", "Brian R.", ""], ["Sun", "Will Wei", ""], ["Zhou", "Hua", ""], ["Yang", "Jian", ""]]}, {"id": "1803.06536", "submitter": "Yuanzhi Huang", "authors": "Yuanzhi Huang, Steven Gilmour, Kalliopi Mylona, Peter Goos", "title": "Optimal Design of Experiments for Nonlinear Response Surface Models", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12313", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many chemical and biological experiments involve multiple treatment factors\nand often it is convenient to fit a nonlinear model in these factors. This\nnonlinear model can be mechanistic, empirical or a hybrid of the two. Motivated\nby experiments in chemical engineering, we focus on D-optimal design for\nmultifactor nonlinear response surfaces in general. In order to find and study\noptimal designs, we first implement conventional point and coordinate exchange\nalgorithms. Next, we develop a novel multiphase optimisation method to\nconstruct D-optimal designs with improved properties. The benefits of this\nmethod are demonstrated by application to two experiments involving nonlinear\nregression models. The designs obtained are shown to be considerably more\ninformative than designs obtained using traditional design optimality\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 16:40:05 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 10:58:34 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Huang", "Yuanzhi", ""], ["Gilmour", "Steven", ""], ["Mylona", "Kalliopi", ""], ["Goos", "Peter", ""]]}, {"id": "1803.06711", "submitter": "Bomin Kim", "authors": "Bomin Kim, Xiaoyue Niu, David R. Hunter, Xun Cao", "title": "A Dynamic Additive and Multiplicative Effects Model with Application to\n  the United Nations Voting Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a regression model for a series of networks that are correlated\nover time. Our model is a dynamic extension of the additive and multiplicative\neffects network model (AMEN) of Hoff (2019) In addition to incorporating a\ntemporal structure, the model accommodates two types of missing data thus\nallows the size of the network to vary over time. We demonstrate via\nsimulations the necessity of various components of the model. We apply the\nmodel to the United Nations General Assembly voting data from 1983 to 2014\n(Voeten (2013)) to answer interesting research questions regarding to\ninternational voting behaviors. In addition to finding important factors that\ncould explain the voting behaviors, the model-estimated additive effects,\nmultiplicative effects, and their movements reveal meaningful foreign policy\npositions and alliances of various countries.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 18:46:10 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:28:53 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 00:22:47 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 02:10:45 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kim", "Bomin", ""], ["Niu", "Xiaoyue", ""], ["Hunter", "David R.", ""], ["Cao", "Xun", ""]]}, {"id": "1803.06730", "submitter": "Yi Wang Mr.", "authors": "Yi Wang, Ning Zhang, Yushi Tan, Tao Hong, Daniel Kirschen, Chongqing\n  Kang", "title": "Combining Probabilistic Load Forecasts", "comments": "Submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic load forecasts provide comprehensive information about future\nload uncertainties. In recent years, many methodologies and techniques have\nbeen proposed for probabilistic load forecasting. Forecast combination, a\nwidely recognized best practice in point forecasting literature, has never been\nformally adopted to combine probabilistic load forecasts. This paper proposes a\nconstrained quantile regression averaging (CQRA) method to create an improved\nensemble from several individual probabilistic forecasts. We formulate the CQRA\nparameter estimation problem as a linear program with the objective of\nminimizing the pinball loss, with the constraints that the parameters are\nnonnegative and summing up to one. We demonstrate the effectiveness of the\nproposed method using two publicly available datasets, the ISO New England data\nand Irish smart meter data. Comparing with the best individual probabilistic\nforecast, the ensemble can reduce the pinball score by 4.39% on average. The\nproposed ensemble also demonstrates superior performance over nine other\nbenchmark ensembles.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:15:00 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wang", "Yi", ""], ["Zhang", "Ning", ""], ["Tan", "Yushi", ""], ["Hong", "Tao", ""], ["Kirschen", "Daniel", ""], ["Kang", "Chongqing", ""]]}, {"id": "1803.06735", "submitter": "Rui Zhu", "authors": "Rui Zhu and Subhashis Ghosal", "title": "Bayesian ROC surface estimation under verification bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) surface is a generalization of\nROC curve and is widely used for assessment of the accuracy of diagnostic tests\non three categories. A complication called the verification bias, meaning that\nnot all subjects have their true disease status verified often occur in real\napplication of ROC analysis. This is a common problem since the gold standard\ntest, which is used to generate true disease status, can be invasive and\nexpensive. In this paper, we will propose a Bayesian approach for estimating\nthe ROC surface based on continuous data under a semi-parametric trinormality\nassumption. Our proposed method often adopted in ROC analysis can also be\nextended to situation in the presence of verification bias. We compute the\nposterior distribution of the parameters under trinormality assumption by using\na rank-based likelihood. Consistency of the posterior under mild conditions is\nalso established. We compare our method with the existing methods for\nestimating ROC surface and conclude that our method performs well in terms of\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:35:05 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zhu", "Rui", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1803.06763", "submitter": "Claire Bowen", "authors": "Claire McKay Bowen, Fang Liu, and Binyue Su", "title": "Differentially Private Data Release via Statistical Election to\n  Partition Sequentially", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy (DP) formalizes privacy in mathematical terms and\nprovides a robust concept for privacy protection. DIfferentially Private Data\nSynthesis (DIPS) techniques produce and release synthetic individual-level data\nin the DP framework. One key challenge to developing DIPS methods is\npreservation of the statistical utility of synthetic data, especially in\nhigh-dimensional settings. We propose a new DIPS approach, STatistical Election\nto Partition Sequentially (STEPS) that partitions data by attributes according\nto their importance ranks according to either a practical or statistical\nimportance measure. STEPS aims to achieve better original information\npreservation for the attributes with higher importance ranks and produce thus\nmore useful synthetic data overall. We present an algorithm to implement the\nSTEPS procedure and employ the privacy budget composability to ensure the\noverall privacy cost is controlled at the pre-specified value. We apply the\nSTEPS procedure to both simulated data and the 2000-2012 Current Population\nSurvey youth voter data. The results suggest STEPS can better preserve the\npopulation-level information and the original information for some analyses\ncompared to PrivBayes, a modified Uniform histogram approach, and the flat\nLaplace sanitizer.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 23:51:17 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 22:41:28 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 21:40:29 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 03:34:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Bowen", "Claire McKay", ""], ["Liu", "Fang", ""], ["Su", "Binyue", ""]]}, {"id": "1803.07141", "submitter": "Zehang Li", "authors": "Samuel J. Clark and Zehang Li and Tyler H. McCormick", "title": "Quantifying the Contributions of Training Data and Algorithm Logic to\n  the Performance of Automated Cause-assignment Algorithms for Verbal Autopsy", "comments": "This version implements Tariff with an additional normalization step\n  that was previously ignored in the package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A verbal autopsy (VA) consists of a survey with a relative or close contact\nof a person who has recently died. VA surveys are commonly used to infer likely\ncauses of death for individuals when deaths happen outside of hospitals or\nhealthcare facilities. Several statistical and algorithmic methods are\navailable to assign cause of death using VA surveys. Each of these methods\nrequire as inputs some information about the joint distribution of symptoms and\ncauses. In this note, we examine the generalizability of this symptom-cause\ninformation by comparing different automated coding methods using various\ncombinations of inputs and evaluation data. VA algorithm performance is\naffected by both the specific SCI themselves and the logic of a given\nalgorithm. Using a variety of performance metrics for all existing VA\nalgorithms, we demonstrate that in general the adequacy of the information\nabout the joint distribution between symptoms and cause affects performance at\nleast as much or more than algorithm logic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:07:53 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 21:19:48 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 01:02:53 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 14:10:01 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Clark", "Samuel J.", ""], ["Li", "Zehang", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1803.07166", "submitter": "Silvia D'Angelo", "authors": "Silvia D'Angelo, Thomas Brendan Murphy and Marco Alf\\`o", "title": "Latent Space Modeling of Multidimensional Networks with Application to\n  the Exchange of Votes in Eurovision Song Contest", "comments": null, "journal-ref": "Ann. Appl. Stat. 13(2): 900-930 (June 2019)", "doi": "10.1214/18-AOAS1221", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Eurovision Song Contest is a popular TV singing competition held annually\namong country members of the European Broadcasting Union. In this competition,\neach member can be both contestant and jury, as it can participate with a song\nand/or vote for other countries' tunes. Throughout the years, the voting system\nhas repeatedly been accused of being biased by the presence of tactical voting,\naccording to which votes would represent strategic interests rather than actual\nmusical preferences of the voting countries. In this work, we develop a latent\nspace model to investigate the presence of a latent structure underlying the\nexchange of votes. Focusing on the period from 1998 to 2015, we represent the\nvote exchange as a multivariate network: each edition is a network, where\ncountries are the nodes and two countries are linked by an edge if one voted\nfor the other. The different networks are taken to be independent replicates of\na common latent space capturing the overall relationships among the countries.\nProximity denotes similarity, and countries close in the latent space are\nassumed to be more likely to exchange votes. Therefore, if the exchange of\nvotes depends on the similarity between countries, the quality of the competing\nsongs might not be a relevant factor in the determination of the voting\npreferences, and this would suggest the presence of bias. A Bayesian\nhierarchical modelling approach is employed to model the probability of a\nconnection between any two countries as a function of their distance in the\nlatent space, and of network-specific parameters and edge-specific covariates.\nThe inferred latent space is found to be relevant in the determination of edge\nprobabilities, however, the positions of the countries in such space only\npartially correspond to their actual geographical positions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:43:38 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["D'Angelo", "Silvia", ""], ["Murphy", "Thomas Brendan", ""], ["Alf\u00f2", "Marco", ""]]}, {"id": "1803.07172", "submitter": "Alessandro Lomi", "authors": "Tom A.B.Snijders and Alessandro Lomi", "title": "Beyond Homophily: Incorporating Actor Variables in Actor-oriented\n  Network Models", "comments": "33 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the specification of effects of numerical actor attributes in\nstatistical models for directed social networks. A fundamental mechanism is\nhomophily or assortativity, where actors have a higher likelihood to be tied\nwith others having similar values of the variable under study. But there are\nother mechanisms that may also play a role in how the attribute values of two\nactors influence the likelihood of a tie. We discuss three additional\nmechanisms: aspiration to send ties to others having high values; conformity in\nthe sense of sending more ties to others whose values are close to what may be\nconsidered the `social norm'; and sociability, where those having higher values\nwill tend to send more ties generally. These mechanisms may operate jointly,\nand then their effects will be confounded. We present a specification\nrepresenting these effects simultaneously by a four-parameter quadratic\nfunction of the values of sender and receiver. Greater flexibility can be\nobtained by a five-parameter extension. We argue that empirical researchers\noften overlook the possibility that homophily may be confounded with these\nother mechanisms, and that for actor attributes that have important effects on\ndirected networks, these specifications may provide an improvement. An\nillustration is given of the dependence of advice ties on academic grades in a\nnetwork of MBA students, analyzed by the Stochastic Actor-oriented Model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 21:35:21 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 06:39:13 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Snijders", "Tom A. B.", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1803.07418", "submitter": "Yang Feng", "authors": "Emre Demirkaya, Yang Feng, Pallavi Basu, Jinchi Lv", "title": "Large-Scale Model Selection with Misspecification", "comments": "38 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1412.7468", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:10:12 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Demirkaya", "Emre", ""], ["Feng", "Yang", ""], ["Basu", "Pallavi", ""], ["Lv", "Jinchi", ""]]}, {"id": "1803.07622", "submitter": "Patrick Diehl", "authors": "Patrick Diehl and Ilyass Tabiai and Felix W. Baumann and Martin\n  Levesque", "title": "Long term availability of raw experimental data in experimental fracture\n  mechanics", "comments": null, "journal-ref": null, "doi": "10.1016/j.engfracmech.2018.04.030", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Experimental data availability is a cornerstone for reproducibility in\nexperimental fracture mechanics, which is crucial to the scientific method.\nThis short communication focuses on the accessibility and long term\navailability of raw experimental data. The corresponding authors of the eleven\nmost cited papers, related to experimental fracture mechanics, for every year\nfrom 2000 up to 2016, were kindly asked about the availability of the raw\nexperimental data associated with each publication. For the 187 e-mails sent:\n22.46% resulted in outdated contact information, 57.75% of the authors did\nreceived our request and did not reply, and 19.79 replied to our request. The\navailability of data is generally low with only $11$ available data sets\n(5.9%). The authors identified two main issues for the lacking availability of\nraw experimental data. First, the ability to retrieve data is strongly attached\nto the the possibility to contact the corresponding author. This study suggests\nthat institutional e-mail addresses are insufficient means for obtaining\nexperimental data sets. Second, lack of experimental data is also due that\nsubmission and publication does not require to make the raw experimental data\navailable. The following solutions are proposed: (1) Requirement of unique\nidentifiers, like ORCID or ResearcherID, to detach the author(s) from their\ninstitutional e-mail address, (2) Provide DOIs, like Zenodo or Dataverse, to\nmake raw experimental data citable, and (3) grant providing organizations\nshould ensure that experimental data by public funded projects is available to\nthe public.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:49:46 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Diehl", "Patrick", ""], ["Tabiai", "Ilyass", ""], ["Baumann", "Felix W.", ""], ["Levesque", "Martin", ""]]}, {"id": "1803.07688", "submitter": "Anastasia Ignatieva", "authors": "Anastasia Ignatieva, Andrew F. Bell, Bruce J. Worton", "title": "Point process models for quasi-periodic volcanic earthquakes", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long period (LP) earthquakes are common at active volcanoes, and are\nubiquitous at persistently active andesitic and dacitic subduction zone\nvolcanoes. They provide critical information regarding the state of volcanic\nunrest, and their occurrence rates are key data for eruption forecasting. LPs\nare commonly quasi-periodic or 'anti-clustered', unlike volcano-tectonic (VT)\nearthquakes, so the existing Poisson point process methods used to model\noccurrence rates of VT earthquakes are unlikely to be optimal for LP data. We\nevaluate the performance of candidate formulations for LP data, based on\ninhomogeneous point process models with four different inter-event time\ndistributions: exponential (IP), Gamma (IG), inverse Gaussian (IIG), and\nWeibull (IW). We examine how well these models explain the observed data, and\nthe quality of retrospective forecasts of eruption time. We use a Bayesian MCMC\napproach to fit the models. Goodness-of-fit is assessed using Quantile-Quantile\nand Kolmogorov-Smirnov methods, and benchmarking against results obtained from\nsynthetic datasets. IG and IIG models were both found to fit the data well,\nwith the IIG model slightly outperforming the IG model. Retrospective\nforecasting analysis shows that the IG model performs best, with the initial\npreference for the IIG model controlled by catalogue incompleteness late in the\nsequence. The IG model fits the data significantly better than the IP model,\nand simulations show it produces better forecasts for highly periodic data.\nSimulations also show that forecast precision increases with the degree of\nperiodicity of the earthquake process using the IG model, and so should be\nbetter for LP earthquakes than VTs. These results provide a new framework for\npoint process modelling of volcanic earthquake time series, and verification of\nalternative models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 23:21:14 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 22:30:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ignatieva", "Anastasia", ""], ["Bell", "Andrew F.", ""], ["Worton", "Bruce J.", ""]]}, {"id": "1803.07734", "submitter": "Zhanglong Cao", "authors": "Zhanglong Cao, David Bryant, Matthew Parry", "title": "Adaptive Sequential MCMC for Combined State and Parameter Estimation", "comments": "adaptive MCMC method application. need further work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the case of a linear state space model, we implement an MCMC sampler with\ntwo phases. In the learning phase, a self-tuning sampler is used to learn the\nparameter mean and covariance structure. In the estimation phase, the parameter\nmean and covariance structure informs the proposed mechanism and is also used\nin a delayed-acceptance algorithm. Information on the resulting state of the\nsystem is given by a Gaussian mixture. In on-line mode, the algorithm is\nadaptive and uses a sliding window approach to accelerate sampling speed and to\nmaintain appropriate acceptance rates. We apply the algorithm to joined state\nand parameter estimation in the case of irregularly sampled GPS time series\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:44:06 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Cao", "Zhanglong", ""], ["Bryant", "David", ""], ["Parry", "Matthew", ""]]}, {"id": "1803.07750", "submitter": "Ferenc Ditr\\'oi Dr", "authors": "F. T\\'ark\\'anyi, F. Ditr\\'oi, S. Tak\\'acs, A. Hermanne, B. Kir\\'aly", "title": "Activation cross-section data for alpha-particle induced nuclear\n  reactions on natural ytterbium for some longer lived radioisotopes", "comments": null, "journal-ref": "J. Radioanalytical and Nuclear Chemistry 311(3)(2017)1825-1829", "doi": "10.1007/s10967-016-5139-0", "report-no": null, "categories": "nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additional experimental cross sections were deduced for the long half-life\nactivation products (172Hf and 173Lu) from the alpha particle induced reactions\non ytterbium up to 38 MeV from late, long measurements and for 175Yb, 167Tm\nfrom a re-evaluation of earlier measured spectra. The cross-sections are\ncompared with the earlier experimental datasets and with the data based on the\nTALYS theoretical nuclear reaction model (available in the TENDL-2014 and 2015\nlibraries) and the ALICE-IPPE code.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 05:22:57 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["T\u00e1rk\u00e1nyi", "F.", ""], ["Ditr\u00f3i", "F.", ""], ["Tak\u00e1cs", "S.", ""], ["Hermanne", "A.", ""], ["Kir\u00e1ly", "B.", ""]]}, {"id": "1803.07981", "submitter": "Manuel Cruz", "authors": "Manuel Cruz, Sandra Ramos, Miguel Pinho", "title": "A time-dependent scoring system for soccer leagues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new continuous scoring system for soccer is proposed, based\non the proportion of time that a team is winning, losing or tied. Several\nsimulations are made applying this technique to complete seasons of different\nleagues. As some preliminary fundamental analysis previews, the simulation\nresults show that this proposal compacts the gap between several teams,\nincreasing the competitiveness of the championships and making the matches even\nmore challenging as every minute may count in order to get the best rank in the\nfinal standings. Based on those results, some characteristics of the model are\nhighlighted and a discussion on some of the advantages, disadvantages and\npractical issues regarding a hypothetical implementation of this model is made.\nFinally, some ideas to tackle the less positive effects of an implementation in\nreal leagues is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 17:28:40 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Cruz", "Manuel", ""], ["Ramos", "Sandra", ""], ["Pinho", "Miguel", ""]]}, {"id": "1803.08010", "submitter": "Zheng Xie", "authors": "Zheng Xie, Guannan Liu, Junjie Wu, and Yong Tan", "title": "Social Media Would Not Lie: Prediction of the 2016 Taiwan Election via\n  Online Heterogeneous Data", "comments": null, "journal-ref": "EPJ Data Science,2018,7:32", "doi": "10.1140/epjds/s13688-018-0163-7", "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of online media has attracted researchers from various domains\nto explore human behavior and make interesting predictions. In this research,\nwe leverage heterogeneous social media data collected from various online\nplatforms to predict Taiwan's 2016 presidential election. In contrast to most\nexisting research, we take a \"signal\" view of heterogeneous information and\nadopt the Kalman filter to fuse multiple signals into daily vote predictions\nfor the candidates. We also consider events that influenced the election in a\nquantitative manner based on the so-called event study model that originated in\nthe field of financial research. We obtained the following interesting\nfindings. First, public opinions in online media dominate traditional polls in\nTaiwan election prediction in terms of both predictive power and timeliness.\nBut offline polls can still function on alleviating the sample bias of online\nopinions. Second, although online signals converge as election day approaches,\nthe simple Facebook \"Like\" is consistently the strongest indicator of the\nelection result. Third, most influential events have a strong connection to\ncross-strait relations, and the Chou Tzu-yu flag incident followed by the\napology video one day before the election increased the vote share of Tsai\nIng-Wen by 3.66%. This research justifies the predictive power of online media\nin politics and the advantages of information fusion. The combined use of the\nKalman filter and the event study method contributes to the data-driven\npolitical analytics paradigm for both prediction and attribution purposes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:53:19 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 01:44:22 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Xie", "Zheng", ""], ["Liu", "Guannan", ""], ["Wu", "Junjie", ""], ["Tan", "Yong", ""]]}, {"id": "1803.08027", "submitter": "Ranjan Maitra", "authors": "Ranjan Maitra", "title": "Efficient Bandwidth Estimation in Two-dimensional Filtered\n  Backprojection Reconstruction", "comments": "12 pages, 7 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2919428", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized cross-validation approach to estimate the reconstruction filter\nbandwidth in two-dimensional Filtered Backprojection is presented. The method\nwrites the reconstruction equation in equivalent backprojected filtering form,\nderives results on eigendecomposition of symmetric two-dimensional circulant\nmatrices and applies them to make bandwidth estimation a computationally\nefficient operation within the context of standard backprojected filtering\nreconstruction. Performance evaluations on a wide range of simulated emission\ntomography experiments give promising results. The superior performance holds\nat both low and high total expected counts, pointing to the method's\napplicability even in weaker signal-noise situations. The approach also applies\nto the more general class of elliptically symmetric filters, with\nreconstruction performance often better than even that obtained with the true\noptimal radially symmetric filter.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:26:41 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 11:13:50 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 16:20:11 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 18:32:06 GMT"}, {"version": "v5", "created": "Sun, 21 Apr 2019 15:35:53 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Maitra", "Ranjan", ""]]}, {"id": "1803.08128", "submitter": "Agatha Rodrigues Mrs.", "authors": "Agatha Sacramento Rodrigues, Vinicius Fernando Calsavara, Vera L\\'ucia\n  Damasceno Tomazella", "title": "Modeling cure fraction with frailty term in latent risk: a Bayesian\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a flexible cure rate model with frailty term in\nlatent risk, which is obtained by incorporating a frailty term in risk function\nof latent competing causes. The number of competing causes of the event of\ninterest follows negative binomial distribution and the frailty variable\nfollows power variance function distribution, in which includes other frailty\nmodels such as gamma, positive stable and inverse Gaussian frailty models as\nspecial cases. The proposed model takes into account the presence of covariates\nand right-censored survival data suitable for populations with a cure rate.\nBesides, it allows quantifying the degree of unobserved heterogeneity induced\nby unobservable risk factors, in which is important to explain the survival\ntime. Once the posterior distribution has not close form, Markov chain Monte\nCarlo simulations are considered for estimation procedure. We performed several\nsimulation studies and the practical relevance of the proposed model is\ndemonstrated in a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 15:36:26 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Rodrigues", "Agatha Sacramento", ""], ["Calsavara", "Vinicius Fernando", ""], ["Tomazella", "Vera L\u00facia Damasceno", ""]]}, {"id": "1803.08424", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 21:36:16 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1803.08444", "submitter": "Alastair Gregory", "authors": "Alastair Gregory, F. Din-Houn Lau and Liam Butler", "title": "A Quantile-Based Approach to Modelling Recovery Time in Structural\n  Health Monitoring", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques play a large role in the structural health monitoring\nof instrumented infrastructure, such as a railway bridge constructed with an\nintegrated network of fibre optic sensors. One possible way to reason about the\nstructural health of such a railway bridge, is to model the time it takes to\nrecover to a no-load (baseline) state after a train passes over. Inherently,\nthis recovery time is random and should be modelled statistically. This paper\nuses a non-parametric model, based on empirical quantile approximations, to\nconstruct a space-memory efficient baseline distribution for the streaming data\nfrom these sensors. A fast statistical test is implemented to detect deviations\naway from, and recovery back to, this distribution when trains pass over the\nbridge, yielding a recovery time. Our method assumes that there are no temporal\nvariations in the data. A median-based detrending scheme is used to remove the\ntemporal variations likely due to temperature changes. This allows for the\ncontinuous recording of sensor data with a space-memory constraint.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:39:27 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Gregory", "Alastair", ""], ["Lau", "F. Din-Houn", ""], ["Butler", "Liam", ""]]}, {"id": "1803.08482", "submitter": "Jake Carson", "authors": "Jake Carson, Michel Crucifix, Simon P. Preston, Richard D. Wilkinson", "title": "Quantifying Age and Model Uncertainties in Paleoclimate Data and\n  Dynamical Climate Models with a Joint Inferential Analysis", "comments": null, "journal-ref": "Proceedings of the Royal Society A: Mathematical, Physical and\n  Engineering Sciences, 475, 2019", "doi": "10.1098/rspa.2018.0854", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major goal in paleoclimate science is to reconstruct historical climates\nusing proxies for climate variables such as those observed in sediment cores,\nand in the process learn about climate dynamics. This is hampered by\nuncertainties in how sediment core depths relate to ages, how proxy quantities\nrelate to climate variables, how climate models are specified, and the values\nof parameters in climate models. Quantifying these uncertainties is key in\ndrawing well founded conclusions. Analyses are often performed in separate\nstages with, for example, a sediment core's depth-age relation being estimated\nas stage one, then fed as an input to calibrate climate models as stage two.\nHere, we show that such \"multi-stage\" approaches can lead to misleading\nconclusions. We develop a joint inferential approach for climate\nreconstruction, model calibration, and age model estimation. We focus on the\nglacial-interglacial cycle over the past 780 kyr, analysing two sediment cores\nthat span this range. Our age estimates are largely in agreement with previous\nstudies, but provides the full joint specification of all uncertainties,\nestimation of model parameters, and the model evidence. By sampling plausible\nchronologies from the posterior distribution, we demonstrate that downstream\nscientific conclusions can differ greatly both between different sampled\nchronologies, and in comparison with conclusions obtained in the complete joint\ninferential analysis. We conclude that multi-stage analyses are insufficient\nwhen dealing with uncertainty, and that to draw sound conclusions the full\njoint inferential analysis must be performed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:33:00 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 15:42:33 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Carson", "Jake", ""], ["Crucifix", "Michel", ""], ["Preston", "Simon P.", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "1803.08503", "submitter": "Yan Zhao", "authors": "Yan Zhao", "title": "Kalman Filter, Unscented Filter and Particle Flow Filter on Non-linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters, especially wide range of Kalman Filters have shown their impacts on\npredicting variables of stochastic models with higher accuracy then traditional\nstatistic methods. Updating mean and covariance each time makes Bayesian\ninferences more meaningful. In this paper, we mainly focused on the derivation\nand implementation of three powerful filters: Kalman Filter, Unscented Kalman\nFilter and Particle Flow Filter. Comparison for these different type of filters\ncould make us more clear about the suitable applications for different\ncircumstances.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:16:09 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Zhao", "Yan", ""]]}, {"id": "1803.08553", "submitter": "Grzegorz Sikora", "authors": "Grzegorz Sikora", "title": "Statistical test for fractional Brownian motion based on detrending\n  moving average algorithm", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.08.031", "report-no": null, "categories": "physics.data-an math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by contemporary and rich applications of anomalous diffusion\nprocesses we propose a new statistical test for fractional Brownian motion,\nwhich is one of the most popular models for anomalous diffusion systems. The\ntest is based on detrending moving average statistic and its probability\ndistribution. Using the theory of Gaussian quadratic forms we determined it as\na generalized chi-squared distribution. The proposed test could be generalized\nfor statistical testing of any centered non-degenerate Gaussian process.\nFinally, we examine the test via Monte Carlo simulations for two exemplary\nscenarios of subdiffusive and superdiffusive dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 19:20:06 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sikora", "Grzegorz", ""]]}, {"id": "1803.08584", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Tingran Gao, and James Evans", "title": "Curvature of Hypergraphs via Multi-Marginal Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.SI math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel definition of curvature for hypergraphs, a natural\ngeneralization of graphs, by introducing a multi-marginal optimal transport\nproblem for a naturally defined random walk on the hypergraph. This curvature,\ntermed \\emph{coarse scalar curvature}, generalizes a recent definition of Ricci\ncurvature for Markov chains on metric spaces by Ollivier [Journal of Functional\nAnalysis 256 (2009) 810-864], and is related to the scalar curvature when the\nhypergraph arises naturally from a Riemannian manifold. We investigate basic\nproperties of the coarse scalar curvature and obtain several bounds. Empirical\nexperiments indicate that coarse scalar curvatures are capable of detecting\n\"bridges\" across connected components in hypergraphs, suggesting it is an\nappropriate generalization of curvature on simple graphs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 21:13:35 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Gao", "Tingran", ""], ["Evans", "James", ""]]}, {"id": "1803.08947", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Gene Whipps, Prudhvi Gurram, and Vahid Tarokh", "title": "Sequential Event Detection Using Multimodal Data in Nonstationary\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sequential detection of anomalies in multimodal data is\nconsidered. The objective is to observe physical sensor data from CCTV cameras,\nand social media data from Twitter and Instagram to detect anomalous behaviors\nor events. Data from each modality is transformed to discrete time count data\nby using an artificial neural network to obtain counts of objects in CCTV\nimages and by counting the number of tweets or Instagram posts in a\ngeographical area. The anomaly detection problem is then formulated as a\nproblem of quickest detection of changes in count statistics. The quickest\ndetection problem is then solved using the framework of partially observable\nMarkov decision processes (POMDP), and structural results on the optimal policy\nare obtained. The resulting optimal policy is then applied to real multimodal\ndata collected from New York City around a 5K race to detect the race. The\ncount data both before and after the change is found to be nonstationary in\nnature. The proposed mathematical approach to this problem provides a framework\nfor event detection in such nonstationary environments and across multiple data\nmodalities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 19:14:04 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Banerjee", "Taposh", ""], ["Whipps", "Gene", ""], ["Gurram", "Prudhvi", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1803.09015", "submitter": "Pedro H. C. Sant'Anna", "authors": "Brantly Callaway, Pedro H. C. Sant'Anna", "title": "Difference-in-Differences with Multiple Time Periods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider identification, estimation, and inference\nprocedures for treatment effect parameters using Difference-in-Differences\n(DiD) with (i) multiple time periods, (ii) variation in treatment timing, and\n(iii) when the \"parallel trends assumption\" holds potentially only after\nconditioning on observed covariates. We show that a family of causal effect\nparameters are identified in staggered DiD setups, even if differences in\nobserved characteristics create non-parallel outcome dynamics between groups.\nOur identification results allow one to use outcome regression, inverse\nprobability weighting, or doubly-robust estimands. We also propose different\naggregation schemes that can be used to highlight treatment effect\nheterogeneity across different dimensions as well as to summarize the overall\neffect of participating in the treatment. We establish the asymptotic\nproperties of the proposed estimators and prove the validity of a\ncomputationally convenient bootstrap procedure to conduct asymptotically valid\nsimultaneous (instead of pointwise) inference. Finally, we illustrate the\nrelevance of our proposed tools by analyzing the effect of the minimum wage on\nteen employment from 2001--2007. Open-source software is available for\nimplementing the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 23:45:05 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 19:30:23 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 03:32:06 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 16:15:59 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Callaway", "Brantly", ""], ["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "1803.09133", "submitter": "Amir Karami", "authors": "Matthew Collins, Amir Karami", "title": "Social Media Analysis For Organizations: Us Northeastern Public And\n  State Libraries Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networking sites such as Twitter have provided a great opportunity for\norganizations such as public libraries to disseminate information for public\nrelations purposes. However, there is a need to analyze vast amounts of social\nmedia data. This study presents a computational approach to explore the content\nof tweets posted by nine public libraries in the northeastern United States of\nAmerica. In December 2017, this study extracted more than 19,000 tweets from\nthe Twitter accounts of seven state libraries and two urban public libraries.\nComputational methods were applied to collect the tweets and discover\nmeaningful themes. This paper shows how the libraries have used Twitter to\nrepresent their services and provides a starting point for different\norganizations to evaluate the themes of their public tweets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 17:23:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Collins", "Matthew", ""], ["Karami", "Amir", ""]]}, {"id": "1803.09134", "submitter": "Amir Karami", "authors": "Frank Webb, Amir Karami, Vanessa Kitzie", "title": "Characterizing Diseases and disorders in Gay Users' tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lack of information exists about the health issues of lesbian, gay,\nbisexual, transgender, and queer (LGBTQ) people who are often excluded from\nnational demographic assessments, health studies, and clinical trials. As a\nresult, medical experts and researchers lack a holistic understanding of the\nhealth disparities facing these populations. Fortunately, publicly available\nsocial media data such as Twitter data can be utilized to support the decisions\nof public health policy makers and managers with respect to LGBTQ people. This\nresearch employs a computational approach to collect tweets from gay users on\nhealth-related topics and model these topics. To determine the nature of\nhealth-related information shared by men who have sex with men on Twitter, we\ncollected thousands of tweets from 177 active users. We sampled these tweets\nusing a framework that can be applied to other LGBTQ sub-populations in future\nresearch. We found 11 diseases in 7 categories based on ICD 10 that are in line\nwith the published studies and official reports.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 17:27:37 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Webb", "Frank", ""], ["Karami", "Amir", ""], ["Kitzie", "Vanessa", ""]]}, {"id": "1803.09206", "submitter": "Cody Champion", "authors": "Camilla Champion and Cody Champion", "title": "Agent-Based Implementation of Particle Hopping Traffic Model With\n  Stochastic and Queuing Elements", "comments": "Submission to the 2014 COMAP Mathematical Contest in Modeling, minor\n  changes made to improve clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lagging or halted traffic is bothersome. As such, it is desirable to have a\nmodel that can begin to determine the efficiency of various traffic\nstandardizations. Our model intended to create a multifaceted realistic\nsimulation of traffic flow while considering several factors. These factors\nincluded: passing conventions, e.g., right except to pass (REP) rule, system\nperturbation caused by insertion of an accident into the system, accessible\nnumber of lanes available with the REP, various human factors such as variation\nof individual maximum speed and likelihood to pass. A succession of models were\ncreated from a variation on an existing single-lane traffic model and adding\nextra dimensionality to the lattice to include multiple lanes, passing\nconventions, stochastic elements for individuality, and queuing rules to\nmovement algorithms. We found that the REP is an effective means of increasing\nthe critical density that a system can support. Eliminating human factors and\nthereby automating the system, results in a 160% increase in the sustainable\ncritical density of the system. The number of lanes increases the critical\ndensity of the system, but the maximum efficiency of the speed distribution\nremains the same. Excluding system automation, the optimal speed distribution\nfor drivers maximal speed was found to be Beta(5,5). Accidents in stable\nsystems can cause small local jams without causing global jams.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 05:59:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Champion", "Camilla", ""], ["Champion", "Cody", ""]]}, {"id": "1803.09461", "submitter": "Nicolas Jullien", "authors": "Shubham Krishna, Romain Billot, Nicolas Jullien", "title": "A clustering approach to infer Wikipedia contributors' profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In online communities, recent studies have strongly improved our knowledge\nabout the different types or profiles of contributors, from casual to very\ninvolved ones, through focused people. However they do so by using very complex\nmethodologies (qualitative-quantitative mix, with a high workload to manually\ncodify/characterize the edits), making their replication for the practitioners\nlimited. These studies are on the English Wikipedia only. The objective of this\npaper is to highlight different profiles of contributors with clustering\ntechniques. The originality is to show how using only the edits, and their\ndistribution over time, allows to build these contributors profiles with a good\naccuracy and stability amongst languages. The methodology is validated with\nboth Romanian and Danish wikis. The highlighted profiles are identifiable early\nin the history of involvement, suggesting that light monitoring of newcomers\nmay be sufficient to adapt the interaction with them and increase the retention\nrate.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:17:10 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Krishna", "Shubham", ""], ["Billot", "Romain", ""], ["Jullien", "Nicolas", ""]]}, {"id": "1803.09507", "submitter": "Jessica Hargreaves", "authors": "Jessica Hargreaves, Marina Knight, Jon Pitchford, Rachael Oakenfull,\n  Sangeeta Chawla, Jack Munns and Seth Davis", "title": "Wavelet spectral testing: application to nonstationary circadian rhythms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rhythmic data are ubiquitous in the life sciences. Biologists need reliable\nstatistical tests to identify whether a particular experimental treatment has\ncaused a significant change in a rhythmic signal. When these signals display\nnonstationary behaviour, as is common in many biological systems, the\nestablished methodologies may be misleading. Therefore, there is a real need\nfor new methodology that enables the formal comparison of nonstationary\nprocesses. As circadian behaviour is best understood in the spectral domain,\nhere we develop novel hypothesis testing procedures in the (wavelet) spectral\ndomain, embedding replicate information when available. The data are modelled\nas realisations of locally stationary wavelet processes, allowing us to define\nand rigorously estimate their evolutionary wavelet spectra. Motivated by three\ncomplementary applications in circadian biology, our new methodology allows the\nidentification of three specific types of spectral difference. We demonstrate\nthe advantages of our methodology over alternative approaches, by means of a\ncomprehensive simulation study and real data applications, using both published\nand newly generated circadian datasets. In contrast to the current standard\nmethodologies, our method successfully identifies differences within the\nmotivating circadian datasets, and facilitates wider ranging analyses of\nrhythmic biological data in general.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 11:14:02 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hargreaves", "Jessica", ""], ["Knight", "Marina", ""], ["Pitchford", "Jon", ""], ["Oakenfull", "Rachael", ""], ["Chawla", "Sangeeta", ""], ["Munns", "Jack", ""], ["Davis", "Seth", ""]]}, {"id": "1803.09590", "submitter": "Siddharth Arora Dr.", "authors": "Siddharth Arora, James W. Taylor", "title": "Rule-based Autoregressive Moving Average Models for Forecasting Load on\n  Special Days: A Case Study for France", "comments": "11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a case study on short-term load forecasting for France,\nwith emphasis on special days, such as public holidays. We investigate the\ngeneralisability to French data of a recently proposed approach, which\ngenerates forecasts for normal and special days in a coherent and unified\nframework, by incorporating subjective judgment in univariate statistical\nmodels using a rule-based methodology. The intraday, intraweek, and intrayear\nseasonality in load are accommodated using a rule-based triple seasonal\nadaptation of a seasonal autoregressive moving average (SARMA) model. We find\nthat, for application to French load, the method requires an important\nadaption. We also adapt a recently proposed SARMA model that accommodates\nspecial day effects on an hourly basis using indicator variables. Using a rule\nformulated specifically for the French load, we compare the SARMA models with a\nrange of different benchmark methods based on an evaluation of their point and\ndensity forecast accuracy. As sophisticated benchmarks, we employ the\nrule-based triple seasonal adaptations of Holt-Winters-Taylor (HWT) exponential\nsmoothing and artificial neural networks (ANNs). We use nine years of\nhalf-hourly French load data, and consider lead times ranging from one\nhalf-hour up to a day ahead. The rule-based SARMA approach generated the most\naccurate forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 13:48:27 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Arora", "Siddharth", ""], ["Taylor", "James W.", ""]]}, {"id": "1803.09730", "submitter": "Brent Schlotfeldt", "authors": "Brent Schlotfeldt, Vasileios Tzoumas, Dinesh Thakur, George J. Pappas", "title": "Resilient Active Information Gathering with Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of safety, security, and rescue in robotics, such as multi-robot\ntarget tracking, involve the execution of information acquisition tasks by\nteams of mobile robots. However, in failure-prone or adversarial environments,\nrobots get attacked, their communication channels get jammed, and their sensors\nmay fail, resulting in the withdrawal of robots from the collective task, and\nconsequently the inability of the remaining active robots to coordinate with\neach other. As a result, traditional design paradigms become insufficient and,\nin contrast, resilient designs against system-wide failures and attacks become\nimportant. In general, resilient design problems are hard, and even though they\noften involve objective functions that are monotone or submodular, scalable\napproximation algorithms for their solution have been hitherto unknown. In this\npaper, we provide the first algorithm, enabling the following capabilities:\nminimal communication, i.e., the algorithm is executed by the robots based only\non minimal communication between them; system-wide resiliency, i.e., the\nalgorithm is valid for any number of denial-of-service attacks and failures;\nand provable approximation performance, i.e., the algorithm ensures for all\nmonotone (and not necessarily submodular) objective functions a solution that\nis finitely close to the optimal. We quantify our algorithm's approximation\nperformance using a notion of curvature for monotone set functions. We support\nour theoretical analyses with simulated and real-world experiments, by\nconsidering an active information gathering scenario, namely, multi-robot\ntarget tracking.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:41:05 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 03:32:18 GMT"}, {"version": "v3", "created": "Sun, 2 Sep 2018 14:59:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Schlotfeldt", "Brent", ""], ["Tzoumas", "Vasileios", ""], ["Thakur", "Dinesh", ""], ["Pappas", "George J.", ""]]}, {"id": "1803.09913", "submitter": "Supun Perera Mr", "authors": "Supun Perera, Michael Bell, Michiel Bliemer", "title": "Network Science approach to Modelling Emergence and Topological\n  Robustness of Supply Networks: A Review and Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasingly complex and interconnected nature of global supply\nchain networks (SCNs), a recent strand of research has applied network science\nmethods to model SCN growth and subsequently analyse various topological\nfeatures, such as robustness. This paper provides: (1) a comprehensive review\nof the methodologies adopted in literature for modelling the topology and\nrobustness of SCNs; (2) a summary of topological features of the real world\nSCNs, as reported in various data driven studies; and (3) a discussion on the\nlimitations of existing network growth models to realistically represent the\nobserved topological characteristics of SCNs. Finally, a novel perspective is\nproposed to mimic the SCN topologies reported in empirical studies, through\nfitness based generative network models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 06:33:09 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Perera", "Supun", ""], ["Bell", "Michael", ""], ["Bliemer", "Michiel", ""]]}, {"id": "1803.10260", "submitter": "Kam Hamidieh", "authors": "Kam Hamidieh", "title": "A Data-Driven Statistical Model for Predicting the Critical Temperature\n  of a Superconductor", "comments": null, "journal-ref": "Computational Materials Science, Volume 154, Pages 346-354 (2018)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate a statistical model to predict the superconducting critical\ntemperature based on the features extracted from the superconductor's chemical\nformula. The statistical model gives reasonable out-of-sample predictions: $\\pm\n9.5$ K based on root-mean-squared-error. Features extracted based on thermal\nconductivity, atomic radius, valence, electron affinity, and atomic mass\ncontribute the most to the model's predictive accuracy. It is crucial to note\nthat our model does not predict whether a material is a superconductor or not,\nit only gives predictions for superconductors.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 02:14:05 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 19:40:43 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hamidieh", "Kam", ""]]}, {"id": "1803.10439", "submitter": "Mingxuan Cai", "authors": "Mingxuan Cai, Mingwei Dai, Jingsi Ming, Heng Peng, Jin Liu, Can Yang", "title": "BIVAS: A scalable Bayesian method for bi-level variable selection with\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a Bayesian bi-level variable selection problem in\nhigh-dimensional regressions. In many practical situations, it is natural to\nassign group membership to each predictor. Examples include that genetic\nvariants can be grouped at the gene level and a covariate from different tasks\nnaturally forms a group. Thus, it is of interest to select important groups as\nwell as important members from those groups. The existing Markov Chain Monte\nCarlo (MCMC) methods are often computationally intensive and not scalable to\nlarge data sets. To address this problem, we consider variational inference for\nbi-level variable selection (BIVAS). In contrast to the commonly used\nmean-field approximation, we propose a hierarchical factorization to\napproximate the posterior distribution, by utilizing the structure of bi-level\nvariable selection. Moreover, we develop a computationally efficient and fully\nparallelizable algorithm based on this variational approximation. We further\nextend the developed method to model data sets from multi-task learning. The\ncomprehensive numerical results from both simulation studies and real data\nanalysis demonstrate the advantages of BIVAS for variable selection, parameter\nestimation and computational efficiency over existing methods. The method is\nimplemented in R package `bivas' available at https://github.com/mxcai/bivas.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 07:44:42 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Cai", "Mingxuan", ""], ["Dai", "Mingwei", ""], ["Ming", "Jingsi", ""], ["Peng", "Heng", ""], ["Liu", "Jin", ""], ["Yang", "Can", ""]]}, {"id": "1803.10722", "submitter": "Daniel Inman PhD", "authors": "Daniel Inman, Laura J. Vimmerstedt, Brian Bush, Dana Stright, and\n  Steve Peterson", "title": "Application of Variance-Based Sensitivity Analysis to a Large System\n  Dynamics Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance-based sensitivity methods can provide insights into large\ncomputational models. We present a novel application of sensitivity analysis to\nthe Biomass Scenario Model (BSM) a large and complex system dynamics model of\nthe developing biofuels industry in the United States. We apply a two-stage\nsensitivity approach consisting of an initial sensitivity screening, followed\nby a variance decomposition approach. Identifying key system levers and\nquantifying their strength is not straightforward in complex system dynamics\nmodels that have numerous feedbacks and nonlinear results. Variance-based\nsensitivity analysis (VBSA) offers a systematic, global approach to assessing\nsystem dynamics models because it addresses nonlinear responses and interactive\neffects. Especially when a large model's size makes manual exploration of the\ninput space difficult and time-consuming, the approach can help to provide a\ncomprehensive understanding of interactions that drive model behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 16:54:45 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Inman", "Daniel", ""], ["Vimmerstedt", "Laura J.", ""], ["Bush", "Brian", ""], ["Stright", "Dana", ""], ["Peterson", "Steve", ""]]}, {"id": "1803.10791", "submitter": "Marc Suchard", "authors": "Martijn J. Schuemie, Patrick B. Ryan, George Hripcsak, David Madigan,\n  Marc A. Suchard", "title": "A systematic approach to improving the reliability and scale of evidence\n  from health care data", "comments": "24 pages, 6 figures, 2 tables, 28 pages supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns over reproducibility in science extend to research using existing\nhealthcare data; many observational studies investigating the same topic\nproduce conflicting results, even when using the same data. To address this\nproblem, we propose a paradigm shift. The current paradigm centers on\ngenerating one estimate at a time using a unique study design with unknown\nreliability and publishing (or not) one estimate at a time. The new paradigm\nadvocates for high-throughput observational studies using consistent and\nstandardized methods, allowing evaluation, calibration, and unbiased\ndissemination to generate a more reliable and complete evidence base. We\ndemonstrate this new paradigm by comparing all depression treatments for a set\nof outcomes, producing 17,718 hazard ratios, each using methodology on par with\nstate-of-the-art studies. We furthermore include control hypotheses to evaluate\nand calibrate our evidence generation process. Results show good transitivity\nand consistency between databases, and agree with four out of the five findings\nfrom clinical trials. The distribution of effect size estimates reported in\nliterature reveals an absence of small or null effects, with a sharp cutoff at\np = 0.05. No such phenomena were observed in our results, suggesting more\ncomplete and more reliable evidence.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 18:13:54 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Schuemie", "Martijn J.", ""], ["Ryan", "Patrick B.", ""], ["Hripcsak", "George", ""], ["Madigan", "David", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1803.10810", "submitter": "Danilo Alvares", "authors": "Helena Reis, Danilo Alvares, Patricia Jaques, Seiji Isotani", "title": "Analysis of permanence time in emotional states: A case study using\n  educational software", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the results of an experiment in which we investigated\nhow prior algebra knowledge and personality can influence the permanence time\nfrom the confusion state to frustration/boredom state in a computer learning\nenvironment. Our experimental results indicate that people with a neurotic\npersonality and a low level of algebra knowledge can deal with confusion for\nless time and can easily feel frustrated/bored when there is no intervention.\nOur analysis also suggest that people with an extroversion personality and a\nlow level of algebra knowledge are able to control confusion for longer,\nleading to later interventions. These findings support that it is possible to\ndetect emotions in a less invasive way and without the need of physiological\nsensors or complex algorithms. Furthermore, obtained median times can be\nincorporated into computational regulation models (e.g. adaptive interfaces) to\nregulate students' emotion during the teaching-learning process.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:05:49 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Reis", "Helena", ""], ["Alvares", "Danilo", ""], ["Jaques", "Patricia", ""], ["Isotani", "Seiji", ""]]}, {"id": "1803.10883", "submitter": "Alessandro Casini", "authors": "Alessandro Casini", "title": "Tests for Forecast Instability and Forecast Failure under a Continuous\n  Record Asymptotic Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel continuous-time asymptotic framework for inference on\nwhether the predictive ability of a given forecast model remains stable over\ntime. We formally define forecast instability from the economic forecaster's\nperspective and highlight that the time duration of the instability bears no\nrelationship with stable period. Our approach is applicable in forecasting\nenvironment involving low-frequency as well as high-frequency macroeconomic and\nfinancial variables. As the sampling interval between observations shrinks to\nzero the sequence of forecast losses is approximated by a continuous-time\nstochastic process (i.e., an Ito semimartingale) possessing certain pathwise\nproperties. We build an hypotheses testing problem based on the local\nproperties of the continuous-time limit counterpart of the sequence of losses.\nThe null distribution follows an extreme value distribution. While controlling\nthe statistical size well, our class of test statistics feature uniform power\nover the location of the forecast failure in the sample. The test statistics\nare designed to have power against general form of insatiability and are robust\nto common forms of non-stationarity such as heteroskedasticty and serial\ncorrelation. The gains in power are substantial relative to extant methods,\nespecially when the instability is short-lasting and when occurs toward the\ntail of the sample.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 00:11:07 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 01:01:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Casini", "Alessandro", ""]]}, {"id": "1803.10895", "submitter": "Kurt Riedel", "authors": "Beatrix Schunke, Kaya Imre, Kurt S. Riedel", "title": "Profile Shape Parameterization of JET Electron Temperature and Density\n  Profiles", "comments": null, "journal-ref": "Nuclear Fusion, Vol. 37, No. 1, pp. 101, (1997)", "doi": "10.1088/0029-5515/37/1/I08", "report-no": null, "categories": "physics.plasm-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temperature and density profiles of the Joint European Torus are\nparameterized using log-additive models in the control variables. Predictive\nerror criteria are used to determine which terms in the log-linear model to\ninclude. The density and temperature profiles are normalised to their line\naverages ($\\bar{n}$ and $\\bar{T}$). The normalised Ohmic density shape depends\nprimarily on the parameter $\\bar{n}/B_t$, where $B_t$ is the toroidal magnetic\nfield. Both the Low-mode (L-mode) and edge localized mode-free (ELM-free) high\nmode (H-mode) temperature profile shapes depend strongly on the type of heating\npower, with ion cyclotron resonant heating producing a more peaked profile than\nneutral beam injection. Given the heating type dependence, the L-mode\ntemperature shape is nearly independent of the other control variables. The\nH-mode temperature shape broadens as the effective charge, $Z_{eff}$,\nincreases. The line average L-mode temperature scales as $B_t^{.96}$(Power per\nparticle)$^{.385}$. The L-mode normalized density shape depends primarily on\nthe ratio of line average density, $\\bar{n}$, to the edge safety factor,\n$q_{95}$. As $\\bar{n}/q_{95}$ increases, the profile shape broadens. The\ncurrent, $I_p$, is the most important control variable for the normalized\nH-mode density. As the current increases, the profile broadens and the gradient\nat the edge sharpens. Increasing the heating power, especially the ion\ncyclotron resonant heating, or decreasing the average density, peaks the H-mode\ndensity profile slightly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 01:53:56 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Schunke", "Beatrix", ""], ["Imre", "Kaya", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.10902", "submitter": "Sina Dabiri", "authors": "Sina Dabiri, Kevin Heaslip", "title": "Transport-domain applications of widely used data sources in the smart\n  transportation: A survey", "comments": "52 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The rapid growth of population and the permanent increase in the number of\nvehicles engender several issues in transportation systems, which in turn call\nfor an intelligent and cost-effective approach to resolve the problems in an\nefficient manner. Smart transportation is a framework that leverages the power\nof Information and Communication Technology for acquisition, management, and\nmining of traffic-related data sources, which, in this study, are categorized\ninto: 1) traffic flow sensors, 2) video image processors, 3) probe people and\nvehicles based on Global Positioning Systems (GPS), mobile phone cellular\nnetworks, and Bluetooth, 4) location-based social networks, 5) transit data\nwith the focus on smart cards, and 6) environmental data. For each data source,\nfirst, the operational mechanism of the technology for capturing the data is\nsuccinctly demonstrated. Secondly, as the most salient feature of this study,\nthe transport-domain applications of each data source that have been conducted\nby the previous studies are reviewed and classified into the main groups.\nThirdly, a number of possible future research directions are provided for all\ntypes of data sources. Moreover, in order to alleviate the shortcomings\npertaining to each single data source and acquire a better understanding of\nmobility behavior in transportation systems, the data fusion architectures are\nintroduced to fuse the knowledge learned from a set of heterogeneous but\ncomplementary data sources. Finally, we briefly mention the current challenges\nand their corresponding solutions in the smart transportation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 02:25:08 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 02:01:07 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 12:04:45 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Dabiri", "Sina", ""], ["Heaslip", "Kevin", ""]]}, {"id": "1803.10915", "submitter": "Guodong Du", "authors": "Guodong Du, Liang Yuan, Kong Joo Shin, Shunsuke Managi", "title": "Modeling the spatio-temporal dynamics of land use change with recurrent\n  neural networks", "comments": "Two authors of this paper disagree with this submission, they think\n  it is too early. Therefore, I have to withdraw this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study applies recurrent neural networks (RNNs), which are known for its\nability to process sequential information, to model the spatio-temporal\ndynamics of land use change (LUC) and to forecast annual land use maps of the\ncity of Tsukuba, Japan. We develop two categories of RNN models: 1) simple RNN,\nwhich is the basic RNN variant, 2) three RNN variants with advanced gated\narchitecture: long short-term memory (LSTM), LSTM with peephole connection\n(LSTM-peephole), and gated recurrent unit (GRU) models. The four models are\ndeveloped using spatio-temporal data with high temporal resolution, annual data\nfor the periods 2000 to 2010, 2011 and 2012 to 2016 are used for training,\nvalidation and testing, respectively. The predictive performances are evaluated\nusing classification metrics (accuracy and F1 score) and the map comparison\nmetrics (Kappa simulation and fuzzy Kappa simulation). The results show that\nall RNN models achieve F1 scores higher than 0.55, and Kappa simulations higher\nthan 0.47. Out of the four RNN models, LSTM and LSTM-peephole models\nsignificantly outperform the other two RNN models. Furthermore, LSTM-peephole\nmodel slightly outperforms the LSTM model. In addition, the results indicate\nthat the RNN models with gated architecture, which have better ability to model\nlonger temporal dependency, significantly outperform the simple RNN model.\nMoreover, the predictive performance of LSTM-peephole model gradually decreases\nwith the decrease of temporal sequential length of the training set. These\nresults demonstrate the benefit of taking temporal dependency into account to\nmodel the LUC process with RNNs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 03:27:50 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 01:42:37 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Du", "Guodong", ""], ["Yuan", "Liang", ""], ["Shin", "Kong Joo", ""], ["Managi", "Shunsuke", ""]]}, {"id": "1803.10975", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "A simulation comparison of tournament designs for the World Men's\n  Handball Championships", "comments": "28 pages, 13 figures, 4 tables", "journal-ref": "International Transactions in Operational Research, 28(5):\n  2377-2401, 2021", "doi": "10.1111/itor.12691", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study aims to compare different designs for the World Men's Handball\nChampionships. This event, organised in every two years, has adopted four\nhybrid formats consisting of knockout and round-robin stages in recent decades,\nincluding a change of design between the two recent championships in 2017 and\n2019. They are evaluated under two extremal seeding policies with respect to\nvarious outcome measures through Monte-Carlo simulations. We find that the\nability to give the first four positions to the strongest teams, as well as the\nexpected quality and outcome uncertainty of the final is not necessarily a\nmonotonic function of the number of matches played: the most frugal format is\nthe second best with respect to these outcome measures, making it a good\ncompromise in an unavoidable trade-off. A possible error is identified in a\nparticular design. The relative performance of the formats is independent of\nthe seeding rules and the competitive balance of the teams. The recent reform\nis demonstrated to have increased the probability of winning for the top teams.\nOur results have useful implications for the organisers of hybrid tournaments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 09:22:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 11:42:06 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 10:55:02 GMT"}, {"version": "v4", "created": "Sat, 12 Jan 2019 10:07:39 GMT"}, {"version": "v5", "created": "Thu, 21 Mar 2019 15:05:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1803.11130", "submitter": "Donya Ghavidel Dobakhshari", "authors": "Donya Ghavidel, Pratyush Chakraborty, Enrique Baeyens, Vijay Gupta,\n  and Pramod P. Khargonekar", "title": "Incentive Design in a Distributed Problem with Strategic Agents", "comments": null, "journal-ref": "American Control Conference (ACC), 2018. IEEE, 2018, pp. 6539 -\n  6544", "doi": null, "report-no": null, "categories": "cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a general distributed system with multiple agents\nwho select and then implement actions in the system. The system has an operator\nwith a centralized objective. The agents, on the other hand, are selfinterested\nand strategic in the sense that each agent optimizes its own individual\nobjective. The operator aims to mitigate this misalignment by designing an\nincentive scheme for the agents. The problem is difficult due to the cost\nfunctions of the agents being coupled, the objective of the operator not being\nsocial welfare, and the operator having no direct control over actions being\nimplemented by the agents. This problem has been studied in many fields,\nparticularly in mechanism design and cost allocation. However, mechanism design\ntypically assumes that the operator has knowledge of the cost functions of the\nagents and the actions being implemented by the operator. On the other hand,\ncost allocation classically assumes that agents do not anticipate the effect of\ntheir actions on the incentive that they obtain. We remove these assumptions\nand present an incentive rule for this setup by bridging the gap between\nmechanism design and classical cost allocation. We analyze whether the proposed\ndesign satisfies various desirable properties such as social optimality, budget\nbalance, participation constraint, and so on. We also analyze which of these\nproperties can be satisfied if the assumptions of cost functions of the agents\nbeing private and the agents being anticipatory are relaxed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:56:36 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 01:15:29 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Ghavidel", "Donya", ""], ["Chakraborty", "Pratyush", ""], ["Baeyens", "Enrique", ""], ["Gupta", "Vijay", ""], ["Khargonekar", "Pramod P.", ""]]}, {"id": "1803.11136", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha, Lars Carlsson and Ola Spjuth", "title": "Conformal Prediction in Learning Under Privileged Information Paradigm\n  with Applications in Drug Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores conformal prediction in the learning under privileged\ninformation (LUPI) paradigm. We use the SVM+ realization of LUPI in an\ninductive conformal predictor, and apply it to the MNIST benchmark dataset and\nthree datasets in drug discovery. The results show that using privileged\ninformation produces valid models and improves efficiency compared to standard\nSVM, however the improvement varies between the tested datasets and is not\nsubstantial in the drug discovery applications. More importantly, using SVM+ in\na conformal prediction framework enables valid prediction intervals at\nspecified significance levels.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 16:21:10 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 10:38:12 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Gauraha", "Niharika", ""], ["Carlsson", "Lars", ""], ["Spjuth", "Ola", ""]]}, {"id": "1803.11194", "submitter": "Andrew Brown", "authors": "Stella Watson Self, Christopher McMahan, D. Andrew Brown, Robert Lund,\n  Jenna Gettings, and Michael Yabsley", "title": "A Large Scale Spatio-temporal Binomial Regression Model for Estimating\n  Seroprevalence Trends", "comments": "19 pages without figures. All figures are available as ancillary\n  files", "journal-ref": null, "doi": "10.1002/env.2538", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a large-scale Bayesian spatio-temporal binomial\nregression model for the purpose of investigating regional trends in antibody\nprevalence to Borrelia burgdorferi, the causative agent of Lyme disease. The\nproposed model uses Gaussian predictive processes to estimate the spatially\nvarying trends and a conditional autoregressive model to account for\nspatio-temporal dependence. Careful consideration is made to develop a novel\nframework that is scalable to large spatio-temporal data. The proposed model is\nused to analyze approximately 16 million Borrelia burgdorferi test results\ncollected on dogs located throughout the conterminous United States over a\nsixty month period. This analysis identifies several regions of increasing\ncanine risk. Specifically, this analysis reveals evidence that Lyme disease is\ngetting worse in some endemic regions and that it could potentially be\nspreading to other non-endemic areas. Further, given the zoonotic nature of\nthis vector-borne disease, this analysis could potentially reveal areas of\nincreasing human risk.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 15:39:26 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Self", "Stella Watson", ""], ["McMahan", "Christopher", ""], ["Brown", "D. Andrew", ""], ["Lund", "Robert", ""], ["Gettings", "Jenna", ""], ["Yabsley", "Michael", ""]]}, {"id": "1803.11233", "submitter": "Kamil Jod\\'z", "authors": "Kamil Jod\\'z", "title": "Mortality in a heterogeneous population - Lee-Carter's methodology", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EU Solvency II directive recommends insurance companies to pay more\nattention to the risk management methods. The sense of risk management is the\nability to quantify risk and apply methods that reduce uncertainty. In life\ninsurance, the risk is a consequence of the random variable describing the life\nexpectancy. The article will present a proposal for stochastic mortality\nmodeling based on the Lee and Carter methodology. The maximum likelihood method\nis often used to estimate parameters in mortality models. This method assumes\nthat the population is homogeneous and the number of deaths has the Poisson\ndistribution. The aim of this article is to change assumptions about the\ndistribution of the number of deaths. The results indicate that the model can\nget a better match to historical data, when the number of deaths has a negative\nbinomial distribution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 19:36:26 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Jod\u017a", "Kamil", ""]]}]