[{"id": "1105.0519", "submitter": "Doug Nychka", "authors": "Doug Nychka, Bo Li", "title": "Discussion of: A statistical analysis of multiple temperature proxies:\n  Are reconstructions of surface temperatures over the last 1000 years\n  reliable?", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS398K the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 80-82", "doi": "10.1214/10-AOAS398K", "report-no": "IMS-AOAS-AOAS398K", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A statistical analysis of multiple temperature proxies: Are\nreconstructions of surface temperatures over the last 1000 years reliable?\" by\nB.B. McShane and A.J. Wyner [arXiv:1104.4002]\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 09:02:21 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Nychka", "Doug", ""], ["Li", "Bo", ""]]}, {"id": "1105.0522", "submitter": "Jonathan Rougier", "authors": "Jonathan Rougier", "title": "Discussion of: A statistical analysis of multiple temperature proxies:\n  Are reconstructions of surface temperatures over the last 1000 years\n  reliable?", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS409 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 96-98", "doi": "10.1214/10-AOAS409", "report-no": "IMS-AOAS-AOAS409", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A statistical analysis of multiple temperature proxies: Are\nreconstructions of surface temperatures over the last 1000 years reliable?\" by\nB.B. McShane and A.J. Wyner [arXiv:1104.4002]\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 09:28:10 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Rougier", "Jonathan", ""]]}, {"id": "1105.0524", "submitter": "Stephen McIntyre", "authors": "Stephen McIntyre, Ross McKitrick", "title": "Discussion of: A statistical analysis of multiple temperature proxies:\n  Are reconstructions of surface temperatures over the last 1000 years\n  reliable?", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS398L the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 56-60", "doi": "10.1214/10-AOAS398L", "report-no": "IMS-AOAS-AOAS398L", "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A statistical analysis of multiple temperature proxies: Are\nreconstructions of surface temperatures over the last 1000 years reliable?\" by\nB.B. McShane and A.J. Wyner [arXiv:1104.4002]\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 09:36:45 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["McIntyre", "Stephen", ""], ["McKitrick", "Ross", ""]]}, {"id": "1105.0543", "submitter": "Li Su", "authors": "Li Su, Joseph W. Hogan", "title": "HIV dynamics and natural history studies: Joint modeling with doubly\n  interval-censored event time and infrequent longitudinal data", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS391 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 400-426", "doi": "10.1214/10-AOAS391", "report-no": "IMS-AOAS-AOAS391", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatitis C virus (HCV) coinfection has become one of the most challenging\nclinical situations to manage in HIV-infected patients. Recently the effect of\nHCV coinfection on HIV dynamics following initiation of highly active\nantiretroviral therapy (HAART) has drawn considerable attention. Post-HAART HIV\ndynamics are commonly studied in short-term clinical trials with frequent data\ncollection design. For example, the elimination process of plasma virus during\ntreatment is closely monitored with daily assessments in viral dynamics studies\nof AIDS clinical trials. In this article instead we use infrequent cohort data\nfrom long-term natural history studies and develop a model for characterizing\npost-HAART HIV dynamics and their associations with HCV coinfection.\nSpecifically, we propose a joint model for doubly interval-censored data for\nthe time between HAART initiation and viral suppression, and the longitudinal\nCD4 count measurements relative to the viral suppression. Inference is\naccomplished using a fully Bayesian approach. Doubly interval-censored data are\nmodeled semiparametrically by Dirichlet process priors and Bayesian penalized\nsplines are used for modeling population-level and individual-level mean CD4\ncount profiles. We use the proposed methods and data from the HIV Epidemiology\nResearch Study (HERS) to investigate the effect of HCV coinfection on the\nresponse to HAART.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 10:55:55 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Su", "Li", ""], ["Hogan", "Joseph W.", ""]]}, {"id": "1105.0685", "submitter": "Andrew Hart PhD", "authors": "Andrew Hart, Servet Mart\\'inez, Felipe Olmos", "title": "A Gibbs approach to Chargaff's second parity rule", "comments": "16 pages", "journal-ref": "J. Statist. Phys. 146(2), 408-422, 2012", "doi": "10.1007/s10955-011-0377-6", "report-no": null, "categories": "math.PR math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chargaff's second parity rule (CSPR) asserts that the frequencies of short\npolynucleotide chains are the same as those of the complementary reversed\nchains. Up to now, this hypothesis has only been observed empirically and there\nis currently no explanation for its presence in DNA strands. Here we argue that\nCSPR is a probabilistic consequence of the reverse complementarity between\npaired strands, because the Gibbs distribution associated with the chemical\nenergy between the bonds satisfies CSPR. We develop a statistical test to study\nthe validity of CSPR under the Gibbsian assumption and we apply it to a large\nset of bacterial genomes taken from the GenBank repository.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 20:23:24 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Hart", "Andrew", ""], ["Mart\u00ednez", "Servet", ""], ["Olmos", "Felipe", ""]]}, {"id": "1105.0695", "submitter": "Stanley Lazic", "authors": "Stanley E. Lazic", "title": "Modelling hippocampal neurogenesis across the lifespan in seven species", "comments": "In press at Neurobiology of Aging", "journal-ref": null, "doi": "10.1016/j.neurobiolaging.2011.03.008", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study was to estimate the number of new cells and neurons\nadded to the dentate gyrus across the lifespan, and to compare the rate of\nage-associated decline in neurogenesis across species. Data from mice (Mus\nmusculus), rats (Rattus norvegicus), lesser hedgehog tenrecs (Echinops\ntelfairi), macaques (Macaca mulatta), marmosets (Callithrix jacchus), tree\nshrews (Tupaia belangeri), and humans (Homo sapiens) were extracted from twenty\none data sets published in fourteen different papers. ANOVA, exponential,\nWeibull, and power models were fit to the data to determine which best\ndescribed the relationship between age and neurogenesis. Exponential models\nprovided a suitable fit and were used to estimate the relevant parameters. The\nrate of decrease of neurogenesis correlated with species longevity r = 0.769, p\n= 0.043), but not body mass or basal metabolic rate. Of all the cells added\npostnatally to the mouse dentate gyrus, only 8.5% (95% CI = 1.0% to 14.7%) of\nthese will be added after middle age. In addition, only 5.7% (95% CI = 0.7% to\n9.9%) of the existing cell population turns over from middle age onwards. Thus,\nrelatively few new cells are added for much of an animal's life, and only a\nproportion of these will mature into functional neurons.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 21:30:46 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Lazic", "Stanley E.", ""]]}, {"id": "1105.0755", "submitter": "Hyokun Yun", "authors": "Hyokun Yun", "title": "Using Logistic Regression to Analyze the Balance of a Game: The Case of\n  StarCraft II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the market size of online game has been increasing astonishingly\nfast, and so does the importance of good game design. In online games, usually\na human user competes with others, so the fairness of the game system to all\nusers is of great importance not to lose interests of users on the game.\nFurthermore, the emergence and success of electronic sports (e-sports) and\nprofessional gaming which specially talented gamers compete with others draws\nmore attention on whether they are competing in the fair environment. No matter\nhow fierce the debates are in the game-design community, it is rarely the case\nthat one employs statistical analysis to answer this question seriously. But\nconsidering the fact that we can easily gather large amount of user behavior\ndata on games, it seems potentially beneficial to make use of this data to aid\nmaking decisions on design problems of games. Actually, modern games do not aim\nto perfectly design the game at once: rather, they first release the game, and\nthen monitor users' behavior to better balance the game. In such a scenario,\nstatistical analysis can be particularly helpful. Specifically, we chose to\nanalyze the balance of StarCraft II, which is a very successful\nrecently-released real-time strategy (RTS) game. It is a central icon in\ncurrent e-Sports and professional gaming community: from April 1st to 15th,\nthere were 18 tournaments of StarCraft II. However, there is endless debate on\nwhether the winner of the tournament is actually superior to others, or it is\nlargely due to certain design flaws of the game. In this paper, we aim to\nanswer such a question using traditional statistical tool, logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 08:15:20 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Yun", "Hyokun", ""]]}, {"id": "1105.0780", "submitter": "Amparo Baillo", "authors": "Amparo Ba\\'illo, Laura Mart\\'inez-Mu\\~noz and Mario Mellado", "title": "Homogeneity tests for Michaelis-Menten curves with application to\n  fluorescence resonance energy transfer data", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resonance energy transfer methods are in wide use for evaluating\nprotein-protein interactions and protein conformational changes in living\ncells. Fluorescence resonance energy transfer (FRET) measures energy transfer\nas a function of the acceptor:donor ratio, generating FRET saturation curves.\nModeling these curves by Michaelis-Menten kinetics allows characterization by\ntwo parameters, which serve to evaluate apparent affinity between two proteins\nand to compare this affinity in different experimental conditions. To reduce\nthe effect of sampling variability, several statistical samples of the\nsaturation curve are generated in the same biological conditions. Here we study\nthree procedures to determine whether statistical samples in a collection are\nhomogeneous, in the sense that they are extracted from the same regression\nmodel. From the hypothesis testing viewpoint, we considered an F test and a\nprocedure based on bootstrap resampling. The third method analyzed the problem\nfrom the model selection viewpoint, and used the Akaike information criterion\n(AIC). Although we only considered the Michaelis-Menten model, all statistical\nprocedures would be applicable to any other nonlinear regression model. We\ncompared the performance of the homogeneity testing methods in a Monte Carlo\nstudy and through analysis in living cells of FRET saturation curves for\ndimeric complexes of CXCR4, a seven-transmembrane receptor of the G\nprotein-coupled receptor family. We show that the F test, the bootstrap\nprocedure and the model selection method lead in general to similar\nconclusions, although AIC gave the best results when sample sizes were small,\nwhereas the F test and the bootstrap method were more appropriate for large\nsamples. In practice, all three methods are easy to use simultaneously and show\nconsistency, facilitating conclusions on sample homogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 09:59:08 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Ba\u00edllo", "Amparo", ""], ["Mart\u00ednez-Mu\u00f1oz", "Laura", ""], ["Mellado", "Mario", ""]]}, {"id": "1105.0828", "submitter": "Daniel Stekhoven", "authors": "Daniel J. Stekhoven and Peter B\\\"uhlmann", "title": "MissForest - nonparametric missing value imputation for mixed-type data", "comments": "Submitted to Oxford Journal's Bioinformatics on 3rd of May 2011", "journal-ref": "Bioinformatics Vol. 28 no. 1 2012, pages 112-118", "doi": "10.1093/bioinformatics/btr597", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data acquisition based on high-throughput technology is often facing\nthe problem of missing data. Algorithms commonly used in the analysis of such\nlarge-scale data often depend on a complete set. Missing value imputation\noffers a solution to this problem. However, the majority of available\nimputation methods are restricted to one type of variable only: continuous or\ncategorical. For mixed-type data the different types are usually handled\nseparately. Therefore, these methods ignore possible relations between variable\ntypes. We propose a nonparametric method which can cope with different types of\nvariables simultaneously. We compare several state of the art methods for the\nimputation of missing values. We propose and evaluate an iterative imputation\nmethod (missForest) based on a random forest. By averaging over many unpruned\nclassification or regression trees random forest intrinsically constitutes a\nmultiple imputation scheme. Using the built-in out-of-bag error estimates of\nrandom forest we are able to estimate the imputation error without the need of\na test set. Evaluation is performed on multiple data sets coming from a diverse\nselection of biological fields with artificially introduced missing values\nranging from 10% to 30%. We show that missForest can successfully handle\nmissing values, particularly in data sets including different types of\nvariables. In our comparative study missForest outperforms other methods of\nimputation especially in data settings where complex interactions and nonlinear\nrelations are suspected. The out-of-bag imputation error estimates of\nmissForest prove to be adequate in all settings. Additionally, missForest\nexhibits attractive computational efficiency and can cope with high-dimensional\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 13:53:59 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2011 14:01:38 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Stekhoven", "Daniel J.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1105.1187", "submitter": "Ali Pezeshki", "authors": "Zhenliang Zhang, Ali Pezeshki, William Moran, Stephen D. Howard, and\n  Edwin K. P. Chong", "title": "Error Probability Bounds for Balanced Binary Relay Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection error probability associated with a balanced binary\nrelay tree, where the leaves of the tree correspond to $N$ identical and\nindependent detectors. The root of the tree represents a fusion center that\nmakes the overall detection decision. Each of the other nodes in the tree are\nrelay nodes that combine two binary messages to form a single output binary\nmessage. In this way, the information from the detectors is aggregated into the\nfusion center via the intermediate relay nodes. In this context, we describe\nthe evolution of Type I and Type II error probabilities of the binary data as\nit propagates from the leaves towards the root. Tight upper and lower bounds\nfor the total error probability at the fusion center as functions of $N$ are\nderived. These characterize how fast the total error probability converges to 0\nwith respect to $N$, even if the individual sensors have error probabilities\nthat converge to 1/2.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2011 22:35:45 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["Zhang", "Zhenliang", ""], ["Pezeshki", "Ali", ""], ["Moran", "William", ""], ["Howard", "Stephen D.", ""], ["Chong", "Edwin K. P.", ""]]}, {"id": "1105.1575", "submitter": "Yuan-chin Chang yc.ivan.chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Evaluating the diagnostic powers of variables and their linear\n  combinations when the gold standard is continuous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic (ROC) curve is a very useful tool for\nanalyzing the diagnostic/classification power of instruments/classification\nschemes as long as a binary-scale gold standard is available. When the gold\nstandard is continuous and there is no confirmative threshold, ROC curve\nbecomes less useful. Hence, there are several extensions proposed for\nevaluating the diagnostic potential of variables of interest. However, due to\nthe computational difficulties of these nonparametric based extensions, they\nare not easy to be used for finding the optimal combination of variables to\nimprove the individual diagnostic power. Therefore, we propose a new measure,\nwhich extends the AUC index for identifying variables with good potential to be\nused in a diagnostic scheme. In addition, we propose a threshold gradient\ndescent based algorithm for finding the best linear combination of variables\nthat maximizes this new measure, which is applicable even when the number of\nvariables is huge. The estimate of the proposed index and its asymptotic\nproperty are studied. The performance of the proposed method is illustrated\nusing both synthesized and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 03:54:34 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1105.1758", "submitter": "Arnau Tibau Puig", "authors": "Arnau Tibau Puig and Alfred O. Hero III", "title": "Order-preserving factor analysis (OPFA)", "comments": "Technical Report - Communications and Signal Processing Laboratory,\n  University of Michigan, Ann Arbor, MI", "journal-ref": null, "doi": null, "report-no": "cspl-396", "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel factor analysis method that can be applied to the\ndiscovery of common factors shared among trajectories in multivariate time\nseries data. These factors satisfy a precedence-ordering property: certain\nfactors are recruited only after some other factors are activated.\nPrecedence-ordering arise in applications where variables are activated in a\nspecific order, which is unknown. The proposed method is based on a linear\nmodel that accounts for each factor's inherent delays and relative order. We\npresent an algorithm to fit the model in an unsupervised manner using\ntechniques from convex and non-convex optimization that enforce sparsity of the\nfactor scores and consistent precedence-order of the factor loadings. We\nillustrate the Order-Preserving Factor Analysis (OPFA) method for the problem\nof extracting precedence-ordered factors from a longitudinal (time course)\nstudy of gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 19:04:11 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Puig", "Arnau Tibau", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1105.1976", "submitter": "Emilio Seijo", "authors": "Rohit Kumar Patra, Emilio Seijo and Bodhisattva Sen", "title": "A Consistent Bootstrap Procedure for the Maximum Score Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the applicability of the bootstrap to do inference on\nManski's maximum score estimator under the full generality of the model. We\npropose three new, model-based bootstrap procedures for this problem and show\ntheir consistency. Simulation experiments are carried out to evaluate their\nperformance and to compare them with subsampling methods. Additionally, we\nprove a uniform convergence theorem for triangular arrays of random variables\ncoming from binary choice models, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 15:28:31 GMT"}, {"version": "v2", "created": "Tue, 17 May 2011 13:46:14 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 17:33:09 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 17:28:46 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2015 03:39:00 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Patra", "Rohit Kumar", ""], ["Seijo", "Emilio", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1105.2145", "submitter": "Gavin A. Schmidt", "authors": "Gavin A. Schmidt, Michael E. Mann, Scott D. Rutherford", "title": "Discussion of: A statistical analysis of multiple temperature proxies:\n  Are reconstructions of surface temperatures over the last 1000 years\n  reliable?", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS398D the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 65-70", "doi": "10.1214/10-AOAS398D", "report-no": "IMS-AOAS-AOAS398D", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A statistical analysis of multiple temperature proxies: Are\nreconstructions of surface temperatures over the last 1000 years reliable?\" by\nB.B. McShane and A.J. Wyner [arXiv:1104.4002]\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 10:18:25 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Schmidt", "Gavin A.", ""], ["Mann", "Michael E.", ""], ["Rutherford", "Scott D.", ""]]}, {"id": "1105.2150", "submitter": "Hung Hung", "authors": "Hung Hung, Chen-Chien Wang", "title": "Matrix Variate Logistic Regression Model with Application to EEG Data", "comments": "19 pages, 1 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression has been widely applied in the field of biomedical\nresearch for a long time. In some applications, covariates of interest have a\nnatural structure, such as being a matrix, at the time of collection. The rows\nand columns of the covariate matrix then have certain physical meanings, and\nthey must contain useful information regarding the response. If we simply stack\nthe covariate matrix as a vector and fit the conventional logistic regression\nmodel, relevant information can be lost, and the problem of inefficiency will\narise. Motivated from these reasons, we propose in this paper the matrix\nvariate logistic (MV-logistic) regression model. Advantages of MV-logistic\nregression model include the preservation of the inherent matrix structure of\ncovariates and the parsimony of parameters needed. In the EEG Database Data\nSet, we successfully extract the structural effects of covariate matrix, and a\nhigh classification accuracy is achieved.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 10:44:13 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 04:09:39 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Hung", "Hung", ""], ["Wang", "Chen-Chien", ""]]}, {"id": "1105.2204", "submitter": "William Astle", "authors": "William Astle, Maria De Iorio, Sylvia Richardson, David Stephens and\n  Timothy Ebbels", "title": "A Bayesian Model of NMR Spectra for the Deconvolution and Quantification\n  of Metabolites in Complex Biological Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear Magnetic Resonance (NMR) spectra are widely used in metabolomics to\nobtain profiles of metabolites dissolved in biofluids such as cell\nsupernatants. Methods for estimating metabolite concentrations from these\nspectra are presently confined to manual peak fitting and to binning procedures\nfor integrating resonance peaks. Extensive information on the patterns of\nspectral resonance generated by human metabolites is now available in online\ndatabases. By incorporating this information into a Bayesian model we can\ndeconvolve resonance peaks from a spectrum and obtain explicit concentration\nestimates for the corresponding metabolites. Spectral resonances that cannot be\ndeconvolved in this way may also be of scientific interest so we model them\njointly using wavelets.\n  We describe a Markov chain Monte Carlo algorithm which allows us to sample\nfrom the joint posterior distribution of the model parameters, using\nspecifically designed block updates to improve mixing. The strong prior on\nresonance patterns allows the algorithm to identify peaks corresponding to\nparticular metabolites automatically, eliminating the need for manual peak\nassignment.\n  We assess our method for peak alignment and concentration estimation. Except\nin cases when the target resonance signal is very weak, alignment is unbiased\nand precise. We compare the Bayesian concentration estimates to those obtained\nfrom a conventional numerical integration method and find that our point\nestimates have sixfold lower mean squared error.\n  Finally, we apply our method to a spectral dataset taken from an\ninvestigation of the metabolic response of yeast to recombinant protein\nexpression. We estimate the concentrations of 26 metabolites and compare to\nmanual quantification by five expert spectroscopists. We discuss the reason for\ndiscrepancies and the robustness of our methods concentration estimates.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 14:24:04 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 23:48:33 GMT"}, {"version": "v3", "created": "Tue, 15 May 2012 18:36:56 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Astle", "William", ""], ["De Iorio", "Maria", ""], ["Richardson", "Sylvia", ""], ["Stephens", "David", ""], ["Ebbels", "Timothy", ""]]}, {"id": "1105.2433", "submitter": "Blakeley B. McShane", "authors": "Blakeley B. McShane, Abraham J. Wyner", "title": "Rejoinder: A statistical analysis of multiple temperature proxies: Are\n  reconstructions of surface temperatures over the last 1000 years reliable?", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS398REJ the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 99-123", "doi": "10.1214/10-AOAS398REJ", "report-no": "IMS-AOAS-AOAS398REJ", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"A statistical analysis of multiple temperature proxies: Are\nreconstructions of surface temperatures over the last 1000 years reliable?\" by\nB.B. McShane and A.J. Wyner [arXiv:1104.4002]\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2011 11:44:07 GMT"}], "update_date": "2011-05-13", "authors_parsed": [["McShane", "Blakeley B.", ""], ["Wyner", "Abraham J.", ""]]}, {"id": "1105.2965", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Sergey Kirshner", "title": "Generating Similar Graphs From Spherical Features", "comments": "29 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for generating graphs similar to a given example\ngraph. Unlike standard approaches that compute features of graphs in Euclidean\nspace, our approach obtains features on a surface of a hypersphere. We then\nutilize a von Mises-Fisher distribution, an exponential family distribution on\nthe surface of a hypersphere, to define a model over possible feature values.\nWhile our approach bears similarity to a popular exponential random graph model\n(ERGM), unlike ERGMs, it does not suffer from degeneracy, a situation when a\nsignificant probability mass is placed on unrealistic graphs. We propose a\nparameter estimation approach for our model, and a procedure for drawing\nsamples from the distribution. We evaluate the performance of our approach both\non the small domain of all 8-node graphs as well as larger real-world social\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 20:23:45 GMT"}, {"version": "v2", "created": "Thu, 19 May 2011 03:26:10 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Lunga", "Dalton", ""], ["Kirshner", "Sergey", ""]]}, {"id": "1105.2976", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Current Trends in Evolving Specialization in UK Universities", "comments": "58th World Statistics Congress of the International Statistical\n  Institute, invited plenary presentation, IPS057, Data Mining and Machine\n  Learning in Statistics Organizations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are very significant changes taking place in the university sector and\nin related higher education institutes in many parts of the world. In this work\nwe look at financial data from 2010 and 2011 from the UK higher education\nsector. Situating ourselves to begin with in the context of teaching versus\nresearch in universities, we look at the data in order to explore the new\ndivergence between the broad agendas of teaching and research in universities.\nThe innovation agenda has become at least equal to the research and teaching\nobjectives of universities. From the financial data, published in the Times\nHigher Education weekly newspaper, we explore the interesting contrast, and\nvery opposite orientations, in specialization of universities in the UK. We\nfind a polarity in specialism that goes considerably beyond the usual one of\nresearch-led elite versus more teaching-oriented new universities. Instead we\npoint to the role of medical/bioscience research income in the former, and\neconomic and business sectoral niche player roles in the latter.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 23:08:02 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2011 12:42:29 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1105.3168", "submitter": "Hao Zhu", "authors": "Hao Zhu and Georgios B. Giannakis", "title": "Lassoing Line Outages in the Smart Power Grid", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate unveiling of power line outages is of paramount importance\nnot only for preventing faults that may lead to blackouts, but also for routine\nmonitoring and control tasks of the smart grid, including state estimation and\noptimal power flow. Existing approaches are either challenged by the\n\\emph{combinatorial complexity} issues involved, and are thus limited to\nidentifying single- and double-line outages; or, they invoke less pragmatic\nassumptions such as \\emph{conditionally independent} phasor angle measurements\navailable across the grid. Using only a subset of voltage phasor angle data,\nthe present paper develops a near real-time algorithm for identifying multiple\nline outages at the affordable complexity of solving a quadratic program via\nblock coordinate descent iterations. The novel approach relies on reformulating\nthe DC linear power flow model as a \\emph{sparse} overcomplete expansion, and\nleveraging contemporary advances in compressive sampling and variable selection\nusing the least-absolute shrinkage and selection operator (Lasso). Analysis and\nsimulated tests on the standard IEEE 118-bus system confirm the effectiveness\nof lassoing line changes in the smart power grid.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2011 18:25:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhu", "Hao", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1105.3169", "submitter": "Ali Arab", "authors": "Ali Arab, Scott H. Holan, Christopher K. Wikle, Mark L. Wildhaber", "title": "Semiparametric Bivariate Zero-Inflated Poisson Models with Application\n  to Studies of Abundance for Multiple Species", "comments": "25 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological studies involving counts of abundance, presence-absence or\noccupancy rates often produce data having a substantial proportion of zeros.\nFurthermore, these types of processes are typically multivariate and only\nadequately described by complex nonlinear relationships involving externally\nmeasured covariates. Ignoring these aspects of the data and implementing\nstandard approaches can lead to models that fail to provide adequate scientific\nunderstanding of the underlying ecological processes, possibly resulting in a\nloss of inferential power. One method of dealing with data having excess zeros\nis to consider the class of univariate zero-inflated generalized linear models.\nHowever, this class of models fails to address the multivariate and nonlinear\naspects associated with the data usually encountered in practice. Therefore, we\npropose a semiparametric bivariate zero-inflated Poisson model that takes into\naccount both of these data attributes. The general modeling framework is\nhierarchical Bayes and is suitable for a broad range of applications. We\ndemonstrate the effectiveness of our model through a motivating example on\nmodeling catch per unit area for multiple species using data from the Missouri\nRiver benthic fish study, implemented by the United States Geological Survey.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2011 18:30:13 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Arab", "Ali", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""], ["Wildhaber", "Mark L.", ""]]}, {"id": "1105.4058", "submitter": "Andrea Spadaccini", "authors": "Francesco Beritelli and Andrea Spadaccini", "title": "Human Identity Verification based on Heart Sounds: Recent Advances and\n  Future Directions", "comments": "18 pages, chapter to be published in the book \"Biometrics / Book 1\",\n  ISBN 978-953-307-618-8, by InTech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Identity verification is an increasingly important process in our daily\nlives, and biometric recognition is a natural solution to the authentication\nproblem.\n  One of the most important research directions in the field of biometrics is\nthe characterization of novel biometric traits that can be used in conjunction\nwith other traits, to limit their shortcomings or to enhance their performance.\n  The aim of this work is to introduce the reader to the usage of heart sounds\nfor biometric recognition, describing the strengths and the weaknesses of this\nnovel trait and analyzing in detail the methods developed so far by different\nresearch groups and their performance.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2011 11:08:48 GMT"}], "update_date": "2011-05-23", "authors_parsed": [["Beritelli", "Francesco", ""], ["Spadaccini", "Andrea", ""]]}, {"id": "1105.4100", "submitter": "Coryn Bailer-Jones", "authors": "C.A.L. Bailer-Jones (Max Planck Institute for Astronomy, Heidelberg)", "title": "Bayesian time series analysis of terrestrial impact cratering", "comments": "Minor typos corrected in arXiv v2. Erratum (minor notation\n  corrections) corrected in arXiv v3. (Erratum available from\n  http://www.mpia-hd.mpg.de/~calj/craterTS_erratum.pdf)", "journal-ref": "MNRAS 416, 1163-1180 (2011)", "doi": "10.1111/j.1365-2966.2011.19112.x", "report-no": null, "categories": "astro-ph.EP astro-ph.IM physics.data-an physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Giant impacts by comets and asteroids have probably had an important\ninfluence on terrestrial biological evolution. We know of around 180 high\nvelocity impact craters on the Earth with ages up to 2400Myr and diameters up\nto 300km. Some studies have identified a periodicity in their age distribution,\nwith periods ranging from 13 to 50Myr. It has further been claimed that such\nperiods may be causally linked to a periodic motion of the solar system through\nthe Galactic plane. However, many of these studies suffer from methodological\nproblems, for example misinterpretation of p-values, overestimation of\nsignificance in the periodogram or a failure to consider plausible alternative\nmodels. Here I develop a Bayesian method for this problem in which impacts are\ntreated as a stochastic phenomenon. Models for the time variation of the impact\nprobability are defined and the evidence for them in the geological record is\ncompared using Bayes factors. This probabilistic approach obviates the need for\nad hoc statistics, and also makes explicit use of the age uncertainties. I find\nstrong evidence for a monotonic decrease in the recorded impact rate going back\nin time over the past 250Myr for craters larger than 5km. The same is found for\nthe past 150Myr when craters with upper age limits are included. This is\nconsistent with a crater preservation/discovery bias modulating an otherwise\nconstant impact rate. The set of craters larger than 35km (so less affected by\nerosion and infilling) and younger than 400Myr are best explained by a constant\nimpact probability model. A periodic variation in the cratering rate is\nstrongly disfavoured in all data sets. There is also no evidence for a\nperiodicity superimposed on a constant rate or trend, although this more\ncomplex signal would be harder to distinguish.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2011 14:13:19 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2011 13:26:11 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2011 11:08:58 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Bailer-Jones", "C. A. L.", "", "Max Planck Institute for Astronomy, Heidelberg"]]}, {"id": "1105.4151", "submitter": "Gautam Thakur", "authors": "Gautam S. Thakur, Pan Hui, Hamed Ketabdar, Ahmed Helmy", "title": "Towards Realistic Vehicular Network Modeling Using Planet-scale Public\n  Webcams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic modeling of vehicular mobility has been particularly challenging\ndue to a lack of large libraries of measurements in the research community. In\nthis paper we introduce a novel method for large-scale monitoring, analysis,\nand identification of spatio-temporal models for vehicular mobility using the\nfreely available online webcams in cities across the globe. We collect\nvehicular mobility traces from 2,700 traffic webcams in 10 different cities for\nseveral months and generate a mobility dataset of 7.5 Terabytes consisting of\n125 million of images. To the best of our knowl- edge, this is the largest data\nset ever used in such study. To process and analyze this data, we propose an\nefficient and scalable algorithm to estimate traffic density based on\nbackground image subtraction. Initial results show that at least 82% of\nindividual cameras with less than 5% deviation from four cities follow\nLoglogistic distribution and also 94% cameras from Toronto follow gamma\ndistribution. The aggregate results from each city also demonstrate that Log-\nLogistic and gamma distribution pass the KS-test with 95% confidence.\nFurthermore, many of the camera traces exhibit long range dependence, with\nself-similarity evident in the aggregates of traffic (per city). We believe our\nnovel data collection method and dataset provide a much needed contribution to\nthe research community for realistic modeling of vehicular networks and\nmobility.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2011 12:36:46 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["Thakur", "Gautam S.", ""], ["Hui", "Pan", ""], ["Ketabdar", "Hamed", ""], ["Helmy", "Ahmed", ""]]}, {"id": "1105.4385", "submitter": "Ping Li", "authors": "Ping Li and Joshua Moore and Christian Konig", "title": "b-Bit Minwise Hashing for Large-Scale Linear SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to (seamlessly) integrate b-bit minwise hashing\nwith linear SVM to substantially improve the training (and testing) efficiency\nusing much smaller memory, with essentially no loss of accuracy. Theoretically,\nwe prove that the resemblance matrix, the minwise hashing matrix, and the b-bit\nminwise hashing matrix are all positive definite matrices (kernels).\nInterestingly, our proof for the positive definiteness of the b-bit minwise\nhashing kernel naturally suggests a simple strategy to integrate b-bit hashing\nwith linear SVM. Our technique is particularly useful when the data can not fit\nin memory, which is an increasingly critical issue in large-scale machine\nlearning. Our preliminary experimental results on a publicly available webspam\ndataset (350K samples and 16 million dimensions) verified the effectiveness of\nour algorithm. For example, the training time was reduced to merely a few\nseconds. In addition, our technique can be easily extended to many other linear\nand nonlinear machine learning applications such as logistic regression.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 01:56:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Li", "Ping", ""], ["Moore", "Joshua", ""], ["Konig", "Christian", ""]]}, {"id": "1105.5038", "submitter": "Emmanuel Guerre", "authors": "Emmanuel Guerre and Camille Sabbah", "title": "Uniform bias study and Bahadur representation for local polynomial\n  estimators of the conditional quantile function", "comments": "This version corrects an error in the proof of Lemma B.2 which was\n  pointed out by Zhongjun Qu but does not change the results of the published\n  version, Econometric Theory, 28, 2012", "journal-ref": "Econometric Theory, Volume 28, Issue 1 February 2012 , pp. 87-129", "doi": "10.1017/S0266466611000132", "report-no": null, "categories": "math.ST stat.AP stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the bias and the weak Bahadur representation of a\nlocal polynomial estimator of the conditional quantile function and its\nderivatives. The bias and Bahadur remainder term are studied uniformly with\nrespect to the quantile level, the covariates and the smoothing parameter. The\norder of the local polynomial estimator can be higher than the\ndifferentiability order of the conditional quantile function. Applications of\nthe results deal with global optimal consistency rates of the local polynomial\nquantile estimator, performance of random bandwidths and estimation of the\nconditional quantile density function. The latter allows to obtain a simple\nestimator of the conditional quantile function of the private values in a first\nprice sealed bids auctions under the independent private values paradigm and\nrisk neutrality.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2011 14:32:11 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 14:28:39 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Guerre", "Emmanuel", ""], ["Sabbah", "Camille", ""]]}, {"id": "1105.5250", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl, Ludwig Fahrmeir, Thomas Kneib", "title": "Spike-and-Slab Priors for Function Selection in Structured Additive\n  Regression Models", "comments": null, "journal-ref": "Journal of the American Statistical Association (2012), 107:500,\n  pages 1518--1532", "doi": "10.1080/01621459.2012.737742", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured additive regression provides a general framework for complex\nGaussian and non-Gaussian regression models, with predictors comprising\narbitrary combinations of nonlinear functions and surfaces, spatial effects,\nvarying coefficients, random effects and further regression terms. The large\nflexibility of structured additive regression makes function selection a\nchallenging and important task, aiming at (1) selecting the relevant\ncovariates, (2) choosing an appropriate and parsimonious representation of the\nimpact of covariates on the predictor and (3) determining the required\ninteractions. We propose a spike-and-slab prior structure for function\nselection that allows to include or exclude single coefficients as well as\nblocks of coefficients representing specific model terms. A novel\nmultiplicative parameter expansion is required to obtain good mixing and\nconvergence properties in a Markov chain Monte Carlo simulation approach and is\nshown to induce desirable shrinkage properties. In simulation studies and with\n(real) benchmark classification data, we investigate sensitivity to\nhyperparameter settings and compare performance to competitors. The flexibility\nand applicability of our approach are demonstrated in an additive piecewise\nexponential model with time-varying effects for right-censored survival times\nof intensive care patients with sepsis. Geoadditive and additive mixed logit\nmodel applications are discussed in an extensive appendix.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2011 10:34:22 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2011 16:54:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Scheipl", "Fabian", ""], ["Fahrmeir", "Ludwig", ""], ["Kneib", "Thomas", ""]]}, {"id": "1105.5542", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie and Hans-Andrea Loeliger", "title": "Monte Carlo Algorithms for the Partition Function and Information Rates\n  of Two-Dimensional Channels", "comments": null, "journal-ref": "IEEE Trans. on Information Theory, Volume 59, Jan. 2013, pp.\n  495-503", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes Monte Carlo algorithms for the computation of the\ninformation rate of two-dimensional source/channel models. The focus of the\npaper is on binary-input channels with constraints on the allowed input\nconfigurations. The problem of numerically computing the information rate, and\neven the noiseless capacity, of such channels has so far remained largely\nunsolved. Both problems can be reduced to computing a Monte Carlo estimate of a\npartition function. The proposed algorithms use tree-based Gibbs sampling and\nmultilayer (multitemperature) importance sampling. The viability of the\nproposed algorithms is demonstrated by simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 12:36:42 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 16:33:38 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Molkaraie", "Mehdi", ""], ["Loeliger", "Hans-Andrea", ""]]}, {"id": "1105.5738", "submitter": "Philippe Heinrich", "authors": "Ph. Heinrich and J. Kahn and L. H\\'eliot and D. Trinel", "title": "Remarks on the statistical study of protein-protein interaction in\n  living cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we focus on a selection model problem: a mono-exponential model\nversus a bi-exponential one. This is done in the biological context of living\ncells, where small data are available. Classical statistics are revisited to\nimprove existing results. Some unavoidable limits are also pointed out.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2011 21:02:27 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Heinrich", "Ph.", ""], ["Kahn", "J.", ""], ["H\u00e9liot", "L.", ""], ["Trinel", "D.", ""]]}, {"id": "1105.5803", "submitter": "Philip Stark", "authors": "Josh Benaloh, Douglas Jones, Eric Lazarus, Mark Lindeman, and Philip\n  B. Stark", "title": "SOBA: Secrecy-preserving Observable Ballot-level Audit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SOBA is an approach to election verification that provides observers with\njustifiably high confidence that the reported results of an election are\nconsistent with an audit trail (\"ballots\"), which can be paper or electronic.\nSOBA combines three ideas: (1) publishing cast vote records (CVRs) separately\nfor each contest, so that anyone can verify that each reported contest outcome\nis correct, if the CVRs reflect voters' intentions with sufficient accuracy;\n(2) shrouding a mapping between ballots and the CVRs for those ballots to\nprevent the loss of privacy that could occur otherwise; (3) assessing the\naccuracy with which the CVRs reflect voters' intentions for a collection of\ncontests while simultaneously assessing the integrity of the shrouded mapping\nbetween ballots and CVRs by comparing randomly selected ballots to the CVRs\nthat purport to represent them. Step (1) is related to work by the Humboldt\nCounty Election Transparency Project, but publishing CVRs separately for\nindividual contests rather than images of entire ballots preserves privacy.\nStep (2) requires a cryptographic commitment from elections officials.\nObservers participate in step (3), which relies on the \"super-simple\nsimultaneous single-ballot risk-limiting audit.\" Step (3) is designed to reveal\nrelatively few ballots if the shrouded mapping is proper and the CVRs\naccurately reflect voter intent. But if the reported outcomes of the contests\ndiffer from the outcomes that a full hand count would show, step (3) is\nguaranteed to have a large chance of requiring all the ballots to be counted by\nhand, thereby limiting the risk that an incorrect outcome will become official\nand final.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2011 17:00:20 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2011 21:46:34 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Benaloh", "Josh", ""], ["Jones", "Douglas", ""], ["Lazarus", "Eric", ""], ["Lindeman", "Mark", ""], ["Stark", "Philip B.", ""]]}, {"id": "1105.5821", "submitter": "Anil Raj", "authors": "Anil Raj, Michael Dewar, Gustavo Palacios, Raul Rabadan and Chris H.\n  Wiggins", "title": "Identifying Hosts of Families of Viruses: A Machine Learning Approach", "comments": "11 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0027631", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Identifying viral pathogens and characterizing their transmission is\nessential to developing effective public health measures in response to a\npandemic. Phylogenetics, though currently the most popular tool used to\ncharacterize the likely host of a virus, can be ambiguous when studying species\nvery distant to known species and when there is very little reliable sequence\ninformation available in the early stages of the pandemic. Motivated by an\nexisting framework for representing biological sequence information, we learn\nsparse, tree-structured models, built from decision rules based on\nsubsequences, to predict viral hosts from protein sequence data using popular\ndiscriminative machine learning tools. Furthermore, the predictive motifs\nrobustly selected by the learning algorithm are found to show strong\nhost-specificity and occur in highly conserved regions of the viral proteome.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2011 19:36:40 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Raj", "Anil", ""], ["Dewar", "Michael", ""], ["Palacios", "Gustavo", ""], ["Rabadan", "Raul", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1105.5887", "submitter": "Francois Orieux", "authors": "F. Orieux, and O. F\\'eron, and J.-F. Giovannelli", "title": "Efficient sampling of high-dimensional Gaussian fields: the\n  non-stationary / non-sparse case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the problem of sampling Gaussian fields in high\ndimension. Solutions exist for two specific structures of inverse covariance :\nsparse and circulant. The proposed approach is valid in a more general case and\nespecially as it emerges in inverse problems. It relies on a\nperturbation-optimization principle: adequate stochastic perturbation of a\ncriterion and optimization of the perturbed criterion. It is shown that the\ncriterion minimizer is a sample of the target density. The motivation in\ninverse problems is related to general (non-convolutive) linear observation\nmodels and their resolution in a Bayesian framework implemented through\nsampling algorithms when existing samplers are not feasible. It finds a direct\napplication in myopic and/or unsupervised inversion as well as in some\nnon-Gaussian inversion. An illustration focused on hyperparameter estimation\nfor super-resolution problems assesses the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 07:31:01 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Orieux", "F.", ""], ["F\u00e9ron", "O.", ""], ["Giovannelli", "J. -F.", ""]]}, {"id": "1105.6061", "submitter": "Premkumar Karumbu", "authors": "Premkumar Karumbu, Anurag Kumar, Joy Kuri", "title": "Distributed Detection/Isolation Procedures for Quickest Event Detection\n  in Large Extent Wireless Sensor Networks", "comments": "Submitted to IEEE Transactions on Signal Processing, Mar. 10, 2011.\n  Revised on Jul. 17, 2011. A part of this work was presented in Forty-Seventh\n  Annual Allerton Conference on Communication, Control, and Computing,\n  Monticello, IL, USA, Sep. - Oct. 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a problem of distributed detection of a stationary point event in a\nlarge extent wireless sensor network ($\\wsn$), where the event influences the\nobservations of the sensors only in the vicinity of where it occurs. An event\noccurs at an unknown time and at a random location in the coverage region (or\nregion of interest ($\\ROI$)) of the $\\wsn$. We consider a general sensing model\nin which the effect of the event at a sensor node depends on the distance\nbetween the event and the sensor node; in particular, in the Boolean sensing\nmodel, all sensors in a disk of a given radius around the event are equally\naffected. Following the prior work reported in\n\\cite{nikiforov95change_isolation},\n\\cite{nikiforov03lower-bound-for-det-isolation},\n\\cite{tartakovsky08multi-decision}, {\\em the problem is formulated as that of\ndetecting the event and locating it to a subregion of the $\\ROI$ as early as\npossible under the constraints that the average run length to false alarm\n($\\tfa$) is bounded below by $\\gamma$, and the probability of false isolation\n($\\pfi$) is bounded above by $\\alpha$}, where $\\gamma$ and $\\alpha$ are target\nperformance requirements. In this setting, we propose distributed procedures\nfor event detection and isolation (namely $\\mx$, $\\all$, and $\\hall$), based on\nthe local fusion of $\\CUSUM$s at the sensors. For these procedures, we obtain\nbounds on the maximum mean detection/isolation delay ($\\add$), and on $\\tfa$\nand $\\pfi$, and thus provide an upper bound on $\\add$ as\n$\\min\\{\\gamma,1/\\alpha\\} \\to \\infty$. For the Boolean sensing model, we show\nthat an asymptotic upper bound on the maximum mean detection/isolation delay of\nour distributed procedure scales with $\\gamma$ and $\\alpha$ in the same way as\nthe asymptotically optimal centralised procedure\n\\cite{nikiforov03lower-bound-for-det-isolation}.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 18:30:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 10:26:02 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Karumbu", "Premkumar", ""], ["Kumar", "Anurag", ""], ["Kuri", "Joy", ""]]}, {"id": "1105.6265", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Buda", "title": "Hierarchical structure in phonographic market", "comments": "10 pages, 3 figures, 2 tables, 2 pictures, presented at FENS 2010 in\n  Warsaw, chapter of book \"Life-time Of Correlation And Its Application (volume\n  1)\"", "journal-ref": "A. Buda, A. Jarynowski, Life-time Of Correlation And Its\n  Application (volume 1), Wydawnictwo Niezalezne, Wroclaw 2010, ISBN\n  978-83915272-9-0", "doi": null, "report-no": null, "categories": "q-fin.GN cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I find a topological arrangement of assets traded in a phonographic market\nwhich has associated a meaningful economic taxonomy. I continue using the\nMinimal Spanning Tree and the Life-time Of Correlations between assets, but now\noutside the stock markets. This is the first attempt to use these methods on\nphonographic market where we have artists instead of stocks. The value of an\nartist is defined by record sales. The graph is obtained starting from the\nmatrix of correlations coefficient computed between the world's most popular 30\nartists by considering the synchronous time evolution of the difference of the\nlogarithm of weekly record sales. This method provides the hierarchical\nstructure of phonographic market and information on which music genre is\nmeaningful according to customers.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 13:14:50 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Buda", "Andrzej", ""]]}, {"id": "1105.6272", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Buda", "title": "Life time of correlation between stocks prices on established and\n  emerging markets", "comments": "17 pages, 9 figures, 1 table; presented at FENS conference in Wroclaw\n  2007 and Rzeszow 208; chapter in book: \"Life-time Of Correlation And Its\n  Application (volume 1)\"", "journal-ref": "A. Buda, A. Jarynowski,Life-time Of Correlation And Its\n  Application (volume 1), Wydawnictwo Niezalezne, Wroclaw 2010, ISBN\n  978-83915272-9-0", "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correlation coefficient between stocks depends on price history and\nincludes information on hierarchical structure in financial markets. It is\nuseful for portfolio selection and estimation of risk. I introduce the Life\nTime of Correlation between stocks prices to know how far we should investigate\nthe price history to obtain the optimal durability of correlation. I carry out\nmy research on emerging (Poland) and established markets (in the USA, Great\nBritain and Germany). Other methods, including the Minimum Spanning Trees, tree\nhalf-life, decomposition of correlations and the Epps effect are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 13:30:38 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Buda", "Andrzej", ""]]}, {"id": "1105.6344", "submitter": "Joseph W. Richards", "authors": "Joseph W. Richards, Ann B. Lee, Chad M. Schafer, Peter E. Freeman", "title": "Prototype selection for parameter estimation in complex models", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS500 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 383-408", "doi": "10.1214/11-AOAS500", "report-no": "IMS-AOAS-AOAS500", "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation in astrophysics often requires the use of complex\nphysical models. In this paper we study the problem of estimating the\nparameters that describe star formation history (SFH) in galaxies. Here,\nhigh-dimensional spectral data from galaxies are appropriately modeled as\nlinear combinations of physical components, called simple stellar populations\n(SSPs), plus some nonlinear distortions. Theoretical data for each SSP is\nproduced for a fixed parameter vector via computer modeling. Though the\nparameters that define each SSP are continuous, optimizing the signal model\nover a large set of SSPs on a fine parameter grid is computationally infeasible\nand inefficient. The goal of this study is to estimate the set of parameters\nthat describes the SFH of each galaxy. These target parameters, such as the\naverage ages and chemical compositions of the galaxy's stellar populations, are\nderived from the SSP parameters and the component weights in the signal model.\nHere, we introduce a principled approach of choosing a small basis of SSP\nprototypes for SFH parameter estimation. The basic idea is to quantize the\nvector space and effective support of the model components. In addition to\ngreater computational efficiency, we achieve better estimates of the SFH target\nparameters. In simulations, our proposed quantization method obtains a\nsubstantial improvement in estimating the target parameters over the common\nmethod of employing a parameter grid. Sparse coding techniques are not\nappropriate for this problem without proper constraints, while constrained\nsparse coding methods perform poorly for parameter estimation because their\nobjective is signal reconstruction, not estimation of the target parameters.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 17:17:15 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2011 17:52:52 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2012 15:07:21 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Richards", "Joseph W.", ""], ["Lee", "Ann B.", ""], ["Schafer", "Chad M.", ""], ["Freeman", "Peter E.", ""]]}]