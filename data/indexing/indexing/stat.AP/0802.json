[{"id": "0802.0113", "submitter": "Nicolas Barbey", "authors": "Nicolas Barbey (IAS, LSS), Fr\\'ed\\'eric Auch\\`ere (IAS), Thomas Rodet\n  (LSS), Jean-Claude Vial (IAS)", "title": "A Time-Evolving 3D Method Dedicated to the Reconstruction of Solar\n  plumes and Results Using Extreme Ultra-Violet Data", "comments": null, "journal-ref": null, "doi": "10.1007/s11207-008-9151-6", "report-no": null, "categories": "astro-ph stat.AP", "license": null, "abstract": "  An important issue in the tomographic reconstruction of the solar poles is\nthe relatively rapid evolution of the polar plumes. We demonstrate that it is\npossible to take into account this temporal evolution in the reconstruction.\nThe difficulty of this problem comes from the fact that we want a 4D\nreconstruction (three spatial dimensions plus time) while we only have 3D data\n(2D images plus time). To overcome this difficulty, we introduce a model that\ndescribes polar plumes as stationary objects whose intensity varies\nhomogeneously with time. This assumption can be physically justified if one\naccepts the stability of the magnetic structure. This model leads to a bilinear\ninverse problem. We describe how to extend linear inversion methods to these\nkinds of problems. Studies of simulations show the reliability of our method.\nResults for SOHO/EIT data show that we are able to estimate the temporal\nevolution of polar plumes in order to improve the reconstruction of the solar\npoles from only one point of view. We expect further improvements from\nSTEREO/EUVI data when the two probes will be separated by about 60 degrees.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 12:38:12 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Barbey", "Nicolas", "", "IAS, LSS"], ["Auch\u00e8re", "Fr\u00e9d\u00e9ric", "", "IAS"], ["Rodet", "Thomas", "", "LSS"], ["Vial", "Jean-Claude", "", "IAS"]]}, {"id": "0802.0191", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Covariance estimation for multivariate conditionally Gaussian dynamic\n  linear models", "comments": "21 pages, 2 figures, 6 tables", "journal-ref": "Journal of Forecasting (2007), 26(8), pp. 551-569.", "doi": "10.1002/for.1039", "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  In multivariate time series, the estimation of the covariance matrix of the\nobservation innovations plays an important role in forecasting as it enables\nthe computation of the standardized forecast error vectors as well as it\nenables the computation of confidence bounds of the forecasts. We develop an\non-line, non-iterative Bayesian algorithm for estimation and forecasting. It is\nempirically found that, for a range of simulated time series, the proposed\ncovariance estimator has good performance converging to the true values of the\nunknown observation covariance matrix. Over a simulated time series, the new\nmethod approximates the correct estimates, produced by a non-sequential Monte\nCarlo simulation procedure, which is used here as the gold standard. The\nspecial, but important, vector autoregressive (VAR) and time-varying VAR models\nare illustrated by considering London metal exchange data consisting of spot\nprices of aluminium, copper, lead and zinc.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 19:48:03 GMT"}], "update_date": "2008-02-04", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0213", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos and P.J. Harrison", "title": "Posterior mean and variance approximation for regression and time series\n  problems", "comments": "25 pages, 2 figures, 2 tables", "journal-ref": "Statistics (2008), 42, pp. 329-350.", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  This paper develops a methodology for approximating the posterior first two\nmoments of the posterior distribution in Bayesian inference. Partially\nspecified probability models, which are defined only by specifying means and\nvariances, are constructed based upon second-order conditional independence, in\norder to facilitate posterior updating and prediction of required\ndistributional quantities. Such models are formulated particularly for\nmultivariate regression and time series analysis with unknown observational\nvariance-covariance components. The similarities and differences of these\nmodels with the Bayes linear approach are established. Several subclasses of\nimportant models, including regression and time series models with errors\nfollowing multivariate $t$, inverted multivariate $t$ and Wishart\ndistributions, are discussed in detail. Two numerical examples consisting of\nsimulated data and of US investment and change in inventory data illustrate the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 22:18:34 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Triantafyllopoulos", "K.", ""], ["Harrison", "P. J.", ""]]}, {"id": "0802.0214", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Multivariate stochastic volatility with Bayesian dynamic linear models", "comments": "24 pages, 3 figures, 2 tables", "journal-ref": "Journal of Statistical Planning and Inference (2008), 138(4), pp.\n  1021-1037", "doi": "10.1016/j.jspi.2007.03.057", "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": null, "abstract": "  This paper develops a Bayesian procedure for estimation and forecasting of\nthe volatility of multivariate time series. The foundation of this work is the\nmatrix-variate dynamic linear model, for the volatility of which we adopt a\nmultiplicative stochastic evolution, using Wishart and singular multivariate\nbeta distributions. A diagonal matrix of discount factors is employed in order\nto discount the variances element by element and therefore allowing a flexible\nand pragmatic variance modelling approach. Diagnostic tests and sequential\nmodel monitoring are discussed in some detail. The proposed estimation theory\nis applied to a four-dimensional time series, comprising spot prices of\naluminium, copper, lead and zinc of the London metal exchange. The empirical\nfindings suggest that the proposed Bayesian procedure can be effectively\napplied to financial data, overcoming many of the disadvantages of existing\nvolatility models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 22:35:49 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0218", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Multivariate control charts based on Bayesian state space models", "comments": "19 pages, 6 figures", "journal-ref": "Quality and Reliability Engineering International (2006), 22(6),\n  pp. 693-707", "doi": "10.1002/qre.807", "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  This paper develops a new multivariate control charting method for vector\nautocorrelated and serially correlated processes. The main idea is to propose a\nBayesian multivariate local level model, which is a generalization of the\nShewhart-Deming model for autocorrelated processes, in order to provide the\npredictive error distribution of the process and then to apply a univariate\nmodified EWMA control chart to the logarithm of the Bayes' factors of the\npredictive error density versus the target error density. The resulting chart\nis proposed as capable to deal with both the non-normality and the\nautocorrelation structure of the log Bayes' factors. The new control charting\nscheme is general in application and it has the advantage to control\nsimultaneously not only the process mean vector and the dispersion covariance\nmatrix, but also the entire target distribution of the process. Two examples of\nLondon metal exchange data and of production time series data illustrate the\ncapabilities of the new control chart.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 22:46:05 GMT"}], "update_date": "2008-02-05", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0219", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Dynamic generalized linear models for non-Gaussian time series\n  forecasting", "comments": "38 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  The purpose of this paper is to provide a discussion, with illustrating\nexamples, on Bayesian forecasting for dynamic generalized linear models\n(DGLMs). Adopting approximate Bayesian analysis, based on conjugate forms and\non Bayes linear estimation, we describe the theoretical framework and then we\nprovide detailed examples of response distributions, including binomial,\nPoisson, negative binomial, geometric, normal, log-normal, gamma, exponential,\nWeibull, Pareto, beta, and inverse Gaussian. We give numerical illustrations\nfor all distributions (except for the normal). Putting together all the above\ndistributions, we give a unified Bayesian approach to non-Gaussian time series\nanalysis, with applications from finance and medicine to biology and the\nbehavioural sciences. Throughout the models we discuss Bayesian forecasting\nand, for each model, we derive the multi-step forecast mean. Finally, we\ndescribe model assessment using the likelihood function, and Bayesian model\nmonitoring.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 22:52:41 GMT"}], "update_date": "2008-02-05", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0220", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Forecasting with time-varying vector autoregressive models", "comments": "17 pages, 7 figures, tables 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": null, "abstract": "  The purpose of this paper is to propose a time-varying vector autoregressive\nmodel (TV-VAR) for forecasting multivariate time series. The model is casted\ninto a state-space form that allows flexible description and analysis. The\nvolatility covariance matrix of the time series is modelled via inverted\nWishart and singular multivariate beta distributions allowing a fully conjugate\nBayesian inference. Model performance and model comparison is done via the\nlikelihood function, sequential Bayes factors, the mean of squared standardized\nforecast errors, the mean of absolute forecast errors (known also as mean\nabsolute deviation), and the mean forecast error. Bayes factors are also used\nin order to choose the autoregressive order of the model. Multi-step\nforecasting is discussed in detail and a flexible formula is proposed to\napproximate the forecast function. Two examples, consisting of bivariate data\nof IBM shares and of foreign exchange (FX) rates for 8 currencies, illustrate\nthe methods. For the IBM data we discuss model performance and multi-step\nforecasting in some detail. For the FX data we discuss sequential portfolio\nallocation; for both data sets our empirical findings suggest that the TV-VAR\nmodels outperform the widely used VAR models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 22:58:24 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2008 12:00:10 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0223", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Multivariate stochastic volatility using state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": null, "abstract": "  A Bayesian procedure is developed for multivariate stochastic volatility,\nusing state space models. An autoregressive model for the log-returns is\nemployed. We generalize the inverted Wishart distribution to allow for\ndifferent correlation structure between the observation and state innovation\nvectors and we extend the convolution between the Wishart and the multivariate\nsingular beta distribution. A multiplicative model based on the generalized\ninverted Wishart and multivariate singular beta distributions is proposed for\nthe evolution of the volatility and a flexible sequential volatility updating\nis employed. The proposed algorithm for the volatility is fast and\ncomputationally cheap and it can be used for on-line forecasting. The methods\nare illustrated with an example consisting of foreign exchange rates data of 8\ncurrencies. The empirical results suggest that time-varying correlations can be\nestimated efficiently, even in situations of high dimensional data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2008 23:34:43 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "0802.0433", "submitter": "Mathieu Ribatet", "authors": "Mathieu Ribatet (UR HHLY, INRS), Eric Sauquet (UR HHLY), Jean-Michel\n  Gr\\'esillon (UR HHLY), Taha B.M.J. Ouarda (INRS)", "title": "A regional Bayesian POT model for flood frequency analysis", "comments": null, "journal-ref": "Stochastic Environmental Research and Risk Assessment 21, 4 (2006)\n  327-339", "doi": "10.1007/s00477-006-0068-z", "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Flood frequency analysis is usually based on the fitting of an extreme value\ndistribution to the local streamflow series. However, when the local data\nseries is short, frequency analysis results become unreliable. Regional\nfrequency analysis is a convenient way to reduce the estimation uncertainty. In\nthis work, we propose a regional Bayesian model for short record length sites.\nThis model is less restrictive than the index flood model while preserving the\nformalism of \"homogeneous regions\". The performance of the proposed model is\nassessed on a set of gauging stations in France. The accuracy of quantile\nestimates as a function of the degree of homogeneity of the pooling group is\nalso analysed. The results indicate that the regional Bayesian model\noutperforms the index flood model and local estimators. Furthermore, it seems\nthat working with relatively large and homogeneous regions may lead to more\naccurate results than working with smaller and highly homogeneous regions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 15:17:34 GMT"}], "update_date": "2008-02-05", "authors_parsed": [["Ribatet", "Mathieu", "", "UR HHLY, INRS"], ["Sauquet", "Eric", "", "UR HHLY"], ["Gr\u00e9sillon", "Jean-Michel", "", "UR HHLY"], ["Ouarda", "Taha B. M. J.", "", "INRS"]]}, {"id": "0802.0436", "submitter": "Mathieu Ribatet", "authors": "Mathieu Ribatet (UR HHLY, INRS), Taha B.M.J. Ouarda (INRS), Eric\n  Sauquet (UR HHLY), Jean-Michel Gr\\'esillon (UR HHLY)", "title": "Modeling All Exceedances Above a Threshold Using an Extremal Dependence\n  Structure: Inferences on Several Flood Characteristics", "comments": null, "journal-ref": null, "doi": "10.1029/2007WR006322", "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Flood quantile estimation is of great importance for many engineering studies\nand policy decisions. However, practitioners must often deal with small data\navailable. Thus, the information must be used optimally. In the last decades,\nto reduce the waste of data, inferential methodology has evolved from annual\nmaxima modeling to peaks over a threshold one. To mitigate the lack of data,\npeaks over a threshold are sometimes combined with additional information -\nmostly regional and historical information. However, whatever the extra\ninformation is, the most precious information for the practitioner is found at\nthe target site. In this study, a model that allows inferences on the whole\ntime series is introduced. In particular, the proposed model takes into account\nthe dependence between successive extreme observations using an appropriate\nextremal dependence structure. Results show that this model leads to more\naccurate flood peak quantile estimates than conventional estimators. In\naddition, as the time dependence is taken into account, inferences on other\nflood characteristics can be performed. An illustration is given on flood\nduration. Our analysis shows that the accuracy of the proposed models to\nestimate the flood duration is related to specific catchment characteristics.\nSome suggestions to increase the flood duration predictions are introduced.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 15:21:26 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Ribatet", "Mathieu", "", "UR HHLY, INRS"], ["Ouarda", "Taha B. M. J.", "", "INRS"], ["Sauquet", "Eric", "", "UR HHLY"], ["Gr\u00e9sillon", "Jean-Michel", "", "UR HHLY"]]}, {"id": "0802.0443", "submitter": "Mathieu Ribatet", "authors": "Bertrand Iooss (LCFR, - M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques), Mathieu Ribatet (UR HHLY), Amandine Marrel (LMTE)", "title": "Global Sensitivity Analysis of Stochastic Computer Models with joint\n  metamodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global sensitivity analysis method, used to quantify the influence of\nuncertain input variables on the response variability of a numerical model, is\napplicable to deterministic computer code (for which the same set of input\nvariables gives always the same output value). This paper proposes a global\nsensitivity analysis methodology for stochastic computer code (having a\nvariability induced by some uncontrollable variables). The framework of the\njoint modeling of the mean and dispersion of heteroscedastic data is used. To\ndeal with the complexity of computer experiment outputs, non parametric joint\nmodels (based on Generalized Additive Models and Gaussian processes) are\ndiscussed. The relevance of these new models is analyzed in terms of the\nobtained variance-based sensitivity indices with two case studies. Results show\nthat the joint modeling approach leads accurate sensitivity index estimations\neven when clear heteroscedasticity is present.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 15:31:30 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2009 12:48:49 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2009 09:36:49 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Iooss", "Bertrand", "", "LCFR, - M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques"], ["Ribatet", "Mathieu", "", "UR HHLY"], ["Marrel", "Amandine", "", "LMTE"]]}, {"id": "0802.0444", "submitter": "Mathieu Ribatet", "authors": "Mathieu Ribatet (UR HHLY, INRS), Eric Sauquet (UR HHLY), Jean-Michel\n  Gr\\'esillon (UR HHLY), Taha B.M.J. Ouarda (INRS)", "title": "Usefulness of the Reversible Jump Markov Chain Monte Carlo Model in\n  Regional Flood Frequency Analysis", "comments": null, "journal-ref": "Water Resources Research 43, 8 (2007) W08403", "doi": "10.1029/2006WR005525", "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Regional flood frequency analysis is a convenient way to reduce estimation\nuncertainty when few data are available at the gauging site. In this work, a\nmodel that allows a non-null probability to a regional fixed shape parameter is\npresented. This methodology is integrated within a Bayesian framework and uses\nreversible jump techniques. The performance on stochastic data of this new\nestimator is compared to two other models: a conventional Bayesian analysis and\nthe index flood approach. Results show that the proposed estimator is\nabsolutely suited to regional estimation when only a few data are available at\nthe target site. Moreover, unlike the index flood estimator, target site index\nflood error estimation seems to have less impact on Bayesian estimators. Some\nsuggestions about configurations of the pooling groups are also presented to\nincrease the performance of each estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 15:33:35 GMT"}], "update_date": "2008-02-05", "authors_parsed": [["Ribatet", "Mathieu", "", "UR HHLY, INRS"], ["Sauquet", "Eric", "", "UR HHLY"], ["Gr\u00e9sillon", "Jean-Michel", "", "UR HHLY"], ["Ouarda", "Taha B. M. J.", "", "INRS"]]}, {"id": "0802.0450", "submitter": "Qingzhao Yu", "authors": "Qingzhao Yu, Bin Li, Richard Scribner, Deborah Cohen", "title": "Hierarchical Additive Modeling of Nonlinear Association with Spatial\n  Correlations-An Application to Relate Alcohol Outlet Density and Neighborhood\n  Assault Rates", "comments": "26 pages, 4 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have suggested a link between alcohol outlets and assaultive\nviolence. In this paper, we explore the effects of alcohol availability on\nassault crimes at the census tract level over time. The statistical analysis is\nchallenged by several features of the data: (1) the effects of possible\ncovariates (for example, the alcohol outlet density of each census tract) on\nthe assaultive crime rates may be complex; (2) the covariates may be highly\ncorrelated with each other; (3) there are a lot of missing inputs in the data;\nand (4) spatial correlations exist in the outcome assaultive crime rates. We\npropose a hierarchical additive model, where the nonlinear correlations and the\ncomplex interaction effects are modeled using the multiple additive regression\ntrees (MART) and the spatial variances in the assaultive rates that cannot be\nexplained by the specified covariates are smoothed trough the Conditional\nAutoregressive (CAR) model. We develop a two-stage algorithm that connect the\nnon-parametric trees with CAR to look for important variables covariates\nassociated with the assaultive crime rates, while taking account of the spatial\ncorrelations among adjacent census tracts. The proposed methods are applied to\nthe Los Angeles assaultive data (1990-1999) and compared with traditional\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 15:56:31 GMT"}], "update_date": "2008-02-05", "authors_parsed": [["Yu", "Qingzhao", ""], ["Li", "Bin", ""], ["Scribner", "Richard", ""], ["Cohen", "Deborah", ""]]}, {"id": "0802.0509", "submitter": "Andrea Karis", "authors": "James Robins", "title": "Causal Models for Estimating the Effects of Weight Gain on Mortality", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose, contrary to fact, in 1950, we had put the cohort of 18 year old\nnon-smoking American men on a stringent mandatory diet that guaranteed that no\none would ever weigh more than their baseline weight established at age 18. How\nwould the counter-factual mortality of these 18 year olds have compared to\ntheir actual observed mortality through 2007? We describe in detail how this\ncounterfactual contrast could be estimated from longitudinal epidemiologic data\nsimiliar to that stored in the electronic medical records of a large HMO by\napplying g-estimation to a novel structural nested model. Our analytic approach\ndiffers from any alternative approach in that in that, in the abscence of model\nmisspecification, it can successfully adjust for (i) measured time-varying\nconfounders such as exercise, hypertension and diabetes that are simultaneously\nintermediate variables on the causal pathway from weight gain to death and\ndeterminants of future weight gain, (ii) unmeasured confounding by undiagnosed\npreclinical disease (i.e reverse causation) that can cause both poor weight\ngain and premature mortality [provided an upper bound can be specified for the\nmaximum length of time a subject may suffer from a subclinical illness severe\nenough to affect his weight without the illness becomes clinically manifest],\nand (iii) the prescence of particular identifiable subgroups, such as those\nsuffering from serious renal, liver, pulmonary, and/or cardiac disease, in whom\nconfounding by unmeasured prognostic factors so severe as to render useless any\nattempt at direct analytic adjustment.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2008 21:14:10 GMT"}], "update_date": "2008-02-06", "authors_parsed": [["Robins", "James", ""]]}, {"id": "0802.0691", "submitter": "Betsabe Blas", "authors": "Betsab\\'e G. Blas Achic, M\\^onica C. Sandoval and Olga Satomi Yoshida", "title": "Homoscedastic controlled calibration model", "comments": "LaTex, 21 pages. Includes 13 tables. Version published in Journal of\n  Chemometrics, v. 21, p. 145-155, 2007", "journal-ref": "Journal of Chemometrics, v. 21, p. 145-155, 2007", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the usual calibration model, we consider the case in which\nthe independent variable is unobservable, but a pre-fixed value on its\nsurrogate is available. Thus, considering controlled variables and assuming\nthat the measurement errors have equal variances we propose a new calibration\nmodel. Likelihood based methodology is used to estimate the model parameters\nand the Fisher information matrix is used to construct a confidence interval\nfor the unknown value of the regressor variable. A simulation study is carried\nout to asses the effect of the measurement error on the estimation of the\nparameter of interest. This new approach is illustrated with an example.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2008 18:44:45 GMT"}], "update_date": "2008-02-06", "authors_parsed": [["Achic", "Betsab\u00e9 G. Blas", ""], ["Sandoval", "M\u00f4nica C.", ""], ["Yoshida", "Olga Satomi", ""]]}, {"id": "0802.1009", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (LCFR), Mathieu Ribatet (UR HHLY, INRS)", "title": "Global sensitivity analysis of computer models with functional inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": null, "abstract": "  Global sensitivity analysis is used to quantify the influence of uncertain\ninput parameters on the response variability of a numerical model. The common\nquantitative methods are applicable to computer codes with scalar input\nvariables. This paper aims to illustrate different variance-based sensitivity\nanalysis techniques, based on the so-called Sobol indices, when some input\nvariables are functional, such as stochastic processes or random spatial\nfields. In this work, we focus on large cpu time computer codes which need a\npreliminary meta-modeling step before performing the sensitivity analysis. We\npropose the use of the joint modeling approach, i.e., modeling simultaneously\nthe mean and the dispersion of the code outputs using two interlinked\nGeneralized Linear Models (GLM) or Generalized Additive Models (GAM). The\n``mean'' model allows to estimate the sensitivity indices of each scalar input\nvariables, while the ``dispersion'' model allows to derive the total\nsensitivity index of the functional input variables. The proposed approach is\ncompared to some classical SA methodologies on an analytical function. Lastly,\nthe proposed methodology is applied to a concrete industrial computer code that\nsimulates the nuclear fuel irradiation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2008 15:34:54 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2008 07:14:01 GMT"}], "update_date": "2008-06-09", "authors_parsed": [["Iooss", "Bertrand", "", "LCFR"], ["Ribatet", "Mathieu", "", "UR HHLY, INRS"]]}, {"id": "0802.1099", "submitter": "Bertrand Iooss", "authors": "Amandine Marrel, Bertrand Iooss, Francois Van Dorpe, Elena Volkova", "title": "An efficient methodology for modeling complex computer codes with\n  Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Complex computer codes are often too time expensive to be directly used to\nperform uncertainty propagation studies, global sensitivity analysis or to\nsolve optimization problems. A well known and widely used method to circumvent\nthis inconvenience consists in replacing the complex computer code by a reduced\nmodel, called a metamodel, or a response surface that represents the computer\ncode and requires acceptable calculation time. One particular class of\nmetamodels is studied: the Gaussian process model that is characterized by its\nmean and covariance functions. A specific estimation procedure is developed to\nadjust a Gaussian process model in complex cases (non linear relations, highly\ndispersed or discontinuous output, high dimensional input, inadequate sampling\ndesigns, ...). The efficiency of this algorithm is compared to the efficiency\nof other existing algorithms on an analytical test case. The proposed\nmethodology is also illustrated for the case of a complex hydrogeological\ncomputer code, simulating radionuclide transport in groundwater.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2008 07:12:13 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2008 04:38:08 GMT"}], "update_date": "2008-04-06", "authors_parsed": [["Marrel", "Amandine", ""], ["Iooss", "Bertrand", ""], ["Van Dorpe", "Francois", ""], ["Volkova", "Elena", ""]]}, {"id": "0802.1103", "submitter": "Mingyan Huang", "authors": "Mingyan Huang, Daowen Zhang", "title": "Testing polynomial covariate effects in linear and generalized linear\n  mixed models", "comments": "Published in at http://dx.doi.org/10.1214/08-SS036 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, 154-169", "doi": "10.1214/08-SS036", "report-no": "IMS-SS-SS_2008_36", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important feature of linear mixed models and generalized linear mixed\nmodels is that the conditional mean of the response given the random effects,\nafter transformed by a link function, is linearly related to the fixed\ncovariate effects and random effects. Therefore, it is of practical importance\nto test the adequacy of this assumption, particularly the assumption of linear\ncovariate effects. In this paper, we review procedures that can be used for\ntesting polynomial covariate effects in these popular models. Specifically,\nfour types of hypothesis testing approaches are reviewed, i.e. R tests,\nlikelihood ratio tests, score tests and residual-based tests. Derivation and\nperformance of each testing procedure will be discussed, including a small\nsimulation study for comparing the likelihood ratio tests with the score tests.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2008 08:01:10 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2008 10:13:59 GMT"}], "update_date": "2008-12-29", "authors_parsed": [["Huang", "Mingyan", ""], ["Zhang", "Daowen", ""]]}, {"id": "0802.1214", "submitter": "S. L. Bridle", "authors": "Sarah Bridle, John Shawe-Taylor, Adam Amara, Douglas Applegate,\n  Sreekumar T. Balan, Joel Berge, Gary Bernstein, Hakon Dahle, Thomas Erben,\n  Mandeep Gill, Alan Heavens, Catherine Heymans, F. William High, Henk\n  Hoekstra, Mike Jarvis, Donnacha Kirk, Thomas Kitching, Jean-Paul Kneib,\n  Konrad Kuijken, David Lagatutta, Rachel Mandelbaum, Richard Massey, Yannick\n  Mellier, Baback Moghaddam, Yassir Moudden, Reiko Nakajima, Stephane\n  Paulin-Henriksson, Sandrine Pires, Anais Rassat, Alexandre Refregier, Jason\n  Rhodes, Tim Schrabback, Elisabetta Semboloni, Marina Shmakova, Ludovic van\n  Waerbeke, Dugan Witherick, Lisa Voigt, David Wittman", "title": "Handbook for the GREAT08 Challenge: An image analysis competition for\n  cosmological lensing", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS222 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 6-37", "doi": "10.1214/08-AOAS222", "report-no": "IMS-AOAS-AOAS222", "categories": "astro-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses\non a problem that is of crucial importance for future observations in\ncosmology. The shapes of distant galaxies can be used to determine the\nproperties of dark energy and the nature of gravity, because light from those\ngalaxies is bent by gravity from the intervening dark matter. The observed\ngalaxy images appear distorted, although only slightly, and their shapes must\nbe precisely disentangled from the effects of pixelisation, convolution and\nnoise. The worldwide gravitational lensing community has made significant\nprogress in techniques to measure these distortions via the Shear TEsting\nProgram (STEP). Via STEP, we have run challenges within our own community, and\ncome to recognise that this particular image analysis problem is ideally\nmatched to experts in statistical inference, inverse problems and computational\nlearning. Thus, in order to continue the progress seen in recent years, we are\nseeking an infusion of new ideas from these communities. This document details\nthe GREAT08 Challenge for potential participants. Please visit\nhttp://www.great08challenge.info for the latest information.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2008 20:57:56 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2008 22:22:17 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2009 13:04:42 GMT"}], "update_date": "2009-06-15", "authors_parsed": [["Bridle", "Sarah", ""], ["Shawe-Taylor", "John", ""], ["Amara", "Adam", ""], ["Applegate", "Douglas", ""], ["Balan", "Sreekumar T.", ""], ["Berge", "Joel", ""], ["Bernstein", "Gary", ""], ["Dahle", "Hakon", ""], ["Erben", "Thomas", ""], ["Gill", "Mandeep", ""], ["Heavens", "Alan", ""], ["Heymans", "Catherine", ""], ["High", "F. William", ""], ["Hoekstra", "Henk", ""], ["Jarvis", "Mike", ""], ["Kirk", "Donnacha", ""], ["Kitching", "Thomas", ""], ["Kneib", "Jean-Paul", ""], ["Kuijken", "Konrad", ""], ["Lagatutta", "David", ""], ["Mandelbaum", "Rachel", ""], ["Massey", "Richard", ""], ["Mellier", "Yannick", ""], ["Moghaddam", "Baback", ""], ["Moudden", "Yassir", ""], ["Nakajima", "Reiko", ""], ["Paulin-Henriksson", "Stephane", ""], ["Pires", "Sandrine", ""], ["Rassat", "Anais", ""], ["Refregier", "Alexandre", ""], ["Rhodes", "Jason", ""], ["Schrabback", "Tim", ""], ["Semboloni", "Elisabetta", ""], ["Shmakova", "Marina", ""], ["van Waerbeke", "Ludovic", ""], ["Witherick", "Dugan", ""], ["Voigt", "Lisa", ""], ["Wittman", "David", ""]]}, {"id": "0802.2050", "submitter": "Kevin Carter", "authors": "Kevin M. Carter, Raviv Raich, William G. Finn, and Alfred O. Hero", "title": "FINE: Fisher Information Non-parametric Embedding", "comments": "30 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of clustering, classification, and visualization of\nhigh-dimensional data when no straightforward Euclidean representation exists.\nTypically, these tasks are performed by first reducing the high-dimensional\ndata to some lower dimensional Euclidean space, as many manifold learning\nmethods have been developed for this task. In many practical problems however,\nthe assumption of a Euclidean manifold cannot be justified. In these cases, a\nmore appropriate assumption would be that the data lies on a statistical\nmanifold, or a manifold of probability density functions (PDFs). In this paper\nwe propose using the properties of information geometry in order to define\nsimilarities between data sets using the Fisher information metric. We will\nshow this metric can be approximated using entirely non-parametric methods, as\nthe parameterization of the manifold is generally unknown. Furthermore, by\nusing multi-dimensional scaling methods, we are able to embed the corresponding\nPDFs into a low-dimensional Euclidean space. This not only allows for\nclassification of the data, but also visualization of the manifold. As a whole,\nwe refer to our framework as Fisher Information Non-parametric Embedding\n(FINE), and illustrate its uses on a variety of practical problems, including\nbio-medical applications and document classification.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2008 16:40:17 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Carter", "Kevin M.", ""], ["Raich", "Raviv", ""], ["Finn", "William G.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "0802.2426", "submitter": "Bertrand Iooss", "authors": "Claire Cannamela, Josselin Garnier, Bertrand Iooss", "title": "Controlled stratification for quantile estimation", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS186 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1554-1580", "doi": "10.1214/08-AOAS186", "report-no": "IMS-AOAS-AOAS186", "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and discuss variance reduction techniques for the\nestimation of quantiles of the output of a complex model with random input\nparameters. These techniques are based on the use of a reduced model, such as a\nmetamodel or a response surface. The reduced model can be used as a control\nvariate; or a rejection method can be implemented to sample the realizations of\nthe input parameters in prescribed relevant strata; or the reduced model can be\nused to determine a good biased distribution of the input parameters for the\nimplementation of an importance sampling strategy. The different strategies are\nanalyzed and the asymptotic variances are computed, which shows the benefit of\nan adaptive controlled stratification method. This method is finally applied to\na real example (computation of the peak cladding temperature during a\nlarge-break loss of coolant accident in a nuclear reactor).\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2008 07:19:52 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2009 12:56:56 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Cannamela", "Claire", ""], ["Garnier", "Josselin", ""], ["Iooss", "Bertrand", ""]]}, {"id": "0802.2959", "submitter": "Keyur Desai", "authors": "Keyur Desai, J.R. Deller, Jr. and J. Justin McCormick", "title": "Tellipsoid: Exploiting inter-gene correlation for improved detection of\n  differential gene expression", "comments": "19 pages, Submitted to Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Algorithms for differential analysis of microarray data are vital\nto modern biomedical research. Their accuracy strongly depends on effective\ntreatment of inter-gene correlation. Correlation is ordinarily accounted for in\nterms of its effect on significance cut-offs. In this paper it is shown that\ncorrelation can, in fact, be exploited {to share information across tests},\nwhich, in turn, can increase statistical power.\n  Results: Vastly and demonstrably improved differential analysis approaches\nare the result of combining identifiability (the fact that in most microarray\ndata sets, a large proportion of genes can be identified a priori as\nnon-differential) with optimization criteria that incorporate correlation. As a\nspecial case, we develop a method which builds upon the widely used two-sample\nt-statistic based approach and uses the Mahalanobis distance as an optimality\ncriterion. Results on the prostate cancer data of Singh et al. (2002) suggest\nthat the proposed method outperforms all published approaches in terms of\nstatistical power.\n  Availability: The proposed algorithm is implemented in MATLAB and in R. The\nsoftware, called Tellipsoid, and relevant data sets are available at\nhttp://www.egr.msu.edu/~desaikey\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2008 02:10:10 GMT"}], "update_date": "2012-08-27", "authors_parsed": [["Desai", "Keyur", ""], ["Deller,", "J. R.", "Jr."], ["McCormick", "J. Justin", ""]]}, {"id": "0802.3458", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Confidence Interval for the Mean of a Bounded Random Variable and Its\n  Applications in Point Estimation", "comments": "7 pages, no figure; added proof of Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive an explicit formula for computing confidence\ninterval for the mean of a bounded random variable. Moreover, we have developed\nmultistage point estimation methods for estimating the mean value with\nprescribed precision and confidence level based on the proposed confidence\ninterval.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2008 19:53:51 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2009 00:31:31 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2009 15:47:40 GMT"}, {"version": "v4", "created": "Sun, 5 Apr 2009 21:24:24 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2009 16:39:18 GMT"}, {"version": "v6", "created": "Thu, 25 Nov 2010 01:23:06 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0802.3459", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Estimating Traffic Parameters with Rigorous Error Control", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform a queuing analysis or design in a communications context, we need\nto estimate the values of the input parameters, specifically the mean of the\narrival rate and service time. In this paper, we propose an approach for\nestimating the arrival rate of Poisson processes and the average service time\nfor servers under the assumption that the service time is exponential. In\nparticular, we derive sample size (i.e., the number of i.i.d. observations)\nrequired to obtain an estimate satisfying a pre-specified relative accuracy\nwith a given confidence level. A remarkable feature of this approach is that no\na priori information about the parameter is needed. In contrast to conventional\nmethods such as, standard error estimation and confidence interval\nconstruction, which only provides post-experimental evaluations of the\nestimate, this approach allows experimenters to rigorously control the error of\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2008 20:03:02 GMT"}], "update_date": "2008-02-26", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0802.3890", "submitter": "Robert Grober", "authors": "Robert D. Grober", "title": "PGA Tour Scores as a Gaussian Random Variable", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper it is demonstrated that the scoring at each PGA Tour stroke\nplay event can be reasonably modeled as a Gaussian random variable. All 46\nstroke play events in the 2007 season are analyzed. The distributions of scores\nare favorably compared with a Gaussian distribution using the\nKolmogorov-Smirnov test. This observation suggests performance tracking on the\nPGA tour should be done in terms of the z-score, calculated by subtracting the\nmean from the raw score and dividing by the standard deviation. This\nmethodology measures performance relative to the field of competitors,\nindependent of the venue, and in terms of a statistic that has quantitative\nmeaning. Several examples of the use of this scoring methodology are provided,\nincluding a calculation of the probability that Tiger Woods will break Byron\nNelson's record of eleven consecutive PGA Tour victories.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2008 20:23:22 GMT"}], "update_date": "2008-02-27", "authors_parsed": [["Grober", "Robert D.", ""]]}, {"id": "0802.3969", "submitter": "Joseph Rynkiewicz", "authors": "A. Dutot (LISA), Joseph Rynkiewicz (CES, Samos), F. Steiner (LISA), J.\n  Rude (LISA)", "title": "A 24-h forecast of ozone peaks and exceedance levels using neural\n  classifiers and weather predictions", "comments": null, "journal-ref": "Environmental Modelling and Software 22, 9 (2007) 1261-1269", "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": null, "abstract": "  A neural network combined to a neural classifier is used in a real time\nforecasting of hourly maximum ozone in the centre of France, in an urban\natmosphere. This neural model is based on the MultiLayer Perceptron (MLP)\nstructure. The inputs of the statistical network are model output statistics of\nthe weather predictions from the French National Weather Service. With this\nneural classifier, the Success Index of forecasting is 78% whereas it is from\n65% to 72% with the classical MLPs. During the validation phase, in the Summer\nof 2003, six ozone peaks above the threshold were detected. They actually were\nseven.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2008 08:16:43 GMT"}], "update_date": "2008-02-28", "authors_parsed": [["Dutot", "A.", "", "LISA"], ["Rynkiewicz", "Joseph", "", "CES, Samos"], ["Steiner", "F.", "", "LISA"], ["Rude", "J.", "", "LISA"]]}, {"id": "0802.4190", "submitter": "Eric Gautier", "authors": "Eric Gautier (CREST)", "title": "Bayesian Estimation of Inequalities with Non-Rectangular Censored Survey\n  Data", "comments": null, "journal-ref": "Annals Of Applied Statistics 5, 2B (2011) 1632-1656", "doi": "10.1214/10-AOAS443", "report-no": null, "categories": "stat.AP stat.ME", "license": null, "abstract": "  Synthetic indices are used in Economics to measure various aspects of\nmonetary inequalities. These scalar indices take as input the distribution over\na finite population, for example the population of a specific country. In this\narticle we consider the case of the French 2004 Wealth survey. We have at hand\na partial measurement on the distribution of interest consisting of bracketed\nand sometimes missing data, over a subsample of the population of interest. We\npresent in this article the statistical methodology used to obtain point and\ninterval estimates taking into account the various uncertainties. The\ninequality indices being nonlinear in the input distribution, we rely on a\nsimulation based approach where the model for the wealth per household is\nmultivariate. Using the survey data as well as matched auxiliary tax\ndeclarations data, we have at hand a quite intricate non-rectangle\nmultidimensional censoring. For practical issues we use a Bayesian approach.\nInference using Monte-Carlo approximations relies on a Monte-Carlo Markov chain\nalgorithm namely the Gibbs sampler. The quantities interesting to the decision\nmaker are taken to be the various inequality indices for the French population.\nTheir distribution conditional on the data of the subsample are assumed to be\nnormal centered on the design-based estimates with variance computed through\nlinearization and taking into account the sample design and total nonresponse.\nExogeneous selection of the subsample, in particular the nonresponse mechanism,\nis assumed and we condition on the adequate covariates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2008 12:35:02 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Gautier", "Eric", "", "CREST"]]}, {"id": "0802.4191", "submitter": "Christine Plumejeaud", "authors": "Christine Plumejeaud (INRIA Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble), Jean-Marc Vincent (INRIA Rh\\^one-Alpes / LIG\n  laboratoire d'Informatique de Grenoble), Claude Grasland (GC, RIATE),\n  J\\'er\\^ome Gensel (LSR - IMAG), H\\'el\\`ene Mathian (GC), Serge Guelton (INRIA\n  Rh\\^one-Alpes / LIG laboratoire d'Informatique de Grenoble), Jo\\\"el Boulier\n  (GC)", "title": "HyperSmooth : calcul et visualisation de cartes de potentiel\n  interactives", "comments": null, "journal-ref": "Dans SAGEO 2007, Rencontres internationales G\\'eomatique et\n  territoire. CdRom. - SAGEO 2007, Rencontres internationales G\\'eomatique et\n  territoire, France (2007)", "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": null, "abstract": "  The HyperCarte research group wishes to offer a new cartographic tool for\nspatial analysis of social data, using the potential smoothing method. The\npurpose of this method is to view the spreading of phenomena's in a continuous\nway, at a macroscopic scale, basing on data sampled on administrative areas. We\naim to offer an interactive tool, accessible via the Web, but guarantying the\nconfidentiality of data. The major difficulty is induced by the high complexity\nof the calculus, working on a great amount of data. We present our solution to\nsuch a technical challenge, and our perspectives of enhancements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2008 12:36:41 GMT"}], "update_date": "2008-02-29", "authors_parsed": [["Plumejeaud", "Christine", "", "INRIA Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"], ["Vincent", "Jean-Marc", "", "INRIA Rh\u00f4ne-Alpes / LIG\n  laboratoire d'Informatique de Grenoble"], ["Grasland", "Claude", "", "GC, RIATE"], ["Gensel", "J\u00e9r\u00f4me", "", "LSR - IMAG"], ["Mathian", "H\u00e9l\u00e8ne", "", "GC"], ["Guelton", "Serge", "", "INRIA\n  Rh\u00f4ne-Alpes / LIG laboratoire d'Informatique de Grenoble"], ["Boulier", "Jo\u00ebl", "", "GC"]]}, {"id": "0802.4317", "submitter": "Shane Jensen", "authors": "Shane T. Jensen, Kenneth E. Shirley, Abraham J. Wyner", "title": "Bayesball: A Bayesian hierarchical model for evaluating fielding in\n  major league baseball", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS228 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 2, 491-520", "doi": "10.1214/08-AOAS228", "report-no": "IMS-AOAS-AOAS228", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of statistical modeling in baseball has received substantial\nattention recently in both the media and academic community. We focus on a\nrelatively under-explored topic: the use of statistical models for the analysis\nof fielding based on high-resolution data consisting of on-field location of\nbatted balls. We combine spatial modeling with a hierarchical Bayesian\nstructure in order to evaluate the performance of individual fielders while\nsharing information between fielders at each position. We present results\nacross four seasons of MLB data (2002--2005) and compare our approach to other\nfielding evaluation procedures.\n", "versions": [{"version": "v1", "created": "Fri, 29 Feb 2008 03:42:42 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2008 15:58:58 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2009 07:46:45 GMT"}], "update_date": "2009-08-14", "authors_parsed": [["Jensen", "Shane T.", ""], ["Shirley", "Kenneth E.", ""], ["Wyner", "Abraham J.", ""]]}, {"id": "0802.4350", "submitter": "Ricardo Lopez-Ruiz", "authors": "Carmen Pellicer-Lostao and Ricardo Lopez-Ruiz", "title": "Role of Symmetry and Geometry in a chaotic Pseudo-Random Bit Generator", "comments": "21 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.CR physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, Pseudo-Random Bit Generation (PRBG) based on 2D chaotic\nmappings of logistic type is considered. The sequences generated with two\nPseudorandom Bit Generators (PRBGs) of this type are statistically tested and\nthe computational effectiveness of the generators is estimated. The role played\nby the symmetry and the geometrical properties of the underlying chaotic\nattractors is also explored. Considering these PRBGs valid for cryptography,\nthe size of the available key spaces are calculated. Additionally, a novel\nmechanism called 'symmetry-swap' is introduced in order to enhance the PRBG\nalgorithm. It is shown that it can increase the degrees of freedom of the key\nspace, while maintaining the speed and performance in the PRBG.\n", "versions": [{"version": "v1", "created": "Fri, 29 Feb 2008 09:50:34 GMT"}], "update_date": "2008-03-03", "authors_parsed": [["Pellicer-Lostao", "Carmen", ""], ["Lopez-Ruiz", "Ricardo", ""]]}]