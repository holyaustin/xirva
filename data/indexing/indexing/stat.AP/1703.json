[{"id": "1703.00056", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova", "title": "Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments", "comments": "The short conference version of the paper was previously uploaded as\n  arXiv:1610.07524", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction instruments (RPI's) provide decision makers with an\nassessment of the likelihood that a criminal defendant will reoffend at a\nfuture point in time. While such instruments are gaining increasing popularity\nacross the country, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses several fairness criteria that have recently\nbeen applied to assess the fairness of recidivism prediction instruments. We\ndemonstrate that the criteria cannot all be simultaneously satisfied when\nrecidivism prevalence differs across groups. We then show how disparate impact\ncan arise when a recidivism prediction instrument fails to satisfy the\ncriterion of error rate balance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:12:37 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chouldechova", "Alexandra", ""]]}, {"id": "1703.00154", "submitter": "Arno Solin", "authors": "Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala", "title": "Inertial Odometry on Handheld Smartphones", "comments": "Appearing in Proceedings of the International Conference on\n  Information Fusion (FUSION 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a complete inertial navigation system using the limited quality data\nprovided by current smartphones has been regarded challenging, if not\nimpossible. This paper shows that by careful crafting and accounting for the\nweak information in the sensor samples, smartphones are capable of pure\ninertial navigation. We present a probabilistic approach for orientation and\nuse-case free inertial odometry, which is based on double-integrating rotated\naccelerations. The strength of the model is in learning additive and\nmultiplicative IMU biases online. We are able to track the phone position,\nvelocity, and pose in real-time and in a computationally lightweight fashion by\nsolving the inference with an extended Kalman filter. The information fusion is\ncompleted with zero-velocity updates (if the phone remains stationary),\naltitude correction from barometric pressure readings (if available), and\npseudo-updates constraining the momentary speed. We demonstrate our approach\nusing an iPad and iPhone in several indoor dead-reckoning applications and in a\nmeasurement tool setup.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 07:00:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 20:40:20 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Solin", "Arno", ""], ["Cortes", "Santiago", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1703.00396", "submitter": "Yakup Kutlu", "authors": "Yakup Kutlu, Apdullah Yay{\\i}k, Esen Y{\\i}ld{\\i}r{\\i}m, Mustafa\n  Yeniad, Serdar Y{\\i}ld{\\i}r{\\i}m", "title": "Patient Specific Congestive Heart Failure Detection From Raw ECG signal", "comments": "Congestive heart failure, ECG, Second-Order Difference Plot,\n  classification, patient based cross-validation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study; in order to diagnose congestive heart failure (CHF) patients,\nnon-linear second-order difference plot (SODP) obtained from raw 256 Hz sampled\nfrequency and windowed record with different time of ECG records are used. All\nof the data rows are labelled with their belongings to classify much more\nrealistically. SODPs are divided into different radius of quadrant regions and\nnumbers of the points fall in the quadrants are computed in order to extract\nfeature vectors. Fisher's linear discriminant, Naive Bayes, Radial basis\nfunction, and artificial neural network are used as classifier. The results are\nconsidered in two step validation methods as general k-fold cross-validation\nand patient based cross-validation. As a result, it is shown that using neural\nnetwork classifier with features obtained from SODP, the constructed system\ncould distinguish normal and CHF patients with 100% accuracy rate. Keywords\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:49:25 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Kutlu", "Yakup", ""], ["Yay\u0131k", "Apdullah", ""], ["Y\u0131ld\u0131r\u0131m", "Esen", ""], ["Yeniad", "Mustafa", ""], ["Y\u0131ld\u0131r\u0131m", "Serdar", ""]]}, {"id": "1703.00398", "submitter": "Liang Zhao", "authors": "Ning Zhang, Liang Zhao", "title": "Analysis on Cohort Effects in view of Differential Geometry and its\n  Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.00327", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes birth cohort effects and develops an approach which is\nbased on differential geometry to identify and measure cohort effects in\nmortality data sets. The measurement is quantitative and provides a potential\nmethod to compare cohort effects among different countries or groups. Data sets\nof four countries (e.g. U.k., U.S., Canada and Japan) are taken as examples to\nexplain our approach and applications of the measurement to a modified\nLee-Carter model are analyzed. In fact, this paper is an upgrade version of our\npaper arXiv:1504.00327. There is a new section which gives applications of our\napproach based on the Lee-Carter and APC models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:38:28 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhang", "Ning", ""], ["Zhao", "Liang", ""]]}, {"id": "1703.00409", "submitter": "Riccardo Di Clemente", "authors": "Riccardo Di Clemente, Miguel Luengo-Oroz, Matias Travizano, Sharon Xu,\n  Bapu Vaitla, Marta C. Gonz\\'alez", "title": "Sequences of purchases in credit card data reveal life styles in urban\n  populations", "comments": "30 pages, 26 figures", "journal-ref": "Nature Communications 9:3330 (2018)", "doi": "10.1038/s41467-018-05690-8", "report-no": null, "categories": "physics.soc-ph cs.IT cs.SI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf-like distributions characterize a wide set of phenomena in physics,\nbiology, economics and social sciences. In human activities, Zipf-laws describe\nfor example the frequency of words appearance in a text or the purchases types\nin shopping patterns. In the latter, the uneven distribution of transaction\ntypes is bound with the temporal sequences of purchases of individual choices.\nIn this work, we define a framework using a text compression technique on the\nsequences of credit card purchases to detect ubiquitous patterns of collective\nbehavior. Clustering the consumers by their similarity in purchases sequences,\nwe detect five consumer groups. Remarkably, post checking, individuals in each\ngroup are also similar in their age, total expenditure, gender, and the\ndiversity of their social and mobility networks extracted by their mobile phone\nrecords. By properly deconstructing transaction data with Zipf-like\ndistributions, this method uncovers sets of significant sequences that reveal\ninsights on collective human behavior.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:42:47 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:33:06 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 14:59:07 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 13:42:20 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Di Clemente", "Riccardo", ""], ["Luengo-Oroz", "Miguel", ""], ["Travizano", "Matias", ""], ["Xu", "Sharon", ""], ["Vaitla", "Bapu", ""], ["Gonz\u00e1lez", "Marta C.", ""]]}, {"id": "1703.00654", "submitter": "Sylvain Sardy", "authors": "Jairo Diaz-Rodriguez, Dominique Eckert, Hatef Monajemi, St\\'ephane\n  Paltani and Sylvain Sardy", "title": "Nonparametric estimation of galaxy cluster's emissivity and point source\n  detection in astrophysics with two lasso penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysicists are interested in recovering the 3D gas emissivity of a\ngalaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and\npresence of point sources make this inverse problem even harder to solve. The\ncurrent state-of-the-art technique is two step: first identify the location of\npotential point sources, then mask these locations and deproject the data.\n  We instead model the data as a Poisson generalized linear model (involving\nblurring, Abel and wavelets operators) regularized by two lasso penalties to\ninduce sparse wavelet representation and sparse point sources. The amount of\nsparsity is controlled by two quantile universal thresholds. As a result, our\nmethod outperforms the existing one.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:57:20 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Diaz-Rodriguez", "Jairo", ""], ["Eckert", "Dominique", ""], ["Monajemi", "Hatef", ""], ["Paltani", "St\u00e9phane", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1703.00981", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson", "title": "A Restaurant Process Mixture Model for Connectivity Based Parcellation\n  of the Cortex", "comments": "In the Proceedings of Information Processing in Medical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary objectives of human brain mapping is the division of the\ncortical surface into functionally distinct regions, i.e. parcellation. While\nit is generally agreed that at macro-scale different regions of the cortex have\ndifferent functions, the exact number and configuration of these regions is not\nknown. Methods for the discovery of these regions are thus important,\nparticularly as the volume of available information grows. Towards this end, we\npresent a parcellation method based on a Bayesian non-parametric mixture model\nof cortical connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:03:56 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Moyer", "Daniel", ""], ["Gutman", "Boris A", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1703.01000", "submitter": "Lucas Macri", "authors": "Wenlong Yuan, Shiyuan He, Lucas M. Macri, James Long and Jianhua Z.\n  Huang (Texas A&M University)", "title": "The M33 Synoptic Stellar Survey. II. Mira Variables", "comments": "Includes small corrections to match the published version", "journal-ref": "The Astronomical Journal, 153:170 (2017)", "doi": "10.3847/1538-3881/aa63f1", "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discovery of 1847 Mira candidates in the Local Group galaxy\nM33 using a novel semi-parametric periodogram technique coupled with a Random\nForest classifier. The algorithms were applied to ~2.4x10^5 I-band light curves\npreviously obtained by the M33 Synoptic Stellar Survey. We derive preliminary\nPeriod-Luminosity relations at optical, near- & mid-infrared wavelengths and\ncompare them to the corresponding relations in the Large Magellanic Cloud.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 00:21:34 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 13:54:01 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Yuan", "Wenlong", "", "Texas A&M University"], ["He", "Shiyuan", "", "Texas A&M University"], ["Macri", "Lucas M.", "", "Texas A&M University"], ["Long", "James", "", "Texas A&M University"], ["Huang", "Jianhua Z.", "", "Texas A&M University"]]}, {"id": "1703.01002", "submitter": "Donald Richards", "authors": "Michelle Li and Donald Richards", "title": "Statistical Implications of the Revenue Transfer Methodology in the\n  Affordable Care Act", "comments": "To appear in the North American Actuarial Journal, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Affordable Care Act (ACA) includes a permanent revenue transfer\nmethodology which provides financial incentives to health insurance plans that\nhave higher than average actuarial risk. In this paper, we derive some\nstatistical implications of the revenue transfer methodology in the ACA. We\ntreat as random variables the revenue transfers between individual insurance\nplans in a given marketplace, where each plan's revenue transfer amount is\nmeasured as a percentage of the plan's total premium. We analyze the means and\nvariances of those random variables, and deduce from the zero sum nature of the\nrevenue transfers that there is no limit to the magnitude of revenue transfer\npayments relative to plans' total premiums. Using data provided by the American\nAcademy of Actuaries and by the Centers for Medicare and Medicaid Services, we\nobtain an explanation for empirical phenomena that revenue transfers were more\nvariable and can be substantially greater for insurance plans with smaller\nmarket shares. We show that it is often the case that an insurer which has\ndecreasing market share will also have increased volatility in its revenue\ntransfers.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 00:32:31 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 20:16:27 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 04:15:04 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Li", "Michelle", ""], ["Richards", "Donald", ""]]}, {"id": "1703.01044", "submitter": "Debasis Kundu Professor", "authors": "Arnab Koley and Debasis Kundu", "title": "On Generalized Progressive Hybrid Censoring in presence of competing\n  risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progressive Type-II hybrid censoring scheme introduced by Kundu and\nJoarder (\\textit{Computational Statistics and Data Analysis}, 2509-2528, 2006),\nhas received some attention in the last few years. One major drawback of this\ncensoring scheme is that very few observations (even no observation at all) may\nbe observed at the end of the experiment. To overcome this problem, Cho, Sun\nand Lee (\\textit{Statistical Methodology}, 23, 18-34, 2015) recently introduced\ngeneralized progressive censoring which ensures to get a pre specified number\nof failures. In this paper we analyze generalized progressive censored data in\npresence of competing risks. For brevity we have considered only two competing\ncauses of failures, and it is assumed that the lifetime of the competing causes\nfollow one parameter exponential distributions with different scale parameters.\nWe obtain the maximum likelihood estimators of the unknown parameters and also\nprovide their exact distributions. Based on the exact distributions of the\nmaximum likelihood estimators exact confidence intervals can be obtained.\nAsymptotic and bootstrap confidence intervals are also provided for comparison\npurposes. We further consider the Bayesian analysis of the unknown parameters\nunder a very flexible Beta-Gamma prior. We provide the Bayes estimates and the\nassociated credible intervals of the unknown parameters based on the above\npriors. We present extensive simulation results to see the effectiveness of the\nproposed method and finally one real data set is analyzed for illustrative\npurpose.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 05:54:09 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Koley", "Arnab", ""], ["Kundu", "Debasis", ""]]}, {"id": "1703.01051", "submitter": "Debasis Kundu Professor", "authors": "Arnab Koley and Debasis Kundu", "title": "Interval Estimation of the Unknown Exponential Parameter Based on Time\n  Truncated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the statistical inference of the unknown parameter\nof an exponential distribution based on the time truncated data. The time\ntruncated data occurs quite often in the reliability analysis for type-I or\nhybrid censoring cases. All the results available today are based on the\nconditional argument that at least one failure occurs during the experiment. In\nthis paper we provide some inferential results based on the unconditional\nargument. We extend the results for some two-parameter distributions also.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 06:28:36 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Koley", "Arnab", ""], ["Kundu", "Debasis", ""]]}, {"id": "1703.01234", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and John Paul Gosling", "title": "A Bayesian computer model analysis of Robust Bayesian analyses", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:29:21 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Vernon", "Ian", ""], ["Gosling", "John Paul", ""]]}, {"id": "1703.01237", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "How real is the random censorship model in medical studies?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis the random censorship model refers to censoring and\nsurvival times being independent of each other. It is one of the fundamental\nassumptions in the theory of survival analysis. We explain the reason for it\nbeing so ubiquitous, and we investigate its presence in medical studies. We\ndifferentiate two types of censoring in medical studies (dropout and\nadministrative), and we explain their importance in examining the existence of\nthe random censorship model. We show that in order to presume the random\ncensorship model it is not enough to have a design study which conforms to it,\nbut that one needs to provide evidence for its presence in the results. Blindly\npresuming the random censorship model might lead to the Kaplan-Meier estimator\nproducing biased results, which might have serious consequences when estimating\nsurvival in medical studies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:38:28 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "1703.01506", "submitter": "Felipe Gutierrez-Barragan", "authors": "Felipe Gutierrez-Barragan, Vamsi K. Ithapu, Chris Hinrichs, Camille\n  Maumet, Sterling C. Johnson, Thomas E. Nichols, Vikas Singh, and the ADNI", "title": "Accelerating Permutation Testing in Voxel-wise Analysis through Subspace\n  Tracking: A new plugin for SnPM", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.07.025", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation testing is a non-parametric method for obtaining the max null\ndistribution used to compute corrected $p$-values that provide strong control\nof false positives. In neuroimaging, however, the computational burden of\nrunning such an algorithm can be significant. We find that by viewing the\npermutation testing procedure as the construction of a very large permutation\ntesting matrix, $T$, one can exploit structural properties derived from the\ndata and the test statistics to reduce the runtime under certain conditions. In\nparticular, we see that $T$ is low-rank plus a low-variance residual. This\nmakes $T$ a good candidate for low-rank matrix completion, where only a very\nsmall number of entries of $T$ ($\\sim0.35\\%$ of all entries in our experiments)\nhave to be computed to obtain a good estimate. Based on this observation, we\npresent RapidPT, an algorithm that efficiently recovers the max null\ndistribution commonly obtained through regular permutation testing in\nvoxel-wise analysis. We present an extensive validation on a synthetic dataset\nand four varying sized datasets against two baselines: Statistical\nNonParametric Mapping (SnPM13) and a standard permutation testing\nimplementation (referred as NaivePT). We find that RapidPT achieves its best\nruntime performance on medium sized datasets ($50 \\leq n \\leq 200$), with\nspeedups of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger\ndatasets ($n \\geq 200$) RapidPT outperforms NaivePT (6x - 200x) on all\ndatasets, and provides large speedups over SnPM13 when more than 10000\npermutations (2x - 15x) are needed. The implementation is a standalone toolbox\nand also integrated within SnPM13, able to leverage multi-core architectures\nwhen available.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 19:07:42 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 18:03:54 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Gutierrez-Barragan", "Felipe", ""], ["Ithapu", "Vamsi K.", ""], ["Hinrichs", "Chris", ""], ["Maumet", "Camille", ""], ["Johnson", "Sterling C.", ""], ["Nichols", "Thomas E.", ""], ["Singh", "Vikas", ""], ["ADNI", "the", ""]]}, {"id": "1703.01526", "submitter": "Prashanth R", "authors": "R. Prashanth, Sumantra Dutta Roy, Pravat K. Mandal, Shantanu Ghosh", "title": "High Accuracy Classification of Parkinson's Disease through Shape\n  Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging", "comments": "9 pages, 5 figures, Accepted in the IEEE Journal of Biomedical and\n  Health Informatics, Additional supplementary documents available at\n  http://ieeexplore.ieee.org/document/7442754/", "journal-ref": null, "doi": "10.1109/JBHI.2016.2547901", "report-no": null, "categories": "stat.AP cs.CV physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate identification of parkinsonian syndromes (PS) involving\npresynaptic degeneration from non-degenerative variants such as Scans Without\nEvidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for\neffective patient management as the course, therapy and prognosis differ\nsubstantially between the two groups. In this study, we use Single Photon\nEmission Computed Tomography (SPECT) images from healthy normal, early PD and\nSWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative\n(PPMI) database, and process them to compute shape- and surface fitting-based\nfeatures for the three groups. We use these features to develop and compare\nvarious classification models that can discriminate between scans showing\ndopaminergic deficit, as in PD, from scans without the deficit, as in healthy\nnormal or SWEDD. Along with it, we also compare these features with Striatal\nBinding Ratio (SBR)-based features, which are well-established and clinically\nused, by computing a feature importance score using Random forests technique.\nWe observe that the Support Vector Machine (SVM) classifier gave the best\nperformance with an accuracy of 97.29%. These features also showed higher\nimportance than the SBR-based features. We infer from the study that shape\nanalysis and surface fitting are useful and promising methods for extracting\ndiscriminatory features that can be used to develop diagnostic models that\nmight have the potential to help clinicians in the diagnostic process.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 21:50:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Prashanth", "R.", ""], ["Roy", "Sumantra Dutta", ""], ["Mandal", "Pravat K.", ""], ["Ghosh", "Shantanu", ""]]}, {"id": "1703.01776", "submitter": "Sylvain Le Corff", "authors": "Pierre Gloaguen (MIA-Paris), Marie-Pierre Etienne (MIA-Paris), Sylvain\n  Le Corff", "title": "Online Sequential Monte Carlo smoother for partially observed stochastic\n  differential equations", "comments": null, "journal-ref": null, "doi": "10.1186/s13634-018-0530-3", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm to approximate smoothed additive\nfunctionals for partially observed stochastic differential equations. This\nmethod relies on a recent procedure which allows to compute such approximations\nonline, i.e. as the observations are received, and with a computational\ncomplexity growing linearly with the number of Monte Carlo samples. This online\nsmoother cannot be used directly in the case of partially observed stochastic\ndifferential equations since the transition density of the latent data is\nusually unknown. We prove that a similar algorithm may still be defined for\npartially observed continuous processes by replacing this unknown quantity by\nan unbiased estimator obtained for instance using general Poisson estimators.\nWe prove that this estimator is consistent and its performance are illustrated\nusing data from two models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:24:07 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gloaguen", "Pierre", "", "MIA-Paris"], ["Etienne", "Marie-Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", ""]]}, {"id": "1703.01937", "submitter": "Nicholas Janetos", "authors": "Nick Janetos and Jan Tilly", "title": "Reputation Dynamics in a Market for Illicit Drugs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze reputation dynamics in an online market for illicit drugs using a\nnovel dataset of prices and ratings. The market is a black market, and so\ncontracts cannot be enforced. We study the role that reputation plays in\nalleviating adverse selection in this market. We document the following\nstylized facts: (i) There is a positive relationship between the price and the\nrating of a seller. This effect is increasing in the number of reviews left for\na seller. A mature highly-rated seller charges a 20% higher price than a mature\nlow-rated seller. (ii) Sellers with more reviews charge higher prices\nregardless of rating. (iii) Low-rated sellers are more likely to exit the\nmarket and make fewer sales. We show that these stylized facts are explained by\na dynamic model of adverse selection, ratings, and exit, in which buyers form\nrational inferences about the quality of a seller jointly from his rating and\nnumber of sales. Sellers who receive low ratings initially charge the same\nprice as highly-rated sellers since early reviews are less informative about\nquality. Bad sellers exit rather than face lower prices in the future. We\nprovide conditions under which our model admits a unique equilibrium. We\nestimate the model, and use the result to compute the returns to reputation in\nthe market. We find that the market would have collapsed due to adverse\nselection in the absence of a rating system.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:58:26 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Janetos", "Nick", ""], ["Tilly", "Jan", ""]]}, {"id": "1703.01977", "submitter": "Bohdan Pavlyshenko", "authors": "B.M. Pavlyshenko", "title": "Linear, Machine Learning and Probabilistic Approaches for Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study different approaches for time series modeling. The\nforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine\nlearning algorithm are described. Results of different model combinations are\nshown. For probabilistic modeling the approaches using copulas and Bayesian\ninference are considered.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 10:41:26 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Pavlyshenko", "B. M.", ""]]}, {"id": "1703.02078", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Dylan S. Small, Paul R. Rosenbaum", "title": "Cross-screening in observational studies that test many hypotheses", "comments": "33 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss observational studies that test many causal hypotheses, either\nhypotheses about many outcomes or many treatments. To be credible an\nobservational study that tests many causal hypotheses must demonstrate that its\nconclusions are neither artifacts of multiple testing nor of small biases from\nnonrandom treatment assignment. In a sense that needs to be defined carefully,\nhidden within a sensitivity analysis for nonrandom assignment is an enormous\ncorrection for multiple testing: in the absence of bias, it is extremely\nimprobable that multiple testing alone would create an association insensitive\nto moderate biases. We propose a new strategy called \"cross-screening\",\ndifferent from but motivated by recent work of Bogomolov and Heller on\nreplicability. Cross-screening splits the data in half at random, uses the\nfirst half to plan a study carried out on the second half, then uses the second\nhalf to plan a study carried out on the first half, and reports the more\nfavorable conclusions of the two studies correcting using the Bonferroni\ninequality for having done two studies. If the two studies happen to concur,\nthen they achieve Bogomolov-Heller replicability; however, importantly,\nreplicability is not required for strong control of the family-wise error rate,\nand either study alone suffices for firm conclusions. In randomized studies\nwith a few hypotheses, cross-split screening is not an attractive method when\ncompared with conventional methods of multiplicity control, but it can become\nattractive when hundreds or thousands of hypotheses are subjected to\nsensitivity analyses in an observational study. We illustrate the technique by\ncomparing 46 biomarkers in individuals who consume large quantities of fish\nversus little or no fish.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:33:24 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1703.02112", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Devin S. Johnson, John W. Durban", "title": "Process convolution approaches for modeling interacting trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are a fundamental statistical tool used in a wide range of\napplications. In the spatio-temporal setting, several families of covariance\nfunctions exist to accommodate a wide variety of dependence structures arising\nin different applications. These parametric families can be restrictive and are\ninsufficient in some situations. In contrast, process convolutions represent a\nflexible, interpretable approach to defining the covariance of a Gaussian\nprocess and have modest requirements to ensure validity. We introduce a\ngeneralization of the process convolution approach that employs multiple\nconvolutions sequentially to form a \"process convolution chain.\" In our\nproposed multi-stage framework, complex dependencies that arise from a\ncombination of different interacting mechanisms are decomposed into a series of\ninterpretable kernel smoothers. We demonstrate an application of process\nconvolution chains to model killer whale movement, in which the paths taken by\nmultiple individuals are not independent, but reflect dynamic social\ninteractions within the population. Our proposed model for dependent movement\nprovides inference for the latent dynamic social structure in the study\npopulation. Additionally, by leveraging the positive dependence among\nindividual paths, we achieve a reduction in uncertainty for the estimated\nlocations of the whales, compared to a model that treats paths as independent.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 21:17:42 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 18:16:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""], ["Durban", "John W.", ""]]}, {"id": "1703.02236", "submitter": "Cheng Ju", "authors": "Cheng Ju, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard\n  Wyss, Sebastian Schneeweiss, Mark J. van der Laan", "title": "Propensity score prediction for electronic healthcare databases using\n  Super Learner and High-dimensional Propensity Score Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal learner for prediction modeling varies depending on the\nunderlying data-generating distribution. Super Learner (SL) is a generic\nensemble learning algorithm that uses cross-validation to select among a\n\"library\" of candidate prediction models. The SL is not restricted to a single\nprediction model, but uses the strengths of a variety of learning algorithms to\nadapt to different databases. While the SL has been shown to perform well in a\nnumber of settings, it has not been thoroughly evaluated in large electronic\nhealthcare databases that are common in pharmacoepidemiology and comparative\neffectiveness research. In this study, we applied and evaluated the performance\nof the SL in its ability to predict treatment assignment using three electronic\nhealthcare databases. We considered a library of algorithms that consisted of\nboth nonparametric and parametric models. We also considered a novel strategy\nfor prediction modeling that combines the SL with the high-dimensional\npropensity score (hdPS) variable selection algorithm. Predictive performance\nwas assessed using three metrics: the negative log-likelihood, area under the\ncurve (AUC), and time complexity. Results showed that the best individual\nalgorithm, in terms of predictive performance, varied across datasets. The SL\nwas able to adapt to the given dataset and optimize predictive performance\nrelative to any individual learner. Combining the SL with the hdPS was the most\nconsistent prediction method and may be promising for PS estimation and\nprediction modeling in electronic healthcare databases.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:38:02 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 18:52:48 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ju", "Cheng", ""], ["Combs", "Mary", ""], ["Lendle", "Samuel D", ""], ["Franklin", "Jessica M", ""], ["Wyss", "Richard", ""], ["Schneeweiss", "Sebastian", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1703.02329", "submitter": "Simone Del Sarto", "authors": "Michela Gnaldi, Simone Del Sarto", "title": "Time and media-use of Italian Generation Y: dimensions of leisure\n  preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time spent in leisure is not a minor research question as it is acknowledged\nas a key aspect of one's quality of life. The primary aim of this article is to\nqualify time and Internet use of Italian Generation Y beyond media hype and\nassumptions. To this aim, we apply a multidimensional extension of Item\nResponse Theory models to the Italian \"Multipurpose survey on households:\naspects of daily life\" to ascertain the relevant dimensions of Generation Y\ntime-use. We show that the use of technology is neither the first nor the\nforemost time-use activity of Italian Generation Y, who still prefers to use\nits time to socialise and have fun with friends in a non media-medalled manner.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 11:15:14 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Gnaldi", "Michela", ""], ["Del Sarto", "Simone", ""]]}, {"id": "1703.02441", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "Statistical Analysis of the Ricker Model", "comments": "16 pages 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ricker model was introduced in the context of managing fishing stocks. It\nis a discrete non-linear iterative model given by $N(t+1)=rN(t)\\exp(-N(t))$\nwhere $N(t)$ is the population at time $t$. The model treated in this paper\nincludes a random component $N(t+1)=rN(t)\\exp(-N(t)+\\varepsilon(t+1))$ and what\nis observed at time $t$ is a Poisson random variable with parameter $\\varphi\nN(t)$. Such a model has been analysed using `synthetic likelihood' and ABC\n(Approximate Bayesian Computation). In contrast this paper takes a\nnon-likelihood approach and treats the model in a consistent manner as an\napproximation. The goal is to specify those parameter values if any which are\nconsistent with the data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:43:22 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "1703.02502", "submitter": "The-Hien Dang-Ha", "authors": "The-Hien Dang-Ha, Roland Olsson, Hao Wang", "title": "Clustering Methods for Electricity Consumers: An Empirical Study in\n  Hvaler-Norway", "comments": "12 pages, 3 figures", "journal-ref": "Norsk Informatikkonferanse (NIK) 2016", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of Smart Grid in Norway in specific and Europe/US in general\nwill shortly lead to the availability of massive amount of fine-grained\nspatio-temporal consumption data from domestic households. This enables the\napplication of data mining techniques for traditional problems in power system.\nClustering customers into appropriate groups is extremely useful for operators\nor retailers to address each group differently through dedicated tariffs or\ncustomer-tailored services. Currently, the task is done based on demographic\ndata collected through questionnaire, which is error-prone. In this paper, we\nused three different clustering techniques (together with their variants) to\nautomatically segment electricity consumers based on their consumption\npatterns. We also proposed a good way to extract consumption patterns for each\nconsumer. The grouping results were assessed using four common internal\nvalidity indexes. We found that the combination of Self Organizing Map (SOM)\nand k-means algorithms produce the most insightful and useful grouping. We also\ndiscovered that grouping quality cannot be measured effectively by automatic\nindicators, which goes against common suggestions in literature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:14:54 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Dang-Ha", "The-Hien", ""], ["Olsson", "Roland", ""], ["Wang", "Hao", ""]]}, {"id": "1703.02650", "submitter": "Ming Jiang", "authors": "Ming Jiang, J\\'er\\^ome Bobin and Jean-Luc Starck", "title": "Joint Multichannel Deconvolution and Blind Source Separation", "comments": null, "journal-ref": "SIAM J. Imaging Sci., 10(4), 1997-2021. (25 pages)", "doi": "10.1137/16M1103713", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Source Separation (BSS) is a challenging matrix factorization problem\nthat plays a central role in multichannel imaging science. In a large number of\napplications, such as astrophysics, current unmixing methods are limited since\nreal-world mixtures are generally affected by extra instrumental effects like\nblurring. Therefore, BSS has to be solved jointly with a deconvolution problem,\nwhich requires tackling a new inverse problem: deconvolution BSS (DBSS). In\nthis article, we introduce an innovative DBSS approach, called DecGMCA, based\non sparse signal modeling and an efficient alternative projected least square\nalgorithm. Numerical results demonstrate that the DecGMCA algorithm performs\nvery well on simulations. It further highlights the importance of jointly\nsolving BSS and deconvolution instead of considering these two problems\nindependently. Furthermore, the performance of the proposed DecGMCA algorithm\nis demonstrated on simulated radio-interferometric data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 00:47:45 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 21:08:06 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Jiang", "Ming", ""], ["Bobin", "J\u00e9r\u00f4me", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1703.02870", "submitter": "Bruce Desmarais", "authors": "Bruce A. Desmarais and Skyler J. Cranmer", "title": "Statistical Inference in Political Networks Research", "comments": null, "journal-ref": "The Oxford Handbook of Political Networks. Jennifer Nicoll Victor,\n  Alexander H. Montgomery, and Mark Lubell, editors. Oxford University Press,\n  2017", "doi": "10.1093/oxfordhb/9780190228217.013.8", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers interested in statistically modeling network data have a\nwell-established and quickly growing set of approaches from which to choose.\nSeveral of these methods have been regularly applied in research on political\nnetworks, while others have yet to permeate the field. Here, we review the most\nprominent methods of inferential network analysis---both for cross-sectionally\nand longitudinally observed networks including (temporal) exponential random\ngraph models, latent space models, the quadratic assignment procedure, and\nstochastic actor oriented models. For each method, we summarize its analytic\nform, identify prominent published applications in political science and\ndiscuss computational considerations. We conclude with a set of guidelines for\nselecting a method for a given application.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 15:23:00 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Desmarais", "Bruce A.", ""], ["Cranmer", "Skyler J.", ""]]}, {"id": "1703.03213", "submitter": "M.I. Borrajo", "authors": "M.I. Borrajo, W. Gonz\\'alez-Manteiga and M.D. Mart\\'inez-Miranda", "title": "Bootstrapping kernel intensity estimation for nonhomogeneous point\n  processes depending on spatial covariates", "comments": "32 pages, 7 figures (15 images), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the spatial point process context, kernel intensity estimation has been\nmainly restricted to exploratory analysis due to its lack of consistency.\nDifferent methods have been analysed to overcome this problem, and the\ninclusion of covariates resulted to be one possible solution. In this paper we\nfocus on de\\-fi\\-ning a theoretical framework to derive a consistent kernel\nintensity estimator using covariates, as well as a consistent smooth bootstrap\nprocedure. We define two new data-driven bandwidth selectors specifically\ndesigned for our estimator: a rule-of-thumb and a plug-in bandwidth based on\nour consistent bootstrap method. A simulation study is accomplished to\nunderstand the performance of our proposals in finite samples. Finally, we\ndescribe an application to a real data set consisting of the wildfires in\nCanada during June 2015, using meteorological information as covariates.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 10:19:53 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 18:35:33 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 14:07:22 GMT"}, {"version": "v4", "created": "Fri, 18 May 2018 14:56:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Borrajo", "M. I.", ""], ["Gonz\u00e1lez-Manteiga", "W.", ""], ["Mart\u00ednez-Miranda", "M. D.", ""]]}, {"id": "1703.03340", "submitter": "Alireza Zaeemzadeh", "authors": "Alireza Zaeemzadeh, Mohsen Joneidi, and Nazanin Rahnavard", "title": "Adaptive Non-uniform Compressive Sampling for Time-varying Signals", "comments": "6 pages, 8 figures, Conference on Information Sciences and Systems\n  (CISS 2017) Baltimore, Maryland", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, adaptive non-uniform compressive sampling (ANCS) of\ntime-varying signals, which are sparse in a proper basis, is introduced. ANCS\nemploys the measurements of previous time steps to distribute the sensing\nenergy among coefficients more intelligently. To this aim, a Bayesian inference\nmethod is proposed that does not require any prior knowledge of importance\nlevels of coefficients or sparsity of the signal. Our numerical simulations\nshow that ANCS is able to achieve the desired non-uniform recovery of the\nsignal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce\nthe number of required measurements significantly.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 16:56:27 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Zaeemzadeh", "Alireza", ""], ["Joneidi", "Mohsen", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "1703.03753", "submitter": "Lanfeng Pan", "authors": "Lanfeng Pan, Yehua Li, Kevin He, Yanming Li and Yi Li", "title": "Latent Gaussian Mixture Models for Nationwide Kidney Transplant Center\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Five year post-transplant survival rate is an important indicator on quality\nof care delivered by kidney transplant centers in the United States. To provide\na fair assessment of each transplant center, an effect that represents the\ncenter-specific care quality, along with patient level risk factors, is often\nincluded in the risk adjustment model. In the past, the center effects have\nbeen modeled as either fixed effects or Gaussian random effects, with various\npros and cons. Our numerical analyses reveal that the distributional\nassumptions do impact the prediction of center effects especially when the\neffect is extreme. To bridge the gap between these two approaches, we propose\nto model the transplant center effect as a latent random variable with a finite\nGaussian mixture distribution. Such latent Gaussian mixture models provide a\nconvenient framework to study the heterogeneity among the transplant centers.\nTo overcome the weak identifiability issues, we propose to estimate the latent\nGaussian mixture model using a penalized likelihood approach, and develop\nsequential locally restricted likelihood ratio tests to determine the number of\ncomponents in the Gaussian mixture distribution. The fitted mixture model\nprovides a convenient means of controlling the false discovery rate when\nscreening for underperforming or outperforming transplant centers. The\nperformance of the methods is verified by simulations and by the analysis of\nthe motivating data example.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 16:35:40 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Pan", "Lanfeng", ""], ["Li", "Yehua", ""], ["He", "Kevin", ""], ["Li", "Yanming", ""], ["Li", "Yi", ""]]}, {"id": "1703.03790", "submitter": "Andrew Noymer", "authors": "Tina Ho and Andrew Noymer", "title": "Summertime, and the livin is easy: Winter and summer pseudoseasonal life\n  expectancy in the United States", "comments": null, "journal-ref": "Demographic Research 2017; vol. 37, art.45, pp: 1445-1476", "doi": "10.4054/DemRes.2017.37.45", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In temperate climates, mortality is seasonal with a winter-dominant pattern,\ndue in part to pneumonia and influenza. Cardiac causes, which are the leading\ncause of death in the United States, are also winter-seasonal although it is\nnot clear why. Interactions between circulating respiratory viruses (f.e.,\ninfluenza) and cardiac conditions have been suggested as a cause of\nwinter-dominant mortality patterns. We propose and implement a way to estimate\nan upper bound on mortality attributable to winter-dominant viruses like\ninfluenza. We calculate 'pseudo-seasonal' life expectancy, dividing the year\ninto two six-month spans, one encompassing winter the other summer. During the\nsummer when the circulation of respiratory viruses is drastically reduced, life\nexpectancy is about one year longer. We also quantify the seasonal mortality\ndifference in terms of seasonal \"equivalent ages\" (defined herein) and\nproportional hazards. We suggest that even if viruses cause excess winter\ncardiac mortality, the population-level mortality reduction of a perfect\ninfluenza vaccine would be much more modest than is often recognized.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 18:41:00 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Ho", "Tina", ""], ["Noymer", "Andrew", ""]]}, {"id": "1703.03853", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Subhajit Sengupta, Peter Mueller and Yuan Ji", "title": "TreeClone: Reconstruction of Tumor Subclone Phylogeny Based on Mutation\n  Pairs using Next Generation Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TreeClone, a latent feature allocation model to reconstruct tumor\nsubclones subject to phylogenetic evolution that mimics tumor evolution.\nSimilar to most current methods, we consider data from next-generation\nsequencing of tumor DNA. Unlike most methods that use information in short\nreads mapped to single nucleotide variants (SNVs), we consider subclone\nphylogeny reconstruction using pairs of two proximal SNVs that can be mapped by\nthe same short reads. As part of the Bayesian inference model, we construct a\nphylogenetic tree prior. The use of the tree structure in the prior greatly\nstrengthens inference. Only subclones that can be explained by a phylogenetic\ntree are assigned non-negligible probabilities. The proposed Bayesian framework\nimplies posterior distributions on the number of subclones, their genotypes,\ncellular proportions, and the phylogenetic tree spanned by the inferred\nsubclones. The proposed method is validated against different sets of simulated\nand real-world data using single and multiple tumor samples. An open source\nsoftware package is available at http://www.compgenome.org/treeclone.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:03:17 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 05:52:55 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Zhou", "Tianjian", ""], ["Sengupta", "Subhajit", ""], ["Mueller", "Peter", ""], ["Ji", "Yuan", ""]]}, {"id": "1703.03862", "submitter": "Jesus Daniel Arroyo Reli\\'on", "authors": "Shangsi Wang, Jes\\'us Arroyo, Joshua T. Vogelstein, Carey E. Priebe", "title": "Joint Embedding of Graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2019.2948619", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and dimension reduction for networks is critical in a wide\nvariety of domains. Efficiently and accurately learning features for multiple\ngraphs has important applications in statistical inference on graphs. We\npropose a method to jointly embed multiple undirected graphs. Given a set of\ngraphs, the joint embedding method identifies a linear subspace spanned by rank\none symmetric matrices and projects adjacency matrices of graphs into this\nsubspace. The projection coefficients can be treated as features of the graphs,\nwhile the embedding components can represent vertex features. We also propose a\nrandom graph model for multiple graphs that generalizes other classical models\nfor graphs. We show through theory and numerical experiments that under the\nmodel, the joint embedding method produces estimates of parameters with small\nerrors. Via simulation experiments, we demonstrate that the joint embedding\nmethod produces features which lead to state of the art performance in\nclassifying graphs. Applying the joint embedding method to human brain graphs,\nwe find it extracts interpretable features with good prediction accuracy in\ndifferent tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:46:09 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 01:19:23 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 03:58:47 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 16:15:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Shangsi", ""], ["Arroyo", "Jes\u00fas", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1703.04056", "submitter": "Phebe Kemmer", "authors": "Phebe Brenne Kemmer, F. DuBois Bowman, Helen Mayberg, Ying Guo", "title": "Quantifying the strength of structural connectivity underlying\n  functional brain networks", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been strong interest in neuroscience studies to\ninvestigate brain organization through networks of brain regions that\ndemonstrate strong functional connectivity (FC). These networks are extracted\nfrom observed fMRI using data-driven analytic methods such as independent\ncomponent analysis (ICA). A notable limitation of these FC methods is that they\ndo not provide any information on the underlying structural connectivity (SC),\nwhich is believed to serve as the basis for interregional interactions in brain\nactivity. We propose a new statistical measure of the strength of SC (sSC)\nunderlying FC networks obtained from data-driven methods. The sSC measure is\ndeveloped using information from diffusion tensor imaging (DTI) data, and can\nbe applied to compare the strength of SC across different FC networks.\nFurthermore, we propose a reliability index for data-driven FC networks to\nmeasure the reproducibility of the networks through re-sampling the observed\ndata. To perform statistical inference such as hypothesis testing on the sSC,\nwe develop a formal variance estimator of sSC based a spatial semivariogram\nmodel with a novel distance metric. We demonstrate the performance of the sSC\nmeasure and its estimation and inference methods with simulation studies. For\nreal data analysis, we apply our methods to a multimodal imaging study with\nresting-state fMRI and DTI data from 20 healthy controls and 20 subjects with\nmajor depressive disorder. Results show that well-known resting state networks\nall demonstrate higher SC within the network as compared to the average\nstructural connections across the brain. We also found that sSC is positively\nassociated with the reliability index, indicating that the FC networks that\nhave stronger underlying SC are more reproducible across samples.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 02:07:37 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Kemmer", "Phebe Brenne", ""], ["Bowman", "F. DuBois", ""], ["Mayberg", "Helen", ""], ["Guo", "Ying", ""]]}, {"id": "1703.04081", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Lena A. J\\\"ager, Bruno Nicenboim", "title": "Feature overwriting as a finite mixture process: Evidence from\n  comprehension data", "comments": "6 pages, 2 figures, 1 table, submitted to MathPsych/ICCM 2017,\n  Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ungrammatical sentence \"The key to the cabinets are on the table\" is\nknown to lead to an illusion of grammaticality. As discussed in the\nmeta-analysis by Jaeger et al., 2017, faster reading times are observed at the\nverb are in the agreement-attraction sentence above compared to the equally\nungrammatical sentence \"The key to the cabinet are on the table\". One\nexplanation for this facilitation effect is the feature percolation account:\nthe plural feature on cabinets percolates up to the head noun key, leading to\nthe illusion. An alternative account is in terms of cue-based retrieval (Lewis\n& Vasishth, 2005), which assumes that the non-subject noun cabinets is\nmisretrieved due to a partial feature-match when a dependency completion\nprocess at the auxiliary initiates a memory access for a subject with plural\nmarking. We present evidence for yet another explanation for the observed\nfacilitation. Because the second sentence has two nouns with identical number,\nit is possible that these are, in some proportion of trials, more difficult to\nkeep distinct, leading to slower reading times at the verb in the first\nsentence above; this is the feature overwriting account of Nairne, 1990. We\nshow that the feature overwriting proposal can be implemented as a finite\nmixture process. We reanalysed ten published data-sets, fitting hierarchical\nBayesian mixture models to these data assuming a two-mixture distribution. We\nshow that in nine out of the ten studies, a mixture distribution corresponding\nto feature overwriting furnishes a superior fit over both the feature\npercolation and the cue-based retrieval accounts.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 08:11:29 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 08:15:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Vasishth", "Shravan", ""], ["J\u00e4ger", "Lena A.", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1703.04312", "submitter": "Felipe Tagle", "authors": "Felipe Tagle, Stefano Castruccio, Paola Crippa, Marc G. Genton", "title": "A Non-Gaussian Spatio-Temporal Model for Daily Wind Speeds Based on a\n  Multivariate Skew-t Distribution", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing increasing domestic energy consumption from population growth and\nindustrialization, Saudi Arabia is aiming to reduce its reliance on fossil\nfuels and to broaden its energy mix by expanding investment in renewable energy\nsources, including wind energy. A preliminary task in the development of wind\nenergy infrastructure is the assessment of wind energy potential, a key aspect\nof which is the characterization of its spatio-temporal behavior. In this study\nwe examine the impact of internal climate variability on seasonal wind power\ndensity fluctuations over Saudi Arabia using 30 simulations from the Large\nEnsemble Project (LENS) developed at the National Center for Atmospheric\nResearch. Furthermore, a spatio-temporal model for daily wind speed is proposed\nwith neighbor-based cross-temporal dependence, and a multivariate skew-t\ndistribution to capture the spatial patterns of higher order moments. The model\ncan be used to generate synthetic time series over the entire spatial domain\nthat adequately reproduce the internal variability of the LENS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 10:14:20 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 06:11:32 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Tagle", "Felipe", ""], ["Castruccio", "Stefano", ""], ["Crippa", "Paola", ""], ["Genton", "Marc G.", ""]]}, {"id": "1703.04341", "submitter": "Sofia S. Villar", "authors": "Sofia S. Villar, Jack Bowden and James Wason", "title": "Response adaptive designs for binary responses: how to offer patient\n  benefit while being robust to time trends?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-adaptive randomisation (RAR) can considerably improve the chances of\na successful treatment outcome for patients in a clinical trial by skewing the\nallocation probability towards better performing treatments as data\naccumulates. There is considerable interest in using RAR designs in drug\ndevelopment for rare diseases, where traditional designs are not feasible or\nethically objectionable. In this paper we discuss and address a major criticism\nof RAR: the undesirable type I error inflation due to unknown time trends in\nthe trial. Time trends can appear because of changes in the characteristics of\nrecruited patients - so-called \"patient drift\". Patient drift is a realistic\nconcern for clinical trials in rare diseases because these typically recruit\npatients over a very long period of time. We compute by simulations how large\nthe type I error inflation is as a function of the time trend magnitude in\norder to determine in which contexts a potentially costly correction is\nactually necessary. We then assess the ability of different correction methods\nto preserve type I error in this context and their performance in terms of\nother operating characteristics, including patient benefit and power. We make\nrecommendations of which correction methods are most suitable in the rare\ndisease context for several RAR rules, differentiating between the two-armed\nand the multi-armed case. We further propose a RAR design for multi-armed\nclinical trials, which is computationally cheap and robust to several time\ntrends considered.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:31:42 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Villar", "Sofia S.", ""], ["Bowden", "Jack", ""], ["Wason", "James", ""]]}, {"id": "1703.04642", "submitter": "Alfonso Garc\\'ia-P\\'erez", "authors": "A. Garcia-Perez and M.A. Cabrero-Ortega", "title": "Robust Morphometric Analysis based on Landmarks. Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procrustes Analysis is a Morphometric method based on Configurations of\nLandmarks that estimates the superimposition parameters by least-squares; for\nthis reason, the procedure is very sensitive to outliers. In the first part of\nthe paper we robustify this technique to classify individuals from a\ndescriptive point of view. In the literature there are also classical results,\nbased on the normality of the observations, to test whether there are\nsignificant differences between individuals. In the second part of the paper we\ndetermine a Von Mises plus Saddlepoint approximation for the tail probability\nof the Procrustes Statistic when the observations come from a model close to\nthe normal. We conclude the paper with some applications using the Geographical\nInformation System QGIS.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:16:06 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Garcia-Perez", "A.", ""], ["Cabrero-Ortega", "M. A.", ""]]}, {"id": "1703.04812", "submitter": "Enrique Calderin", "authors": "Emilio Gomez-Deniz, Enrique Calderin-Ojeda", "title": "An alternative representation of the negative binomial-Lindley\n  distribution. New results and applications", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an alternative representation of the Negative\nBinomial--Lindley distribution recently proposed by Zamani and Ismail (2010)\nwhich shows some advantages over the latter model. This new formulation\nprovides a tractable model with attractive properties which makes it suitable\nfor application not only in insurance settings but also in other fields where\noverdispersion is observed. Basic properties of the new distribution are\nstudied. A recurrence for the probabilities of the new distribution and an\nintegral equation for the probability density function of the compound version,\nwhen the claim severities are absolutely continuous, are derived. Estimation\nmethods are discussed and a numerical application is given.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 23:04:14 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Gomez-Deniz", "Emilio", ""], ["Calderin-Ojeda", "Enrique", ""]]}, {"id": "1703.04957", "submitter": "James Johndrow", "authors": "James E. Johndrow and Kristian Lum", "title": "An algorithm for removing sensitive information: application to\n  race-independent recidivism prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing or augmenting human\njudgment with computer models in high stakes settings-- such as sentencing,\nhiring, policing, college admissions, and parole decisions-- is the perceived\n\"neutrality\" of computers. It is argued that because computer models do not\nhold personal prejudice, the predictions they produce will be equally free from\nprejudice. There is growing recognition that employing algorithms does not\nremove the potential for bias, and can even amplify it if the training data\nwere generated by a process that is itself biased. In this paper, we provide a\nprobabilistic notion of algorithmic bias. We propose a method to eliminate bias\nfrom predictive models by removing all information regarding protected\nvariables from the data to which the models will ultimately be trained. Unlike\nprevious work in this area, our framework is general enough to accommodate data\non any measurement scale. Motivated by models currently in use in the criminal\njustice system that inform decisions on pre-trial release and parole, we apply\nour proposed method to a dataset on the criminal histories of individuals at\nthe time of sentencing to produce \"race-neutral\" predictions of re-arrest. In\nthe process, we demonstrate that a common approach to creating \"race-neutral\"\nmodels-- omitting race as a covariate-- still results in racially disparate\npredictions. We then demonstrate that the application of our proposed method to\nthese data removes racial disparities from predictions with minimal impact on\npredictive accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:36:58 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Johndrow", "James E.", ""], ["Lum", "Kristian", ""]]}, {"id": "1703.04961", "submitter": "Christina Bogner", "authors": "Christina Bogner, Anna K\\\"uhnel and Bernd Huwe", "title": "Predicting with limited data - Increasing the accuracy in VIS-NIR\n  diffuse reflectance spectroscopy by SMOTE", "comments": "4 pages, 2 figures, This is the final version of the full\n  peer-reviewed paper presented at the 6th GRSS Workshop on Hyperspectral Image\n  and Signal Processing: Evolution in Remote Sensing (WHISPERS) in Lausanne,\n  Switzerland, June 25-27, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffuse reflectance spectroscopy is a powerful technique to predict soil\nproperties. It can be used in situ to provide data inexpensively and rapidly\ncompared to the standard laboratory measurements. Because most spectral data\nbases contain air-dried samples scanned in the laboratory, field spectra\nacquired in situ are either absent or rare in calibration data sets. However,\nwhen models are calibrated on air-dried spectra, prediction using field spectra\nare often inaccurate. We propose a framework to calibrate partial least squares\nmodels when field spectra are rare using synthetic minority oversampling\ntechnique (SMOTE). We calibrated a model to predict soil organic carbon content\nusing air-dried spectra spiked with synthetic field spectra. The root\nmean-squared error of prediction decreased from 6.18 to 2.12 mg g$^{-1}$ and\n$R^2$ increased from $-$0.53 to 0.82 compared to the model calibrated on\nair-dried spectra only.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:42:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Bogner", "Christina", ""], ["K\u00fchnel", "Anna", ""], ["Huwe", "Bernd", ""]]}, {"id": "1703.05103", "submitter": "Alina Peluso", "authors": "Alina Peluso, Paolo Berta, Veronica Vinciotti", "title": "Do pay-for-performance incentives lead to a better health outcome?", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pay-for-performance approaches have been widely adopted in order to drive\nimprovements in the quality of healthcare provision. Previous studies\nevaluating the impact of these programs are either limited by the number of\nhealth outcomes or of medical conditions considered. In this paper, we evaluate\nthe effectiveness of a pay-for-performance program on the basis of five health\noutcomes and across a wide range of medical conditions. The context of the\nstudy is the Lombardy region in Italy, where a rewarding program was introduced\nin 2012. The policy evaluation is based on a difference-in-differences\napproach. The model includes multiple dependent outcomes, that allow\nquantifying the joint effect of the program, and random effects, that account\nfor the heterogeneity of the data at the ward and hospital level. Our results\nshow that the policy had a positive effect on the hospitals' performance in\nterms of those outcomes that can be more influenced by a managerial activity,\nnamely the number of readmissions, transfers and returns to the surgery room.\nNo significant changes which can be related to the pay-for-performance\nintroduction are observed for the number of voluntary discharges and for\nmortality. Finally, our study shows evidence that the medical wards have\nreacted more strongly to the pay-for-performance program than the surgical\nones, whereas only limited evidence is found in support of a different policy\nreaction across different types of hospital ownership.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:07:47 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Peluso", "Alina", ""], ["Berta", "Paolo", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1703.05172", "submitter": "Sofia S. Villar", "authors": "Adam Smith and Sofia S. Villar", "title": "Bayesian adaptive bandit-based designs using the Gittins index for\n  multi-armed trials with normally distributed endpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive designs for multi-armed clinical trials have become increasingly\npopular recently in many areas of medical research because of their potential\nto shorten development times and to increase patient response. However,\ndeveloping response-adaptive trial designs that offer patient benefit while\nensuring the resulting trial avoids bias and provides a statistically rigorous\ncomparison of the different treatments included is highly challenging. In this\npaper, the theory of Multi-Armed Bandit Problems is used to define a family of\nnear optimal adaptive designs in the context of a clinical trial with a\nnormally distributed endpoint with known variance. Through simulation studies\nbased on an ongoing trial as a motivation we report the operating\ncharacteristics (type I error, power, bias) and patient benefit of these\napproaches and compare them to traditional and existing alternative designs.\nThese results are then compared to those recently published in the context of\nBernoulli endpoints. Many limitations and advantages are similar in both cases\nbut there are also important differences, specially with respect to type I\nerror control. This paper proposes a simulation-based testing procedure to\ncorrect for the observed type I error inflation that bandit-based and adaptive\nrules can induce. Results presented extend recent work by considering a\nnormally distributed endpoint, a very common case in clinical practice yet\nmostly ignored in the response-adaptive theoretical literature, and illustrate\nthe potential advantages of using these methods in a rare disease context. We\nalso recommend a suitable modified implementation of the bandit-based adaptive\ndesigns for the case of common diseases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:22:09 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Smith", "Adam", ""], ["Villar", "Sofia S.", ""]]}, {"id": "1703.05264", "submitter": "Bowei Yan", "authors": "Ying Liu, Bowei Yan, Kathleen Merikangas and Haochang Shou", "title": "Total Variation Regularized Tensor-on-scalar Regression", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Total Variation Regularized Tensor-on-scalar\nRegression(TVTR), a novel method for estimating the association between a\ntensor outcome (a one dimensional or multidimensional array) and scalar\npredictors. While the statistical developments proposed here were motivated by\nthe brain mapping and activity tracking, the methodology is designed and\npresented in generality and is applicable to many other areas of scientific\nresearch. The estimator is the solution of a penalized regression problem where\nthe objective is the sum of square error plus a total variation (TV)\nregularization on the predicted mean across all subjects. We propose an\nalgorithm for the parameter estimation, which is efficient and scalable in\ndistributed computing platform. Proof of the algorithm convergence is provided\nand the statistical consistency of the estimator is presented via an oracle\ninequality. We presented 1D and 2D simulation results, and demonstrate that\nTVTR outperforms existing methods in most cases. We also demonstrate the\ngeneral applicability of the method by two real data examples including the\nanalysis of the 1D accelerometry subsample of a large community-based study for\nmood disorders and the analysis of the 3D MRI data from the attention\ndeficient/hyperactive deficient (ADHD) 200 consortium.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 17:03:56 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 22:37:24 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 22:48:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liu", "Ying", ""], ["Yan", "Bowei", ""], ["Merikangas", "Kathleen", ""], ["Shou", "Haochang", ""]]}, {"id": "1703.05339", "submitter": "M\\'arton S\\'oskuthy", "authors": "M\\'arton S\\'oskuthy", "title": "Generalised additive mixed models for dynamic analysis in linguistics: a\n  practical introduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a hands-on introduction to Generalised Additive Mixed Models (GAMMs)\nin the context of linguistics with a particular focus on dynamic speech\nanalysis (e.g. formant contours, pitch tracks, diachronic change, etc.). The\nmain goal is to explain some of the main ideas underlying GAMMs, and to provide\na practical guide to frequentist significance testing using these models. The\nintroduction covers a range of topics including basis functions, the smoothing\npenalty, random smooths, difference smooths, smooth interactions, model\ncomparison and autocorrelation. It is divided into two parts. The first part\nlooks at what GAMMs are, how they work and why/when we should use them.\nAlthough the reader can replicate some of the example analyses in this section,\nthis is not essential. The second part is a tutorial introduction that\nillustrates the process of fitting and evaluating GAMMs in the R statistical\nsoftware environment, and the reader is strongly encouraged to work through the\nexamples on their own machine.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 18:04:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["S\u00f3skuthy", "M\u00e1rton", ""]]}, {"id": "1703.05502", "submitter": "Evgeny Burnaev", "authors": "Denis Volkhonskiy and Ivan Nazarov and Evgeny Burnaev", "title": "Steganographic Generative Adversarial Networks", "comments": "15 pages, 10 figures, 5 tables, Workshop on Adversarial Training\n  (NIPS 2016, Barcelona, Spain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganography is collection of methods to hide secret information (\"payload\")\nwithin non-secret information \"container\"). Its counterpart, Steganalysis, is\nthe practice of determining if a message contains a hidden payload, and\nrecovering it if possible. Presence of hidden payloads is typically detected by\na binary classifier. In the present study, we propose a new model for\ngenerating image-like containers based on Deep Convolutional Generative\nAdversarial Networks (DCGAN). This approach allows to generate more\nsetganalysis-secure message embedding using standard steganography algorithms.\nExperiment results demonstrate that the new model successfully deceives the\nsteganography analyzer, and for this reason, can be used in steganographic\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 08:28:11 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 19:56:14 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Nazarov", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1703.05532", "submitter": "Asis Chattopadhyay", "authors": "Soumita Modak, Asis Kumar Chattopadhyay and Tanuka Chattopadhyay", "title": "Clustering of Gamma-Ray bursts through kernel principal component\n  analysis", "comments": "30 pages, 10 figures", "journal-ref": "Communications in Statistics- Simulation and Computation, 2018,\n  47, 1088-1102", "doi": "10.1080/03610918.2017.1307393", "report-no": null, "categories": "stat.AP astro-ph.HE astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem related to clustering of gamma-ray bursts (from\n\"BATSE\" catalogue) through kernel principal component analysis in which our\nproposed kernel outperforms results of other competent kernels in terms of\nclustering accuracy and we obtain three physically interpretable groups of\ngamma-ray bursts. The effectivity of the suggested kernel in combination with\nkernel principal component analysis in revealing natural clusters in noisy and\nnonlinear data while reducing the dimension of the data is also explored in two\nsimulated data sets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 09:37:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Modak", "Soumita", ""], ["Chattopadhyay", "Asis Kumar", ""], ["Chattopadhyay", "Tanuka", ""]]}, {"id": "1703.05545", "submitter": "Lewis Mitchell", "authors": "Peter Mathews, Lewis Mitchell, Giang T. Nguyen, Nigel G. Bean", "title": "The nature and origin of heavy tails in retweet activity", "comments": "To appear in MSM 2017: 8th International Workshop on Modelling Social\n  Media: Machine Learning and AI for Modelling and Analysing Social Media,\n  April 2017, Perth, Australia", "journal-ref": null, "doi": "10.1145/3041021.3053903", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social media platforms facilitate the rapid spread of information\nonline. Modelling phenomena such as social contagion and information diffusion\nare contingent upon a detailed understanding of the information-sharing\nprocesses. In Twitter, an important aspect of this occurs with retweets, where\nusers rebroadcast the tweets of other users. To improve our understanding of\nhow these distributions arise, we analyse the distribution of retweet times. We\nshow that a power law with exponential cutoff provides a better fit than the\npower laws previously suggested. We explain this fit through the burstiness of\nhuman behaviour and the priorities individuals place on different tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 10:06:45 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Mathews", "Peter", ""], ["Mitchell", "Lewis", ""], ["Nguyen", "Giang T.", ""], ["Bean", "Nigel G.", ""]]}, {"id": "1703.05687", "submitter": "Robert Richardson", "authors": "Robert R. Richardson, Michael A. Osborne and David A. Howey", "title": "Gaussian process regression for forecasting battery state of health", "comments": "13 pages, 7 figures, published in the Journal of Power Sources, 2017", "journal-ref": "Journal of Power Sources, Volume 357, 31 July 2017, Pages 209 to\n  219", "doi": "10.1016/j.jpowsour.2017.05.004", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting the future capacity and remaining useful life of\nbatteries is necessary to ensure reliable system operation and to minimise\nmaintenance costs. The complex nature of battery degradation has meant that\nmechanistic modelling of capacity fade has thus far remained intractable;\nhowever, with the advent of cloud-connected devices, data from cells in various\napplications is becoming increasingly available, and the feasibility of\ndata-driven methods for battery prognostics is increasing. Here we propose\nGaussian process (GP) regression for forecasting battery state of health, and\nhighlight various advantages of GPs over other data-driven and mechanistic\napproaches. GPs are a type of Bayesian non-parametric method, and hence can\nmodel complex systems whilst handling uncertainty in a principled manner. Prior\ninformation can be exploited by GPs in a variety of ways: explicit mean\nfunctions can be used if the functional form of the underlying degradation\nmodel is available, and multiple-output GPs can effectively exploit\ncorrelations between data from different cells. We demonstrate the predictive\ncapability of GPs for short-term and long-term (remaining useful life)\nforecasting on a selection of capacity vs. cycle datasets from lithium-ion\ncells.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:00:00 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:23:37 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Richardson", "Robert R.", ""], ["Osborne", "Michael A.", ""], ["Howey", "David A.", ""]]}, {"id": "1703.05799", "submitter": "Andrea Saltelli", "authors": "Samuele Lo Piano, Federico Ferretti, Arnald Puy, Daniel Albrecht,\n  Stefano Tarantola and Andrea Saltelli", "title": "A new sample-based algorithms to compute the total sensitivity index", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance-based sensitivity indices have established themselves as a reference\namong practitioners of sensitivity analysis of model output. It is not unusual\nto consider a variance-based sensitivity analysis as informative if it produces\nat least the first order sensitivity indices S_j and the so-called total-effect\nsensitivity indices T_j for all the uncertain factors of the mathematical model\nunder analysis. Computational economy is critical in sensitivity analysis. It\ndepends mostly upon the number of model evaluations needed to obtain stable\nvalues of the estimates. While efficient estimation procedures independent from\nthe number of factors under analysis are available for the first order indices,\nthis is less the case for the total sensitivity indices. When estimating T_j,\none can either use a sample-based approach, whose computational cost depends\nfromon the number of factors, or approaches based on meta-modelling/emulators,\ne.g. based on Gaussian processes. The present work focuses on sample-based\nestimation procedures for T_j and tries different avenues to achieve an\nalgorithmic improvement over the designs proposed in the existing best\npractices. We conclude that some proposed sample-based improvements found in\nthe literature do not work as claimed, and that improving on the existing best\npractice is indeed fraught with difficulties. We motivate our conclusions\nintroducing the concepts of explorativity and efficiency of the design.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 19:11:58 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 21:05:58 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Piano", "Samuele Lo", ""], ["Ferretti", "Federico", ""], ["Puy", "Arnald", ""], ["Albrecht", "Daniel", ""], ["Tarantola", "Stefano", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1703.05926", "submitter": "Daniel Graham", "authors": "Daniel J Graham and Cian Naik and Emma J McCoy and Haojie Li", "title": "Quantifying the causal effect of speed cameras on road traffic accidents\n  via an approximate Bayesian doubly robust estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper quantifies the effect of speed cameras on road traffic collisions\nusing an approximate Bayesian doubly-robust (DR) causal inference estimation\nmethod. Previous empirical work on this topic, which shows a diverse range of\nestimated effects, is based largely on outcome regression (OR) models using the\nEmpirical Bayes approach or on simple before and after comparisons. Issues of\ncausality and confounding have received little formal attention. A causal DR\napproach combines propensity score (PS) and OR models to give an average\ntreatment effect (ATE) estimator that is consistent and asymptotically normal\nunder correct specification of either of the two component models. We develop\nthis approach within a novel approximate Bayesian framework to derive posterior\npredictive distributions for the ATE of speed cameras on road traffic\ncollisions. Our results for England indicate significant reductions in the\nnumber of collisions at speed cameras sites (mean ATE = -15%). Our proposed\nmethod offers a promising approach for evaluation of transport safety\ninterventions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 09:01:03 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 09:36:42 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 13:04:25 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 22:07:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Graham", "Daniel J", ""], ["Naik", "Cian", ""], ["McCoy", "Emma J", ""], ["Li", "Haojie", ""]]}, {"id": "1703.06375", "submitter": "Hossein Sangrody", "authors": "Hossein Sangrody, Ning Zhou", "title": "An Initial Study on Load Forecasting Considering Economic Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new objective function and quantile regression (QR)\nalgorithm for load forecasting (LF). In LF, the positive forecasting errors\noften have different economic impact from the negative forecasting errors.\nConsidering this difference, a new objective function is proposed to put\ndifferent prices on the positive and negative forecasting errors. QR is used to\nfind the optimal solution of the proposed objective function. Using normalized\nnet energy load of New England network, the proposed method is compared with a\ntime series method, the artificial neural network method, and the support\nvector machine method. The simulation results show that the proposed method is\nmore effective in reducing the economic cost of the LF errors than the other\nthree methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 00:53:48 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Sangrody", "Hossein", ""], ["Zhou", "Ning", ""]]}, {"id": "1703.06378", "submitter": "Hossein Sangrody", "authors": "Hossein Sangrody, Ning Zhou, Xingye Qiao", "title": "Probabilistic Models for Daily Peak Loads at Distribution Feeders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load forecasting at distribution networks is more challenging than load\nforecasting at transmission networks because its load pattern is more\nstochastic and unpredictable. To plan sufficient resources and estimate DER\nhosting capacity, it is invaluable for a distribution network planner to get\nthe probabilistic distribution of daily peak-load under a feeder over long\nterm. In this paper, we model the probabilistic distribution functions of daily\npeak-load under a feeder using power law distributions, which is tested by\nimproved Kolmogorov Smirnov test enhanced by the Monte Carlo simulation\napproach. In addition, the uncertainty of the modeling is quantified using the\nbootstrap method. The methodology of parameter estimation of the probabilistic\nmodel and the hypothesis test is elaborated in detail. In the case studies, it\nis shown using measurement data sets that the daily peak loads under several\nfeeders follow the power law distribution by applying the proposed testing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 01:14:47 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Sangrody", "Hossein", ""], ["Zhou", "Ning", ""], ["Qiao", "Xingye", ""]]}, {"id": "1703.06602", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha", "title": "Dual Lasso Selector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model selection and estimation in sparse high\ndimensional linear regression models with strongly correlated variables. First,\nwe study the theoretical properties of the dual Lasso solution, and we show\nthat joint consideration of the Lasso primal and its dual solutions are useful\nfor selecting correlated active variables. Second, we argue that correlations\namong active predictors are not problematic, and we derive a new weaker\ncondition on the design matrix, called Pseudo Irrepresentable Condition (PIC).\nThird, we present a new variable selection procedure, Dual Lasso Selector, and\nwe prove that the PIC is a necessary and sufficient condition for consistent\nvariable selection for the proposed method. Finally, by combining the dual\nLasso selector further with the Ridge estimation even better prediction\nperformance is achieved. We call the combination (DLSelect+Ridge), it can be\nviewed as a new combined approach for inference in high-dimensional regression\nmodels with correlated variables. We illustrate DLSelect+Ridge method and\ncompare it with popular existing methods in terms of variable selection,\nprediction accuracy, estimation accuracy and computation speed by considering\nvarious simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 05:18:30 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "1703.06603", "submitter": "Pritam Ranjan", "authors": "Sujay Mukhoti and Pritam Ranjan", "title": "A New Class of Discrete-time Stochastic Volatility Model with Correlated\n  Errors", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an efficient stock market, the returns and their time-dependent volatility\nare often jointly modeled by stochastic volatility models (SVMs). Over the last\nfew decades several SVMs have been proposed to adequately capture the defining\nfeatures of the relationship between the return and its volatility. Among one\nof the earliest SVM, Taylor (1982) proposed a hierarchical model, where the\ncurrent return is a function of the current latent volatility, which is further\nmodeled as an auto-regressive process. In an attempt to make the SVMs more\nappropriate for complex realistic market behavior, a leverage parameter was\nintroduced in the Taylor SVM, which however led to the violation of the\nefficient market hypothesis (EMH, a necessary mean-zero condition for the\nreturn distribution that prevents arbitrage possibilities). Subsequently, a\nhost of alternative SVMs had been developed and are currently in use. In this\npaper, we propose mean-corrections for several generalizations of Taylor SVM\nthat capture the complex market behavior as well as satisfy EMH. We also\nestablish a few theoretical results to characterize the key desirable features\nof these models, and present comparison with other popular competitors.\nFurthermore, four real-life examples (Oil price, CITI bank stock price,\nEuro-USD rate, and S&P 500 index returns) have been used to demonstrate the\nperformance of this new class of SVMs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 05:18:45 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Mukhoti", "Sujay", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1703.06645", "submitter": "Paul Sheridan", "authors": "Paul Sheridan and Taku Onodera", "title": "A Preferential Attachment Paradox: How Preferential Attachment Combines\n  with Growth to Produce Networks with Log-normal In-degree Distributions", "comments": "13 pages, 4 figures, 2 table, 1 supplementary notes file; fixed\n  broken figure and table references", "journal-ref": "Scientific Reports 8 (1), 1-11 (2020)", "doi": "10.1038/s41598-018-21133-2", "report-no": null, "categories": "cs.DL cond-mat.stat-mech physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every network scientist knows that preferential attachment combines with\ngrowth to produce networks with power-law in-degree distributions. How, then,\nis it possible for the network of American Physical Society journal collection\ncitations to enjoy a log-normal citation distribution when it was found to have\ngrown in accordance with preferential attachment? This anomalous result, which\nwe exalt as the preferential attachment paradox, has remained unexplained since\nthe physicist Sidney Redner first made light of it over a decade ago. Here we\npropose a resolution. The chief source of the mischief, we contend, lies in\nRedner having relied on a measurement procedure bereft of the accuracy required\nto distinguish preferential attachment from another form of attachment that is\nconsistent with a log-normal in-degree distribution. There was a high-accuracy\nmeasurement procedure in use at the time, but it would have have been difficult\nto use it to shed light on the paradox, due to the presence of a systematic\nerror inducing design flaw. In recent years the design flaw had been recognised\nand corrected. We show that the bringing of the newly corrected measurement\nprocedure to bear on the data leads to a resolution of the paradox.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 09:41:34 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 06:10:31 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 22:10:53 GMT"}, {"version": "v4", "created": "Mon, 22 Jan 2018 01:51:22 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Sheridan", "Paul", ""], ["Onodera", "Taku", ""]]}, {"id": "1703.06670", "submitter": "Kai G\\\"orgen", "authors": "Kai G\\\"orgen (1), Martin N. Hebart (2 and 3), Carsten Allefeld (1 and\n  6), John-Dylan Haynes (1 and 4 and 5 and 6) ((1) Charite, FU, HU, BIH, BCCN,\n  BCAN, Neurocure, Berlin, (2) University Medical Center Hamburg-Eppendorf, (3)\n  NIMH, Bethesda, (4) Mind and Brain, HU Berlin, (5) TU Dresden, (6) Equal\n  contribution)", "title": "The Same Analysis Approach: Practical protection against the pitfalls of\n  novel neuroimaging analysis methods", "comments": "Manuscript [29 pages, 7 Figures] + Supplemental Information [21\n  pages, 13 Figures], published in NeuroImage as: G\\\"orgen, K., Hebart, M. N.,\n  Allefeld, C., & Haynes, J.-D. (2018). The same analysis approach: Practical\n  protection against the pitfalls of novel neuroimaging analysis methods.\n  NeuroImage 180, 19-30. doi:10.1016/j.neuroimage.2017.12.083", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.12.083", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard neuroimaging data analysis based on traditional principles of\nexperimental design, modelling, and statistical inference is increasingly\ncomplemented by novel analysis methods, driven e.g. by machine learning\nmethods. While these novel approaches provide new insights into neuroimaging\ndata, they often have unexpected properties, generating a growing literature on\npossible pitfalls. We propose to meet this challenge by adopting a habit of\nsystematic testing of experimental design, analysis procedures, and statistical\ninference. Specifically, we suggest to apply the analysis method used for\nexperimental data also to aspects of the experimental design, simulated\nconfounds, simulated null data, and control data. We stress the importance of\nkeeping the analysis method the same in main and test analyses, because only\nthis way possible confounds and unexpected properties can be reliably detected\nand avoided. We describe and discuss this Same Analysis Approach in detail, and\ndemonstrate it in two worked examples using multivariate decoding. With these\nexamples, we reveal two sources of error: A mismatch between counterbalancing\n(crossover designs) and cross-validation which leads to systematic below-chance\naccuracies, and linear decoding of a nonlinear effect, a difference in\nvariance.\n  Highlights: 1. Traditional design principles can be unsuitable when combined\nwith cross-validation; 2. This can explain both inflated accuracies and\nbelow-chance accuracies; 3. We propose the novel \"same analysis approach\" (SAA)\nfor checking analysis pipelines; 4. The principle of SAA is to perform\nadditional analyses using the same analysis; 5. SAA analysis should be\nperformed on design variables, control data, and simulations\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 11:01:15 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 17:19:46 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 15:38:21 GMT"}, {"version": "v4", "created": "Thu, 31 Aug 2017 17:25:27 GMT"}, {"version": "v5", "created": "Mon, 5 Feb 2018 15:11:47 GMT"}, {"version": "v6", "created": "Wed, 26 Sep 2018 10:10:29 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["G\u00f6rgen", "Kai", "", "2 and 3"], ["Hebart", "Martin N.", "", "2 and 3"], ["Allefeld", "Carsten", "", "1 and\n  6"], ["Haynes", "John-Dylan", "", "1 and 4 and 5 and 6"]]}, {"id": "1703.06719", "submitter": "Behnaz Pirzamanbein", "authors": "Behnaz Pirzamanbein, Anneli Poska, and Johan Lindstr\\\"om", "title": "Bayesian reconstruction of past land-cover from pollen data: model\n  robustness and sensitivity to auxiliary variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic depictions of past land cover are needed to investigate prehistoric\nenvironmental changes, effects of anthropogenic deforestation, and long term\nland cover-climate feedbacks. Observation based reconstructions of past land\ncover are rare and commonly used model based reconstructions exhibit\nconsiderable differences. Recently \\citet[Spatial Statistics,\n24:14--31,][]{PirzaLPG2018_24} developed a statistical interpolation method\nthat produces spatially complete reconstructions of past land cover from pollen\nassemblage. These reconstructions incorporate a number of auxiliary datasets\nraising questions regarding the method's sensitivity to different auxiliary\ndatasets.\n  Here the sensitivity of the method is examined by performing spatial\nreconstructions for northern Europe during three time periods (1900 CE, 1725 CE\nand 4000 BCE). The auxiliary datasets considered include the most commonly\nutilized sources of past land-cover data --- e.g.\\ estimates produced by a\ndynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models.\nFive different auxiliary datasets were considered, including different climate\ndata driving the DVM and different ALCC models. The resulting reconstructions\nwere also evaluated using cross-validation for all the time periods. For the\nrecent time period, 1900 CE, the different land-cover reconstructions were\ncompared against a present day forest map.\n  The validation confirms that the statistical model provides a robust spatial\ninterpolation tool with low sensitivity to differences in auxiliary data and\nhigh capacity to capture information in the pollen based proxy data. Further\nauxiliary data with high spatial detail improves model performance for areas\nwith complex topography or few observations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 12:52:20 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 22:33:34 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Pirzamanbein", "Behnaz", ""], ["Poska", "Anneli", ""], ["Lindstr\u00f6m", "Johan", ""]]}, {"id": "1703.06804", "submitter": "Marcio Laurini", "authors": "Marcio Poletti Laurini", "title": "A continuous spatio-temporal approach to estimate climate change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for decomposition of trend, cycle and seasonal\ncomponents in spatio-temporal models and apply it to investigate the existence\nof climate changes in temperature and rainfall series. The method incorporates\ncritical features in the analysis of climatic problems - the importance of\nspatial heterogeneity, information from a large number of weather stations, and\nthe presence of missing data. The spatial component is based on continuous\nprojections of spatial covariance functions, allowing modeling the complex\npatterns of dependence observed in climatic data.\n  We apply this method to study climate changes in the Northeast region of\nBrazil, characterized by a great wealth of climates and large amplitudes of\ntemperatures and rainfall. The results show the presence of a tendency for\ntemperature increases, indicating changes in the climatic patterns in this\nregion.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 15:36:13 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Laurini", "Marcio Poletti", ""]]}, {"id": "1703.06808", "submitter": "Luis Fernando Campos", "authors": "Luke W. Miratrix, Jasjeet S. Sekhon, Alexander G. Theodoridis, Luis F.\n  Campos", "title": "Worth Weighting? How to Think About and Use Weights in Survey\n  Experiments", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of online surveys has increased the prominence of using\nweights that capture units' probabilities of inclusion for claims of\nrepresentativeness. Yet, much uncertainty remains regarding how these weights\nshould be employed in the analysis of survey experiments: Should they be used\nor ignored? If they are used, which estimators are preferred? We offer\npractical advice, rooted in the Neyman-Rubin model, for researchers producing\nand working with survey experimental data. We examine simple, efficient\nestimators for analyzing these data, and give formulae for their biases and\nvariances. We provide simulations that examine these estimators as well as real\nexamples from experiments administered online through YouGov. We find that for\nexamining the existence of population treatment effects using high-quality,\nbroadly representative samples recruited by top online survey firms, sample\nquantities, which do not rely on weights, are often sufficient. We found that\nSample Average Treatment Effect (SATE) estimates did not appear to differ\nsubstantially from their weighted counterparts, and they avoided the\nsubstantial loss of statistical power that accompanies weighting. When precise\nestimates of Population Average Treatment Effects (PATE) are essential, we\nanalytically show post-stratifying on survey weights and/or covariates highly\ncorrelated with the outcome to be a conservative choice. While we show these\nsubstantial gains in simulations, we find limited evidence of them in practice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 15:45:44 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 15:05:44 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 22:55:41 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 13:48:15 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Miratrix", "Luke W.", ""], ["Sekhon", "Jasjeet S.", ""], ["Theodoridis", "Alexander G.", ""], ["Campos", "Luis F.", ""]]}, {"id": "1703.06933", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Andreas Wieser, and Xuezhi Tan", "title": "Fast Radio Map Construction and Position Estimation via Direct Mapping\n  for WLAN Indoor Localization System", "comments": "more refined analysis required", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main limitation that constrains the fast and comprehensive application of\nWireless Local Area Network (WLAN) based indoor localization systems with\nReceived Signal Strength (RSS) positioning algorithms is the building of the\nfingerprinting radio map, which is time-consuming especially when the indoor\nenvironment is large and/or with high frequent changes. Different approaches\nhave been proposed to reduce workload, including fingerprinting deployment and\nupdate efforts, but the performance degrades greatly when the workload is\nreduced below a certain level. In this paper, we propose an indoor localization\nscenario that applies metric learning and manifold alignment to realize direct\nmapping localization (DML) using a low resolution radio map with single sample\nof RSS that reduces the fingerprinting workload by up to 87\\%. Compared to\nprevious work. The proposed two localization approaches, DML and $k$ nearest\nneighbors based on reconstructed radio map (reKNN), were shown to achieve less\nthan 4.3\\ m and 3.7\\ m mean localization error respectively in a typical office\nenvironment with an area of approximately 170\\ m$^2$, while the unsupervised\nlocalization with perturbation algorithm was shown to achieve 4.7\\ m mean\nlocalization error with 8 times more workload than the proposed methods. As for\nthe room level localization application, both DML and reKNN can meet the\nrequirement with at most 9\\ m of localization error which is enough to tell\napart different rooms with over 99\\% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 19:46:32 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 13:51:01 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""], ["Tan", "Xuezhi", ""]]}, {"id": "1703.06946", "submitter": "Ashley Petersen", "authors": "Ashley Petersen, Noah Simon, Daniela Witten", "title": "SCALPEL: Extracting Neurons from Calcium Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, new technologies in the field of neuroscience have\nmade it possible to simultaneously image activity in large populations of\nneurons at cellular resolution in behaving animals. In mid-2016, a huge\nrepository of this so-called \"calcium imaging\" data was made\npublicly-available. The availability of this large-scale data resource opens\nthe door to a host of scientific questions, for which new statistical methods\nmust be developed.\n  In this paper, we consider the first step in the analysis of calcium imaging\ndata: namely, identifying the neurons in a calcium imaging video. We propose a\ndictionary learning approach for this task. First, we perform image\nsegmentation to develop a dictionary containing a huge number of candidate\nneurons. Next, we refine the dictionary using clustering. Finally, we apply the\ndictionary in order to select neurons and estimate their corresponding activity\nover time, using a sparse group lasso optimization problem. We apply our\nproposal to three calcium imaging data sets.\n  Our proposed approach is implemented in the R package scalpel, which is\navailable on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 19:45:39 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Petersen", "Ashley", ""], ["Simon", "Noah", ""], ["Witten", "Daniela", ""]]}, {"id": "1703.06957", "submitter": "Tobias M\\\"utze", "authors": "Tobias M\\\"utze, Heinz Schmidli, Tim Friede", "title": "Sample size re-estimation incorporating prior information on a nuisance\n  parameter", "comments": null, "journal-ref": "Pharmaceutical Statistics (2018) 17.2:126-143", "doi": "10.1002/pst.1837", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior information is often incorporated informally when planning a clinical\ntrial. Here, we present an approach on how to incorporate prior information,\nsuch as data from historical clinical trials, into the nuisance parameter based\nsample size re-estimation in a design with an internal pilot study. We focus on\ntrials with continuous endpoints in which the outcome variance is the nuisance\nparameter. For planning and analyzing the trial frequentist methods are\nconsidered. Moreover, the external information on the variance is summarized by\nthe Bayesian meta-analytic-predictive (MAP) approach. To incorporate external\ninformation into the sample size re-estimation, we propose to update the MAP\nprior based on the results of the internal pilot study and to re-estimate the\nsample size using an estimator from the posterior. By means of a simulation\nstudy, we compare the operating characteristics such as power and sample size\ndistribution of the proposed procedure with the traditional sample size\nre-estimation approach which uses the pooled variance estimator. The simulation\nstudy shows that, if no prior-data conflict is present, incorporating external\ninformation into the sample size re-estimation improves the operating\ncharacteristics compared to the traditional approach. In the case of a\nprior-data conflict, that is when the variance of the ongoing clinical trial is\nunequal to the prior location, the performance of the traditional sample size\nre-estimation procedure is in general superior, even when the prior information\nis robustified. When considering to include prior information in sample size\nre-estimation, the potential gains should be balanced against the risks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 20:26:10 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 19:36:25 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["M\u00fctze", "Tobias", ""], ["Schmidli", "Heinz", ""], ["Friede", "Tim", ""]]}, {"id": "1703.07030", "submitter": "Bradley Sliz", "authors": "Bradley A. Sliz", "title": "An Investigation of Three-point Shooting through an Analysis of NBA\n  Player Tracking Data", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I address the difficult challenge of measuring the relative influence of\ncompeting basketball game strategies, and I apply my analysis to plays\nresulting in three-point shots. I use a glut of SportVU player tracking data\nfrom over 600 NBA games to derive custom position-based features that capture\ntangible game strategies from game-play data, such as teamwork, player\nmatchups, and on-ball defender distances. Then, I demonstrate statistical\nmethods for measuring the relative importance of any given basketball strategy.\nIn doing so, I highlight the high importance of teamwork based strategies in\naffecting three-point shot success. By coupling SportVU data with an advanced\nvariable importance algorithm I am able to extract meaningful results that\nwould have been impossible to achieve even 3 years ago. Further, I demonstrate\nhow player-tracking based features can be used to measure the three- point\nshooting propensity of players, and I show how this measurement can identify\neffective shooters that are either highly-utilized or under-utilized.\nAltogether, my findings provide a substantial body of work for influencing\nbasketball strategy, and for measuring the effectiveness of basketball players.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 02:45:23 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Sliz", "Bradley A.", ""]]}, {"id": "1703.07137", "submitter": "Stefan Bauer", "authors": "Gabriele Abbati and Stefan Bauer and Peter J. Sch\\\"uffler and Jakob\n  Burgstaller and Ulrike Held and Sebastian Winklhofer and Johann Steurer and\n  Joachim M. Buhmann", "title": "MRI-based Surgical Planning for Lumbar Spinal Stenosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common reason for spinal surgery in elderly patients is lumbar\nspinal stenosis(LSS). For LSS, treatment decisions based on clinical and\nradiological information as well as personal experience of the surgeon shows\nlarge variance. Thus a standardized support system is of high value for a more\nobjective and reproducible decision. In this work, we develop an automated\nalgorithm to localize the stenosis causing the symptoms of the patient in\nmagnetic resonance imaging (MRI). With 22 MRI features of each of five spinal\nlevels of 321 patients, we show it is possible to predict the location of\nlesion triggering the symptoms. To support this hypothesis, we conduct an\nautomated analysis of labeled and unlabeled MRI scans extracted from 788\npatients. We confirm quantitatively the importance of radiological information\nand provide an algorithmic pipeline for working with raw MRI scans.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 10:45:07 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Abbati", "Gabriele", ""], ["Bauer", "Stefan", ""], ["Sch\u00fcffler", "Peter J.", ""], ["Burgstaller", "Jakob", ""], ["Held", "Ulrike", ""], ["Winklhofer", "Sebastian", ""], ["Steurer", "Johann", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1703.07256", "submitter": "Irina Makarenko Dr", "authors": "Robin Henderson, Irina Makarenko, Paul Bushby, Andrew Fletcher, Anvar\n  Shukurov", "title": "Statistical Topology and the Random Interstellar Medium", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current astrophysical models of the interstellar medium assume that small\nscale variation and noise can be modelled as Gaussian random fields or simple\ntransformations thereof, such as lognormal. We use topological methods to\ninvestigate this assumption for three regions of the southern sky. We consider\nGaussian random fields on two-dimensional lattices and investigate the expected\ndistribution of topological structures quantified through Betti numbers. We\ndemonstrate that there are circumstances where differences in topology can\nidentify differences in distributions when conventional marginal or correlation\nanalyses may not. We propose a non-parametric method for comparing two fields\nbased on the counts of topological features and the geometry of the associated\npersistence diagrams. When we apply the methods to the astrophysical data, we\nfind strong evidence against a Gaussian random field model for each of the\nthree regions of the interstellar medium that we consider. Further, we show\nthat there are topological differences at a local scale between these different\nregions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:02:45 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Henderson", "Robin", ""], ["Makarenko", "Irina", ""], ["Bushby", "Paul", ""], ["Fletcher", "Andrew", ""], ["Shukurov", "Anvar", ""]]}, {"id": "1703.07309", "submitter": "Arnold Kalmbach", "authors": "Arnold Kalmbach, Yogesh Girdhar, Heidi M. Sosik, Gregory Dudek", "title": "Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community\n  Model", "comments": "To appear in ICRA 2017, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting natural phenomena are sparsely distributed and discrete.\nLocating the hotspots of such sparsely distributed phenomena is often difficult\nbecause their density gradient is likely to be very noisy. We present a novel\napproach to this search problem, where we model the co-occurrence relations\nbetween a robot's observations with a Bayesian nonparametric topic model. This\napproach makes it possible to produce a robust estimate of the spatial\ndistribution of the target, even in the absence of direct target observations.\nWe apply the proposed approach to the problem of finding the spatial locations\nof the hotspots of a specific phytoplankton taxon in the ocean. We use\nclassified image data from Imaging FlowCytobot (IFCB), which automatically\nmeasures individual microscopic cells and colonies of cells. Given these\nindividual taxon-specific observations, we learn a phytoplankton community\nmodel that characterizes the co-occurrence relations between taxa. We present\nexperiments with simulated robot missions drawn from real observation data\ncollected during a research cruise traversing the US Atlantic coast. Our\nresults show that the proposed approach outperforms nearest neighbor and\nk-means based methods for predicting the spatial distribution of hotspots from\nin-situ observations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:48:50 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Kalmbach", "Arnold", ""], ["Girdhar", "Yogesh", ""], ["Sosik", "Heidi M.", ""], ["Dudek", "Gregory", ""]]}, {"id": "1703.07355", "submitter": "Srijan Kumar", "authors": "Srijan Kumar, Justin Cheng, Jure Leskovec, V.S. Subrahmanian", "title": "An Army of Me: Sockpuppets in Online Discussion Communities", "comments": "26th International World Wide Web conference 2017 (WWW 2017)", "journal-ref": null, "doi": "10.1145/3038912.3052677", "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In online discussion communities, users can interact and share information\nand opinions on a wide variety of topics. However, some users may create\nmultiple identities, or sockpuppets, and engage in undesired behavior by\ndeceiving others or manipulating discussions. In this work, we study\nsockpuppetry across nine discussion communities, and show that sockpuppets\ndiffer from ordinary users in terms of their posting behavior, linguistic\ntraits, as well as social network structure. Sockpuppets tend to start fewer\ndiscussions, write shorter posts, use more personal pronouns such as \"I\", and\nhave more clustered ego-networks. Further, pairs of sockpuppets controlled by\nthe same individual are more likely to interact on the same discussion at the\nsame time than pairs of ordinary users. Our analysis suggests a taxonomy of\ndeceptive behavior in discussion communities. Pairs of sockpuppets can vary in\ntheir deceptiveness, i.e., whether they pretend to be different users, or their\nsupportiveness, i.e., if they support arguments of other sockpuppets controlled\nby the same user. We apply these findings to a series of prediction tasks,\nnotably, to identify whether a pair of accounts belongs to the same underlying\nuser or not. Altogether, this work presents a data-driven view of deception in\nonline discussion communities and paves the way towards the automatic detection\nof sockpuppets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:00:02 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Kumar", "Srijan", ""], ["Cheng", "Justin", ""], ["Leskovec", "Jure", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1703.07408", "submitter": "Irina Kovalenko", "authors": "Irina D. Kovalenko, Radu S. Stoica and Nikolay V. Emelyanov", "title": "Maximum a posteriori estimation through simulated annealing for binary\n  asteroid orbit determination", "comments": "Accepted for publication in Monthly Notices of the Royal Astronomical\n  Society", "journal-ref": "Monthly Notices of the Royal Astronomical Society 471(4) (2017)\n  4637-4647", "doi": "10.1093/mnras/stx1899", "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a new method for the binary asteroid orbit determination\nproblem. The method is based on the Bayesian approach with a global\noptimisation algorithm. The orbital parameters to be determined are modelled\nthrough an a posteriori distribution made of a priori and likelihood terms. The\nfirst term constrains the parameters space and it allows the introduction of\navailable knowledge about the orbit. The second term is based on given\nobservations and it allows us to use and compare different observational error\nmodels. Once the a posteriori model is built, the estimator of the orbital\nparameters is computed using a global optimisation procedure: the simulated\nannealing algorithm. The maximum a posteriori (MAP) techniques are verified\nusing simulated and real data. The obtained results validate the proposed\nmethod. The new approach guarantees independence of the initial parameters\nestimation and theoretical convergence towards the global optimisation\nsolution. It is particularly useful in these situations, whenever a good\ninitial orbit estimation is difficult to get, whenever observations are not\nwell-sampled, and whenever the statistical behaviour of the observational\nerrors cannot be stated Gaussian like.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 19:53:59 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 12:10:08 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 13:38:23 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 16:56:38 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Kovalenko", "Irina D.", ""], ["Stoica", "Radu S.", ""], ["Emelyanov", "Nikolay V.", ""]]}, {"id": "1703.08071", "submitter": "Manuel Sebastian Mariani", "authors": "Giacomo Vaccario, Matus Medo, Nicolas Wider, Manuel Sebastian Mariani", "title": "Quantifying and suppressing ranking bias in a large citation network", "comments": "Main text (pp. 1-12) and Appendices (pp. 13-17)", "journal-ref": "Journal of Informetrics 11, 766-782 (2017)", "doi": "10.1016/j.joi.2017.05.014", "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized that citation counts for papers from different fields\ncannot be directly compared because different scientific fields adopt different\ncitation practices. Citation counts are also strongly biased by paper age since\nolder papers had more time to attract citations. Various procedures aim at\nsuppressing these biases and give rise to new normalized indicators, such as\nthe relative citation count. We use a large citation dataset from Microsoft\nAcademic Graph and a new statistical framework based on the Mahalanobis\ndistance to show that the rankings by well known indicators, including the\nrelative citation count and Google's PageRank score, are significantly biased\nby paper field and age. We propose a general normalization procedure motivated\nby the $z$-score which produces much less biased rankings when applied to\ncitation count and PageRank score.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 13:53:06 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Vaccario", "Giacomo", ""], ["Medo", "Matus", ""], ["Wider", "Nicolas", ""], ["Mariani", "Manuel Sebastian", ""]]}, {"id": "1703.08111", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, Ashley Wazana, Eszter Szekely, Meir\n  Steiner, Alison S. Fleming, James L. Kennedy, Michael J. Meaney, Celia M.T.\n  Greenwood (and the MAVAN team)", "title": "Alternating optimization for GxE modelling with weighted genetic and\n  environmental scores: examples from the MAVAN study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the goal of expanding currently existing genotype x environment\ninteraction (GxE) models to simultaneously include multiple genetic variants\nand environmental exposures in a parsimonious way, we developed a novel method\nto estimate the parameters in a GxE model, where G is a weighted sum of genetic\nvariants (genetic score) and E is a weighted sum of environments (environmental\nscore). The approach uses alternating optimization to estimate the parameters\nof the GxE model. This is an iterative process where the genetic score weights,\nthe environmental score weights, and the main model parameters are estimated in\nturn assuming the other parameters to be constant. This technique can be used\nto construct relatively complex interaction models that are constrained to a\nparticular structure, and hence contain fewer parameters.\n  We present the model as a two-way interaction longitudinal mixed model, for\nwhich ordinary linear regression is a special case, but it can easily be\nextended to be compatible with k-way interaction models and generalized linear\nmixed models. The model is implemented in R (LEGIT package) and using SAS\nmacros (LEGIT_SAS). Here we present examples from the Maternal Adversity,\nVulnerability, and Neurodevelopment (MAVAN) study where we improve\nsignificantly upon already existing models using alternating optimization.\nFurthermore, through simulations, we demonstrate the power and validity of this\napproach even with small sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 15:38:17 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 20:12:43 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", "", "and the MAVAN team"], ["Wazana", "Ashley", "", "and the MAVAN team"], ["Szekely", "Eszter", "", "and the MAVAN team"], ["Steiner", "Meir", "", "and the MAVAN team"], ["Fleming", "Alison S.", "", "and the MAVAN team"], ["Kennedy", "James L.", "", "and the MAVAN team"], ["Meaney", "Michael J.", "", "and the MAVAN team"], ["Greenwood", "Celia M. T.", "", "and the MAVAN team"]]}, {"id": "1703.08254", "submitter": "Le Zheng", "authors": "Le Zheng and Xiaodong Wang", "title": "Improved NN-JPDAF for Joint Multiple Target Tracking and Feature\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature aided tracking can often yield improved tracking performance over the\nstandard multiple target tracking (MTT) algorithms with only kinematic\nmeasurements. However, in many applications, the feature signal of the targets\nconsists of sparse Fourier-domain signals. It changes quickly and nonlinearly\nin the time domain, and the feature measurements are corrupted by missed\ndetections and mis-associations. These two factors make it hard to extract the\nfeature information to be used in MTT. In this paper, we develop a\nfeature-aided nearest neighbour joint probabilistic data association filter\n(NN-JPDAF) for joint MTT and feature extraction in dense target environments.\nTo estimate the rapidly varying feature signal from incomplete and corrupted\nmeasurements, we use the atomic norm constraint to formulate the sparsity of\nfeature signal and use the $\\ell_1$-norm to formulate the sparsity of the\ncorruption induced by mis-associations. Based on the sparse representation, the\nfeature signal are estimated by solving a semidefinite program (SDP) which is\nconvex. We also provide an iterative method for solving this SDP via the\nalternating direction method of multipliers (ADMM) where each iteration\ninvolves closed-form computation. With the estimated feature signal,\nre-filtering is performed to estimate the kinematic states of the targets,\nwhere the association makes use of both kinematic and feature information.\nSimulation results are presented to illustrate the performance of the proposed\nalgorithm in a radar application.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 00:05:08 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Zheng", "Le", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1703.08429", "submitter": "Nicholas Clark", "authors": "Nicholas J. Clark, Philip M. Dixon", "title": "Modeling and Estimation for Self-Exciting Spatio-Temporal Models of\n  Terrorist Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal hierarchical modeling is an extremely attractive way to model\nthe spread of crime or terrorism data over a given region, especially when the\nobservations are counts and must be modeled discretely. The spatio-temporal\ndiffusion is placed, as a matter of convenience, in the process model allowing\nfor straightforward estimation of the diffusion parameters through Bayesian\ntechniques. However, this method of modeling does not allow for the existence\nof self-excitation, or a temporal data model dependency, that has been shown to\nexist in criminal and terrorism data. In this manuscript we will use existing\ntheories on how violence spreads to create models that allow for both\nspatio-temporal diffusion in the process model as well as temporal diffusion,\nor self-excitation, in the data model. We will further demonstrate how Laplace\napproximations similar to their use in Integrated Nested Laplace Approximation\ncan be used to quickly and accurately conduct inference of self-exciting\nspatio-temporal models allowing practitioners a new way of fitting and\ncomparing multiple process models. We will illustrate this approach by fitting\na self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate\nhow choice of process model leads to differing conclusions on the existence of\nself-excitation in the data and differing conclusions on how violence is\nspreading spatio-temporally.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 14:40:45 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 20:30:37 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Clark", "Nicholas J.", ""], ["Dixon", "Philip M.", ""]]}, {"id": "1703.08487", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Giandomenico Nollo, Sebastiano Stramaglia, Daniele\n  Marinazzo", "title": "Multiscale Granger causality", "comments": null, "journal-ref": "Phys. Rev. E 96, 042150 (2017)", "doi": "10.1103/PhysRevE.96.042150", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of complex physical and biological systems represented by\nmultivariate stochastic processes, an issue of great relevance is the\ndescription of the system dynamics spanning multiple temporal scales. While\nmethods to assess the dynamic complexity of individual processes at different\ntime scales are well-established, multiscale analysis of directed interactions\nhas never been formalized theoretically, and empirical evaluations are\ncomplicated by practical issues such as filtering and downsampling. Here we\nextend the very popular measure of Granger causality (GC), a prominent tool for\nassessing directed lagged interactions between joint processes, to quantify\ninformation transfer across multiple time scales. We show that the multiscale\nprocessing of a vector autoregressive (AR) process introduces a moving average\n(MA) component, and describe how to represent the resulting ARMA process using\nstate space (SS) models and to combine the SS model parameters for computing\nexact GC values at arbitrarily large time scales. We exploit the theoretical\nformulation to identify peculiar features of multiscale GC in basic AR\nprocesses, and demonstrate with numerical simulations the much larger\nestimation accuracy of the SS approach compared with pure AR modeling of\nfiltered and downsampled data. The improved computational reliability is\nexploited to disclose meaningful multiscale patterns of information transfer\nbetween global temperature and carbon dioxide concentration time series, both\nin paleoclimate and in recent years.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:08:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:02:12 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Faes", "Luca", ""], ["Nollo", "Giandomenico", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1703.08620", "submitter": "Maryclare Griffin", "authors": "Maryclare Griffin and Peter D. Hoff", "title": "Lasso ANOVA Decompositions for Matrix and Tensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating the entries of an unknown mean matrix or\ntensor given a single noisy realization. In the matrix case, this problem can\nbe addressed by decomposing the mean matrix into a component that is additive\nin the rows and columns, i.e.\\ the additive ANOVA decomposition of the mean\nmatrix, plus a matrix of elementwise effects, and assuming that the elementwise\neffects may be sparse. Accordingly, the mean matrix can be estimated by solving\na penalized regression problem, applying a lasso penalty to the elementwise\neffects. Although solving this penalized regression problem is straightforward,\nspecifying appropriate values of the penalty parameters is not. Leveraging the\nposterior mode interpretation of the penalized regression problem, moment-based\nempirical Bayes estimators of the penalty parameters can be defined. Estimation\nof the mean matrix using these these moment-based empirical Bayes estimators\ncan be called LANOVA penalization, and the corresponding estimate of the mean\nmatrix can be called the LANOVA estimate. The empirical Bayes estimators are\nshown to be consistent. Additionally, LANOVA penalization is extended to\naccommodate sparsity of row and column effects and to estimate an unknown mean\ntensor. The behavior of the LANOVA estimate is examined under misspecification\nof the distribution of the elementwise effects, and LANOVA penalization is\napplied to several datasets, including a matrix of microarray data, a three-way\ntensor of fMRI data and a three-way tensor of wheat infection data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 23:04:17 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 02:55:49 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Griffin", "Maryclare", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1703.08644", "submitter": "Sean Jewell", "authors": "Sean Jewell and Daniela Witten", "title": "Exact Spike Train Inference Via $\\ell_0$ Optimization", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, new technologies in neuroscience have made it possible to\nmeasure the activities of large numbers of neurons simultaneously in behaving\nanimals. For each neuron, a fluorescence trace is measured; this can be seen as\na first-order approximation of the neuron's activity over time. Determining the\nexact time at which a neuron spikes on the basis of its fluorescence trace is\nan important open problem in the field of computational neuroscience.\n  Recently, a convex optimization problem involving an $\\ell_1$ penalty was\nproposed for this task. In this paper, we slightly modify that recent proposal\nby replacing the $\\ell_1$ penalty with an $\\ell_0$ penalty. In stark contrast\nto the conventional wisdom that $\\ell_0$ optimization problems are\ncomputationally intractable, we show that the resulting optimization problem\ncan be efficiently solved for the global optimum using an extremely simple and\nefficient dynamic programming algorithm. Our R-language implementation of the\nproposed algorithm runs in a few minutes on fluorescence traces of $100,000$\ntimesteps. Furthermore, our proposal leads to substantial improvements over the\nprevious $\\ell_1$ proposal, in simulations as well as on two calcium imaging\ndata sets.\n  R-language software for our proposal is available on CRAN in the package\nLZeroSpikeInference. Instructions for running this software in python can be\nfound at https://github.com/jewellsean/LZeroSpikeInference.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 03:44:05 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 22:35:01 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 17:46:48 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Jewell", "Sean", ""], ["Witten", "Daniela", ""]]}, {"id": "1703.08723", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang, Emilie Shireman, Paul D. McNicholas and Douglas\n  L. Steinley", "title": "Extending Growth Mixture Models Using Continuous Non-Elliptical\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth mixture models (GMMs) incorporate both conventional random effects\ngrowth modeling and latent trajectory classes as in finite mixture modeling;\ntherefore, they offer a way to handle the unobserved heterogeneity between\nsubjects in their development. GMMs with Gaussian random effects dominate the\nliterature. When the data are asymmetric and/or have heavier tails, more than\none latent class is required to capture the observed variable distribution.\nTherefore, a GMM with continuous non-elliptical distributions is proposed to\ncapture skewness and heavier tails in the data set. Specifically, multivariate\nskew-t distributions and generalized hyperbolic distributions are introduced to\nextend GMMs. When extending GMMs, four statistical models are considered with\ndiffering distributions of measurement errors and random effects. The\nmathematical development of GMMs with non-elliptical distributions relies on\ntheir expression as normal variance-mean mixtures and the resultant\nrelationship with the generalized inverse Gaussian distribution. Parameter\nestimation is outlined within the expectation-maximization framework before the\nperformance of our GMMs with non-elliptical distributions is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 17:57:31 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 00:14:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["Shireman", "Emilie", ""], ["McNicholas", "Paul D.", ""], ["Steinley", "Douglas L.", ""]]}, {"id": "1703.08920", "submitter": "Lechuan Hu", "authors": "Lechuan Hu, Norbert Fortin and Hernando Ombao", "title": "Modeling high dimensional multichannel brain signals", "comments": "45 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to model and measure functional and effective (directional)\nconnectivity in multichannel brain physiological signals (e.g.,\nelectroencephalograms, local field potentials). The difficulties from analyzing\nthese data mainly come from two aspects: first, there are major statistical and\ncomputational challenges for modeling and analyzing high dimensional\nmultichannel brain signals; second, there is no set of universally-agreed\nmeasures for characterizing connectivity. To model multichannel brain signals,\nour approach is to fit a vector autoregressive (VAR) model with potentially\nhigh lag order so that complex lead-lag temporal dynamics between the channels\ncan be captured. Estimates of the VAR model will be obtained by our proposed\nhybrid LASSLE (LASSO+LSE) method which combines regularization (to control for\nsparsity) and least squares estimation (to improve bias and mean-squared\nerror). Then we employ some measures of connectivity but put an emphasis on\npartial directed coherence (PDC) which can capture the directional connectivity\nbetween channels. PDC is a frequency-specific measure that explains the extent\nto which the present oscillatory activity in a sender channel influences the\nfuture oscillatory activity in a specific receiver channel relative to all\npossible receivers in the network. The proposed modeling approach provided key\ninsights into potential functional relationships among simultaneously recorded\nsites during performance of a complex memory task. Specifically, this novel\nmethod was successful in quantifying patterns of effective connectivity across\nelectrode locations, and in capturing how these patterns varied across trial\nepochs and trial types.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 03:54:15 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 22:12:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hu", "Lechuan", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "1703.08954", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Exact and approximate limit behaviour of the Yule tree's cophenetic\n  index", "comments": null, "journal-ref": "Mathematical Biosciences 303:26-45, 2018", "doi": "10.1016/j.mbs.2018.05.005", "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the limit distribution of an appropriately normalized\ncophenetic index of the pure-birth tree conditioned on $n$ contemporary tips.\nWe show that this normalized phylogenetic balance index is a submartingale that\nconverges almost surely and in L2. We link our work with studies on trees\nwithout branch lengths and show that in this case the limit distribution is a\ncontraction-type distribution, similar to the Quicksort limit distribution. In\nthe continuous branch case we suggest approximations to the limit distribution.\nWe propose heuristic methods of simulating from these distributions and it may\nbe observed that these algorithms result in reasonable tails. Therefore, we\npropose a way based on the quantiles of the derived distributions for\nhypothesis testing, whether an observed phylogenetic tree is consistent with\nthe pure-birth process. Simulating a sample by the proposed heuristics is\nrapid, while exact simulation (simulating the tree and then calculating the\nindex) is a time-consuming procedure. We conduct a power study to investigate\nhow well the cophenetic indices detect deviations from the Yule tree and apply\nthe methodology to empirical phylogenies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 07:11:05 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 11:16:17 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 17:58:08 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1703.08994", "submitter": "Christopher Jackson", "authors": "Christopher Jackson, Anne Presanis, Stefano Conti, Daniela De Angelis", "title": "Value of Information: Sensitivity Analysis and Research Design in\n  Bayesian Evidence Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have a Bayesian model which combines evidence from several\ndifferent sources. We want to know which model parameters most affect the\nestimate or decision from the model, or which of the parameter uncertainties\ndrive the decision uncertainty. Furthermore we want to prioritise what further\ndata should be collected. These questions can be addressed by Value of\nInformation (VoI) analysis, in which we estimate expected reductions in loss\nfrom learning specific parameters or collecting data of a given design. We\ndescribe the theory and practice of VoI for Bayesian evidence synthesis, using\nand extending ideas from health economics, computer modelling and Bayesian\ndesign. The methods are general to a range of decision problems including point\nestimation and choices between discrete actions. We apply them to a model for\nestimating prevalence of HIV infection, combining indirect information from\nseveral surveys, registers and expert beliefs. This analysis shows which\nparameters contribute most of the uncertainty about each prevalence estimate,\nand provides the expected improvements in precision from collecting specific\namounts of additional data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 10:14:54 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Jackson", "Christopher", ""], ["Presanis", "Anne", ""], ["Conti", "Stefano", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1703.09007", "submitter": "Adway Mitra", "authors": "Adway Mitra, Ashwin K. Seshadri", "title": "Detection of Spatiotemporally Coherent Rainfall Anomalies Using Markov\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precipitation is a large-scale, spatio-temporally heterogeneous phenomenon,\nwith frequent anomalies exhibiting unusually high or low values. We use Markov\nRandom Fields (MRFs) to detect spatio-temporally coherent anomalies in gridded\nannual rainfall data across India from 1901-2005. MRFs are undirected graphical\nmodels where each node is associated with a \\{location,year\\} pair, with edges\nconnecting nodes representing adjacent locations or years. Some nodes represent\nobservations of precipitation, while the rest represent unobserved\n(\\emph{latent}) states that can take one of three values: high/low/normal. The\nMRF represents a probability distribution over the variables, using \\emph{node\npotential} and \\emph{edge potential} functions defined on nodes and edges of\nthe graph. Optimal values of latent state variables are estimated by maximizing\nthe posterior probability of the observations, using Gibbs sampling. Edge\npotentials enforce spatial and temporal coherence, and node potentials\ninfluence threshold for anomalies by affecting the prior probabilities of the\nstates. The model can be tuned to recover anomalies detected by threshold-based\nmethods. The competing influences of spatial and temporal coherence can be\nadjusted through edge potentials.\n  We study spatio-temporal properties of rainfall anomalies discovered by this\nmethod, using suitable measures. We identify nonstationarities in occurrence of\npositive and negative anomalies between the first and second halves of the 20th\ncentury. We find that between these periods, there has been decrease in\nrainfall during June-September (JJAS) and an increase during other months.\nThese effects are highlighted prominently in the statistics of anomalies.\nProperties of anomalies learnt from this approach could present tests of\nregional-scale rainfall simulations by climate models and statistical\nsimulators.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 11:04:00 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 09:03:20 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mitra", "Adway", ""], ["Seshadri", "Ashwin K.", ""]]}, {"id": "1703.09147", "submitter": "Sean Yiu", "authors": "Sean Yiu and Brian Tom", "title": "Two-part models with stochastic processes for modelling longitudinal\n  semicontinuous data: computationally efficient inference and modelling the\n  overall marginal mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have described two-part models with patient-specific\nstochastic processes for analysing longitudinal semicontinuous data. In theory,\nsuch models can offer greater flexibility than the standard two-part model with\npatient-specific random effects. However, in practice the high dimensional\nintegrations involved in the marginal likelihood (i.e. integrated over the\nstochastic processes) significantly complicates model fitting. Thus\nnon-standard computationally intensive procedures based on simulating the\nmarginal likelihood have so far only been proposed. In this paper, we describe\nan efficient method of implementation by demonstrating how the high dimensional\nintegrations involved in the marginal likelihood can be computed efficiently.\nSpecifically, by using a property of the multivariate normal distribution and\nthe standard marginal cumulative distribution function identity, we transform\nthe marginal likelihood so that the high dimensional integrations are contained\nin the cumulative distribution function of a multivariate normal distribution,\nwhich can then be efficiently evaluated. Hence maximum likelihood estimation\ncan be used to obtain parameter estimates and asymptotic standard errors (from\nthe observed information matrix) of model parameters. We describe our proposed\nefficient implementation procedure for the standard two-part model\nparameterisation and when it is of interest to directly model the overall\nmarginal mean. The methodology is applied on a psoriatic arthritis data set\nconcerning functional disability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 15:36:40 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Yiu", "Sean", ""], ["Tom", "Brian", ""]]}, {"id": "1703.09430", "submitter": "Joshua Bon", "authors": "Joshua J Bon, Timothy Ballard, Bernard Baffour", "title": "Polling bias and undecided voter allocations: US Presidential elections,\n  2004 - 2016", "comments": "32 pages, 9 figures, 6 tables", "journal-ref": "J. R. Stat. Soc. A, 182: 467-493 (2019)", "doi": "10.1111/rssa.12414", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accounting for undecided and uncertain voters is a challenging issue for\npredicting election results from public opinion polls. Undecided voters typify\nthe uncertainty of swing voters in polls but are often ignored or allocated to\neach candidate in a simple, deterministic manner. Historically this may have\nbeen adequate because the undecided were comparatively small enough to assume\nthat they do not affect the relative proportions of the decided voters.\nHowever, in the presence of high numbers of undecided voters, these static\nrules may in fact bias election predictions from election poll authors and\nmeta-poll analysts. In this paper, we examine the effect of undecided voters in\nthe 2016 US presidential election to the previous three presidential elections.\nWe show there were a relatively high number of undecided voters over the\ncampaign and on election day, and that the allocation of undecided voters in\nthis election was not consistent with two-party proportional (or even)\nallocations. We find evidence that static allocation regimes are inadequate for\nelection prediction models and that probabilistic allocations may be superior.\nWe also estimate the bias attributable to polling agencies, often referred to\nas \"house effects\".\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 07:30:31 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 08:55:03 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 01:14:43 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 00:32:27 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Bon", "Joshua J", ""], ["Ballard", "Timothy", ""], ["Baffour", "Bernard", ""]]}, {"id": "1703.09472", "submitter": "Stefanie L\\\"osch", "authors": "Dilya Khakimova, Stefanie L\\\"osch, Danny Wende, Hans Wiesmeth, Ostap\n  Okhrin", "title": "Index of Environmental Awareness in Russia - MIMIC Approaches for\n  Different Economic Situations", "comments": "We are very grateful to Yandex. Research was supported by RSF grant\n  Nr. 15-18-20029 'Projection of optimal socio-economic systems in turbulence\n  of external and internal environment'. D. Khakimova wishes to acknowledge the\n  support of the Ministry of Education and Science of the Russian Federation,\n  grant No. 14.U04.31.0002, administered through the NES CSDSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the issue of environmental awareness in the regions of\nthe Russian Federation. Russia provides an important study area to investigate\nthe relationship between economic development and environmental consciousness\nin general. This paper introduces an index of environmental awareness, which is\nderived as a latent variable from various categories of search entries in\nYandex, the prominent Russian search engine, during two periods in years 2014\nand 2015. These indicators are presumably dependent on certain causes, which\nare also integrated into the model. The resulting Multiple Indicators-Multiple\nCauses model allows to estimate the proposed index of environmental awareness\nand to rank the Russian regions for each period. Comparing the results of 2014\nand 2015 is especially interesting, because of the RUR devaluation at the end\nof 2014. In additional, we answer the question of the existence of an\nEnvironmental Kuznets Curve with respect to environmental understanding.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:19:12 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 08:18:38 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Khakimova", "Dilya", ""], ["L\u00f6sch", "Stefanie", ""], ["Wende", "Danny", ""], ["Wiesmeth", "Hans", ""], ["Okhrin", "Ostap", ""]]}, {"id": "1703.09701", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Mike Hobson and Anthony Lasenby", "title": "Sampling Errors in Nested Sampling Parameter Estimation", "comments": "Very minor changes. 22 pages + appendix, 10 figures. Accepted by\n  Bayesian Analysis", "journal-ref": "Bayesian Analysis 13(3):873-896 (2018)", "doi": "10.1214/17-BA1075", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling errors in nested sampling parameter estimation differ from those in\nBayesian evidence calculation, but have been little studied in the literature.\nThis paper provides the first explanation of the two main sources of sampling\nerrors in nested sampling parameter estimation, and presents a new diagrammatic\nrepresentation for the process. We find no current method can accurately\nmeasure the parameter estimation errors of a single nested sampling run, and\npropose a method for doing so using a new algorithm for dividing nested\nsampling runs. We empirically verify our conclusions and the accuracy of our\nnew method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:00:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:37:45 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1703.09710", "submitter": "Daniel Foreman-Mackey", "authors": "Daniel Foreman-Mackey, Eric Agol, Sivaram Ambikasaran, Ruth Angus", "title": "Fast and scalable Gaussian process modeling with applications to\n  astronomical time series", "comments": "Updated in response to referee. Submitted to the AAS Journals.\n  Comments (still) welcome. Code available: https://github.com/dfm/celerite", "journal-ref": null, "doi": "10.3847/1538-3881/aa9332", "report-no": null, "categories": "astro-ph.IM astro-ph.EP astro-ph.SR physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growing field of large-scale time domain astronomy requires methods for\nprobabilistic data analysis that are computationally tractable, even with large\ndatasets. Gaussian Processes are a popular class of models used for this\npurpose but, since the computational cost scales, in general, as the cube of\nthe number of data points, their application has been limited to small\ndatasets. In this paper, we present a novel method for Gaussian Process\nmodeling in one-dimension where the computational requirements scale linearly\nwith the size of the dataset. We demonstrate the method by applying it to\nsimulated and real astronomical time series datasets. These demonstrations are\nexamples of probabilistic inference of stellar rotation periods, asteroseismic\noscillation spectra, and transiting planet parameters. The method exploits\nstructure in the problem when the covariance function is expressed as a mixture\nof complex exponentials, without requiring evenly spaced observations or\nuniform noise. This form of covariance arises naturally when the process is a\nmixture of stochastically-driven damped harmonic oscillators -- providing a\nphysical motivation for and interpretation of this choice -- but we also\ndemonstrate that it can be a useful effective model in some other cases. We\npresent a mathematical description of the method and compare it to existing\nscalable Gaussian Process methods. The method is fast and interpretable, with a\nrange of potential applications within astronomical data analysis and beyond.\nWe provide well-tested and documented open-source implementations of this\nmethod in C++, Python, and Julia.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 16:09:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Foreman-Mackey", "Daniel", ""], ["Agol", "Eric", ""], ["Ambikasaran", "Sivaram", ""], ["Angus", "Ruth", ""]]}, {"id": "1703.09795", "submitter": "Anders Eklund", "authors": "Anders Eklund, Thomas Nichols, Hans Knutsson", "title": "Reply to Cox et al. and Kessler et al.: Data and code sharing is the way\n  forward for fMRI", "comments": null, "journal-ref": "PNAS, vol. 114 no. 17, E3374 - E3375 (2017)", "doi": "10.1073/pnas.1620285114", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are glad that our paper has generated intense discussions in the fMRI\nfield, on how to analyze fMRI data and how to correct for multiple comparisons.\nThe goal of the paper was not to disparage any specific fMRI software, but to\npoint out that parametric statistical methods are based on a number of\nassumptions that are not always valid for fMRI data, and that non-parametric\nstatistical methods are a good alternative. Through AFNIs introduction of\nnon-parametric statistics in the function 3dttest++, the three most common fMRI\nsoftwares now all support non-parametric group inference (SPM through the\ntoolbox SnPM, and FSL through the function randomise).\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 20:45:29 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Eklund", "Anders", ""], ["Nichols", "Thomas", ""], ["Knutsson", "Hans", ""]]}, {"id": "1703.09851", "submitter": "Mohamed Abuella", "authors": "Mohamed Abuella and Badrul Chowdhury", "title": "Solar Power Forecasting Using Support Vector Regression", "comments": "This works has been presented in the American Society for Engineering\n  Management, International Annual Conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generation and load balance is required in the economic scheduling of\ngenerating units in the smart grid. Variable energy generations, particularly\nfrom wind and solar energy resources, are witnessing a rapid boost, and, it is\nanticipated that with a certain level of their penetration, they can become\nnoteworthy sources of uncertainty. As in the case of load demand, energy\nforecasting can also be used to mitigate some of the challenges that arise from\nthe uncertainty in the resource. While wind energy forecasting research is\nconsidered mature, solar energy forecasting is witnessing a steadily growing\nattention from the research community. This paper presents a support vector\nregression model to produce solar power forecasts on a rolling basis for 24\nhours ahead over an entire year, to mimic the practical business of energy\nforecasting. Twelve weather variables are considered from a high-quality\nbenchmark dataset and new variables are extracted. The added value of the heat\nindex and wind speed as additional variables to the model is studied across\ndifferent seasons. The support vector regression model performance is compared\nwith artificial neural networks and multiple linear regression models for\nenergy forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 00:58:01 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Abuella", "Mohamed", ""], ["Chowdhury", "Badrul", ""]]}, {"id": "1703.10002", "submitter": "Mark Risser", "authors": "Mark D. Risser, Christopher J. Paciorek, and Daithi Stone", "title": "Spatially-Dependent Multiple Testing Under Model Misspecification, with\n  Application to Detection of Anthropogenic Influence on Extreme Climate Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weather Risk Attribution Forecast (WRAF) is a forecasting tool that uses\noutput from global climate models to make simultaneous attribution statements\nabout whether and how greenhouse gas emissions have contributed to extreme\nweather across the globe. However, in conducting a large number of simultaneous\nhypothesis tests, the WRAF is prone to identifying false \"discoveries.\" A\ncommon technique for addressing this multiple testing problem is to adjust the\nprocedure in a way that controls the proportion of true null hypotheses that\nare incorrectly rejected, or the false discovery rate (FDR). Unfortunately,\ngeneric FDR procedures suffer from low power when the hypotheses are dependent,\nand techniques designed to account for dependence are sensitive to\nmisspecification of the underlying statistical model. In this paper, we develop\na Bayesian decision theoretic approach for dependent multiple testing and a\nnonparametric hierarchical statistical model that flexibly controls false\ndiscovery and is robust to model misspecification. We illustrate the robustness\nof our procedure to model error with a simulation study, using a framework that\naccounts for generic spatial dependence and allows the practitioner to flexibly\nspecify the decision criteria. Finally, we apply our procedure to several\nseasonal forecasts and discuss implementation for the WRAF workflow.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 12:40:31 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:42:47 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Risser", "Mark D.", ""], ["Paciorek", "Christopher J.", ""], ["Stone", "Daithi", ""]]}, {"id": "1703.10266", "submitter": "Michael Donohue", "authors": "Dan Li, Samuel Iddi, Wesley K. Thompson, Michael C. Donohue", "title": "Bayesian latent time joint mixed effect models for multicohort\n  longitudinal data", "comments": null, "journal-ref": "Stat.Methods.Med.Res. (2017)", "doi": "10.1177/0962280217737566", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Characterization of long-term disease dynamics, from disease-free to\nend-stage, is integral to understanding the course of neurodegenerative\ndiseases such as Parkinson's and Alzheimer's; and ultimately, how best to\nintervene. Natural history studies typically recruit multiple cohorts at\ndifferent stages of disease and follow them longitudinally for a relatively\nshort period of time. We propose a latent time joint mixed effects model to\ncharacterize long-term disease dynamics using this short-term data. Markov\nchain Monte Carlo methods are proposed for estimation, model selection, and\ninference. We apply the model to detailed simulation studies and data from the\nAlzheimer's Disease Neuroimaging Initiative.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 23:19:34 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 21:55:34 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Li", "Dan", ""], ["Iddi", "Samuel", ""], ["Thompson", "Wesley K.", ""], ["Donohue", "Michael C.", ""]]}, {"id": "1703.10806", "submitter": "Florian Ziel", "authors": "Florian Ziel, Rick Steinert", "title": "Probabilistic Mid- and Long-Term Electricity Price Forecasting", "comments": "accepted for: Renewable & Sustainable Energy Reviews", "journal-ref": "Renewable and Sustainable Energy Reviews, 32.3 (2018) 251-266", "doi": "10.1016/j.rser.2018.05.038", "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The liberalization of electricity markets and the development of renewable\nenergy sources has led to new challenges for decision makers. These challenges\nare accompanied by an increasing uncertainty about future electricity price\nmovements. The increasing amount of papers, which aim to model and predict\nelectricity prices for a short period of time provided new opportunities for\nmarket participants. However, the electricity price literature seem to be very\nscarce on the issue of medium- to long-term price forecasting, which is\nmandatory for investment and political decisions. Our paper closes this gap by\nintroducing a new approach to simulate electricity prices with hourly\nresolution for several months up to three years. Considering the uncertainty of\nfuture events we are able to provide probabilistic forecasts which are able to\ndetect probabilities for price spikes even in the long-run. As market we\ndecided to use the EPEX day-ahead electricity market for Germany and Austria.\nOur model extends the X-Model which mainly utilizes the sale and purchase curve\nfor electricity day-ahead auctions. By applying our procedure we are able to\ngive probabilities for the due to the EEG practical relevant event of six\nconsecutive hours of negative prices. We find that using the supply and demand\ncurve based model in the long-run yields realistic patterns for the time series\nof electricity prices and leads to promising results considering common error\nmeasures.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 09:13:08 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 08:12:59 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ziel", "Florian", ""], ["Steinert", "Rick", ""]]}]