[{"id": "1601.00009", "submitter": "Shuo Chen", "authors": "Shuo Chen, Jian Kang, Yishi Xing, Yunpeng Zhao, and Don Milton", "title": "Network induced large correlation matrix estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correlation matrix of massive biomedical data (e.g. gene expression or\nneuroimaging data) often exhibits a complex and organized, yet latent graph\ntopological structure. We propose a two step procedure that first detects the\nlatent graph topology with parsimony from the sample correlation matrix and\nthen regularizes the correlation matrix by leveraging the detected graph\ntopological information. We show that the graph topological information guided\nthresholding can reduce false positive and false negative rates simultaneously\nbecause it allows edges to borrow strengths from each other precisely. Several\nexamples illustrate that the parsimoniously detected latent graph topological\nstructures may reveal underlying biological networks and guide correlation\nmatrix estimation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 21:06:17 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 18:57:38 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 16:37:00 GMT"}, {"version": "v4", "created": "Fri, 25 Mar 2016 19:12:35 GMT"}, {"version": "v5", "created": "Mon, 28 Mar 2016 14:11:28 GMT"}, {"version": "v6", "created": "Thu, 2 Jun 2016 19:07:24 GMT"}, {"version": "v7", "created": "Mon, 7 Aug 2017 20:57:09 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Chen", "Shuo", ""], ["Kang", "Jian", ""], ["Xing", "Yishi", ""], ["Zhao", "Yunpeng", ""], ["Milton", "Don", ""]]}, {"id": "1601.00129", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Razvan Stefanescu, and Adrian Sandu", "title": "The Reduced-Order Hybrid Monte Carlo Sampling Smoother", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": "10.1002/fld.4255", "report-no": "CSTR-1/2016", "categories": "cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Monte-Carlo (HMC) sampling smoother is a fully non-Gaussian\nfour-dimensional data assimilation algorithm that works by directly sampling\nthe posterior distribution formulated in the Bayesian framework. The smoother\nin its original formulation is computationally expensive due to the intrinsic\nrequirement of running the forward and adjoint models repeatedly. Here we\npresent computationally efficient versions of the HMC sampling smoother based\non reduced-order approximations of the underlying model dynamics. The schemes\ndeveloped herein are tested numerically using the shallow-water equations model\non Cartesian coordinates. The results reveal that the reduced-order versions of\nthe smoother are capable of accurately capturing the posterior probability\ndensity, while being significantly faster than the original full order\nformulation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 01:55:07 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Attia", "Ahmed", ""], ["Stefanescu", "Razvan", ""], ["Sandu", "Adrian", ""]]}, {"id": "1601.00306", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Alfred Hero", "title": "Multimodal Event Detection in Twitter Hashtag Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection in a multimodal Twitter dataset is considered. We treat the\nhashtags in the dataset as instances with two modes: text and geolocation\nfeatures. The text feature consists of a bag-of-words representation. The\ngeolocation feature consists of geotags (i.e., geographical coordinates) of the\ntweets. Fusing the multimodal data we aim to detect, in terms of topic and\ngeolocation, the interesting events and the associated hashtags. To this end, a\ngenerative latent variable model is assumed, and a generalized\nexpectation-maximization (EM) algorithm is derived to learn the model\nparameters. The proposed method is computationally efficient, and lends itself\nto big datasets. Experimental results on a Twitter dataset from August 2014\nshow the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 15:48:36 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 01:59:30 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Hero", "Alfred", ""]]}, {"id": "1601.00496", "submitter": "S{\\o}ren F{\\o}ns Vind Nielsen", "authors": "S{\\o}ren F. V. Nielsen and Kristoffer H. Madsen and Rasmus R{\\o}ge and\n  Mikkel N. Schmidt and Morten M{\\o}rup", "title": "Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data", "comments": "8 pages, 1 figure. Presented at the Machine Learning and\n  Interpretation in Neuroimaging Workshop (MLINI-2015), 2015 (arXiv:1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/08", "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity (FC) has in recent years become a topic of\ninterest in the neuroimaging community. Several models and methods exist for\nboth functional magnetic resonance imaging (fMRI) and electroencephalography\n(EEG), and the results point towards the conclusion that FC exhibits dynamic\nchanges. The existing approaches modeling dynamic connectivity have primarily\nbeen based on time-windowing the data and k-means clustering. We propose a\nnon-parametric generative model for dynamic FC in fMRI that does not rely on\nspecifying window lengths and number of dynamic states. Rooted in Bayesian\nstatistical modeling we use the predictive likelihood to investigate if the\nmodel can discriminate between a motor task and rest both within and across\nsubjects. We further investigate what drives dynamic states using the model on\nthe entire data collated across subjects and task/rest. We find that the number\nof states extracted are driven by subject variability and preprocessing\ndifferences while the individual states are almost purely defined by either\ntask or rest. This questions how we in general interpret dynamic FC and points\nto the need for more research on what drives dynamic FC.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 13:24:45 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 07:42:53 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Nielsen", "S\u00f8ren F. V.", ""], ["Madsen", "Kristoffer H.", ""], ["R\u00f8ge", "Rasmus", ""], ["Schmidt", "Mikkel N.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1601.00900", "submitter": "Lachlan Gunn", "authors": "Lachlan J. Gunn, Fran\\c{c}ois Chapeau-Blondeau, Mark McDonnell, Bruce\n  Davis, Andrew Allison, and Derek Abbott", "title": "Too good to be true: when overwhelming evidence fails to convince", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2015.0748", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible for a large sequence of measurements or observations, which\nsupport a hypothesis, to counterintuitively decrease our confidence? Can\nunanimous support be too good to be true? The assumption of independence is\noften made in good faith, however rarely is consideration given to whether a\nsystemic failure has occurred.\n  Taking this into account can cause certainty in a hypothesis to decrease as\nthe evidence for it becomes apparently stronger. We perform a probabilistic\nBayesian analysis of this effect with examples based on (i) archaeological\nevidence, (ii) weighing of legal evidence, and (iii) cryptographic primality\ntesting.\n  We find that even with surprisingly low systemic failure rates high\nconfidence is very difficult to achieve and in particular we find that certain\nanalyses of cryptographically-important numerical tests are highly optimistic,\nunderestimating their false-negative rate by as much as a factor of $2^{80}$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 16:44:38 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Gunn", "Lachlan J.", ""], ["Chapeau-Blondeau", "Fran\u00e7ois", ""], ["McDonnell", "Mark", ""], ["Davis", "Bruce", ""], ["Allison", "Andrew", ""], ["Abbott", "Derek", ""]]}, {"id": "1601.01126", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth and Bruno Nicenboim", "title": "Statistical methods for linguistic research: Foundational Ideas - Part I", "comments": "30 pages, 9 figures, 3 tables. Under review with Language and\n  Linguistics Compass. Comments and suggestions for improvement welcome.\n  (Replaced version corrects several typos)", "journal-ref": "Language and Linguistics Compass 2016", "doi": "10.1111/lnc3.12201", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the fundamental ideas underlying statistical hypothesis testing\nusing the frequentist framework. We begin with a simple example that builds up\nthe one-sample t-test from the beginning, explaining important concepts such as\nthe sampling distribution of the sample mean, and the iid assumption. Then we\nexamine the p-value in detail, and discuss several important misconceptions\nabout what a p-value does and does not tell us. This leads to a discussion of\nType I, II error and power, and Type S and M error. An important conclusion\nfrom this discussion is that one should aim to carry out appropriately powered\nstudies. Next, we discuss two common issues we have encountered in\npsycholinguistics and linguistics: running experiments until significance is\nreached, and the \"garden-of-forking-paths\" problem discussed by Gelman and\nothers, whereby the researcher attempts to find statistical significance by\nanalyzing the data in different ways. The best way to use frequentist methods\nis to run appropriately powered studies, check model assumptions, clearly\nseparate exploratory data analysis from confirmatory hypothesis testing, and\nalways attempt to replicate results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 10:27:38 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 13:47:08 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Vasishth", "Shravan", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1601.01147", "submitter": "Won Chang", "authors": "Won Chang, Michael L. Stein, Jiali Wang, V. Rao Kotamarthi and\n  Elisabeth J. Moyer", "title": "Changes in Spatio-temporal Precipitation Patterns in Changing Climate\n  Conditions", "comments": "This work has been submitted for publication. Copyright in this work\n  may be transferred without further notice, and this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1175/JCLI-D-15-0844.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate models robustly imply that some significant change in precipitation\npatterns will occur. Models consistently project that the intensity of\nindividual precipitation events increases by approximately 6-7%/K, following\nthe increase in atmospheric water content, but that total precipitation\nincreases by a lesser amount (1-2 %/K in the global average in transient runs).\nSome other aspect of precipitation events must then change to compensate for\nthis difference. We develop here a new methodology for identifying individual\nrainstorms and studying their physical characteristics - including starting\nlocation, intensity, spatial extent, duration, and trajectory - that allows\nidentifying that compensating mechanism. We apply this technique to\nprecipitation over the contiguous U.S. from both radar-based data products and\nhigh-resolution model runs simulating 80 years of business-as-usual warming. In\nmodel studies, we find that the dominant compensating mechanism is a reduction\nof storm size. In summer, rainstorms become more intense but smaller, in\nwinter, rainstorm shrinkage still dominates, but storms also become less\nnumerous and shorter duration. These results imply that flood impacts from\nclimate change will be less severe than would be expected from changes in\nprecipitation intensity alone. We show also that projected changes are smaller\nthan model-observation biases, implying that the best means of incorporating\nthem into impact assessments is via \"data-driven simulations\" that apply\nmodel-projected changes to observational data. We therefore develop a\nsimulation algorithm that statistically describes model changes in\nprecipitation characteristics and adjusts data accordingly, and show that,\nespecially for summertime precipitation, it outperforms simulation approaches\nthat do not include spatial information.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 11:35:00 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 03:59:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Chang", "Won", ""], ["Stein", "Michael L.", ""], ["Wang", "Jiali", ""], ["Kotamarthi", "V. Rao", ""], ["Moyer", "Elisabeth J.", ""]]}, {"id": "1601.01420", "submitter": "Hailiang Du", "authors": "Hailiang Du and Leonard A. Smith", "title": "Multi-model Cross Pollination in Time", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2017.06.001", "report-no": null, "categories": "physics.data-an physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive skill of complex models is often not uniform in model-state space;\nin weather forecasting models, for example, the skill of the model can be\ngreater in populated regions of interest than in \"remote\" regions of the globe.\nGiven a collection of models, a multi-model forecast system using the cross\npollination in time approach can be generalised to take advantage of instances\nwhere some models produce systematically more accurate forecast of some\ncomponents of the model-state. This generalisation is stated and then\nsuccessfully demonstrated in a moderate ~40 dimensional nonlinear dynamical\nsystem suggested by Lorenz. In this demonstration four imperfect models, each\nwith similar global forecast skill, are used. Future applications in weather\nforecasting and in economic forecasting are discussed. The demonstration\nestablishes that cross pollinating forecast trajectories to enrich the\ncollection of simulations upon which the forecast is built can yield a new\nforecast system with significantly more skills than the original multi-model\nforecast system.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 07:21:59 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Du", "Hailiang", ""], ["Smith", "Leonard A.", ""]]}, {"id": "1601.01557", "submitter": "Zhiwen Liu Prof.", "authors": "Xiaoming Gou, Zhiwen Liu, Wei Liu, Yougen Xu, Jiabin Wang", "title": "Quaternion-Valued Single-Phase Model for Three-Phase Power Systems", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a quaternion-valued model is proposed in lieu of the Clarke's\n\\alpha, \\beta transformation to convert three-phase quantities to a\nhypercomplex single-phase signal. The concatenated signal can be used for\nharmonic distortion detection in three-phase power systems. In particular, the\nproposed model maps all the harmonic frequencies into frequencies in the\nquaternion domain, while the Clarke's transformation-based methods will fail to\ndetect the zero sequence voltages. Based on the quaternion-valued model, the\nFourier transform, the minimum variance distortionless response (MVDR)\nalgorithm and the multiple signal classification (MUSIC) algorithm are\npresented as examples to detect harmonic distortion. Simulations are provided\nto demonstrate the potentials of this new modeling method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 15:10:49 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Gou", "Xiaoming", ""], ["Liu", "Zhiwen", ""], ["Liu", "Wei", ""], ["Xu", "Yougen", ""], ["Wang", "Jiabin", ""]]}, {"id": "1601.01604", "submitter": "Ali Reza Fotouhi", "authors": "Ali Reza Fotouhi and Theresa Mulder", "title": "Modelling correlated ordinal data by random-effects logistic regression\n  models: simulation and application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered categorical data frequently arise in the analysis of biomedical,\nagricultural, and social sciences data. The logistic regression model is\nattractive in analyzing ordered categorical data because of its use in\ninterpretation of a parameter estimate. The ordered responses may be clustered\nand the subjects within the clusters may be positively correlated. To\naccommodate this correlation we add a random component to the linear predictor\nof each clustered response. This article presents and compares random-effects\nlogistic regression models for analyzing ordered categorical data. The proposed\nmodels are applied to an agricultural experimental data. In order to assess the\nperformance of the random-effects and homogeneous models we perform a\nsimulation study. Our analysis, application to real data and simulation, show\nthat the probability of the individual categories are estimated poorly in\nhomogeneous models. The random-effects models fit the data statistically\nsignificant and estimate the probability of the individual categories more\nprecisely.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 17:16:19 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Fotouhi", "Ali Reza", ""], ["Mulder", "Theresa", ""]]}, {"id": "1601.01849", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Simon N. Wood, Florian Hartig, Mark V. Bravington", "title": "An Extended Empirical Saddlepoint Approximation for Intractable\n  Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges posed by complex stochastic models used in computational\necology, biology and genetics have stimulated the development of approximate\napproaches to statistical inference. Here we focus on Synthetic Likelihood\n(SL), a procedure that reduces the observed and simulated data to a set of\nsummary statistics, and quantifies the discrepancy between them through a\nsynthetic likelihood function. SL requires little tuning, but it relies on the\napproximate normality of the summary statistics. We relax this assumption by\nproposing a novel, more flexible, density estimator: the Extended Empirical\nSaddlepoint approximation. In addition to proving the consistency of SL, under\neither the new or the Gaussian density estimator, we illustrate the method\nusing two examples. One of these is a complex individual-based forest model for\nwhich SL offers one of the few practical possibilities for statistical\ninference. The examples show that the new density estimator is able to capture\nlarge departures from normality, while being scalable to high dimensions, and\nthis in turn leads to more accurate parameter estimates, relative to the\nGaussian alternative. The new density estimator is implemented by the esaddle R\npackage, which can be found on the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 12:20:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 15:31:47 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 20:34:46 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 14:52:31 GMT"}, {"version": "v5", "created": "Thu, 8 Jun 2017 16:44:11 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""], ["Hartig", "Florian", ""], ["Bravington", "Mark V.", ""]]}, {"id": "1601.02043", "submitter": "Harald Baayen", "authors": "R. Harald Baayen and Jacolien van Rij and Cecile de Cat and Simon N.\n  Wood", "title": "Autocorrelated errors in experimental data in the language sciences:\n  Some solutions offered by Generalized Additive Mixed Models", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem that tends to be ignored in the statistical analysis of\nexperimental data in the language sciences is that responses often constitute\ntime series, which raises the problem of autocorrelated errors. If the errors\nindeed show autocorrelational structure, evaluation of the significance of\npredictors in the model becomes problematic due to potential anti-conservatism\nof p-values. This paper illustrates two tools offered by Generalized Additive\nMixed Models (GAMMs) (Lin and Zhang, 1999; Wood, 2006, 2011, 2013) for dealing\nwith autocorrelated errors, as implemented in the current version of the fourth\nauthor's mgcv package (1.8.9): the possibility to specify an ar(1) error model\nfor Gaussian models, and the possibility of using factor smooths for\nrandom-effect factors such as subject and item. These factor smooths are set up\nto have the same smoothing parameters, and are penalized to yield the\nnon-linear equivalent of random intercepts and random slopes in the classical\nlinear framework. Three case studies illustrate these issues.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 22:11:34 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Baayen", "R. Harald", ""], ["van Rij", "Jacolien", ""], ["de Cat", "Cecile", ""], ["Wood", "Simon N.", ""]]}, {"id": "1601.02224", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio and Marc G. Genton", "title": "Bayesian linear regression with skew-symmetric error distributions with\n  applications to survival analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian linear regression models with skew-symmetric scale mixtures\nof normal error distributions. These kinds of models can be used to capture\ndepartures from the usual assumption of normality of the errors in terms of\nheavy tails and asymmetry. We propose a general non-informative prior structure\nfor these regression models and show that the corresponding posterior\ndistribution is proper under mild conditions. We extend these propriety results\nto cases where the response variables are censored. The latter scenario is of\ninterest in the context of accelerated failure time models, which are relevant\nin survival analysis. We present a simulation study that demonstrates good\nfrequentist properties of the posterior credible intervals associated to the\nproposed priors. This study also sheds some light on the trade-off between\nincreased model flexibility and the risk of over-fitting. We illustrate the\nperformance of the proposed models with real data. Although we focus on models\nwith univariate response variables, we also present some extensions to the\nmultivariate case in the Supporting Web Material.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 14:37:17 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Genton", "Marc G.", ""]]}, {"id": "1601.02304", "submitter": "Ajith Gunatilaka Dr", "authors": "Branko Ristic, Ajith Gunatilaka, Ralph Gailis", "title": "Localisation of a source of biochemical agent dispersion using binary\n  measurements", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.atmosenv.2016.07.011", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the measurements collected at a number of known locations by a moving\nbinary sensor, characterised by an unknown threshold, the problem is to\nestimate the parameters of a biochemical source, continuously releasing\nmaterial into the atmosphere. The solution is formulated in the Bayesian\nframework using a dispersion model of Poisson distributed particle encounters\nin a turbulent flow. The method is implemented using the importance sampling\ntechnique and successfully validated with three experimental datasets under\ndifferent wind conditions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 02:34:47 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Ristic", "Branko", ""], ["Gunatilaka", "Ajith", ""], ["Gailis", "Ralph", ""]]}, {"id": "1601.02429", "submitter": "Allan De Freitas", "authors": "Allan De Freitas, Lyudmila Mihaylova, Amadou Gning, Donka Angelova,\n  Visakan Kadirkamanathan", "title": "Autonomous Crowds Tracking with Box Particle Filtering and Convolution\n  Particle Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems such as Unmanned Aerial Vehicles (UAVs) need to be able to\nrecognise and track crowds of people, e.g. for rescuing and surveillance\npurposes. Large groups generate multiple measurements with uncertain origin.\nAdditionally, often the sensor noise characteristics are unknown but\nmeasurements are bounded within certain intervals. In this work we propose two\nsolutions to the crowds tracking problem - with a box particle filtering\napproach and with a convolution particle filtering approach. The developed\nfilters can cope with the measurement origin uncertainty in an elegant way,\ni.e. resolve the data association problem. For the box particle filter (PF) we\nderive a theoretical expression of the generalised likelihood function in the\npresence of clutter. An adaptive convolution particle filter (CPF) is also\ndeveloped and the performance of the two filters is compared with the standard\nsequential importance resampling (SIR) PF. The pros and cons of the two filters\nare illustrated over a realistic scenario (representing a crowd motion in a\nstadium) for a large crowd of pedestrians. Accurate estimation results are\nachieved.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 12:58:23 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["De Freitas", "Allan", ""], ["Mihaylova", "Lyudmila", ""], ["Gning", "Amadou", ""], ["Angelova", "Donka", ""], ["Kadirkamanathan", "Visakan", ""]]}, {"id": "1601.02522", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathana\\\"el Perraudin, Pierre Vandergheynst", "title": "Stationary signal processing on graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2690388", "report-no": null, "categories": "cs.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a central tool in machine learning and information processing as\nthey allow to conveniently capture the structure of complex datasets. In this\ncontext, it is of high importance to develop flexible models of signals defined\nover graphs or networks. In this paper, we generalize the traditional concept\nof wide sense stationarity to signals defined over the vertices of arbitrary\nweighted undirected graphs. We show that stationarity is expressed through the\ngraph localization operator reminiscent of translation. We prove that\nstationary graph signals are characterized by a well-defined Power Spectral\nDensity that can be efficiently estimated even for large graphs. We leverage\nthis new concept to derive Wiener-type estimation procedures of noisy and\npartially observed signals and illustrate the performance of this new model for\ndenoising and regression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:58:45 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 16:42:30 GMT"}, {"version": "v3", "created": "Thu, 21 Apr 2016 16:34:34 GMT"}, {"version": "v4", "created": "Fri, 8 Jul 2016 21:25:26 GMT"}, {"version": "v5", "created": "Fri, 21 Apr 2017 18:30:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Perraudin", "Nathana\u00ebl", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.02748", "submitter": "Gregory Giecold", "authors": "Gregory Giecold, Eugenio Marco, Lorenzo Trippa and Guo-Cheng Yuan", "title": "Robust Lineage Reconstruction from High-Dimensional Single-Cell Data", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell gene expression data provide invaluable resources for systematic\ncharacterization of cellular hierarchy in multi-cellular organisms. However,\ncell lineage reconstruction is still often associated with significant\nuncertainty due to technological constraints. Such uncertainties have not been\ntaken into account in current methods. We present ECLAIR, a novel computational\nmethod for the statistical inference of cell lineage relationships from\nsingle-cell gene expression data. ECLAIR uses an ensemble approach to improve\nthe robustness of lineage predictions, and provides a quantitative estimate of\nthe uncertainty of lineage branchings. We show that the application of ECLAIR\nto published datasets successfully reconstructs known lineage relationships and\nsignificantly improves the robustness of predictions. In conclusion, ECLAIR is\na powerful bioinformatics tool for single-cell data analysis. It can be used\nfor robust lineage reconstruction with quantitative estimate of prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 07:01:55 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Giecold", "Gregory", ""], ["Marco", "Eugenio", ""], ["Trippa", "Lorenzo", ""], ["Yuan", "Guo-Cheng", ""]]}, {"id": "1601.02775", "submitter": "Lars Lau Raket", "authors": "Lars Lau Raket, Britta Grimme, Gregor Sch\\\"oner, Christian Igel and Bo\n  Markussen", "title": "Separating timing, movement conditions and individual differences in the\n  analysis of human movement", "comments": "30 pages, 21 figures and tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central task in the analysis of human movement behavior is to determine\nsystematic patterns and differences across experimental conditions,\nparticipants and repetitions. This is possible because human movement is highly\nregular, being constrained by invariance principles. Movement timing and\nmovement path, in particular, are linked through scaling laws. Separating\nvariations of movement timing from the spatial variations of movements is a\nwell-known challenge that is addressed in current approaches only through forms\nof preprocessing that bias analysis. Here we propose a novel nonlinear\nmixed-effects model for analyzing temporally continuous signals that contain\nsystematic effects in both timing and path. Identifiability issues of path\nrelative to timing are overcome by using maximum likelihood estimation in which\nthe most likely separation of space and time is chosen given the variation\nfound in data. The model is applied to analyze experimental data of human arm\nmovements in which participants move a hand-held object to a target location\nwhile avoiding an obstacle. The model is used to classify movement data\naccording to participant. Comparison to alternative approaches establishes\nnonlinear mixed-effects models as viable alternatives to conventional analysis\nframeworks. The model is then combined with a novel factor-analysis model that\nestimates the low-dimensional subspace within which movements vary when the\ntask demands vary. Our framework enables us to visualize different dimensions\nof movement variation and to test hypotheses about the effect of obstacle\nplacement and height on the movement path. We demonstrate that the approach can\nbe used to uncover new properties of human movement.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 09:14:58 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 21:01:14 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Raket", "Lars Lau", ""], ["Grimme", "Britta", ""], ["Sch\u00f6ner", "Gregor", ""], ["Igel", "Christian", ""], ["Markussen", "Bo", ""]]}, {"id": "1601.02789", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Danijel Kor\\v{z}inek", "title": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking", "comments": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking. arXiv admin note: text overlap with\n  arXiv:1509.09088", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-speaking is a mechanism for obtaining high quality subtitles for use in\nlive broadcast and other public events. Because it relies on humans performing\nthe actual re-speaking, the task of estimating the quality of the results is\nnon-trivial. Most organisations rely on humans to perform the actual quality\nassessment, but purely automatic methods have been developed for other similar\nproblems, like Machine Translation. This paper will try to compare several of\nthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will\nthen be matched to the human-derived NER metric, commonly used in re-speaking.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:06:52 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Kor\u017einek", "Danijel", ""]]}, {"id": "1601.02806", "submitter": "Gianluca Rosso", "authors": "Gianluca Rosso", "title": "Identification of Risk Extreme Values in a Time Series and Analysis with\n  an Autoregressive Method - Application for Climate Risk Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article there is no intention to repeat basic concepts about risk\nmanagement, but we will try to define why often is usefull the time series\nanalysis during the assessment of risks, and how is possible to compute a\nsignificative analysis using regression and autoregression. After some basic\nconcepts about trend analysis, will be introduced some methods to identify\npeaks. This is often usefull when there is no need to use the full time series,\nbecause sometimes is more practical to focus only on the extremes. With a\ncorrect time series without not-anomalous data, the extremes time series are\ntreated with a simply autoregression model. This drives to know if the time\nseries has a correlation between periods, and how many periods could be\nconsidered lagged among them. We think that climate events frequently are\nlagged because the climate show a clear increasing tendency, and that climate\nrisks are potentially increasing during the time. There will be no specific\nconclusion related with risk management, because the proposed solution with\nautoregression can be adapted to any time series analysis.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 11:27:57 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Rosso", "Gianluca", ""]]}, {"id": "1601.02900", "submitter": "Jan Palczewski", "authors": "Jhonny Gonzalez, John Moriarty, Jan Palczewski", "title": "Bayesian calibration and number of jump components in electricity spot\n  price models", "comments": "Extended discussion of empirical findings plus minor editorial\n  updates", "journal-ref": null, "doi": "10.1016/j.eneco.2017.04.022", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find empirical evidence that mean-reverting jump processes are not\nstatistically adequate to model electricity spot price spikes but independent,\nsigned sums of such processes are statistically adequate. Further we\ndemonstrate a change in the composition of these sums after a major economic\nevent. This is achieved by developing a Markov Chain Monte Carlo (MCMC)\nprocedure for Bayesian model calibration and a Bayesian assessment of model\nadequacy (posterior predictive checking). In particular we determine the number\nof signed mean-reverting jump components required in the APXUK and EEX markets,\nin time periods both before and after the recent global financial crises.\nStatistically, consistent structural changes occur across both markets, with a\nreduction of the intensity and size, or the disappearance, of positive price\nspikes in the later period. All code and data are provided to enable\nreplication of results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:11:03 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 20:30:35 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 14:21:42 GMT"}, {"version": "v4", "created": "Fri, 5 May 2017 12:18:09 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Gonzalez", "Jhonny", ""], ["Moriarty", "John", ""], ["Palczewski", "Jan", ""]]}, {"id": "1601.02921", "submitter": "Cesar Manchein", "authors": "Roberto Artuso, Cesar Manchein, Matteo Sala", "title": "Correlation decay and large deviations for mixed systems", "comments": "6 pages, 2 figures, submitted to publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider low--dimensional dynamical systems with a mixed phase space and\ndiscuss the typical appearance of slow, polynomial decay of correlations: in\nparticular we emphasize how this mixing rate is related to large deviations\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:40:39 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 14:26:15 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Artuso", "Roberto", ""], ["Manchein", "Cesar", ""], ["Sala", "Matteo", ""]]}, {"id": "1601.03067", "submitter": "Stefano Peluso", "authors": "Stefano Peluso, Antonietta Mira, Pietro Muliere, Alessandro Lomi", "title": "International Trade: a Reinforced Urn Network Model", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified modelling framework that theoretically justifies the\nmain empirical regularities characterizing the international trade network.\nEach country is associated to a Polya urn whose composition controls the\npropensity of the country to trade with other countries. The urn composition is\nupdated through the walk of the Reinforced Urn Process of Muliere et al.\n(2000). The model implies a local preferential attachment scheme and a power\nlaw right tail behaviour of bilateral trade flows. Different assumptions on the\nurns' reinforcement parameters account for local clustering, path-shortening\nand sparsity. Likelihood-based estimation approaches are facilitated by\nfeasible likelihood analytical derivation in various network settings. A\nsimulated example and the empirical results on the international trade network\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 21:27:35 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Peluso", "Stefano", ""], ["Mira", "Antonietta", ""], ["Muliere", "Pietro", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1601.03425", "submitter": "Radu Balan", "authors": "Radu Balan", "title": "Frames and Phaseless Reconstruction", "comments": "Lecture Notes for the 2015 AMS Short Course \"Finite Frame Theory: A\n  Complete Introduction to Overcompleteness\", Jan. 2015, San Antonio. To appear\n  in Proceedings of Symposia in Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.MG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frame design for phaseless reconstruction is now part of the broader problem\nof nonlinear reconstruction and is an emerging topic in harmonic analysis. The\nproblem of phaseless reconstruction can be simply stated as follows. Given the\nmagnitudes of the coefficients generated by a linear redundant system (frame),\nwe want to reconstruct the unknown input. This problem first occurred in X-ray\ncrystallography starting in the early 20th century. The same nonlinear\nreconstruction problem shows up in speech processing, particularly in speech\nrecognition.\n  In this lecture we shall cover existing analysis results as well as stability\nbounds for signal recovery including: necessary and sufficient conditions for\ninjectivity, Lipschitz bounds of the nonlinear map and its left inverses,\nstochastic performance bounds, and algorithms for signal recovery.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 22:05:59 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Balan", "Radu", ""]]}, {"id": "1601.03474", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Nagesh Adluru, Houri K. Vorperian", "title": "Kernel Regression on Manifolds and Its Application to Modeling\n  Disconnected Anatomic Structures", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified statistical approach to modeling disconnected 3D\nanatomical structures extracted from medical images. Due to image acquisition\nand preprocessing noises, it is expected the imaging data is noisy. The surface\ncoordinates of the structures are regressed using the weighted linear\ncombination of Laplace-Beltrami (LB) eigenfunctions to smooth out noisy data\nand perform statistical analysis. The method is applied in characterizing the\n3D growth pattern of human hyoid bone between ages 0 and 20. We detected a\nsignificant age effect on localized parts of the hyoid bone.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 04:08:48 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 19:38:39 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Chung", "Moo K.", ""], ["Adluru", "Nagesh", ""], ["Vorperian", "Houri K.", ""]]}, {"id": "1601.03519", "submitter": "Durba Bhattacharya ms", "authors": "Durba Bhattacharya and Sourabh Bhattacharya", "title": "Effects of Gene-Environment and Gene-Gene Interactions in Case-Control\n  Studies: A Novel Bayesian Semiparametric Approach", "comments": "The latest version. arXiv admin note: text overlap with\n  arXiv:1411.7571", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognizance of gene-environment interactions may help prevent or detain the\nonset of complex diseases like cardiovascular disease, cancer, type2 diabetes,\nautism or asthma by adjustments to lifestyle. In this regard, we extend the\nBayesian semiparametric gene-gene interaction model of Bhattacharya &\nBhattacharya (2015) to include the possibility of influencing gene-gene\ninteractions by environmental variables and possible mutations caused by the\nenvironment. Our model accounts for the unknown number of genetic\nsub-populations via finite mixtures composed of Dirichlet processes, which are\nrelated to each other through a hierarchical matrix normal structure\nresponsible for inducing gene-gene interactions and possible mutations in\nassociation with environmental variables. We also extend the Bayesian\nhypotheses testing procedures of Bhattacharya & Bhattacharya (2015) to detect\nthe roles of genes and their interactions, environment and the influence of\nenvironment on gene-gene interactions, in case-control studies. We develop an\neffective parallel computing methodology, which harnesses the power of parallel\nprocessing technology to the efficiencies of our conditionally independent\nGibbs sampling and Transformation based MCMC (TMCMC) methods.Applications of\nour model and methods to simulation studies with biologically realistic\ncase-control genotype datasets obtained under five distinct set-ups yield\nencouraging results in each case. We followed these up by application of our\nideas to a real, case-control based genotype dataset on early onset of\nmyocardial infarction. Beside being in broad agreement with the reported\nliterature on this dataset, the results obtained give some interesting insights\nto the differential effect of gender on MI.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 08:48:17 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 17:28:11 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 10:31:10 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Bhattacharya", "Durba", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1601.03670", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston, Laura M. Sangalli", "title": "Smooth Principal Component Analysis over two-dimensional manifolds with\n  an application to Neuroimaging", "comments": "33 pages", "journal-ref": null, "doi": "10.1214/16-AOAS975", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of high-dimensional neuroimaging signals located\nover the cortical surface, we introduce a novel Principal Component Analysis\ntechnique that can handle functional data located over a two-dimensional\nmanifold. For this purpose a regularization approach is adopted, introducing a\nsmoothing penalty coherent with the geodesic distance over the manifold. The\nmodel introduced can be applied to any manifold topology, can naturally handle\nmissing data and functional samples evaluated in different grids of points. We\napproach the discretization task by means of finite element analysis and\npropose an efficient iterative algorithm for its resolution. We compare the\nperformances of the proposed algorithm with other approaches classically\nadopted in literature. We finally apply the proposed method to resting state\nfunctional magnetic resonance imaging data from the Human Connectome Project,\nwhere the method shows substantial differential variations between brain\nregions that were not apparent with other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 17:22:04 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 11:33:02 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""], ["Sangalli", "Laura M.", ""]]}, {"id": "1601.03684", "submitter": "Rui J. Costa", "authors": "Rui J. Costa and Hilde Wilkinson-Herbots", "title": "Efficient Maximum-Likelihood Inference For The\n  Isolation-With-Initial-Migration Model With Potentially Asymmetric Gene Flow", "comments": "Computer code in R included in the ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The isolation-with-migration (IM) model is a common tool to make inferences\nabout the presence of gene flow during speciation, using polymorphism data.\nHowever, Becquet and Przeworski (2009) report that the parameter estimates\nobtained by fitting the IM model are very sensitive to the model's assumptions,\nincluding the assumption of constant gene flow until the present. This paper is\nconcerned with the isolation-with-initial-migration (IIM) model of\nWilkinson-Herbots (2012), which drops precisely this assumption. In the IIM\nmodel, one ancestral population divides into two descendant subpopulations,\nbetween which there is an initial period of gene flow and a subsequent period\nof isolation. We derive a fast method of fitting an extended version of the IIM\nmodel, which allows for asymmetric gene flow and unequal subpopulation sizes.\nThis is a maximum-likelihood method, applicable to observations on the number\nof different sites between pairs of DNA sequences from a large number of\nindependent loci. In addition to obtaining parameter estimates, our method can\nalso be used to distinguish between alternative models representing different\nevolutionary cenarios, by means of likelihood ratio tests. We illustrate the\nprocedure on pairs of Drosophila sequences from approximately 30,000 loci. The\ncomputing time needed to fit the most complex version of the model to this data\nset is only a couple of minutes. The code to fit the IIM model can be found in\nthe supplementary files of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 18:08:08 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Costa", "Rui J.", ""], ["Wilkinson-Herbots", "Hilde", ""]]}, {"id": "1601.03868", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Grigory Sokolov", "title": "An Analytic Expression for the Distribution of the Generalized\n  Shiryaev-Roberts Diffusion", "comments": "45 pages; 8 figures; to appear in Methodology and Computing in\n  Applied Probability", "journal-ref": null, "doi": "10.1007/s11009-016-9478-7", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quickest change-point detection problem where the aim is to\ndetect the onset of a pre-specified drift in \"live\"-monitored standard Brownian\nmotion; the change-point is assumed unknown (nonrandom). The topic of interest\nis the distribution of the Generalized Shryaev-Roberts (GSR) detection\nstatistic set up to \"sense\" the presence of the drift. Specifically, we derive\na closed-form formula for the transition probability density function (pdf) of\nthe time-homogeneous Markov diffusion process generated by the GSR statistic\nwhen the Brownian motion under surveillance is \"drift-free\", i.e., in the\npre-change regime; the GSR statistic's (deterministic) nonnegative headstart is\nassumed arbitrarily given. The transition pdf formula is found analytically,\nthrough direct solution of the respective Kolmogorov forward equation via the\nFourier spectral method to achieve separation of the spacial and temporal\nvariables. The obtained result generalizes the well-known formula for the\n(pre-change) stationary distribution of the GSR statistic: the latter's\nstationary distribution is the temporal limit of the distribution sought in\nthis work. To conclude, we exploit the obtained formula numerically and briefly\nstudy the pre-change behavior of the GSR statistic versus three factors: (a)\ndrift-shift magnitude, (b) time, and (c) the GSR statistic's headstart.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 10:46:31 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""]]}, {"id": "1601.04083", "submitter": "Alexander Volfovsky", "authors": "Guillaume W. Basse, Alexander Volfovsky and Edoardo M. Airoldi", "title": "Observational studies with unknown time of treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time plays a fundamental role in causal analyses, where the goal is to\nquantify the effect of a specific treatment on future outcomes. In a randomized\nexperiment, times of treatment, and when outcomes are observed, are typically\nwell defined. In an observational study, treatment time marks the point from\nwhich pre-treatment variables must be regarded as outcomes, and it is often\nstraightforward to establish. Motivated by a natural experiment in online\nmarketing, we consider a situation where useful conceptualizations of the\nexperiment behind an observational study of interest lead to uncertainty in the\ndetermination of times at which individual treatments take place. Of interest\nis the causal effect of heavy snowfall in several parts of the country on daily\nmeasures of online searches for batteries, and then purchases. The data\navailable give information on actual snowfall, whereas the natural treatment is\nthe anticipation of heavy snowfall, which is not observed. In this article, we\nintroduce formal assumptions and inference methodology centered around a novel\nnotion of plausible time of treatment. These methods allow us to explicitly\nbound the last plausible time of treatment in observational studies with\nunknown times of treatment, and ultimately yield valid causal estimates in such\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 21:39:43 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Volfovsky", "Alexander", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1601.04251", "submitter": "Giulia Prando", "authors": "Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso", "title": "On-line Bayesian System Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an on-line system identification setting, in which new data\nbecome available at given time steps. In order to meet real-time estimation\nrequirements, we propose a tailored Bayesian system identification procedure,\nin which the hyper-parameters are still updated through Marginal Likelihood\nmaximization, but after only one iteration of a suitable iterative optimization\nalgorithm. Both gradient methods and the EM algorithm are considered for the\nMarginal Likelihood optimization. We compare this \"1-step\" procedure with the\nstandard one, in which the optimization method is run until convergence to a\nlocal minimum. The experiments we perform confirm the effectiveness of the\napproach we propose.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 05:20:19 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Romeres", "Diego", ""], ["Prando", "Giulia", ""], ["Pillonetto", "Gianluigi", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1601.04302", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis, Evangelos Papalexakis", "title": "Footballonomics: The Anatomy of American Football; Evidence from 7 years\n  of NFL game data", "comments": "Working study - Papers has been presented at the Machine Learning and\n  Data Mining for Sports Analytics 2016 workshop and accepted at PLOS ONE", "journal-ref": null, "doi": "10.1371/journal.pone.0168716", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do NFL teams make rational decisions? What factors potentially affect the\nprobability of wining a game in NFL? How can a team come back from a\ndemoralizing interception? In this study we begin by examining the hypothesis\nof rational coaching, that is, coaching decisions are always rational with\nrespect to the maximization of the expected points scored. We reject this\nhypothesis by analyzing the decisions made in the past 7 NFL seasons for two\nparticular plays; (i) the Point(s) After Touchdown (PAT) and (ii) the fourth\ndown decisions. Having rejected the rational coaching hypothesis we move on to\nexamine how the detailed game data collected can potentially inform game-day\ndecisions. While NFL teams personnel definitely have an intuition on which\nfactors are crucial for winning a game, in this work we take a data-driven\napproach and provide quantifiable evidence using a large dataset of NFL games\nfor the 7-year period between 2009 and 2015. In particular, we use a logistic\nregression model to identify the impact and the corresponding statistical\nsignificance of factors such as possession time, number of penalty yards,\nbalance between passing and rushing offense etc. Our results clearly imply that\navoiding turnovers is the best strategy for winning a game but turnovers can be\novercome with letting the offense on the field for more time. Finally we\ncombine our descriptive model with statistical bootstrap in order to provide a\nprediction engine for upcoming NFL games. Our evaluations indicate that even by\nonly considering a small number of (straightforward) factors, we can achieve a\nvery good prediction accuracy. In particular, the average accuracy during\nseasons 2014 and 2015 is approximately 63%. This performance is comparable to\nthe more complicated state-of-the-art prediction systems, while it outperforms\nexpert analysts 60% of the time.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 15:21:14 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 13:33:53 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2016 15:50:03 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2016 18:37:46 GMT"}, {"version": "v5", "created": "Mon, 21 Nov 2016 06:27:47 GMT"}, {"version": "v6", "created": "Tue, 20 Dec 2016 13:36:46 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Pelechrinis", "Konstantinos", ""], ["Papalexakis", "Evangelos", ""]]}, {"id": "1601.04351", "submitter": "Youssouf Toukourou", "authors": "Fran\\c{c}ois Dufresne, Enkelejd Hashorva, Gildas Ratovomirija and\n  Youssouf Toukourou", "title": "On bivariate lifetime modelling in life insurance applications", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance and annuity products covering several lives require the modelling\nof the joint distribution of future lifetimes. In the interest of simplifying\ncalculations, it is common in practice to assume that the future lifetimes\namong a group of people are independent. However, extensive research over the\npast decades suggests otherwise. In this paper, a copula approach is used to\nmodel the dependence between lifetimes within a married couple \\eH{using data\nfrom a large Canadian insurance company}. As a novelty, the age difference and\nthe \\eH{gender} of the elder partner are introduced as an argument of the\ndependence parameter. \\green{Maximum likelihood techniques are} thus\nimplemented for the parameter estimation. Not only do the results make clear\nthat the correlation decreases with age difference, but also the dependence\nbetween the lifetimes is higher when husband is older than wife. A\ngoodness-of-fit procedure is applied in order to assess the validity of the\nmodel. Finally, considering several products available on the life insurance\nmarket, the paper concludes with practical illustrations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 21:20:56 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Dufresne", "Fran\u00e7ois", ""], ["Hashorva", "Enkelejd", ""], ["Ratovomirija", "Gildas", ""], ["Toukourou", "Youssouf", ""]]}, {"id": "1601.04736", "submitter": "Sarah Holte", "authors": "Sarah E. Holte", "title": "A Consistent Direct Method for Estimating Parameters in Ordinary\n  Differential Equations Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations provide an attractive framework for modeling\ntemporal dynamics in a variety of scientific settings. We show how consistent\nestimation for parameters in ODE models can be obtained by modifying a direct\n(non-iterative) least squares method similar to the direct methods originally\ndeveloped by Himmelbau, Jones and Bischoff. Our method is called the\nbias-corrected least squares (BCLS) method since it is a modification of least\nsquares methods known to be biased. Consistency of the BCLS method is\nestablished and simulations are used to compare the BCLS method to other\nmethods for parameter estimation in ODE models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 21:59:19 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Holte", "Sarah E.", ""]]}, {"id": "1601.04753", "submitter": "Andres Christen", "authors": "Nicol\\'as Kuschinski, J. Andr\\'es Christen, Adriana Monroy and\n  Silvestre Alavez", "title": "Modeling Oral Glucose Tolerance Test (OGTT) data and its Bayesian\n  Inverse Problem", "comments": "10 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One common way to test for diabetes is the Oral Glucose Tolerance Test or\nOGTT. Most common methods for the analysis of the data on this test are\nwasteful of much of the information contained therein. We propose to model\nblood glucose during an OGTT using a compartmental dynamic model with a system\nof ODEs. Our model works well in describing most scenarios that occur during an\nOGTT considering only 4 parameters. Fitting the model to data is an inverse\nproblem, which is suitable for Bayesian inference. Priors are specified and\nposterior inference results are shown using real data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 23:05:35 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 14:39:26 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kuschinski", "Nicol\u00e1s", ""], ["Christen", "J. Andr\u00e9s", ""], ["Monroy", "Adriana", ""], ["Alavez", "Silvestre", ""]]}, {"id": "1601.04879", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Cinzia Viroli, Ernst Wit", "title": "Mixture model with multiple allocations for clustering spatially\n  correlated observations in the analysis of ChIP-Seq data", "comments": "25 pages; 3 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering is a technique widely used to group a collection of\nunits into mutually exclusive groups. There are, however, situations in which\nan observation could in principle belong to more than one cluster. In the\ncontext of Next-Generation Sequencing (NGS) experiments, for example, the\nsignal observed in the data might be produced by two (or more) different\nbiological processes operating together and a gene could participate in both\n(or all) of them. We propose a novel approach to cluster NGS discrete data,\ncoming from a ChIP-Seq experiment, with a mixture model, allowing each unit to\nbelong potentially to more than one group: these multiple allocation clusters\ncan be flexibly defined via a function combining the features of the original\ngroups without introducing new parameters. The formulation naturally gives rise\nto a `zero-inflation group' in which values close to zero can be allocated,\nacting as a correction for the abundance of zeros that manifest in this type of\ndata. We take into account the spatial dependency between observations, which\nis described through a latent Conditional Auto-Regressive process that can\nreflect different dependency patterns. We assess the performance of our model\nwithin a simulation environment and then we apply it to ChIP-seq real data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 11:40:50 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 11:02:11 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Ranciati", "Saverio", ""], ["Viroli", "Cinzia", ""], ["Wit", "Ernst", ""]]}, {"id": "1601.05104", "submitter": "Ryan Womack", "authors": "Ryan P. Womack", "title": "ARL Libraries and Research: Correlates of Grant Funding", "comments": "revised June 2016 version of original January 2016 submission.\n  Revision contains no substantive changes, made exclusively to correct typos\n  detected in the original manuscript", "journal-ref": null, "doi": "10.1016/j.acalib.2016.06.006", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While providing the resources and tools that make advanced research possible\nis a primary mission of academic libraries at large research universities, many\nother elements also contribute to the success of the research enterprise, such\nas institutional funding, staffing, labs, and equipment. This study focuses on\nU.S. members of the ARL, the Association for Research Libraries. Research\nsuccess is measured by the total grant funding received by the University,\ncreating an ordered set of categories. Combining data from the NSF National\nCenter for Science and Engineering Statistics, ARL Statistics, and IPEDS, the\nprimary explanatory factors for research success are examined. Using linear\nregression, logistic regression, and the cumulative logit model, the\nbest-fitting models generated by ARL data, NSF data, and the combined data set\nfor both nominal and per capita funding are compared. These models produce the\nmost relevant explanatory variables for research funding, which do not include\nlibrary-related variables in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 05:17:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 04:13:36 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Womack", "Ryan P.", ""]]}, {"id": "1601.05156", "submitter": "Boyu Ren", "authors": "Boyu Ren, Sergio Bacallado, Stefano Favaro, Susan Holmes, Lorenzo\n  Trippa", "title": "Bayesian Nonparametric Ordination for the Analysis of Microbial\n  Communities", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1288631", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human microbiome studies use sequencing technologies to measure the abundance\nof bacterial species or Operational Taxonomic Units (OTUs) in samples of\nbiological material. Typically the data are organized in contingency tables\nwith OTU counts across heterogeneous biological samples. In the microbial\necology community, ordination methods are frequently used to investigate latent\nfactors or clusters that capture and describe variations of OTU counts across\nbiological samples. It remains important to evaluate how uncertainty in\nestimates of each biological sample's microbial distribution propagates to\nordination analyses, including visualization of clusters and projections of\nbiological samples on low dimensional spaces. We propose a Bayesian analysis\nfor dependent distributions to endow frequently used ordinations with estimates\nof uncertainty. A Bayesian nonparametric prior for dependent normalized random\nmeasures is constructed, which is marginally equivalent to the normalized\ngeneralized Gamma process, a well-known prior for nonparametric analyses. In\nour prior the dependence and similarity between microbial distributions is\nrepresented by latent factors that concentrate in a low dimensional space. We\nuse a shrinkage prior to tune the dimensionality of the latent factors. The\nresulting posterior samples of model parameters can be used to evaluate\nuncertainty in analyses routinely applied in microbiome studies. Specifically,\nby combining them with multivariate data analysis techniques we can visualize\ncredible regions in ecological ordination plots. The characteristics of the\nproposed model are illustrated through a simulation study and applications in\ntwo microbiome datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 03:05:10 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 23:09:08 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ren", "Boyu", ""], ["Bacallado", "Sergio", ""], ["Favaro", "Stefano", ""], ["Holmes", "Susan", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1601.05257", "submitter": "Manon Kok", "authors": "Manon Kok and Thomas B. Sch\\\"on", "title": "Magnetometer calibration using inertial sensors", "comments": "19 pages, 8 figures", "journal-ref": "IEEE Sensors Journal, Volume 16, Issue 14, Pages 5679--5689, 2016", "doi": "10.1109/JSEN.2016.2569160", "report-no": null, "categories": "cs.SY cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a practical algorithm for calibrating a magnetometer\nfor the presence of magnetic disturbances and for magnetometer sensor errors.\nTo allow for combining the magnetometer measurements with inertial measurements\nfor orientation estimation, the algorithm also corrects for misalignment\nbetween the magnetometer and the inertial sensor axes. The calibration\nalgorithm is formulated as the solution to a maximum likelihood problem and the\ncomputations are performed offline. The algorithm is shown to give good results\nusing data from two different commercially available sensor units. Using the\ncalibrated magnetometer measurements in combination with the inertial sensors\nto determine the sensor's orientation is shown to lead to significantly\nimproved heading estimates.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 12:55:42 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 15:29:06 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 12:15:52 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Kok", "Manon", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1601.05788", "submitter": "Eric Hare", "authors": "Eric Hare, Heike Hofmann, and Alicia Carriquiry", "title": "Automatic Matching of Bullet Land Impressions", "comments": "27 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2009, the National Academy of Sciences published a report questioning the\nscientific validity of many forensic methods including firearm examination.\nFirearm examination is a forensic tool used to help the court determine whether\ntwo bullets were fired from the same gun barrel. During the firing process,\nrifling, manufacturing defects, and impurities in the barrel create striation\nmarks on the bullet. Identifying these striation markings in an attempt to\nmatch two bullets is one of the primary goals of firearm examination. We\npropose an automated framework for the analysis of the 3D surface measurements\nof bullet land impressions which transcribes the individual characteristics\ninto a set of features that quantify their similarities. This makes\nidentification of matches easier and allows for a quantification of both\nmatches and matchability of barrels. The automatic matching routine we propose\nmanages to (a) correctly identify land impressions (the surface between two\nbullet groove impressions) with too much damage to be suitable for comparison,\nand (b) correctly identify all 10,384 land-to-land matches of the James Hamby\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 20:59:17 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 16:28:16 GMT"}, {"version": "v3", "created": "Sat, 8 Oct 2016 20:20:32 GMT"}, {"version": "v4", "created": "Thu, 2 Feb 2017 20:27:30 GMT"}, {"version": "v5", "created": "Wed, 28 Jun 2017 12:45:54 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Hare", "Eric", ""], ["Hofmann", "Heike", ""], ["Carriquiry", "Alicia", ""]]}, {"id": "1601.05886", "submitter": "Zhendong Huang", "authors": "Zhendong Huang, Davide Ferrari and Guoqi Qian", "title": "Parsimonious and powerful composite likelihood testing for group\n  difference and genotype-phenotype association", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the association between a phenotype and many genetic variants from\ncase-control data is essential in genome-wide association study (GWAS). This is\na challenging task as many such variants are correlated or non-informative.\nSimilarities exist in testing the population difference between two groups of\nhigh dimensional data with intractable full likelihood function. Testing may be\ntackled by a maximum composite likelihood (MCL) not entailing the full\nlikelihood, but current MCL tests are subject to power loss for involving\nnon-informative or redundant sub-likelihoods. In this paper, we develop a\nforward search and test method for simultaneous powerful group difference\ntesting and informative sub-likelihoods composition. Our method constructs a\nsequence of Wald-type test statistics by including only informative\nsub-likelihoods progressively so as to improve the test power under local\nsparsity alternatives. Numerical studies show that it achieves considerable\nimprovement over the available tests as the modeling complexity grows. Our\nmethod is further validated by testing the motivating GWAS data on breast\ncancer with interesting results obtained.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 06:26:10 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Huang", "Zhendong", ""], ["Ferrari", "Davide", ""], ["Qian", "Guoqi", ""]]}, {"id": "1601.05922", "submitter": "Gergely Palla", "authors": "Gergely Tib\\'ely, P\\'eter Pollner, Gergely Palla", "title": "Partial order similarity based on mutual information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing the ranking of candidates by different voters is an important topic\nin social and information science with a high relevance from the point of view\nof practical applications. In general, ties and pairs of incomparable\ncandidates may occur, thus, the alternative rankings are described by partial\norders. Various distance measures between partial orders have already been\nintroduced, where zero distance is corresponding to a perfect match between a\npair of partial orders, and larger values signal greater differences. Here we\ntake a different approach and propose a similarity measure based on adjusted\nmutual information. In general, the similarity value of unity is corresponding\nto exactly matching partial orders, while a low similarity is associated to a\npair of independent partial orders. The time complexity of the computation of\nthis similarity measure is $\\mathcal{O}(\\left|{\\mathcal C}\\right|^3)$ in the\nworst case, and $\\mathcal{O}(\\left|{\\mathcal C}\\right|^2\\ln \\left|{\\mathcal\nC}\\right|)$ in the typical case of partial orders corresponding to trees with\nconstant branching number, where $\\left|{\\mathcal C}\\right|$ denotes the number\nof candidates. An interesting feature of our approach is that the similarity\nmeasure is sensitive to the position of the disagreements in the ranking:\nDifferences at the highly ranked candidates induce larger similarity drop\ncompared to disagreements at the bottom candidates.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 09:41:55 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Tib\u00e9ly", "Gergely", ""], ["Pollner", "P\u00e9ter", ""], ["Palla", "Gergely", ""]]}, {"id": "1601.06065", "submitter": "Peruru Subrahmanya Swamy", "authors": "Peruru Subrahmanya Swamy, Radha Krishna Ganti, Krishna Jagannathan", "title": "Adaptive CSMA under the SINR Model: Efficient Approximation Algorithms\n  for Throughput and Utility Maximization", "comments": "Accepted for publication in the IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Carrier Sense Multiple Access (CSMA) based scheduling algorithm\nfor a single-hop wireless network under a realistic\nSignal-to-interference-plus-noise ratio (SINR) model for the interference. We\npropose two local optimization based approximation algorithms to efficiently\nestimate certain attempt rate parameters of CSMA called fugacities. It is known\nthat adaptive CSMA can achieve throughput optimality by sampling feasible\nschedules from a Gibbs distribution, with appropriate fugacities.\nUnfortunately, obtaining these optimal fugacities is an NP-hard problem.\nFurther, the existing adaptive CSMA algorithms use a stochastic gradient\ndescent based method, which usually entails an impractically slow (exponential\nin the size of the network) convergence to the optimal fugacities. To address\nthis issue, we first propose an algorithm to estimate the fugacities, that can\nsupport a given set of desired service rates. The convergence rate and the\ncomplexity of this algorithm are independent of the network size, and depend\nonly on the neighborhood size of a link. Further, we show that the proposed\nalgorithm corresponds exactly to performing the well-known Bethe approximation\nto the underlying Gibbs distribution. Then, we propose another local algorithm\nto estimate the optimal fugacities under a utility maximization framework, and\ncharacterize its accuracy. Numerical results indicate that the proposed methods\nhave a good degree of accuracy, and achieve extremely fast convergence to\nnear-optimal fugacities, and often outperform the convergence rate of the\nstochastic gradient descent by a few orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:38:21 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 13:08:23 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Swamy", "Peruru Subrahmanya", ""], ["Ganti", "Radha Krishna", ""], ["Jagannathan", "Krishna", ""]]}, {"id": "1601.06149", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Ximing Ren and Aongus McCarthy and Gerald S. Buller\n  and Steve McLaughlin", "title": "Robust Bayesian target detection algorithm for depth imaging from sparse\n  single-photon data", "comments": "arXiv admin note: text overlap with arXiv:1507.02511", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian model and associated algorithm for depth\nand intensity profiling using full waveforms from time-correlated single-photon\ncounting (TCSPC) measurements in the limit of very low photon counts (i.e.,\ntypically less than 20 photons per pixel). The model represents each Lidar\nwaveform as an unknown constant background level, which is combined in the\npresence of a target, to a known impulse response weighted by the target\nintensity and finally corrupted by Poisson noise. The joint target detection\nand depth imaging problem is expressed as a pixel-wise model selection and\nestimation problem which is solved using Bayesian inference. Prior knowledge\nabout the problem is embedded in a hierarchical model that describes the\ndependence structure between the model parameters while accounting for their\nconstraints. In particular, Markov random fields (MRFs) are used to model the\njoint distribution of the background levels and of the target presence labels,\nwhich are both expected to exhibit significant spatial correlations. An\nadaptive Markov chain Monte Carlo algorithm including reversible-jump updates\nis then proposed to compute the Bayesian estimates of interest. This algorithm\nis equipped with a stochastic optimization adaptation mechanism that\nautomatically adjusts the parameters of the MRFs by maximum marginal likelihood\nestimation. Finally, the benefits of the proposed methodology are demonstrated\nthrough a series of experiments using real data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 16:32:43 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 14:18:25 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Altmann", "Yoann", ""], ["Ren", "Ximing", ""], ["McCarthy", "Aongus", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1601.06236", "submitter": "Pei Wang", "authors": "Lin S. Chen, Jiebiao Wang, Xianlong Wang and Pei Wang", "title": "A Mixed-effects Model for Incomplete Data With Batch-Level\n  Abundance-Dependent Missing-Data Mechanism", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In mass spectrometry based quantitative proteomics research, the emerging\niTRAQ technique has been widely adopted for high throughput protein profiling,\nas it enables one to measure multiple samples simultaneously in one multiplex\nexperiment and thus greatly enhances the throughput of protein quantification.\nHowever, the technical variation across different iTRAQ multiplex experiments\nis often large due to the dynamic nature of MS instruments. This leads to\nstrong batch effects in the iTRAQ data. Moreover, the iTRAQ data often contain\nsubstantial batch-level non-ignorable missingness. Specifically, the abundance\nmeasures of a given protein/peptide are often missing altogether in all the\nsamples from the same batch, with the missing probability depending on the\ncombined batch-level abundances. We term this unique missing-data mechanism as\nthe Batch-level Abundance-Dependent Missing-data mechanism (BADMM). We\nintroduce a new method, mixEMM, for analyzing iTRAQ data with batch effects and\nbatch-level non-ignorable missingness. The mixEMM method employs a linear\nmixed-effects model and explicitly models the batch effects and the BADMM in\nthe likelihood function. With simulation studies, we showed that compared with\nexisting approaches that utilize relative abundances and ignore the missing\nbatches under the missing completely at random assumption, the mixEMM method\nachieves more accurate parameter estimation and inference.We applied the method\nto an iTRAQ proteomics data from a breast cancer study and identified\nphosphopeptides differentially expressed between different breast cancer\nsubtypes. The method can be applied to general clustered data with cluster\nlevel non ignorable missing-data mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 05:37:16 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Chen", "Lin S.", ""], ["Wang", "Jiebiao", ""], ["Wang", "Xianlong", ""], ["Wang", "Pei", ""]]}, {"id": "1601.06288", "submitter": "Zijian Guo", "authors": "Zijian Guo, Jing Cheng, Scott A. Lorch and Dylan S. Small", "title": "Using an Instrumental Variable to Test for Unmeasured Confounding", "comments": null, "journal-ref": "Statistics in Medicine 2014, Volume 33, Issue 20, Pages 3528-3546", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important concern in an observational study is whether or not there is\nunmeasured confounding, i.e., unmeasured ways in which the treatment and\ncontrol groups differ before treatment that affect the outcome. We develop a\ntest of whether there is unmeasured confounding when an instrumental variable\n(IV) is available. An IV is a variable that is independent of the unmeasured\nconfounding and encourages a subject to take one treatment level vs. another,\nwhile having no effect on the outcome beyond its encouragement of a certain\ntreatment level. We show what types of unmeasured confounding can be tested for\nwith an IV and develop a test for this type of unmeasured confounding that has\ncorrect type I error rate. We show that the widely used Durbin-Wu-Hausman (DWH)\ntest can have inflated type I error rates when there is treatment effect\nheterogeneity. Additionally, we show that our test provides more insight into\nthe nature of the unmeasured confounding than the DWH test. We apply our test\nto an observational study of the effect of a premature infant being delivered\nin a high-level neonatal intensive care unit (one with mechanical assisted\nventilation and high volume) vs. a lower level unit, using the excess travel\ntime a mother lives from the nearest high-level unit to the nearest lower-level\nunit as an IV.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 16:54:11 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Guo", "Zijian", ""], ["Cheng", "Jing", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1601.06294", "submitter": "Zijian Guo", "authors": "Zijian Guo, Dylan S. Small, Stuart A. Gansky and Jing Cheng", "title": "Mediation Analysis for Count and Zero-Inflated Count Data without\n  Sequential Ignorability and Its Application in Dental Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis seeks to understand the mechanism by which a treatment\naffects an outcome. Count or zero-inflated count outcome are common in many\nstudies in which mediation analysis is of interest. For example, in dental\nstudies, outcomes such as decayed, missing and filled teeth are typically zero\ninflated. Existing mediation analysis approaches for count data assume\nsequential ignorability of the mediator. This is often not plausible because\nthe mediator is not randomized so that there are unmeasured confounders\nassociated with the mediator and the outcome. In this paper, we develop causal\nmethods based on instrumental variable (IV) approaches for mediation analysis\nfor count data possibly with a lot of zeros that do not require the assumption\nof sequential ignorability. We first define the direct and indirect effect\nratios for those data, and then propose estimating equations and use empirical\nlikelihood to estimate the direct and indirect effects consistently. A\nsensitivity analysis is proposed for violations of the IV exclusion restriction\nassumption. Simulation studies demonstrate that our method works well for\ndifferent types of outcomes under different settings. Our method is applied to\na randomized dental caries prevention trial and a study of the effect of a\nmassive flood in Bangladesh on children's diarrhea.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 17:19:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 11:34:17 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Guo", "Zijian", ""], ["Small", "Dylan S.", ""], ["Gansky", "Stuart A.", ""], ["Cheng", "Jing", ""]]}, {"id": "1601.06344", "submitter": "Ming Yang", "authors": "Ming Yang, Jianhui Wang, Haoran Diao, Junjian Qi and Xueshan Han", "title": "Interval Estimation for Conditional Failure Rates of Transmission Lines\n  with Limited Samples", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TSG.2016.2618623", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the conditional failure rate (CFR) of an overhead\ntransmission line (OTL) is essential for power system operational reliability\nassessment. It is hard to predict the CFR precisely, although great efforts\nhave been made to improve the estimation accuracy. One significant difficulty\nis the lack of available outage samples, due to which the law of large numbers\nis no longer applicable and no convincing statistical result can be obtained.\nTo address this problem, in this paper a novel imprecise probabilistic approach\nis proposed to estimate the CFR of an OTL. The imprecise Dirichlet model (IDM)\nis applied to establish the imprecise probabilistic relation between a single\nconditional variable and the failure rate of an OTL. Then a credal network is\nconstructed to integrate the IDM estimation results corresponding to different\nconditional variables and infer the CFR. Instead of providing a single-valued\nestimation result, the proposed approach predicts the possible interval of the\nCFR in order to explicitly indicate the uncertainty of the estimation and more\nobjectively represent the available knowledge. The proposed approach is\nillustrated by estimating the CFRs of two LGJ-300 transmission lines located in\nthe same region; the test results validate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 04:55:51 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 01:48:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Yang", "Ming", ""], ["Wang", "Jianhui", ""], ["Diao", "Haoran", ""], ["Qi", "Junjian", ""], ["Han", "Xueshan", ""]]}, {"id": "1601.06447", "submitter": "Shang Li", "authors": "Shang Li and Xiaoou Li and Xiaodong Wang and Jingchen Liu", "title": "Sequential Hypothesis Test with Online Usage-Constrained Sensor\n  Selection", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the sequential hypothesis testing problem with online\nsensor selection and sensor usage constraints. That is, in a sensor network,\nthe fusion center sequentially acquires samples by selecting one \"most\ninformative\" sensor at each time until a reliable decision can be made. In\nparticular, the sensor selection is carried out in the online fashion since it\ndepends on all the previous samples at each time. Our goal is to develop the\nsequential test (i.e., stopping rule and decision function) and sensor\nselection strategy that minimize the expected sample size subject to the\nconstraints on the error probabilities and sensor usages. To this end, we first\nrecast the usage-constrained formulation into a Bayesian optimal stopping\nproblem with different sampling costs for the usage-contrained sensors. The\nBayesian problem is then studied under both finite- and infinite-horizon\nsetups, based on which, the optimal solution to the original usage-constrained\nproblem can be readily established. Moreover, by capitalizing on the structures\nof the optimal solution, a lower bound is obtained for the optimal expected\nsample size. In addition, we also propose algorithms to approximately evaluate\nthe parameters in the optimal sequential test so that the sensor usage and\nerror probability constraints are satisfied. Finally, numerical experiments are\nprovided to illustrate the theoretical findings, and compare with the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 23:33:02 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Li", "Shang", ""], ["Li", "Xiaoou", ""], ["Wang", "Xiaodong", ""], ["Liu", "Jingchen", ""]]}, {"id": "1601.06537", "submitter": "Quentin Paris", "authors": "Geoffrey Decrouez, Michael Grabchak and Quentin Paris", "title": "Finite sample properties of the mean occupancy counts and probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a probability distribution $P$ on an at most countable alphabet $\\mathcal\nA$, this article gives finite sample bounds for the expected occupancy counts\n$\\mathbb E K_{n,r}$ and probabilities $\\mathbb E M_{n,r}$. Both upper and lower\nbounds are given in terms of the counting function $\\nu$ of $P$. Special\nattention is given to the case where $\\nu$ is bounded by a regularly varying\nfunction. In this case, it is shown that our general results lead to an\noptimal-rate control of the expected occupancy counts and probabilities with\nexplicit constants. Our results are also put in perspective with Turing's\nformula and recent concentration bounds to deduce bounds in probability. At the\nend of the paper, we discuss an extension of the occupancy problem to arbitrary\ndistributions in a metric space.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 10:00:30 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 17:51:13 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Decrouez", "Geoffrey", ""], ["Grabchak", "Michael", ""], ["Paris", "Quentin", ""]]}, {"id": "1601.06630", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Bayesian Estimation of Bipartite Matchings for Record Linkage", "comments": "This is a preprint of an article accepted for publication in the\n  Journal of the American Statistical Association. The final version contains\n  more materials and is organized differently", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bipartite record linkage task consists of merging two disparate datafiles\ncontaining information on two overlapping sets of entities. This is non-trivial\nin the absence of unique identifiers and it is important for a wide variety of\napplications given that it needs to be solved whenever we have to combine\ninformation from different sources. Most statistical techniques currently used\nfor record linkage are derived from a seminal paper by Fellegi and Sunter\n(1969). These techniques usually assume independence in the matching statuses\nof record pairs to derive estimation procedures and optimal point estimators.\nWe argue that this independence assumption is unreasonable and instead target a\nbipartite matching between the two datafiles as our parameter of interest.\nBayesian implementations allow us to quantify uncertainty on the matching\ndecisions and derive a variety of point estimators using different loss\nfunctions. We propose partial Bayes estimates that allow uncertain parts of the\nbipartite matching to be left unresolved. We evaluate our approach to record\nlinkage using a variety of challenging scenarios and show that it outperforms\nthe traditional methodology. We illustrate the advantages of our methods\nmerging two datafiles on casualties from the civil war of El Salvador.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:58:41 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1601.06749", "submitter": "Deirel Paz-Linares", "authors": "Deirel Paz-Linares, Mayrim Vega-Hern\\'andez, Pedro A. Rojas-L\\'opez,\n  Pedro A. Vald\\'es-Sosa and Eduardo Mart\\'inez-Montes", "title": "Empirical bayes formulation of the elastic net and mixed-norm models:\n  application to the eeg inverse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of EEG generating sources constitutes an Inverse Problem (IP)\nin Neuroscience. This is an ill-posed problem, due to the non-uniqueness of the\nsolution, and many kinds of prior information have been used to constrain it. A\ncombination of smoothness (L2 norm-based) and sparseness (L1 norm-based)\nconstraints is a flexible approach that have been pursued by important examples\nsuch as the Elastic Net (ENET) and mixed-norm (MXN) models. The former is used\nto find solutions with a small number of smooth non-zero patches, while the\nlatter imposes sparseness and smoothness simultaneously along different\ndimensions of the spatio-temporal matrix solutions. Both models have been\naddressed within the penalized regression approach, where the regularization\nparameters are selected heuristically, leading usually to non-optimal\nsolutions. The existing Bayesian formulation of ENET allows hyperparameter\nlearning, but using computationally intensive Monte Carlo/Expectation\nMaximization methods. In this work we attempt to solve the EEG IP using a\nBayesian framework for models based on mixtures of L1/L2 norms penalization\nfunctions (Laplace/Normal priors) such as ENET and MXN. We propose a Sparse\nBayesian Learning algorithm based on combining the Empirical Bayes and the\niterative coordinate descent procedures to estimate both the parameters and\nhyperparameters. Using simple but realistic simulations we found that our\nmethods are able to recover complicated source setups more accurately and with\na more robust variable selection than the ENET and LASSO solutions using\nclassical algorithms. We also solve the EEG IP using data coming from a visual\nattention experiment, finding more interpretable neurophysiological patterns\nwith our methods, as compared with other known methods such as LORETA, ENET and\nLASSO FUSION using the classical regularization approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:14:05 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 01:28:00 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Paz-Linares", "Deirel", ""], ["Vega-Hern\u00e1ndez", "Mayrim", ""], ["Rojas-L\u00f3pez", "Pedro A.", ""], ["Vald\u00e9s-Sosa", "Pedro A.", ""], ["Mart\u00ednez-Montes", "Eduardo", ""]]}, {"id": "1601.06805", "submitter": "Taha Yasseri", "authors": "Bertie Vidgen and Taha Yasseri", "title": "P-values: misunderstood and misused", "comments": "Published in Frontiers in Physics: Vidgen B and Yasseri T (2016)\n  P-Values: Misunderstood and Misused. Front. Phys. 4:6", "journal-ref": null, "doi": "10.3389/fphy.2016.00006", "report-no": null, "categories": "stat.AP cs.CY physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-values are widely used in both the social and natural sciences to quantify\nthe statistical significance of observed results. The recent surge of big data\nresearch has made the p-value an even more popular tool to test the\nsignificance of a study. However, substantial literature has been produced\ncritiquing how p-values are used and understood. In this paper we review this\nrecent critical literature, much of which is routed in the life sciences, and\nconsider its implications for social scientific research. We provide a coherent\npicture of what the main criticisms are, and draw together and disambiguate\ncommon themes. In particular, we explain how the False Discovery Rate is\ncalculated, and how this differs from a p-value. We also make explicit the\nBayesian nature of many recent criticisms, a dimension that is often\nunderplayed or ignored. We conclude by identifying practical steps to help\nremediate some of the concerns identified. We recommend that (i) far lower\nsignificance levels are used, such as $0.01$ or $0.001$, and (ii) p-values are\ninterpreted contextually, and situated within both the findings of the\nindividual study and the broader field of inquiry (through, for example,\nmeta-analyses).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 21:09:17 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 20:04:27 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Vidgen", "Bertie", ""], ["Yasseri", "Taha", ""]]}, {"id": "1601.06911", "submitter": "Irene Epifanio", "authors": "Irene Epifanio", "title": "Functional archetype and archetypoid analysis", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 104, December\n  2016, Pages 24-34", "doi": "10.1016/j.csda.2016.06.007", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetype and archetypoid analysis can be extended to functional data. Each\nfunction is represented as a mixture of actual observations (functional\narchetypoids) or functional archetypes, which are a mixture of observations in\nthe data set. Well-known Canadian temperature data are used to illustrate the\nanalysis developed. Computational methods are proposed for performing these\nanalyses, based on the coefficients of a basis. Unlike a previous attempt to\ncompute functional archetypes, which was only valid for an orthogonal basis,\nthe proposed methodology can be used for any basis. It is computationally less\ndemanding than the simple approach of discretizing the functions. Multivariate\nfunctional archetype and archetypoid analysis are also introduced and applied\nin an interesting problem about the study of human development around the world\nover the last 50 years. These tools can contribute to the understanding of a\nfunctional data set, as in the multivariate case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 07:34:41 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 06:15:21 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Epifanio", "Irene", ""]]}, {"id": "1601.07189", "submitter": "Sergey Porotsky", "authors": "Sergey Porotsky", "title": "Rare-Event Estimation for Dynamic Fault Trees", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Article describes the results of the development and using of Rare-Event\nMonte-Carlo Simulation Algorithms for Dynamic Fault Trees Estimation. For Fault\nTrees estimation usually analytical methods are used (Minimal Cut sets, Markov\nChains, etc.), but for complex models with Dynamic Gates it is necessary to use\nMonte-Carlo simulation with combination of Importance Sampling method. Proposed\narticle describes approach for this problem solution according for specific\nfeatures of Dynamic Fault Trees. There are assumed, that failures are\nnon-repairable with general distribution functions of times to failures (there\nmay be Exponential distribution, Weibull, Normal and Log-Normal, etc.).\nExpessions for Importance Sampling Re-Calculations are proposed and some\nnumerical results are considered\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 12:54:54 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Porotsky", "Sergey", ""]]}, {"id": "1601.07225", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, I. V. Tchervensky, V. S. Dimitrov, M. P. Mintchev", "title": "Wavelet Analysis in a Canine Model of Gastric Electrical Uncoupling", "comments": "18 pages, Fixed equation (6). arXiv admin note: substantial text\n  overlap with arXiv:1502.00239", "journal-ref": "Physiological Measurement, Volume 25, Number 6, 2004", "doi": "10.1088/0967-3334/25/6/002", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal gastric motility function could be related to gastric electrical\nuncoupling, the lack of electrical, and respectively mechanical,\nsynchronization in different regions of the stomach. Therefore, non-invasive\ndetection of the onset of gastric electrical uncoupling can be important for\ndiagnosing associated gastric motility disorders. The aim of this study is to\nprovide a wavelet-based analysis of electrogastrograms (EGG, the cutaneous\nrecordings of gastric electric activity), to detect gastric electric\nuncoupling. Eight-channel EGG recordings were acquired from sixteen dogs in\nbasal state and after each of two circular gastric myotomies. These myotomies\nsimulated mild and severe gastric electrical uncoupling, while keeping the\nseparated gastric sections electrophysiologically active by preserving their\nblood supply. After visual inspection, manually selected 10-minute EGG segments\nwere submitted to wavelet analysis. Quantitative methodology to choose an\noptimal wavelet was derived. This \"matching\" wavelet was determined using the\nPollen parameterization for 6-tap wavelet filters and error minimization\ncriteria. After a wavelet-based compression, the distortion of the approximated\nEGG signals was computed. Statistical analysis on the distortion values allowed\nto significantly ($p<0.05$) distinguish basal state from mild and severe\ngastric electrical uncoupling groups in particular EGG channels.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 23:35:19 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Cintra", "R. J.", ""], ["Tchervensky", "I. V.", ""], ["Dimitrov", "V. S.", ""], ["Mintchev", "M. P.", ""]]}, {"id": "1601.07252", "submitter": "Anshul Gupta", "authors": "Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard\n  Furuta, Laura Mandell", "title": "Font Identification in Historical Documents Using Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DL stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identifying the type of font (e.g., Roman, Blackletter) used in historical\ndocuments can help optical character recognition (OCR) systems produce more\naccurate text transcriptions. Towards this end, we present an active-learning\nstrategy that can significantly reduce the number of labeled samples needed to\ntrain a font classifier. Our approach extracts image-based features that\nexploit geometric differences between fonts at the word level, and combines\nthem into a bag-of-word representation for each page in a document. We evaluate\nsix sampling strategies based on uncertainty, dissimilarity and diversity\ncriteria, and test them on a database containing over 3,000 historical\ndocuments with Blackletter, Roman and Mixed fonts. Our results show that a\ncombination of uncertainty and diversity achieves the highest predictive\naccuracy (89% of test cases correctly classified) while requiring only a small\nfraction of the data (17%) to be labeled. We discuss the implications of this\nresult for mass digitization projects of historical documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:24:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Gupta", "Anshul", ""], ["Gutierrez-Osuna", "Ricardo", ""], ["Christy", "Matthew", ""], ["Furuta", "Richard", ""], ["Mandell", "Laura", ""]]}, {"id": "1601.07375", "submitter": "Sophia Sulis", "authors": "Sophia Sulis, David Mary and Lionel Bigot", "title": "Using hydrodynamical simulations of stellar atmospheres for periodogram\n  standardization : application to exoplanet detection", "comments": "5 pages, 3 figures. This manuscript was submitted and accepted to the\n  41st IEEE International Conference on Acoustics Speech and Signal Processing\n  (ICASSP), 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472514", "report-no": null, "categories": "stat.AP astro-ph.IM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to devise a detection method for exoplanet signatures (multiple\nsinusoids) that is both powerful and robust to partially unknown statistics\nunder the null hypothesis. In the considered application, the noise is mostly\ncreated by the stellar atmosphere, with statistics depending on the complicated\ninterplay of several parameters. Recent progresses in hydrodynamic (HD)\nsimulations show however that realistic stellar noise realizations can be\nnumerically produced off-line by astrophysicists. We propose a detection method\nthat is calibrated by HD simulations and analyze its performances. A comparison\nof the theoretical results with simulations on synthetic and real data shows\nthat the proposed method is powerful and robust.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 14:12:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sulis", "Sophia", ""], ["Mary", "David", ""], ["Bigot", "Lionel", ""]]}, {"id": "1601.07606", "submitter": "Narges Montazeri Shahtori", "authors": "Narges Montazeri Shahtori, Caterina Scoglio, Arash Pourhabib, Faryad\n  Darabi Sahneh", "title": "Sequential Monte Carlo Filtering Estimation of Ebola Progression in West\n  Africa", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a multivariate formulation of sequential Monte Carlo filter that\nutilizes mechanistic models for Ebola virus propagation and available incidence\ndata to simultaneously estimate the disease progression states and the model\nparameters. This method has the advantage of performing the inference online as\nthe new data becomes available and estimates the evolution of basic\nreproductive ratio $R_0(t)$ of the Ebola outbreak through time. Our analysis\nidentifies a peak in the basic reproductive ratio close to the time when Ebola\ncases were reported in Europe and the USA.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 00:13:06 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Shahtori", "Narges Montazeri", ""], ["Scoglio", "Caterina", ""], ["Pourhabib", "Arash", ""], ["Sahneh", "Faryad Darabi", ""]]}, {"id": "1601.07617", "submitter": "Chelsea Lofland", "authors": "Chelsea Lofland, Abel Rodriguez, Scott Moser", "title": "Assessing differences in legislators' revealed preferences: A case study\n  on the 107th U.S. Senate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roll call data are widely used to assess legislators' preferences and\nideology, as well as test theories of legislative behavior. In particular, roll\ncall data is often used to determine whether the revealed preferences of\nlegislators are affected by outside forces such as party pressure, minority\nstatus or procedural rules. This paper describes a Bayesian hierarchical model\nthat extends existing spatial voting models to test sharp hypotheses about\ndifferences in preferences the using posterior probabilities associated with\nsuch hypotheses. We use our model to investigate the effect of the change of\nparty majority status during the 107th U.S. Senate on the revealed preferences\nof senators. This analysis provides evidence that change in party affiliation\nmight affect the revealed preferences of legislators, but provides no evidence\nabout the effect of majority status on the revealed preferences of legislators.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 01:40:56 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Lofland", "Chelsea", ""], ["Rodriguez", "Abel", ""], ["Moser", "Scott", ""]]}, {"id": "1601.07624", "submitter": "Jieli Zhu", "authors": "Jieli Zhu, Tong Zhao, Tianyao Huang and Dengfeng Zhang", "title": "Analysis of Random Pulse Repetition Interval Radar", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/RADAR.2016.7485193", "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random pulse repetition interval (PRI) waveform arouses great interests in\nthe field of modern radars due to its ability to alleviate range and Doppler\nambiguities as well as enhance electronic counter-countermeasures (ECCM)\ncapabilities. Theoretical results pertaining to the statistical characteristics\nof ambiguity function (AF) are derived in this work, indicating that the range\nand Doppler ambiguities can be effectively suppressed by increasing the number\nof pulses and the range of PRI jitters. This provides an important guidance in\nterms of waveform design. As is well known, the significantly lifted sidelobe\npedestal induced by PRI randomization will degrade the performance of weak\ntarget detection. Proceeding from that, we propose to employ orthogonal\nmatching pursuit (OMP) to overcome this issue. Simulation results demonstrate\nthat the OMP method can effectively lower the sidelobe pedestal of strong\ntarget and improve the performance of weak target estimation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 02:28:47 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Zhu", "Jieli", ""], ["Zhao", "Tong", ""], ["Huang", "Tianyao", ""], ["Zhang", "Dengfeng", ""]]}, {"id": "1601.07931", "submitter": "Luke Kelly", "authors": "Luke J. Kelly and Geoff K. Nicholls", "title": "Lateral transfer in Stochastic Dollo models", "comments": "Improvements suggested by reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lateral transfer, a process whereby species exchange evolutionary traits\nthrough non-ancestral relationships, is a frequent source of model\nmisspecification in phylogenetic inference. Lateral transfer obscures the\nphylogenetic signal in the data as the histories of affected traits are mosaics\nof the overall phylogeny. We control for the effect of lateral transfer in a\nStochastic Dollo model and a Bayesian setting. Our likelihood is highly\nintractable as the parameters are the solution of a sequence of large systems\nof differential equations representing the expected evolution of traits along a\ntree. We illustrate our method on a data set of lexical traits in Eastern\nPolynesian languages and obtain an improved fit over the corresponding model\nwithout lateral transfer.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 22:09:12 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 10:02:27 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 18:36:02 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Kelly", "Luke J.", ""], ["Nicholls", "Geoff K.", ""]]}, {"id": "1601.07994", "submitter": "Scott Powers", "authors": "Scott Powers, Trevor Hastie, Robert Tibshirani", "title": "Customized training with an application to mass spectrometric imaging of\n  cancer tissue", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS866 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1709-1725", "doi": "10.1214/15-AOAS866", "report-no": "IMS-AOAS-AOAS866", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple, interpretable strategy for making predictions on test\ndata when the features of the test data are available at the time of model\nfitting. Our proposal - customized training - clusters the data to find\ntraining points close to each test point and then fits an $\\ell_1$-regularized\nmodel (lasso) separately in each training cluster. This approach combines the\nlocal adaptivity of $k$-nearest neighbors with the interpretability of the\nlasso. Although we use the lasso for the model fitting, any supervised learning\nmethod can be applied to the customized training sets. We apply the method to a\nmass-spectrometric imaging data set from an ongoing collaboration in gastric\ncancer detection which demonstrates the power and interpretability of the\ntechnique. Our idea is simple but potentially useful in situations where the\ndata have some underlying structure.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 07:55:27 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Powers", "Scott", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1601.07999", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron, Etienne C\\^ome, Julien Jacques", "title": "The discriminative functional mixture model for a comparative analysis\n  of bike sharing systems", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS861 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1726-1760", "doi": "10.1214/15-AOAS861", "report-no": "IMS-AOAS-AOAS861", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bike sharing systems (BSSs) have become a means of sustainable intermodal\ntransport and are now proposed in many cities worldwide. Most BSSs also provide\nopen access to their data, particularly to real-time status reports on their\nbike stations. The analysis of the mass of data generated by such systems is of\nparticular interest to BSS providers to update system structures and policies.\nThis work was motivated by interest in analyzing and comparing several European\nBSSs to identify common operating patterns in BSSs and to propose practical\nsolutions to avoid potential issues. Our approach relies on the identification\nof common patterns between and within systems. To this end, a model-based\nclustering method, called FunFEM, for time series (or more generally functional\ndata) is developed. It is based on a functional mixture model that allows the\nclustering of the data in a discriminative functional subspace. This model\npresents the advantage in this context to be parsimonious and to allow the\nvisualization of the clustered systems. Numerical experiments confirm the good\nbehavior of FunFEM, particularly compared to state-of-the-art methods. The\napplication of FunFEM to BSS data from JCDecaux and the Transport for London\nInitiative allows us to identify 10 general patterns, including pathological\nones, and to propose practical improvement strategies based on the system\ncomparison. The visualization of the clustered data within the discriminative\nsubspace turns out to be particularly informative regarding the system\nefficiency. The proposed methodology is implemented in a package for the R\nsoftware, named funFEM, which is available on the CRAN. The package also\nprovides a subset of the data analyzed in this work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 08:39:11 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Bouveyron", "Charles", ""], ["C\u00f4me", "Etienne", ""], ["Jacques", "Julien", ""]]}, {"id": "1601.08034", "submitter": "Ramin Moghaddass", "authors": "Ramin Moghaddass, Cynthia Rudin", "title": "The latent state hazard model, with application to wind turbine\n  reliability", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS859 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1823-1863", "doi": "10.1214/15-AOAS859", "report-no": "IMS-AOAS-AOAS859", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for reliability analysis that is able to distinguish\nthe latent internal vulnerability state of the equipment from the vulnerability\ncaused by temporary external sources. Consider a wind farm where each turbine\nis running under the external effects of temperature, wind speed and direction,\netc. The turbine might fail because of the external effects of a spike in\ntemperature. If it does not fail during the temperature spike, it could still\nfail due to internal degradation, and the spike could cause (or be an\nindication of) this degradation. The ability to identify the underlying latent\nstate can help better understand the effects of external sources and thus lead\nto more robust decision-making. We present an experimental study using SCADA\nsensor measurements from wind turbines in Italy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 10:29:40 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Moghaddass", "Ramin", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1601.08090", "submitter": "Laina D. Mercer", "authors": "Laina D. Mercer, Jon Wakefield, Athena Pantazis, Angelina M. Lutambi,\n  Honorati Masanja, Samuel Clark", "title": "Space-time smoothing of complex survey data: Small area estimation for\n  child mortality", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS872 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1889-1905", "doi": "10.1214/15-AOAS872", "report-no": "IMS-AOAS-AOAS872", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people living in low- and middle-income countries are not covered by\ncivil registration and vital statistics systems. Consequently, a wide variety\nof other types of data, including many household sample surveys, are used to\nestimate health and population indicators. In this paper we combine data from\nsample surveys and demographic surveillance systems to produce small area\nestimates of child mortality through time. Small area estimates are necessary\nto understand geographical heterogeneity in health indicators when\nfull-coverage vital statistics are not available. For this endeavor\nspatio-temporal smoothing is beneficial to alleviate problems of data sparsity.\nThe use of conventional hierarchical models requires careful thought since the\nsurvey weights may need to be considered to alleviate bias due to nonrandom\nsampling and nonresponse. The application that motivated this work is an\nestimation of child mortality rates in five-year time intervals in regions of\nTanzania. Data come from Demographic and Health Surveys conducted over the\nperiod 1991-2010 and two demographic surveillance system sites. We derive a\nvariance estimator of under five years child mortality that accounts for the\ncomplex survey weighting. For our application, the hierarchical models we\nconsider include random effects for area, time and survey and we compare models\nusing a variety of measures including the conditional predictive ordinate\n(CPO). The method we propose is implemented via the fast and accurate\nintegrated nested Laplace approximation (INLA).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 13:14:18 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Mercer", "Laina D.", ""], ["Wakefield", "Jon", ""], ["Pantazis", "Athena", ""], ["Lutambi", "Angelina M.", ""], ["Masanja", "Honorati", ""], ["Clark", "Samuel", ""]]}, {"id": "1601.08097", "submitter": "T. R. Fanshawe", "authors": "T. R. Fanshawe, C. M. Chapman, T. Crick", "title": "Lymphangiogenesis and carcinoma in the uterine cervix: Joint and\n  hierarchical models for random cluster sizes and continuous outcomes", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS867 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1932-1949", "doi": "10.1214/15-AOAS867", "report-no": "IMS-AOAS-AOAS867", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the lymphatic system is clearly linked to the metastasis of most\nhuman carcinomas, the mechanisms by which lymphangiogenesis occurs in response\nto the presence of carcinoma remain unclear. Hierarchical models are presented\nto investigate the properties of lymphatic vessel production in 2997 fields\ntaken from 20 individuals with invasive carcinoma, 21 individuals with cervical\nintraepithelial neoplasia and 21 controls. Such data demonstrate a high degree\nof correlation within tumour samples from the same individual. Joint\nhierarchical models utilising shared random effects are discussed and fitted in\na Bayesian framework to allow for the correlation between two key outcome\nmeasures: a random cluster size (the number of lymphatic vessels in a tissue\nsample) and a continuous outcome (vessel size). Results show that invasive\ncarcinoma samples are associated with increased production of smaller and more\nirregularly-shaped lymphatic vessels and suggest a mechanistic link between\ncarcinoma of the cervix and lymphangiogenesis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 13:29:30 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Fanshawe", "T. R.", ""], ["Chapman", "C. M.", ""], ["Crick", "T.", ""]]}]