[{"id": "1406.0063", "submitter": "Chris Oates", "authors": "C. J. Oates, F. Dondelinger, N. Bayani, J. Korola, J. W. Gray, S.\n  Mukherjee", "title": "Causal network inference using biochemical kinetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are widely used as structural summaries of biochemical\nsystems. Statistical estimation of networks is usually based on linear or\ndiscrete models. However, the dynamics of these systems are generally\nnonlinear, suggesting that suitable nonlinear formulations may offer gains with\nrespect to network inference and associated prediction problems. We present a\ngeneral framework for both network inference and dynamical prediction that is\nrooted in nonlinear biochemical kinetics. This is done by considering a\ndynamical system based on a chemical reaction graph and associated kinetics\nparameters. Inference regarding both parameters and the reaction graph itself\nis carried out within a fully Bayesian framework. Prediction of dynamical\nbehavior is achieved by averaging over both parameters and reaction graphs,\nallowing prediction even when the underlying reactions themselves are unknown\nor uncertain. Results, based on (i) data simulated from a mechanistic model of\nmitogen-activated protein kinase signaling and (ii) phosphoproteomic data from\ncancer cell lines, demonstrate that nonlinear formulations can yield gains in\nnetwork inference and permit dynamical prediction in the challenging setting\nwhere the reaction graph is unknown.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 10:19:56 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Oates", "C. J.", ""], ["Dondelinger", "F.", ""], ["Bayani", "N.", ""], ["Korola", "J.", ""], ["Gray", "J. W.", ""], ["Mukherjee", "S.", ""]]}, {"id": "1406.0148", "submitter": "Serkan Hosten", "authors": "Javier Arsuaga, Ido Heskia, Serkan Hosten, Tatsiana Maskalevich", "title": "Uncovering Proximity of Chromosome Territories using Classical Algebraic\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.AC math.ST q-bio.GN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchange type chromosome aberrations (ETCAs) are rearrangements of the genome\nthat occur when chromosomes break and the resulting fragments rejoin with other\nfragments from other chromosomes. ETCAs are commonly observed in cancer cells\nand in cells exposed to radiation. The frequency of these chromosome\nrearrangements is correlated with their spatial proximity, therefore it can be\nused to infer the three dimensional organization of the genome. Extracting\nstatistical significance of spatial proximity from cancer and radiation data\nhas remained somewhat elusive because of the sparsity of the data. We here\npropose a new approach to study the three dimensional organization of the\ngenome using algebraic statistics. We test our method on a published data set\nof irradiated human blood lymphocyte cells. We provide a rigorous method for\ntesting the overall organization of the genome, and in agreement with previous\nresults we find a random relative positioning of chromosomes with the exception\nof the chromosome pairs \\{1,22\\} and \\{13,14\\} that have a significantly larger\nnumber of ETCAs than the rest of the chromosome pairs suggesting their spatial\nproximity. We conclude that algebraic methods can successfully be used to\nanalyze genetic data and have potential applications to larger and more complex\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 09:15:41 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Arsuaga", "Javier", ""], ["Heskia", "Ido", ""], ["Hosten", "Serkan", ""], ["Maskalevich", "Tatsiana", ""]]}, {"id": "1406.0189", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov and Nikolai Slavov", "title": "Convex Total Least Squares", "comments": "9 pages, 4 figures", "journal-ref": "JMLR W&CP 32 (1) :109-117, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 18:13:08 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1406.0193", "submitter": "Nikolai Slavov", "authors": "Nikolai Slavov", "title": "Inference of Sparse Networks with Unobserved Variables. Application to\n  Gene Regulatory Networks", "comments": "8 pages, 5 figures", "journal-ref": "JMLR W&CP 9:757-764, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.MN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a unifying framework for modeling complex systems and network\ninference problems are frequently encountered in many fields. Here, I develop\nand apply a generative approach to network inference (RCweb) for the case when\nthe network is sparse and the latent (not observed) variables affect the\nobserved ones. From all possible factor analysis (FA) decompositions explaining\nthe variance in the data, RCweb selects the FA decomposition that is consistent\nwith a sparse underlying network. The sparsity constraint is imposed by a novel\nmethod that significantly outperforms (in terms of accuracy, robustness to\nnoise, complexity scaling, and computational efficiency) Bayesian methods and\nMLE methods using l1 norm relaxation such as K-SVD and l1--based sparse\nprinciple component analysis (PCA). Results from simulated models demonstrate\nthat RCweb recovers exactly the model structures for sparsity as low (as\nnon-sparse) as 50% and with ratio of unobserved to observed variables as high\nas 2. RCweb is robust to noise, with gradual decrease in the parameter ranges\nas the noise level increases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 19:09:14 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Slavov", "Nikolai", ""]]}, {"id": "1406.0389", "submitter": "J.D. Opdyke", "authors": "J.D. Opdyke", "title": "Estimating Operational Risk Capital with Greater Accuracy, Precision,\n  and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest US banks are required by regulatory mandate to estimate the\noperational risk capital they must hold using an Advanced Measurement Approach\n(AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution\nApproach (LDA) which defines the aggregate loss distribution as the convolution\nof a frequency and a severity distribution representing the number and\nmagnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th\npercentile) estimate of this annual loss distribution. In practice, the\nseverity distribution drives the capital estimate, which is essentially a very\nhigh quantile of the estimated severity distribution. Unfortunately, because\nthe relevant severities are heavy-tailed AND the quantiles being estimated are\nso high, VaR always appears to be a convex function of the severity parameters,\ncausing all widely-used estimators to generate biased capital estimates\n(apparently) due to Jensen's Inequality. The observed capital inflation is\nsometimes enormous, even at the unit-of-measure (UoM) level (even billions\nUSD). Herein I present an estimator of capital that essentially eliminates this\nupward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with\nthe regulatory intent of the LDA framework than implementations that fail to\nmitigate this bias. RCE also notably increases the precision of the capital\nestimate and consistently increases its robustness to violations of the i.i.d.\ndata presumption (which are endemic to operational risk loss event data). So\nwith greater capital accuracy, precision, and robustness, RCE lowers capital\nrequirements at both the UoM and enterprise levels, increases capital stability\nfrom quarter to quarter, ceteris paribus, and does both while more accurately\nand precisely reflecting regulatory intent. RCE is straightforward to implement\nusing any major statistical software package.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 14:45:44 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 18:44:33 GMT"}, {"version": "v3", "created": "Sat, 5 Jul 2014 17:29:56 GMT"}, {"version": "v4", "created": "Sun, 26 Oct 2014 21:17:14 GMT"}, {"version": "v5", "created": "Sun, 16 Nov 2014 21:30:17 GMT"}, {"version": "v6", "created": "Thu, 27 Nov 2014 13:40:22 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Opdyke", "J. D.", ""]]}, {"id": "1406.0498", "submitter": "Sachin Malik", "authors": "Sachin Malik, Rajesh Singh, SB Gupta", "title": "An almost unbiased estimator for population mean using known value of\n  population parameter(s)", "comments": "arXiv admin note: text overlap with arXiv:1405.4182", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed an almost unbiased estimator using known value\nof some population parameter(s) with known population proportion of an\nauxiliary variable. A class of estimators is defined which includes [1], [2]\nand [3] estimators. Under simple random sampling without replacement (SRSWOR)\nscheme the expressions for bias and mean square error (MSE) are derived.\nNumerical illustrations are given in support of the present study. Key words:\nAuxiliary information, proportion, bias, mean square error, unbiased estimator.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 04:33:31 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Malik", "Sachin", ""], ["Singh", "Rajesh", ""], ["Gupta", "SB", ""]]}, {"id": "1406.0581", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Thomas C. M. Lee, Debashis Paul, Jie Peng, the\n  Alzheimer's Disease Neuroimaging Initiative", "title": "Fiber Direction Estimation, Smoothing and Tracking in Diffusion MRI", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging is an imaging technology designed to\nprobe anatomical architectures of biological samples in an in vivo and\nnon-invasive manner through measuring water diffusion. The contribution of this\npaper is threefold. First it proposes a new method to identify and estimate\nmultiple diffusion directions within a voxel through a new and identifiable\nparametrization of the widely used multi-tensor model. Unlike many existing\nmethods, this method focuses on the estimation of diffusion directions rather\nthan the diffusion tensors. Second, this paper proposes a novel direction\nsmoothing method which greatly improves direction estimation in regions with\ncrossing fibers. This smoothing method is shown to have excellent theoretical\nand empirical properties. Lastly, this paper develops a fiber tracking\nalgorithm that can handle multiple directions within a voxel. The overall\nmethodology is illustrated with simulated data and a data set collected for the\nstudy of Alzheimer's disease by the Alzheimer's Disease Neuroimaging Initiative\n(ADNI).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 06:03:48 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 02:43:54 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Lee", "Thomas C. M.", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1406.1245", "submitter": "Paul McNicholas", "authors": "Amay Cheam and Paul D. McNicholas", "title": "Modelling Receiver Operating Characteristic Curves Using Gaussian\n  Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.04.010", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic curve is widely applied in measuring\nthe performance of diagnostic tests. Many direct and indirect approaches have\nbeen proposed for modelling the ROC curve, and because of its tractability, the\nGaussian distribution has typically been used to model both populations. We\npropose using a Gaussian mixture model, leading to a more flexible approach\nthat better accounts for atypical data. Monte Carlo simulation is used to\ncircumvent the issue of absence of a closed-form. We show that our method\nperforms favourably when compared to the crude binormal curve and to the\nsemi-parametric frequentist binormal ROC using the famous LABROC procedure.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:12:23 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Cheam", "Amay", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1406.1340", "submitter": "Raj Thilak Rajan", "authors": "Raj Thilak Rajan and Alle-Jan van der Veen", "title": "Joint ranging and synchronization for an anchorless network of mobile\n  nodes", "comments": "In submission", "journal-ref": null, "doi": "10.1109/TSP.2015.2391076", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization and localization are critical challenges for the coherent\nfunctioning of a wireless network, which are conventionally solved\nindependently. Recently, various estimators have been proposed for pairwise\nsynchronization between immobile nodes, based on time stamp exchanges via\ntwo-way communication. In this paper, we consider a \\textit{network of mobile\nnodes} for which a novel joint time-range model is presented, treating both\nunsynchronized clocks and the pairwise distances as a polynomial function of\n\\textit{true} time. For a set of nodes, a pairwise least squares solution is\nproposed for estimating the pairwise range parameters between the nodes, in\naddition to estimating the clock offsets and clock skews. Extending these\npairwise solutions to network-wide ranging and clock synchronization, we\npresent a central data fusion based global least squares algorithm. A unique\nsolution is non-existent without a constraint on the cost function (\\eg clock\nreference node). Ergo, a constrained framework is proposed and a new\nConstrained \\Cramer\\ Rao Bound (CCRB) is derived for the joint time-range\nmodel. In addition, various constraints are proposed and their effects on the\nproposed algorithms are studied. Simulations are conducted and the proposed\nalgorithm is shown to approach the theoretical limits.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 11:23:46 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Rajan", "Raj Thilak", ""], ["van der Veen", "Alle-Jan", ""]]}, {"id": "1406.1651", "submitter": "Stefano Andreon", "authors": "S. Andreon, P. Congdon", "title": "The insignificant evolution of the richness-mass relation of galaxy\n  clusters", "comments": "A&A, in press. Fixed pdf generation problem", "journal-ref": "A&A 568, A23 (2014)", "doi": "10.1051/0004-6361/201423616", "report-no": null, "categories": "astro-ph.CO astro-ph.GA astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analysed the richness--mass scaling of 23 very massive clusters at\n$0.15<z<0.55$ with homogenously measured weak-lensing masses and richnesses\nwithin a fixed aperture of $0.5$ Mpc radius. We found that the richness--mass\nscaling is very tight (the scatter is $<0.09$ dex with 90 \\% probability) and\nindependent of cluster evolutionary status and morphology. This implies a close\nassociation between infall and evolution of dark matter and galaxies in the\ncentral region of clusters. We also found that the evolution of the\nrichness-mass intercept is minor at most, and, given the minor mass evolution\nacross the studied redshift range, the richness evolution of individual massive\nclusters also turns out to be very small. Finally, it was paramount to account\nfor the cluster mass function and the selection function. Ignoring them would\nled to biases larger than the (otherwise quoted) errors. Our study benefits\nfrom: a) weak-lensing masses instead of proxy-based masses thereby removing the\nambiguity between a real trend and one induced by an accounted evolution of the\nused mass proxy; b) the use of projected masses that simplify the statistical\nanalysis thereby not requiring consideration of the unknown covariance induced\nby the cluster orientation/triaxiality; c) the use of aperture masses as they\nare free of the pseudo-evolution of mass definitions anchored to the evolving\ndensity of the Universe; d) a proper accounting of the sample selection\nfunction and of the Malmquist-like effect induced by the cluster mass function;\ne) cosmological simulations for the computation of the cluster mass function,\nits evolution, and the mass growth of each individual cluster.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 11:40:16 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 10:08:59 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Andreon", "S.", ""], ["Congdon", "P.", ""]]}, {"id": "1406.1761", "submitter": "Vivek Goyal", "authors": "Dongeek Shin, Ahmed Kirmani, Vivek K Goyal, and Jeffrey H. Shapiro", "title": "Photon-Efficient Computational 3D and Reflectivity Imaging with\n  Single-Photon Detectors", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing depth and reflectivity images at low light levels from active\nillumination of a scene has wide-ranging applications. Conventionally, even\nwith single-photon detectors, hundreds of photon detections are needed at each\npixel to mitigate Poisson noise. We develop a robust method for estimating\ndepth and reflectivity using on the order of 1 detected photon per pixel\naveraged over the scene. Our computational imager combines physically accurate\nsingle-photon counting statistics with exploitation of the spatial correlations\npresent in real-world reflectivity and 3D structure. Experiments conducted in\nthe presence of strong background light demonstrate that our computational\nimager is able to accurately recover scene depth and reflectivity, while\ntraditional maximum-likelihood based imaging methods lead to estimates that are\nhighly noisy. Our framework increases photon efficiency 100-fold over\ntraditional processing and also improves, somewhat, upon first-photon imaging\nunder a total acquisition time constraint in raster-scanned operation. Thus our\nnew imager will be useful for rapid, low-power, and noise-tolerant active\noptical imaging, and its fixed dwell time will facilitate parallelization\nthrough use of a detector array.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 18:06:39 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Shin", "Dongeek", ""], ["Kirmani", "Ahmed", ""], ["Goyal", "Vivek K", ""], ["Shapiro", "Jeffrey H.", ""]]}, {"id": "1406.1845", "submitter": "Lucas Mentch", "authors": "Lucas Mentch and Giles Hooker", "title": "Formal Hypothesis Tests for Additive Structure in Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistical learning methods have proved powerful tools for predictive\nmodeling, the black-box nature of the models they produce can severely limit\ntheir interpretability and the ability to conduct formal inference. However,\nthe natural structure of ensemble learners like bagged trees and random forests\nhas been shown to admit desirable asymptotic properties when base learners are\nbuilt with proper subsamples. In this work, we demonstrate that by defining an\nappropriate grid structure on the covariate space, we may carry out formal\nhypothesis tests for both variable importance and underlying additive model\nstructure. To our knowledge, these tests represent the first statistical tools\nfor investigating the underlying regression structure in a context such as\nrandom forests. We develop notions of total and partial additivity and further\ndemonstrate that testing can be carried out at no additional computational cost\nby estimating the variance within the process of constructing the ensemble.\nFurthermore, we propose a novel extension of these testing procedures utilizing\nrandom projections in order to allow for computationally efficient testing\nprocedures that retain high power even when the grid size is much larger than\nthat of the training set.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 00:58:30 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 22:55:32 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 21:10:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1406.1901", "submitter": "Fabrizio Lecci", "authors": "Fr\\'ed\\'eric Chazal, Brittany Terese Fasy, Fabrizio Lecci, Bertrand\n  Michel, Alessandro Rinaldo, Larry Wasserman", "title": "Subsampling Methods for Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology is a multiscale method for analyzing the shape of sets\nand functions from point cloud data arising from an unknown distribution\nsupported on those sets. When the size of the sample is large, direct\ncomputation of the persistent homology is prohibitive due to the combinatorial\nnature of the existing algorithms. We propose to compute the persistent\nhomology of several subsamples of the data and then combine the resulting\nestimates. We study the risk of two estimators and we prove that the\nsubsampling approach carries stable topological information while achieving a\ngreat reduction in computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 16:06:26 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Fasy", "Brittany Terese", ""], ["Lecci", "Fabrizio", ""], ["Michel", "Bertrand", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.2082", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas and Ryan J. Tibshirani", "title": "Fast and Flexible ADMM Algorithms for Trend Filtering", "comments": "22 pages, 10 figures; published in Journal of Computational and\n  Graphical Statistics, 2015", "journal-ref": null, "doi": "10.1080/10618600.2015.1054033", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast and robust algorithm for trend filtering, a\nrecently developed nonparametric regression tool. It has been shown that, for\nestimating functions whose derivatives are of bounded variation, trend\nfiltering achieves the minimax optimal error rate, while other popular methods\nlike smoothing splines and kernels do not. Standing in the way of a more\nwidespread practical adoption, however, is a lack of scalable and numerically\nstable algorithms for fitting trend filtering estimates. This paper presents a\nhighly efficient, specialized ADMM routine for trend filtering. Our algorithm\nis competitive with the specialized interior point methods that are currently\nin use, and yet is far more numerically robust. Furthermore, the proposed ADMM\nimplementation is very simple, and importantly, it is flexible enough to extend\nto many interesting related problems, such as sparse trend filtering and\nisotonic trend filtering. Software for our method is freely available, in both\nthe C and R languages.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:50:20 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 19:24:51 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 21:09:02 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2015 00:46:34 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1406.2280", "submitter": "Gelu M. Nita", "authors": "Gelu M. Nita, Gregory D. Fleishman, Dale E. Gary, William Marin, and\n  Kristine Boone", "title": "Fitting FFT-derived Spectra: Theory, Tool, and Application to Solar\n  Radio Spike Decomposition", "comments": "Accepted to ApJ, 57 pages, 16 figures", "journal-ref": null, "doi": "10.1088/0004-637X/789/2/152", "report-no": null, "categories": "astro-ph.SR math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectra derived from fast Fourier transform (FFT) analysis of time-domain\ndata intrinsically contain statistical fluctuations whose distribution depends\non the number of accumulated spectra contributing to a measurement. The tail of\nthis distribution, which is essential for separation of the true signal from\nthe statistical fluctuations, deviates noticeably from the normal distribution\nfor a finite number of the accumulations. In this paper we develop a theory to\nproperly account for the statistical fluctuations when fitting a model to a\ngiven accumulated spectrum. The method is implemented in software for the\npurpose of automatically fitting a large body of such FFT-derived spectra. We\napply this tool to analyze a portion of a dense cluster of spikes recorded by\nour FST instrument during a record-breaking event that occurred on 06 Dec 2006.\nThe outcome of this analysis is briefly discussed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 18:54:38 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Nita", "Gelu M.", ""], ["Fleishman", "Gregory D.", ""], ["Gary", "Dale E.", ""], ["Marin", "William", ""], ["Boone", "Kristine", ""]]}, {"id": "1406.2518", "submitter": "Romain Campigotto", "authors": "Romain Campigotto, Patricia Conde C\\'espedes and Jean-Loup Guillaume", "title": "A Generalized and Adaptive Method for Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks represent interactions between entities. They appear in\nvarious contexts such as sociology, biology, etc., and they generally contain\nhighly connected subgroups called communities. Community detection is a\nwell-studied problem and most of the algorithms aim to maximize the\nNewman-Girvan modularity function, the most popular being the Louvain method\n(it is well-suited on very large graphs). However, the classical modularity has\nmany drawbacks: we can find partitions of high quality in graphs without\ncommunity structure, e.g., on random graphs; it promotes large communities.\nThen, we have adapted the Louvain method to other quality functions. In this\npaper, we describe a generic version of the Louvain method. In particular, we\ngive a sufficient condition to plug a quality function into it. We also show\nthat global performance of this new version is similar to the classical Louvain\nalgorithm, that promotes it to the best rank of the community detection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 12:06:42 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Campigotto", "Romain", ""], ["C\u00e9spedes", "Patricia Conde", ""], ["Guillaume", "Jean-Loup", ""]]}, {"id": "1406.2558", "submitter": "Ryuji Uozumi", "authors": "Ryuji Uozumi and Chikuma Hamada", "title": "Interim decision-making strategies in adaptive designs for population\n  selection considering post-progression survival magnitudes", "comments": "This paper has been withdrawn because of the insufficiency of\n  numerical experiments in Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The development of targeted therapies, which benefit only a subgroup of\npatients treated for a given type of cancer, has been extremely attractive to\nmany investigators. Adaptive seamless phase II/III designs in oncology clinical\ntrials with interim analyses for subpopulation selection could be used if\npre-defined biomarker hypothesis exists. We consider the interim analysis using\ntime-to-event endpoints, e.g., overall survival (OS) and progression-free\nsurvival (PFS), to identify whether the whole population or only the\nbiomarker-positive population should be continued into the subsequent stage,\nwhereas a final decision is based on OS data. In this paper, we propose the\ninterim decision-making strategies in adaptive designs with correlated\nendpoints, considering post-progression survival (PPS) magnitudes. In our\napproach, the interim decision is made on the basis of predictive power, by\nincorporating information on OS as well as PFS. We consider PFS data only in\nmaking interim decision in order to supplement the immature OS data. Simulation\nstudies assuming a targeted therapy show that our interim decision procedure\nperforms well in terms of selecting the proper population, especially under a\nscenario in which PPS affects the translation of the benefit from PFS to OS.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 14:01:17 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 06:14:55 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Uozumi", "Ryuji", ""], ["Hamada", "Chikuma", ""]]}, {"id": "1406.3064", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski, Andrzej Buda", "title": "Hierarchical representation of socio-economic complex systems according\n  to minimal sapnning trees", "comments": "Biotech Conference, Gdansk 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate hierarchical structure in various complex systems according to\nMinimum Spanning Tree methods. Firstly, we investigate stock markets where the\ngraphis obtained from the matrix of correlations coefficient computed between\nall pairs of assets by considering the synchronous time evolution of the\ndifference of the logarithm of daily stock price. The hierarchical tree\nprovides information useful to investigate the number and nature of economic\nfactors that have associated a meaningful economic taxonomy. We continue to use\nthis method in social systems (sport, political parties and pharmacy) to\ninvestigate collective effects and detect how single element of the system\ninfluences on the other ones. The level of correlations and Minimum Spanning\nTrees in various complex systems is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 20:54:17 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Jarynowski", "Andrzej", ""], ["Buda", "Andrzej", ""]]}, {"id": "1406.3089", "submitter": "Fernando Antoneli Jr", "authors": "Guilherme C.P. Innocentini, Michael Forger, Ovidiu Radulescu and\n  Fernando Antoneli", "title": "Protein synthesis driven by dynamical stochastic transcription", "comments": "20 pages, 3 figures", "journal-ref": "Bulletin of Mathematical Biology, January 2016, Volume 78, Issue\n  1, pp 110-131", "doi": "10.1007/s11538-015-0131-3", "report-no": null, "categories": "q-bio.SC math.PR physics.bio-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we propose a mathematical framework to couple\ntranscription and translation in which mRNA production is described by a set of\nmaster equations while the dynamics of protein density is governed by a random\ndifferential equation. The coupling between the two processes is given by a\nstochastic perturbation whose statistics satisfies the master equations. In\nthis approach, from the knowledge of the analytical time dependent distribution\nof mRNA number, we are able to calculate the dynamics of the probability\ndensity of the protein population.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 00:34:05 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 15:40:45 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2015 17:48:17 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Innocentini", "Guilherme C. P.", ""], ["Forger", "Michael", ""], ["Radulescu", "Ovidiu", ""], ["Antoneli", "Fernando", ""]]}, {"id": "1406.3106", "submitter": "Deepesh Bhati Mr.", "authors": "Deepesh Bhati and Mohd. Aamir Malik", "title": "On Lindley-Exponential Distribution: Properties and Application", "comments": "17 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we introduce a new distribution generated by Lindley random\nvariable which offers a more flexible model for modelling lifetime data.\nVarious statistical properties like distribution function, survival function,\nmoments, entropy, and limiting distribution of extreme order statistics are\nestablished. Inference for a random sample from the proposed distribution is\ninvestigated and maximum likelihood estimation method is used for estimating\nparameters of this distribution. The applicability of the proposed distribution\nis shown through real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 02:39:09 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Bhati", "Deepesh", ""], ["Malik", "Mohd. Aamir", ""]]}, {"id": "1406.3258", "submitter": "Nancy Zhang", "authors": "Nancy R. Zhang, Benjamin Yakir, Charlie L. Xia, David Siegmund", "title": "Scanning a Poisson Random Field for Local Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of local genomic signals using high-throughput DNA sequencing\ndata can be cast as a problem of scanning a Poisson random field for local\nchanges in the rate of the process. We propose a likelihood-based framework for\nfor such scans, and derive formulas for false positive rate control and power\ncalculations. The framework can also accommodate mixtures of Poisson processes\nto deal with over-dispersion. As a specific, detailed example, we consider the\ndetection of insertions and deletions by paired-end DNA-sequencing. We propose\nseveral statistics for this problem, compare their power under current\nexperimental designs, and illustrate their application on an Illumina Platinum\nGenomes data set.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 14:53:47 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Zhang", "Nancy R.", ""], ["Yakir", "Benjamin", ""], ["Xia", "Charlie L.", ""], ["Siegmund", "David", ""]]}, {"id": "1406.3402", "submitter": "Steven Miller", "authors": "Victor Luo and Steven J. Miller", "title": "Relieving and Readjusting Pythagoras", "comments": "Version 1.1, 15 pages, 9 figures (correct some minor typos and two\n  images)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bill James invented the Pythagorean expectation in the late 70's to predict a\nbaseball team's winning percentage knowing just their runs scored and allowed.\nHis original formula estimates a winning percentage of ${\\rm RS}^2/({\\rm\nRS}^2+{\\rm RA}^2)$, where ${\\rm RS}$ stands for runs scored and ${\\rm RA}$ for\nruns allowed; later versions found better agreement with data by replacing the\nexponent 2 with numbers near 1.83. Miller and his colleagues provided a\ntheoretical justification by modeling runs scored and allowed by independent\nWeibull distributions. They showed that a single Weibull distribution did a\nvery good job of describing runs scored and allowed, and led to a predicted\nwon-loss percentage of $({\\rm RS_{\\rm obs}}-1/2)^\\gamma / (({\\rm RS_{\\rm\nobs}}-1/2)^\\gamma + ({\\rm RA_{\\rm obs}}-1/2)^\\gamma)$, where ${\\rm RS_{\\rm\nobs}}$ and ${\\rm RA_{\\rm obs}}$ are the observed runs scored and allowed and\n$\\gamma$ is the shape parameter of the Weibull (typically close to 1.8). We\nshow a linear combination of Weibulls more accurately determines a team's run\nproduction and increases the prediction accuracy of a team's winning percentage\nby an average of about 25% (thus while the currently used variants of the\noriginal predictor are accurate to about four games a season, the new\ncombination is accurate to about three). The new formula is more involved\ncomputationally; however, it can be easily computed on a laptop in a matter of\nminutes from publicly available season data. It performs as well (or slightly\nbetter) than the related Pythagorean formulas in use, and has the additional\nadvantage of having a theoretical justification for its parameter values (and\nnot just an optimization of parameters to minimize prediction error).\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 01:28:05 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 03:41:08 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Luo", "Victor", ""], ["Miller", "Steven J.", ""]]}, {"id": "1406.3496", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Jo\\~ao Gama", "title": "EigenEvent: An Algorithm for Event Detection from Complex Data Streams\n  in Syndromic Surveillance", "comments": "To appear in Intelligent Data Analysis Journal, vol. 19(3), 2015", "journal-ref": "PP. 597-616, Vol. 19, No. 3, June 2015, Intelligent Data Analysis", "doi": "10.3233/IDA-150734", "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syndromic surveillance systems continuously monitor multiple pre-diagnostic\ndaily streams of indicators from different regions with the aim of early\ndetection of disease outbreaks. The main objective of these systems is to\ndetect outbreaks hours or days before the clinical and laboratory confirmation.\nThe type of data that is being generated via these systems is usually\nmultivariate and seasonal with spatial and temporal dimensions. The algorithm\nWhat's Strange About Recent Events (WSARE) is the state-of-the-art method for\nsuch problems. It exhaustively searches for contrast sets in the multivariate\ndata and signals an alarm when find statistically significant rules. This\nbottom-up approach presents a much lower detection delay comparing the existing\ntop-down approaches. However, WSARE is very sensitive to the small-scale\nchanges and subsequently comes with a relatively high rate of false alarms. We\npropose a new approach called EigenEvent that is neither fully top-down nor\nbottom-up. In this method, we instead of top-down or bottom-up search, track\nchanges in data correlation structure via eigenspace techniques. This new\nmethodology enables us to detect both overall changes (via eigenvalue) and\ndimension-level changes (via eigenvectors). Experimental results on hundred\nsets of benchmark data reveals that EigenEvent presents a better overall\nperformance comparing state-of-the-art, in particular in terms of the false\nalarm rate.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:38:09 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1406.3506", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Jo\\~ao Gama", "title": "Eigenspace Method for Spatiotemporal Hotspot Detection", "comments": "To appear in Expert Systems Journal", "journal-ref": null, "doi": "10.1111/exsy.12088", "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hotspot detection aims at identifying subgroups in the observations that are\nunexpected, with respect to the some baseline information. For instance, in\ndisease surveillance, the purpose is to detect sub-regions in spatiotemporal\nspace, where the count of reported diseases (e.g. Cancer) is higher than\nexpected, with respect to the population. The state-of-the-art method for this\nkind of problem is the Space-Time Scan Statistics (STScan), which exhaustively\nsearch the whole space through a sliding window looking for significant\nspatiotemporal clusters. STScan makes some restrictive assumptions about the\ndistribution of data, the shape of the hotspots and the quality of data, which\ncan be unrealistic for some nontraditional data sources. A novel methodology\ncalled EigenSpot is proposed where instead of an exhaustive search over the\nspace, tracks the changes in a space-time correlation structure. Not only does\nthe new approach presents much more computational efficiency, but also makes no\nassumption about the data distribution, hotspot shape or the data quality. The\nprincipal idea is that with the joint combination of abnormal elements in the\nprincipal spatial and the temporal singular vectors, the location of hotspots\nin the spatiotemporal space can be approximated. A comprehensive experimental\nevaluation, both on simulated and real data sets reveals the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 11:26:13 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1406.3792", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based\n  Support Vector Regression Modeling Framework", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijepes.2014.06.010", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly accurate interval forecasting of electricity demand is fundamental to\nthe success of reducing the risk when making power system planning and\noperational decisions by providing a range rather than point estimation. In\nthis study, a novel modeling framework integrating bivariate empirical mode\ndecomposition (BEMD) and support vector regression (SVR), extended from the\nwell-established empirical mode decomposition (EMD) based time series modeling\nframework in the energy demand forecasting literature, is proposed for interval\nforecasting of electricity demand. The novelty of this study arises from the\nemployment of BEMD, a new extension of classical empirical model decomposition\n(EMD) destined to handle bivariate time series treated as complex-valued time\nseries, as decomposition method instead of classical EMD only capable of\ndecomposing one-dimensional single-valued time series. This proposed modeling\nframework is endowed with BEMD to decompose simultaneously both the lower and\nupper bounds time series, constructed in forms of complex-valued time series,\nof electricity demand on a monthly per hour basis, resulting in capturing the\npotential interrelationship between lower and upper bounds. The proposed\nmodeling framework is justified with monthly interval-valued electricity demand\ndata per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicating\nit as a promising method for interval-valued electricity demand forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 02:39:37 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1406.3863", "submitter": "Gabriela B. Cybis", "authors": "Gabriela B. Cybis, Janet S. Sinsheimer, Trevor Bedford, Alison E.\n  Mather, Philippe Lemey, Marc A. Suchard", "title": "Assessing phenotypic correlation through the multivariate phylogenetic\n  latent liability model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS821 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 969-991", "doi": "10.1214/15-AOAS821", "report-no": "IMS-AOAS-AOAS821", "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding which phenotypic traits are consistently correlated throughout\nevolution is a highly pertinent problem in modern evolutionary biology. Here,\nwe propose a multivariate phylogenetic latent liability model for assessing the\ncorrelation between multiple types of data, while simultaneously controlling\nfor their unknown shared evolutionary history informed through molecular\nsequences. The latent formulation enables us to consider in a single model\ncombinations of continuous traits, discrete binary traits and discrete traits\nwith multiple ordered and unordered states. Previous approaches have\nentertained a single data type generally along a fixed history, precluding\nestimation of correlation between traits and ignoring uncertainty in the\nhistory. We implement our model in a Bayesian phylogenetic framework, and\ndiscuss inference techniques for hypothesis testing. Finally, we showcase the\nmethod through applications to columbine flower morphology, antibiotic\nresistance in Salmonella and epitope evolution in influenza.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 21:38:01 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 10:27:12 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Cybis", "Gabriela B.", ""], ["Sinsheimer", "Janet S.", ""], ["Bedford", "Trevor", ""], ["Mather", "Alison E.", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1406.4068", "submitter": "Jeffrey Morris", "authors": "Jeffrey S. Morris", "title": "Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) involves the analysis of data whose ideal\nunits of observation are functions defined on some continuous domain, and the\nobserved data consist of a sample of functions taken from some population,\nsampled on a discrete grid. Ramsay and Silverman's 1997 textbook sparked the\ndevelopment of this field, which has accelerated in the past 10 years to become\none of the fastest growing areas of statistics, fueled by the growing number of\napplications yielding this type of data. One unique characteristic of FDA is\nthe need to combine information both across and within functions, which Ramsay\nand Silverman called replication and regularization, respectively. This article\nwill focus on functional regression, the area of FDA that has received the most\nattention in applications and methodological development. First will be an\nintroduction to basis functions, key building blocks for regularization in\nfunctional regression methods, followed by an overview of functional regression\nmethods, split into three types: [1] functional predictor regression\n(scalar-on-function), [2] functional response regression (function-on-scalar)\nand [3] function-on-function regression. For each, the role of replication and\nregularization will be discussed and the methodological development described\nin a roughly chronological manner, at times deviating from the historical\ntimeline to group together similar methods. The primary focus is on modeling\nand methodology, highlighting the modeling structures that have been developed\nand the various regularization approaches employed. At the end is a brief\ndiscussion describing potential areas of future development in this field.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 16:56:35 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Morris", "Jeffrey S.", ""]]}, {"id": "1406.4287", "submitter": "Marko Robnik-\\v{S}ikonja", "authors": "Andreja \\v{C}ufar, Ale\\v{s} Mrhar, Marko Robnik-\\v{S}ikonja", "title": "Identifying roles of clinical pharmacy with survey evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survey data sets are important sources of data and their successful\nexploitation is of key importance for informed policy-decision making. We\npresent how a survey analysis approach initially developed for customer\nsatisfaction research in marketing can be adapted for the introduction of\nclinical pharmacy services into hospital. We use two analytical approaches to\nextract relevant managerial consequences. With OrdEval algorithm we first\nevaluate the importance of competences for the users of clinical pharmacy and\nextract their nature according to the users expectations. Next, we build a\nmodel for predicting a successful introduction of clinical pharmacy to the\nclinical departments. We the wards with the highest probability of successful\ncooperation with a clinical pharmacist. We obtain useful managerially relevant\ninformation from a relatively small sample of highly relevant respondents. We\nshow how the OrdEval algorithm exploits the information hidden in the ordering\nof class and attribute values and their inherent correlation. Its output can be\neffectively visualized and complemented with confidence intervals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 09:29:00 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["\u010cufar", "Andreja", ""], ["Mrhar", "Ale\u0161", ""], ["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "1406.4386", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Visualising rate of change: application to age-specific fertility", "comments": "17 pages, 5 figures, To appear in Journal of the Royal Statistical\n  Society: Series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisation methods help in the discovery of characteristics that might not\nhave been apparent using mathematical models and summary statistics. However,\nvisualisation methods have not received much attention in demography, with the\nexceptions of scatter plot and Lexis surface. We utilise a phase-plane plot to\nvisualise the rate of change, obtained from derivatives of a continuous\nfunction. The phase-plane plot bears a resemblance to hysteresis loops,\nisogrowth curves, and solutions to differential equations. Using Australian and\nChilean fertility, we present phase-plane plots. Similarly to the scatter plot\nand Lexis surface, the phase-plane plot identifies the age with maximum\nfertility rate and displays skewness of fertility distribution. Unlike the\nscatter plot and Lexis surface, the phase-plane plot identifies the age with\nmaximum positive or negative velocity (i.e., trend), can compare the magnitude\nof the rate of change between any two years based on the size of the radius of\ncircles. The phase-plane plot allows the visualisation of dynamic changes in\nfertility for a given age over the years and is potentially useful for\nvisualising dynamic changes in birth-cohort fertility. Via the animate package\nin LaTeX, a dynamic phase-plane plot is also proposed to visualise changes in\nfertility over age or year.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 14:39:32 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 01:36:57 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 00:26:37 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1406.4592", "submitter": "Flora Alarcon", "authors": "Flora Alarcon, Vittorio Perduca and Gregory Nuel", "title": "Non-subjective power analysis to detect G*E interactions in Genome-Wide\n  Association Studies in presence of confounding factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally acknowledged that most complex diseases are affected in part\nby interactions between genes and genes and/or between genes and environmental\nfactors. Taking into account environmental exposures and their interactions\nwith genetic factors in genome-wide association studies (GWAS) can help to\nidentify high-risk subgroups in the population and provide a better\nunderstanding of the disease. For this reason, many methods have been developed\nto detect gene-environment (G*E) interactions. Despite this, few loci that\ninteract with environmental exposures have been identified so far. Indeed, the\nmodest effect of G*E interactions as well as confounding factors entail low\nstatistical power to detect such interactions. In this work, we provide a\nsimulated dataset in order to study methods for detecting G*E interactions in\nGWAS in presence of confounding factor and population structure. Our work\napplies a recently introduced non-subjective method for H1 simulations called\nwaffect and exploits the publicly available HapMap project to build a datasets\nwith real genotypes and population structures. We use this dataset to study the\nimpact of confounding factors and compare the relative performance of popular\nmethods such as PLINK, random forests and linear mixed models to detect G*E\ninteractions. Presence of confounding factor is an obstacle to detect G*E\ninteractions in GWAS and the approaches considered in our power study all have\ninsufficient power to detect the strong simulated interaction. Our simulated\ndataset could help to develop new methods which account for confounding factors\nthrough latent exposures in order to improve power.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 04:43:56 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Alarcon", "Flora", ""], ["Perduca", "Vittorio", ""], ["Nuel", "Gregory", ""]]}, {"id": "1406.5071", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Unsupervised Unmixing of Hyperspectral Images Accounting for Endmember\n  Variability", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2471182", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised Bayesian algorithm for hyperspectral\nimage unmixing accounting for endmember variability. The pixels are modeled by\na linear combination of endmembers weighted by their corresponding abundances.\nHowever, the endmembers are assumed random to take into account their\nvariability in the image. An additive noise is also considered in the proposed\nmodel generalizing the normal compositional model. The proposed algorithm\nexploits the whole image to provide spectral and spatial information. It\nestimates both the mean and the covariance matrix of each endmember in the\nimage. This allows the behavior of each material to be analyzed and its\nvariability to be quantified in the scene. A spatial segmentation is also\nobtained based on the estimated abundances. In order to estimate the parameters\nassociated with the proposed Bayesian model, we propose to use a Hamiltonian\nMonte Carlo algorithm. The performance of the resulting unmixing strategy is\nevaluated via simulations conducted on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 15:06:02 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1406.5083", "submitter": "Faustino Prieto", "authors": "Jos\\'e Mar\\'ia Sarabia, Faustino Prieto, Vanesa Jord\\'a", "title": "A variation of the Dragulescu-Yakovenko income model", "comments": "This is a preprint (7 pages, 4 tables, 2 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Dragulescu-Yakovenko (2000) model, we show that\nempirical income distribution with truncated datasets, cannot be properly\nmodeled by the one-parameter exponential distribution. However, a truncated\nversion characterized by an exponential distribution with two parameters gives\nan accurate fit.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 15:38:20 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Sarabia", "Jos\u00e9 Mar\u00eda", ""], ["Prieto", "Faustino", ""], ["Jord\u00e1", "Vanesa", ""]]}, {"id": "1406.5231", "submitter": "Albert Oh", "authors": "Jonathan M. Nichols, Albert K. Oh, Rebecca M. Willett", "title": "Reducing Basis Mismatch in Harmonic Signal Recovery via Alternating\n  Convex Search", "comments": "18 pages, 5 figures, IEEE Signal Processing Letters (Aug. 2014), in\n  press", "journal-ref": null, "doi": "10.1109/LSP.2014.2322444", "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory behind compressive sampling pre-supposes that a given sequence of\nobservations may be exactly represented by a linear combination of a small\nnumber of basis vectors. In practice, however, even small deviations from an\nexact signal model can result in dramatic increases in estimation error; this\nis the so-called \"basis mismatch\" problem. This work provides one possible\nsolution to this problem in the form of an iterative, biconvex search\nalgorithm. The approach uses standard $\\ell_1$-minimization to find the signal\nmodel coefficients followed by a maximum likelihood estimate of the signal\nmodel. The algorithm is illustrated on harmonic signals of varying sparsity and\noutperforms the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 22:57:55 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 19:26:00 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Nichols", "Jonathan M.", ""], ["Oh", "Albert K.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1406.5540", "submitter": "Philip Dawid", "authors": "A. Philip Dawid", "title": "On Individual Risk", "comments": "31 pages", "journal-ref": "Synthese 194 (2017(, 3445-3474", "doi": "10.1007/s11229-015-0953-4", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey a variety of possible explications of the term \"Individual Risk.\"\nThese in turn are based on a variety of interpretations of \"Probability,\"\nincluding Classical, Enumerative, Frequency, Formal, Metaphysical, Personal,\nPropensity, Chance and Logical conceptions of Probability, which we review and\ncompare. We distinguish between \"groupist\" and \"individualist\" understandings\nof Probability, and explore both \"group to individual\" (G2i) and \"individual to\ngroup\" (i2G) approaches to characterising Individual Risk. Although in the end\nthat concept remains subtle and elusive, some pragmatic suggestions for\nprogress are made.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 21:07:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. Philip", ""]]}, {"id": "1406.5556", "submitter": "Raja Manish", "authors": "Raja Manish", "title": "Linear and Non-linear Estimation Techniques: Theory and Comparison", "comments": "17 Pages, Matlab algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide application of estimation techniques in system analysis enable us to\nbest determine and understand the history of system states. This paper attempts\nto delineate the theory behind linear and non-linear estimation with a suitable\nexample for the comparison of some of the techniques of non-linear estimation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 23:34:36 GMT"}, {"version": "v2", "created": "Sat, 18 Oct 2014 22:19:19 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Manish", "Raja", ""]]}, {"id": "1406.5799", "submitter": "Alexander Franks", "authors": "Alexander Franks, Florian Markowetz and Edoardo Airoldi", "title": "Estimating cellular pathways from an ensemble of heterogeneous data\n  sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building better models of cellular pathways is one of the major challenges of\nsystems biology and functional genomics. There is a need for methods to build\non established expert knowledge and reconcile it with results of\nhigh-throughput studies. Moreover, the available data sources are heterogeneous\nand need to be combined in a way specific for the part of the pathway in which\nthey are most informative. Here, we present a compartment specific strategy to\nintegrate edge, node and path data for the refinement of a network hypothesis.\nSpecifically, we use a local-move Gibbs sampler for refining pathway hypotheses\nfrom a compendium of heterogeneous data sources, including novel methodology\nfor integrating protein attributes. We demonstrate the utility of this approach\nin a case study of the pheromone response MAPK pathway in the yeast S.\ncerevisiae.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:45:49 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Franks", "Alexander", ""], ["Markowetz", "Florian", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1406.5854", "submitter": "Peder Bacher pbac", "authors": "Lisa Buth Rasmussen, Peder Bacher, Henrik Madsen, Henrik Aalborg\n  Nielsen, Christian Heerup, Torben Green", "title": "Load Forecasting of Supermarket Refrigeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study of models for forecasting the electrical load for\nsupermarket refrigeration. The data used for building the models consists of\nload measurements, local climate measurements and weather forecasts. The load\nmeasurements are from a supermarket located in a village in Denmark. Every hour\nthe hourly electrical load for refrigeration is forecasted for the following 42\nhours. The forecast models are adaptive linear time series models. The model\nhas two regimes; one for opening hours and one for closing hours, this is\nmodelled by a regime switching model and two different methods for predicting\nthe regimes are tested. The dynamic relation between the weather and the load\nis modelled by simple transfer functions and the non-linearities are described\nusing spline functions. The results are thoroughly evaluated and it is shown\nthat the spline functions are suitable for handling the non-linear relations\nand that after applying an auto-regressive noise model the one-step ahead\nresiduals do not contain further significant information.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 10:23:59 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Rasmussen", "Lisa Buth", ""], ["Bacher", "Peder", ""], ["Madsen", "Henrik", ""], ["Nielsen", "Henrik Aalborg", ""], ["Heerup", "Christian", ""], ["Green", "Torben", ""]]}, {"id": "1406.6133", "submitter": "Ming Jin", "authors": "Zhaoyi Kang, Ming Jin, Costas J. Spanos", "title": "Modeling of End-Use Energy Profile: An Appliance-Data-Driven Stochastic\n  Approach", "comments": "accepted to be published in The 40th Annual Conference of the IEEE\n  Industrial Electronics Society, 2014", "journal-ref": null, "doi": "10.1109/IECON.2014.7049322", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the modeling of building end-use energy profile is\ncomprehensively investigated. Top-down and Bottom-up approaches are discussed\nwith a focus on the latter for better integration with occupant information.\nCompared to the Time-Of-Use (TOU) data used in previous Bottom-up models, this\nwork utilizes high frequency sampled appliance power consumption data from\nwireless sensor network, and hence builds an appliance-data-driven probability\nbased end-use energy profile model. ON/OFF probabilities of appliances are used\nin this model, to build a non-homogeneous Markov Chain, compared to the\nduration statistics based model that is widely used in other works. The\nsimulation results show the capability of the model to capture the diversity\nand variability of different categories of end-use appliance energy profile,\nwhich can further help on the design of a modern robust building power system.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 04:33:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kang", "Zhaoyi", ""], ["Jin", "Ming", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1406.6145", "submitter": "Tyler Maunu", "authors": "Gilad Lerman and Tyler Maunu", "title": "Fast, Robust and Non-convex Subspace Recovery", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA 7 (2018) 277-336", "doi": "10.1093/imaiai/iax012", "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fast and non-convex algorithm for robust subspace\nrecovery. The data sets considered include inliers drawn around a\nlow-dimensional subspace of a higher dimensional ambient space, and a possibly\nlarge portion of outliers that do not lie nearby this subspace. The proposed\nalgorithm, which we refer to as Fast Median Subspace (FMS), is designed to\nrobustly determine the underlying subspace of such data sets, while having\nlower computational complexity than existing methods. We prove convergence of\nthe FMS iterates to a stationary point. Further, under a special model of data,\nFMS converges to a point which is near to the global minimum with overwhelming\nprobability. Under this model, we show that the iteration complexity is\nglobally bounded and locally $r$-linear. The latter theorem holds for any fixed\nfraction of outliers (less than 1) and any fixed positive distance between the\nlimit point and the global minimum. Numerical experiments on synthetic and real\ndata demonstrate its competitive speed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 06:15:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 22:58:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lerman", "Gilad", ""], ["Maunu", "Tyler", ""]]}, {"id": "1406.6508", "submitter": "Torben Tvedebrink", "authors": "Torben Tvedebrink, Poul Svante Eriksen and Niels Morling", "title": "The multivariate Dirichlet-multinomial distribution and its application\n  in forensic genetics to adjust for sub-population effects using the\n  {\\theta}-correction", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the construction of a multivariate generalisation\nof the Dirichlet-multinomial distribution. An example from forensic genetics in\nthe statistical analysis of DNA mixtures motivates the study of this\nmultivariate extension.\n  In forensic genetics, adjustment of the match probabilities due to remote\nancestry in the population is often done using the so-called\n{\\theta}-correction. This correction increases the probability of observing\nmultiple copies of rare alleles and thereby reduces the weight of the evidence\nfor rare genotypes.\n  By numerical examples, we show how the {\\theta}-correction incorporated by\nthe use of the multivariate Dirichlet-multinomial distribution affects the\nweight of evidence. Furthermore, we demonstrate how the {\\theta}-correction can\nbe incorporated in a Markov structure needed to make efficient computations in\na Bayesian network.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 09:38:20 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 14:01:03 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 07:46:28 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""], ["Morling", "Niels", ""]]}, {"id": "1406.6728", "submitter": "Catalina A.  Vallejos", "authors": "Catalina A. Vallejos, Mark F.J. Steel", "title": "Bayesian Survival Modelling of University Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to model the length of registration at university\nand its associated academic outcome for undergraduate students at the\nPontificia Universidad Cat\\'olica de Chile. Survival time is defined as the\ntime until the end of the enrollment period, which can relate to different\nreasons - graduation or two types of dropout - that are driven by different\nprocesses. Hence, a competing risks model is employed for the analysis. The\nissue of separation of the outcomes (which precludes maximum likelihood\nestimation) is handled through the use of Bayesian inference with an\nappropriately chosen prior. We are interested in identifying important\ndeterminants of university outcomes and the associ- ated model uncertainty is\nformally addressed through Bayesian model averaging. The methodology introduced\nfor modelling university outcomes is applied to three selected degree\nprogrammes, which are particularly affected by dropout and late graduation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 22:40:49 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Vallejos", "Catalina A.", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1406.6768", "submitter": "Arnab Pal", "authors": "Satya N. Majumdar and Arnab Pal", "title": "Extreme value statistics of correlated random variables", "comments": "A pedagogical brief overview based on the lectures given in the GGI\n  workshop ` http://www.ggi.fi.infn.it/index.php?p=workshops.inc&id=118 ',\n  Florence, Italy, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme value statistics (EVS) concerns the study of the statistics of the\nmaximum or the minimum of a set of random variables. This is an important\nproblem for any time-series and has applications in climate, finance, sports,\nall the way to physics of disordered systems where one is interested in the\nstatistics of the ground state energy. While the EVS of uncorrelated variables\nare well understood, little is known for strongly correlated random variables.\nOnly recently this subject has gained much importance both in statistical\nphysics and in probability theory. In this note, we will first review the\nclassical EVS for uncorrelated variables and discuss few examples of correlated\nvariables where analytical progress can be made.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 04:35:51 GMT"}, {"version": "v2", "created": "Fri, 1 Aug 2014 13:04:17 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 06:15:27 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Majumdar", "Satya N.", ""], ["Pal", "Arnab", ""]]}, {"id": "1406.6799", "submitter": "Achraf Mallat", "authors": "Achraf Mallat and Luc Vandendorpe", "title": "Joint Estimation of the Time Delay and the Clock Drift and Offset Using\n  UWB signals", "comments": "Accepted and published in the IEEE ICC 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two transceivers, the first with perfect clock and the second\nwith imperfect clock. We investigate the joint estimation of the delay between\nthe transceivers and the offset and the drift of the imperfect clock. We\npropose a protocol for the synchronization of the clocks. We derive some\nempirical estimators for the delay, the offset and the drift, and compute the\nCramer-Rao lower bounds and the joint maximum likelihood estimator of the delay\nand the drift. We study the impact of the protocol parameters and the\ntime-of-arrival estimation variance on the achieved performances. We validate\nsome theoretical results by simulation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 08:07:51 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Mallat", "Achraf", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1406.6862", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad and Anders L{\\o}land", "title": "Coping with area price risk in electricity markets: Forecasting\n  Contracts for Difference in the Nordic power market", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contracts for Difference (CfDs) are forwards on the spread between an area\nprice and the system price. Together with the system price forwards, these\nproducts are used to hedge the area price risk in the Nordic electricity\nmarket. The CfDs are typically available for the next two months, three\nquarters and three years. This is fine, except that CfDs are not traded at\nNASDAQ OMX Commodities for every Nord Pool Spot price area. We therefore ask\nthe hypothetical question: What would the CfD market price have been, say in\nthe price area NO2, if it had been traded? We build regression models for each\nobservable price area, and use Bayesian elicitation techniques to obtain prior\ninformation on how similar the different price areas are to forecast the price\nin an area where CfDs are not traded.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 12:21:51 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Ferkingstad", "Egil", ""], ["L\u00f8land", "Anders", ""]]}, {"id": "1406.7091", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Do altmetrics point to the broader impact of research? An overview of\n  benefits and disadvantages of altmetrics", "comments": "Accepted for publication in the Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, it is not clear how the impact of research on other areas of society\nthan science should be measured. While peer review and bibliometrics have\nbecome standard methods for measuring the impact of research in science, there\nis not yet an accepted framework within which to measure societal impact.\nAlternative metrics (called altmetrics to distinguish them from bibliometrics)\nare considered an interesting option for assessing the societal impact of\nresearch, as they offer new ways to measure (public) engagement with research\noutput. Altmetrics is a term to describe web-based metrics for the impact of\npublications and other scholarly material by using data from social media\nplatforms (e.g. Twitter or Mendeley). This overview of studies explores the\npotential of altmetrics for measuring societal impact. It deals with the\ndefinition and classification of altmetrics. Furthermore, their benefits and\ndisadvantages for measuring impact are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 07:31:12 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 13:08:51 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1406.7536", "submitter": "Giuseppe Vinci", "authors": "Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman and\n  Christopher Genovese", "title": "Estimating the distribution of Galaxy Morphologies on a continuous space", "comments": "4 pages, 3 figures, Statistical Challenges in 21st Century Cosmology,\n  Proceedings IAU Symposium No. 306, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incredible variety of galaxy shapes cannot be summarized by human defined\ndiscrete classes of shapes without causing a possibly large loss of\ninformation. Dictionary learning and sparse coding allow us to reduce the high\ndimensional space of shapes into a manageable low dimensional continuous vector\nspace. Statistical inference can be done in the reduced space via probability\ndistribution estimation and manifold estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 18:47:18 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Vinci", "Giuseppe", ""], ["Freeman", "Peter", ""], ["Newman", "Jeffrey", ""], ["Wasserman", "Larry", ""], ["Genovese", "Christopher", ""]]}, {"id": "1406.7611", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Validity of altmetrics data for measuring societal impact: A study using\n  data from Altmetric and F1000Prime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can altmetric data be validly used for the measurement of societal impact?\nThe current study seeks to answer this question with a comprehensive dataset\n(about 100,000 records) from very disparate sources (F1000, Altmetric, and an\nin-house database based on Web of Science). In the F1000 peer review system,\nexperts attach particular tags to scientific papers which indicate whether a\npaper could be of interest for science or rather for other segments of society.\nThe results show that papers with the tag \"good for teaching\" do achieve higher\naltmetric counts than papers without this tag - if the quality of the papers is\ncontrolled. At the same time, a higher citation count is shown especially by\npapers with a tag that is specifically scientifically oriented (\"new finding\").\nThe findings indicate that papers tailored for a readership outside the area of\nresearch should lead to societal impact. If altmetric data is to be used for\nthe measurement of societal impact, the question arises of its normalization.\nIn bibliometrics, citations are normalized for the papers' subject area and\npublication year. This study has taken a second analytic step involving a\npossible normalization of altmetric data. As the results show there are\nparticular scientific topics which are of especial interest for a wide\naudience. Since these more or less interesting topics are not completely\nreflected in Thomson Reuters' journal sets, a normalization of altmetric data\nshould not be based on the level of subject categories, but on the level of\ntopics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 06:19:24 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 08:37:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1406.7642", "submitter": "Alina S\\^irbu", "authors": "Pierluigi Contucci, Emanuele Panizzi, Federico Ricci-Tersenghi and\n  Alina S\\^irbu", "title": "Egalitarianism in the rank aggregation problem: a new dimension for\n  democracy", "comments": "18 pages, 14 page appendix, RateIt Web Tool:\n  http://www.sapienzaapps.it/rateit.php, RankIt Android mobile application:\n  https://play.google.com/store/apps/details?id=sapienza.informatica.rankit.\n  Appears in Quality & Quantity, 10 Apr 2015, Online First", "journal-ref": "Quality & Quantity, 50(3), 1185-1200, 2016", "doi": "10.1007/s11135-015-0197-x", "report-no": null, "categories": "physics.soc-ph math.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winner selection by majority, in an election between two candidates, is the\nonly rule compatible with democratic principles. Instead, when the candidates\nare three or more and the voters rank candidates in order of preference, there\nare no univocal criteria for the selection of the winning (consensus) ranking\nand the outcome is known to depend sensibly on the adopted rule. Building upon\nXVIII century Condorcet theory, whose idea was to maximize total voter\nsatisfaction, we propose here the addition of a new basic principle (dimension)\nto guide the selection: satisfaction should be distributed among voters as\nequally as possible. With this new criterion we identify an optimal set of\nrankings. They range from the Condorcet solution to the one which is the most\negalitarian with respect to the voters. We show that highly egalitarian\nrankings have the important property to be more stable with respect to\nfluctuations and that classical consensus rankings (Copeland, Tideman, Schulze)\noften turn out to be non optimal. The new dimension we have introduced\nprovides, when used together with that of Condorcet, a clear classification of\nall the possible rankings. By increasing awareness in selecting a consensus\nranking our method may lead to social choices which are more egalitarian\ncompared to those achieved by presently available voting systems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 09:21:32 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 13:59:34 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Contucci", "Pierluigi", ""], ["Panizzi", "Emanuele", ""], ["Ricci-Tersenghi", "Federico", ""], ["S\u00eerbu", "Alina", ""]]}, {"id": "1406.7665", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Nigel Goddard, Charles Sutton", "title": "Interleaved Factorial Non-Homogeneous Hidden Markov Models for Energy\n  Disaggregation", "comments": "5 pages, 1 figure, conference, The NIPS workshop on Machine Learning\n  for Sustainability, Lake Tahoe, NV, USA, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce energy demand in households it is useful to know which electrical\nappliances are in use at what times. Monitoring individual appliances is costly\nand intrusive, whereas data on overall household electricity use is more easily\nobtained. In this paper, we consider the energy disaggregation problem where a\nhousehold's electricity consumption is disaggregated into the component\nappliances. The factorial hidden Markov model (FHMM) is a natural model to fit\nthis data. We enhance this generic model by introducing two constraints on the\nstate sequence of the FHMM. The first is to use a non-homogeneous Markov chain,\nmodelling how appliance usage varies over the day, and the other is to enforce\nthat at most one chain changes state at each time step. This yields a new model\nwhich we call the interleaved factorial non-homogeneous hidden Markov model\n(IFNHMM). We evaluated the ability of this model to perform disaggregation in\nan ultra-low frequency setting, over a data set of 251 English households. In\nthis new setting, the IFNHMM outperforms the FHMM in terms of recovering the\nenergy used by the component appliances, due to that stronger constraints have\nbeen imposed on the states of the hidden Markov chains. Interestingly, we find\nthat the variability in model performance across households is significant,\nunderscoring the importance of using larger scale data in the disaggregation\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 10:53:27 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Zhong", "Mingjun", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}]