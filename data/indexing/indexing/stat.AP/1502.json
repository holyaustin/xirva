[{"id": "1502.00088", "submitter": "Liat Shenhav", "authors": "Liat Shenhav, Ruth Heller and Yoav Benjamini", "title": "Quantifying replicability in systematic reviews: the r-value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to assess the effect of a health care intervention, it is useful to\nlook at an ensemble of relevant studies. The Cochrane Collaboration's admirable\ngoal is to provide systematic reviews of all relevant clinical studies, in\norder to establish whether or not there is a conclusive evidence about a\nspecific intervention. This is done mainly by conducting a meta-analysis: a\nstatistical synthesis of results from a series of systematically collected\nstudies. Health practitioners often interpret a significant meta-analysis\nsummary effect as a statement that the treatment effect is consistent across a\nseries of studies. However, the meta-analysis significance may be driven by an\neffect in only one of the studies. Indeed, in an analysis of two domains of\nCochrane reviews we show that in a non-negligible fraction of reviews, the\nremoval of a single study from the meta-analysis of primary endpoints makes the\nconclusion non-significant. Therefore, reporting the evidence towards\nreplicability of the effect across studies in addition to the significant\nmeta-analysis summary effect will provide credibility to the interpretation\nthat the effect was replicated across studies. We suggest an objective, easily\ncomputed quantity, we term the r-value, that quantifies the extent of this\nreliance on single studies. We suggest adding the r-values to the main results\nand to the forest plots of systematic reviews.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 09:54:42 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 13:28:10 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Shenhav", "Liat", ""], ["Heller", "Ruth", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1502.00210", "submitter": "Jing Tian", "authors": "Jing Tian, Wei Cui and Si-liang Wu", "title": "A New Parameter Estimation Algorithm Based on Sub-band Dual Frequency\n  Conjugate LVT", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new parameter estimation algorithm, known as Sub-band Dual Frequency\nConjugate LVT (SDFC-LVT), is proposed for the ground moving targets. This\nalgorithm first constructs two sub-band signals with different central\nfrequencies. After that, the two signals are shifted by different values in\nfrequency domain and a new signal is constructed by multiplying one with the\nconjugate of the other. Finally, Keystone transform and LVT operation are\nperformed on the constructed signal to attain the estimates. The cross-term and\nthe performance of the proposed method are analyzed in detail. Since the\nequivalent carrier frequency is reduced greatly, the proposed method is capable\nof obtaining the accurate parameter estimates and resolving the problem of\nambiguity which invalidates Keystone transform. It is search-free and can\ncompensate the range walk of multiple targets simultaneously, thereby reducing\nthe computational burden. The effectiveness of the proposed method is\ndemonstrated by both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 07:49:11 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Tian", "Jing", ""], ["Cui", "Wei", ""], ["Wu", "Si-liang", ""]]}, {"id": "1502.00234", "submitter": "Marcel Ausloos", "authors": "G. Rotundo, M. Ausloos, C. Herteliu, B. Ileanu", "title": "Hurst exponent of very long birth time series in XX century Romania.\n  Social and religious aspects", "comments": "19 pages, 37 references, 6 figures, 2 tables, to be published in\n  Physica A", "journal-ref": "Physica A 429 (2015) 109-117", "doi": "10.1016/j.physa.2015.02.003", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hurst exponent of very long birth time series in Romania has been\nextracted from official daily records, i.e. over 97 years between 1905 and 2001\nincluded. The series result from distinguishing between families located in\nurban (U) or rural (R) areas, and belonging (Ox) or not (NOx) to the orthodox\nreligion. Four time series combining both criteria, (U,R) and (Ox, NOx), are\nalso examined.\n  A statistical information is given on these sub-populations measuring their\nXX-th century state as a snapshot. However, the main goal is to investigate\nwhether the \"daily\" production of babies is purely noisy or is fluctuating\naccording to some non trivial fractional Brownian motion, - in the four types\nof populations, characterized by either their habitat or their religious\nattitude, yet living within the same political regime. One of the goals was\nalso to find whether combined criteria implied a different behavior. Moreover,\nwe wish to observe whether some seasonal periodicity exists.\n  The detrended fluctuation analysis technique is used for finding the fractal\ncorrelation dimension of such (9) signals. It has been first necessary, due to\ntwo periodic tendencies, to define the range regime in which the Hurst exponent\nis meaningfully defined. It results that the birth of babies in all cases is a\nvery strongly persistent signal. It is found that the signal fractal\ncorrelation dimension is weaker (i) for NOx than for Ox, and (ii) or U with\nrespect to R. Moreover, it is observed that the combination of U or R with NOx\nor OX enhances the UNOx, UOx, and ROx fluctuations, but smoothens the RNOx\nsignal, thereby suggesting a stronger conditioning on religiosity rituals or\nrules.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 11:21:43 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Rotundo", "G.", ""], ["Ausloos", "M.", ""], ["Herteliu", "C.", ""], ["Ileanu", "B.", ""]]}, {"id": "1502.00239", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, I. V. Tchervensky, V. S. Dimitrov, M. P. Mintchev", "title": "Optimal Wavelets for Electrogastrography", "comments": "6 pages, 4 figures, 2 tables, corrected Eq. (3)", "journal-ref": "26th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (IEMBS), vol. 1, pp. 329--332, 2004", "doi": "10.1109/IEMBS.2004.1403159", "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching a wavelet to class of signals can be of interest in feature\ndetection and classification based on wavelet representation. The aim of this\nwork is to provide a quantitative approach to the problem of matching a wavelet\nto electrogastrographic (EGG) signals. Visually inspected EGG recordings from\nsixteen dogs and six volunteers were submitted to wavelet analysis.\nApproximated wavelet-based versions of EGG signals were calculated using Pollen\nparameterization of 6-tap wavelet filters and wavelet compression techniques.\nWavelet parameterization values that minimize the approximation error of\ncompressed EGG signals were sought and considered optimal. The wavelets\ngenerated from the optimal parameterization values were remarkably similar to\nthe standard Daubechies-3 wavelet.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 12:24:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Cintra", "R. J.", ""], ["Tchervensky", "I. V.", ""], ["Dimitrov", "V. S.", ""], ["Mintchev", "M. P.", ""]]}, {"id": "1502.00526", "submitter": "Kyu Ha Lee", "authors": "Kyu Ha Lee, Francesca Dominici, Deborah Schrag, and Sebastien Haneuse", "title": "Hierarchical models for semi-competing risks data with application to\n  quality of end-of-life care for pancreatic cancer", "comments": null, "journal-ref": "Journal of the American Statistical Association 2016, Volume 111,\n  Issue 515, pages 1075-1095", "doi": "10.1080/01621459.2016.1164052", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readmission following discharge from an initial hospitalization is a key\nmarker of quality of health care in the United States. For the most part,\nreadmission has been used to study quality of care for patients with acute\nhealth conditions, such as pneumonia and heart failure, with analyses typically\nbased on a logistic-Normal generalized linear mixed model. Applying this model\nto the study readmission among patients with increasingly prevalent advanced\nhealth conditions such as pancreatic cancer is problematic, however, because it\nignores death as a competing risk. A more appropriate analysis is to imbed such\nstudies within the semi-competing risks framework. To our knowledge, however,\nno comprehensive statistical methods have been developed for cluster-correlated\nsemi-competing risks data. In this paper we propose a novel hierarchical\nmodeling framework for the analysis of cluster-correlated semi-competing risks\ndata. The framework permits parametric or non-parametric specifications for a\nrange of model components, including baseline hazard functions and\ndistributions for key random effects, giving analysts substantial flexibility\nas they consider their own analyses. Estimation and inference is performed\nwithin the Bayesian paradigm since it facilitates the straightforward\ncharacterization of (posterior) uncertainty for all model parameters including\nhospital-specific random effects. The proposed framework is used to study the\nrisk of readmission among 5,298 Medicare beneficiaries diagnosed with\npancreatic cancer at 112 hospitals in the six New England states between\n2000-2009, specifically to investigate the role of patient-level risk factors\nand to characterize variation in risk across hospitals that is not explained by\ndifferences in patient case-mix.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:48:09 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 19:28:12 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lee", "Kyu Ha", ""], ["Dominici", "Francesca", ""], ["Schrag", "Deborah", ""], ["Haneuse", "Sebastien", ""]]}, {"id": "1502.00592", "submitter": "Renato J Cintra", "authors": "C. J. Tablada, F. M. Bayer, R. J. Cintra", "title": "A Class of DCT Approximations Based on the Feig-Winograd Algorithm", "comments": "26 pages, 4 figures, 5 tables, fixed arithmetic complexity in Table\n  IV", "journal-ref": "Signal Processing, vol. 113, pp. 38-51, August 2015", "doi": "10.1016/j.sigpro.2015.01.011", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of matrices based on a parametrization of the Feig-Winograd\nfactorization of 8-point DCT is proposed. Such parametrization induces a matrix\nsubspace, which unifies a number of existing methods for DCT approximation. By\nsolving a comprehensive multicriteria optimization problem, we identified\nseveral new DCT approximations. Obtained solutions were sought to possess the\nfollowing properties: (i) low multiplierless computational complexity, (ii)\northogonality or near orthogonality, (iii) low complexity invertibility, and\n(iv) close proximity and performance to the exact DCT. Proposed approximations\nwere submitted to assessment in terms of proximity to the DCT, coding\nperformance, and suitability for image compression. Considering Pareto\nefficiency, particular new proposed approximations could outperform various\nexisting methods archived in literature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 19:39:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 21:25:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tablada", "C. J.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.00680", "submitter": "Martin Gould", "authors": "Martin D. Gould, Mason A. Porter, Sam D. Howison", "title": "Quasi-Centralized Limit Order Books", "comments": "43 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR nlin.AO physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quasi-centralized limit order book (QCLOB) is a limit order book (LOB) in\nwhich financial institutions can only access the trading opportunities offered\nby counterparties with whom they possess sufficient bilateral credit. We\nperform an empirical analysis of a recent, high-quality data set from a large\nelectronic trading platform that utilizes QCLOBs to facilitate trade. We find\nmany significant differences between our results and those widely reported for\nother LOBs. We also uncover a remarkable empirical universality: although the\ndistributions describing order flow and market state vary considerably across\ndays, a simple, linear rescaling causes them to collapse onto a single curve.\nMotivated by this finding, we propose a semi-parametric model of order flow and\nmarket state in a QCLOB on a single trading day. Our model provides similar\nperformance to that of parametric curve-fitting techniques, while being simpler\nto compute and faster to implement.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 22:43:17 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 12:35:25 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Gould", "Martin D.", ""], ["Porter", "Mason A.", ""], ["Howison", "Sam D.", ""]]}, {"id": "1502.00725", "submitter": "Hongwei Li", "authors": "Hongwei Li and Qiang Liu", "title": "Cheaper and Better: Selecting Good Workers for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a popular paradigm for data collection at scale. We\nstudy the problem of selecting subsets of workers from a given worker pool to\nmaximize the accuracy under a budget constraint. One natural question is\nwhether we should hire as many workers as the budget allows, or restrict on a\nsmall number of top-quality workers. By theoretically analyzing the error rate\nof a typical setting in crowdsourcing, we frame the worker selection problem\ninto a combinatorial optimization problem and propose an algorithm to solve it\nefficiently. Empirical results on both simulated and real-world datasets show\nthat our algorithm is able to select a small number of high-quality workers,\nand performs as good as, sometimes even better than, the much larger crowds as\nthe budget allows.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:45:48 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Li", "Hongwei", ""], ["Liu", "Qiang", ""]]}, {"id": "1502.00746", "submitter": "Jiahan Li", "authors": "Jiahan Li, Wei Zhong, Runze Li, Rongling Wu", "title": "A fast algorithm for detecting gene-gene interactions in genome-wide\n  association studies", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS771 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2292-2318", "doi": "10.1214/14-AOAS771", "report-no": "IMS-AOAS-AOAS771", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advent of high-throughput genotyping techniques, genetic data\nfor genome-wide association studies (GWAS) have become increasingly available,\nwhich entails the development of efficient and effective statistical\napproaches. Although many such approaches have been developed and used to\nidentify single-nucleotide polymorphisms (SNPs) that are associated with\ncomplex traits or diseases, few are able to detect gene-gene interactions among\ndifferent SNPs. Genetic interactions, also known as epistasis, have been\nrecognized to play a pivotal role in contributing to the genetic variation of\nphenotypic traits. However, because of an extremely large number of SNP-SNP\ncombinations in GWAS, the model dimensionality can quickly become so\noverwhelming that no prevailing variable selection methods are capable of\nhandling this problem. In this paper, we present a statistical framework for\ncharacterizing main genetic effects and epistatic interactions in a GWAS study.\nSpecifically, we first propose a two-stage sure independence screening (TS-SIS)\nprocedure and generate a pool of candidate SNPs and interactions, which serve\nas predictors to explain and predict the phenotypes of a complex trait. We also\npropose a rates adjusted thresholding estimation (RATE) approach to determine\nthe size of the reduced model selected by an independence screening.\nRegularization regression methods, such as LASSO or SCAD, are then applied to\nfurther identify important genetic effects. Simulation studies show that the\nTS-SIS procedure is computationally efficient and has an outstanding finite\nsample performance in selecting potential SNPs as well as gene-gene\ninteractions. We apply the proposed framework to analyze an\nultrahigh-dimensional GWAS data set from the Framingham Heart Study, and select\n23 active SNPs and 24 active epistatic interactions for the body mass index\nvariation. It shows the capability of our procedure to resolve the complexity\nof genetic control.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 05:52:03 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Li", "Jiahan", ""], ["Zhong", "Wei", ""], ["Li", "Runze", ""], ["Wu", "Rongling", ""]]}, {"id": "1502.00754", "submitter": "Elasma Milanzi", "authors": "Elasma Milanzi, Ariel Alonso, Christophe Buyck, Geert Molenberghs, Luc\n  Bijnens", "title": "A permutational-splitting sample procedure to quantify expert opinion on\n  clusters of chemical compounds using high-dimensional data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS772 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2319-2335", "doi": "10.1214/14-AOAS772", "report-no": "IMS-AOAS-AOAS772", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert opinion plays an important role when selecting promising clusters of\nchemical compounds in the drug discovery process. We propose a method to\nquantify these qualitative assessments using hierarchical models. However, with\nthe most commonly available computing resources, the high dimensionality of the\nvectors of fixed effects and correlated responses renders maximum likelihood\nunfeasible in this scenario. We devise a reliable procedure to tackle this\nproblem and show, using theoretical arguments and simulations, that the new\nmethodology compares favorably with maximum likelihood, when the latter option\nis available. The approach was motivated by a case study, which we present and\nanalyze.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:29:11 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Milanzi", "Elasma", ""], ["Alonso", "Ariel", ""], ["Buyck", "Christophe", ""], ["Molenberghs", "Geert", ""], ["Bijnens", "Luc", ""]]}, {"id": "1502.00757", "submitter": "Zhiwei Zhang", "authors": "Zhiwei Zhang, Lei Nie, Guoxing Soon, Aiyi Liu", "title": "The use of covariates and random effects in evaluating predictive\n  biomarkers under a potential outcome framework", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS773 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2336-2355", "doi": "10.1214/14-AOAS773", "report-no": "IMS-AOAS-AOAS773", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive or treatment selection biomarkers are usually evaluated in a\nsubgroup or regression analysis with focus on the treatment-by-marker\ninteraction. Under a potential outcome framework (Huang, Gilbert and Janes\n[Biometrics 68 (2012) 687-696]), a predictive biomarker is considered a\npredictor for a desirable treatment benefit (defined by comparing potential\noutcomes for different treatments) and evaluated using familiar concepts in\nprediction and classification. However, the desired treatment benefit is\nunobservable because each patient can receive only one treatment in a typical\nstudy. Huang et al. overcome this problem by assuming monotonicity of potential\noutcomes, with one treatment dominating the other in all patients. Motivated by\nan HIV example that appears to violate the monotonicity assumption, we propose\na different approach based on covariates and random effects for evaluating\npredictive biomarkers under the potential outcome framework. Under the proposed\napproach, the parameters of interest can be identified by assuming conditional\nindependence of potential outcomes given observed covariates, and a sensitivity\nanalysis can be performed by incorporating an unobserved random effect that\naccounts for any residual dependence. Application of this approach to the\nmotivating example shows that baseline viral load and CD4 cell count are both\nuseful as predictive biomarkers for choosing antiretroviral drugs for\ntreatment-naive patients.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:56:00 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Zhang", "Zhiwei", ""], ["Nie", "Lei", ""], ["Soon", "Guoxing", ""], ["Liu", "Aiyi", ""]]}, {"id": "1502.00770", "submitter": "Cecilia A. Cotton", "authors": "Cecilia A. Cotton, Patrick J. Heagerty", "title": "Evaluating epoetin dosing strategies using observational longitudinal\n  data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS774 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2356-2377", "doi": "10.1214/14-AOAS774", "report-no": "IMS-AOAS-AOAS774", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epoetin is commonly used to treat anemia in chronic kidney disease and End\nStage Renal Disease subjects undergoing dialysis, however, there is\nconsiderable uncertainty about what level of hemoglobin or hematocrit should be\ntargeted in these subjects. In order to address this question, we treat epoetin\ndosing guidelines as a type of dynamic treatment regimen. Specifically, we\npresent a methodology for comparing the effects of alternative treatment\nregimens on survival using observational data. In randomized trials patients\ncan be assigned to follow a specific management guideline, but in observational\nstudies subjects can have treatment paths that appear to be adherent to\nmultiple regimens at the same time. We present a cloning strategy in which each\nsubject contributes follow-up data to each treatment regimen to which they are\ncontinuously adherent and artificially censored at first nonadherence. We\ndetail an inverse probability weighted log-rank test with a valid asymptotic\nvariance estimate that can be used to test survival distributions under two\nregimens. To compare multiple regimens, we propose several marginal structural\nCox proportional hazards models with robust variance estimation to account for\nthe creation of clones. The methods are illustrated through simulations and\napplied to an analysis comparing epoetin dosing regimens in a cohort of 33,873\nadult hemodialysis patients from the United States Renal Data System.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 07:57:27 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Cotton", "Cecilia A.", ""], ["Heagerty", "Patrick J.", ""]]}, {"id": "1502.00818", "submitter": "Wolfgang Jank", "authors": "Yingying Fan, Natasha Foutz, Gareth M. James, Wolfgang Jank", "title": "Functional response additive model estimation with online virtual stock\n  markets", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS781 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2435-2460", "doi": "10.1214/14-AOAS781", "report-no": "IMS-AOAS-AOAS781", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While functional regression models have received increasing attention\nrecently, most existing approaches assume both a linear relationship and a\nscalar response variable. We suggest a new method, \"Functional Response\nAdditive Model Estimation\" (FRAME), which extends the usual linear regression\nmodel to situations involving both functional predictors, $X_j(t)$, scalar\npredictors, $Z_k$, and functional responses, $Y(s)$. Our approach uses a\npenalized least squares optimization criterion to automatically perform\nvariable selection in situations involving multiple functional and scalar\npredictors. In addition, our method uses an efficient coordinate descent\nalgorithm to fit general nonlinear additive relationships between the\npredictors and response. We develop our model for novel forecasting challenges\nin the entertainment industry. In particular, we set out to model the decay\nrate of demand for Hollywood movies using the predictive power of online\nvirtual stock markets (VSMs). VSMs are online communities that, in a\nmarket-like fashion, gather the crowds' prediction about demand for a\nparticular product. Our fully functional model captures the pattern of\npre-release VSM trading prices and provides superior predictive accuracy of a\nmovie's post-release demand in comparison to traditional methods. In addition,\nwe propose graphical tools which give a glimpse into the causal relationship\nbetween market behavior and box office revenue patterns, and hence provide\nvaluable insight to movie decision makers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 11:33:30 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Fan", "Yingying", ""], ["Foutz", "Natasha", ""], ["James", "Gareth M.", ""], ["Jank", "Wolfgang", ""]]}, {"id": "1502.00871", "submitter": "Casey Olives", "authors": "Casey Olives, Lianne Sheppard, Johan Lindstr\\\"om, Paul D. Sampson,\n  Joel D. Kaufman, Adam A. Szpiro", "title": "Reduced-rank spatio-temporal modeling of air pollution concentrations in\n  the Multi-Ethnic Study of Atherosclerosis and Air Pollution", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS786 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2509-2537", "doi": "10.1214/14-AOAS786", "report-no": "IMS-AOAS-AOAS786", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence in the epidemiologic literature of the relationship\nbetween air pollution and adverse health outcomes. Prediction of individual air\npollution exposure in the Environmental Protection Agency (EPA) funded\nMulti-Ethnic Study of Atheroscelerosis and Air Pollution (MESA Air) study\nrelies on a flexible spatio-temporal prediction model that integrates land-use\nregression with kriging to account for spatial dependence in pollutant\nconcentrations. Temporal variability is captured using temporal trends\nestimated via modified singular value decomposition and temporally varying\nspatial residuals. This model utilizes monitoring data from existing regulatory\nnetworks and supplementary MESA Air monitoring data to predict concentrations\nfor individual cohort members. In general, spatio-temporal models are limited\nin their efficacy for large data sets due to computational intractability. We\ndevelop reduced-rank versions of the MESA Air spatio-temporal model. To do so,\nwe apply low-rank kriging to account for spatial variation in the mean process\nand discuss the limitations of this approach. As an alternative, we represent\nspatial variation using thin plate regression splines. We compare the\nperformance of the outlined models using EPA and MESA Air monitoring data for\npredicting concentrations of oxides of nitrogen (NO$_x$)-a pollutant of primary\ninterest in MESA Air-in the Los Angeles metropolitan area via cross-validated\n$R^2$. Our findings suggest that use of reduced-rank models can improve\ncomputational efficiency in certain cases. Low-rank kriging and thin plate\nregression splines were competitive across the formulations considered,\nalthough TPRS appeared to be more robust in some settings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 14:20:27 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Olives", "Casey", ""], ["Sheppard", "Lianne", ""], ["Lindstr\u00f6m", "Johan", ""], ["Sampson", "Paul D.", ""], ["Kaufman", "Joel D.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "1502.00908", "submitter": "Ra\\'ul Torres D\\'iaz", "authors": "Ra\\'ul Torres, Rosa E. Lillo and Henry Laniado", "title": "A Directional Multivariate Value at Risk", "comments": "30 pages, 9 figures", "journal-ref": "Insurance: Mathematics and Economics, Volume 65, November 2015,\n  Pages 111-123", "doi": "10.1016/j.insmatheco.2015.09.002", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In economics, insurance and finance, value at risk (VaR) is a widely used\nmeasure of the risk of loss on a specific portfolio of financial assets. For a\ngiven portfolio, time horizon, and probability $\\alpha$, the $100\\alpha\\%$ VaR\nis defined as a threshold loss value, such that the probability that the loss\non the portfolio over the given time horizon exceeds this value is $\\alpha$.\nThat is to say, it is a quantile of the distribution of the losses, which has\nboth good analytic properties and easy interpretation as a risk measure.\nHowever, its extension to the multivariate framework is not unique because a\nunique definition of multivariate quantile does not exist. In the current\nliterature, the multivariate quantiles are related to a specific partial order\nconsidered in $\\mathbb{R}^{n}$, or to a property of the univariate quantile\nthat is desirable to be extended to $\\mathbb{R}^{n}$. In this work, we\nintroduce a multivariate value at risk as a vector-valued directional risk\nmeasure, based on a directional multivariate quantile, which has recently been\nintroduced in the literature. The directional approach allows the manager to\nconsider external information or risk preferences in her/his analysis. We have\nderived some properties of the risk measure and we have compared the univariate\n\\textit{VaR} over the marginals with the components of the directional\nmultivariate VaR. We have also analyzed the relationship between some families\nof copulas, for which it is possible to obtain closed forms of the multivariate\nVaR that we propose. Finally, comparisons with other alternative multivariate\nVaR given in the literature, are provided in terms of robustness.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 16:10:35 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Torres", "Ra\u00fal", ""], ["Lillo", "Rosa E.", ""], ["Laniado", "Henry", ""]]}, {"id": "1502.00932", "submitter": "Lucio Anderlini", "authors": "Lucio Anderlini", "title": "Density Estimation Trees in High Energy Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Estimation Trees can play an important role in exploratory data\nanalysis for multidimensional, multi-modal data models of large samples. I\nbriefly discuss the algorithm, a self-optimization technique based on kernel\ndensity estimation, and some applications in High Energy Physics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 17:16:16 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Anderlini", "Lucio", ""]]}, {"id": "1502.01002", "submitter": "Alexander Wong", "authors": "Alexander Wong, Xiao Yu Wang, and Maud Gorbet", "title": "Bayesian-based deconvolution fluorescence microscopy using dynamically\n  updated nonparametric nonstationary expectation estimates", "comments": "13", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy is widely used for the study of biological specimens.\nDeconvolution can significantly improve the resolution and contrast of images\nproduced using fluorescence microscopy; in particular, Bayesian-based methods\nhave become very popular in deconvolution fluorescence microscopy. An ongoing\nchallenge with Bayesian-based methods is in dealing with the presence of noise\nin low SNR imaging conditions. In this study, we present a Bayesian-based\nmethod for performing deconvolution using dynamically updated nonparametric\nnonstationary expectation estimates that can improve the fluorescence\nmicroscopy image quality in the presence of noise, without explicit use of\nspatial regularization.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 20:34:28 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Wong", "Alexander", ""], ["Wang", "Xiao Yu", ""], ["Gorbet", "Maud", ""]]}, {"id": "1502.01236", "submitter": "Luiz Max  Carvalho", "authors": "Flavio Coelho and Luiz Max Carvalho", "title": "Estimating the Attack Ratio of Dengue Epidemics under Time-varying Force\n  of Infection using Aggregated Notification Data", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Quantifying the attack ratio of disease is key to epidemiological inference\nand Public Health planning. For multi-serotype pathogens, however, different\nlevels of serotype-specific immunity make it difficult to assess the population\nat risk. In this paper we propose a Bayesian method for estimation of the\nattack ratio of an epidemic and the initial fraction of susceptibles using\naggregated incidence data. We derive the probability distribution of the\neffective reproductive number, R t , and use MCMC to obtain posterior\ndistributions of the parameters of a single-strain SIR transmission model with\ntime-varying force of infection. Our method is showcased in a data set\nconsisting of 18 years of dengue incidence in the city of Rio de Janeiro,\nBrazil. We demonstrate that it is possible to learn about the initial fraction\nof susceptibles and the attack ratio even in the absence of serotype specific\ndata. On the other hand, the information provided by this approach is limited,\nstressing the need for detailed serological surveys to characterise the\ndistribution of serotype-specific immunity in the population.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 15:43:51 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Coelho", "Flavio", ""], ["Carvalho", "Luiz Max", ""]]}, {"id": "1502.01377", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, V. S. Dimitrov", "title": "The Arithmetic Cosine Transform: Exact and Approximate Algorithms", "comments": "17 pages, 3 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 58, no. 6, pp.\n  3076-3085, June 2010", "doi": "10.1109/TSP.2010.2045781", "report-no": null, "categories": "cs.NA math.NA stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of transform method --- the\narithmetic cosine transform (ACT). We provide the central mathematical\nproperties of the ACT, necessary in designing efficient and accurate\nimplementations of the new transform method. The key mathematical tools used in\nthe paper come from analytic number theory, in particular the properties of the\nRiemann zeta function. Additionally, we demonstrate that an exact signal\ninterpolation is achievable for any block-length. Approximate calculations were\nalso considered. The numerical examples provided show the potential of the ACT\nfor various digital signal processing applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 22:10:21 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1502.01477", "submitter": "Bruno Sudret", "authors": "G. Deman, K. Konakli, B. Sudret, J. Kerrou, P. Perrochet, H.\n  Benabderrahmane", "title": "Using sparse polynomial chaos expansions for the global sensitivity\n  analysis of groundwater lifetime expectancy in a multi-layered\n  hydrogeological model", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2015.11.005", "report-no": "RSUQ-2015-001", "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study makes use of polynomial chaos expansions to compute Sobol' indices\nwithin the frame of a global sensitivity analysis of hydro-dispersive\nparameters in a simplified vertical cross-section of a segment of the\nsubsurface of the Paris Basin. Applying conservative ranges, the uncertainty in\n78 input variables is propagated upon the mean lifetime expectancy of water\nmolecules departing from a specific location within a highly confining layer\nsituated in the middle of the model domain. Lifetime expectancy is a\nhydrogeological performance measure pertinent to safety analysis with respect\nto subsurface contaminants, such as radionuclides. The sensitivity analysis\nindicates that the variability in the mean lifetime expectancy can be\nsufficiently explained by the uncertainty in the petrofacies, \\ie the sets of\nporosity and hydraulic conductivity, of only a few layers of the model. The\nobtained results provide guidance regarding the uncertainty modeling in future\ninvestigations employing detailed numerical models of the subsurface of the\nParis Basin. Moreover, the study demonstrates the high efficiency of sparse\npolynomial chaos expansions in computing Sobol' indices for high-dimensional\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 09:46:22 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 12:42:53 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 22:43:14 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Deman", "G.", ""], ["Konakli", "K.", ""], ["Sudret", "B.", ""], ["Kerrou", "J.", ""], ["Perrochet", "P.", ""], ["Benabderrahmane", "H.", ""]]}, {"id": "1502.01750", "submitter": "Donald Richards", "authors": "Linda V. Hansen, Thordis L. Thorarinsdottir, Evgeni Ovcharov, Tilmann\n  Gneiting, and Donald Richards", "title": "Gaussian Random Particles with Flexible Hausdorff Dimension", "comments": "22 pages, 5 figures, 3 tables; to appear in Advances in Applied\n  Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian particles provide a flexible framework for modelling and simulating\nthree-dimensional star-shaped random sets. In our framework, the radial\nfunction of the particle arises from a kernel smoothing, and is associated with\nan isotropic random field on the sphere. If the kernel is a von Mises--Fisher\ndensity, or uniform on a spherical cap, the correlation function of the\nassociated random field admits a closed form expression. The Hausdorff\ndimension of the surface of the Gaussian particle reflects the decay of the\ncorrelation function at the origin, as quantified by the fractal index. Under\npower kernels we obtain particles with boundaries of any Hausdorff dimension\nbetween 2 and 3.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 22:37:37 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 20:31:09 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 19:39:26 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Hansen", "Linda V.", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Ovcharov", "Evgeni", ""], ["Gneiting", "Tilmann", ""], ["Richards", "Donald", ""]]}, {"id": "1502.01780", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias and Athina P. Petropulu", "title": "Sequential Channel State Tracking & SpatioTemporal Channel Prediction in\n  Mobile Wireless Sensor Networks", "comments": "Original paper submitted to the IEEE Transactions on Signal and\n  Information Processing over Networks; 22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonlinear filtering framework for approaching the problems of\nchannel state tracking and spatiotemporal channel gain prediction in mobile\nwireless sensor networks, in a Bayesian setting. We assume that the wireless\nchannel constitutes an observable (by the sensors/network nodes),\nspatiotemporal, conditionally Gaussian stochastic process, which is\nstatistically dependent on a set of hidden channel parameters, called the\nchannel state. The channel state evolves in time according to a known, non\nstationary, nonlinear and/or non Gaussian Markov stochastic kernel. This\nformulation results in a partially observable system, with a temporally varying\nglobal state and spatiotemporally varying observations. Recognizing the\nintractability of general nonlinear state estimation, we advocate the use of\ngrid based approximate filters as an effective and robust means for recursive\ntracking of the channel state. We also propose a sequential spatiotemporal\npredictor for tracking the channel gains at any point in time and space,\nproviding real time sequential estimates for the respective channel gain map,\nfor each sensor in the network. Additionally, we show that both estimators\nconverge towards the true respective MMSE optimal estimators, in a common,\nrelatively strong sense. Numerical simulations corroborate the practical\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 02:57:05 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1502.01880", "submitter": "Helio M. de Oliveira", "authors": "E.F. Melo and H.M. de Oliveira", "title": "A Fingerprint-based Access Control using Principal Component Analysis\n  and Edge Detection", "comments": "5 pages, 9 figures. SBrT/IEEE International Telecommunication\n  Symposium, ITS 2010, Manaus, AM, Brazil", "journal-ref": null, "doi": "10.14209/SBRT.2010.63", "report-no": null, "categories": "cs.CV cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for deciding on the appropriateness or\nnot of an acquired fingerprint image into a given database. The process begins\nwith the assembly of a training base in an image space constructed by combining\nPrincipal Component Analysis (PCA) and edge detection. Then, the parameter H, a\nnew feature that helps in the decision making about the relevance of a\nfingerprint image in databases, is derived from a relationship between\nEuclidean and Mahalanobian distances. This procedure ends with the lifting of\nthe curve of the Receiver Operating Characteristic (ROC), where the thresholds\ndefined on the parameter H are chosen according to the acceptable rates of\nfalse positives and false negatives.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 13:31:54 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Melo", "E. F.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.01955", "submitter": "Robert Wolstenholme", "authors": "R. J. Wolstenholme and A. T. Walden", "title": "An Efficient Approach to Graphical Modelling of Time Series", "comments": "Replaced a typo in section 4.1; it is deemed an *incorrect graph* and\n  the missing edge; it is deemed a correct graph and the missing edge;", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for selecting a graphical model for $p$-vector-valued stationary\nGaussian time series was recently proposed by Matsuda and uses the\nKullback-Leibler divergence measure to define a test statistic. This statistic\nwas used in a backward selection procedure, but the algorithm is prohibitively\nexpensive for large $p.$ A high degree of sparsity is not assumed. We show that\nreformulation in terms of a multiple hypothesis test reduces computation time\nby $O(p^2)$ and simulations support the assertion that power levels are\nattained at least as good as those achieved by Matsuda's much slower approach.\nMoreover, the new scheme is readily parallelizable for even greater speed\ngains.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:01:00 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 18:58:03 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Wolstenholme", "R. J.", ""], ["Walden", "A. T.", ""]]}, {"id": "1502.01975", "submitter": "Govinda Kamath", "authors": "Govinda M. Kamath and Eren \\c{S}a\\c{s}o\\u{g}lu and David Tse", "title": "Optimal Haplotype Assembly from High-Throughput Mate-Pair Reads", "comments": "10 pages, 4 figures, Submitted to ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CE math.IT q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have $23$ pairs of homologous chromosomes. The homologous pairs are\nalmost identical pairs of chromosomes. For the most part, differences in\nhomologous chromosome occur at certain documented positions called single\nnucleotide polymorphisms (SNPs). A haplotype of an individual is the pair of\nsequences of SNPs on the two homologous chromosomes. In this paper, we study\nthe problem of inferring haplotypes of individuals from mate-pair reads of\ntheir genome. We give a simple formula for the coverage needed for haplotype\nassembly, under a generative model. The analysis here leverages connections of\nthis problem with decoding convolutional codes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 18:16:33 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Kamath", "Govinda M.", ""], ["\u015ea\u015fo\u011flu", "Eren", ""], ["Tse", "David", ""]]}, {"id": "1502.02049", "submitter": "Renato J Cintra", "authors": "L. R. Soares, H. M. de Oliveira, R. J. Cintra", "title": "The Fourier-Like and Hartley-Like Wavelet Analysis Based on Hilbert\n  Transforms", "comments": "7 pages, 10 figures, Anais do XXII Simp\\'osio Brasileiro de\n  Telecomunica\\c{c}\\~oes, Campinas, 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.ST physics.data-an stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continuous-time wavelet analysis, most wavelet present some kind of\nsymmetry. Based on the Fourier and Hartley transform kernels, a new wavelet\nmultiresolution analysis is proposed. This approach is based on a pair of\northogonal wavelet functions and is named as the Fourier-Like and Hartley-Like\nwavelet analysis. A Hilbert transform analysis on the wavelet theory is also\nincluded.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 21:16:31 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Soares", "L. R.", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.02312", "submitter": "Matt Taddy", "authors": "Matt Taddy, Chun-Sheng Chen, Jun Yu, Mitch Wyle", "title": "Bayesian and empirical Bayesian forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive ensembles of decision trees through a nonparametric Bayesian model,\nallowing us to view random forests as samples from a posterior distribution.\nThis insight provides large gains in interpretability, and motivates a class of\nBayesian forest (BF) algorithms that yield small but reliable performance\ngains. Based on the BF framework, we are able to show that high-level tree\nhierarchy is stable in large samples. This leads to an empirical Bayesian\nforest (EBF) algorithm for building approximate BFs on massive distributed\ndatasets and we show that EBFs outperform sub-sampling based alternatives by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 22:42:42 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 22:47:30 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Taddy", "Matt", ""], ["Chen", "Chun-Sheng", ""], ["Yu", "Jun", ""], ["Wyle", "Mitch", ""]]}, {"id": "1502.02406", "submitter": "Giulia  Cereda", "authors": "Giulia Cereda", "title": "Bayesian approach to LR assessment in case of rare type match: careful\n  derivation and limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood ratio (LR) is largely used to evaluate the relative weight of\nforensic data regarding two hypotheses and for its assessment Bayesian methods\nare widespread in the forensic field. However, the Bayesian `recipe' for the LR\npresented in most of literature consists in plugging-in Bayesian estimates of\nthe involved nuisance parameters into a frequentist-defined LR: frequentist and\nBayesian methods are thus mixed, giving rise to solutions obtained by hybrid\nreasoning. This paper provides the derivation of a proper Bayesian approach to\nassess LR for the `rare type match problem', the situation in which the expert\nwants to evaluate a match between the profile of a suspect and that of a trace\nfrom the crime scene, and this profile has never been observed before in the\ndatabase of reference. Bayesian LR assessment using the two most popular\nBayesian models (beta-binomial and Dirichlet-multinomial) is discussed and\ncompared to corresponding plug-in versions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 09:26:36 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 16:16:43 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 09:21:36 GMT"}, {"version": "v4", "created": "Sun, 22 Feb 2015 10:04:19 GMT"}, {"version": "v5", "created": "Tue, 24 Feb 2015 09:50:22 GMT"}, {"version": "v6", "created": "Wed, 25 Feb 2015 16:09:08 GMT"}, {"version": "v7", "created": "Wed, 24 Jun 2015 08:13:24 GMT"}, {"version": "v8", "created": "Mon, 7 Dec 2015 14:37:41 GMT"}, {"version": "v9", "created": "Wed, 13 Apr 2016 08:48:35 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Cereda", "Giulia", ""]]}, {"id": "1502.02445", "submitter": "Giovanni Montana", "authors": "Alexandre de Brebisson, Giovanni Montana", "title": "Deep Neural Networks for Anatomical Brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to automatically segment magnetic resonance (MR)\nimages of the human brain into anatomical regions. Our methodology is based on\na deep artificial neural network that assigns each voxel in an MR image of the\nbrain to its corresponding anatomical region. The inputs of the network capture\ninformation at different scales around the voxel of interest: 3D and orthogonal\n2D intensity patches capture the local spatial context while large, compressed\n2D orthogonal patches and distances to the regional centroids enforce global\nspatial consistency. Contrary to commonly used segmentation methods, our\ntechnique does not require any non-linear registration of the MR images. To\nbenchmark our model, we used the dataset provided for the MICCAI 2012 challenge\non multi-atlas labelling, which consists of 35 manually segmented MR images of\nthe brain. We obtained competitive results (mean dice coefficient 0.725, error\nrate 0.163) showing the potential of our approach. To our knowledge, our\ntechnique is the first to tackle the anatomical segmentation of the whole brain\nusing deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:48:42 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:19:44 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["de Brebisson", "Alexandre", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02506", "submitter": "Giovanni Montana", "authors": "Adrien Payan, Giovanni Montana", "title": "Predicting Alzheimer's disease: a neuroimaging study with 3D\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition methods using neuroimaging data for the diagnosis of\nAlzheimer's disease have been the subject of extensive research in recent\nyears. In this paper, we use deep learning methods, and in particular sparse\nautoencoders and 3D convolutional neural networks, to build an algorithm that\ncan predict the disease status of a patient, based on an MRI scan of the brain.\nWe report on experiments using the ADNI data set involving 2,265 historical\nscans. We demonstrate that 3D convolutional neural networks outperform several\nother classifiers reported in the literature and produce state-of-art results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:46:40 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Payan", "Adrien", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02512", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira", "title": "The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster\n  Technique", "comments": "4 pages, 2 figures, 2 tables. Congresso Brasileiro de Automatica CBA,\n  Natal, RN, Brazil, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a variant of the classical hierarchical cluster analysis is\nreported. This agglomerative (bottom-up) cluster technique is referred to as\nthe Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkage\nalgorithm where the value of the threshold is conveniently up-dated at each\ninteraction. The superiority of the adaptive clustering with respect to the\naverage-linkage algorithm follows because it achieves a good compromise on\nthreshold values: Thresholds based on the cut-off distance are sufficiently\nsmall to assure the homogeneity and also large enough to guarantee at least a\npair of merging sets. This approach is applied to a set of possible\nsubstituents in a chemical series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:57:58 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["de Oliveira", "H. M.", ""]]}, {"id": "1502.02513", "submitter": "Manuel Martin", "authors": "M.P. Martin, T.G. Orton, E. Lacarce, J. Meersmans, N.P.A. Saby, J.B.\n  Paroissien, C. Jolivet, L. Boulonne, D. Arrouays", "title": "Evaluation of modelling approaches for predicting the spatial\n  distribution of soil organic carbon stocks at the national scale", "comments": null, "journal-ref": "Geoderma, Volumes 223-225, July 2014, Pages 97-107", "doi": "10.1016/j.geoderma.2014.01.005", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil organic carbon (SOC) plays a major role in the global carbon budget. It\ncan act as a source or a sink of atmospheric carbon, thereby possibly\ninfluencing the course of climate change. Improving the tools that model the\nspatial distributions of SOC stocks at national scales is a priority, both for\nmonitoring changes in SOC and as an input for global carbon cycles studies. In\nthis paper, we compare and evaluate two recent and promising modelling\napproaches. First, we considered several increasingly complex boosted\nregression trees (BRT), a convenient and efficient multiple regression model\nfrom the statistical learning field. Further, we considered a robust\ngeostatistical approach coupled to the BRT models. Testing the different\napproaches was performed on the dataset from the French Soil Monitoring\nNetwork, with a consistent cross-validation procedure. We showed that when a\nlimited number of predictors were included in the BRT model, the standalone BRT\npredictions were significantly improved by robust geostatistical modelling of\nthe residuals. However, when data for several SOC drivers were included, the\nstandalone BRT model predictions were not significantly improved by\ngeostatistical modelling. Therefore, in this latter situation, the BRT\npredictions might be considered adequate without the need for geostatistical\nmodelling, provided that i) care is exercised in model fitting and validating,\nand ii) the dataset does not allow for modelling of local spatial\nautocorrelations, as is the case for many national systematic sampling schemes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 15:00:13 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Martin", "M. P.", ""], ["Orton", "T. G.", ""], ["Lacarce", "E.", ""], ["Meersmans", "J.", ""], ["Saby", "N. P. A.", ""], ["Paroissien", "J. B.", ""], ["Jolivet", "C.", ""], ["Boulonne", "L.", ""], ["Arrouays", "D.", ""]]}, {"id": "1502.03035", "submitter": "Anders Bredahl Kock", "authors": "Laurent Callot, Mehmet Caner, Anders Bredahl Kock, Juan Andres\n  Riquelme", "title": "Sharp Threshold Detection Based on Sup-norm Error rates in\n  High-dimensional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator, the thresholded scaled Lasso, in high dimensional\nthreshold regressions. First, we establish an upper bound on the $\\ell_\\infty$\nestimation error of the scaled Lasso estimator of Lee et al. (2012). This is a\nnon-trivial task as the literature on high-dimensional models has focused\nalmost exclusively on $\\ell_1$ and $\\ell_2$ estimation errors. We show that\nthis sup-norm bound can be used to distinguish between zero and non-zero\ncoefficients at a much finer scale than would have been possible using\nclassical oracle inequalities. Thus, our sup-norm bound is tailored to\nconsistent variable selection via thresholding.\n  Our simulations show that thresholding the scaled Lasso yields substantial\nimprovements in terms of variable selection. Finally, we use our estimator to\nshed further empirical light on the long running debate on the relationship\nbetween the level of debt (public and private) and GDP growth.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:48:19 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Callot", "Laurent", ""], ["Caner", "Mehmet", ""], ["Kock", "Anders Bredahl", ""], ["Riquelme", "Juan Andres", ""]]}, {"id": "1502.03062", "submitter": "S. Stanley Young", "authors": "Kenneth K. Lopiano, Richard L. Smith, S. Stanley Young", "title": "Air quality and acute deaths in California, 2000-2012", "comments": "Statistics, epidemiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have sought to determine if there is an association between air\nquality and acute deaths. Many consider it plausible that current levels of air\nquality cause acute deaths. However, several factors call causation and even\nassociation into question. Observational data sets are large and complex.\nMultiple testing and multiple modeling can lead to false positive findings.\nPublication, confirmation and other biases are also possible problems.\n  Moreover, the fact that most data sets used in studies evaluating the\nrelationships among air quality and public health outcomes are not publicly\navailable makes reproducing the claims nearly impossible. Here we have built\nand made publicly available a dataset containing daily air quality levels,\nPM2.5 and ozone, daily temperature levels, minimum and maximum and daily\nrelative humidity levels for the eight most populous California air basins. We\nanalyzed the dataset using a moving median analysis, a standard time series\nanalysis, and a prediction analysis within the following analysis strategy. We\nexamine the eight air basins separately to see if estimates replicate across\nlocations. We use leave one year out cross validation analysis to evaluate\npredictions. Both the moving medians analysis and the standard time series\nanalysis found little evidence for association between air quality and acute\ndeaths. The prediction analysis process was a run as a large factorial design\nusing different models and holding out one year at a time. Among the variables\nused to predict acute death, most of the daily death variability was explained\nby time of year or weather variables. In summary, the empirical evidence is\nthat current levels of air quality, ozone and PM2.5, are not causally related\nto acute deaths for California. An empirical and logical case can be made air\nquality is not causally related to acute deaths for the rest of the United\nStates.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 20:08:14 GMT"}, {"version": "v2", "created": "Sat, 14 Feb 2015 15:34:09 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 20:13:20 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Lopiano", "Kenneth K.", ""], ["Smith", "Richard L.", ""], ["Young", "S. Stanley", ""]]}, {"id": "1502.03153", "submitter": "Robert Krafty", "authors": "Robert T. Krafty, Ori Rosen, David S. Stoffer, Daniel J. Buysse, and\n  Martica H. Hall", "title": "Conditional Spectral Analysis of Replicated Multiple Time Series with\n  Application to Nocturnal Physiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of analyzing associations between power\nspectra of multiple time series and cross-sectional outcomes when data are\nobserved from multiple subjects. The motivating application comes from sleep\nmedicine, where researchers are able to non-invasively record physiological\ntime series signals during sleep. The frequency patterns of these signals,\nwhich can be quantified through the power spectrum, contain interpretable\ninformation about biological processes. An important problem in sleep research\nis drawing connections between power spectra of time series signals and\nclinical characteristics; these connections are key to understanding biological\npathways through which sleep affects, and can be treated to improve, health.\nSuch analyses are challenging as they must overcome the complicated structure\nof a power spectrum from multiple time series as a complex positive-definite\nmatrix-valued function. This article proposes a new approach to such analyses\nbased on a tensor-product spline model of Cholesky components of\noutcome-dependent power spectra. The approach flexibly models power spectra as\nnonparametric functions of frequency and outcome while preserving geometric\nconstraints. Formulated in a fully Bayesian framework, a Whittle likelihood\nbased Markov chain Monte Carlo (MCMC) algorithm is developed for automated\nmodel fitting and for conducting inference on associations between outcomes and\nspectral measures. The method is used to analyze data from a study of sleep in\nolder adults and uncovers new insights into how stress and arousal are\nconnected to the amount of time one spends in bed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:04:05 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 19:00:48 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 23:52:33 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Krafty", "Robert T.", ""], ["Rosen", "Ori", ""], ["Stoffer", "David S.", ""], ["Buysse", "Daniel J.", ""], ["Hall", "Martica H.", ""]]}, {"id": "1502.03169", "submitter": "Jeffrey Leek", "authors": "Jeffrey T. Leek and Roger D. Peng", "title": "Reproducible Research Can Still Be Wrong: Adopting a Prevention Approach", "comments": "3 pages, 1 figure", "journal-ref": "PNAS 112 (6) 1645-1645, 2015", "doi": "10.1073/pnas.1421412111", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility, the ability to recompute results, and replicability, the\nchances other experimenters will achieve a consistent result, are two\nfoundational characteristics of successful scientific research. Consistent\nfindings from independent investigators are the primary means by which\nscientific evidence accumulates for or against an hypothesis. And yet, of late\nthere has been a crisis of confidence among researchers worried about the rate\nat which studies are either reproducible or replicable. In order to maintain\nthe integrity of science research and maintain the public's trust in science,\nthe scientific community must ensure reproducibility and replicability by\nengaging in a more preventative approach that greatly expands data analysis\neducation and routinely employs software tools.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 02:00:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Leek", "Jeffrey T.", ""], ["Peng", "Roger D.", ""]]}, {"id": "1502.03301", "submitter": "Helio M. de Oliveira", "authors": "N.S. Santos-Magalhaes, H.M. de Oliveira and A.J. Alves", "title": "On Preparing a List of Random treatment Assigns", "comments": "8 pages. XIV Reuniao Anual da Federacao de Sociedades de Biologia\n  Experimental FeSBE, Caxamb\\'u, Brazil, 1999", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the foundations of a computer oriented approach for\npreparing a list of random treatment assignments to be adopted in randomised\ncontrolled trials. Software is presented which can be applied in the earliest\nstage of clinical trials and bioequivalence assays. This allocation of patients\nto treatment in clinical trials ensures exactly equal treatment numbers. The\ninvestigation of the randomness properties of an assignment leads to the\nconcept of a 'strong randomised list'. The new approach introduced in this note\nis based on thresholds and produces a strong randomised list of treatment\nassignments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 13:25:57 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Santos-Magalhaes", "N. S.", ""], ["de Oliveira", "H. M.", ""], ["Alves", "A. J.", ""]]}, {"id": "1502.03400", "submitter": "Helio M. de Oliveira", "authors": "R.M. Campello de Souza and H.M. de Oliveira", "title": "Eigensequences for Multiuser Communication over the Real Adder Channel", "comments": "6 pages, 1 figure, 1 table. VI International Telecommunications\n  Symposium (ITS2006)", "journal-ref": null, "doi": "10.1109/ITS.2006.4433415", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-invariant signals under the Discrete Fourier Transform are\ninvestigated, leading to a class of eigenfunctions for the unitary discrete\nFourier operator. Such invariant sequences (eigensequences) are suggested as\nuser signatures over the real adder channel (t-RAC) and a multiuser\ncommunication system over the t-RAC is presented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 18:39:29 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["de Souza", "R. M. Campello", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.03411", "submitter": "Saugata Ghosh", "authors": "Manu Kalia and Saugata Ghosh", "title": "Cross-Correlation in cricket data and RMT", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze cross-correlation between runs scored over a time interval in\ncricket matches of different teams using methods of random matrix theory (RMT).\nWe obtain an ensemble of cross-correlation matrices $C$ from runs scored by\neight cricket playing nations for (i) test cricket from 1877 -2014 (ii)one-day\ninternationals from 1971 -2014 and (iii) seven teams participating in the\nIndian Premier league T20 format (2008-2014) respectively. We find that a\nmajority of the eigenvalues of C fall within the bounds of random matrices\nhaving joint probability distribution $P(x_1...,x_n)=C_{N \\beta} \\,\n\\prod_{j<k}w(x_j)| x_j-x_k |^\\beta$ where $w(x)=x^{N\\beta a}\\exp(-N\\beta b x)$\nand $\\beta$ is the Dyson parameter. The corresponding level density gives\nMarchenko-Pastur (MP) distribution while fluctuations of every participating\nteam agrees with the universal behavior of Gaussian Unitary Ensemble (GUE). We\nanalyze the components of the deviating eigenvalues and find that the largest\neigenvalue corresponds to an influence common to all matches played during\nthese periods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 13:51:03 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Kalia", "Manu", ""], ["Ghosh", "Saugata", ""]]}, {"id": "1502.03494", "submitter": "Joshua Patrick", "authors": "Joshua Patrick, Jane Harvill, and Clifford Hansen", "title": "A semiparametric spatio-temporal model for solar irradiance data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design and operation of a utility scale photovoltaic (PV) power plant depends\non accurate modeling of the power generated, which is highly correlated with\naggregate solar irradiance on the plant's PV modules. At present, aggregate\nsolar irradiance over the area of a typical PV power plant cannot be measured\ndirectly. Rather, irradiance measurements are typically available from a few,\nrelatively small sensors and thus aggregate solar irradiance must be estimated\nfrom these data. As a step towards finding more accurate methods for estimating\naggregate irradiance from avaialble measurements, we evaluate semiparametric\nspatio-temporal models for global horizontal irradiance. Using data from a 1.2\nMW PV plant located in Lanai, Hawaii, we show that a semiparametric model can\nbe more accurate than simple intepolation between sensor locations. We\ninvestigate spatio-temporal models with separable and nonseparable covariance\nstructures and find no evidence to support assuming a separable covariance\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:54:43 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 21:35:43 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Patrick", "Joshua", ""], ["Harvill", "Jane", ""], ["Hansen", "Clifford", ""]]}, {"id": "1502.03609", "submitter": "Juho Kopra", "authors": "Juho Kopra, Tommi H\\\"ark\\\"anen, Hanna Tolonen, Juha Karvanen", "title": "Correcting for non-ignorable missingness in smoking trends", "comments": "in Stat, 2015", "journal-ref": null, "doi": "10.1002/sta4.73", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data missing not at random (MNAR) is a major challenge in survey sampling. We\npropose an approach based on registry data to deal with non-ignorable\nmissingness in health examination surveys. The approach relies on follow-up\ndata available from administrative registers several years after the survey.\nFor illustration we use data on smoking prevalence in Finnish National FINRISK\nstudy conducted in 1972-1997. The data consist of measured survey information\nincluding missingness indicators, register-based background information and\nregister-based time-to-disease survival data. The parameters of missingness\nmechanism are estimable with these data although the original survey data are\nMNAR. The underlying data generation process is modelled by a Bayesian model.\nThe results indicate that the estimated smoking prevalence rates in Finland may\nbe significantly affected by missing data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 11:37:19 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Kopra", "Juho", ""], ["H\u00e4rk\u00e4nen", "Tommi", ""], ["Tolonen", "Hanna", ""], ["Karvanen", "Juha", ""]]}, {"id": "1502.03732", "submitter": "Helio M. de Oliveira", "authors": "N.S. Santos-Magalhaes and H.M. de Oliveira", "title": "Of Protein Size and Genomes", "comments": "6 pages, 1 figure, 5 tables", "journal-ref": "WSEAS Transactions on Mathematics and Computers in Biology and\n  Biomedicine, Issue 2, vol.3, n.2, pp.133-138, 2006", "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach for approximately calculating the number of genes in a genome is\npresented, which takes into account the average protein length expected for the\nspecies. A number of virus, bacterial and eukaryotic genomes are scrutinized.\nGenome figures are presented, which support the average protein size of a\nspecies as a criterion for assessing life complexity. The human gene\ndistribution in the 23 chromosomes is investigated emphasizing the genomic\nrate, the mean 'exon' length, and the mean 'exons per gene'. It is shown that\nstoring all genes of a single human definitely requires less than 12 MB.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 17:03:36 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Santos-Magalhaes", "N. S.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.03813", "submitter": "James Barrett", "authors": "James E. Barrett", "title": "Information-adaptive clinical trials: a selective recruitment design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive design for clinical trials with time-to-event\noutcomes and covariates (which may consist of or include biomarkers). Our\nmethod is based on the expected entropy of the posterior distribution of a\nproportional hazards model. The expected entropy is evaluated as a function of\na patient's covariates, and the information gained due to a patient is defined\nas the decrease in the corresponding entropy. Candidate patients are only\nrecruited onto the trial if they are likely to provide sufficient information.\nPatients with covariates that are deemed uninformative are filtered out. A\nspecial case is where all patients are recruited, and we determine the optimal\ntreatment arm allocation. This adaptive design has the advantage of potentially\nelucidating the relationship between covariates, treatments, and survival\nprobabilities using fewer patients, albeit at the cost of rejecting some\ncandidates. We assess the performance of our adaptive design using data from\nthe German Breast Cancer Study group and numerical simulations of a biomarker\nvalidation trial.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 20:59:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 16:19:45 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 16:06:34 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Barrett", "James E.", ""]]}, {"id": "1502.03853", "submitter": "Manjari Narayan", "authors": "Manjari Narayan, Genevera I. Allen and Steffie Tomson", "title": "Two Sample Inference for Populations of Graphical Models with\n  Applications to Functional Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Gaussian Graphical Models (GGM) are popularly used in neuroimaging studies\nbased on fMRI, EEG or MEG to estimate functional connectivity, or relationships\nbetween remote brain regions. In multi-subject studies, scientists seek to\nidentify the functional brain connections that are different between two groups\nof subjects, i.e. connections present in a diseased group but absent in\ncontrols or vice versa. This amounts to conducting two-sample large scale\ninference over network edges post graphical model selection, a novel problem we\ncall Population Post Selection Inference. Current approaches to this problem\ninclude estimating a network for each subject, and then assuming the subject\nnetworks are fixed, conducting two-sample inference for each edge. These\napproaches, however, fail to account for the variability associated with\nestimating each subject's graph, thus resulting in high numbers of false\npositives and low statistical power. By using resampling and random\npenalization to estimate the post selection variability together with proper\nrandom effects test statistics, we develop a new procedure we call $R^{3}$ that\nsolves these problems. Through simulation studies we show that $R^{3}$ offers\nmajor improvements over current approaches in terms of error control and\nstatistical power. We apply our method to identify functional connections\npresent or absent in autistic subjects using the ABIDE multi-subject fMRI\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 22:48:32 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Narayan", "Manjari", ""], ["Allen", "Genevera I.", ""], ["Tomson", "Steffie", ""]]}, {"id": "1502.03979", "submitter": "Susan Bryan", "authors": "S. R. Bryan, P.H.C. Eilers, B. Li, D. Rizopoulos, K.A. Vermeer, H.G.\n  Lemij and E.M.E.H. Lesaffre", "title": "Bayesian Hierarchical Modeling of Longitudinal Glaucomatous Visual\n  Fields using a Two-Stage Approach", "comments": "29 pages, 2 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach has become increasingly popular because it allows to\nmodel quite complex models via Markov chain Monte Carlo (MCMC) sampling.\nHowever, it is also recognized nowadays that MCMC sampling can become\ncomputationally prohibitive when a complex model needs to be fit to a large\ndata set. To overcome this problem, we applied and extended a recently proposed\ntwo-stage approach to model a complex hierarchical data structure of glaucoma\npatients who participate in an ongoing Dutch study. Glaucoma is one of the\nleading causes of blindness in the world. In order to detect deterioration at\nan early stage, a model for predicting visual fields (VF) in time is needed.\nHence, the true underlying VF progression can be determined, and treatment\nstrategies can then be optimized to prevent further VF loss. Since we were\nunable to fit these data with the classical one-stage approach upon which the\ncurrent popular Bayesian software is based, we made use of the two-stage\nBayesian approach. The considered hierarchical longitudinal model involves\nestimating a large number of random effects and deals with censoring and high\nmeasurement variability. In addition, we extended the approach with tools for\nmodel evaluation\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 13:18:47 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Bryan", "S. R.", ""], ["Eilers", "P. H. C.", ""], ["Li", "B.", ""], ["Rizopoulos", "D.", ""], ["Vermeer", "K. A.", ""], ["Lemij", "H. G.", ""], ["Lesaffre", "E. M. E. H.", ""]]}, {"id": "1502.04047", "submitter": "Vishal Midya", "authors": "Vishal Midya, Sneha Chakraborty, Srijita Manna, Ranjan Sengupta", "title": "On Statistical Analysis of the Pattern of Evolution of Perceived\n  Emotions Induced by Hindustani Music- A Study Based on Listener Responses", "comments": "Conference: Proceedings of 7th International Conference of IMBIC on\n  \"Mathematical Sciences for Advancement of Science and Technology\" (MSAST\n  2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to find the underlying pattern of how\nperception of emotions has evolved in India. Here Hindustani Music has been\nused as a reference frame for tracking the changing perception of emotions. It\nhas been found that different emotions perceived from Hindustani Music form a\nparticular sequential pattern when their corresponding pitch periods are\nanalyzed using the standard deviations and mean successive squared\ndifferences.This sequential pattern of emotions coincides with their\ncorresponding sequential pattern of tempos or average number of steady states.\nOn the basis of this result we further found that the range of perception of\nemotions has diminished significantly these days compared to what it was\nbefore. The proportion of responses for the perceived emotions like Anger,\nSerenity, Romantic and Sorrow has also decreased to a great extent than what it\nwas previously. The proportion of responses for the perceived emotion Anxiety\nhas increased phenomenally. Both standard deviation and mean successive squared\ndifference are two very good measures in tracking the changing perception of\nemotions. The overall pattern of the change of perceived emotions has\ncorresponded to the psychological and sociological change of human life.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 18:32:06 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Midya", "Vishal", ""], ["Chakraborty", "Sneha", ""], ["Manna", "Srijita", ""], ["Sengupta", "Ranjan", ""]]}, {"id": "1502.04058", "submitter": "Kerstin Johnsson", "authors": "Jonas Wallin, Kerstin Johnsson and Magnus Fontes", "title": "Latent modeling of flow cytometry cell populations", "comments": "Supplemental material provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry is a widespread single-cell measurement technology with a\nmultitude of clinical and research applications. Interpretation of flow\ncytometry data is hard; the instrumentation is delicate and can not render\nabsolute measurements, hence samples can only be interpreted in relation to\neach other while at the same time comparisons are confounded by inter-sample\nvariation. Despite this, current automated flow cytometry data analysis methods\neither treat samples individually or ignore the variation by for example\npooling the data. In this article we introduce a Bayesian hierarchical model\nfor studying latent relations between cell populations in flow cytometry\nsamples, thereby systematizing inter-sample variation. The model is applied to\na data set containing replicated flow cytometry measurements of samples from\nhealthy individuals, with informative priors capturing expert knowledge. It is\nshown that the technical variation in the inferred cell population sizes is\nsmall in comparison to the intrinsic biological variation. The large size of\nflow cytometry data, where a single sample can contain measurements on hundreds\nof thousands of cells, necessitates computationally efficient methods. To\naddress this, we have implemented a parallel Markov Chain Monte Carlo scheme\nfor sampling the posterior distribution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 17:04:00 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Wallin", "Jonas", ""], ["Johnsson", "Kerstin", ""], ["Fontes", "Magnus", ""]]}, {"id": "1502.04083", "submitter": "Giulia  Cereda", "authors": "Giulia Cereda", "title": "Impact of model choice on LR assessment in case of rare haplotype match\n  (frequentist approach)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood ratio (LR) measures the relative weight of forensic data\nregarding two hypotheses. Several levels of uncertainty arise if frequentist\nmethods are chosen for its assessment: the assumed population model only\napproximates the true one and its parameters are estimated through a database.\nMoreover, it may be wise to discard part of data, especially that only\nindirectly related to the hypotheses. Different reductions define different\nLRs. Therefore, it is more sensible to talk about \"a\" LR instead of \"the\" LR,\nand the error involved in the estimation should be quantified. Two frequentist\nmethods are proposed in the light of these points for the `rare type match\nproblem', that is when a match between the perpetrator's and the suspect's DNA\nprofile, never observed before in the database of reference, is to be\nevaluated.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 18:46:39 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 13:17:08 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 09:34:48 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 09:31:22 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 08:01:19 GMT"}, {"version": "v6", "created": "Wed, 13 Apr 2016 09:20:39 GMT"}, {"version": "v7", "created": "Fri, 23 Dec 2016 09:09:09 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Cereda", "Giulia", ""]]}, {"id": "1502.04211", "submitter": "Jing Tian", "authors": "Jing Tian, Wei Cui, and Si-liang Wu", "title": "Parameter Estimation of Ground Moving Targets Based on SKT-DLVT\n  Processing", "comments": "39 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the motion of a ground moving target may induce the\nrange cell migration, spectrum spread and velocity ambiguity during the imaging\ntime, which makes the image smeared. To eliminate the influence of these\nfactors on image focusing, a novel method for parameter estimation of ground\nmoving targets, known as SKT-DLVT, is proposed in this paper. In this method,\nthe segmental keystone transform (SKT) is used to correct the range walk of\ntargets simultaneously, and a new transform, namely, Doppler Lv's transform\n(LVT) is applied on the azimuth signal to estimate the parameters. Theoretical\nanalysis confirms that no interpolation is needed for the proposed method and\nthe targets can be well focused within limited searching range of the ambiguity\nnumber. The proposed method is capable of obtaining the accurate parameter\nestimates efficiently in the low signal-to-noise ratio (SNR) scenario with low\ncomputational burden and memory cost, making it suitable to be applied in\nmemory-limited and real-time processing systems. The effectiveness of the\nproposed method is demonstrated by both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 14:08:20 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Tian", "Jing", ""], ["Cui", "Wei", ""], ["Wu", "Si-liang", ""]]}, {"id": "1502.04243", "submitter": "Ben Letham", "authors": "Benjamin Letham and Lydia M. Letham and Cynthia Rudin", "title": "Bayesian Inference of Arrival Rate and Substitution Behavior from Sales\n  Transaction Data with Stockouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an item goes out of stock, sales transaction data no longer reflect the\noriginal customer demand, since some customers leave with no purchase while\nothers substitute alternative products for the one that was out of stock. Here\nwe develop a Bayesian hierarchical model for inferring the underlying customer\narrival rate and choice model from sales transaction data and the corresponding\nstock levels. The model uses a nonhomogeneous Poisson process to allow the\narrival rate to vary throughout the day, and allows for a variety of choice\nmodels. Model parameters are inferred using a stochastic gradient MCMC\nalgorithm that can scale to large transaction databases. We fit the model to\ndata from a local bakery and show that it is able to make accurate\nout-of-sample predictions, and to provide actionable insight into lost cookie\nsales.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 21:08:11 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 05:24:32 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 04:40:17 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Letham", "Benjamin", ""], ["Letham", "Lydia M.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1502.04269", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "comments": "This version reflects our findings on SLIM as of January 2016\n  (arXiv:1306.5860 and arXiv:1405.4047 are out-of-date). The final published\n  version of this articled is available at http://www.springerlink.com", "journal-ref": null, "doi": "10.1007/s10994-015-5528-6", "report-no": null, "categories": "stat.ML cs.DM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are linear classification models that only require users to\nadd, subtract and multiply a few small numbers in order to make a prediction.\nThese models are in widespread use by the medical community, but are difficult\nto learn from data because they need to be accurate and sparse, have coprime\ninteger coefficients, and satisfy multiple operational constraints. We present\na new method for creating data-driven scoring systems called a Supersparse\nLinear Integer Model (SLIM). SLIM scoring systems are built by solving an\ninteger program that directly encodes measures of accuracy (the 0-1 loss) and\nsparsity (the $\\ell_0$-seminorm) while restricting coefficients to coprime\nintegers. SLIM can seamlessly incorporate a wide range of operational\nconstraints related to accuracy and sparsity, and can produce highly tailored\nmodels without parameter tuning. We provide bounds on the testing and training\naccuracy of SLIM scoring systems, and present a new data reduction technique\nthat can improve scalability by eliminating a portion of the training data\nbeforehand. Our paper includes results from a collaboration with the\nMassachusetts General Hospital Sleep Laboratory, where SLIM was used to create\na highly tailored scoring system for sleep apnea screening\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 01:26:41 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 14:46:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 17:34:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1502.04491", "submitter": "Philipp Wilking", "authors": "Philipp Wilking, Randolf R\\\"oseler, Peter Schneider", "title": "Constrained correlation functions from the Millennium Simulation", "comments": "11 pages, 13 figures, updated to match version accepted by A&A", "journal-ref": "A&A 582, A107 (2015)", "doi": "10.1051/0004-6361/201525906", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. In previous work, we developed a quasi-Gaussian approximation for\nthe likelihood of correlation functions, which, in contrast to the usual\nGaussian approach, incorporates fundamental mathematical constraints on\ncorrelation functions. The analytical computation of these constraints is only\nfeasible in the case of correlation functions of one-dimensional random fields.\n  Aims. In this work, we aim to obtain corresponding constraints in the case of\nhigher-dimensional random fields and test them in a more realistic context.\n  Methods. We develop numerical methods to compute the constraints on\ncorrelation functions which are also applicable for two- and three-dimensional\nfields. In order to test the accuracy of the numerically obtained constraints,\nwe compare them to the analytical results for the one-dimensional case.\nFinally, we compute correlation functions from the halo catalog of the\nMillennium Simulation, check whether they obey the constraints, and examine the\nperformance of the transformation used in the construction of the\nquasi-Gaussian likelihood.\n  Results. We find that our numerical methods of computing the constraints are\nrobust and that the correlation functions measured from the Millennium\nSimulation obey them. Despite the fact that the measured correlation functions\nlie well inside the allowed region of parameter space, i.e. far away from the\nboundaries of the allowed volume defined by the constraints, we find strong\nindications that the quasi-Gaussian likelihood yields a substantially more\naccurate description than the Gaussian one.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:01:06 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 07:43:41 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Wilking", "Philipp", ""], ["R\u00f6seler", "Randolf", ""], ["Schneider", "Peter", ""]]}, {"id": "1502.04858", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Corinne Mailhes and Jean-Yves Tourneret and\n  Hichem Snoussi", "title": "Bayesian Estimation of Smooth Altimetric Parameters: Application to\n  Conventional and Delay/Doppler Altimetry", "comments": "30 pages and 6 figures", "journal-ref": null, "doi": "10.1109/TGRS.2015.2497583", "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Bayesian strategy for the smooth estimation of\naltimetric parameters. The altimetric signal is assumed to be corrupted by a\nthermal and speckle noise distributed according to an independent and non\nidentically Gaussian distribution. We introduce a prior enforcing a smooth\ntemporal evolution of the altimetric parameters which improves their physical\ninterpretation. The posterior distribution of the resulting model is optimized\nusing a gradient descent algorithm which allows us to compute the maximum a\nposteriori estimator of the unknown model parameters. This algorithm presents a\nlow computational cost which is suitable for real time applications. The\nproposed Bayesian strategy and the corresponding estimation algorithm are\nvalidated on both synthetic and real data associated with conventional and\ndelay/Doppler altimetry. The analysis of real Jason-2 and Cryosat-2 waveforms\nshows an improvement in parameter estimation when compared to the\nstate-of-the-art estimation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 10:51:48 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Mailhes", "Corinne", ""], ["Tourneret", "Jean-Yves", ""], ["Snoussi", "Hichem", ""]]}, {"id": "1502.05180", "submitter": "Lazhar Benkhelifa", "authors": "Lazhar Benkhelifa", "title": "The Weibull Birnbaum-Saunders Distribution: Properties and Applications", "comments": "28 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1204.1389 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new four-parameter lifetime model called the Weibull\nBirnbaum-Saunders distribution. This new distribution represents a more\nflexible model for the lifetime data. Its failure rate function can be\nincreasing, decreasing, upside-down bathtub shaped, bathtub-shaped or modified\nbathtub shaped depending on its parameters. Some structural properties of the\nproposed model are investigated including expansions for the cumulative and\ndensity functions, moments, generating function, mean deviations, order\nstatistics and reliability. The maximum likelihood estimation method is used to\nestimate the model parameters and the observed information matrix is\ndetermined. The flexibility of the new model is shown by means of two real data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 11:14:24 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2016 07:28:10 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Benkhelifa", "Lazhar", ""]]}, {"id": "1502.05370", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Thakshila Wimalajeewa, Pramod K. Varshney", "title": "Collaborative Compressive Detection with Physical Layer Secrecy\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detecting a high dimensional signal (not\nnecessarily sparse) based on compressed measurements with physical layer\nsecrecy guarantees. First, we propose a collaborative compressive detection\n(CCD) framework to compensate for the performance loss due to compression with\na single sensor. We characterize the trade-off between dimensionality reduction\nachieved by a universal compressive sensing (CS) based measurement scheme and\nthe achievable performance of CCD analytically. Next, we consider a scenario\nwhere the network operates in the presence of an eavesdropper who wants to\ndiscover the state of the nature being monitored by the system. To keep the\ndata secret from the eavesdropper, we propose to use cooperating trustworthy\nnodes that assist the fusion center (FC) by injecting artificial noise to\ndeceive the eavesdropper. We seek the answers to the questions: Does CS help\nimprove the security performance in such a framework? What are the optimal\nvalues of parameters which maximize the CS based collaborative detection\nperformance at the FC while ensuring perfect secrecy at the eavesdropper?\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 20:31:04 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1502.05786", "submitter": "Arpan Mukhopadhyay", "authors": "Arpan Mukhopadhyay, A. Karthik, Ravi R. Mazumdar", "title": "Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of\n  Shared Servers for Low Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the job assignment problem in a multi-server system consisting of\n$N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$)\ndifferent types according to their processing capacity or speed. Jobs of random\nsizes arrive at the system according to a Poisson process with rate $N\n\\lambda$. Upon each arrival, a small number of servers from each type is\nsampled uniformly at random. The job is then assigned to one of the sampled\nservers based on a selection rule. We propose two schemes, each corresponding\nto a specific selection rule that aims at reducing the mean sojourn time of\njobs in the system.\n  We first show that both methods achieve the maximal stability region. We then\nanalyze the system operating under the proposed schemes as $N \\to \\infty$ which\ncorresponds to the mean field. Our results show that asymptotic independence\namong servers holds even when $M$ is finite and exchangeability holds only\nwithin servers of the same type. We further establish the existence and\nuniqueness of stationary solution of the mean field and show that the tail\ndistribution of server occupancy decays doubly exponentially for each server\ntype. When the estimates of arrival rates are not available, the proposed\nschemes offer simpler alternatives to achieving lower mean sojourn time of\njobs, as shown by our numerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 06:51:01 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Mukhopadhyay", "Arpan", ""], ["Karthik", "A.", ""], ["Mazumdar", "Ravi R.", ""]]}, {"id": "1502.05827", "submitter": "Aristides Moustakas", "authors": "Matthew R. Evans, Aristides Moustakas, Gregory Carey, Yadvinder Malhi,\n  Nathalie Butt, Sue Benham, Denise Pallett and Stefanie Schaefer", "title": "Allometry and growth of eight tree taxa in United Kingdom woodlands", "comments": "To appear (in press). Scientific Data 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allometry and growth rates of 8 forest species in the UK. The data were\ncollected from two United Kingdom woodlands - Wytham Woods and Alice Holt. Here\nwe present data from 582 individual trees of eight taxa in the form of summary\nvariables. In addition the raw data files containing the variables from which\nthe summary data were obtained. Large sample sizes with longitudinal data\nspanning 22 years make these datasets useful for future studies concerned with\nthe way trees change in size and shape over their life-span. The allometric\nrelationships include (1) trunk diameter, (2) height, (3) crown height, (4)\ncrown radius and (5) trunk radial growth rate to (A) the light environment of\neach tree and (B) diameter at breast height.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:04:15 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Evans", "Matthew R.", ""], ["Moustakas", "Aristides", ""], ["Carey", "Gregory", ""], ["Malhi", "Yadvinder", ""], ["Butt", "Nathalie", ""], ["Benham", "Sue", ""], ["Pallett", "Denise", ""], ["Schaefer", "Stefanie", ""]]}, {"id": "1502.06197", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "On Online Control of False Discovery Rate", "comments": "31 pages, 6 figures (minor edits)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypotheses testing is a core problem in statistical inference and\narises in almost every scientific field. Given a sequence of null hypotheses\n$\\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\n\\cite{benjamini1995controlling} introduced the false discovery rate (FDR)\ncriterion, which is the expected proportion of false positives among rejected\nnull hypotheses, and proposed a testing procedure that controls FDR below a\npre-assigned significance level. They also proposed a different criterion,\ncalled mFDR, which does not control a property of the realized set of tests;\nrather it controls the ratio of expected number of false discoveries to the\nexpected number of discoveries.\n  In this paper, we propose two procedures for multiple hypotheses testing that\nwe will call \"LOND\" and \"LORD\". These procedures control FDR and mFDR in an\n\\emph{online manner}. Concretely, we consider an ordered --possibly infinite--\nsequence of null hypotheses $\\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each\nstep $i$, the statistician must decide whether to reject hypothesis $H_i$\nhaving access only to the previous decisions. To the best of our knowledge, our\nwork is the first that controls FDR in this setting. This model was introduced\nby Foster and Stine \\cite{alpha-investing} whose alpha-investing rule only\ncontrols mFDR in online manner.\n  In order to compare different procedures, we develop lower bounds on the\ntotal discovery rate under the mixture model and prove that both LOND and LORD\nhave nearly linear number of discoveries. We further propose adjustment to LOND\nto address arbitrary correlation among the $p$-values. Finally, we evaluate the\nperformance of our procedures on both synthetic and real data comparing them\nwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 09:07:07 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 00:39:16 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1502.06557", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 100 (2016) 773-793", "doi": "10.1016/j.csda.2015.11.016", "report-no": null, "categories": "stat.ME q-fin.CP stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 19:14:39 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 23:05:43 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1502.06745", "submitter": "Hammou Elotmany", "authors": "H Elotma (FSSM)", "title": "Parameter estimation for stochastic diffusion process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we propose a new stochastic diffusion process with drift\nproportional to the Weibull density function defined as X $\\epsilon$ = x, dX t\n= $\\gamma$ t (1 - t $\\gamma$+1) - t $\\gamma$ X t dt + $\\sigma$X t dB t , t\n\\textgreater{} 0, with parameters $\\gamma$ \\textgreater{} 0 and $\\sigma$\n\\textgreater{} 0, where B is a standard Brownian motion and t = $\\epsilon$ is a\ntime proche to zero. First we interested to probabilistic solution of this\nprocess as the explicit expression of this process. By using the maximum\nlikelihood method and by considering a discrete sampling of the sample of the\nnew process we estimate the parameters $\\gamma$ and $\\sigma$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 10:11:25 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 13:04:09 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Elotma", "H", "", "FSSM"]]}, {"id": "1502.07199", "submitter": "Meitner Cadena", "authors": "Meitner Cadena", "title": "Mortality Models based on the Transform $\\log(-\\log x)$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stochastic method for describing mortality is proposed and explored. It\nis based on differences of observed times series of the transform $\\log(-\\log\nx)$ of survival probabilities which seem to follow simple patterns over the\nyears. These common structures are gathered by a representation based on\nage-constants and time-stochastic processes. From the projection of the\ntime-processes the mortality forecasting is straigthforward. Comparisons of the\nnew model with the well-known Lee-Carter and Cairns-Blake-Dowd models employing\nsex-based mortality data of some countries are provided. Some in-sample and\nout-of-sample goodness-of-fit criteria show that in some situations the new\nmodel performs better than the ones mentioned above. Assessments of the\nperformance of these models using rates of mortality improvement are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 15:25:22 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Cadena", "Meitner", ""]]}, {"id": "1502.07246", "submitter": "Federico Bassetti", "authors": "Federico Bassetti, Roberto Casarin, Francesco Ravazzolo", "title": "Bayesian Nonparametric Calibration and Combination of Predictive\n  Distributions", "comments": "arXiv admin note: text overlap with arXiv:1305.2026 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach to predictive density calibration and\ncombination that accounts for parameter uncertainty and model set\nincompleteness through the use of random calibration functionals and random\ncombination weights. Building on the work of Ranjan, R. and Gneiting, T. (2010)\nand Gneiting, T. and Ranjan, R. (2013), we use infinite beta mixtures for the\ncalibration. The proposed Bayesian nonparametric approach takes advantage of\nthe flexibility of Dirichlet process mixtures to achieve any continuous\ndeformation of linearly combined predictive distributions. The inference\nprocedure is based on Gibbs sampling and allows accounting for uncertainty in\nthe number of mixture components, mixture weights, and calibration parameters.\nThe weak posterior consistency of the Bayesian nonparametric calibration is\nprovided under suitable conditions for unknown true density. We study the\nmethodology in simulation examples with fat tails and multimodal densities and\napply it to density forecasts of daily S&P returns and daily maximum wind speed\nat the Frankfurt airport.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 17:06:05 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 08:26:56 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bassetti", "Federico", ""], ["Casarin", "Roberto", ""], ["Ravazzolo", "Francesco", ""]]}, {"id": "1502.07436", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Se Un Park, Dennis Wei, Gregory Newstadt, Michael\n  Jackson, Jeff P. Simmons, Marc De Graef and Alfred O. Hero", "title": "A Dictionary Approach to EBSD Indexing", "comments": "This paper is in press in the Journal of Microscopy and\n  Microanalysis, Cambridge University Press, Feb. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for indexing of grain and sub-grain structures in\nelectron backscatter diffraction (EBSD) images of polycrystalline materials.\nThe framework is based on a previously introduced physics-based forward model\nby Callahan and De Graef (2013) relating measured patterns to grain\norientations (Euler angle). The forward model is tuned to the microscope and\nthe sample symmetry group. We discretize the domain of the forward model onto a\ndense grid of Euler angles and for each measured pattern we identify the most\nsimilar patterns in the dictionary. These patterns are used to identify\nboundaries, detect anomalies, and index crystal orientations. The statistical\ndistribution of these closest matches is used in an unsupervised binary\ndecision tree (DT) classifier to identify grain boundaries and anomalous\nregions. The DT classifies a pattern as an anomaly if it has an abnormally low\nsimilarity to any pattern in the dictionary. It classifies a pixel as being\nnear a grain boundary if the highly ranked patterns in the dictionary differ\nsignificantly over the pixels 3x3 neighborhood. Indexing is accomplished by\ncomputing the mean orientation of the closest dictionary matches to each\npattern. The mean orientation is estimated using a maximum likelihood approach\nthat models the orientation distribution as a mixture of Von Mises-Fisher\ndistributions over the quaternionic 3-sphere. The proposed dictionary matching\napproach permits segmentation, anomaly detection, and indexing to be performed\nin a unified manner with the additional benefit of uncertainty quantification.\nWe demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 05:03:49 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:29:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Park", "Se Un", ""], ["Wei", "Dennis", ""], ["Newstadt", "Gregory", ""], ["Jackson", "Michael", ""], ["Simmons", "Jeff P.", ""], ["De Graef", "Marc", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1502.07505", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A mixed effect model for bivariate meta-analysis of diagnostic test\n  accuracy studies using a copula representation of the random effects\n  distribution", "comments": null, "journal-ref": "Statistics in Medicine, 2015, 34(29):3842--3865", "doi": "10.1002/sim.6595", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic test accuracy studies typically report the number of true\npositives, false positives, true negatives and false negatives. There usually\nexists a negative association between the number of true positives and true\nnegatives, because studies that adopt less stringent criterion for declaring a\ntest positive invoke higher sensitivities and lower specificities. A\ngeneralized linear mixed model (GLMM) is currently recommended to synthesize\ndiagnostic test accuracy studies. We propose a copula mixed model for bivariate\nmeta-analysis of diagnostic test accuracy studies. Our general model includes\nthe GLMM as a special case and can also operate on the original scale of\nsensitivity and specificity. Summary receiver operating characteristic curves\nare deduced for the proposed model through quantile regression techniques and\ndifferent characterizations of the bivariate random effects distribution. Our\ngeneral methodology is demonstrated with an extensive simulation study and\nillustrated by re-analysing the data of two published meta-analyses. Our study\nsuggests that there can be an improvement on GLMM in fit to data and makes the\nargument for moving to copula random effects models. Our modelling framework is\nimplemented in the package CopulaREMADA within the open source statistical\nenvironment R.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 11:11:18 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1502.07625", "submitter": "Andrew B Whitford", "authors": "Derrick M. Anderson, Andrew B. Whitford", "title": "Developing Knowledge States: Technology and the Enhancement of National\n  Statistical Capacity", "comments": "32 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National statistical systems are the enterprises tasked with collecting,\nvalidating and reporting societal attributes. These data serve many purposes -\nthey allow governments to improve services, economic actors to traverse\nmarkets, and academics to assess social theories. National statistical systems\nvary in quality, especially in developing countries. This study examines\ndeterminants of national statistical capacity in developing countries, focusing\non the impact of general purpose technologies (GPTs). Just as technological\nprogress helps to explain differences in economic growth, states with markets\nwith greater technological attainment (specifically, general purpose\ntechnologies) arguably have greater capacity for gathering and processing\nquality data. Analysis using panel methods shows a strong, statistically\nsignificant positive linear relationship between GPTs and national statistical\ncapacity. There is no evidence to support a non-linear function in this\nrelationship. Which is to say, there does not appear to be a marginal\ndepreciating National Statistical Capacity benefit associated with increases in\nGPTs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 16:33:46 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Anderson", "Derrick M.", ""], ["Whitford", "Andrew B.", ""]]}, {"id": "1502.07959", "submitter": "Korbinian Strimmer", "authors": "Sebastian Gibb and Korbinian Strimmer", "title": "Differential protein expression and peak selection in mass spectrometry\n  data by binary discriminant analysis", "comments": "18 pages, 4 figures", "journal-ref": "Bionformatics 2015, Vol. 31, 3156-3162", "doi": "10.1093/bioinformatics/btv334", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Proteomic mass spectrometry analysis is becoming routine in\nclinical diagnostics, for example to monitor cancer biomarkers using blood\nsamples. However, differential proteomics and identification of peaks relevant\nfor class separation remains challenging.\n  Results: Here, we introduce a simple yet effective approach for identifying\ndifferentially expressed proteins using binary discriminant analysis. This\napproach works by data-adaptive thresholding of protein expression values and\nsubsequent ranking of the dichotomized features using a relative entropy\nmeasure. Our framework may be viewed as a generalization of the `peak\nprobability contrast' approach of Tibshirani et al. (2004) and can be applied\nboth in the two-group and the multi-group setting.\n  Our approach is computationally inexpensive and shows in the analysis of a\nlarge-scale drug discovery test data set equivalent prediction accuracy as a\nrandom forest. Furthermore, we were able to identify in the analysis of mass\nspectrometry data from a pancreas cancer study biological relevant and\nstatistically predictive marker peaks unrecognized in the original study.\n  Availability: The methodology for binary discriminant analysis is implemented\nin the R package binda, which is freely available under the GNU General Public\nLicense (version 3 or later) from CRAN at URL\nhttp://cran.r-project.org/web/packages/binda/ . R scripts reproducing all\ndescribed analyzes are available from the web page\nhttp://strimmerlab.org/software/binda/ .\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 16:33:10 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 16:24:27 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gibb", "Sebastian", ""], ["Strimmer", "Korbinian", ""]]}]