[{"id": "1102.0242", "submitter": "Sotiris Adamakis", "authors": "S. Adamakis, C. L. Raftery, R. W. Walsh, P. T. Gallagher", "title": "A Bayesian approach to comparing theoretic models to observational data:\n  A case study from solar flare physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar flares are large-scale releases of energy in the solar atmosphere,\nwhich are characterised by rapid changes in the hydrodynamic properties of\nplasma from the photosphere to the corona. Solar physicists have typically\nattempted to understand these complex events using a combination of theoretical\nmodels and observational data. From a statistical perspective, there are many\nchallenges associated with making accurate and statistically significant\ncomparisons between theory and observations, due primarily to the large number\nof free parameters associated with physical models. This class of ill-posed\nstatistical problem is ideally suited to Bayesian methods. In this paper, the\nsolar flare studied by Raftery et al. (2009) is reanalysed using a Bayesian\nframework. This enables us to study the evolution of the flare's temperature,\nemission measure and energy loss in a statistically self-consistent manner. The\nBayesian-based model selection techniques imply that no decision can be made\nregarding which of the conductive or non-thermal beam heating play the most\nimportant role in heating the flare plasma during the impulsive phase of this\nevent.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 18:30:14 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2011 16:54:09 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2012 18:00:21 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Adamakis", "S.", ""], ["Raftery", "C. L.", ""], ["Walsh", "R. W.", ""], ["Gallagher", "P. T.", ""]]}, {"id": "1102.0630", "submitter": "Nicolai Bissantz", "authors": "Nicolai Bissantz, Hajo Holzmann, Miros{\\l}aw Pawlak", "title": "Improving PSF calibration in confocal microscopic imaging---estimating\n  and exploiting bilateral symmetry", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS343 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 1871-1891", "doi": "10.1214/10-AOAS343", "report-no": "IMS-AOAS-AOAS343", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for estimating the axis of reflectional symmetry of an image\n$f(x,y)$ on the unit disc $D=\\{(x,y):x^2+y^2\\leq1\\}$ is proposed, given that\nnoisy data of $f(x,y)$ are observed on a discrete grid of edge width $\\Delta$.\nOur estimation procedure is based on minimizing over $\\beta\\in[0,\\pi)$ the\n$L_2$ distance between empirical versions of $f$ and $\\tau_{\\beta}f$, the image\nof $f$ after reflection at the axis along $(\\cos\\beta,\\sin\\beta)$. Here, $f$\nand $\\tau_{\\beta}f$ are estimated using truncated radial series of the Zernike\ntype. The inherent symmetry properties of the Zernike functions result in a\nparticularly simple estimation procedure for $\\beta$. It is shown that the\nestimate $\\hat{\\beta}$ converges at the parametric rate $\\Delta^{-1}$ for\nimages $f$ of bounded variation. Further, we establish asymptotic normality of\n$\\hat{\\beta}$ if $f$ is Lipschitz continuous. The method is applied to\ncalibrating the point spread function (PSF) for the deconvolution of images\nfrom confocal microscopy. For various reasons the PSF characterizing the\nproblem may not be rotationally invariant but rather only reflection symmetric\nwith respect to two orthogonal axes. For an image of a bead acquired by a\nconfocal laser scanning microscope (Leica TCS), these axes are estimated and\ncorresponding confidence intervals are constructed. They turn out to be close\nto the coordinate axes of the imaging device. As cause for deviation from\nrotational invariance, this indicates some slight misalignment of the optical\nsystem or anisotropy of the immersion medium rather than some irregular shape\nof the bead. In an extensive simulation study, we show that using a symmetrized\nversion of the observed PSF significantly improves the subsequent\nreconstruction process of the target image.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 09:30:58 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Bissantz", "Nicolai", ""], ["Holzmann", "Hajo", ""], ["Pawlak", "Miros\u0142aw", ""]]}, {"id": "1102.2015", "submitter": "Raydonal Ospina", "authors": "Lutemberg Florencio, Francisco Cribari-Neto, Raydonal Ospina", "title": "Real estate appraisal of land lots using GAMLSS models", "comments": "19 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The valuation of real estates (e.g., house, land, among others) is of extreme\nimportance for decision making. Their singular characteristics make valuation\nthrough hedonic pricing methods dificult since the theory does not specify the\ncorrect regression functional form nor which explanatory variables should be\nincluded in the hedonic equation. In this article we perform real estate\nappraisal using a class of regression models proposed by Rigby & Stasinopoulos\n(2005): generalized additive models for location, scale and shape (GAMLSS). Our\nempirical analysis shows that these models seem to be more appropriate for\nestimation of the hedonic prices function than the regression models currently\nused to that end.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 00:24:21 GMT"}, {"version": "v2", "created": "Tue, 15 Feb 2011 20:27:23 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2011 00:06:53 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2011 21:25:04 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Florencio", "Lutemberg", ""], ["Cribari-Neto", "Francisco", ""], ["Ospina", "Raydonal", ""]]}, {"id": "1102.2322", "submitter": "Matthew Sperrin", "authors": "Matthew Sperrin and Iain Buchan", "title": "Modelling time to event with observations made at arbitrary times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new methods of analysing time to event data via extended\nversions of the proportional hazards and accelerated failure time (AFT) models.\nIn many time to event studies, the time of first observation is arbitrary, in\nthe sense that no risk modifying event occurs. This is particularly common in\nepidemiological studies. We show formally that, in these situations, it is not\nsensible to take the first observation as the time origin, either in AFT or\nproportional hazards type models. Instead, we advocate using age of the subject\nas the time scale. We account for the fact that baseline observations may be\nmade at different ages in different patients via a two stage procedure. First,\nwe marginally regress any potentially age-varying covariates against age,\nretaining the residuals. These residuals are then used as covariates in the\nfitting of either an AFT model or a proportional hazards model. We call the\nprocedures residual accelerated failure time (RAFT) regression and residual\nproportional hazards (RPH) regression respectively. We compare standard AFT\nwith RAFT, and demonstrate superior predictive ability of RAFT in real\nexamples. In epidemiology, this has real implications in terms of risk\ncommunication to both patients and policy makers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Feb 2011 10:51:10 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Sperrin", "Matthew", ""], ["Buchan", "Iain", ""]]}, {"id": "1102.2407", "submitter": "Giulio Palombo", "authors": "Giulio Palombo", "title": "Multivariate Goodness of Fit Procedures for Unbinned Data: An Annotated\n  Bibliography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbinned maximum likelihood is a common procedure for parameter estimation.\nAfter parameters have been estimated, it is crucial to know whether the fit\nmodel adequately describes the experimental data. Univariate Goodness of Fit\nprocedures have been thoroughly analyzed. In multi-dimensions, Goodness of Fit\ntest powers have rarely been studied on realistic problems. There is no\ndefinitive answer to regarding which method is better. Test performance is\nstrictly related to specific analysis characteristics. In this work, a review\nof multi-variate Goodness of Fit techniques is presented.\n", "versions": [{"version": "v1", "created": "Fri, 11 Feb 2011 18:16:50 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Palombo", "Giulio", ""]]}, {"id": "1102.3582", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Pavel Shevchenko, Mark Young, Wendy Yip", "title": "Analytic Loss Distributional Approach Model for Operational Risk from\n  the alpha-Stable Doubly Stochastic Compound Processes and Implications for\n  Capital Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach is not prescriptive regarding the class of statistical\nmodel utilised to undertake capital estimation. It has however become well\naccepted to utlise a Loss Distributional Approach (LDA) paradigm to model the\nindividual OpRisk loss process corresponding to the Basel II Business\nline/event type. In this paper we derive a novel class of doubly stochastic\nalpha-stable family LDA models. These models provide the ability to capture the\nheavy tailed loss process typical of OpRisk whilst also providing analytic\nexpressions for the compound process annual loss density and distributions as\nwell as the aggregated compound process annual loss models. In particular we\ndevelop models of the annual loss process in two scenarios. The first scenario\nconsiders the loss process with a stochastic intensity parameter, resulting in\nan inhomogeneous compound Poisson processes annually. The resulting arrival\nprocess of losses under such a model will have independent counts over\nincrements within the year. The second scenario considers discretization of the\nannual loss process into monthly increments with dependent time increments as\ncaptured by a Binomial process with a stochastic probability of success\nchanging annually. Each of these models will be coupled under an LDA framework\nwith heavy-tailed severity models comprised of $\\alpha$-stable severities for\nthe loss amounts per loss event. In this paper we will derive analytic results\nfor the annual loss distribution density and distribution under each of these\nmodels and study their properties.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 13:38:56 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel", ""], ["Young", "Mark", ""], ["Yip", "Wendy", ""]]}, {"id": "1102.3712", "submitter": "Rafal Weron", "authors": "Joanna Janczura, Rafal Weron", "title": "Black swans or dragon kings? A simple test for deviations from the power\n  law", "comments": null, "journal-ref": null, "doi": "10.1140/epjst/e2012-01563-9", "report-no": null, "categories": "q-fin.ST physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simple test for deviations from power law tails, which is based\non the asymptotic properties of the empirical distribution function. We use\nthis test to answer the question whether great natural disasters, financial\ncrashes or electricity price spikes should be classified as dragon kings or\n'only' as black swans.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 21:58:06 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Janczura", "Joanna", ""], ["Weron", "Rafal", ""]]}, {"id": "1102.3939", "submitter": "Harpreet S. Dhillon", "authors": "Harpreet S. Dhillon, Jeong-O Jeong, Dinesh Datla, Michael Benonis, R.\n  Michael Buehrer and Jeffrey H. Reed", "title": "A Sub-Space Method to Detect Multiple Wireless Microphone Signals in TV\n  Band White Space", "comments": "To appear in SPRINGER: Analog Integrated Circuits and Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main hurdle in the realization of dynamic spectrum access (DSA) systems\nfrom physical layer perspective is the reliable sensing of low power licensed\nusers. One such scenario shows up in the unlicensed use of TV bands where the\nTV Band Devices (TVBDs) are required to sense extremely low power wireless\nmicrophones (WMs). The lack of technical standard among various wireless\nmanufacturers and the resemblance of certain WM signals to narrow-band\ninterference signals, such as spurious emissions, further aggravate the\nproblem. Due to these uncertainties, it is extremely difficult to abstract the\nfeatures of WM signals and hence develop robust sensing algorithms. To partly\ncounter these challenges, we develop a two-stage sub-space algorithm that\ndetects multiple narrow-band analog frequency-modulated signals generated by\nWMs. The performance of the algorithm is verified by using experimentally\ncaptured low power WM signals with received power ranging from -100 to -105\ndBm. The problem of differentiating between the WM and other narrow-band\nsignals is left as a future work.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 23:51:59 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Dhillon", "Harpreet S.", ""], ["Jeong", "Jeong-O", ""], ["Datla", "Dinesh", ""], ["Benonis", "Michael", ""], ["Buehrer", "R. Michael", ""], ["Reed", "Jeffrey H.", ""]]}, {"id": "1102.4003", "submitter": "Piet Groeneboom", "authors": "Piet Groeneboom", "title": "Likelihood ratio type two-sample tests for current status data", "comments": "38 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce fully nonparametric two-sample tests for testing the null\nhypothesis that the samples come from the same distribution if the values are\nonly indirectly given via current status censoring. The tests are based on the\nlikelihood ratio principle and allow the observation distributions to be\ndifferent for the two samples, in contrast with earlier proposals for this\nsituation. A bootstrap method is given for determining critical values and\nasymptotic theory is developed. A simulation study, using Weibull\ndistributions, is presented to compare the power behavior of the tests with the\npower of other nonparametric tests in this situation.\n", "versions": [{"version": "v1", "created": "Sat, 19 Feb 2011 16:57:39 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2011 13:50:22 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2011 15:42:16 GMT"}, {"version": "v4", "created": "Tue, 29 Nov 2011 18:03:42 GMT"}, {"version": "v5", "created": "Thu, 1 Dec 2011 13:21:10 GMT"}, {"version": "v6", "created": "Sat, 23 Jun 2012 04:59:31 GMT"}, {"version": "v7", "created": "Thu, 11 Jul 2013 15:01:17 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Groeneboom", "Piet", ""]]}, {"id": "1102.4101", "submitter": "Cosma Rohilla Shalizi", "authors": "Cosma Rohilla Shalizi", "title": "Scaling and Hierarchy in Urban Economies", "comments": "v1: 15 pages, 9 figures, combines main text and supporting\n  information into one document. Submitted to PNAS. v2: Text re-arranged to\n  comply with journal policies; added analysis with logistic (asymptotically\n  constant) scaling relations; minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several recent publications, Bettencourt, West and collaborators claim\nthat properties of cities such as gross economic production, personal income,\nnumbers of patents filed, number of crimes committed, etc., show super-linear\npower-scaling with total population, while measures of resource use show\nsub-linear power-law scaling. Re-analysis of the gross economic production and\npersonal income for cities in the United States, however, shows that the data\ncannot distinguish between power laws and other functional forms, including\nlogarithmic growth, and that size predicts relatively little of the variation\nbetween cities. The striking appearance of scaling in previous work is largely\nartifact of using extensive quantities (city-wide totals) rather than intensive\nones (per-capita rates). The remaining dependence of productivity on city size\nis explained by concentration of specialist service industries, with high\nvalue-added per worker, in larger cities, in accordance with the long-standing\neconomic notion of the \"hierarchy of central places\".\n", "versions": [{"version": "v1", "created": "Sun, 20 Feb 2011 20:17:02 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2011 01:01:01 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1102.4110", "submitter": "Eric F. Lock", "authors": "Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel", "title": "Joint and individual variation explained (JIVE) for integrated analysis\n  of multiple data types", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS597 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 523-542", "doi": "10.1214/12-AOAS597", "report-no": "IMS-AOAS-AOAS597", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in several fields now requires the analysis of data sets in which\nmultiple high-dimensional types of data are available for a common set of\nobjects. In particular, The Cancer Genome Atlas (TCGA) includes data from\nseveral diverse genomic technologies on the same cancerous tumor samples. In\nthis paper we introduce Joint and Individual Variation Explained (JIVE), a\ngeneral decomposition of variation for the integrated analysis of such data\nsets. The decomposition consists of three terms: a low-rank approximation\ncapturing joint variation across data types, low-rank approximations for\nstructured variation individual to each data type, and residual noise. JIVE\nquantifies the amount of joint variation between data types, reduces the\ndimensionality of the data and provides new directions for the visual\nexploration of joint and individual structures. The proposed method represents\nan extension of Principal Component Analysis and has clear advantages over\npopular two-block methods such as Canonical Correlation Analysis and Partial\nLeast Squares. A JIVE analysis of gene expression and miRNA data on\nGlioblastoma Multiforme tumor samples reveals gene-miRNA associations and\nprovides better characterization of tumor types. Data and software are\navailable at https://genome.unc.edu/jive/\n", "versions": [{"version": "v1", "created": "Sun, 20 Feb 2011 23:52:20 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 06:15:04 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Lock", "Eric F.", ""], ["Hoadley", "Katherine A.", ""], ["Marron", "J. S.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1102.4210", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist, Hans R. K\\\"unsch, Werner A. Stahel", "title": "A dynamic nonstationary spatio-temporal model for short term prediction\n  of precipitation", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS564 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1452-1477", "doi": "10.1214/12-AOAS564", "report-no": "IMS-AOAS-AOAS564", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precipitation is a complex physical process that varies in space and time.\nPredictions and interpolations at unobserved times and/or locations help to\nsolve important problems in many areas. In this paper, we present a\nhierarchical Bayesian model for spatio-temporal data and apply it to obtain\nshort term predictions of rainfall. The model incorporates physical knowledge\nabout the underlying processes that determine rainfall, such as advection,\ndiffusion and convection. It is based on a temporal autoregressive convolution\nwith spatially colored and temporally white innovations. By linking the\nadvection parameter of the convolution kernel to an external wind vector, the\nmodel is temporally nonstationary. Further, it allows for nonseparable and\nanisotropic covariance structures. With the help of the Voronoi tessellation,\nwe construct a natural parametrization, that is, space as well as time\nresolution consistent, for data lying on irregular grid points. In the\napplication, the statistical model combines forecasts of three other\nmeteorological variables obtained from a numerical weather prediction model\nwith past precipitation observations. The model is then used to predict\nthree-hourly precipitation over 24 hours. It performs better than a separable,\nstationary and isotropic version, and it performs comparably to a deterministic\nnumerical weather prediction model for precipitation and has the advantage that\nit quantifies prediction uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 21 Feb 2011 12:57:56 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 15:06:11 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2011 07:01:13 GMT"}, {"version": "v4", "created": "Fri, 27 Apr 2012 07:16:29 GMT"}, {"version": "v5", "created": "Tue, 29 May 2012 12:29:25 GMT"}, {"version": "v6", "created": "Wed, 16 Jan 2013 12:34:51 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Sigrist", "Fabio", ""], ["K\u00fcnsch", "Hans R.", ""], ["Stahel", "Werner A.", ""]]}, {"id": "1102.4610", "submitter": "Vinay Kashyap", "authors": "Hyunsook Lee, Vinay L. Kashyap, David A. van Dyk, Alanna Connors,\n  Jeremy J. Drake, Rima Izem, Xiao-Li Meng, Shandong Min, Taeyoung Park, Pete\n  Ratzlaff, Aneta Siemiginowska, Andreas Zezas", "title": "Accounting for Calibration Uncertainties in X-ray Analysis: Effective\n  Areas in Spectral Fitting", "comments": "61 pages double spaced, 8 figures, accepted for publication in ApJ", "journal-ref": "The Astrophysical Journal (2011), 731, 126", "doi": "10.1088/0004-637X/731/2/126", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While considerable advance has been made to account for statistical\nuncertainties in astronomical analyses, systematic instrumental uncertainties\nhave been generally ignored. This can be crucial to a proper interpretation of\nanalysis results because instrumental calibration uncertainty is a form of\nsystematic uncertainty. Ignoring it can underestimate error bars and introduce\nbias into the fitted values of model parameters. Accounting for such\nuncertainties currently requires extensive case-specific simulations if using\nexisting analysis packages. Here we present general statistical methods that\nincorporate calibration uncertainties into spectral analysis of high-energy\ndata. We first present a method based on multiple imputation that can be\napplied with any fitting method, but is necessarily approximate. We then\ndescribe a more exact Bayesian approach that works in conjunction with a Markov\nchain Monte Carlo based fitting. We explore methods for improving computational\nefficiency, and in particular detail a method of summarizing calibration\nuncertainties with a principal component analysis of samples of plausible\ncalibration files. This method is implemented using recently codified Chandra\neffective area uncertainties for low-resolution spectral analysis and is\nverified using both simulated and actual Chandra data. Our procedure for\nincorporating effective area uncertainty is easily generalized to other types\nof calibration uncertainties.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 20:51:04 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Lee", "Hyunsook", ""], ["Kashyap", "Vinay L.", ""], ["van Dyk", "David A.", ""], ["Connors", "Alanna", ""], ["Drake", "Jeremy J.", ""], ["Izem", "Rima", ""], ["Meng", "Xiao-Li", ""], ["Min", "Shandong", ""], ["Park", "Taeyoung", ""], ["Ratzlaff", "Pete", ""], ["Siemiginowska", "Aneta", ""], ["Zezas", "Andreas", ""]]}, {"id": "1102.4803", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Detection of objects in noisy images and site percolation on square\n  lattices", "comments": "This paper first appeared as EURANDOM Report 2009-035 on November 11,\n  2009. Link to the paper at the EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2009/035-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2009/035-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2009-035", "categories": "math.ST cs.CV math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic method for detection of objects in noisy\nimages. The method uses results from percolation and random graph theories. We\npresent an algorithm that allows to detect objects of unknown shapes in the\npresence of random noise. Our procedure substantially differs from\nwavelets-based algorithms. The algorithm has linear complexity and exponential\naccuracy and is appropriate for real-time systems. We prove results on\nconsistency and algorithmic complexity of our procedure.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 17:28:21 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4811", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Robust nonparametric detection of objects in noisy images", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-049.\n  Link to the abstract at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/049-abstract.pdf Link to the paper at\n  EURANDOM repository: http://www.eurandom.tue.nl/reports/2010/049-report.pdf", "journal-ref": "Journal of Nonparametric Statistics (2013), Vol. 25(2), pp.\n  409-426", "doi": "10.1080/10485252.2012.759570", "report-no": "EURANDOM Report 2010-049", "categories": "math.ST math-ph math.MP math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel statistical hypothesis testing method for detection of\nobjects in noisy images. The method uses results from percolation theory and\nrandom graph theory. We present an algorithm that allows to detect objects of\nunknown shapes in the presence of nonparametric noise of unknown level and of\nunknown distribution. No boundary shape constraints are imposed on the object,\nonly a weak bulk condition for the object's interior is required. The algorithm\nhas linear complexity and exponential accuracy and is appropriate for real-time\nsystems. In this paper, we develop further the mathematical formalism of our\nmethod and explore important connections to the mathematical theory of\npercolation and statistical physics. We prove results on consistency and\nalgorithmic complexity of our testing procedure. In addition, we address not\nonly an asymptotic behavior of the method, but also a finite sample performance\nof our test.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:21:58 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4816", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Computationally efficient algorithms for statistical image processing.\n  Implementation in R", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-053.\n  Link to EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2010-053", "categories": "stat.CO cs.CV stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the series of our earlier papers on the subject, we proposed a novel\nstatistical hypothesis testing method for detection of objects in noisy images.\nThe method uses results from percolation theory and random graph theory. We\ndeveloped algorithms that allowed to detect objects of unknown shapes in the\npresence of nonparametric noise of unknown level and of unknown distribution.\nNo boundary shape constraints were imposed on the objects, only a weak bulk\ncondition for the object's interior was required. Our algorithms have linear\ncomplexity and exponential accuracy. In the present paper, we describe an\nimplementation of our nonparametric hypothesis testing method. We provide a\nprogram that can be used for statistical experiments in image processing. This\nprogram is written in the statistical programming language R.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:46:56 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.5094", "submitter": "Bulent Kiziltan", "authors": "Bulent Kiziltan", "title": "Reassessing The Fundamentals: New Constraints on the Evolution, Ages and\n  Masses of Neutron Stars", "comments": "4 pages, 4 figures; To appear in the AIP proceedings of \"Astrophysics\n  of Neutron Stars-2010\", eds. E. Gogus, T. Belloni, U. Ertan", "journal-ref": null, "doi": "10.1063/1.3629483", "report-no": null, "categories": "astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ages and masses of neutron stars (NSs) are two fundamental threads that\nmake pulsars accessible to other sub-disciplines of astronomy and physics. A\nrealistic and accurate determination of these two derived parameters play an\nimportant role in understanding of advanced stages of stellar evolution and the\nphysics that govern relevant processes. Here I summarize new constraints on the\nages and masses of NSs with an evolutionary perspective. I show that the\nobserved P-Pdot demographics is more diverse than what is theoretically\npredicted for the standard evolutionary channel. In particular, standard\nrecycling followed by dipole spin-down fails to reproduce the population of\nmillisecond pulsars with higher magnetic fields (B > 4 x 10^{8} G) at rates\ndeduced from observations. A proper inclusion of constraints arising from\nbinary evolution and mass accretion offers a more realistic insight into the\nage distribution. By analytically implementing these constraints, I propose a\n\"modified\" spin-down age for millisecond pulsars that gives estimates closer to\nthe true age. Finally, I independently analyze the peak, skewness and cutoff\nvalues of the underlying mass distribution from a comprehensive list of radio\npulsars for which secure mass measurements are available. The inferred mass\ndistribution shows clear peaks at 1.35 Msun and 1.50 Msun for NSs in double\nneutron star (DNS) and neutron star-white dwarf (NS-WD) systems respectively. I\nfind a mass cutoff at 2 Msun for NSs with WD companions, which establishes a\nfirm lower bound for the maximum mass of NSs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 21:00:03 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Kiziltan", "Bulent", ""]]}, {"id": "1102.5288", "submitter": "Derin Babacan", "authors": "S. Derin Babacan, Martin Luessi, Rafael Molina, Aggelos K. Katsaggelos", "title": "Sparse Bayesian Methods for Low-Rank Matrix Estimation", "comments": "This paper has been withdrawn by the author due to significant\n  revisions in the paper. The new version will be uploaded soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery of low-rank matrices has recently seen significant activity in many\nareas of science and engineering, motivated by recent theoretical results for\nexact reconstruction guarantees and interesting practical applications. A\nnumber of methods have been developed for this recovery problem. However, a\nprincipled method for choosing the unknown target rank is generally not\nprovided. In this paper, we present novel recovery algorithms for estimating\nlow-rank matrices in matrix completion and robust principal component analysis\nbased on sparse Bayesian learning (SBL) principles. Starting from a matrix\nfactorization formulation and enforcing the low-rank constraint in the\nestimates as a sparsity constraint, we develop an approach that is very\neffective in determining the correct rank while providing high recovery\nperformance. We provide connections with existing methods in other similar\nproblems and empirical results and comparisons with current state-of-the-art\nmethods that illustrate the effectiveness of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Feb 2011 17:13:00 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 19:06:10 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Babacan", "S. Derin", ""], ["Luessi", "Martin", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1102.5496", "submitter": "Ronny Luss", "authors": "Ronny Luss, Saharon Rosset, Moni Shahar", "title": "Efficient regularized isotonic regression with application to gene--gene\n  interaction search", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS504 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 253-283", "doi": "10.1214/11-AOAS504", "report-no": "IMS-AOAS-AOAS504", "categories": "stat.ME cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic regression is a nonparametric approach for fitting monotonic models\nto data that has been widely studied from both theoretical and practical\nperspectives. However, this approach encounters computational and statistical\noverfitting issues in higher dimensions. To address both concerns, we present\nan algorithm, which we term Isotonic Recursive Partitioning (IRP), for isotonic\nregression based on recursively partitioning the covariate space through\nsolution of progressively smaller \"best cut\" subproblems. This creates a\nregularized sequence of isotonic models of increasing model complexity that\nconverges to the global isotonic regression solution. The models along the\nsequence are often more accurate than the unregularized isotonic regression\nmodel because of the complexity control they offer. We quantify this complexity\ncontrol through estimation of degrees of freedom along the path. Success of the\nregularized models in prediction and IRPs favorable computational properties\nare demonstrated through a series of simulated and real data experiments. We\ndiscuss application of IRP to the problem of searching for gene--gene\ninteractions and epistasis, and demonstrate it on data from genome-wide\nassociation studies of three common diseases.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 12:13:52 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2012 06:23:50 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Luss", "Ronny", ""], ["Rosset", "Saharon", ""], ["Shahar", "Moni", ""]]}, {"id": "1102.5509", "submitter": "Leo Lahti", "authors": "Leo Lahti", "title": "Probabilistic analysis of the human transcriptome with side information", "comments": "Doctoral thesis. 103 pages, 11 figures", "journal-ref": "TKK Dissertations in Information and Computer Science TKK-ICS-D19.\n  Aalto University School of Science and Technology, Department of Information\n  and Computer Science, Espoo, Finland, 2010", "doi": null, "report-no": "TKK-ICS-D19", "categories": "stat.ML cs.CE q-bio.GN q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Understanding functional organization of genetic information is a major\nchallenge in modern biology. Following the initial publication of the human\ngenome sequence in 2001, advances in high-throughput measurement technologies\nand efficient sharing of research material through community databases have\nopened up new views to the study of living organisms and the structure of life.\nIn this thesis, novel computational strategies have been developed to\ninvestigate a key functional layer of genetic information, the human\ntranscriptome, which regulates the function of living cells through protein\nsynthesis. The key contributions of the thesis are general exploratory tools\nfor high-throughput data analysis that have provided new insights to\ncell-biological networks, cancer mechanisms and other aspects of genome\nfunction.\n  A central challenge in functional genomics is that high-dimensional genomic\nobservations are associated with high levels of complex and largely unknown\nsources of variation. By combining statistical evidence across multiple\nmeasurement sources and the wealth of background information in genomic data\nrepositories it has been possible to solve some the uncertainties associated\nwith individual observations and to identify functional mechanisms that could\nnot be detected based on individual measurement sources. Statistical learning\nand probabilistic models provide a natural framework for such modeling tasks.\nOpen source implementations of the key methodological contributions have been\nreleased to facilitate further adoption of the developed methods by the\nresearch community.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 14:11:30 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Lahti", "Leo", ""]]}, {"id": "1102.5549", "submitter": "Gagan Sidhu", "authors": "Gagan Sidhu", "title": "Instant Replay: Investigating statistical Analysis in Sports", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has had an unquestionable impact on the way people watch sports.\nAlong with this technological evolution has come a higher standard to ensure a\ngood viewing experience for the casual sports fan. It can be argued that the\npervasion of statistical analysis in sports serves to satiate the fan's desire\nfor detailed sports statistics. The goal of statistical analysis in sports is a\nsimple one: to eliminate subjective analysis. In this paper, we review previous\nwork that attempts to analyze various aspects in sports by using ideas from\nMarkov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods.\nThe unifying goal of these works is to achieve an accurate representation of\nthe player's ability, the sport, or the environmental effects on the player's\nperformance. With the prevalence of cheap computation, it is possible that\nusing techniques in Artificial Intelligence could improve the result of\nstatistical analysis in sport. This is best illustrated when evaluating\nfootball using Neuro Dynamic Programming, a Control Theory paradigm heavily\nbased on theory in Stochastic processes. The results from this method suggest\nthat statistical analysis in sports may benefit from using ideas from the area\nof Control Theory or Machine Learning\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 21:16:56 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 20:40:42 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2011 16:56:19 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2011 04:20:03 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Sidhu", "Gagan", ""]]}, {"id": "1102.5665", "submitter": "William Shaw", "authors": "William T. Shaw", "title": "Risk, VaR, CVaR and their associated Portfolio Optimizations when Asset\n  Returns have a Multivariate Student T Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to reduce the problem of computing VaR and CVaR with Student T\nreturn distributions to evaluation of analytical functions of the moments. This\nallows an analysis of the risk properties of systems to be carefully attributed\nbetween choices of risk function (e.g. VaR vs CVaR); choice of return\ndistribution (power law tail vs Gaussian) and choice of event frequency, for\nrisk assessment. We exploit this to provide a simple method for portfolio\noptimization when the asset returns follow a standard multivariate T\ndistribution. This may be used as a semi-analytical verification tool for more\ngeneral optimizers, and for practical assessment of the impact of fat tails on\nasset allocation for shorter time horizons.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 14:10:35 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Shaw", "William T.", ""]]}, {"id": "1102.5704", "submitter": "Wei Zhang", "authors": "Wei Zhang, Wei Zhang and Wei Chen", "title": "Phase transition behavior in a cellular automaton model with different\n  initial configurations", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical transition from free-flow to jammed traffic,\nwhich is related to the divergence of the relaxation time and susceptibility of\nthe energy dissipation rate $E_d$, in the Nagel-Schreckenberg (NS) model with\ntwo different initial configurations. Different initial configurations give\nrise to distinct phase transition. We argue that the phase transition of the\ndeterministic NS model with megajam and random initial configuration is first-\nand second-order phase transition, respectively. The energy dissipation rate\n$E_d$ and relaxation time follow power-law behavior in some cases. The\nassociated dynamic exponents have also been presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 16:42:14 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Zhang", "Wei", ""], ["Zhang", "Wei", ""], ["Chen", "Wei", ""]]}]