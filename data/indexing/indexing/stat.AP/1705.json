[{"id": "1705.00359", "submitter": "Jian Wang", "authors": "Ruizhi Zhang, Jian Wang and Yajun Mei", "title": "Search for Evergreens in Science: A Functional Data Analysis", "comments": "40 pages, 9 figures", "journal-ref": "Journal of Informetrics, Volume 11, Issue 3, Pages 629-644 (August\n  2017)", "doi": "10.1016/j.joi.2017.05.007", "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evergreens in science are papers that display a continual rise in annual\ncitations without decline, at least within a sufficiently long time period.\nAiming to better understand evergreens in particular and patterns of citation\ntrajectory in general, this paper develops a functional data analysis method to\ncluster citation trajectories of a sample of 1699 research papers published in\n1980 in the American Physical Society (APS) journals. We propose a functional\nPoisson regression model for individual papers' citation trajectories, and fit\nthe model to the observed 30-year citations of individual papers by functional\nprincipal component analysis and maximum likelihood estimation. Based on the\nestimated paper-specific coefficients, we apply the K-means clustering\nalgorithm to cluster papers into different groups, for uncovering general types\nof citation trajectories. The result demonstrates the existence of an evergreen\ncluster of papers that do not exhibit any decline in annual citations over 30\nyears.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 19:00:39 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 17:09:48 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Zhang", "Ruizhi", ""], ["Wang", "Jian", ""], ["Mei", "Yajun", ""]]}, {"id": "1705.00395", "submitter": "Lingzhou Xue", "authors": "Wei Luo, Lingzhou Xue, Jiawei Yao and Xiufan Yu", "title": "Inverse Moment Methods for Sufficient Forecasting using High-Dimensional\n  Predictors", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series using a large number of\npredictors in the presence of a possible nonlinear forecast function. Assuming\nthat the predictors affect the response through the latent factors, we propose\nto first conduct factor analysis and then apply sufficient dimension reduction\non the estimated factors, to derive the reduced data for subsequent\nforecasting. Using directional regression and the inverse third-moment method\nin the stage of sufficient dimension reduction, the proposed methods can\ncapture the non-monotone effect of factors on the response. We also allow a\ndiverging number of factors and only impose general regularity conditions on\nthe distribution of factors, avoiding the undesired time reversibility of the\nfactors by the latter. These make the proposed methods fundamentally more\napplicable than the sufficient forecasting method in Fan et al. (2017). The\nproposed methods are demonstrated in both simulation studies and an empirical\nstudy of forecasting monthly macroeconomic data from 1959 to 2016. Also, our\ntheory contributes to the literature of sufficient dimension reduction, as it\nincludes an invariance result, a path to perform sufficient dimension reduction\nunder the high-dimensional setting without assuming sparsity, and the\ncorresponding order-determination procedure.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:10:35 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 04:56:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Luo", "Wei", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""], ["Yu", "Xiufan", ""]]}, {"id": "1705.00546", "submitter": "Fernando J. Iglesias-Garcia", "authors": "Fernando J. Iglesias-Garcia, Pranab K. Mandal, M\\'elanie Bocquel,\n  Antonio G. Marques", "title": "Riemann-Langevin Particle Filtering in Track-Before-Detect", "comments": "Minor grammatical updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Track-before-detect (TBD) is a powerful approach that consists in providing\nthe tracker with sensor measurements directly without pre-detection. Due to the\nmeasurement model non-linearities, online state estimation in TBD is most\ncommonly solved via particle filtering. Existing particle filters for TBD do\nnot incorporate measurement information in their proposal distribution. The\nLangevin Monte Carlo (LMC) is a sampling method whose proposal is able to\nexploit all available knowledge of the posterior (that is, both prior and\nmeasurement information). This letter synthesizes recent advances in LMC-based\nfiltering to describe the Riemann-Langevin particle filter and introduces its\nnovel application to TBD. The benefits of our approach are illustrated in a\nchallenging low-noise scenario.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:41:50 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 22:57:05 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Iglesias-Garcia", "Fernando J.", ""], ["Mandal", "Pranab K.", ""], ["Bocquel", "M\u00e9lanie", ""], ["Marques", "Antonio G.", ""]]}, {"id": "1705.00644", "submitter": "Adam Smith", "authors": "A.D. Smith, B. Hofner, J.E. Osenkowski, T. Allison, G. Sadoti, S.R.\n  McWilliams, P.W.C. Paton", "title": "Spatiotemporal modelling of sea duck abundance: implications for marine\n  spatial planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective marine spatial plans require information on the distribution and\nabundance of biological resources that are potentially vulnerable to\nanthropogenic change. In North America, spatially-explicit abundance estimates\nof marine birds are necessary to assess potential impacts from planned offshore\nwind energy developments (OWED). Sea ducks are particularly relevant in this\ncontext as populations of most North American species are below historic levels\nand European studies suggest OWEDs. We modelled species occupancy using a\ngeneralized additive model and conditional abundance with generalized additive\nmodels for location, scale, and shape; the models were subsequently combined to\nestimate unconditional abundance. We demonstrate this flexible, model-based\napproach using sea ducks (Common Eider [Somateria mollissima], Black Scoter\n[Melanitta americana], Surf Scoter [M. perspicillata], White-winged Scoter [M.\ndeglandi], and Long-tailed Duck [Clangula hyemalis]) in Nantucket Sound,\nMassachusetts, USA, which supports some of the largest concentrations of\nwintering sea ducks in eastern North America and where a 454-MW OWED is\nproposed. Our approach to species distribution and abundance modelling offers\nseveral useful features including (1) the ability to model all conditional\ndistribution parameters as a function of covariates, (2) integrated variable\nreduction and selection among many covariates, (3) integrated model selection,\nand (4) the efficient incorporation of smooth effects to capture spatiotemporal\ntrends poorly explained by other covariates. This modelling approach should\nprove useful for marine spatial planners in siting OWEDs while considering key\nhabitats and areas potentially vulnerable to anthropogenic stressors. Moreover,\nthe approach is equally suitable for terrestrial or aquatic systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 18:13:51 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Smith", "A. D.", ""], ["Hofner", "B.", ""], ["Osenkowski", "J. E.", ""], ["Allison", "T.", ""], ["Sadoti", "G.", ""], ["McWilliams", "S. R.", ""], ["Paton", "P. W. C.", ""]]}, {"id": "1705.00885", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Paolo Cintia", "title": "Quantifying the relation between performance and success in soccer", "comments": null, "journal-ref": "Advances in Complex Systems, 2017", "doi": "10.1142/S021952591750014X", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of massive data about sports activities offers nowadays the\nopportunity to quantify the relation between performance and success. In this\nstudy, we analyze more than 6,000 games and 10 million events in six European\nleagues and investigate this relation in soccer competitions. We discover that\na team's position in a competition's final ranking is significantly related to\nits typical performance, as described by a set of technical features extracted\nfrom the soccer data. Moreover we find that, while victory and defeats can be\nexplained by the team's performance during a game, it is difficult to detect\ndraws by using a machine learning approach. We then simulate the outcomes of an\nentire season of each league only relying on technical data, i.e. excluding the\ngoals scored, exploiting a machine learning model trained on data from past\nseasons. The simulation produces a team ranking (the PC ranking) which is close\nto the actual ranking, suggesting that a complex systems' view on soccer has\nthe potential of revealing hidden patterns regarding the relation between\nperformance and success.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:05:59 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 20:14:50 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 11:07:06 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""]]}, {"id": "1705.00971", "submitter": "S. Mohammadreza Rouzegar", "authors": "S. Mohammadreza Rouzegar, Umberto Spagnolini", "title": "Channel Estimation for Diffusive MIMO Molecular Communications", "comments": "5 pages, 5 figures, EuCNC 2017", "journal-ref": null, "doi": "10.1109/EuCNC.2017.7980701", "report-no": null, "categories": "cs.IT cs.ET math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In diffusion-based communication, as for molecular systems, the achievable\ndata rate is very low due to the slow nature of diffusion and the existence of\nsevere inter-symbol interference (ISI). Multiple-input multiple-output (MIMO)\ntechnique can be used to improve the data rate. Knowledge of channel impulse\nresponse (CIR) is essential for equalization and detection in MIMO systems.\nThis paper presents a training-based CIR estimation for diffusive MIMO (D-MIMO)\nchannels. Maximum likelihood and least-squares estimators are derived, and the\ntraining sequences are designed to minimize the corresponding Cram\\'er-Rao\nbound. Sub-optimal estimators are compared to Cram\\'er-Rao bound to validate\ntheir performance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 23:02:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 09:37:53 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Rouzegar", "S. Mohammadreza", ""], ["Spagnolini", "Umberto", ""]]}, {"id": "1705.00998", "submitter": "Yixin Wang", "authors": "Yixin Wang, Jos\\'e R. Zubizarreta", "title": "Minimal Dispersion Approximately Balancing Weights: Asymptotic\n  Properties and Practical Considerations", "comments": "41 pages", "journal-ref": null, "doi": "10.1093/biomet/asz050", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighting methods are widely used to adjust for covariates in observational\nstudies, sample surveys, and regression settings. In this paper, we study a\nclass of recently proposed weighting methods which find the weights of minimum\ndispersion that approximately balance the covariates. We call these weights\n\"minimal weights\" and study them under a common optimization framework. The key\nobservation is the connection between approximate covariate balance and\nshrinkage estimation of the propensity score. This connection leads to both\ntheoretical and practical developments. From a theoretical standpoint, we\ncharacterize the asymptotic properties of minimal weights and show that, under\nstandard smoothness conditions on the propensity score function, minimal\nweights are consistent estimates of the true inverse probability weights. Also,\nwe show that the resulting weighting estimator is consistent, asymptotically\nnormal, and semiparametrically efficient. From a practical standpoint, we\npresent a finite sample oracle inequality that bounds the loss incurred by\nbalancing more functions of the covariates than strictly needed. This\ninequality shows that minimal weights implicitly bound the number of active\ncovariate balance constraints. We finally provide a tuning algorithm for\nchoosing the degree of approximate balance in minimal weights. We conclude the\npaper with four empirical studies that suggest approximate balance is\npreferable to exact balance, especially when there is limited overlap in\ncovariate distributions. In these studies, we show that the root mean squared\nerror of the weighting estimator can be reduced by as much as a half with\napproximate balance.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:31:32 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 03:37:40 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 02:35:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Yixin", ""], ["Zubizarreta", "Jos\u00e9 R.", ""]]}, {"id": "1705.01407", "submitter": "Sourish Das", "authors": "Sourish Das, Rituparna Sen", "title": "Sparse Portfolio selection via Bayesian Multiple testing", "comments": "23 pages, 8 figures, 9 tables", "journal-ref": "Sankhya-B 2020", "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We presented Bayesian portfolio selection strategy, via the $k$ factor asset\npricing model. If the market is information efficient, the proposed strategy\nwill mimic the market; otherwise, the strategy will outperform the market. The\nstrategy depends on the selection of a portfolio via Bayesian multiple testing\nmethodologies. We present the \"discrete-mixture prior\" model and the\n\"hierarchical Bayes model with horseshoe prior.\" We define the Oracle set and\nprove that asymptotically the Bayes rule attains the risk of Bayes Oracle up to\n$O(1)$. Our proposed Bayes Oracle test guarantees statistical power by\nproviding the upper bound of the type-II error. Simulation study indicates that\nthe proposed Bayes oracle test is suitable for the efficient market with few\nstocks inefficiently priced. However, as the model becomes dense, i.e., the\nmarket is highly inefficient, one should not use the Bayes oracle test. The\nstatistical power of the Bayes Oracle portfolio is uniformly better for the\n$k$-factor model ($k>1$) than the one factor CAPM. We present the empirical\nstudy, where we considered the 500 constituent stocks of S\\&P 500 from the New\nYork Stock Exchange (NYSE), and S\\&P 500 index as the benchmark for thirteen\nyears from the year 2006 to 2018. We showed the out-sample risk and return\nperformance of the four different portfolio selection strategies and compared\nwith the S\\&P 500 index as the benchmark market index. Empirical results\nindicate that it is possible to propose a strategy which can outperform the\nmarket.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 05:35:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 12:56:00 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 12:42:46 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 10:43:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Das", "Sourish", ""], ["Sen", "Rituparna", ""]]}, {"id": "1705.01440", "submitter": "Pamphile Roy", "authors": "Pamphile Tupui Roy, Nabil El Mo\\c{c}ayd, Sophie Ricci, Jean-Christophe\n  Jouhaud, Nicole Goutal, Matthias De Lozzo, M\\'elanie C. Rochoux", "title": "Comparison of Polynomial Chaos and Gaussian Process surrogates for\n  uncertainty quantification and correlation estimation of spatially\n  distributed open-channel steady flows", "comments": null, "journal-ref": null, "doi": "10.1007/s00477-017-1470-4", "report-no": null, "categories": "stat.AP physics.data-an physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation is widely used to improve flood forecasting capability,\nespecially through parameter inference requiring statistical information on the\nuncertain input parameters (upstream discharge, friction coefficient) as well\nas on the variability of the water level and its sensitivity with respect to\nthe inputs. For particle filter or ensemble Kalman filter, stochastically\nestimating probability density function and covariance matrices from a Monte\nCarlo random sampling requires a large ensemble of model evaluations, limiting\ntheir use in real-time application. To tackle this issue, fast surrogate models\nbased on Polynomial Chaos and Gaussian Process can be used to represent the\nspatially distributed water level in place of solving the shallow water\nequations. This study investigates the use of these surrogates to estimate\nprobability density functions and covariance matrices at a reduced\ncomputational cost and without the loss of accuracy, in the perspective of\nensemble-based data assimilation. This study focuses on 1-D steady state flow\nsimulated with MASCARET over the Garonne River (South-West France). Results\nshow that both surrogates feature similar performance to the Monte-Carlo random\nsampling, but for a much smaller computational budget; a few MASCARET\nsimulations (on the order of 10-100) are sufficient to accurately retrieve\ncovariance matrices and probability density functions all along the river, even\nwhere the flow dynamic is more complex due to heterogeneous bathymetry. This\npaves the way for the design of surrogate strategies suitable for representing\nunsteady open-channel flows in data assimilation.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 14:21:53 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 08:09:24 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 12:30:01 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Roy", "Pamphile Tupui", ""], ["Mo\u00e7ayd", "Nabil El", ""], ["Ricci", "Sophie", ""], ["Jouhaud", "Jean-Christophe", ""], ["Goutal", "Nicole", ""], ["De Lozzo", "Matthias", ""], ["Rochoux", "M\u00e9lanie C.", ""]]}, {"id": "1705.01581", "submitter": "Timothy Meehan", "authors": "Timothy D. Meehan, Nicole L. Michel, H{\\aa}vard Rue", "title": "Estimating animal abundance with N-mixture models using the R-INLA\n  package for R", "comments": "25 pages, 3 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful management of wildlife populations requires accurate estimates of\nabundance. Abundance estimates can be confounded by imperfect detection during\nwildlife surveys. N-mixture models enable quantification of detection\nprobability and, under appropriate conditions, produce abundance estimates that\nare less biased. Here, we demonstrate use of the R-INLA package for R to\nanalyze N-mixture models and compare performance of R-INLA to two other common\napproaches: JAGS (via the runjags package for R), which uses Markov chain Monte\nCarlo and allows Bayesian inference, and the unmarked package for R, which uses\nmaximum likelihood and allows frequentist inference. We show that R-INLA is an\nattractive option for analyzing N-mixture models when (i) fast computing times\nare necessary (R-INLA is 10 times faster than unmarked and 500 times faster\nthan JAGS), (ii) familiar model syntax and data format (relative to other R\npackages) is desired, (iii) survey-level covariates of detection are not\nessential, and (iv) Bayesian inference is preferred.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 18:59:05 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 22:47:06 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Meehan", "Timothy D.", ""], ["Michel", "Nicole L.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1705.01654", "submitter": "Andrii Babii", "authors": "Andrii Babii and Jean-Pierre Florens", "title": "Are Unobservables Separable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to assume in empirical research that observables and\nunobservables are additively separable, especially, when the former are\nendogenous. This is done because it is widely recognized that identification\nand estimation challenges arise when interactions between the two are allowed\nfor. Starting from a nonseparable IV model, where the instrumental variable is\nindependent of unobservables, we develop a novel nonparametric test of\nseparability of unobservables. The large-sample distribution of the test\nstatistics is nonstandard and relies on a novel Donsker-type central limit\ntheorem for the empirical distribution of nonparametric IV residuals, which may\nbe of independent interest. Using a dataset drawn from the 2015 US Consumer\nExpenditure Survey, we find that the test rejects the separability in Engel\ncurves for most of the commodities.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 23:33:34 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 18:22:41 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 23:20:43 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:54:33 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Babii", "Andrii", ""], ["Florens", "Jean-Pierre", ""]]}, {"id": "1705.01727", "submitter": "Jun Yu", "authors": "Kristi Kuljus, Fekadu L. Bayisa, David Bolin, J\\\"uri Lember, and Jun\n  Yu", "title": "Comparison of hidden Markov chain models and hidden Markov random field\n  models in estimation of computed tomography images", "comments": "17 pages, 5 figures, corresponding author, e-mail:\n  kristi.kuljus@ut.ee", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an interest to replace computed tomography (CT) images with magnetic\nresonance (MR) images for a number of diagnostic and therapeutic workflows. In\nthis article, predicting CT images from a number of magnetic resonance imaging\n(MRI) sequences using regression approach is explored. Two principal areas of\napplication for estimated CT images are dose calculations in MRI-based\nradiotherapy treatment planning and attenuation correction for positron\nemission tomography (PET)/MRI. The main purpose of this work is to investigate\nthe performance of hidden Markov (chain) models (HMMs) in comparison to hidden\nMarkov random field (HMRF) models when predicting CT images of head. Our study\nshows that HMMs have clear advantages over HMRF models in this particular\napplication. Obtained results suggest that HMMs deserve a further study for\ninvestigating their potential in modelling applications where the most natural\ntheoretical choice would be the class of HMRF models.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:03:13 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Kuljus", "Kristi", ""], ["Bayisa", "Fekadu L.", ""], ["Bolin", "David", ""], ["Lember", "J\u00fcri", ""], ["Yu", "Jun", ""]]}, {"id": "1705.01730", "submitter": "Anthony Coolen", "authors": "ACC Coolen, JE Barrett, P Paga, CJ Perez-Vicente", "title": "Replica analysis of overfitting in regression models for time-to-event\n  data", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": "10.1088/1751-8121/aa812f", "report-no": null, "categories": "stat.AP cond-mat.dis-nn physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting, which happens when the number of parameters in a model is too\nlarge compared to the number of data points available for determining these\nparameters, is a serious and growing problem in survival analysis. While modern\nmedicine presents us with data of unprecedented dimensionality, these data\ncannot yet be used effectively for clinical outcome prediction. Standard error\nmeasures in maximum likelihood regression, such as p-values and z-scores, are\nblind to overfitting, and even for Cox's proportional hazards model (the main\ntool of medical statisticians), one finds in literature only rules of thumb on\nthe number of samples required to avoid overfitting. In this paper we present a\nmathematical theory of overfitting in regression models for time-to-event data,\nwhich aims to increase our quantitative understanding of the problem and\nprovide practical tools with which to correct regression outcomes for the\nimpact of overfitting. It is based on the replica method, a statistical\nmechanical technique for the analysis of heterogeneous many-variable systems\nthat has been used successfully for several decades in physics, biology, and\ncomputer science, but not yet in medical statistics. We develop the theory\ninitially for arbitrary regression models for time-to-event data, and verify\nits predictions in detail for the popular Cox model.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:13:47 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 13:23:09 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Coolen", "ACC", ""], ["Barrett", "JE", ""], ["Paga", "P", ""], ["Perez-Vicente", "CJ", ""]]}, {"id": "1705.01772", "submitter": "Palash Ghosh", "authors": "Palash Ghosh and Inbal Nahum-Shani and Bonnie Spring and Bibhas\n  Chakraborty", "title": "Non-Inferiority and Equivalence Tests in A Sequential\n  Multiple-Assignment Randomized Trial (SMART)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive interventions (AIs) are increasingly becoming popular in medical and\nbehavioral sciences. An AI is a sequence of individualized intervention options\nthat specify for whom and under what conditions different intervention options\nshould be offered, in order to address the changing needs of individuals as\nthey progress over time. The sequential, multiple assignment, randomized trial\n(SMART) is a novel trial design that was developed to aid in empirically\nconstructing effective AIs. The sequential randomizations in a SMART often\nyield multiple AIs that are embedded in the trial by design. Many SMARTs are\nmotivated by scientific questions pertaining to the comparison of such embedded\nAIs. Existing data analytic methods and sample size planning resources for\nSMARTs are suitable for superiority testing, namely for testing whether one\nembedded AI yields better primary outcomes on average than another. This\nrepresents a major scientific gap since AIs are often motivated by the need to\ndeliver support/care in a less costly or less burdensome manner, while still\nyielding benefits that are equivalent or non-inferior to those produced by a\nmore costly/burdensome standard of care. Here, we develop data analytic methods\nand sample size formulas for SMART studies aiming to test the non-inferiority\nor equivalence of one AI over another. Sample size and power considerations are\ndiscussed with supporting simulations, and online sample size planning\nresources are provided. For illustration, we use an example from a SMART study\naiming to develop an AI for promoting weight loss among overweight/obese\nadults.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 10:05:31 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 09:04:30 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 03:42:38 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ghosh", "Palash", ""], ["Nahum-Shani", "Inbal", ""], ["Spring", "Bonnie", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "1705.01789", "submitter": "Huang Huang", "authors": "Huang Huang and Ying Sun", "title": "Visualization and Assessment of Spatio-temporal Covariance Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal covariances are important for describing the spatio-temporal\nvariability of underlying random processes in geostatistical data. For\nsecond-order stationary processes, there exist subclasses of covariance\nfunctions that assume a simpler spatio-temporal dependence structure with\nseparability and full symmetry. However, it is challenging to visualize and\nassess separability and full symmetry from spatio-temporal observations. In\nthis work, we propose a functional data analysis approach that constructs test\nfunctions using the cross-covariances from time series observed at each pair of\nspatial locations. These test functions of temporal lags summarize the\nproperties of separability or symmetry for the given spatial pairs. We use\nfunctional boxplots to visualize the functional median and the variability of\nthe test functions, where the extent of departure from zero at all temporal\nlags indicates the degree of non-separability or asymmetry. We also develop a\nrank-based nonparametric testing procedure for assessing the significance of\nthe non-separability or asymmetry. The performances of the proposed methods are\nexamined by simulations with various commonly used spatio-temporal covariance\nmodels. To illustrate our methods in practical applications, we apply it to\nreal datasets, including weather station data and climate model outputs.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 10:48:17 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Huang", "Huang", ""], ["Sun", "Ying", ""]]}, {"id": "1705.01795", "submitter": "Eduardo Calvo", "authors": "Eduardo Calvo", "title": "An\\'alisis econom\\'etrico de series temporales en Gretl: La Ley de Okun", "comments": "in French. JEL Classification: C22, C51, E24", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gretl is an econometrics package, including a shared library, a command-line\nclient program and a graphical user interface which offers an intuitive user\ninterface. This paper explains the Okun's Law, which is an empirically observed\nrelationship between unemployment and losses in a country's production, with\nspanish data processed in Gretl.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 11:01:50 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Calvo", "Eduardo", ""]]}, {"id": "1705.02026", "submitter": "Carlos Martinez Mr.", "authors": "Carlos A. Mart\\'inez, Kshitij Khare, Syed Rahman, Mauricio A. Elzo", "title": "Inferring the Partial Correlation Structure of Allelic Effects and\n  Incorporating it in Genome-wide Prediction", "comments": "25 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we addressed the problem of genome-wide prediction accounting\nfor partial correlation of marker effects when the partial correlation\nstructure, or equivalently, the pattern of zeros of the precision matrix is\nunknown. This problem requires estimating the partial correlation structure of\nmarker effects, that is, learning the pattern of zeros of the corresponding\nprecision matrix, estimating its non-null entries, and incorporating the\ninferred concentration matrix in the prediction of marker allelic effects. To\nthis end, we developed a set of statistical methods based on Gaussian\nconcentration graph models (GCGM) and Gaussian directed acyclic graph models\n(GDAGM) that adapt the existing theory to perform covariance model selection\n(GCGM) or DAG selection (GDAGM) to genome-wide prediction. Bayesian and\nfrequentist approaches were formulated. Our frequentist formulations combined\nsome existing methods with the EM algorithm and were termed Glasso-EM,\nCONCORD-EM and CSCS-EM, whereas our Bayesian formulations corresponded to\nhierarchical models termed Bayes G-Sel and Bayes DAG-Sel. Results from a\nsimulation study showed that our methods can accurately recover the partial\ncorrelation structure and estimate the precision matrix. Methods CONCORD-EM and\nBayes G-Sel had an outstanding performance in estimating the partial\ncorrelation structure and a method based on CONCORD-EM yielded the most\naccurate estimates of the precision matrix. Our methods can be used as\npredictive machines and as tools to learn about the covariation of effects of\npairs of loci on a given phenotype conditioned on the effects of all the other\nloci considered in the model. Therefore, they are useful tools to learn about\nthe underlying biology of a given trait because they help to understand\nrelationships between different regions of the genome in terms of the partial\ncorrelations of their effects on that trait.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 21:25:54 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Mart\u00ednez", "Carlos A.", ""], ["Khare", "Kshitij", ""], ["Rahman", "Syed", ""], ["Elzo", "Mauricio A.", ""]]}, {"id": "1705.02128", "submitter": "Vasyl Zhabotynsky", "authors": "Vasyl Zhabotynsky, Wei Sun, Kaoru Inoue, Terry Magnuson, Mauro\n  Calabrese", "title": "Joint estimation of genetic and parent-of-origin effects using RNA-seq\n  data from human", "comments": "main text & web appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA sequencing allows one to study allelic imbalance of gene expression,\nwhich may be due to genetic factors or genomic imprinting. It is desirable to\nmodel both genetic and parent-of-origin effects simultaneously to avoid\nconfounding and to improve the power to detect either effect. In a study of\nexperimental cross, separation of genetic and parent-of-origin effects can be\nachieved by studying reciprocal cross of two inbred strains. In contrast, this\ntask is much more challenging for an outbred population such as human\npopulation. To address this challenge, we propose a new framework to combine\nexperimental strategies and novel statistical methods. Specifically, we propose\nto collect genotype data from family trios as well as RNA-seq data from the\nchildren of family trios. We have developed a new statistical method to\nestimate both genetic and parent-of-origin effects from such data sets. We\ndemonstrated this approach by studying 30 trios of HapMap samples. Our results\nsupport some of previous finding of imprinted genes and also recover new\ncandidate imprinted genes.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 08:45:27 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhabotynsky", "Vasyl", ""], ["Sun", "Wei", ""], ["Inoue", "Kaoru", ""], ["Magnuson", "Terry", ""], ["Calabrese", "Mauro", ""]]}, {"id": "1705.02154", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Paul Cockshott", "title": "Leontief Meets Shannon - Measuring the Complexity of the Economic System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a complexity measure for large-scale economic systems based on\nShannon's concept of entropy. By adopting Leontief's perspective of the\nproduction process as a circular flow, we formulate the process as a Markov\nchain. Then we derive a measure of economic complexity as the average number of\nbits required to encode the flow of goods and services in the production\nprocess. We illustrate this measure using data from seven national economies,\nspanning several decades.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 09:56:36 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 13:02:19 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Zachariah", "Dave", ""], ["Cockshott", "Paul", ""]]}, {"id": "1705.02423", "submitter": "Jaewoo Park", "authors": "Jaewoo Park, Joshua Goldstein, Murali Haran, and Matthew Ferrari", "title": "An Ensemble Approach to Predicting the Impact of Vaccination on\n  Rotavirus Disease in Niger", "comments": "9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed vaccines provide a new way of controlling rotavirus in\nsub-Saharan Africa. Models for the transmission dynamics of rotavirus are\ncritical both for estimating current burden from imperfect surveillance and for\nassessing potential effects of vaccine intervention strategies. We examine\nrotavirus infection in the Maradi area in southern Niger using hospital\nsurveillance data provided by Epicentre collected over two years. Additionally,\na cluster survey of households in the region allows us to estimate the\nproportion of children with diarrhea who consulted at a health structure. Model\nfit and future projections are necessarily particular to a given model; thus,\nwhere there are competing models for the underlying epidemiology an ensemble\napproach can account for that uncertainty. We compare our results across\nseveral variants of Susceptible-Infectious-Recovered (SIR) compartmental models\nto quantify the impact of modeling assumptions on our estimates. Model-specific\nparameters are estimated by Bayesian inference using Markov chain Monte Carlo.\nWe then use Bayesian model averaging to generate ensemble estimates of the\ncurrent dynamics, including estimates of $R_0$, the burden of infection in the\nregion, as well as the impact of vaccination on both the short-term dynamics\nand the long-term reduction of rotavirus incidence under varying levels of\ncoverage. The ensemble of models predicts that the current burden of severe\nrotavirus disease is 2.9 to 4.1% of the population each year and that a 2-dose\nvaccine schedule achieving 70% coverage could reduce burden by 37-43%.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 00:42:21 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Park", "Jaewoo", ""], ["Goldstein", "Joshua", ""], ["Haran", "Murali", ""], ["Ferrari", "Matthew", ""]]}, {"id": "1705.02441", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Yinchu Zhu", "title": "Comments on `High-dimensional simultaneous inference with the bootstrap'", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide comments on the article \"High-dimensional simultaneous inference\nwith the bootstrap\" by Ruben Dezeure, Peter Buhlmann and Cun-Hui Zhang.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 04:15:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bradic", "Jelena", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1705.02479", "submitter": "Tianwei Yu", "authors": "Tianwei Yu", "title": "DCA: Dynamic Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-throughput data, dynamic correlation between genes, i.e. changing\ncorrelation patterns under different biological conditions, can reveal\nimportant regulatory mechanisms. Given the complex nature of dynamic\ncorrelation, and the underlying conditions for dynamic correlation may not\nmanifest into clinical observations, it is difficult to recover such signal\nfrom the data. Current methods seek underlying conditions for dynamic\ncorrelation by using certain observed genes as surrogates, which may not\nfaithfully represent true latent conditions. In this study we develop a new\nmethod that directly identifies strong latent signals that regulate the dynamic\ncorrelation of many pairs of genes, named DCA: Dynamic Correlation Analysis. At\nthe center of the method is a new metric for the identification of gene pairs\nthat are highly likely to be dynamically correlated, without knowing the\nunderlying conditions of the dynamic correlation. We validate the performance\nof the method with extensive simulations. In real data analysis, the method\nreveals novel latent factors with clear biological meaning, bringing new\ninsights into the data.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 12:29:22 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Yu", "Tianwei", ""]]}, {"id": "1705.02687", "submitter": "Seyed Sajjadi", "authors": "Seyed Sajjadi, Bruce Shapiro, Christopher McKinlay, Allen Sarkisyan,\n  Carol Shubin, Efunwande Osoba", "title": "Finding Bottlenecks: Predicting Student Attrition with Unsupervised\n  Classifier", "comments": "7 pages, 10 figures, Finding Bottlenecks: Predicting Student\n  Attrition with Unsupervised Classifiers, IEEE, IntelliSys 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With pressure to increase graduation rates and reduce time to degree in\nhigher education, it is important to identify at-risk students early. Automated\nearly warning systems are therefore highly desirable. In this paper, we use\nunsupervised clustering techniques to predict the graduation status of declared\nmajors in five departments at California State University Northridge (CSUN),\nbased on a minimal number of lower division courses in each major. In addition,\nwe use the detected clusters to identify hidden bottleneck courses.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:45:49 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed", ""], ["Shapiro", "Bruce", ""], ["McKinlay", "Christopher", ""], ["Sarkisyan", "Allen", ""], ["Shubin", "Carol", ""], ["Osoba", "Efunwande", ""]]}, {"id": "1705.02786", "submitter": "Sylvain Robert", "authors": "Sylvain Robert, Daniel Leuenberger, Hans R. K\\\"unsch", "title": "A local ensemble transform Kalman particle filter for convective scale\n  data assimilation", "comments": "submitted", "journal-ref": null, "doi": "10.1002/qj.3116", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\nare a key component of probabilistic weather forecasting. They represent the\nuncertainty in the initial conditions by an ensemble which incorporates\ninformation coming from the physical model with the latest observations.\nHigh-resolution numerical weather prediction models ran at operational centers\nare able to resolve non-linear and non-Gaussian physical phenomena such as\nconvection. There is therefore a growing need to develop ensemble assimilation\nalgorithms able to deal with non-Gaussianity while staying computationally\nfeasible. In the present paper we address some of these needs by proposing a\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\nformulated in ensemble space and uses a deterministic scheme such that it has\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\nlimiting case. A new criterion for choosing the proportion of particle filter\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\nframework and numerical experiments in a quasi-operational convective-scale\nsetup are conducted. The results show the feasibility of the new algorithm in\npractice and indicate a strong potential for such local hybrid methods, in\nparticular for forecasting non-Gaussian variables such as wind and hourly\nprecipitation.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 09:06:30 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 08:24:41 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Robert", "Sylvain", ""], ["Leuenberger", "Daniel", ""], ["K\u00fcnsch", "Hans R.", ""]]}, {"id": "1705.02935", "submitter": "Timon Elmer", "authors": "Timon Elmer, Zsofia Boda, Christoph Stadtfeld", "title": "The co-evolution of emotional well-being with weak and strong friendship\n  ties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social ties are strongly related to well-being. But what characterizes this\nrelationship? This study investigates social mechanisms explaining how social\nties affect well-being through social integration and social influence, and how\nwell-being affects social ties through social selection. We hypothesize that\nhighly integrated individuals - those with more extensive and dense friendship\nnetworks - report higher emotional well-being than others. Moreover, emotional\nwell-being should be influenced by the well-being of close friends. Finally,\nwell-being should affect friendship selection when individuals prefer others\nwith higher levels of well-being, and others whose well-being is similar to\ntheirs. We test our hypotheses using longitudinal social network and well-being\ndata of 117 individuals living in a graduate housing community. The application\nof a novel extension of Stochastic Actor-Oriented Models for ordered networks\n(ordered SAOMs) allows us to detail and test our hypotheses for weak- and\nstrong-tied friendship networks simultaneously. Results do not support our\nsocial integration and social influence hypotheses but provide evidence for\nselection: individuals with higher emotional well-being tend to have more\nstrong-tied friends, and there are homophily processes regarding emotional\nwell-being in strong-tied networks. Our study highlights the two-directional\nrelationship between social ties and well-being, and demonstrates the\nimportance of considering different tie strengths for various social processes.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 08:38:25 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Elmer", "Timon", ""], ["Boda", "Zsofia", ""], ["Stadtfeld", "Christoph", ""]]}, {"id": "1705.03050", "submitter": "Yuanyuan Duan", "authors": "Yuanyuan Duan, Yili Hong, William Q. Meeker, Deborah L. Stanley, and\n  Xiaohong Gu", "title": "Development of an Accelerated Test Methodology to the Predict Service\n  Life of Polymeric Materials Subject to Outdoor Weathering", "comments": "32 pages, 11 figures Key Words: Degradation, Photodegradation,\n  Nonlinear model, Random effects, Reliability, UV exposure, Weathering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service life prediction is of great importance to manufacturers of coatings\nand other polymeric materials. Photodegradation, driven primarily by\nultraviolet (UV) radiation, is the primary cause of failure for organic paints\nand coatings, as well as many other products made from polymeric materials\nexposed to sunlight. Traditional methods of service life prediction involve the\nuse of outdoor exposure in harsh UV environments (e.g., Florida and Arizona).\nSuch tests, however, require too much time (generally many years) to do an\nevaluation. Non-scientific attempts to simply \"speed up the clock\" result in\nincorrect predictions. This paper describes the statistical methods that were\ndeveloped for a scientifically-based approach to using laboratory accelerated\ntests to produce timely predictions of outdoor service life. The approach\ninvolves careful experimentation and identifying a physics/chemistry-motivated\nmodel that will adequately describe photodegradation paths of polymeric\nmaterials. The model incorporates the effects of explanatory variables UV\nspectrum, UV intensity, temperature, and humidity. We use a nonlinear\nmixed-effects model to describe the sample paths. The methods are illustrated\nwith accelerated laboratory test data for a model epoxy coating. The validity\nof the methodology is checked by extending our model to allow for dynamic\ncovariates and comparing predictions with specimens that were exposed in an\noutdoor environment where the explanatory variables are uncontrolled but\nrecorded.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 18:59:37 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Duan", "Yuanyuan", ""], ["Hong", "Yili", ""], ["Meeker", "William Q.", ""], ["Stanley", "Deborah L.", ""], ["Gu", "Xiaohong", ""]]}, {"id": "1705.03089", "submitter": "Fabo  Feng", "authors": "Fabo Feng and Mikko Tuomi and Hugh R. A. Jones", "title": "Agatha: disentangling periodic signals from correlated noise in a\n  periodogram framework", "comments": "22 pages, 16 figures, 5 tables, MNRAS in press, the app is available\n  at http://www.agatha.herts.ac.uk", "journal-ref": null, "doi": "10.1093/mnras/stx1126", "report-no": null, "categories": "astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodograms are used as a key significance assessment and visualisation tool\nto display the significant periodicities in unevenly sampled time series. We\nintroduce a framework of periodograms, called \"Agatha\", to disentangle periodic\nsignals from correlated noise and to solve the 2-dimensional model selection\nproblem: signal dimension and noise model dimension. These periodograms are\ncalculated by applying likelihood maximization and marginalization and combined\nin a self-consistent way. We compare Agatha with other periodograms for the\ndetection of Keplerian signals in synthetic radial velocity data produced for\nthe Radial Velocity Challenge as well as in radial velocity datasets of several\nSun-like stars. In our tests we find Agatha is able to recover signals to the\nadopted detection limit of the radial velocity challenge. Applied to real\nradial velocity, we use Agatha to confirm previous analysis of CoRoT-7 and to\nfind two new planet candidates with minimum masses of 15.1 $M_\\oplus$ and 7.08\n$M_\\oplus$ orbiting HD177565 and HD41248, with periods of 44.5 d and 13.4 d,\nrespectively. We find that Agatha outperforms other periodograms in terms of\nremoving correlated noise and assessing the significances of signals with more\nrobust metrics. Moreover, it can be used to select the optimal noise model and\nto test the consistency of signals in time. Agatha is intended to be flexible\nenough to be applied to time series analyses in other astronomical and\nscientific disciplines. Agatha is available at http://www.agatha.herts.ac.uk.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 21:13:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Feng", "Fabo", ""], ["Tuomi", "Mikko", ""], ["Jones", "Hugh R. A.", ""]]}, {"id": "1705.03134", "submitter": "Paul McNicholas", "authors": "Yang Tang and Paul D. McNicholas", "title": "Clustering Airbnb Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, online customer reviews increasingly exert influence on\nconsumers' decision when booking accommodation online. The renewal importance\nto the concept of word-of mouth is reflected in the growing interests in\ninvestigating consumers' experience by analyzing their online reviews through\nthe process of text mining and sentiment analysis. A clustering approach is\ndeveloped for Boston Airbnb reviews submitted in the English language and\ncollected from 2009 to 2016. This approach is based on a mixture of latent\nvariable models, which provides an appealing framework for handling clustered\nbinary data. We address here the problem of discovering meaningful segments of\nconsumers that are coherent from both the underlying topics and the sentiment\nbehind the reviews. A penalized mixture of latent traits approach is developed\nto reduce the number of parameters and identify variables that are not\ninformative for clustering. The introduction of component-specific rate\nparameters avoids the over-penalization that can occur when inferring a shared\nrate parameter on clustered data. We divided the guests into four groups --\nproperty driven guests, host driven guests, guests with recent overall negative\nstay and guests with some negative experiences.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:53:36 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 00:13:54 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 23:34:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1705.03318", "submitter": "Luiz Gustavo de Andrade  Alves", "authors": "Luiz G. A. Alves, Peter B. Winter, Leonardo N. Ferreira, Ren\\'ee M.\n  Brielmann, Richard I. Morimoto, Lu\\'is A. N. Amaral", "title": "Long-range correlations and fractal dynamics in C. elegans: changes with\n  aging and stress", "comments": "Accepted for publication in Physical Review E", "journal-ref": "Phys. Rev. E 96, 022417 (2017)", "doi": "10.1103/PhysRevE.96.022417", "report-no": null, "categories": "q-bio.QM physics.bio-ph physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced motor control is one of the most frequent features associated with\naging and disease. Nonlinear and fractal analyses have proved to be useful in\ninvestigating human physiological alterations with age and disease. Similar\nfindings have not been established for any of the model organisms typically\nstudied by biologists, though. If the physiology of a simpler model organism\ndisplays the same characteristics, this fact would open a new research window\non the control mechanisms that organisms use to regulate physiological\nprocesses during aging and stress. Here, we use a recently introduced animal\ntracking technology to simultaneously follow tens of Caenorhabdits elegans for\nseveral hours and use tools from fractal physiology to quantitatively evaluate\nthe effects of aging and temperature stress on nematode motility. Similarly to\nhuman physiological signals, scaling analysis reveals long-range correlations\nin numerous motility variables, fractal properties in behavioral shifts, and\nfluctuation dynamics over a wide range of timescales. These properties change\nas a result of a superposition of age and stress-related adaptive mechanisms\nthat regulate motility.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:57:52 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 01:48:53 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Alves", "Luiz G. A.", ""], ["Winter", "Peter B.", ""], ["Ferreira", "Leonardo N.", ""], ["Brielmann", "Ren\u00e9e M.", ""], ["Morimoto", "Richard I.", ""], ["Amaral", "Lu\u00eds A. N.", ""]]}, {"id": "1705.03396", "submitter": "Pavel Shevchenko V", "authors": "Philippe Deprez, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Machine Learning Techniques for Mortality Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various stochastic models have been proposed to estimate mortality rates. In\nthis paper we illustrate how machine learning techniques allow us to analyze\nthe quality of such mortality models. In addition, we present how these\ntechniques can be used for differentiating the different causes of death in\nmortality modeling.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 13:37:37 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Deprez", "Philippe", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "1705.03457", "submitter": "Omer Faruk Gulban", "authors": "Omer Faruk Gulban", "title": "The relation between color spaces and compositional data analysis\n  demonstrated with magnetic resonance image processing applications", "comments": "13 pages, 3 figures, short paper, submitted to Austrian Journal of\n  Statistics compositional data analysis special issue, first revision, fix\n  rendering error in fig2", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel application of compositional data analysis\nmethods in the context of color image processing. A vector decomposition method\nis proposed to reveal compositional components of any vector with positive\ncomponents followed by compositional data analysis to demonstrate the relation\nbetween color space concepts such as hue and saturation to their compositional\ncounterparts. The proposed methods are applied to a magnetic resonance imaging\ndataset acquired from a living human brain and a digital color photograph to\nperform image fusion. Potential future applications in magnetic resonance\nimaging are mentioned and the benefits/disadvantages of the proposed methods\nare discussed in terms of color image processing.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 07:14:26 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 17:50:56 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 16:35:10 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 10:19:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Gulban", "Omer Faruk", ""]]}, {"id": "1705.03534", "submitter": "Chad Babcock", "authors": "Chad Babcock, Andrew O. Finley, Hans-Erik Andersen, Robert Pattison,\n  Bruce D. Cook, Douglas C. Morton, Michael Alonzo, Ross Nelson, Timothy\n  Gregoire, Liviu Ene, Terje Gobakken, Erik N{\\ae}sset", "title": "Geostatistical estimation of forest biomass in interior Alaska combining\n  Landsat-derived tree cover, sampled airborne lidar and field observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research was to develop and examine the performance of a\ngeostatistical coregionalization modeling approach for combining field\ninventory measurements, strip samples of airborne lidar and Landsat-based\nremote sensing data products to predict aboveground biomass (AGB) in interior\nAlaska's Tanana Valley. The proposed modeling strategy facilitates pixel-level\nmapping of AGB density predictions across the entire spatial domain.\nAdditionally, the coregionalization framework allows for statistically sound\nestimation of total AGB for arbitrary areal units within the study area---a key\nadvance to support diverse management objectives in interior Alaska. This\nresearch focuses on appropriate characterization of prediction uncertainty in\nthe form of posterior predictive coverage intervals and standard deviations.\nUsing the framework detailed here, it is possible to quantify estimation\nuncertainty for any spatial extent, ranging from pixel-level predictions of AGB\ndensity to estimates of AGB stocks for the full domain. The lidar-informed\ncoregionalization models consistently outperformed their counterpart lidar-free\nmodels in terms of point-level predictive performance and total AGB precision.\nAdditionally, the inclusion of Landsat-derived forest cover as a covariate\nfurther improved estimation precision in regions with lower lidar sampling\nintensity. Our findings also demonstrate that model-based approaches that do\nnot explicitly account for residual spatial dependence can grossly\nunderestimate uncertainty, resulting in falsely precise estimates of AGB. On\nthe other hand, in a geostatistical setting, residual spatial structure can be\nmodeled within a Bayesian hierarchical framework to obtain statistically\ndefensible assessments of uncertainty for AGB estimates.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:40:30 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 22:06:33 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Babcock", "Chad", ""], ["Finley", "Andrew O.", ""], ["Andersen", "Hans-Erik", ""], ["Pattison", "Robert", ""], ["Cook", "Bruce D.", ""], ["Morton", "Douglas C.", ""], ["Alonzo", "Michael", ""], ["Nelson", "Ross", ""], ["Gregoire", "Timothy", ""], ["Ene", "Liviu", ""], ["Gobakken", "Terje", ""], ["N\u00e6sset", "Erik", ""]]}, {"id": "1705.03540", "submitter": "Michael Lash", "authors": "Michael T. Lash, Jason Slater, Philip M. Polgreen, and Alberto M.\n  Segre", "title": "A Large-Scale Exploration of Factors Affecting Hand Hygiene Compliance\n  Using Linear Predictive Models", "comments": "Accepted to ICHI 2017. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This large-scale study, consisting of 24.5 million hand hygiene opportunities\nspanning 19 distinct facilities in 10 different states, uses linear predictive\nmodels to expose factors that may affect hand hygiene compliance. We examine\nthe use of features such as temperature, relative humidity, influenza severity,\nday/night shift, federal holidays and the presence of new residents in\npredicting daily hand hygiene compliance. The results suggest that colder\ntemperatures and federal holidays have an adverse effect on hand hygiene\ncompliance rates, and that individual cultures and attitudes regarding hand\nhygiene seem to exist among facilities.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 15:57:30 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 18:15:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lash", "Michael T.", ""], ["Slater", "Jason", ""], ["Polgreen", "Philip M.", ""], ["Segre", "Alberto M.", ""]]}, {"id": "1705.03799", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa and Jun Yu", "title": "Model-based Computed Tomography Image Estimation: Partitioning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest to get a fully MR based radiotherapy. The most\nimportant development needed is to obtain improved bone tissue estimation. The\nexisting model-based methods perform poorly on bone tissues. This paper was\naimed at obtaining improved bone tissue estimation. Skew Gaussian mixture model\nand Gaussian mixture model were proposed to investigate CT image estimation\nfrom MR images by partitioning the data into two major tissue types. The\nperformance of the proposed models was evaluated using leave-one-out\ncross-validation method on real data. In comparison with the existing\nmodel-based approaches, the model-based partitioning approach outperformed in\nbone tissue estimation, especially in dense bone tissue estimation.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 14:39:03 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:30:35 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["Yu", "Jun", ""]]}, {"id": "1705.03853", "submitter": "Kevin Schultz", "authors": "Kevin Schultz", "title": "Exponential Families for Bayesian Quantum Process Tomography", "comments": "13 pages, 7 figures; Minor editorial changes", "journal-ref": "Phys. Rev. A 100, 062316 (2019)", "doi": "10.1103/PhysRevA.100.062316", "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach to quantum process tomography has yet to be fully\ndeveloped due to the lack of appropriate probability distributions on the space\nof quantum channels. Here, by associating the Choi matrix form of a completely\npositive, trace preserving (CPTP) map with a particular space of matrices with\northonormal columns, called a Stiefel manifold, we present two parametric\nprobability distributions on the space of CPTP maps that enable Bayesian\nanalysis of process tomography. The first is a probability distribution that\nhas an average Choi matrix as a sufficient statistic. The second is a\ndistribution resulting from binomial likelihood data that enables a simple\nconnection to data gathered through process tomography experiments. To our\nknowledge these are the first examples of continuous, non-unitary random CPTP\nmaps, that capture meaningful prior information for use in Bayesian estimation.\nWe show how these distributions can be used for point estimation using either\nmaximum a posteriori estimates or expected a posteriori estimates, as well as\nfull Bayesian tomography resulting in posterior credibility intervals. This\napproach will enable the full power of Bayesian analysis in all forms of\nquantum characterization, verification, and validation.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 16:57:41 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 15:21:46 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 16:28:24 GMT"}, {"version": "v4", "created": "Fri, 1 Nov 2019 13:47:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Schultz", "Kevin", ""]]}, {"id": "1705.03922", "submitter": "Arash Andalib", "authors": "Arash Andalib, Raheleh Baharloo, Jose C. Principe", "title": "Pre-earthquake State Identification by Micro-earthquake Spike Trains\n  Dissimilarity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact mechanisms leading to an earthquake are not fully understood and\nthe space-time structural features are non-trivial. Previous studies suggest\nthe seismicity of very low intensity earthquakes, known as micro-earthquakes,\nmay contain information about the source process before major earthquakes, as\nthey can quantify modifications to stress or strain across time that finally\nlead to a major earthquake. This work uses the history of seismic activity of\nmicro-earthquakes to analyze the spatio-temporal statistical independence among\nthe monitoring stations of a seismic network. Using point process distance\nmeasures applied to the micro-earthquakes' spike trains recorded in these\nstations, a pre-earthquake state is defined statistically with the aim of\nfinding a relation between the level of dissimilarity among stations' readings\nand the future occurrence of larger earthquakes in the region. This paper also\naddresses the compatibility of this statistical approach with the\nBurridge-Knopoff spring-block physical model for earthquakes. Based on the\nresults, there is evidence for an earthquake precursory state associated with\nan increase in spike train dissimilarity as evaluated by a statistical\nsurrogate test.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 18:54:30 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Andalib", "Arash", ""], ["Baharloo", "Raheleh", ""], ["Principe", "Jose C.", ""]]}, {"id": "1705.03938", "submitter": "David Bolin", "authors": "Sandra Barman and David Bolin", "title": "A three-dimensional statistical model for imaged microstructures of\n  porous polymer films", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A thresholded Gaussian random field model is developed for the microstructure\nof porous materials. Defining the random field as a solution to stochastic\npartial differential equation allows for flexible modelling of\nnon-stationarities in the material and facilitates computationally efficient\nmethods for simulation and model fitting. A Markov Chain Monte Carlo algorithm\nis developed and used to fit the model to three-dimensional confocal laser\nscanning microscopy images. The methods are applied to study a porous\nethylcellulose/hydroxypropylcellulose polymer blend that is used as a coating\nto control drug release from pharmaceutical tablets. The aim is to investigate\nhow mass transport through the material depends on the microstructure. We\nderive a number of goodness-of-fit measures based on numerically calculated\ndiffusion through the material. These are used in combination with measures\nthat characterize the geometry of the pore structure to assess model fit. The\nmodel is found to fit stationary parts of the material well.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 20:05:35 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 14:17:38 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Barman", "Sandra", ""], ["Bolin", "David", ""]]}, {"id": "1705.04312", "submitter": "Alexej Gossmann", "authors": "Alexej Gossmann, Pascal Zille, Vince Calhoun, and Yu-Ping Wang", "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to\n  Imaging Genomics", "comments": "- Clarification of the definition of FDR for CCA in Section III;\n  results unchanged. - Corrected typos. - Added IEEE copyright notice for the\n  accepted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the number of false discoveries is presently one of the most\npressing issues in the life sciences. It is of especially great importance for\nmany applications in neuroimaging and genomics, where datasets are typically\nhigh-dimensional, which means that the number of explanatory variables exceeds\nthe sample size. The false discovery rate (FDR) is a criterion that can be\nemployed to address that issue. Thus it has gained great popularity as a tool\nfor testing multiple hypotheses. Canonical correlation analysis (CCA) is a\nstatistical technique that is used to make sense of the cross-correlation of\ntwo sets of measurements collected on the same set of samples (e.g., brain\nimaging and genomic data for the same mental illness patients), and sparse CCA\nextends the classical method to high-dimensional settings. Here we propose a\nway of applying the FDR concept to sparse CCA, and a method to control the FDR.\nThe proposed FDR correction directly influences the sparsity of the solution,\nadapting it to the unknown true sparsity level. Theoretical derivation as well\nas simulation studies show that our procedure indeed keeps the FDR of the\ncanonical vectors below a user-specified target level. We apply the proposed\nmethod to an imaging genomics dataset from the Philadelphia Neurodevelopmental\nCohort. Our results link the brain connectivity profiles derived from brain\nactivity during an emotion identification task, as measured by functional\nmagnetic resonance imaging (fMRI), to the corresponding subjects' genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:57:40 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 01:28:42 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 00:12:42 GMT"}, {"version": "v4", "created": "Sat, 23 Jun 2018 04:45:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gossmann", "Alexej", ""], ["Zille", "Pascal", ""], ["Calhoun", "Vince", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1705.04356", "submitter": "Marcio Diniz", "authors": "Marcio A. Diniz, Rafael Izbicki, Danilo Lopes, Luis Ernesto Salasar", "title": "Comparing probabilistic predictive models applied to football", "comments": null, "journal-ref": null, "doi": "10.1080/01605682.2018.1457485", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two Bayesian multinomial-Dirichlet models to predict the final\noutcome of football (soccer) matches and compare them to three well-known\nmodels regarding their predictive power. All the models predicted the full-time\nresults of 1710 matches of the first division of the Brazilian football\nchampionship and the comparison used three proper scoring rules, the proportion\nof errors and a calibration assessment. We also provide a goodness of fit\nmeasure. Our results show that multinomial-Dirichlet models are not only\ncompetitive with standard approaches, but they are also well calibrated and\npresent reasonable goodness of fit.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 19:18:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Diniz", "Marcio A.", ""], ["Izbicki", "Rafael", ""], ["Lopes", "Danilo", ""], ["Salasar", "Luis Ernesto", ""]]}, {"id": "1705.04517", "submitter": "Jorge Ma\\~nana-Rodr\\'iguez", "authors": "Elea Gim\\'enez-Toledo and Jorge Ma\\~nana-Rodr\\'iguez", "title": "Is there agreement on the prestige of scholarly book publishers in the\n  Humanities? DELPHI over survey results", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having an important role supporting assessment processes, criticism\ntowards evaluation systems and the categorizations used are frequent.\nConsidering the acceptance by the scientific community as an essential issue\nfor using rankings or categorizations in research evaluation, the aim of this\npaper is testing the results of rankings of scholarly book publishers'\nprestige, Scholarly Publishers Indicators (SPI hereafter). SPI is a public,\nsurvey-based ranking of scholarly publishers' prestige (among other\nindicators). The latest version of the ranking (2014) was based on an expert\nconsultation with a large number of respondents. In order to validate and\nrefine the results for Humanities' fields as proposed by the assessment\nagencies, a Delphi technique was applied with a panel of randomly selected\nexperts over the initial rankings. The results show an equalizing effect of the\ntechnique over the initial rankings as well as a high degree of concordance\nbetween its theoretical aim (consensus among experts) and its empirical results\n(summarized with Gini Index). The resulting categorization is understood as\nmore conclusive and susceptible of being accepted by those under evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:30:01 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Gim\u00e9nez-Toledo", "Elea", ""], ["Ma\u00f1ana-Rodr\u00edguez", "Jorge", ""]]}, {"id": "1705.04537", "submitter": "Fabian Kr\\\"uger", "authors": "Johanna F. Ziegel, Fabian Kr\\\"uger, Alexander Jordan, Fernando\n  Fasciati", "title": "Murphy Diagrams: Forecast Evaluation of Expected Shortfall", "comments": null, "journal-ref": null, "doi": null, "report-no": "Discussion paper nr. 632, AWI, Heidelberg University", "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the Basel 3 regulations, recent studies have considered joint\nforecasts of Value-at-Risk and Expected Shortfall. A large family of scoring\nfunctions can be used to evaluate forecast performance in this context.\nHowever, little intuitive or empirical guidance is currently available, which\nrenders the choice of scoring function awkward in practice. We therefore\ndevelop graphical checks (Murphy diagrams) of whether one forecast method\ndominates another under a relevant class of scoring functions, and propose an\nassociated hypothesis test. We illustrate these tools with simulation examples\nand an empirical analysis of S&P 500 and DAX returns.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 12:24:40 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Ziegel", "Johanna F.", ""], ["Kr\u00fcger", "Fabian", ""], ["Jordan", "Alexander", ""], ["Fasciati", "Fernando", ""]]}, {"id": "1705.04557", "submitter": "Andreas Jakobsson", "authors": "Shiwen Lei, Andreas Jakobsson, Zhiqin Zhao", "title": "CFAR Adaptive Matched Detector for Target Detection in Non-Gaussian\n  Noise With Inverse Gamma Texture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptive matched detector of a signal corrupted\nby a non-Gaussian noise with an inverse gamma texture. The detector is formed\nusing a set of secondary data measurements, and is analytically shown to have a\nconstant false alarm rate. The analytic performance is validated using Monte\nCarlo simulations, and the proposed detector is shown to offer preferable\nperformance as compared to the related one-step generalized likelihood ratio\ntest (1S-GLRT) and the adaptive subspace detector (ASD).\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:10:57 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Lei", "Shiwen", ""], ["Jakobsson", "Andreas", ""], ["Zhao", "Zhiqin", ""]]}, {"id": "1705.04584", "submitter": "Haiming Zhou", "authors": "Haiming Zhou, Timothy Hanson, Jiajia Zhang", "title": "spBayesSurv: Fitting Bayesian Spatial Survival Models Using R", "comments": "arXiv admin note: text overlap with arXiv:1701.06976", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial survival analysis has received a great deal of attention over the\nlast 20 years due to the important role that geographical information can play\nin predicting survival. This paper provides an introduction to a set of\nprograms for implementing some Bayesian spatial survival models in R using the\npackage spBayesSurv. The function survregbayes includes the three most\ncommonly-used semiparametric models: proportional hazards, proportional odds,\nand accelerated failure time. All manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated. Time-dependent covariates are allowed under the\npiecewise constant assumption. Both georeferenced and areally observed spatial\nlocations are handled via frailties. Model fit is assessed with conditional\nCox-Snell residual plots, and model choice is carried out via the log pseudo\nmarginal likelihood, the deviance information criterion and the Watanabe-Akaike\ninformation criterion. The accelerated failure time frailty model with a\ncovariate-dependent baseline is included in the function frailtyGAFT. In\naddition, the package also provides two marginal survival models: proportional\nhazards and linear dependent Dirichlet process mixture, where the spatial\ndependence is modeled via spatial copulas. Note that the package can also\nhandle non-spatial data using non-spatial versions of aforementioned models.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 19:24:03 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:07:15 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 16:36:00 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zhou", "Haiming", ""], ["Hanson", "Timothy", ""], ["Zhang", "Jiajia", ""]]}, {"id": "1705.04757", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa and Andrew Wallace", "title": "Multiple Target, Multiple Type Filtering in the RFS Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multiple Target, Multiple Type Filtering (MTMTF) algorithm is developed\nusing Random Finite Set (RFS) theory. First, we extend the standard Probability\nHypothesis Density (PHD) filter for multiple types of targets, each with\ndistinct detection properties, to develop a multiple target, multiple type\nfiltering, N-type PHD filter, where $N\\geq2$, for handling confusions among\ntarget types. In this approach, we assume that there will be confusions between\ndetections, i.e. clutter arises not just from background false positives, but\nalso from target confusions. Then, under the assumptions of Gaussianity and\nlinearity, we extend the Gaussian mixture (GM) implementation of the standard\nPHD filter for the proposed N-type PHD filter termed the N-type GM-PHD filter.\nFurthermore, we analyze the results from simulations to track sixteen targets\nof four different types using a four-type (quad) GM-PHD filter as a typical\nexample and compare it with four independent GM-PHD filters using the Optimal\nSubpattern Assignment (OSPA) metric. This shows the improved performance of our\nstrategy that accounts for target confusions by efficiently discriminating\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 21:33:49 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 15:40:04 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 14:38:36 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 10:22:18 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 16:46:11 GMT"}, {"version": "v6", "created": "Mon, 4 Feb 2019 19:17:34 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Baisa", "Nathanael L.", ""], ["Wallace", "Andrew", ""]]}, {"id": "1705.04813", "submitter": "Christian Mulder PhD", "authors": "Teodoro Semeraro, Norbert Marwan, Bruce K. Jones, Roberta Aretano,\n  Maria Rita Pasimeni, Irene Petrosillo, Christian Mulder, and Giovanni Zurlini", "title": "Recurrence Analysis of Vegetation Time Series and Phase Transitions in\n  Mediterranean Rangelands", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mediterranean rangelands should be conceived as socio-ecological landscapes\n(SEL) because of the close interaction and coevolution between socio-economic\nand natural systems. A significant threat to these Mediterranean rangelands is\nrelated to uncontrolled fires that can cause potential damages due to the\nreduction or even the loss of ecosystems. Our results show that time series of\nforest and grassland for unburned and burned areas are characterized by both\nperiodic and chaotic components. The fire event caused a clear simplification\nof vegetation structures as well as of SEL dynamics that is more regular and\npredictable after the burning and less chaotic. However grassland evolution\ncould be more predictable than forest considering the effect of fire\ndisturbance on successional cycles and stages of the two land-cover types. In\nparticular, we applied recurrence analysis with sliding temporal windows\nthree-year length on the original time series. This analysis indicates that\ngrasslands and forests behaved similarly in correspondence with the burning,\nalthough their phase states slowly diverge after fire. Recurrence is useful to\nstudy the vegetation recovery as it enables mapping landscape transitions\nderived from remote sensing. The approach helps stakeholders to draw landscape\ninterventions and improve management strategies to sustain the delivery of\necosystem services.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 12:11:41 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Semeraro", "Teodoro", ""], ["Marwan", "Norbert", ""], ["Jones", "Bruce K.", ""], ["Aretano", "Roberta", ""], ["Pasimeni", "Maria Rita", ""], ["Petrosillo", "Irene", ""], ["Mulder", "Christian", ""], ["Zurlini", "Giovanni", ""]]}, {"id": "1705.05180", "submitter": "Ivan Kiskin", "authors": "Ivan Kiskin, Bernardo P\\'erez Orozco, Theo Windebank, Davide Zilli,\n  Marianne Sinka, Kathy Willis and Stephen Roberts", "title": "Mosquito Detection with Neural Networks: The Buzz of Deep Learning", "comments": "For data and software related to this paper, see\n  http://humbug.ac.uk/kiskin2017/. Submitted as a conference paper to ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world time-series analysis problems are characterised by scarce\ndata. Solutions typically rely on hand-crafted features extracted from the time\nor frequency domain allied with classification or regression engines which\ncondition on this (often low-dimensional) feature vector. The huge advances\nenjoyed by many application domains in recent years have been fuelled by the\nuse of deep learning architectures trained on large data sets. This paper\npresents an application of deep learning for acoustic event detection in a\nchallenging, data-scarce, real-world problem. Our candidate challenge is to\naccurately detect the presence of a mosquito from its acoustic signature. We\ndevelop convolutional neural networks (CNNs) operating on wavelet\ntransformations of audio recordings. Furthermore, we interrogate the network's\npredictive power by visualising statistics of network-excitatory samples. These\nvisualisations offer a deep insight into the relative informativeness of\ncomponents in the detection problem. We include comparisons with conventional\nclassifiers, conditioned on both hand-tuned and generic features, to stress the\nstrength of automatic deep feature learning. Detection is achieved with\nperformance metrics significantly surpassing those of existing algorithmic\nmethods, as well as marginally exceeding those attained by individual human\nexperts.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:19:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kiskin", "Ivan", ""], ["Orozco", "Bernardo P\u00e9rez", ""], ["Windebank", "Theo", ""], ["Zilli", "Davide", ""], ["Sinka", "Marianne", ""], ["Willis", "Kathy", ""], ["Roberts", "Stephen", ""]]}, {"id": "1705.05238", "submitter": "Swasti Khuntia", "authors": "Swasti R. Khuntia, Jos\\'e L. Rueda, and Mart A. M. M. van der Meijden", "title": "Long-Term Load Forecasting Considering Volatility Using Multiplicative\n  Error Model", "comments": "19 pages, 11 figures, 3 tables", "journal-ref": "Energies 2018", "doi": "10.3390/en11123308", "report-no": "3308", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term load forecasting plays a vital role for utilities and planners in\nterms of grid development and expansion planning. An overestimate of long-term\nelectricity load will result in substantial wasted investment in the\nconstruction of excess power facilities, while an underestimate of future load\nwill result in insufficient generation and unmet demand. This paper presents\nfirst-of-its-kind approach to use multiplicative error model (MEM) in\nforecasting load for long-term horizon. MEM originates from the structure of\nautoregressive conditional heteroscedasticity (ARCH) model where conditional\nvariance is dynamically parameterized and it multiplicatively interacts with an\ninnovation term of time-series. Historical load data, accessed from a U.S.\nregional transmission operator, and recession data for years 1993-2016 is used\nin this study. The superiority of considering volatility is proven by\nout-of-sample forecast results as well as directional accuracy during the great\neconomic recession of 2008. To incorporate future volatility, backtesting of\nMEM model is performed. Two performance indicators used to assess the proposed\nmodel are mean absolute percentage error (for both in-sample model fit and\nout-of-sample forecasts) and directional accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 13:46:17 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 09:13:17 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 10:32:09 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Khuntia", "Swasti R.", ""], ["Rueda", "Jos\u00e9 L.", ""], ["van der Meijden", "Mart A. M. M.", ""]]}, {"id": "1705.05265", "submitter": "Seyoung Park", "authors": "Seyoung Park, Kerby Shedden, and Shuheng Zhou", "title": "Non-separable covariance models for spatio-temporal data, with\n  applications to neural encoding analysis", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural encoding studies explore the relationships between measurements of\nneural activity and measurements of a behavior that is viewed as a response to\nthat activity. The coupling between neural and behavioral measurements is\ntypically imperfect and difficult to measure.To enhance our ability to\nunderstand neural encoding relationships, we propose that a behavioral\nmeasurement may be decomposable as a sum of two latent components, such that\nthe direct neural influence and prediction is primarily localized to the\ncomponent which encodes temporal dependence. For this purpose, we propose to\nuse a non-separable Kronecker sum covariance model to characterize the\nbehavioral data as the sum of terms with exclusively trial-wise, and\nexclusively temporal dependencies. We then utilize a corrected form of Lasso\nregression in combination with the nodewise regression approach for estimating\nthe conditional independence relationships between and among variables for each\ncomponent of the behavioral data, where normality is necessarily assumed. We\nprovide the rate of convergence for estimating the precision matrices\nassociated with the temporal as well as spatial components in the Kronecker sum\nmodel. We illustrate our methods and theory using simulated data, and data from\na neural encoding study of hawkmoth flight; we demonstrate that the neural\nencoding signal for hawkmoth wing strokes is primarily localized to a latent\ncomponent with temporal dependence, which is partially obscured by a second\ncomponent with trial-wise dependencies.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:26:53 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Park", "Seyoung", ""], ["Shedden", "Kerby", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1705.05391", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, Martin J.\n  Wainwright", "title": "Optimal Rates and Tradeoffs in Multiple Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a central topic in statistics, but despite\nabundant work on the false discovery rate (FDR) and the corresponding Type-II\nerror concept known as the false non-discovery rate (FNR), a fine-grained\nunderstanding of the fundamental limits of multiple testing has not been\ndeveloped. Our main contribution is to derive a precise non-asymptotic tradeoff\nbetween FNR and FDR for a variant of the generalized Gaussian sequence model.\nOur analysis is flexible enough to permit analyses of settings where the\nproblem parameters vary with the number of hypotheses $n$, including various\nsparse and dense regimes (with $o(n)$ and $\\mathcal{O}(n)$ signals). Moreover,\nwe prove that the Benjamini-Hochberg algorithm as well as the Barber-Cand\\`{e}s\nalgorithm are both rate-optimal up to constants across these regimes.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:00:25 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1705.05496", "submitter": "Leif Ellingson", "authors": "Chalani Prematilake, Leif Ellingson", "title": "Evaluation and Prediction of Polygon Approximations of Planar Contours\n  for Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contours may be viewed as the 2D outline of the image of an object. This type\nof data arises in medical imaging as well as in computer vision and can be\nmodeled as data on a manifold and can be studied using statistical shape\nanalysis. Practically speaking, each observed contour, while theoretically\ninfinite dimensional, must be discretized for computations. As such, the\ncoordinates for each contour as obtained at k sampling times, resulting in the\ncontour being represented as a k-dimensional complex vector. While choosing\nlarge values of k will result in closer approximations to the original contour,\nthis will also result in higher computational costs in the subsequent analysis.\nThe goal of this study is to determine reasonable values for k so as to keep\nthe computational cost low while maintaining accuracy. To do this, we consider\ntwo methods for selecting sample points and determine lower bounds for k for\nobtaining a desired level of approximation error using two different criteria.\nBecause this process is computationally inefficient to perform on a large\nscale, we then develop models for predicting the lower bounds for k based on\nsimple characteristics of the contours.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 01:15:11 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Prematilake", "Chalani", ""], ["Ellingson", "Leif", ""]]}, {"id": "1705.05715", "submitter": "Ashutosh Maurya", "authors": "Ashutosh K. Maurya", "title": "Data Sharing and Resampled LASSO: A word based sentiment Analysis for\n  IMDb data", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study variable selection problem using LASSO with new\nimprovisations. LASSO uses $\\ell_{1}$ penalty, it shrinks most of the\ncoefficients to zero when number of explanatory variables $(p)$ are much larger\nthe number of observations $(N)$. Novelty of the approach developed in this\narticle blends basic ideas behind resampling and LASSO together which provides\na significant variable reduction and improved prediction accuracy in terms of\nmean squared error in the test sample. Different weighting schemes have been\nexplored using Bootstrapped LASSO, the basic methodology developed in here.\nWeighting schemes determine to what extent of data blending in case of grouped\ndata. Data sharing (DSL) technique developed by [11] lies at the root of the\npresent methodology. We apply the technique to analyze the IMDb dataset as\ndiscussed in [11] and compare our result with [11].\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:13:47 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 05:34:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Maurya", "Ashutosh K.", ""]]}, {"id": "1705.05752", "submitter": "Guillaume Basse", "authors": "Guillaume Basse and Edoardo Airoldi", "title": "Limitations of design-based causal inference and A/B testing under\n  arbitrary and network interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments on a network often involve interference between\nconnected units; i.e., a situation in which an individual's treatment can\naffect the response of another individual. Current approaches to deal with\ninterference, in theory and in practice, often make restrictive assumptions on\nits structure---for instance, assuming that interference is local---even when\nusing otherwise nonparametric inference strategies. This reliance on explicit\nrestrictions on the interference mechanism suggests a shared intuition that\ninference is impossible without any assumptions on the interference structure.\nIn this paper, we begin by formalizing this intuition in the context of a\nclassical nonparametric approach to inference, referred to as design-based\ninference of causal effects. Next, we show how, always in the context of\ndesign-based inference, even parametric structural assumptions that allow the\nexistence of unbiased estimators, cannot guarantee a decreasing variance even\nin the large sample limit. This lack of concentration in large samples is often\nobserved empirically, in randomized experiments in which interference of some\nform is expected to be present. This result has direct consequences for the\ndesign and analysis of large experiments---for instance, in online social\nplatforms---where the belief is that large sample sizes automatically guarantee\nsmall variance. More broadly, our results suggest that although strategies for\ncausal inference in the presence of interference borrow their formalism and\nmain concepts from the traditional causal inference literature, much of the\nintuition from the no-interference case do not easily transfer to the\ninterference setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:07:51 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Basse", "Guillaume", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1705.05831", "submitter": "Eiji Konaka", "authors": "Eiji Konaka", "title": "Match results prediction ability of official ATP singles ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses match result prediction ability of ATP ranking points,\nwhich is official ranking points for men's professional tennis players. The\nstructure of overall ATP World Tour and the ranking point attribution system\nleads that the ranking point ratio between two players is an essential\nvariable. The match result prediction model is a logistic model. The fact is\nverified using statistics of over 24000 matches from 2009.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 00:58:58 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Konaka", "Eiji", ""]]}, {"id": "1705.05962", "submitter": "Carlos Domingo-Felez", "authors": "Carlos Domingo-F\\'elez, Mar\\'ia Calder\\'o-Pascual, G\\\"urkan Sin,\n  Benedek Gy Pl\\'osz and Barth F. Smets", "title": "Calibration of the NDHA model to describe N2O dynamics from\n  respirometric assays", "comments": "Main text (27 pages, 7 figures, 2 tables) and Supplementary\n  Information (25 pages, 10 sections)", "journal-ref": null, "doi": "10.1016/j.watres.2017.09.013", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The NDHA model comprehensively describes nitrous oxide (N2O) producing\npathways by both autotrophic ammonium oxidizing and heterotrophic bacteria. The\nmodel was calibrated via a set of targeted extant respirometric assays using\nenriched nitrifying biomass from a lab-scale reactor. Biomass response to\nammonium, hydroxylamine, nitrite and N2O additions under aerobic and anaerobic\nconditions were tracked with continuous measurement of dissolved oxygen (DO)\nand N2O. The sequential addition of substrate pulses allowed the isolation of\noxygen-consuming processes. The parameters to be estimated were determined by\nthe information content of the datasets using identifiability analysis. Dynamic\nDO profiles were used to calibrate five parameters corresponding to endogenous,\nnitrite oxidation and ammonium oxidation processes. The subsequent N2O\ncalibration was not significantly affected by the uncertainty propagated from\nthe DO calibration because of the high accuracy of the estimates. Five\nparameters describing the individual contribution of three biological N2O\npathways were estimated accurately (variance/mean < 10% for all estimated\nparameters). The NDHA model response was evaluated with statistical metrics\n(F-test, autocorrelation function). The 95% confidence intervals of DO and N2O\npredictions based on the uncertainty obtained during calibration are studied\nfor the first time. The measured data fall within the 95% confidence interval\nof the predictions, indicating a good model description. Overall, accurate\nparameter estimation and identifiability analysis of ammonium removal\nsignificantly decreases the uncertainty propagated to N2O production, which is\nexpected to benefit N2O model discrimination studies and reliable full scale\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 00:16:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Domingo-F\u00e9lez", "Carlos", ""], ["Calder\u00f3-Pascual", "Mar\u00eda", ""], ["Sin", "G\u00fcrkan", ""], ["Pl\u00f3sz", "Benedek Gy", ""], ["Smets", "Barth F.", ""]]}, {"id": "1705.06025", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Yang Gu", "title": "Joint Positioning and Radio Map Generation Based on Stochastic\n  Variational Bayesian Inference for FWIPS", "comments": "10 pages, 16 figures, and 2 tables. A paper under review of IPIN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprinting based WLAN indoor positioning system (FWIPS) provides a\npromising indoor positioning solution to meet the growing interests for indoor\nlocation-based services (e.g., indoor way finding or geo-fencing). FWIPS is\npreferred because it requires no additional infrastructure for deploying an\nFWIPS and achieving the position estimation by reusing the available WLAN and\nmobile devices, and capable of providing absolute position estimation. For\nfingerprinting based positioning (FbP), a model is created to provide reference\nvalues of observable features (e.g., signal strength from access point (AP)) as\na function of location during offline stage. One widely applied method to build\na complete and an accurate reference database (i.e. radio map (RM)) for FWIPS\nis carrying out a site survey throughout the region of interest (RoI). Along\nthe site survey, the readings of received signal strength (RSS) from all\nvisible APs at each reference point (RP) are collected. This site survey,\nhowever, is time-consuming and labor-intensive, especially in the case that the\nRoI is large (e.g., an airport or a big mall). This bottleneck hinders the wide\ncommercial applications of FWIPS (e.g., proximity promotions in a shopping\ncenter). To diminish the cost of site survey, we propose a probabilistic model,\nwhich combines fingerprinting based positioning (FbP) and RM generation based\non stochastic variational Bayesian inference (SVBI). This SVBI based position\nand RSS estimation has three properties: i) being able to predict the\ndistribution of the estimated position and RSS, ii) treating each observation\nof RSS at each RP as an example to learn for FbP and RM generation instead of\nusing the whole RM as an example, and iii) requiring only one time training of\nthe SVBI model for both localization and RSS estimation. These benefits make it\noutperforms the previous proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 06:48:05 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Zhou", "Caifa", ""], ["Gu", "Yang", ""]]}, {"id": "1705.06073", "submitter": "Thomas Lundgaard Hansen", "authors": "Thomas Lundgaard Hansen, Bernard Henri Fleury and Bhaskar D. Rao", "title": "Superfast Line Spectral Estimation", "comments": "16 pages, 7 figures, accepted for IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2018.2807417", "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have proposed to solve the line spectral estimation\nproblem by applying off-the-grid extensions of sparse estimation techniques.\nThese methods are preferable over classical line spectral estimation algorithms\nbecause they inherently estimate the model order. However, they all have\ncomputation times which grow at least cubically in the problem size, thus\nlimiting their practical applicability in cases with large dimensions. To\nalleviate this issue, we propose a low-complexity method for line spectral\nestimation, which also draws on ideas from sparse estimation. Our method is\nbased on a Bayesian view of the problem. The signal covariance matrix is shown\nto have Toeplitz structure, allowing superfast Toeplitz inversion to be used.\nWe demonstrate that our method achieves estimation accuracy at least as good as\ncurrent methods and that it does so while being orders of magnitudes faster.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 10:03:50 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 09:16:21 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Hansen", "Thomas Lundgaard", ""], ["Fleury", "Bernard Henri", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1705.06502", "submitter": "Chee-Ming Ting PhD", "authors": "Chee-Ming Ting, Hernando Ombao, Sh-Hussain Salleh", "title": "Multi-Scale Factor Analysis of High-Dimensional Brain Signals", "comments": "43 pages", "journal-ref": "IEEE Trans. Network Science and Engineering 7(1), 449 - 465, 2020", "doi": "10.1109/TNSE.2018.2869862", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an approach to modeling high-dimensional networks\nwith a large number of nodes arranged in a hierarchical and modular structure.\nWe propose a novel multi-scale factor analysis (MSFA) model which partitions\nthe massive spatio-temporal data defined over the complex networks into a\nfinite set of regional clusters. To achieve further dimension reduction, we\nrepresent the signals in each cluster by a small number of latent factors. The\ncorrelation matrix for all nodes in the network are approximated by\nlower-dimensional sub-structures derived from the cluster-specific factors. To\nestimate regional connectivity between numerous nodes (within each cluster), we\napply principal components analysis (PCA) to produce factors which are derived\nas the optimal reconstruction of the observed signals under the squared loss.\nThen, we estimate global connectivity (between clusters or sub-networks) based\non the factors across regions using the RV-coefficient as the cross-dependence\nmeasure. This gives a reliable and computationally efficient multi-scale\nanalysis of both regional and global dependencies of the large networks. The\nproposed novel approach is applied to estimate brain connectivity networks\nusing functional magnetic resonance imaging (fMRI) data. Results on\nresting-state fMRI reveal interesting modular and hierarchical organization of\nhuman brain networks during rest.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 10:05:56 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""], ["Salleh", "Sh-Hussain", ""]]}, {"id": "1705.06505", "submitter": "Martina Vittorietti", "authors": "Martina Vittorietti, Geurt Jongbloed, Piet J.J. Kok, Jilt Sietsma", "title": "Accurate approximation of the distributions of the 3D Poisson-Voronoi\n  typical cell geometrical features", "comments": "15 pages, 12 images (7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Poisson-Voronoi diagrams have interesting mathematical properties,\nthere is still much to discover about the geometrical properties of its grains.\nThrough simulations, many authors were able to obtain numerical approximations\nof the moments of the distributions of more or less all geometrical\ncharacteristics of the grain. Furthermore, many proposals on how to get close\nparametric approximations to the real distributions were put forward by several\nauthors. In this paper we show that exploiting the scaling property of the\nunderlying Poisson process, we are able to derive the distribution of the main\ngeometrical features of the grain for every value of the intensity parameter.\nMoreover, we use a sophisticated simulation program to construct a close Monte\nCarlo based approximation for the distributions of interest. Using this, we\nalso determine the closest approximating distributions within the mentioned\nfrequently used parametric classes of distributions and conclude that these\napproximations can be quite accurate.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 10:13:11 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Vittorietti", "Martina", ""], ["Jongbloed", "Geurt", ""], ["Kok", "Piet J. J.", ""], ["Sietsma", "Jilt", ""]]}, {"id": "1705.06721", "submitter": "Alexander V. Mantzaris Dr", "authors": "Alexander V. Mantzaris, Samuel R. Rein, Alexander D. Hopkins", "title": "Examining collusion and voting biases between countries during the\n  Eurovision song contest since 1957", "comments": "to be published in JASSS", "journal-ref": "Journal of Artificial Societies and Social Simulation 21 (1) 1,\n  2018", "doi": "10.18564/jasss.3580", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Eurovision Song Contest (ESC) is an annual event which attracts millions\nof viewers. It is an interesting activity to examine since the participants of\nthe competition represent a particular country's musical performance that will\nbe awarded a set of scores from other participating countries based upon a\nquality assessment of a performance. There is a question of whether the\ncountries will vote exclusively according to the artistic merit of the song, or\nif the vote will be a public signal of national support for another country.\nSince the competition aims to bring people together, any consistent biases in\nthe awarding of scores would defeat the purpose of the celebration of\nexpression and this has attracted researchers to investigate the supporting\nevidence for biases. This paper builds upon an approach which produces a set of\nrandom samples from an unbiased distribution of score allocation, and extends\nthe methodology to use the full set of years of the competition's life span\nwhich has seen fundamental changes to the voting schemes adopted.\n  By building up networks from statistically significant edge sets of vote\nallocations during a set of years, the results display a plausible network for\nthe origins of the culture anchors for the preferences of the awarded votes.\nWith 60 years of data, the results support the hypothesis of regional collusion\nand biases arising from proximity, culture and other irrelevant factors in\nregards to the music which that alone is intended to affect the judgment of the\ncontest.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:49:56 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 19:48:08 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mantzaris", "Alexander V.", ""], ["Rein", "Samuel R.", ""], ["Hopkins", "Alexander D.", ""]]}, {"id": "1705.06732", "submitter": "Francisco Traversaro Prof.", "authors": "Francisco Traversaro, Francisco Redelico", "title": "Confidence Intervals and Hypothesis Testing for the Permutation Entropy\n  with an application to Epilepsy", "comments": null, "journal-ref": null, "doi": "10.1016/j.cnsns.2017.10.013", "report-no": null, "categories": "stat.ME physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear dynamics, and to a lesser extent in other fields, a widely used\nmeasure of complexity is the Permutation Entropy. But there is still no known\nmethod to determine the accuracy of this measure. There has been little\nresearch on the statistical properties of this quantity that characterize time\nseries. The literature describes some resampling methods of quantities used in\nnonlinear dynamics - as the largest Lyapunov exponent - but all of these seems\nto fail. In this contribution we propose a parametric bootstrap methodology\nusing a symbolic representation of the time series in order to obtain the\ndistribution of the Permutation Entropy estimator. We perform several time\nseries simulations given by well known stochastic processes: the 1=f? noise\nfamily, and show in each case that the proposed accuracy measure is as\nefficient as the one obtained by the frequentist approach of repeating the\nexperiment. The complexity of brain electrical activity, measured by the\nPermutation Entropy, has been extensively used in epilepsy research for\ndetection in dynamical changes in electroencephalogram (EEG) signal with no\nconsideration of the variability of this complexity measure. An application of\nthe parametric bootstrap methodology is used to compare normal and pre-ictal\nEEG signals.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 03:17:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Traversaro", "Francisco", ""], ["Redelico", "Francisco", ""]]}, {"id": "1705.06760", "submitter": "Valerie Robert", "authors": "Valerie Robert, Yann Vasseur, Vincent Brault", "title": "Comparing high dimensional partitions, with the Coclustering Adjusted\n  Rand Index", "comments": "52 pages", "journal-ref": null, "doi": "10.1007/s00357-020-09379-w", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the simultaneous clustering of rows and columns of a matrix and\nmore particularly the ability to measure the agreement between two\nco-clustering partitions. The new criterion we developed is based on the\nAdjusted Rand Index and is called the Co-clustering Adjusted Rand Index named\nCARI. We also suggest new improvements to existing criteria such as the\nClassification Error which counts the proportion of misclassified cells and the\nExtended Normalized Mutual Information criterion which is a generalization of\nthe criterion based on mutual information in the case of classic\nclassifications. We study these criteria with regard to some desired properties\nderiving from the co-clustering context. Experiments on simulated and real\nobserved data are proposed to compare the behavior of these criteria.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 18:26:59 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 13:12:37 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 06:30:55 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Robert", "Valerie", ""], ["Vasseur", "Yann", ""], ["Brault", "Vincent", ""]]}, {"id": "1705.06771", "submitter": "Curtis Storlie", "authors": "Curtis B Storlie, Megan E Branda, Michael R Gionfriddo, Nilay D Shah,\n  Matthew A Rank", "title": "Prediction of Individual Outcomes for Asthma Sufferers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of individual-specific medication level\nrecommendation (initiation, removal, increase, or decrease) for asthma\nsufferers. Asthma is one of the most common chronic diseases in both adults and\nchildren, affecting 8% of the US population and costing $37-63 billion/year in\nthe US. Asthma is a complex disease, whose symptoms may wax and wane, making it\ndifficult for clinicians to predict outcomes and prognosis. Improved ability to\npredict prognosis can inform decision making and may promote conversations\nbetween clinician and provider around optimizing medication therapy. Data from\nthe US Medical Expenditure Panel Survey (MEPS) years 2000-2010 were used to fit\na longitudinal model for a multivariate response of adverse events (Emergency\nDepartment or In-patient visits, excessive rescue inhaler use, and oral steroid\nuse). To reduce bias in the estimation of medication effects, medication level\nwas treated as a latent process which was restricted to be consistent with\nprescription refill data. This approach is demonstrated to be effective in the\nMEPS cohort via predictions on a validation hold out set and a synthetic data\nsimulation study. This framework can be easily generalized to medication\ndecisions for other conditions as well.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:15:40 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Storlie", "Curtis B", ""], ["Branda", "Megan E", ""], ["Gionfriddo", "Michael R", ""], ["Shah", "Nilay D", ""], ["Rank", "Matthew A", ""]]}, {"id": "1705.06828", "submitter": "Laio Oriel Seman", "authors": "Laio Oriel Seman, Romeu Hausmann, Eduardo Augusto Bezerra", "title": "Agent-based simulation of the learning dissemination on a Project-Based\n  Learning context considering the human aspects", "comments": "8 pages, 6 figures, minor corrections", "journal-ref": null, "doi": "10.1109/TE.2017.2754987", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an agent-based simulation (ABS) of the active learning\nprocess in an Electrical Engineering course. In order to generate input data to\nthe simulation, an active learning methodology developed especially for\npart-time degree courses, called Project-Based Learning Agile (PBLA), has been\nproposed and implemented at the Regional University of Blumenau (FURB), Brazil.\nThrough the analysis of survey responses obtained over five consecutive\nsemesters, using partial least squares path modeling (PLS-PM), it was possible\nto generate data parameters to use as an input in a hybrid kind of agent-based\nsimulation known as PLS agent. The simulation of the scenario suggests that the\nlearning occur faster when the student has higher levels of humanist's aspects\nas self-esteem, self-realization and cooperation.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 23:10:37 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 01:43:40 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Seman", "Laio Oriel", ""], ["Hausmann", "Romeu", ""], ["Bezerra", "Eduardo Augusto", ""]]}, {"id": "1705.07158", "submitter": "Jethro Browell", "authors": "Jethro Browell, Daniel R. Drew, Kostas Philippopoulos", "title": "Improved Very-short-term Wind Forecasting using Atmospheric\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1002/we.2207", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a regime-switching vector-autoregressive method for\nvery-short-term wind speed forecasting at multiple locations with regimes based\non large-scale meteorological phenomena. Statistical methods short-term wind\nforecasting out-perform numerical weather prediction for forecast horizons up\nto a few hours, and the spatio-temporal interdependency between geographically\ndispersed locations may be exploited to improve forecast skill. Here we show\nthat conditioning spatio-temporal interdependency on `atmospheric modes' can\nfurther improve forecast performance. The modes are defined from the\natmospheric classification of wind and pressure fields at the surface level,\nand the geopotential height field at the 500hPa level. The data fields are\nextracted from the MERRA-2 reanalysis dataset with an hourly temporal\nresolution over the UK, atmospheric patterns are classified using\nself-organising maps and then clustered to optimise forecast performance. In a\ncase study based on 6 years of measurements from 23 weather stations in the UK,\na set of three atmospheric modes are found to be optimal for forecast\nperformance. The skill in the one- to six-hour-ahead forecasts is improved at\nall sites compared to persistence and competitive benchmarks. Across the 23\ntest sites, one-hour-ahead root mean squared error is reduced by between 0.3%\nand 4.1% compared to the best performing benchmark, and by and average of 1.6%\nover all sites; the six-hour-ahead accuracy is improved by an average of 3.1%.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:33:17 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Browell", "Jethro", ""], ["Drew", "Daniel R.", ""], ["Philippopoulos", "Kostas", ""]]}, {"id": "1705.07463", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Spatially Controlled Relay Beamforming: $2$-Stage Optimal Policies", "comments": "68 pages, 10 figures, this work constitutes an extended\n  preprint/version of a two part paper (soon to be) submitted for publication\n  to the IEEE Transactions on Signal Processing in Spring/Summer 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of enhancing Quality-of-Service (QoS) in power constrained,\nmobile relay beamforming networks, by optimally and dynamically controlling the\nmotion of the relaying nodes, is considered, in a dynamic channel environment.\nWe assume a time slotted system, where the relays update their positions before\nthe beginning of each time slot. Modeling the wireless channel as a Gaussian\nspatiotemporal stochastic field, we propose a novel $2$-stage stochastic\nprogramming problem formulation for optimally specifying the positions of the\nrelays at each time slot, such that the expected QoS of the network is\nmaximized, based on causal Channel State Information (CSI) and under a total\nrelay transmit power budget. This results in a schema where, at each time slot,\nthe relays, apart from optimally beamforming to the destination, also\noptimally, predictively decide their positions at the next time slot, based on\ncausally accumulated experience. Exploiting either the Method of Statistical\nDifferentials, or the multidimensional Gauss-Hermite Quadrature Rule, the\nstochastic program considered is shown to be approximately equivalent to a set\nof simple subproblems, which are solved in a distributed fashion, one at each\nrelay. Optimality and performance of the proposed spatially controlled system\nare also effectively assessed, under a rigorous technical framework; strict\noptimality is rigorously demonstrated via the development of a version of the\nFundamental Lemma of Stochastic Control, and, performance-wise, it is shown\nthat, quite interestingly, the optimal average network QoS exhibits an\nincreasing trend across time slots, despite our myopic problem formulation.\nNumerical simulations are presented, experimentally corroborating the success\nof the proposed approach and the validity of our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:29:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1705.07561", "submitter": "Rakshith Jagannath", "authors": "Rakshith Jagannath", "title": "Detection Estimation and Grid matching of Multiple Targets with Single\n  Snapshot Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the problems of detecting the number of narrow-band,\nfar-field targets and estimating their corresponding directions from single\nsnapshot measurements. The principles of sparse signal recovery (SSR) are used\nfor the single snapshot detection and estimation of multiple targets. In the\nSSR framework, the DoA estimation problem is grid based and can be posed as the\nlasso optimization problem. However, the SSR framework for DoA estimation gives\nrise to the grid mismatch problem, when the unknown targets (sources) are not\nmatched with the estimation grid chosen for the construction of the array\nsteering matrix at the receiver. The block sparse recovery framework is known\nto mitigate the grid mismatch problem by jointly estimating the targets and\ntheir corresponding offsets from the estimation grid using the group lasso\nestimator. The corresponding detection problem reduces to estimating the\noptimal regularization parameter ($\\tau$) of the lasso (in case of perfect\ngrid-matching) or group-lasso estimation problem for achieving the required\nprobability of correct detection ($P_c$). We propose asymptotic and finite\nsample test statistics for detecting the number of sources with the required\n$P_c$ at moderate to high signal to noise ratios. Once the number of sources\nare detected, or equivalently the optimal $\\hat{\\tau}$ is estimated, the\ncorresponding estimation and grid matching of the DoAs can be performed by\nsolving the lasso or group-lasso problem at $\\hat{\\tau}$\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:32:39 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Jagannath", "Rakshith", ""]]}, {"id": "1705.07872", "submitter": "Andr\\'es F. Barrientos", "authors": "Andr\\'es F. Barrientos, Alexander Bolton, Tom Balmat, Jerome P.\n  Reiter, John M. de Figueiredo, Ashwin Machanavajjhala, Yan Chen, Charley\n  Kneifel, Mark DeLong", "title": "Providing Access to Confidential Research Data Through Synthesis and\n  Verification: An Application to Data on Employees of the U.S. Federal\n  Government", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data stewards seeking to provide access to large-scale social science data\nface a difficult challenge. They have to share data in ways that protect\nprivacy and confidentiality, are informative for many analyses and purposes,\nand are relatively straightforward to use by data analysts. One approach\nsuggested in the literature is that data stewards generate and release\nsynthetic data, i.e., data simulated from statistical models, while also\nproviding users access to a verification server that allows them to assess the\nquality of inferences from the synthetic data. We present an application of the\nsynthetic data plus verification server approach to longitudinal data on\nemployees of the U.S. federal government. As part of the application, we\npresent a novel model for generating synthetic career trajectories, as well as\nstrategies for generating high dimensional, longitudinal synthetic datasets. We\nalso present novel verification algorithms for regression coefficients that\nsatisfy differential privacy. We illustrate the integrated use of synthetic\ndata plus verification via analysis of differentials in pay by race. The\nintegrated system performs as intended, allowing users to explore the synthetic\ndata for potential pay differentials and learn through verifications which\nfindings in the synthetic data hold up and which do not. The analysis on the\nconfidential data reveals pay differentials across races not documented in\npublished studies.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:31:59 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 18:52:17 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Barrientos", "Andr\u00e9s F.", ""], ["Bolton", "Alexander", ""], ["Balmat", "Tom", ""], ["Reiter", "Jerome P.", ""], ["de Figueiredo", "John M.", ""], ["Machanavajjhala", "Ashwin", ""], ["Chen", "Yan", ""], ["Kneifel", "Charley", ""], ["DeLong", "Mark", ""]]}, {"id": "1705.07926", "submitter": "Bradley Saul", "authors": "Bradley C. Saul, Michael G. Hudgens, Michael A. Mallin", "title": "Upstream Causes of Downstream Effects", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2019.1574226", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States Environmental Protection Agency considers nutrient\npollution in stream ecosystems one of the U.S. most pressing environmental\nchallenges. But limited independent replicates, lack of experimental\nrandomization, and space- and time-varying confounding handicap causal\ninference on effects of nutrient pollution. In this paper the causal g-methods\ndeveloped by Robins and colleagues are extended to allow for exposures to vary\nin time and space in order to assess the effects of nutrient pollution on\nchlorophyll a, a proxy for algal production. Publicly available data from the\nNorth Carolina Cape Fear River and a simulation study are used to show how\ncausal effects of upstream nutrient concentrations on downstream chlorophyll a\nlevels may be estimated from typical water quality monitoring data. Estimates\nobtained from the parametric g-formula, a marginal structural model, and a\nstructural nested model indicate that chlorophyll a concentrations at Lock and\nDam 1 were influenced by nitrate concentrations measured 86 to 109 km upstream,\nan area where four major industrial and municipal point sources discharge\nwastewater.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:09:43 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 15:47:22 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 14:36:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Saul", "Bradley C.", ""], ["Hudgens", "Michael G.", ""], ["Mallin", "Michael A.", ""]]}, {"id": "1705.07953", "submitter": "Raydonal Ospina", "authors": "Luciano Digiampietri, Leandro R\\^ego, Filipe Costa de Souza, Raydonal\n  Ospina, Jes\\'us Mena-Chalco", "title": "Brazilian Network of PhDs Working with Probability and Statistics", "comments": "Accepted for publication in BJPS. Manuscript ID: BJPS362", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical and probabilistic reasoning enlightens our judgments about\nuncertainty and the chance or beliefs on the occurrence of random events in\neveryday life. Therefore, there are scientists working with Probability and\nStatistics in various fields of knowledge, what favors the formation of\nscientific network collaborations of researchers with different backgrounds.\nHere, we propose to describe the Brazilian PhDs who work with probability and\nstatistics. In particular, we analyze national and states collaboration\nnetworks of such researchers by calculating different metrics. We show that\nthere is a greater concentration of nodes in and around the cites which host\nProbability and Statistics graduate programs. Moreover, the states that host P\n& S Doctoral programs are the most central. We also observe a disparity in the\nsize of the states networks. The clustering coefficient of the national network\nsuggests that this network and regional differences especially with respect to\nstates from South-east and North is not cohesive and, probably, it is in a\nmaturing stage\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:10:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Digiampietri", "Luciano", ""], ["R\u00eago", "Leandro", ""], ["de Souza", "Filipe Costa", ""], ["Ospina", "Raydonal", ""], ["Mena-Chalco", "Jes\u00fas", ""]]}, {"id": "1705.08001", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Grouped multivariate and functional time series forecasting: An\n  application to annuity pricing", "comments": "36 pages, 10 tables, 6 figures, forthcoming at Insurance: Mathematics\n  and Economics (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-specific mortality rates are often disaggregated by different attributes,\nsuch as sex, state, ethnic group and socioeconomic status. In making social\npolicies and pricing annuity at national and subnational levels, it is\nimportant not only to forecast mortality accurately, but also to ensure that\nforecasts at the subnational level add up to the forecasts at the national\nlevel. This motivates recent developments in grouped functional time series\nmethods (Shang and Hyndman, 2017) to reconcile age-specific mortality\nforecasts. We extend these grouped functional time series forecasting methods\nto multivariate time series, and apply them to produce point forecasts of\nmortality rates at older ages, from which fixed-term annuities for different\nages and maturities can be priced. Using the regional age-specific mortality\nrates in Japan obtained from the Japanese Mortality Database, we investigate\nthe one-step-ahead to 15-step-ahead point-forecast accuracy between the\nindependent and grouped forecasting methods. The grouped forecasting methods\nare shown not only to be useful for reconciling forecasts of age-specific\nmortality rates at national and subnational levels, but they are also shown to\nallow improved forecast accuracy. The improved forecast accuracy of mortality\nrates is of great interest to the insurance and pension industries for\nestimating annuity prices, in particular at the level of population subgroups,\ndefined by key factors such as sex, region, and socioeconomic grouping.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 20:59:37 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "1705.08006", "submitter": "Mainak Jas", "authors": "Mainak Jas and Tom Dupr\\'e La Tour and Umut \\c{S}im\\c{s}ekli and\n  Alexandre Gramfort", "title": "Learning the Morphology of Brain Signals Using Alpha-Stable\n  Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural time-series data contain a wide variety of prototypical signal\nwaveforms (atoms) that are of significant importance in clinical and cognitive\nresearch. One of the goals for analyzing such data is hence to extract such\n'shift-invariant' atoms. Even though some success has been reported with\nexisting algorithms, they are limited in applicability due to their heuristic\nnature. Moreover, they are often vulnerable to artifacts and impulsive noise,\nwhich are typically present in raw neural recordings. In this study, we address\nthese issues and propose a novel probabilistic convolutional sparse coding\n(CSC) model for learning shift-invariant atoms from raw neural signals\ncontaining potentially severe artifacts. In the core of our model, which we\ncall $\\alpha$CSC, lies a family of heavy-tailed distributions called\n$\\alpha$-stable distributions. We develop a novel, computationally efficient\nMonte Carlo expectation-maximization algorithm for inference. The maximization\nstep boils down to a weighted CSC problem, for which we develop a\ncomputationally efficient optimization algorithm. Our results show that the\nproposed algorithm achieves state-of-the-art convergence speeds. Besides,\n$\\alpha$CSC is significantly more robust to artifacts when compared to three\ncompeting algorithms: it can extract spike bursts, oscillations, and even\nreveal more subtle phenomena such as cross-frequency coupling when applied to\nnoisy neural time series.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:09:13 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:51:41 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Jas", "Mainak", ""], ["La Tour", "Tom Dupr\u00e9", ""], ["\u015eim\u015fekli", "Umut", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1705.08079", "submitter": "Luca Pappalardo", "authors": "Alessio Rossi and Luca Pappalardo and Paolo Cintia and Marcello Iaia\n  and Javier Fernandez and Daniel Medina", "title": "Effective injury forecasting in soccer with GPS training data and\n  machine learning", "comments": null, "journal-ref": "PLoS One 13(7) 2018", "doi": "10.1371/journal.pone.0201264", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injuries have a great impact on professional soccer, due to their large\ninfluence on team performance and the considerable costs of rehabilitation for\nplayers. Existing studies in the literature provide just a preliminary\nunderstanding of which factors mostly affect injury risk, while an evaluation\nof the potential of statistical models in forecasting injuries is still\nmissing. In this paper, we propose a multi-dimensional approach to injury\nforecasting in professional soccer that is based on GPS measurements and\nmachine learning. By using GPS tracking technology, we collect data describing\nthe training workload of players in a professional soccer club during a season.\nWe then construct an injury forecaster and show that it is both accurate and\ninterpretable by providing a set of case studies of interest to soccer\npractitioners. Our approach opens a novel perspective on injury prevention,\nproviding a set of simple and practical rules for evaluating and interpreting\nthe complex relations between injury risk and training performance in\nprofessional soccer.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:21:02 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 14:06:49 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Rossi", "Alessio", ""], ["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""], ["Iaia", "Marcello", ""], ["Fernandez", "Javier", ""], ["Medina", "Daniel", ""]]}, {"id": "1705.08082", "submitter": "Sebastian Del Barco", "authors": "Sebastian Del Barco, Erast Davidjuk", "title": "An Investigation of the Different Levels of Poverty and the\n  Corresponding Variance in Student Academic Prosperity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underprivileged students, especially in primary school, have shown to have\nless access to educational materials often resulting in general dissatisfaction\nin the school system and lower academic performance (Saatcioglu and Rury, 2012,\np.23). The relationship between family socioeconomic status and student\ninterest in academic endeavors, level of classroom engagement, and\nparticipation in extracurricular programs were analyzed. Socioeconomic status\nwas categorized as below poverty level, at or above poverty level, 100 to 199\npercent of poverty, and 200 percent of poverty or higher (United States Census\nBureau). Student interest, engagement, and persistence were measured as a\nscalar quantity of three variables: never, sometimes, and often. The\nparticipation of students in extracurricular activities was also compared based\non the same categories of socioeconomic status. After running the multivariate\nanalysis of variance, it was found that there was a statistically significant\nvariance of student academic prosperity and poverty level.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:26:26 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Del Barco", "Sebastian", ""], ["Davidjuk", "Erast", ""]]}, {"id": "1705.08323", "submitter": "Amanda Ramcharan", "authors": "Amanda Ramcharan, Tomislav Hengl, Travis Nauman, Colby Brungard,\n  Sharon Waltman, Skye Wills, James Thompson", "title": "Soil Property and Class Maps of the Conterminous US at 100 meter Spatial\n  Resolution based on a Compilation of National Soil Point Observations and\n  Machine Learning", "comments": "Submitted to Soil Science Society of America Journal, 40 pages, 12\n  figures, 3 tables", "journal-ref": null, "doi": "10.2136/sssaj2017.04.0122", "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing concern for the depletion of soil resources, conventional soil\ndata must be updated to support spatially explicit human-landscape models.\nThree US soil point datasetswere combined with a stack of over 200\nenvironmental datasets to generate complete coverage gridded predictions at 100\nm spatial resolution of soil properties (percent organic C, total N, bulk\ndensity, pH, and percent sand and clay) and US soil taxonomic classes (291\ngreat groups and 78 modified particle size classes) for the conterminous US.\nModels were built using parallelized random forest and gradient boosting\nalgorithms. Soil property predictions were generated at seven standard soil\ndepths (0, 5, 15, 30, 60, 100 and 200 cm). Prediction probability maps for US\nsoil taxonomic classifications were also generated. Model validation results\nindicate an out-of-bag classification accuracy of 60 percent for great groups,\nand 66 percent for modified particle size classes; for soil properties\ncross-validated R-square ranged from 62 percent for total N to 87 percent for\npH. Nine independent validation datasets were used to assess prediction\naccuracies for soil class models and results ranged between 24-58 percent and\n24-93 percent for great group and modified particle size class prediction\naccuracies, respectively. The hybrid \"SoilGrids+\" modeling system that\nincorporates remote sensing data, local predictions of soil properties,\nconventional soil polygon maps, and machine learning opens the possibility for\nupdating conventional soil survey data with machine learning technology to make\nsoil information easier to integrate with spatially explicit models, compared\nto multi-component map units.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:48:07 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 21:09:46 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 18:52:50 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 13:02:47 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ramcharan", "Amanda", ""], ["Hengl", "Tomislav", ""], ["Nauman", "Travis", ""], ["Brungard", "Colby", ""], ["Waltman", "Sharon", ""], ["Wills", "Skye", ""], ["Thompson", "James", ""]]}, {"id": "1705.08516", "submitter": "Zhanwei Du", "authors": "Zhanwei Du, Jiming Liu, Songwei Shan", "title": "An Open-Data Analysis of Heterogeneities in Lung Cancer Premature\n  Mortality Rate and Associated Factors among Toronto Neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In public health, various data are rigorously collected and published with\nopen access. These data reflect the environmental and non-environmental\ncharacteristics of heterogeneous neighborhoods in cities. In the present study,\nwe aimed to study the relations between these data and disease risks in\nheterogeneous neighborhoods. A flexible framework was developed to determine\nthe key factors correlated with diseases and find the most relevant combination\nof factors to explain observations of diseases through nonlinear analyses.\nTaking Lung Cancer Premature Mortality Rate (LCPMR) in Toronto as an example,\ntwo environmental factors (green space, and industrial pollution) and two\nnon-environmental factors (immigrants, and mental health visits) were\nidentified in the relational analysis of all of the target neighborhoods. To\ndetermine the influence of the heterogeneity of the neighborhoods, they were\nclustered into three different classes. In the most severe class, two\nadditional factors related to dwellings were determined to be involved, which\nincreased the observation's deviance from 48.1% to 80%. The factors determined\nin this study may assist governments in improving public health policies.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 09:02:14 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Du", "Zhanwei", ""], ["Liu", "Jiming", ""], ["Shan", "Songwei", ""]]}, {"id": "1705.08533", "submitter": "Lina Kulakova", "authors": "Lina Kulakova and Georgios Arampatzis and Panagiotis Angelikopoulos\n  and Panagiotis Chatzidoukas and Costas Papadimitriou and Petros Koumoutsakos", "title": "Experimental data over quantum mechanics simulations for inferring the\n  repulsive exponent of the Lennard-Jones potential in Molecular Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)\nsimulations and among the most widely used computational kernels in science.\nThe potential models atomistic attraction and repulsion with century old\nprescribed parameters ($q=6, \\; p=12$, respectively), originally related by a\nfactor of two for simplicity of calculations. We re-examine the value of the\nrepulsion exponent through data driven uncertainty quantification. We perform\nHierarchical Bayesian inference on MD simulations of argon using experimental\ndata of the radial distribution function (RDF) for a range of thermodynamic\nconditions, as well as dimer interaction energies from quantum mechanics\nsimulations. The experimental data suggest a repulsion exponent ($p \\approx\n6.5$), in contrast to the quantum simulations data that support values closer\nto the original ($p=12$) exponent. Most notably, we find that predictions of\nRDF, diffusion coefficient and density of argon are more accurate and robust in\nproducing the correct argon phase around its triple point, when using the\nvalues inferred from experimental data over those from quantum mechanics\nsimulations. The present results suggest the need for data driven recalibration\nof the LJ potential across MD simulations.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 22:36:27 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Kulakova", "Lina", ""], ["Arampatzis", "Georgios", ""], ["Angelikopoulos", "Panagiotis", ""], ["Chatzidoukas", "Panagiotis", ""], ["Papadimitriou", "Costas", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1705.08805", "submitter": "Marta Crispino", "authors": "Marta Crispino, Elja Arjas, Valeria Vitelli, Natasha Barrett and\n  Arnoldo Frigessi", "title": "A Bayesian Mallows approach to non-transitive pair comparison data: how\n  human are sounds?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning how listeners perceive sounds as having human\norigins. An experiment was performed with a series of electronically\nsynthesized sounds, and listeners were asked to compare them in pairs. We\npropose a Bayesian probabilistic method to learn individual preferences from\nnon-transitive pairwise comparison data, as happens when one (or more)\nindividual preferences in the data contradicts what is implied by the others.\nWe build a Bayesian Mallows model in order to handle non-transitive data, with\na latent layer of uncertainty which captures the generation of preference\nmisreporting. We then develop a mixture extension of the Mallows model, able to\nlearn individual preferences in a heterogeneous population. The results of our\nanalysis of the musicology experiment are of interest to electroacoustic\ncomposers and sound designers, and to the audio industry in general, whose aim\nis to understand how computer generated sounds can be produced in order to\nsound more human.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:52:53 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:41:22 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Crispino", "Marta", ""], ["Arjas", "Elja", ""], ["Vitelli", "Valeria", ""], ["Barrett", "Natasha", ""], ["Frigessi", "Arnoldo", ""]]}, {"id": "1705.08815", "submitter": "Francesco Fusco", "authors": "Francesco Fusco, Seshu Tirupathi and Robert Gormally", "title": "Power Systems Data Fusion based on Belief Propagation", "comments": "Version as accepted for publication at the 7th IEEE International\n  Conference on Innovative Smart Grid Technologies (ISGT) Europe 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of the power grid, due to higher penetration of\ndistributed resources and the growing availability of interconnected,\ndistributed metering devices re- quires novel tools for providing a unified and\nconsistent view of the system. A computational framework for power systems data\nfusion, based on probabilistic graphical models, capable of combining\nheterogeneous data sources with classical state estimation nodes and other\ncustomised computational nodes, is proposed. The framework allows flexible\nextension of the notion of grid state beyond the view of flows and injection in\nbus-branch models, and an efficient, naturally distributed inference algorithm\ncan be derived. An application of the data fusion model to the quantification\nof distributed solar energy is proposed through numerical examples based on\nsemi-synthetic simulations of the standard IEEE 14-bus test case.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:20:11 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Fusco", "Francesco", ""], ["Tirupathi", "Seshu", ""], ["Gormally", "Robert", ""]]}, {"id": "1705.09088", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan and Maria De Iorio", "title": "Dynamic degree-corrected blockmodels for social networks: a\n  nonparametric approach", "comments": null, "journal-ref": "Statistical Modelling (2019), 19, 386-411", "doi": "10.1177/1471082X18770760", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric approach to the modeling of social networks using\ndegree-corrected stochastic blockmodels is proposed. The model for static\nnetwork consists of a stochastic blockmodel using a probit regression\nformulation and popularity parameters are incorporated to account for degree\nheterogeneity. Dirichlet processes are used to detect community structure as\nwell as induce clustering in the popularity parameters. This approach is\nflexible yet parsimonious as it allows the appropriate number of communities\nand popularity clusters to be determined automatically by the data. We further\ndiscuss some ways of extending the static model to dynamic networks. We\nconsider a Bayesian approach and derive Gibbs samplers for posterior inference.\nThe models are illustrated using several real-world benchmark social networks.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 08:20:59 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Tan", "Linda S. L.", ""], ["De Iorio", "Maria", ""]]}, {"id": "1705.09249", "submitter": "Xinwei Sun", "authors": "Xinwei Sun, Lingjing Hu, Yuan Yao, Yizhou Wang", "title": "GSplit LBI: Taming the Procedural Bias in Neuroimaging for Disease\n  Prediction", "comments": "Conditional Accepted by Miccai,2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In voxel-based neuroimage analysis, lesion features have been the main focus\nin disease prediction due to their interpretability with respect to the related\ndiseases. However, we observe that there exists another type of features\nintroduced during the preprocessing steps and we call them \"\\textbf{Procedural\nBias}\". Besides, such bias can be leveraged to improve classification accuracy.\nNevertheless, most existing models suffer from either under-fit without\nconsidering procedural bias or poor interpretability without differentiating\nsuch bias from lesion ones. In this paper, a novel dual-task algorithm namely\n\\emph{GSplit LBI} is proposed to resolve this problem. By introducing an\naugmented variable enforced to be structural sparsity with a variable splitting\nterm, the estimators for prediction and selecting lesion features can be\noptimized separately and mutually monitored by each other following an\niterative scheme. Empirical experiments have been evaluated on the Alzheimer's\nDisease Neuroimaging Initiative\\thinspace(ADNI) database. The advantage of\nproposed model is verified by improved stability of selected lesion features\nand better classification results.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 16:25:14 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 06:01:19 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Sun", "Xinwei", ""], ["Hu", "Lingjing", ""], ["Yao", "Yuan", ""], ["Wang", "Yizhou", ""]]}, {"id": "1705.09393", "submitter": "Gregory S. Warrington", "authors": "Gregory S. Warrington", "title": "Quantifying gerrymandering using the vote distribution", "comments": "32 pages, 4 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the presence of gerrymandering, one can consider the shapes of\ndistricts or the distribution of votes. The \"efficiency gap,\" which does the\nlatter, plays a central role in a 2016 federal court case on the\nconstitutionality of Wisconsin's state legislative district plan.\nUnfortunately, however, the efficiency gap reduces to proportional\nrepresentation, an expectation that is not a constitutional right. We present a\nnew measure of partisan asymmetry that does not rely on the shapes of\ndistricts, is simple to compute, is provably related to the \"packing and\ncracking\" integral to gerrymandering, and that avoids the constitutionality\nissue presented by the efficiency gap. In addition, we introduce a\ngeneralization of the efficiency gap that also avoids the equivalency to\nproportional representation. We apply the first function to US congressional\nand state legislative plans from recent decades to identify candidate\ngerrymanders.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 02:17:01 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Warrington", "Gregory S.", ""]]}, {"id": "1705.09563", "submitter": "Jason Black", "authors": "Jason Black, Amanda Terry, Daniel Lizotte", "title": "FRAMR-EMR: Framework for Prognostic Predictive Model Development Using\n  Electronic Medical Record Data with a Case Study in Osteoarthritis Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background-Prognostic predictive models are used in the delivery of primary\ncare to estimate a patients risk of future disease development. Electronic\nmedical record, EMR, data can be used for the construction of these models.\nObjectives- To provide a framework for those seeking to develop prognostic\npredictive models using EMR data, and to illustrate these steps using\nosteoarthritis risk estimation as an example. FRAMR-EMR-The FRAmework for\nModelling Risk from EMR data, FRAMR-EMR, was created, which outlines\nstep-by-step guidance for the construction of a prognostic predictive model\nusing EMR data. Throughout these steps, several potential pitfalls specific to\nusing EMR data for predictive purposes are described and methods for addressing\nthem are suggested. Case Study-We used the DELPHI, DELiver Primary Healthcare\nInformation, database to develop our prognostic predictive model for estimation\nof osteoarthritis risk. We constructed a retrospective cohort of 28447 eligible\nprimary care patients. Patients were included if they had an encounter with\ntheir primary care practitioner between 1 January 2008 and 31 December 2009.\nPatients were excluded if they had a diagnosis of osteoarthritis prior to\nbaseline. Construction of a prognostic predictive model following FRAMR-EMR\nyielded a predictive model capable of estimating 5-year risk of osteoarthritis\ndiagnosis. Logistic regression was used to predict osteoarthritis based on age,\nsex, BMI, previous leg injury, and osteoporosis. Internal validation of the\nmodels performance demonstrated good discrimination and moderate calibration.\nConclusions-This study provides guidance to those interested in developing\nprognostic predictive models based on EMR data. The production of high quality\nprognostic predictive models allows for practitioner communication of\naccurately estimated risks of developing future disease among primary care\npatients.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:59:29 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 02:28:48 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Black", "Jason", ""], ["Terry", "Amanda", ""], ["Lizotte", "Daniel", ""]]}, {"id": "1705.09575", "submitter": "Christophe Ley", "authors": "Christophe Ley and Tom Van de Wiele and Hans Van Eetvelde", "title": "Ranking soccer teams on basis of their current strength: a comparison of\n  maximum likelihood approaches", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ten different strength-based statistical models that we use to\nmodel soccer match outcomes with the aim of producing a new ranking. The models\nare of four main types: Thurstone-Mosteller, Bradley-Terry, Independent Poisson\nand Bivariate Poisson, and their common aspect is that the parameters are\nestimated via weighted maximum likelihood, the weights being a match importance\nfactor and a time depreciation factor giving less weight to matches that are\nplayed a long time ago. Since our goal is to build a ranking reflecting the\nteams' current strengths, we compare the 10 models on basis of their predictive\nperformance via the Rank Probability Score at the level of both domestic\nleagues and national teams. We find that the best models are the Bivariate and\nIndependent Poisson models. We then illustrate the versatility and usefulness\nof our new rankings by means of three examples where the existing rankings fail\nto provide enough information or lead to peculiar results.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 13:16:47 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 13:53:57 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 21:04:15 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Ley", "Christophe", ""], ["Van de Wiele", "Tom", ""], ["Van Eetvelde", "Hans", ""]]}, {"id": "1705.09778", "submitter": "Mathurin Massias", "authors": "Mathurin Massias, Olivier Fercoq, Alexandre Gramfort and Joseph Salmon", "title": "Generalized Concomitant Multi-Task Lasso for sparse multimodal\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimension, it is customary to consider Lasso-type estimators to\nenforce sparsity. For standard Lasso theory to hold, the regularization\nparameter should be proportional to the noise level, yet the latter is\ngenerally unknown in practice. A possible remedy is to consider estimators,\nsuch as the Concomitant/Scaled Lasso, which jointly optimize over the\nregression coefficients as well as over the noise level, making the choice of\nthe regularization independent of the noise level. However, when data from\ndifferent sources are pooled to increase sample size, or when dealing with\nmultimodal datasets, noise levels typically differ and new dedicated estimators\nare needed. In this work we provide new statistical and computational solutions\nto deal with such heteroscedastic regression models, with an emphasis on\nfunctional brain imaging with combined magneto- and electroencephalographic\n(M/EEG) signals. Adopting the formulation of Concomitant Lasso-type estimators,\nwe propose a jointly convex formulation to estimate both the regression\ncoefficients and the (square root of the) noise covariance. When our framework\nis instantiated to de-correlated noise, it leads to an efficient algorithm\nwhose computational cost is not higher than for the Lasso and Concomitant\nLasso, while addressing more complex noise structures. Numerical experiments\ndemonstrate that our estimator yields improved prediction and support\nidentification while correctly estimating the noise (square root) covariance.\nResults on multimodal neuroimaging problems with M/EEG data are also reported.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:24:38 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 07:53:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Massias", "Mathurin", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1705.09874", "submitter": "Oleg Sofrygin", "authors": "Oleg Sofrygin, Zheng Zhu, Julie A Schmittdiel, Alyce S. Adams, Richard\n  W. Grant, Mark J. van der Laan, and Romain Neugebauer", "title": "Targeted Learning with Daily EHR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 22:43:08 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 21:53:24 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sofrygin", "Oleg", ""], ["Zhu", "Zheng", ""], ["Schmittdiel", "Julie A", ""], ["Adams", "Alyce S.", ""], ["Grant", "Richard W.", ""], ["van der Laan", "Mark J.", ""], ["Neugebauer", "Romain", ""]]}, {"id": "1705.09976", "submitter": "Xiaoqi Zhang", "authors": "Yanqiao Zheng and Xiaoqi Zhang", "title": "Insert \"Price\" to Coxian Phase-Type Models: An Application to Hospital\n  Charge and Length of Stay Data", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the connection between the RGRST models (Gardiner\net al 2002, Polverejan et al 2003) and the Coxian Phase-Type (CPH) models\n(Marshall et al 2007, Tang 2012) through a construction that converts a special\nsub-class of RGRST models to CPH models. Both of the two models are widely used\nto characterize the distribution of hospital charge and length of stay (LOS),\nbut the lack of connections between them makes the two models rarely used\ntogether. We claim that our construction can make up this gap and make it\npossible to take advantage of the two different models simultaneously. As a\nconsequence, we derive a measure of the \"price\" of staying in each medical\nstage (identified with phases of a CPH model), which can't be approached\nwithout considering the RGRST and CPH models together.A two-stage algorithm is\nprovided to generate consistent estimation of model parameters. Applying the\nalgorithm to a sample drawn from the New York State's Statewide Planning and\nResearch Cooperative System 2013 (SPARCS 2013), we estimate the prices in a\nfour-phase CPH model and discuss the implications.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 18:32:39 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zheng", "Yanqiao", ""], ["Zhang", "Xiaoqi", ""]]}, {"id": "1705.10131", "submitter": "Gerard Keogh Dr.", "authors": "Gerard Keogh", "title": "A Matched Pairs Analysis of International Protection Outcomes in Ireland", "comments": "21 pages, 1 Figure, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine over 40,000 International Protection (IP) determinations for\nnon-EEA nationals covering a 16 year period in Ireland. We reconfigure these\nindividual outcomes into a set of over 23,000 matched pairs based on\ncombination of direct matching and propensity score matching. A key feature of\nthis approach is that it replicates the statistical features of an experimental\nset-up where observational data only are to hand. As a consequence we are able\nto identify those explanatory factors that in fact contribute to the grant of\nIP. This is a key innovation in the analysis of protection outcomes. We centre\nour study in the realm of International Relations studies on protection. We are\nparticularly interested in whether immigration policy is a latent tool used to\ninfluence the odds of a grant of IP, specifically via the introduction of the\nImmigration Act 2004. Using both conditional maximum likelihood and mixed\neffects models we find this is not the case, this conclusion is both novel and\nprofound in a matched pair context. On this basis we conclude there can be\nlittle justification for the perception that immigration policy is a latent\ntool affecting the protection process in Ireland.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:44:53 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Keogh", "Gerard", ""]]}, {"id": "1705.10220", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Liam Solus, Karren Dai Yang and Caroline Uhler", "title": "Permutation-based Causal Inference Algorithms with Interventions", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning directed acyclic graphs using both observational and interventional\ndata is now a fundamentally important problem due to recent technological\ndevelopments in genomics that generate such single-cell gene expression data at\na very large scale. In order to utilize this data for learning gene regulatory\nnetworks, efficient and reliable causal inference algorithms are needed that\ncan make use of both observational and interventional data. In this paper, we\npresent two algorithms of this type and prove that both are consistent under\nthe faithfulness assumption. These algorithms are interventional adaptations of\nthe Greedy SP algorithm and are the first algorithms using both observational\nand interventional data with consistency guarantees. Moreover, these algorithms\nhave the advantage that they are nonparametric, which makes them useful also\nfor analyzing non-Gaussian data. In this paper, we present these two algorithms\nand their consistency guarantees, and we analyze their performance on simulated\ndata, protein signaling data, and single-cell gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:45:02 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 03:31:57 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Wang", "Yuhao", ""], ["Solus", "Liam", ""], ["Yang", "Karren Dai", ""], ["Uhler", "Caroline", ""]]}, {"id": "1705.10310", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten and Devin S. Johnson", "title": "Imputation Approaches for Animal Movement Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of telemetry data is common in animal ecological studies. While\nthe collection of telemetry data for individual animals has improved\ndramatically, the methods to properly account for inherent uncertainties (e.g.,\nmeasurement error, dependence, barriers to movement) have lagged behind. Still,\nmany new statistical approaches have been developed to infer unknown quantities\naffecting animal movement or predict movement based on telemetry data.\nHierarchical statistical models are useful to account for some of the\naforementioned uncertainties, as well as provide population-level inference,\nbut they often come with an increased computational burden. For certain types\nof statistical models, it is straightforward to provide inference if the latent\ntrue animal trajectory is known, but challenging otherwise. In these cases,\napproaches related to multiple imputation have been employed to account for the\nuncertainty associated with our knowledge of the latent trajectory. Despite the\nincreasing use of imputation approaches for modeling animal movement, the\ngeneral sensitivity and accuracy of these methods have not been explored in\ndetail. We provide an introduction to animal movement modeling and describe how\nimputation approaches may be helpful for certain types of models. We also\nassess the performance of imputation approaches in a simulation study. Our\nsimulation study suggests that inference for model parameters directly related\nto the location of an individual may be more accurate than inference for\nparameters associated with higher-order processes such as velocity or\nacceleration. Finally, we apply these methods to analyze a telemetry data set\ninvolving northern fur seals (Callorhinus ursinus) in the Bering Sea.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:36:23 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 14:56:34 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""]]}, {"id": "1705.10312", "submitter": "Dajiang Zhu", "authors": "Dajiang Zhu, Brandalyn C. Riedel, Neda Jahanshad, Nynke A. Groenewold,\n  Dan J. Stein, Ian H. Gotlib, Matthew D. Sacchet, Danai Dima, James H. Cole,\n  Cynthia H.Y. Fu, Henrik Walter, Ilya M. Veer, Thomas Frodl, Lianne Schmaal,\n  Dick J. Veltman, Paul M. Thompson", "title": "Classification of Major Depressive Disorder via Multi-Site Weighted\n  LASSO Model", "comments": "Accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale collaborative analysis of brain imaging data, in psychiatry and\nneu-rology, offers a new source of statistical power to discover features that\nboost ac-curacy in disease classification, differential diagnosis, and outcome\nprediction. However, due to data privacy regulations or limited accessibility\nto large datasets across the world, it is challenging to efficiently integrate\ndistributed information. Here we propose a novel classification framework\nthrough multi-site weighted LASSO: each site performs an iterative weighted\nLASSO for feature selection separately. Within each iteration, the\nclassification result and the selected features are collected to update the\nweighting parameters for each feature. This new weight is used to guide the\nLASSO process at the next iteration. Only the fea-tures that help to improve\nthe classification accuracy are preserved. In tests on da-ta from five sites\n(299 patients with major depressive disorder (MDD) and 258 normal controls),\nour method boosted classification accuracy for MDD by 4.9% on average. This\nresult shows the potential of the proposed new strategy as an ef-fective and\npractical collaborative platform for machine learning on large scale\ndistributed imaging and biobank data.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 21:19:22 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 18:54:04 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhu", "Dajiang", ""], ["Riedel", "Brandalyn C.", ""], ["Jahanshad", "Neda", ""], ["Groenewold", "Nynke A.", ""], ["Stein", "Dan J.", ""], ["Gotlib", "Ian H.", ""], ["Sacchet", "Matthew D.", ""], ["Dima", "Danai", ""], ["Cole", "James H.", ""], ["Fu", "Cynthia H. Y.", ""], ["Walter", "Henrik", ""], ["Veer", "Ilya M.", ""], ["Frodl", "Thomas", ""], ["Schmaal", "Lianne", ""], ["Veltman", "Dick J.", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1705.10354", "submitter": "Mircea Dumitru", "authors": "Mircea Dumitru", "title": "Sparsity enforcing priors in inverse problems via Normal variance\n  mixtures: model selection, algorithms and applications", "comments": "Internal report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse structure of the solution for an inverse problem can be modelled\nusing different sparsity enforcing priors when the Bayesian approach is\nconsidered. Analytical expression for the unknowns of the model can be obtained\nby building hierarchical models based on sparsity enforcing distributions\nexpressed via conjugate priors. We consider heavy tailed distributions with\nthis property: the Student-t distribution, which is expressed as a Normal scale\nmixture, with the mixing distribution the Inverse Gamma distribution, the\nLaplace distribution, which can also be expressed as a Normal scale mixture,\nwith the mixing distribution the Exponential distribution or can be expressed\nas a Normal inverse scale mixture, with the mixing distribution the Inverse\nGamma distribution, the Hyperbolic distribution, the Variance-Gamma\ndistribution, the Normal-Inverse Gaussian distribution, all three expressed via\nconjugate distributions using the Generalized Hyperbolic distribution. For all\ndistributions iterative algorithms are derived based on hierarchical models\nthat account for the uncertainties of the forward model. For estimation,\nMaximum A Posterior (MAP) and Posterior Mean (PM) via variational Bayesian\napproximation (VBA) are used. The performances of resulting algorithm are\ncompared in applications in 3D computed tomography (3D-CT) and chronobiology.\nFinally, a theoretical study is developed for comparison between sparsity\nenforcing algorithms obtained via the Bayesian approach and the sparsity\nenforcing algorithms issued from regularization techniques, like LASSO and some\nothers.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:42:38 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Dumitru", "Mircea", ""]]}, {"id": "1705.10374", "submitter": "Meitner Cadena", "authors": "Meitner Cadena", "title": "Extensions of the Burr Type XII distribution and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burr type XII (BXII) distribution has been largely used in different\nfields due to its great flexibility for fitting data. These applications have\ntypically involved data showing heavy-tailed behaviors. In order to give more\nflexibility to the BXII distribution, in this paper, modifications to this\ndistribution through the use of parametric functions are introduced. For\ninstance, members of this new family of distributions allow the analysis not\nonly of data containing extreme values as the BXII distribution, but also of\nlight-tailed data. We refer to this new family of distributions as the extended\nBurr Type XII distribution (EBXIID) family. Statistical properties of members\nof the EBXIID family are discussed. The maximum likelihood method is proposed\nfor estimating model parameters. The performance of the new family of\ndistributions is studied using simulations. Applications of the new models to\nreal data sets coming from different domains show that models of the EBXIID\nfamily are an alternative to other known distributions.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:41:43 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Cadena", "Meitner", ""]]}, {"id": "1705.10376", "submitter": "Oleg Sofrygin", "authors": "Oleg Sofrygin, Romain Neugebauer, Mark J. van der Laan", "title": "Conducting Simulations in Causal Inference with Networks-Based\n  Structural Equation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen an increasing body of literature devoted to the\nestimation of causal effects in network-dependent data. However, the validity\nof many classical statistical methods in such data is often questioned. There\nis an emerging need for objective and practical ways to assess which causal\nmethodologies might be applicable and valid in network-dependent data. This\npaper describes a set of tools implemented in the simcausal R package that\nallow simulating data based on user-specified structural equation model for\nconnected units. Specification and simulation of counterfactual data is\nimplemented for static, dynamic and stochastic interventions. A new interface\naims to simplify the specification of network-based functional relationships\nbetween connected units. A set of examples illustrates how these simulations\nmay be applied to evaluation of different statistical methods for estimation of\ncausal effects in network-dependent data.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:45:41 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Sofrygin", "Oleg", ""], ["Neugebauer", "Romain", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1705.10446", "submitter": "Cornelis Potgieter", "authors": "Cornelis J. Potgieter, Akihito Kamata, Yusuf Kara", "title": "An EM Algorithm for Estimating an Oral Reading Speed and Accuracy Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a two-part model that includes components for reading\naccuracy and reading speed. The speed component is a log-normal factor model,\nfor which speed data are measured by reading time for each sentence being\nassessed. The accuracy component is a binomial-count factor model, where the\naccuracy data are measured by the number of correctly read words in each\nsentence. Both underlying latent components are assumed to be Gaussian in\nnature. In this paper, the theoretical properties of the proposed model are\ndeveloped and an Monte Carlo EM algorithm for model fitting is outlined. The\npredictive power of the model is illustrated in a real data application.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:29:17 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Potgieter", "Cornelis J.", ""], ["Kamata", "Akihito", ""], ["Kara", "Yusuf", ""]]}, {"id": "1705.10510", "submitter": "Gerard Keogh Dr.", "authors": "Gerard Keogh", "title": "Is the annual growth rate in balance of trade time series for Ireland\n  nonlinear", "comments": "21 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Time Series Multivariate Adaptive Regressions Splines\n(TSMARS) method. This method is useful for identifying nonlinear structure in a\ntime series. We use TSMARS to model the annual change in the balance of trade\nfor Ireland from 1970 to 2007. We compare the TSMARS estimate with long memory\nARFIMA estimates and long-term parsimonious linear models. We show that the\nchange in the balance of trade is nonlinear and possesses weakly long range\neffects. Moreover, we compare the period prior to the introduction of the\nIntrastat system in 1993 with the period from 1993 onward. Here we show that in\nthe earlier period the series had a substantial linear signal embedded in it\nsuggesting that estimation efforts in the earlier period may have resulted in\nan over-smoothed series.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:52:03 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Keogh", "Gerard", ""]]}, {"id": "1705.10553", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok and Andreas Meyer-Lindenberg", "title": "Machine learning for precision psychiatry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nature of mental illness remains a conundrum. Traditional disease\ncategories are increasingly suspected to mis-represent the causes underlying\nmental disturbance. Yet, psychiatrists and investigators now have an\nunprecedented opportunity to benefit from complex patterns in brain, behavior,\nand genes using methods from machine learning (e.g., support vector machines,\nmodern neural-network algorithms, cross-validation procedures). Combining these\nanalysis techniques with a wealth of data from consortia and repositories has\nthe potential to advance a biologically grounded re-definition of major\npsychiatric disorders. Within the next 10-20 years, incoming patients could be\nstratified into distinct biological subgroups that cut across classical\ndiagnostic boundaries. In a new era of evidence-based psychiatry tailored to\nsingle patients, objectively measurable endophenotypes could allow for\nindividualized prediction of early diagnosis, treatment selection, and dosage\nadjustment to reduce the burden of disease. This primer aims to introduce\nclinicians and researchers to the opportunities and challenges in bringing\nmachine intelligence into psychiatric practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:30:39 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Bzdok", "Danilo", ""], ["Meyer-Lindenberg", "Andreas", ""]]}, {"id": "1705.10585", "submitter": "Perry Oddo", "authors": "Perry C. Oddo, Ben Seiyon Lee, Gregory G. Garner, Vivek Srikrishnan,\n  Patrick M. Reed, Chris E. Forest, Klaus Keller", "title": "Deep uncertainties in sea-level rise and storm surge projections:\n  Implications for coastal flood risk management", "comments": null, "journal-ref": null, "doi": "10.1111/risa.12888", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sea-levels are rising in many areas around the world, posing risks to coastal\ncommunities and infrastructures. Strategies for managing these flood risks\npresent decision challenges that require a combination of geophysical,\neconomic, and infrastructure models. Previous studies have broken important new\nground on the considerable tensions between the costs of upgrading\ninfrastructure and the damages that could result from extreme flood events.\nHowever, many risk-based adaptation strategies remain silent on certain\npotentially important uncertainties, as well as the trade-offs between\ncompeting objectives. Here, we implement and improve on a classic\ndecision-analytical model (van Dantzig 1956) to: (i) capture trade-offs across\nconflicting stakeholder objectives, (ii) demonstrate the consequences of\nstructural uncertainties in the sea-level rise and storm surge models, and\n(iii) identify the parametric uncertainties that most strongly influence each\nobjective using global sensitivity analysis. We find that the flood adaptation\nmodel produces potentially myopic solutions when formulated using traditional\nmean-centric decision theory. Moving from a single-objective problem\nformulation to one with multi-objective trade-offs dramatically expands the\ndecision space, and highlights the need for compromise solutions to address\nstakeholder preferences. We find deep structural uncertainties that have large\neffects on the model outcome, with the storm surge parameters accounting for\nthe greatest impacts. Global sensitivity analysis effectively identifies\nimportant parameter interactions that local methods overlook, and which could\nhave critical implications for flood adaptation strategies.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:14:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Oddo", "Perry C.", ""], ["Lee", "Ben Seiyon", ""], ["Garner", "Gregory G.", ""], ["Srikrishnan", "Vivek", ""], ["Reed", "Patrick M.", ""], ["Forest", "Chris E.", ""], ["Keller", "Klaus", ""]]}, {"id": "1705.10865", "submitter": "Xiaotong Suo", "authors": "Xiaotong Suo, Victor Minden, Bradley Nelson, Robert Tibshirani,\n  Michael Saunders", "title": "Sparse canonical correlation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis was proposed by Hotelling [6] and it measures\nlinear relationship between two multidimensional variables. In high dimensional\nsetting, the classical canonical correlation analysis breaks down. We propose a\nsparse canonical correlation analysis by adding l1 constraints on the canonical\nvectors and show how to solve it efficiently using linearized alternating\ndirection method of multipliers (ADMM) and using TFOCS as a black box. We\nillustrate this idea on simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 20:54:17 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 20:04:55 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Suo", "Xiaotong", ""], ["Minden", "Victor", ""], ["Nelson", "Bradley", ""], ["Tibshirani", "Robert", ""], ["Saunders", "Michael", ""]]}, {"id": "1705.10876", "submitter": "Jonathan Auerbach", "authors": "Jonathan Auerbach, Christopher Eshleman and Rob Trangucci", "title": "A Hierarchical Bayes Approach to Adjust for Selection Bias in\n  Before-After Analyses of Vision Zero Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American cities devote significant resources to the implementation of traffic\nsafety countermeasures that prevent pedestrian fatalities. However, the\nbefore-after comparisons typically used to evaluate the success of these\ncountermeasures often suffer from selection bias. This paper motivates the\ntendency for selection bias to overestimate the benefits of traffic safety\npolicy, using New York City's Vision Zero strategy as an example. The NASS\nGeneral Estimates System, Fatality Analysis Reporting System and other\ndatabases are combined into a Bayesian hierarchical model to calculate a more\nrealistic before-after comparison. The results confirm the before-after\nanalysis of New York City's Vision Zero policy did in fact overestimate the\neffect of the policy, and a more realistic estimate is roughly two-thirds the\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 21:50:15 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 17:48:58 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Auerbach", "Jonathan", ""], ["Eshleman", "Christopher", ""], ["Trangucci", "Rob", ""]]}, {"id": "1705.10896", "submitter": "Saskia Bollmann", "authors": "Saskia Bollmann, Alexander M. Pucket, Ross Cunnington and Markus Barth", "title": "Serial Correlations in Single-Subject fMRI with Sub-Second TR", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.10.043", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing statistical analysis of single-subject fMRI data, serial\ncorrelations need to be taken into account to allow for valid inference.\nOtherwise, the variability in the parameter estimates might be under-estimated\nresulting in increased false-positive rates. Serial correlations in fMRI data\nare commonly characterized in terms of a first-order autoregressive (AR)\nprocess and then removed via pre-whitening. The required noise model for the\npre-whitening depends on a number of parameters, particularly the repetition\ntime (TR). Here we investigate how the sub-second temporal resolution provided\nby simultaneous multislice (SMS) imaging changes the noise structure in fMRI\ntime series. We fit a higher-order AR model and then estimate the optimal AR\nmodel order for a sequence with a TR of less than 600 ms providing whole brain\ncoverage. We show that physiological noise modelling successfully reduces the\nrequired AR model order, but remaining serial correlations necessitate an\nadvanced noise model. We conclude that commonly used noise models, such as the\nAR(1) model, are inadequate for modelling serial correlations in fMRI using\nsub-second TRs. Rather, physiological noise modelling in combination with\nadvanced pre-whitening schemes enable valid inference in single-subject\nanalysis using fast fMRI sequences.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:49:02 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 09:27:47 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 05:37:29 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Bollmann", "Saskia", ""], ["Pucket", "Alexander M.", ""], ["Cunnington", "Ross", ""], ["Barth", "Markus", ""]]}, {"id": "1705.10922", "submitter": "Sahil Shah", "authors": "Sahil D. Shah and Rosemary Braun", "title": "Network-based identification of disease genes in expression data: the\n  GeneSurrounder method", "comments": "We have extended the application and evaluation of our GeneSurrounder\n  method to a second disease (gene expression data from bladder cancer) and\n  added additional analyses of GeneSurrounder's ability to identify known\n  cancer-associated genes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of high--throughput transcription profiling technologies has\nenabled identification of genes and pathways associated with disease, providing\nnew avenues for precision medicine. A key challenge is to analyze this data in\nthe context of the regulatory networks and pathways that control cellular\nprocesses, while still obtaining insights that can be used to design new\ndiagnostic and therapeutic interventions. While classical differential\nexpression analysis provides specific and hence targetable gene-level insights,\nit does not include any systems-level information. On the other hand, pathway\nanalyses integrate systems-level information with expression data, but are\noften limited in their ability to indicate specific molecular targets. We\nintroduce GeneSurrounder, an analysis method that takes into account the\ncomplex structure of interaction networks to identify specific genes that\ndisrupt pathway activity in a disease-specific manner. GeneSurrounder\nintegrates transcriptomic data and pathway network information in a novel\ntwo-step procedure to detect genes that (i) appear to influence the expression\nof other genes local to it in the network and (ii) are part of a subnetwork of\ndifferentially expressed genes. Combined, this evidence can be used to pinpoint\nspecific genes that have a mechanistic role in the phenotype of interest.\nApplying GeneSurrounder to three distinct ovarian cancer studies using a global\nKEGG network, we show that our method is able to identify biologically relevant\ngenes and genes missed by single-gene association tests, integrate pathway and\nexpression data, and yield more consistent results across multiple studies of\nthe same phenotype than competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:40:18 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 23:08:36 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Shah", "Sahil D.", ""], ["Braun", "Rosemary", ""]]}, {"id": "1705.11055", "submitter": "Andrey Gorshenin", "authors": "V.Yu. Korolev, A.K. Gorshenin, S.K. Gulev, K.P.Belyaev, A.A. Grusho", "title": "Statistical Analysis of Precipitation Events", "comments": "5 pages, 4 figures; ICNAAM 2016", "journal-ref": "AIP Conf. Proc.1863 (2017) 090011", "doi": "10.1063/1.4992276", "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we demonstrate the results of a statistical analysis of\nsome characteristics of precipitation events and propose a kind of a\ntheoretical explanation of the proposed models in terms of mixed Poisson and\nmixed exponential distributions based on the information-theoretical entropy\nreasoning. The proposed models can be also treated as the result of following\nthe popular Bayesian approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 12:18:53 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Korolev", "V. Yu.", ""], ["Gorshenin", "A. K.", ""], ["Gulev", "S. K.", ""], ["Belyaev", "K. P.", ""], ["Grusho", "A. A.", ""]]}, {"id": "1705.11082", "submitter": "Sylwia Bujkiewicz", "authors": "Sze Huey Tan, Keith R Abrams and Sylwia Bujkiewicz", "title": "Bayesian multi-parameter evidence synthesis to inform decision-making: a\n  case study in hormone-refractory metastatic prostate cancer", "comments": null, "journal-ref": null, "doi": "10.1177/0272989X18788537", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In health technology assessment, decisions are based on complex\ncost-effectiveness models which, to be implemented, require numerous input\nparameters. When some of relevant estimates are not available the model may\nhave to be simplified. Multi-parameter evidence synthesis allows to combine\ndata from diverse sources of evidence resulting in obtaining estimates required\nin clinical decision-making that otherwise may not be available. We demonstrate\nhow bivariate meta-analysis (BVMA) can be used to predict unreported estimate\nof a treatment effect enabling implementation of multi-state Markov model,\nwhich otherwise needs to be simplified. To illustrate this, we used an example\nof cost-effectiveness analysis for docetaxel in combination with prednisolone\nin metastatic hormone-refractory prostate cancer (mHRPC). BVMA was used to\nmodel jointly available data on treatment effects on overall survival (OS) and\nprogression-free survival (PFS) to predict the unreported effect on PFS in a\nstudy evaluating docetaxel. Predicted treatment effect on PFS allowed\nimplementation of a three-state Markov model comprising of stable disease,\nprogressive disease and death states, whilst lack of the estimate restricted\nthe model to two-state model (stable disease and death states). The two-state\nand three-state models were compared by calculating incremental\ncost-effectiveness ratios, which was much lower in the three-state model:\n{\\pounds}21966 per QALY gained compared to {\\pounds}30026 obtained from the\ntwo-state model. In contrast to the two-state model, the three-state model has\nthe advantage of distinguishing patients who progressed from those who did not\nprogress. The use of advanced meta-analytic technique helped to obtain relevant\nparameter estimate to populate a model which describes natural history more\naccurately, and at the same helped to prevent valuable clinical data from being\ndiscarded.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 13:32:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Tan", "Sze Huey", ""], ["Abrams", "Keith R", ""], ["Bujkiewicz", "Sylwia", ""]]}]