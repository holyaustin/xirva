[{"id": "1910.00006", "submitter": "Behnaz Pirzamanbein PhD", "authors": "Behnaz Pirzamanbein", "title": "Spatial methods and their applications to environmental and climate data", "comments": "Book (this report/review of methodology and applications was written\n  as an introductory paper for PhD study in Lund University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental and climate processes are often distributed over large\nspace-time domains. Their complexity and the amount of available data make\nmodelling and analysis a challenging task. Statistical modelling of environment\nand climate data can have several different motivations including\ninterpretation or characterisation of the data. Results from statistical\nanalysis are often used as a integral part of larger environmental studies.\nSpatial statistics is an active and modern statistical field, concerned with\nthe quantitative analysis of spatial data; their dependencies and\nuncertainties. Spatio-temporal statistics extends spatial statistics through\nthe addition of time to the, two or three, spatial dimensions. The focus of\nthis introductory paper is to provide an overview of spatial methods and their\napplication to environmental and climate data. This paper also gives an\noverview of several important topics including large data sets and\nnon-stationary covariance structures. Further, it is discussed how Bayesian\nhierarchical models can provide a flexible way of constructing models.\nHierarchical models may seem to be a good solution, but they have challenges of\ntheir own such as, parameter estimation. Finally, the application of\nspatio-temporal models to the LANDCLIM data (LAND cover - CLIMate interactions\nin NW Europe during the Holocene) will be discussed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 11:31:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pirzamanbein", "Behnaz", ""]]}, {"id": "1910.00095", "submitter": "Shreyas Fadnavis", "authors": "Shreyas Fadnavis, Hamza Farooq, Maryam Afzali, Christoph Lenglet,\n  Tryphon Georgiou, Hu Cheng, Sharlene Newman, Shahnawaz Ahmed, Rafael Neto\n  Henriques, Eric Peterson, Serge Koudoro, Ariel Rokem, Eleftherios\n  Garyfallidis", "title": "Fitting IVIM with Variable Projection and Simplicial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting multi-exponential models to Diffusion MRI (dMRI) data has always been\nchallenging due to various underlying complexities. In this work, we introduce\na novel and robust fitting framework for the standard two-compartment IVIM\nmicrostructural model. This framework provides a significant improvement over\nthe existing methods and helps estimate the associated diffusion and perfusion\nparameters of IVIM in an automatic manner. As a part of this work we provide\ncapabilities to switch between more advanced global optimization methods such\nas simplicial homology (SH) and differential evolution (DE). Our experiments\nshow that the results obtained from this simultaneous fitting procedure\ndisentangle the model parameters in a reduced subspace. The proposed framework\nextends the seminal work originated in the MIX framework, with improved\nprocedures for multi-stage fitting. This framework has been made available as\nan open-source Python implementation and disseminated to the community through\nthe DIPY project.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:40:16 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:42:40 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 19:49:11 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Fadnavis", "Shreyas", ""], ["Farooq", "Hamza", ""], ["Afzali", "Maryam", ""], ["Lenglet", "Christoph", ""], ["Georgiou", "Tryphon", ""], ["Cheng", "Hu", ""], ["Newman", "Sharlene", ""], ["Ahmed", "Shahnawaz", ""], ["Henriques", "Rafael Neto", ""], ["Peterson", "Eric", ""], ["Koudoro", "Serge", ""], ["Rokem", "Ariel", ""], ["Garyfallidis", "Eleftherios", ""]]}, {"id": "1910.00150", "submitter": "Faten Alamri", "authors": "Faten S. Alamri, Edward L. Boone, David J. Edwards", "title": "Monotonic Nonparametric Dose Response Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxicologists are often concerned with determining the dosage to which an\nindividual can be exposed with an acceptable risk of adverse effect. These\ntypes of studies have been conducted widely in the past, and many novel\napproaches have been developed. Parametric techniques utilizing ANOVA and\nnonlinear regression models are well represented in the literature. The biggest\ndrawback of parametric approaches is the need to specify the correct model.\nRecently, there has been an interest in nonparametric approaches to tolerable\ndosage estimation. In this work, we focus on the monotonically decreasing dose\nresponse model where the response is a percent to control. This poses two\nconstraints to the nonparametric approach. The doseresponse function must be\none at control (dose = 0), and the function must always be positive. Here we\npropose a Bayesian solution to this problem using a novel class of\nnonparametric models. A basis function developed in this research is the Alamri\nMonotonic spline (AM-spline). Our approach is illustrated using both simulated\ndata and an experimental dataset from pesticide related research at the US\nEnvironmental Protection Agency.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:35:03 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 11:29:50 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 04:54:05 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Alamri", "Faten S.", ""], ["Boone", "Edward L.", ""], ["Edwards", "David J.", ""]]}, {"id": "1910.00213", "submitter": "Masaaki Inoue", "authors": "Masaaki Inoue, Thong Pham, Hidetoshi Shimodaira", "title": "Joint Estimation of the Non-parametric Transitivity and Preferential\n  Attachment Functions in Scientific Co-authorship Networks", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical method to estimate simultaneously the non-parametric\ntransitivity and preferential attachment functions in a growing network, in\ncontrast to conventional methods that either estimate each function in\nisolation or assume some functional form for them. Our model is shown to be a\ngood fit to two real-world co-authorship networks and be able to bring to light\nintriguing details of the preferential attachment and transitivity phenomena\nthat would be unavailable under traditional methods. We also introduce a method\nto quantify the amount of contributions of those phenomena in the growth\nprocess of a network based on the probabilistic dynamic process induced by the\nmodel formula. Applying this method, we found that transitivity dominated PA in\nboth co-authorship networks. This suggests the importance of indirect relations\nin scientific creative processes. The proposed methods are implemented in the R\npackage FoFaF.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:17:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Inoue", "Masaaki", ""], ["Pham", "Thong", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1910.00304", "submitter": "Marjan Cugmas", "authors": "Jana Kolar, Marjan Cugmas, Anu\\v{s}ka Ferligoj", "title": "Towards Key Performance Indicators of Research Infrastructures", "comments": "15 pages, 8 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2018, the European Strategic Forum for research infrastructures (ESFRI)\nwas tasked by the Competitiveness Council, a configuration of the Council of\nthe EU, to develop a common approach for monitoring of Research\nInfrastructures' performance. To this end, ESFRI established a working group,\nwhich has proposed 21 Key Performance Indicators (KPIs) to monitor the progress\nof the Research Infrastructures (RIs) addressed towards their objectives. The\nRIs were then asked to assess their relevance for their institution. The paper\naims to identify the relevance of certain indicators for particular groups of\nRIs by using cluster and discriminant analysis. This could contribute to\ndevelopment of a monitoring system, tailored to particular RIs.\n  To obtain a typology of the RIs, we first performed cluster analysis of the\nRIs according to their properties, which revealed clusters of RIs with similar\ncharacteristics, based on to the domain of operation, such as food, environment\nor engineering. Then, discriminant analysis was used to study how the relevance\nof the KPIs differs among the obtained clusters. This analysis revealed that\nthe percentage of RIs correctly classified into five clusters, using the KPIs,\nis 80%. Such a high percentage indicates that there are significant differences\nin the relevance of certain indicators, depending on the ESFRI domain of the\nRI. The indicators therefore need to be adapted to the type of infrastructure.\nIt is therefore proposed that the Strategic Working Groups of ESFRI addressing\nspecific domains should be involved in the tailored development of the\nmonitoring of pan-European RIs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:13:51 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Kolar", "Jana", ""], ["Cugmas", "Marjan", ""], ["Ferligoj", "Anu\u0161ka", ""]]}, {"id": "1910.00310", "submitter": "Paul Deheuvels", "authors": "Fr\\'ed\\'eric Chabolle, Paul Deheuvels (LSTA)", "title": "On choking and ingestion hazards for children in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risks of Unintentional Suffocation injuries in the U.S. must be\nreconsidered in view of the existent mortality and morbidity statistics. In\nparticular, fatal injuries due to the Sudden Unexpected Infant Death Syndrome\n[SUIDS] should be treated on their own and separated from this group. Because\nof a non-appropriate nomenclature, the risks of injuries due to Aspiration\nand/or Ingestion of Foreign Bodies have been overestimated in the recent\ndecades, and should be reconsidered by a factual scrutiny of the statistical\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:24:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Chabolle", "Fr\u00e9d\u00e9ric", "", "LSTA"], ["Deheuvels", "Paul", "", "LSTA"]]}, {"id": "1910.00393", "submitter": "Johannes Haupt", "authors": "Johannes Haupt, Daniel Jacob, Robin M. Gubela, Stefan Lessmann", "title": "Affordable Uplift: Supervised Randomization in Controlled Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer scoring models are the core of scalable direct marketing. Uplift\nmodels provide an estimate of the incremental benefit from a treatment that is\nused for operational decision-making. Training and monitoring of uplift models\nrequire experimental data. However, the collection of data under randomized\ntreatment assignment is costly, since random targeting deviates from an\nestablished targeting policy. To increase the cost-efficiency of\nexperimentation and facilitate frequent data collection and model training, we\nintroduce supervised randomization. It is a novel approach that integrates\nexisting scoring models into randomized trials to target relevant customers,\nwhile ensuring consistent estimates of treatment effects through correction for\nactive sample selection. An empirical Monte Carlo study shows that data\ncollection under supervised randomization is cost-efficient, while downstream\nuplift models perform competitively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:01:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Haupt", "Johannes", ""], ["Jacob", "Daniel", ""], ["Gubela", "Robin M.", ""], ["Lessmann", "Stefan", ""]]}, {"id": "1910.00405", "submitter": "Samuel Clark", "authors": "Samuel J. Clark, Martin W. Bratschi, Philip Setel, Carla Abouzahr, Don\n  de Savigny, Zehang Li, Tyler McCormick, Peter Byass, Daniel Chandramohan", "title": "Verbal Autopsy in Civil Registration and Vital Statistics: The\n  Symptom-Cause Information Archive", "comments": "arXiv admin note: text overlap with arXiv:1802.07807", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The burden of disease is fundamental to understanding, prioritizing, and\nmonitoring public health interventions. Cause of death is required to calculate\nthe burden of disease, but in many parts of the developing world deaths are\nneither detected nor given a cause. Verbal autopsy is a feasible way of\nassigning a cause to a death based on an interview with those who cared for the\nperson who died. As civil registration and vital statistics systems improve in\nthe developing world, verbal autopsy is playing and increasingly important role\nin providing information about cause of death and the burden of disease. This\nnote motivates the creation of a global symptom-cause archive containing\nreference deaths with both a verbal autopsy and a cause assigned through an\nindependent mechanism. This archive could provide training and validation data\nfor refining, developing, and testing machine-based algorithms to automate\ncause assignment from verbal autopsy data. This, in turn, would improve the\ncomparability of machine-assigned causes and provide a means to fine-tune\nindividual cause assignment within specific contexts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:07:40 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 16:12:27 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Clark", "Samuel J.", ""], ["Bratschi", "Martin W.", ""], ["Setel", "Philip", ""], ["Abouzahr", "Carla", ""], ["de Savigny", "Don", ""], ["Li", "Zehang", ""], ["McCormick", "Tyler", ""], ["Byass", "Peter", ""], ["Chandramohan", "Daniel", ""]]}, {"id": "1910.00460", "submitter": "Ivan Stankevich", "authors": "Konstantin Korishchenko, Ivan Stankevich, Nikolay Pilnik, Daria\n  Petrova", "title": "Usage-Based Vehicle Insurance: Driving Style Factors of Accident\n  Probability and Severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces an approach to telematics devices data application in\nautomotive insurance. We conduct a comparative analysis of different types of\ndevices that collect information on vehicle utilization and driving style of\nits driver, describe advantages and disadvantages of these devices and indicate\nthe most efficient from the insurer point of view. The possible formats of\ntelematics data are described and methods of their processing to a format\nconvenient for modelling are proposed. We also introduce an approach to\nclassify the accidents strength. Using all the available information, we\nestimate accident probability models for different types of accidents and\nidentify an optimal set of factors for each of the models. We assess the\nquality of resulting models using both in-sample and out-of-sample estimates.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:45:50 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 16:09:06 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Korishchenko", "Konstantin", ""], ["Stankevich", "Ivan", ""], ["Pilnik", "Nikolay", ""], ["Petrova", "Daria", ""]]}, {"id": "1910.00547", "submitter": "Bruno Ribeiro", "authors": "S Chandra Mouli, Leonardo Teixeira, Jennifer Neville, Bruno Ribeiro", "title": "Deep Lifetime Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of lifetime clustering is to develop an inductive model that maps\nsubjects into $K$ clusters according to their underlying (unobserved) lifetime\ndistribution. We introduce a neural-network based lifetime clustering model\nthat can find cluster assignments by directly maximizing the divergence between\nthe empirical lifetime distributions of the clusters. Accordingly, we define a\nnovel clustering loss function over the lifetime distributions (of entire\nclusters) based on a tight upper bound of the two-sample Kuiper test p-value.\nThe resultant model is robust to the modeling issues associated with the\nunobservability of termination signals, and does not assume proportional\nhazards. Our results in real and synthetic datasets show significantly better\nlifetime clusters (as evaluated by C-index, Brier Score, Logrank score and\nadjusted Rand index) as compared to competing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:10:16 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 02:57:43 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mouli", "S Chandra", ""], ["Teixeira", "Leonardo", ""], ["Neville", "Jennifer", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1910.00699", "submitter": "Yugandhar Sarkale", "authors": "Yugandhar Sarkale, Saeed Nozhati, Edwin K. P. Chong, Bruce R.\n  Ellingwood", "title": "Decision Automation for Electric Power Network Recovery", "comments": "Submitted to IEEE Transactions on Automation Science and Engineering\n  (13 pages and 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical infrastructure systems such as electric power networks, water\nnetworks, and transportation systems play a major role in the welfare of any\ncommunity. In the aftermath of disasters, their recovery is of paramount\nimportance; orderly and efficient recovery involves the assignment of limited\nresources (a combination of human repair workers and machines) to repair\ndamaged infrastructure components. The decision maker must also deal with\nuncertainty in the outcome of the resource-allocation actions during recovery.\nThe manual assignment of resources seldom is optimal despite the expertise of\nthe decision maker because of the large number of choices and uncertainties in\nconsequences of sequential decisions. This combinatorial assignment problem\nunder uncertainty is known to be \\mbox{NP-hard}. We propose a novel decision\ntechnique that addresses the massive number of decision choices for large-scale\nreal-world problems; in addition, our method also features an experiential\nlearning component that adaptively determines the utilization of the\ncomputational resources based on the performance of a small number of choices.\nOur framework is closed-loop, and naturally incorporates all the attractive\nfeatures of such a decision-making system. In contrast to myopic approaches,\nwhich do not account for the future effects of the current choices, our\nmethodology has an anticipatory learning component that effectively\nincorporates \\emph{lookahead} into the solutions. To this end, we leverage the\ntheory of regression analysis, Markov decision processes (MDPs), multi-armed\nbandits, and stochastic models of community damage from natural disasters to\ndevelop a method for near-optimal recovery of communities. Our method\ncontributes to the general problem of MDPs with massive action spaces with\napplication to recovery of communities affected by hazards.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:30:02 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 16:59:33 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 22:53:35 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Sarkale", "Yugandhar", ""], ["Nozhati", "Saeed", ""], ["Chong", "Edwin K. P.", ""], ["Ellingwood", "Bruce R.", ""]]}, {"id": "1910.00867", "submitter": "Michael Golosovsky", "authors": "Michael Golosovsky", "title": "Prediction of citation dynamics of individual papers", "comments": "15 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply stochastic model of citation dynamics of individual papers developed\nin our previous work (M. Golosovsky and S. Solomon, Phys. Rev. E\\textbf{ 95},\n012324 (2017)) to forecast citation career of individual papers. We focus not\nonly on the estimate of the future citations of a paper but on the\nprobabilistic margins of such estimate as well.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 10:36:36 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Golosovsky", "Michael", ""]]}, {"id": "1910.01116", "submitter": "Irene Y. Chen", "authors": "Irene Y. Chen, Monica Agrawal, Steven Horng, David Sontag", "title": "Robustly Extracting Medical Knowledge from EHRs: A Case Study of\n  Learning a Health Knowledge Graph", "comments": "12 pages, presented at PSB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly large electronic health records (EHRs) provide an opportunity to\nalgorithmically learn medical knowledge. In one prominent example, a causal\nhealth knowledge graph could learn relationships between diseases and symptoms\nand then serve as a diagnostic tool to be refined with additional clinical\ninput. Prior research has demonstrated the ability to construct such a graph\nfrom over 270,000 emergency department patient visits. In this work, we\ndescribe methods to evaluate a health knowledge graph for robustness. Moving\nbeyond precision and recall, we analyze for which diseases and for which\npatients the graph is most accurate. We identify sample size and unmeasured\nconfounders as major sources of error in the health knowledge graph. We\nintroduce a method to leverage non-linear functions in building the causal\ngraph to better understand existing model assumptions. Finally, to assess model\ngeneralizability, we extend to a larger set of complete patient visits within a\nhospital system. We conclude with a discussion on how to robustly extract\nmedical knowledge from EHRs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:42:03 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Irene Y.", ""], ["Agrawal", "Monica", ""], ["Horng", "Steven", ""], ["Sontag", "David", ""]]}, {"id": "1910.01165", "submitter": "Abhishek Pratap", "authors": "Abhishek Pratap, Elias Chaibub Neto, Phil Snyder, Carl Stepnowsky,\n  No\\'emie Elhadad, Daniel Grant, Matthew H. Mohebbi, Sean Mooney, Christine\n  Suver, John Wilbanks, Lara Mangravite, Patrick Heagerty, Pat Arean, Larsson\n  Omberg", "title": "Indicators of retention in remote digital health studies: A cross-study\n  evaluation of 100,000 participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Digital technologies such as smartphones are transforming the way scientists\nconduct biomedical research using real-world data. Several remotely-conducted\nstudies have recruited thousands of participants over a span of a few months.\nUnfortunately, these studies are hampered by substantial participant attrition,\ncalling into question the representativeness of the collected data including\ngeneralizability of findings from these studies. We report the challenges in\nretention and recruitment in eight remote digital health studies comprising\nover 100,000 participants who participated for more than 850,000 days,\ncompleting close to 3.5 million remote health evaluations. Survival modeling\nsurfaced several factors significantly associated(P < 1e-16) with increase in\nmedian retention time i) Clinician referral(increase of 40 days), ii) Effect of\ncompensation (22 days), iii) Clinical conditions of interest to the study (7\ndays) and iv) Older adults(4 days). Additionally, four distinct patterns of\ndaily app usage behavior that were also associated(P < 1e-10) with participant\ndemographics were identified. Most studies were not able to recruit a\nrepresentative sample, either demographically or regionally. Combined together\nthese findings can help inform recruitment and retention strategies to enable\nequitable participation of populations in future digital health research.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:55:41 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Pratap", "Abhishek", ""], ["Neto", "Elias Chaibub", ""], ["Snyder", "Phil", ""], ["Stepnowsky", "Carl", ""], ["Elhadad", "No\u00e9mie", ""], ["Grant", "Daniel", ""], ["Mohebbi", "Matthew H.", ""], ["Mooney", "Sean", ""], ["Suver", "Christine", ""], ["Wilbanks", "John", ""], ["Mangravite", "Lara", ""], ["Heagerty", "Patrick", ""], ["Arean", "Pat", ""], ["Omberg", "Larsson", ""]]}, {"id": "1910.01313", "submitter": "Sarini Abdullah", "authors": "Sarini Abdullah, Nicole White, James McGree, Kerrie Mengersen, Graham\n  Kerr", "title": "Assessing the predictive ability of the UPDRS for falls classification\n  in early stage Parkinson's disease", "comments": "29 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of risk factors associated with falls in people with\nParkinson's Disease (PD) is important due to their high risk of falling. In\nthis study, various ways of utilizing the Unified Parkinson's Disease Rating\nScale (UPDRS) were assessed for the identification of risk factors and for the\nprediction of falls. Three statistical methods for classification were\nconsidered:decision trees, random forests, and logistic regression. UPDRS\nmeasurements on 51 participants with early stage PD, who completed monthly\nfalls diaries for 12 months of follow-up were analyzed. All classification\nmethods applied produced similar results in regards to classification accuracy\nand the selected important variables. The highest classification rates were\nobtained from model with individual items of the UPDRS with 80% accuracy (85%\nsensitivity and 77% specificity), higher than in any previous study. A\ncomparison of the independent performance of the four parts of the UPDRS\nrevealed the comparably high classification rates for Parts II and III of the\nUPDRS. Similar patterns with slightly different classification rates were\nobserved for the 6- and 12-month of follow-up times. Consistent predictors for\nfalls selected by all classification methods at two follow-up times are:\nthought disorder for UPDRS I, dressing and falling for UPDRS II, hand\npronate/supinate for UPDRS III, and sleep disturbance and symptomatic\northostasis for UPDRS IV. While for the aggregate measures, subtotal 2 (sum of\nUPDRS II items) and bradykinesia showed high association with fall/non-fall.\nFall/non-fall occurrences were more associated with individual items of the\nUPDRS than with the aggregate measures. UPDRS parts II and III produced\ncomparably high classification rates for fall/non-fall prediction. Similar\nresults were obtained for modelling data at 6-month and 12-month follow-up\ntimes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:54:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Abdullah", "Sarini", ""], ["White", "Nicole", ""], ["McGree", "James", ""], ["Mengersen", "Kerrie", ""], ["Kerr", "Graham", ""]]}, {"id": "1910.01325", "submitter": "Andriy Olenko", "authors": "Bandar Alsharari, Andriy Olenko, Hossam Abuel-Naga", "title": "Modeling of Electrical Resistivity of Soil Based on Geotechnical\n  Properties", "comments": "22 pages, 13 figures", "journal-ref": "Expert Systems with Applications, Volume 141, 1 March 2020, 112966", "doi": "10.1016/j.eswa.2019.112966", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the relationship between the electrical resistivity of soil and\nits geotechnical properties is an important engineering problem. This study\naims to develop methodology for finding the best model that can be used to\npredict the electrical resistivity of soil, based on knowing its geotechnical\nproperties. The research develops several linear models, three non-linear\nmodels, and three artificial neural network models (ANN). These models are\napplied to the experimental data set comprises 864 observations and five\nvariables. The results show that there are significant exponential negative\nrelationships between the electrical resistivity of soil and its geotechnical\nproperties. The most accurate prediction values are obtained using the ANN\nmodel. The cross-validation analysis confirms the high precision of the\nselected predictive model. This research is the first rigorous systematic\nanalysis and comparison of difference methodologies in ground electrical\nresistivity studies. It provides practical guidelines and examples of design,\ndevelopment and testing non-linear relationships in engineering intelligent\nsystems and applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:03:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Alsharari", "Bandar", ""], ["Olenko", "Andriy", ""], ["Abuel-Naga", "Hossam", ""]]}, {"id": "1910.01398", "submitter": "Thais Fonseca Dr", "authors": "T. C. O. Fonseca, V. S. Cerqueira, H. S. Migon, C. A. C. Torres", "title": "The effects of degrees of freedom estimation in the Asymmetric GARCH\n  model with Student-t Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the effects of using the independent Jeffreys prior\nfor the degrees of freedom parameter of a Student-t model in the asymmetric\ngeneralised autoregressive conditional heteroskedasticity (GARCH) model. To\ncapture asymmetry in the reaction to past shocks, smooth transition models are\nassumed for the variance. We adopt the fully Bayesian approach for inference,\nprediction and model selection We discuss problems related to the estimation of\ndegrees of freedom in the Student-t model and propose a solution based on\nindependent Jeffreys priors which correct problems in the likelihood function.\nA simulated study is presented to investigate how the estimation of model\nparameters in the Student-t GARCH model are affected by small sample sizes,\nprior distributions and misspecification regarding the sampling distribution.\nAn application to the Dow Jones stock market data illustrates the usefulness of\nthe asymmetric GARCH model with Student-t errors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:52:22 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Fonseca", "T. C. O.", ""], ["Cerqueira", "V. S.", ""], ["Migon", "H. S.", ""], ["Torres", "C. A. C.", ""]]}, {"id": "1910.01407", "submitter": "Danilo Vassallo", "authors": "Danilo Vassallo, Giacomo Bormetti and Fabrizio Lillo", "title": "A tale of two sentiment scales: Disentangling short-run and long-run\n  components in multivariate sentiment dynamics", "comments": "37 pages, 8 figures. The authors thank Thomson Reuters for kindly\n  providing Thomson Reuters MarketPsych Indices time series. We benefited from\n  discussion with Giuseppe Buccheri, Fulvio Corsi, Luca Trapin, as well as with\n  conference participants to the Quantitative Finance Workshop 2019 at ETH in\n  Zurich and the AMASES XLIII Conference in Perugia", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to sentiment data filtering for a portfolio of\nassets. In our framework, a dynamic factor model drives the evolution of the\nobserved sentiment and allows to identify two distinct components: a long-term\ncomponent, modeled as a random walk, and a short-term component driven by a\nstationary VAR(1) process. Our model encompasses alternative approaches\navailable in literature and can be readily estimated by means of Kalman\nfiltering and expectation maximization. This feature makes it convenient when\nthe cross-sectional dimension of the portfolio increases. By applying the model\nto a portfolio of Dow Jones stocks, we find that the long term component\nco-integrates with the market principal factor, while the short term one\ncaptures transient swings of the market associated with the idiosyncratic\ncomponents and captures the correlation structure of returns. Using quantile\nregressions, we assess the significance of the contemporaneous and lagged\nexplanatory power of sentiment on returns finding strong statistical evidence\nwhen extreme returns, especially negative ones, are considered. Finally, the\nlagged relation is exploited in a portfolio allocation exercise.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:22:37 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 19:47:02 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 16:50:29 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 16:32:23 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Vassallo", "Danilo", ""], ["Bormetti", "Giacomo", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1910.01445", "submitter": "Andrew Daw", "authors": "Michelangelo Harris, Brian Liu, Cean Park, Ravi Ramireddy, Gloria Ren,\n  Max Ren, Shangdi Yu, Andrew Daw, Jamol Pender", "title": "Analyzing the Spotify Top 200 Through a Point Process Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every generation throws a hero up the pop charts. For the current generation,\none of the most relevant pop charts is the Spotify Top 200. Spotify is the\nworld's largest music streaming service and the Top 200 is a daily list of the\nplatform's 200 most streamed songs. In this paper, we analyze a data set\ncollected from over 20 months of these rankings. Via exploratory data analysis,\nwe investigate the popularity, rarity, and longevity of songs on the Top 200\nand we construct a stochastic process model for the daily streaming counts that\ndraws upon ideas from stochastic intensity point processes and marked point\nprocesses. Using the parameters of this model as estimated from the Top 200\ndata, we apply a clustering algorithm to identify songs with similar features\nand performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:43:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Harris", "Michelangelo", ""], ["Liu", "Brian", ""], ["Park", "Cean", ""], ["Ramireddy", "Ravi", ""], ["Ren", "Gloria", ""], ["Ren", "Max", ""], ["Yu", "Shangdi", ""], ["Daw", "Andrew", ""], ["Pender", "Jamol", ""]]}, {"id": "1910.01491", "submitter": "Kei Nakagawa", "authors": "Kei Nakagawa, Masaya Abe, Junpei Komiyama", "title": "A Robust Transferable Deep Learning Framework for Cross-sectional\n  Investment Strategy", "comments": null, "journal-ref": null, "doi": "10.1109/DSAA49011.2020.00051", "report-no": null, "categories": "q-fin.ST cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock return predictability is an important research theme as it reflects our\neconomic and social organization, and significant efforts are made to explain\nthe dynamism therein. Statistics of strong explanative power, called \"factor\"\nhave been proposed to summarize the essence of predictive stock returns.\nAlthough machine learning methods are increasingly popular in stock return\nprediction, an inference of the stock returns is highly elusive, and still most\ninvestors, if partly, rely on their intuition to build a better decision\nmaking. The challenge here is to make an investment strategy that is consistent\nover a reasonably long period, with the minimum human decision on the entire\nprocess. To this end, we propose a new stock return prediction framework that\nwe call Ranked Information Coefficient Neural Network (RIC-NN). RIC-NN is a\ndeep learning approach and includes the following three novel ideas: (1)\nnonlinear multi-factor approach, (2) stopping criteria with ranked information\ncoefficient (rank IC), and (3) deep transfer learning among multiple regions.\nExperimental comparison with the stocks in the Morgan Stanley Capital\nInternational (MSCI) indices shows that RIC-NN outperforms not only\noff-the-shelf machine learning methods but also the average return of major\nequity investment funds in the last fourteen years.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:02:57 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Nakagawa", "Kei", ""], ["Abe", "Masaya", ""], ["Komiyama", "Junpei", ""]]}, {"id": "1910.01688", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Daniel Apley, Wei Chen", "title": "Bayesian Optimization for Materials Design with Mixed Quantitative and\n  Qualitative Variables", "comments": "29 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.mtrl-sci cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although Bayesian Optimization (BO) has been employed for accelerating\nmaterials design in computational materials engineering, existing works are\nrestricted to problems with quantitative variables. However, real designs of\nmaterials systems involve both qualitative and quantitative design variables\nrepresenting material compositions, microstructure morphology, and processing\nconditions. For mixed-variable problems, existing Bayesian Optimization (BO)\napproaches represent qualitative factors by dummy variables first and then fit\na standard Gaussian process (GP) model with numerical variables as the\nsurrogate model. This approach is restrictive theoretically and fails to\ncapture complex correlations between qualitative levels. We present in this\npaper the integration of a novel latent-variable (LV) approach for\nmixed-variable GP modeling with the BO framework for materials design. LVGP is\na fundamentally different approach that maps qualitative design variables to\nunderlying numerical LV in GP, which has strong physical justification. It\nprovides flexible parameterization and representation of qualitative factors\nand shows superior modeling accuracy compared to the existing methods. We\ndemonstrate our approach through testing with numerical examples and materials\ndesign examples. It is found that in all test examples the mapped LVs provide\nintuitive visualization and substantial insight into the nature and effects of\nthe qualitative factors. Though materials designs are used as examples, the\nmethod presented is generic and can be utilized for other mixed variable design\noptimization problems that involve expensive physics-based simulations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:05:20 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhang", "Yichi", ""], ["Apley", "Daniel", ""], ["Chen", "Wei", ""]]}, {"id": "1910.01722", "submitter": "Osnat Mokryn", "authors": "Hadar Miller and Osnat Mokryn", "title": "Constant State of Change: Engagement Inequality in Temporal Dynamic\n  Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.09613", "journal-ref": "PLOS ONE 15(4): e0231035 (2020)", "doi": "10.1371/journal.pone.0231035", "report-no": null, "categories": "cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal changes in complex systems of interactions have excited the\nresearch community in recent years as they encompass understandings on their\ndynamics and evolution. From the collective dynamics of organizations and\nonline communities to the spreading of information and fake news, to name a\nfew, temporal dynamics are fundamental in the understanding of complex systems.\nIn this work, we quantify the level of engagement in dynamic complex systems of\ninteractions, modeled as networks. We focus on interaction networks for which\nthe dynamics of the interactions are coupled with that of the topology, such as\nonline messaging, forums, and emails. We define two indices to capture the\ntemporal level of engagement: the Temporal Network (edge) Intensity index, and\nthe Temporal Dominance Inequality index. Our surprising results are that these\nmeasures are stationary for most measured networks, regardless of vast\nfluctuations in the size of the networks in time. Moreover, more than 80% of\nweekly changes in the indices values are bounded by less than 10%. The indices\nare stable between the temporal evolution of a network but are different\nbetween networks, and a classifier can determine the network the temporal\nindices belong to with high success. We find an exception in the Enron\nmanagement email exchange during the year before its disintegration, in which\nboth indices show high volatility throughout the inspected period.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:12:57 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 19:25:59 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Miller", "Hadar", ""], ["Mokryn", "Osnat", ""]]}, {"id": "1910.01786", "submitter": "Zhen Zeng Dr.", "authors": "Zhen Zeng, Yuefeng Lu, Judong Shen, Wei Zheng, Peter Shaw, Mary Beth\n  Dorr", "title": "A Random Interaction Forest for Prioritizing Predictive Biomarkers", "comments": "15 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine is becoming a focus in medical research recently, as its\nimplementation brings values to all stakeholders in the healthcare system.\nVarious statistical methodologies have been developed tackling problems in\ndifferent aspects of this field, e.g., assessing treatment heterogeneity,\nidentifying patient subgroups, or building treatment decision models. However,\nthere is a lack of new tools devoted to selecting and prioritizing predictive\nbiomarkers. We propose a novel tree-based ensemble method, random interaction\nforest (RIF), to generate predictive importance scores and prioritize candidate\nbiomarkers for constructing refined treatment decision models. RIF was\nevaluated by comparing with the conventional random forest and univariable\nregression methods and showed favorable properties under various simulation\nscenarios. We applied the proposed RIF method to a biomarker dataset from two\nphase III clinical trials of bezlotoxumab on $\\textit{Clostridium difficile}$\ninfection recurrence and obtained biologically meaningful results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:28:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zeng", "Zhen", ""], ["Lu", "Yuefeng", ""], ["Shen", "Judong", ""], ["Zheng", "Wei", ""], ["Shaw", "Peter", ""], ["Dorr", "Mary Beth", ""]]}, {"id": "1910.01864", "submitter": "Sarini Abdullah", "authors": "Sarini Abdullah, James McGree, Nicole White, Kerrie Mengersen, Graham\n  Kerr", "title": "Profile regression for subgrouping patients with early stage Parkinson's\n  disease", "comments": "30 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falls are detrimental to people with Parkinson's Disease (PD) because of the\npotentially severe consequences to the patients' quality of life. While many\nstudies have attempted to predict falls/non-falls, this study aimed to\ndetermine factors related to falls frequency in people with early PD. Ninety\nnine participants with early stage PD were assessed based on two types of\ntests. The first type of tests is disease-specific tests, comprised of the\nUnified Parkinson's Disease Rating Scale (UPDRS) and the Schwab and England\nactivities of daily living scale (SEADL). A measure of postural instability and\ngait disorder (PIGD) and subtotal scores for subscales I, II, and III were\nderived from the UPDRS. The second type of tests is functional tests, including\nTinetti gait and balance, Berg Balance Scale (BBS), Timed-Up and Go (TUG),\nFunctional Reach (FR), Freezing of Gait (FOG), Mini Mental State Examination\n(MMSE), and Melbourne Edge Test (MET). Falls were recorded each month for 6\nmonths. Clustering of patients via Finite Mixture Model (FMM) was conducted.\nThree clusters of patients were found: non-or single-fallers, low frequency\nfallers, and high frequency fallers. Several factors that are important to\nclustering PD patients were identified: UPDRS subscales II and III subtotals,\nPIGD and SE ADL. However these factors could not differentiate PD patients with\nlow frequency fallers from high frequency fallers. While Tinetti,TUG, and BBS\nturned to be important factors in clustering PD patients, and could\ndifferentiate the three clusters. FMM is able to cluster people with PD into\nthree groups. We obtain several factors important to explaining the clusters\nand also found different role of disease specific measures and functional tests\nto clustering PD patients. Upon examining these measures, it might be possible\nto develop new disease treatment to prevent, or to delay, the occurrence of\nfalls.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 10:42:00 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Abdullah", "Sarini", ""], ["McGree", "James", ""], ["White", "Nicole", ""], ["Mengersen", "Kerrie", ""], ["Kerr", "Graham", ""]]}, {"id": "1910.01932", "submitter": "Theodoros Mathikolonis", "authors": "Theodoros Mathikolonis, Volker Roeber, Serge Guillas", "title": "Computationally efficient surrogate-based optimization of coastal storm\n  waves heights and run-ups", "comments": "arXiv admin note: text overlap with arXiv:1909.04600", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storm surges cause coastal inundations due to the setup of the water surface\nresulting from atmospheric pressure, surface winds and breaking waves. The\nlatter is particularly difficult to be accounted for. For instance, it was\nobserved that during Typhoon Haiyan (2013, Philippines), a stretch of coral\nreef near the coast, which was expected to protect the coastal communities,\nactually amplified the waves. The propagation and breaking process of such\nlarge nearshore waves can be successfully captured by a phase-resolving wave\nmodel. Building defences requires estimating not only the maximum storm surge\nheight, but also maxima of breaking wave height and run-up on land, under\nconsideration of a variety of storm characteristics. However, the computational\ncomplexity of the simulator makes optimization tasks impractical. To find the\nmaximum breaking wave (bore) height and the maximum run-up, we employ\noptim-MICE, a new surrogate-based optimization scheme based on Gaussian Process\nemulation and information theory. In two idealised settings, we efficiently\nidentify the conditions that create the largest storm waves at the coast using\na minimal number of simulations. This is the first surrogate-based optimization\nof storm waves. It opens the door to previously inconceivable coastal risk\nassessments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:52:31 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mathikolonis", "Theodoros", ""], ["Roeber", "Volker", ""], ["Guillas", "Serge", ""]]}, {"id": "1910.01964", "submitter": "Feriel Bouhadjera", "authors": "Bouhadjera Feriel (ULCO, Facult\\'e des Sciences Universit\\'e Badji\n  Mokhtar), Elias Ould Said (ULCO)", "title": "On the strong uniform consistency for relative error of the regression\n  function estimator for censoring times series model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector (X, T), where X is d-dimensional and T is\none-dimensional. We suppose that the random variable T is subject to random\nright censoring and satisfies the $\\alpha$-mixing property. The aim of this\npaper is to study the behavior of the kernel estimator of the relative error\nregression and to establish its uniform almost sure consistency with rate.\nFurthermore, we have highlighted the covariance term which measures the\ndependency. The simulation study shows that the proposed estimator performs\nwell for a finite sample size in different cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:15:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Feriel", "Bouhadjera", "", "ULCO, Facult\u00e9 des Sciences Universit\u00e9 Badji\n  Mokhtar"], ["Said", "Elias Ould", "", "ULCO"]]}, {"id": "1910.02017", "submitter": "Pamela Llop", "authors": "Maria Jose Llop, Pamela Llop, Maria Soledad Lopez, Andrea Gomez and\n  Gabriela V. Muller", "title": "Comparing statistical methods to predict leptospirosis incidence using\n  hydro-climatic covariables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leptospiroris, the infectious disease caused by the spirochete bacteria\nLeptospira interrogans, constitutes an important public health problem all over\nthe world. In Argentina, some regions present climate and geographic\ncharacteristics that favors the habitat of the bacteria Leptospira, whose\nsurvival strongly depends on climatic factors. For this reason, regional public\nhealth systems should include, as a main factor, the incidence of the disease\nin order to improve the prediction of potential outbreaks, helping to stop or\ndelay the virus transmission. The classic methods used to perform this kind of\npredictions are based in autoregressive time series tools which, as it is well\nknown, perform poorly when the data do not meet their requirements. Recently,\nseveral nonparametric methods have been introduced to deal with those problems.\nIn this work, we compare a semiparametric method, called Semi-Functional\nPartial Linear Regression (SFPLR) with the classic ARIMA and a new alternative\nARIMAX, in order to select the best predictive tool for the incidence of\nleptospirosis in the Argentinian Litoral region. In particular, SFPLR and\nARIMAX are methods that allow the use of (hydrometeorological) covariables\nwhich could improve the prediction of outbreaks of leptospirosis.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 21:22:35 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Llop", "Maria Jose", ""], ["Llop", "Pamela", ""], ["Lopez", "Maria Soledad", ""], ["Gomez", "Andrea", ""], ["Muller", "Gabriela V.", ""]]}, {"id": "1910.02042", "submitter": "Michael J Lew", "authors": "Michael J. Lew", "title": "A reckless guide to P-values: local evidence, global errors", "comments": "Chapter 13 in the book Good Research Practice in Experimental\n  Pharmacology, editors A. Bespalov, MC Michel, and T Steckler, to be published\n  by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This chapter demystifies P-values, hypothesis tests and significance tests,\nand introduces the concepts of local evidence and global error rates. The local\nevidence is embodied in \\textit{this} data and concerns the hypotheses of\ninterest for \\textit{this} experiment, whereas the global error rate is a\nproperty of the statistical analysis and sampling procedure. It is shown using\nsimple examples that local evidence and global error rates can be, and should\nbe, considered together when making inferences. Power analysis for experimental\ndesign for hypothesis testing are explained, along with the more locally\nfocussed expected P-values. Issues relating to multiple testing, HARKing, and\nP-hacking are explained, and it is shown that, in many situation, their effects\non local evidence and global error rates are in conflict, a conflict that can\nalways be overcome by a fresh dataset from replication of key experiments.\nStatistics is complicated, and so is science. There is no singular right way to\ndo either, and universally acceptable compromises may not exist. Statistics\noffers a wide array of tools for assisting with scientific inference by\ncalibrating uncertainty, but statistical inference is not a substitute for\nscientific inference. P-values are useful indices of evidence and deserve their\nplace in the statistical toolbox of basic pharmacologists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 01:20:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lew", "Michael J.", ""]]}, {"id": "1910.02043", "submitter": "Eduardo Soares Mr", "authors": "Eduardo Soares, Plamen Angelov", "title": "Fair-by-design explainable models for prediction of recidivism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction provides decision makers with an assessment of the\nlikelihood that a criminal defendant will reoffend that can be used in\npre-trial decision-making. It can also be used for prediction of locations\nwhere crimes most occur, profiles that are more likely to commit violent\ncrimes. While such instruments are gaining increasing popularity, their use is\ncontroversial as they may present potential discriminatory bias in the risk\nassessment. In this paper we propose a new fair-by-design approach to predict\nrecidivism. It is prototype-based, learns locally and extracts empirically the\ndata distribution. The results show that the proposed method is able to reduce\nthe bias and provide human interpretable rules to assist specialists in the\nexplanation of the given results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:13:20 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Soares", "Eduardo", ""], ["Angelov", "Plamen", ""]]}, {"id": "1910.02105", "submitter": "Allison Meisner", "authors": "Allison Meisner, Chirag R. Parikh, and Kathleen F. Kerr", "title": "Developing Biomarker Combinations in Multicenter Studies via Direct\n  Maximization and Penalization", "comments": "31 pages (including appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a study of acute kidney injury, we consider the setting of\nbiomarker studies involving patients at multiple centers where the goal is to\ndevelop a biomarker combination for diagnosis, prognosis, or screening. As\nbiomarker studies become larger, this type of data structure will be\nencountered more frequently. In the presence of multiple centers, one way to\nassess the predictive capacity of a given combination is to consider the\ncenter-adjusted AUC (aAUC), a summary of the ability of the combination to\ndiscriminate between cases and controls in each center. Rather than using a\ngeneral method, such as logistic regression, to construct the biomarker\ncombination, we propose directly maximizing the aAUC. Furthermore, it may be\ndesirable to have a biomarker combination with similar performance across\ncenters. To that end, we allow for penalization of the variability in the\ncenter-specific AUCs. We demonstrate desirable asymptotic properties of the\nresulting combinations. Simulations provide small-sample evidence that\nmaximizing the aAUC can lead to combinations with improved performance. We also\nuse simulated data to illustrate the utility of constructing combinations by\nmaximizing the aAUC while penalizing variability. Finally, we apply these\nmethods to data from the study of acute kidney injury.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:48:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Meisner", "Allison", ""], ["Parikh", "Chirag R.", ""], ["Kerr", "Kathleen F.", ""]]}, {"id": "1910.02114", "submitter": "Katherine Kempfert", "authors": "Katherine C. Kempfert, Yishi Wang, Cuixian Chen, and Samuel W.K. Wong", "title": "A Comparison Study on Nonlinear Dimension Reduction Methods with Kernel\n  Variations: Visualization, Optimization and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of high dimensionality, correlation among covariates, and noise\ncontained in data, dimension reduction (DR) techniques are often employed to\nthe application of machine learning algorithms. Principal Component Analysis\n(PCA), Linear Discriminant Analysis (LDA), and their kernel variants (KPCA,\nKLDA) are among the most popular DR methods. Recently, Supervised Kernel\nPrincipal Component Analysis (SKPCA) has been shown as another successful\nalternative. In this paper, brief reviews of these popular techniques are\npresented first. We then conduct a comparative performance study based on three\nsimulated datasets, after which the performance of the techniques are evaluated\nthrough application to a pattern recognition problem in face image analysis.\nThe gender classification problem is considered on MORPH-II and FG-NET, two\npopular longitudinal face aging databases. Several feature extraction methods\nare used, including biologically-inspired features (BIF), local binary patterns\n(LBP), histogram of oriented gradients (HOG), and the Active Appearance Model\n(AAM). After applications of DR methods, a linear support vector machine (SVM)\nis deployed with gender classification accuracy rates exceeding 95% on\nMORPH-II, competitive with benchmark results. A parallel computational approach\nis also proposed, attaining faster processing speeds and similar recognition\nrates on MORPH-II. Our computational approach can be applied to practical\ngender classification systems and generalized to other face analysis tasks,\nsuch as race classification and age prediction.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:33:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kempfert", "Katherine C.", ""], ["Wang", "Yishi", ""], ["Chen", "Cuixian", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1910.02160", "submitter": "Satabdi Saha", "authors": "Satabdi Saha, Duchwan Ryu and Nader Ebrahimi", "title": "Variable Selection with Random Survival Forest and Bayesian Additive\n  Regression Tree for Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we utilize a survival analysis methodology incorporating\nBayesian additive regression trees to account for nonlinear and additive\ncovariate effects. We compare the performance of Bayesian additive regression\ntrees, Cox proportional hazards and random survival forests models for censored\nsurvival data, using simulation studies and survival analysis for breast cancer\nwith U.S. SEER database for the year 2005. In simulation studies, we compare\nthe three models across varying sample sizes and censoring rates on the basis\nof bias and prediction accuracy. In survival analysis for breast cancer, we\nretrospectively analyze a subset of 1500 patients having invasive ductal\ncarcinoma that is a common form of breast cancer mostly affecting older woman.\nPredictive potential of the three models are then compared using some widely\nused performance assessment measures in survival literature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 22:18:17 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 05:38:12 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Saha", "Satabdi", ""], ["Ryu", "Duchwan", ""], ["Ebrahimi", "Nader", ""]]}, {"id": "1910.02170", "submitter": "Evan Rosenman", "authors": "Evan Rosenman and Karthik Rajkumar", "title": "Optimized Partial Identification Bounds for Regression Discontinuity\n  Designs with Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity (RD) design is one of the most popular\nquasi-experimental methods for applied causal inference. In practice, the\nmethod is quite sensitive to the assumption that individuals cannot control\ntheir value of a \"running variable\" that determines treatment status precisely.\nIf individuals are able to precisely manipulate their scores, then point\nidentification is lost. We propose a procedure for obtaining partial\nidentification bounds in the case of a discrete running variable where\nmanipulation is present. Our method relies on two stages: first, we derive the\ndistribution of non-manipulators under several assumptions about the data.\nSecond, we obtain bounds on the causal effect via a sequential convex\nprogramming approach. We also propose methods for tightening the partial\nidentification bounds using an auxiliary covariate, and derive confidence\nintervals via the bootstrap. We demonstrate the utility of our method on a\nsimulated dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 23:32:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rosenman", "Evan", ""], ["Rajkumar", "Karthik", ""]]}, {"id": "1910.02220", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "A Fundamental Bound on Performance of Non-Intrusive Load Monitoring with\n  Application to Smart Meter Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.SY eess.SY math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the expected estimation error of non-intrusive load monitoring\nalgorithms is lower bounded by the trace of the inverse of the\ncross-correlation matrix between the derivatives of the load profiles of the\nappliances. We use this fundamental bound to develop privacy-preserving\npolicies. Particularly, we devise a load-scheduling policy by maximizing the\nlower bound on the expected estimation error of non-intrusive load monitoring\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 06:34:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "1910.02379", "submitter": "Sarini Abdullah", "authors": "Sarini Abdullah, James McGree, Nicole White, Kerrie Mengersen, Graham\n  Kerr", "title": "Factors associated with injurious from falls in people with early stage\n  Parkinson's disease", "comments": "18 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falls are common in people with Parkinson's disease (PD) and have detrimental\neffects which can lower the quality of life. While studies have been conducted\nto learn about falling in general, factors distinguishing injurious from\nnon-injurious falls are less clear. We develop a two-stage Bayesian logistic\nregression model was used to model the association of falls and injurious falls\nwith data measured on patients. The forward stepwise selection procedure was\nused to determine which patient measures were associated with falls and\ninjurious falls, and Bayesian model averaging (BMA) was used to account for\nuncertainty in this variable selection procedure. Data on 99 patients for a\n12-month time period were considered in this analysis. Fifty five percent of\nthe patients experienced at least one fall, with a total of 335 falls cases;\n25% of which were injurious falls. Fearful, Tinetti gait, and previous falls\nwere the risk factors for fall/non-fall, with 77% accuracy, 76% sensitivity,\nand 76% specificity. Fall time, body mass index, anxiety, balance, gait, and\ngender were the risk factors associated with injurious falls. Thus, attaining\nnormal body mass index, improving balance and gait could be seen as preventive\nefforts for injurious falls. There was no significant difference in the risk of\nfalls between males and females, yet if falls occurred, females were more\nlikely to get injured than males.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:38:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Abdullah", "Sarini", ""], ["McGree", "James", ""], ["White", "Nicole", ""], ["Mengersen", "Kerrie", ""], ["Kerr", "Graham", ""]]}, {"id": "1910.02381", "submitter": "Kurt Riedel", "authors": "P. Yushmanov, T. Takizuka, K. Riedel, O. Kardaun, J. Cordey, S. Kaye,\n  D. Post", "title": "Scalings for Tokamak Energy Confinement", "comments": null, "journal-ref": "Nucl. Fusion 1990", "doi": null, "report-no": null, "categories": "physics.plasm-ph cs.SY eess.SY stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the basis of an analysis of the ITER L-mode energy confinement database,\ntwo new scaling expressions for tokamak L-mode energy confinement are proposed,\nnamely a power law scaling and an offset-linear scaling. The analysis indicates\nthat the present multiplicity of scaling expressions for the energy confinement\ntime TE in tokamaks (Goldston, Kaye, Odajima-Shimomura, Rebut-Lallia, etc.) is\ndue both to the lack of variation of a key parameter combination in the\ndatabase, fs = 0.32 R a^.75 k^ 5 ~ A a^.25 k^.5, and to variations in the\ndependence of rE on the physical parameters among the different tokamaks in the\ndatabase. By combining multiples of fs and another factor, fq = 1.56 a^2 kB/R\nIp = qeng/3.2, which partially reflects the tokamak to tokamak variation of the\ndependence of TE on q and therefore implicitly the dependence of TE on Ip and\nn,., the two proposed confinement scaling expressions can be transformed to\nforms very close to most of the common scaling expressions. To reduce the\nmultiplicity of the scalings for energy confinement, the database must be\nimproved by adding new data with significant variations in fs, and the physical\nreasons for the tokamak to tokamak variation of some of the dependences of the\nenergy confinement time on tokamak parameters must be clarified\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:39:57 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yushmanov", "P.", ""], ["Takizuka", "T.", ""], ["Riedel", "K.", ""], ["Kardaun", "O.", ""], ["Cordey", "J.", ""], ["Kaye", "S.", ""], ["Post", "D.", ""]]}, {"id": "1910.02484", "submitter": "Fangyi Yang", "authors": "Fangyi Yang, Weihua Gu, Michael Cassidy, Xin Li, Tiezhu Li", "title": "Achieving higher taxi outflows from a congested drop-off lane: a\n  simulation-based policy study", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine special lanes used by taxis and other shared-ride services to\ndrop-off patrons at airport and rail terminals. Vehicles are prohibited from\novertaking each other within the lane. They must therefore wait in a\nfirst-in-first-out queue during busy periods. Patrons are often discharged from\nvehicles only upon reaching a desired drop-off area near the terminal entrance.\nWhen wait times grow long, however, some vehicles discharge their patrons in\nadvance of that desired area. A train station in Eastern China is selected as a\ncase study. Its FIFO drop-off lane is presently managed by policemen who allow\ntaxis to enter the lane in batched fashion. Inefficiencies are observed because\ncurb space near the terminal often goes unused. This is true even when\nsupplemental batches of taxis are released into the lane in efforts to fill\nthose spaces. A microscopic simulation model of a FIFO drop-off lane is\ndeveloped in-house, and is painstakingly calibrated to data measured at the\nstudy site. Simulation experiments indicate that rescinding the FIFO lane's\npresent batching strategy can increase taxi outflow by more than 26 percent.\nFurther experiments show that even greater gains can be achieved by batching\ntaxis, but requiring them to discharge patrons when forced by downstream queues\nto stop a prescribed distance in advance of a desired drop-off area. Further\ngains were predicted by requiring the lead taxi in each batch to discharge its\npatron only after travelling a prescribed distance beyond a desired location.\nPractical implications are discussed in light of the present boom in\nshared-ride services.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 17:50:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yang", "Fangyi", ""], ["Gu", "Weihua", ""], ["Cassidy", "Michael", ""], ["Li", "Xin", ""], ["Li", "Tiezhu", ""]]}, {"id": "1910.02498", "submitter": "Milan Straka", "authors": "Milan Straka, Pasquale De Falco, Gabriella Ferruzzi, Daniela Proto,\n  Gijs van der Poel, Shahab Khormali, \\v{L}ubo\\v{s} Buzna", "title": "Predicting popularity of EV charging infrastructure from GIS data", "comments": null, "journal-ref": "IEEE Access ( Volume: 8 ) 2020", "doi": "10.1109/ACCESS.2020.2965621", "report-no": null, "categories": "stat.AP cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of charging infrastructure is essential for large-scale\nadoption of electric vehicles (EV). Charging patterns and the utilization of\ninfrastructure have consequences not only for the energy demand, loading local\npower grids but influence the economic returns, parking policies and further\nadoption of EVs. We develop a data-driven approach that is exploiting\npredictors compiled from GIS data describing the urban context and urban\nactivities near charging infrastructure to explore correlations with a\ncomprehensive set of indicators measuring the performance of charging\ninfrastructure. The best fit was identified for the size of the unique group of\nvisitors (popularity) attracted by the charging infrastructure. Consecutively,\ncharging infrastructure is ranked by popularity. The question of whether or not\na given charging spot belongs to the top tier is posed as a binary\nclassification problem and predictive performance of logistic regression\nregularized with an l-1 penalty, random forests and gradient boosted regression\ntrees is evaluated. Obtained results indicate that the collected predictors\ncontain information that can be used to predict the popularity of charging\ninfrastructure. The significance of predictors and how they are linked with the\npopularity are explored as well. The proposed methodology can be used to inform\ncharging infrastructure deployment strategies.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:38:40 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Straka", "Milan", ""], ["De Falco", "Pasquale", ""], ["Ferruzzi", "Gabriella", ""], ["Proto", "Daniela", ""], ["van der Poel", "Gijs", ""], ["Khormali", "Shahab", ""], ["Buzna", "\u013dubo\u0161", ""]]}, {"id": "1910.02501", "submitter": "Ali Reza Fotouhi", "authors": "Ali Reza Fotouhi", "title": "Bayesian analysis of dynamic binary data: A simulation study and\n  application to economic index SP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proposed in the literature that in some complicated problems maximum\nlikelihood estimates (MLE) are not suitable or even do not exist. An\nalternative to MLE for estimation of the parameters is the Bayesian method. The\nMarkov chain Monte Carlo (MCMC) simulation procedure is designed to fit\nBayesian models. Bayesian method like classical method (MLE) has advantages and\ndisadvantages. One of the advantages of Bayesian method over MLE method is the\nability of saving the information included in past data through the posterior\ndistributions of the model parameters to be used for modelling future data. In\nthis article we investigate the performance of Bayesian method in modelling\ndynamic binary data when the data are growing over time and individuals.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:54:56 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Fotouhi", "Ali Reza", ""]]}, {"id": "1910.02506", "submitter": "Subharup Guha", "authors": "Subharup Guha, Rex Jung and David Dunson", "title": "Predicting Phenotypes from Brain Connection Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the problem of predicting a response variable based\non a network-valued predictor. Our particular motivation is developing\ninterpretable and accurate predictive models for cognitive traits and\nneuro-psychiatric disorders based on an individual's brain connection network\n(connectome). Current methods focus on reducing the complex and\nhigh-dimensional brain network into a low-dimensional set of pre-specified\nfeatures prior to applying standard predictive algorithms. Such methods are\nsensitive to feature choice and inevitably discard information. We instead\npropose a nonparametric Bayes class of models that utilize information from the\nentire adjacency matrix defining connections among brain regions in adaptively\ndefining flexible predictive algorithms, while maintaining interpretability.\nThe proposed Bayesian Connectomics (BaCon) model class utilizes\nPoisson-Dirichlet processes to detect a lower-dimensional, bidirectional\n(covariate, subject) pattern in the adjacency matrix. The small n, large p\nproblem is transformed into a \"small n, small q\" problem, facilitating an\neffective stochastic search of the predictors. A spike-and-slab prior for the\ncluster predictors strikes a balance between regression model parsimony and\nflexibility, resulting in improved inferences and test case predictions. We\ndescribe basic properties of the BaCon model class and develop efficient\nalgorithms for posterior computation. The resulting methods are shown to\noutperform existing approaches in simulations and applied to a creative\nreasoning data set.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:19:05 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 16:23:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Guha", "Subharup", ""], ["Jung", "Rex", ""], ["Dunson", "David", ""]]}, {"id": "1910.02908", "submitter": "Viviana Vargas", "authors": "Viviana Lorena Vargas and Sinesio Pesco", "title": "Skeleton based simulation of turbidite channels system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to model turbidite channels using training images is\npresented, it is called skeleton based simulation. This is an object based\nmodel that uses some elements of multipoint geostatistics. The main idea is to\nsimplify the representation of the training image by a one-dimensional object\ncalled training skeleton. From this new object, information about the direction\nand length of the channels is extracted and it is used to simulate others\nskeletons. These new skeletons are used to create a 3D model of channels inside\na turbidite lobe.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 16:54:23 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Vargas", "Viviana Lorena", ""], ["Pesco", "Sinesio", ""]]}, {"id": "1910.02958", "submitter": "Robert Feldmann", "authors": "R. Feldmann", "title": "LEO-Py: Estimating likelihoods for correlated, censored, and uncertain\n  data with given marginal distributions", "comments": "21 pages, 8 figures, 2 tables, to appear in Astronomy and Computing,\n  LEO-Py is available at github.com/rfeldmann/leopy", "journal-ref": null, "doi": "10.1016/j.ascom.2019.100331", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data with uncertain, missing, censored, and correlated values are commonplace\nin many research fields including astronomy. Unfortunately, such data are often\ntreated in an ad hoc way in the astronomical literature potentially resulting\nin inconsistent parameter estimates. Furthermore, in a realistic setting, the\nvariables of interest or their errors may have non-normal distributions which\ncomplicates the modeling. I present a novel approach to compute the likelihood\nfunction for such data sets. This approach employs Gaussian copulas to decouple\nthe correlation structure of variables and their marginal distributions\nresulting in a flexible method to compute likelihood functions of data in the\npresence of measurement uncertainty, censoring, and missing data. I demonstrate\nits use by determining the slope and intrinsic scatter of the star forming\nsequence of nearby galaxies from observational data. The outlined algorithm is\nimplemented as the flexible, easy-to-use, open-source Python package LEO-Py.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:00:00 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Feldmann", "R.", ""]]}, {"id": "1910.03203", "submitter": "Amanda Kowalczyk", "authors": "Zijian Gao and Amanda Kowalczyk", "title": "Random forest model identifies serve strength as a key predictor of\n  tennis match outcome", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tennis is a popular sport worldwide, boasting millions of fans and numerous\nnational and international tournaments. Like many sports, tennis has benefitted\nfrom the popularity of rigorous record-keeping of game and player information,\nas well as the growth of machine learning methods for use in sports analytics.\nOf particular interest to bettors and betting companies alike is potential use\nof sports records to predict tennis match outcomes prior to match start. We\ncompiled, cleaned, and used the largest database of tennis match information to\ndate to predict match outcome using fairly simple machine learning methods.\nUsing such methods allows for rapid fit and prediction times to readily\nincorporate new data and make real-time predictions. We were able to predict\nmatch outcomes with upwards of 80% accuracy, much greater than predictions\nusing betting odds alone, and identify serve strength as a key predictor of\nmatch outcome. By combining prediction accuracies from three models, we were\nable to nearly recreate a probability distribution based on average betting\nodds from betting companies, which indicates that betting companies are using\nsimilar information to assign odds to matches. These results demonstrate the\ncapability of relatively simple machine learning models to quite accurately\npredict tennis match outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 04:05:59 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Gao", "Zijian", ""], ["Kowalczyk", "Amanda", ""]]}, {"id": "1910.03346", "submitter": "Eniko Szekely", "authors": "Eniko Sz\\'ekely, Sebastian Sippel, Reto Knutti, Guillaume Obozinski,\n  Nicolai Meinshausen", "title": "A direct approach to detection and attribution of climate change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a novel statistical learning approach for detection and\nattribution (D&A) of climate change. Traditional optimal D&A studies try to\ndirectly model the observations from model simulations, but practically this is\nchallenging due to high-dimensionality. Dimension reduction techniques reduce\nthe dimensionality, typically using empirical orthogonal functions, but as\nthese techniques are unsupervised, the reduced space considered is somewhat\narbitrary. Here, we propose a supervised approach where we predict a given\nexternal forcing, e.g., anthropogenic forcing, directly from the spatial\npattern of climate variables, and use the predicted forcing as a test statistic\nfor D&A. We want the prediction to work well even under changes in the\ndistribution of other external forcings, e.g., solar or volcanic forcings, and\ntherefore formulate the optimization problem from a distributional robustness\nperspective.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 11:41:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sz\u00e9kely", "Eniko", ""], ["Sippel", "Sebastian", ""], ["Knutti", "Reto", ""], ["Obozinski", "Guillaume", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1910.03368", "submitter": "Natalia R. Kunst", "authors": "Natalia R. Kunst, Edward Wilson, Fernando Alarid-Escudero, Gianluca\n  Baio, Alan Brennan, Michael Fairley, David Glynn, Jeremy D.\n  Goldhaber-Fiebert, Chris Jackson, Hawre Jalal, Nicolas A. Menzies, Mark\n  Strong, Howard Thom, Anna Heath (on behalf of the Collaborative Network for\n  Value of Information (ConVOI))", "title": "Computing the Expected Value of Sample Information Efficiently:\n  Expertise and Skills Required for Four Model-Based Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Value of information (VOI) analyses can help policy-makers make\ninformed decisions about whether to conduct and how to design future studies.\nHistorically, a computationally expensive method to compute the Expected Value\nof Sample Information (EVSI) restricted the use of VOI to simple decision\nmodels and study designs. Recently, four EVSI approximation methods have made\nsuch analyses more feasible and accessible. We provide practical\nrecommendations for analysts computing EVSI by evaluating these novel methods.\nMethods: Members of the Collaborative Network for Value of Information (ConVOI)\ncompared the inputs, analyst's expertise and skills, and software required for\nfour recently developed approximation methods. Information was also collected\non the strengths and limitations of each approximation method. Results: All\nfour EVSI methods require a decision-analytic model's probabilistic sensitivity\nanalysis (PSA) output. One of the methods also requires the model to be re-run\nto obtain new PSA outputs for each EVSI estimation. To compute EVSI, analysts\nmust be familiar with at least one of the following skills: advanced regression\nmodeling, likelihood specification, and Bayesian modeling. All methods have\ndifferent strengths and limitations, e.g., some methods handle evaluation of\nstudy designs with more outcomes more efficiently while others quantify\nuncertainty in EVSI estimates. All methods are programmed in the statistical\nlanguage R and two of the methods provide online applications. Conclusion: Our\npaper helps to inform the choice between four efficient EVSI estimation\nmethods, enabling analysts to assess the methods' strengths and limitations and\nselect the most appropriate EVSI method given their situation and skills.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 12:40:05 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kunst", "Natalia R.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Wilson", "Edward", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Alarid-Escudero", "Fernando", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Baio", "Gianluca", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Brennan", "Alan", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Fairley", "Michael", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Glynn", "David", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Goldhaber-Fiebert", "Jeremy D.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Jackson", "Chris", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Jalal", "Hawre", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Menzies", "Nicolas A.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Strong", "Mark", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Thom", "Howard", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Heath", "Anna", "", "on behalf of the Collaborative Network for\n  Value of Information"]]}, {"id": "1910.03447", "submitter": "Eric Lock", "authors": "Sarah Samorodnitsky, Katherine A. Hoadley, and Eric F. Lock", "title": "A Pan-Cancer and Polygenic Bayesian Hierarchical Model for the Effect of\n  Somatic Mutations on Survival", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We built a novel Bayesian hierarchical survival model based on the somatic\nmutation profile of patients across 50 genes and 27 cancer types. The\npan-cancer quality allows for the model to \"borrow\" information across cancer\ntypes, motivated by the assumption that similar mutation profiles may have\nsimilar (but not necessarily identical) effects on survival across different\ntissues-of-origin or tumor types. The effect of a mutation at each gene was\nallowed to vary by cancer type while the mean effect of each gene was shared\nacross cancers. Within this framework we considered four parametric survival\nmodels (normal, log-normal, exponential, and Weibull), and we compared their\nperformance via a cross-validation approach in which we fit each model on\ntraining data and estimate the log-posterior predictive likelihood on test\ndata. The log-normal model gave the best fit, and we investigated the partial\neffect of each gene on survival via a forward selection procedure. Through this\nwe determined that mutations at TP53 and FAT4 were together the most useful for\npredicting patient survival. We validated the model via simulation to ensure\nthat our algorithm for posterior computation gave nominal coverage rates. The\ncode used for this analysis can be found at\nhttp://github.com/sarahsamorodnitsky/Pan-Cancer-Survival-Modeling , and the\nresults are at http://ericfrazerlock.com/surv_figs/SurvivalDisplay.html .\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:11:29 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Samorodnitsky", "Sarah", ""], ["Hoadley", "Katherine A.", ""], ["Lock", "Eric F.", ""]]}, {"id": "1910.03458", "submitter": "Reinis Osis", "authors": "Reinis Osis (ESO), Fran\\c{c}ois Laurent (ESO), Ren\\'e Poccard-Chapuis\n  (UMR SELMET)", "title": "Simulation of land use dynamics in Paragominas-PA: differences in\n  spatial rules between smallholdings and agribusiness areas", "comments": "in Portuguese", "journal-ref": "Semin{\\'a}rio internacional de desenvolvimento rural\n  sustent{\\'a}vel, cooperativismo e economia solid{\\'a}ria -- SICOOPES, Aug\n  2016, Castanhal, Brazil", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present the results of the land use dynamic\nsimulations in the municipality of Paragominas-PA. The simulation is based on\nmodels built from past land use data and spatial variables of the natural\nenvironment and infrastructure. Two spatial units were analyzed: the central\narea of commercial agricultural production and the area of settlements and\nsmallholdings the east. The results show distinct spatial dynamics between the\nanalyzed areas, among which we highlight the role of soil characteristics and,\nassociated with the topography and the occupation history, are part of the\ncontext in which is defined rationality producers. Considering the transition\nfrom forest to pasture in the commercial farming area most often associated\nsoils are sandy. This raises the following hypothesis: the deforestation that\noccurred in the period are related to livestock activities. Livestock favors\naccess to water and low fertility sands does not affect production. On the\nother hand, the soybean expansion occurred preferentially on existing pastures\nand on clay soils (Belterra clay), reducing the availability of pastures on\nthese soils. The relative importance of types of soil increases with time. In\nthe area of settlements, the transition from forest to pasture and family crops\noccurred preferentially on the variegated clay. However, it is possible that\nthe prevalence of this transition on this texture has been given due more to\nthe history of occupation of this area. Historically the sandy valleys were the\nfirst to be occupied, and the continuity of the deforestation occurred toward\nthe slopes dominated by variegated clay and plateaus with Belterra clay. These\nassociations observed indicate that, within a wider context of social, economic\nand political factors, natural variable factors in space are important for the\nchoice of managements in the properties, but they are done differently in the\nterritory, and the best knowledge of these relationships are useful for\nterritorial planning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:26:55 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Osis", "Reinis", "", "ESO"], ["Laurent", "Fran\u00e7ois", "", "ESO"], ["Poccard-Chapuis", "Ren\u00e9", "", "UMR SELMET"]]}, {"id": "1910.03491", "submitter": "Adriano Siqueira Francisco", "authors": "Adriano Francisco Siqueira, Morun Bernardino Neto, Ana Lucia Gabas\n  Ferreira, Luciana Alves de Medeiros, Mario da Silva Garrote-Filho, Ubirajara\n  Coutinho Filho, Nilson Penha-Silva", "title": "Stochastic modeling of hyposmotic lysis and characterization of\n  different osmotic stability subgroups of human erythrocytes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a novel stochastic model for the study of hyposmotic\nhemolysis. This model is capable of reproducing both the kinetics in the\ntransient phase and the lysis equilibrium in the stationary phase, as well as\nthe variability of the experimental measurements. The stationary distribution\nof this model can be approximated to a normal distribution, with mean and\nvariance related to the salt concentration used in the erythrocyte osmotic\nfragility assay. The proposed model can generalize the classical Boltzmann\nsigmoidal model often used in adjusting the stationary experimental data\ndistribution. A typical osmotic fragility curve is constructed from the\nabsorbance of free hemoglobin as a function of the decrease in NaCl (X)\nconcentration and allows the determination of H50, an osmotic fragility\nvariable that represents the saline concentration capable of promoting 50%\nlysis, and dX, an osmotic stability variable that represents 1/4 of the\nvariation in salt concentration required to promote 100% lysis. Based on the\nstationary distribution of the proposed model it is possible to stratify a\npopulation into different groups of individuals with similar levels of cell\nstability. These groups are very suitable to study the factors associated with\ncell stability, such as gender, age and lipids, among others. The method\npresented here was applied to a sample of 71 individuals and several results\nwere obtained. In a group of 25 female subjects, with H50 values between 0.42\nand 0.47 g/dL NaCl, for example, the use of a quadratic model to study the\ndependence of the stability index dX/H50 with blood LDL-cholesterol levels,\nshowed that the erythrocyte osmotic stability increases with increasing LDL-C\nto a maximum value close to 90 mg/dL and then decreases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:57:40 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Siqueira", "Adriano Francisco", ""], ["Neto", "Morun Bernardino", ""], ["Ferreira", "Ana Lucia Gabas", ""], ["de Medeiros", "Luciana Alves", ""], ["Garrote-Filho", "Mario da Silva", ""], ["Filho", "Ubirajara Coutinho", ""], ["Penha-Silva", "Nilson", ""]]}, {"id": "1910.03536", "submitter": "Michael Hudgens", "authors": "Sujatro Chakladar, Michael G. Hudgens, M. Elizabeth Halloran, John D.\n  Clemens, Mohammad Ali, Michael E. Emch", "title": "Inverse Probability Weighted Estimators of Vaccine Effects Accommodating\n  Partial Interference and Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating population-level effects of a vaccine is challenging because there\nmay be interference, i.e., the outcome of one individual may depend on the\nvaccination status of another individual. Partial interference occurs when\nindividuals can be partitioned into groups such that interference occurs only\nwithin groups. In the absence of interference, inverse probability weighted\n(IPW) estimators are commonly used to draw inference about causal effects of an\nexposure or treatment. Tchetgen Tchetgen and VanderWeele (2012) proposed a\nmodified IPW estimator for causal effects in the presence of partial\ninterference. Motivated by a cholera vaccine study in Bangladesh, this paper\nconsiders an extension of the Tchetgen Tchetgen and VanderWeele IPW estimator\nto the setting where the outcome is subject to right censoring using inverse\nprobability of censoring weights (IPCW). Censoring weights are estimated using\nproportional hazards frailty models. The large sample properties of the IPCW\nestimators are derived, and simulation studies are presented demonstrating the\nestimators' performance in finite samples. The methods are then used to analyze\ndata from the cholera vaccine study.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:46:32 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chakladar", "Sujatro", ""], ["Hudgens", "Michael G.", ""], ["Halloran", "M. Elizabeth", ""], ["Clemens", "John D.", ""], ["Ali", "Mohammad", ""], ["Emch", "Michael E.", ""]]}, {"id": "1910.03577", "submitter": "Jin Ming", "authors": "Suprateek Kundu, Jin Ming, and Jennifer Stevens", "title": "Dynamic Brain Functional Networks Guided By Anatomical Knowledge", "comments": "45 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the potential of dynamic brain networks as a neuroimaging\nbiomarkers for mental illnesses is being increasingly recognized. However,\nthere are several unmet challenges in developing such biomarkers, including the\nneed for methods to model rapidly changing network states. In one of the first\nsuch efforts, we develop a novel approach for computing dynamic brain\nfunctional connectivity (FC), that is guided by brain structural connectivity\n(SC) computed from diffusion tensor imaging (DTI) data. The proposed approach\ninvolving dynamic Gaussian graphical models decomposes the time course into\nnon-overlapping state phases determined by change points, each having a\ndistinct network. We develop an optimization algorithm to implement the method\nsuch that the estimation of both the change points and the state-phase specific\nnetworks are fully data driven and unsupervised, and guided by SC information.\nThe approach is scalable to large dimensions and extensive simulations\nillustrate its clear advantages over existing methods in terms of network\nestimation accuracy and detecting dynamic network changes. An application of\nthe method to a posttraumatic stress disorder (PTSD) study reveals important\ndynamic resting state connections in regions of the brain previously implicated\nin PTSD. We also illustrate that the dynamic networks computed under the\nproposed method are able to better predict psychological resilience among\ntrauma exposed individuals compared to existing dynamic and stationary\nconnectivity approaches, which highlights its potential as a neuroimaging\nbiomarker.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:48:27 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kundu", "Suprateek", ""], ["Ming", "Jin", ""], ["Stevens", "Jennifer", ""]]}, {"id": "1910.03628", "submitter": "Stefan Thurner", "authors": "Stefan Thurner, Wenyuan Liu, Peter Klimek, Siew Ann Cheong", "title": "The role of mainstreamness and interdisciplinarity for the relevance of\n  scientific papers", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0230325", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is demand from science funders, industry, and the public that science\nshould become more risk-taking, more out-of-the-box, and more\ninterdisciplinary. Is it possible to tell how interdisciplinary and\nout-of-the-box scientific papers are, or which papers are mainstream? Here we\nuse the bibliographic coupling network, derived from all physics papers that\nwere published in the Physical Review journals in the past century, to try to\nidentify them as mainstream, out-of-the-box, or interdisciplinary. We show that\nthe network clusters into scientific fields. The position of individual papers\nwith respect to these clusters allows us to estimate their degree of\nmainstreamness or interdisciplinary. We show that over the past decades the\nfraction of mainstream papers increases, the fraction of out-of-the-box\ndecreases, and the fraction of interdisciplinary papers remains constant.\nStudying the rewards of papers, we find that in terms of absolute citations,\nboth, mainstream and interdisciplinary papers are rewarded. In the long run,\nmainstream papers perform less than interdisciplinary ones in terms of citation\nrates. We conclude that to avoid a trend towards mainstreamness a new incentive\nscheme is necessary.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:33:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Thurner", "Stefan", ""], ["Liu", "Wenyuan", ""], ["Klimek", "Peter", ""], ["Cheong", "Siew Ann", ""]]}, {"id": "1910.03788", "submitter": "Alex Deng", "authors": "Alex Deng, Yicheng Li, Jiannan Lu, Vivek Ramamurthy", "title": "On Post-Selection Inference in A/B Tests", "comments": null, "journal-ref": null, "doi": "10.1145/3447548.3467129", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When interpreting A/B tests, we typically focus only on the statistically\nsignificant results and take them by face value. This practice, termed\npost-selection inference in the statistical literature, may negatively affect\nboth point estimation and uncertainty quantification, and therefore hinder\ntrustworthy decision making in A/B testing. To address this issue, in this\npaper we explore two seemingly unrelated paths, one based on supervised machine\nlearning and the other on empirical Bayes, and propose post-selection\ninferential approaches that combine the strengths of both. Through large-scale\nsimulated and empirical examples, we demonstrate that our proposed\nmethodologies stand out among other existing ones in both reducing\npost-selection biases and improving confidence interval coverage rates, and\ndiscuss how they can be conveniently adjusted to real-life scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:39:47 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 17:21:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Deng", "Alex", ""], ["Li", "Yicheng", ""], ["Lu", "Jiannan", ""], ["Ramamurthy", "Vivek", ""]]}, {"id": "1910.04086", "submitter": "David Ginsbourger", "authors": "Poompol Buathong, David Ginsbourger, Tipaluck Krityakierne", "title": "Kernels over Sets of Finite Sets using RKHS Embeddings, with Application\n  to Bayesian (Combinatorial) Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on kernel methods for set-valued inputs and their application to\nBayesian set optimization, notably combinatorial optimization. We investigate\ntwo classes of set kernels that both rely on Reproducing Kernel Hilbert Space\nembeddings, namely the ``Double Sum'' (DS) kernels recently considered in\nBayesian set optimization, and a class introduced here called ``Deep\nEmbedding'' (DE) kernels that essentially consists in applying a radial kernel\non Hilbert space on top of the canonical distance induced by another kernel\nsuch as a DS kernel. We establish in particular that while DS kernels typically\nsuffer from a lack of strict positive definiteness, vast subclasses of DE\nkernels built upon DS kernels do possess this property, enabling in turn\ncombinatorial optimization without requiring to introduce a jitter parameter.\nProofs of theoretical results about considered kernels are complemented by a\nfew practicalities regarding hyperparameter fitting. We furthermore demonstrate\nthe applicability of our approach in prediction and optimization tasks, relying\nboth on toy examples and on two test cases from mechanical engineering and\nhydrogeology, respectively. Experimental results highlight the applicability\nand compared merits of the considered approaches while opening new perspectives\nin prediction and sequential design with set inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:38 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:55:58 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Buathong", "Poompol", ""], ["Ginsbourger", "David", ""], ["Krityakierne", "Tipaluck", ""]]}, {"id": "1910.04107", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Boris Savelyev, Vladimir V. Arlazarov", "title": "Next integrated result modelling for stopping the text field recognition\n  process in a video using a result model with per-character alternatives", "comments": "6 pages, 3 figures, 1 table, submitted and accepted for the 12th\n  International Conference on Machine Vision (ICMV 2019)", "journal-ref": "Proc. SPIE 11433 ICMV-2019 (2020), 114332M", "doi": "10.1117/12.2559447", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of document analysis and recognition using mobile devices for\ncapturing, and the field of object recognition in a video stream, an important\nproblem is determining the time when the capturing process should be stopped.\nEfficient stopping influences not only the total time spent for performing\nrecognition and data entry, but the expected accuracy of the result as well.\nThis paper is directed on extending the stopping method based on next\nintegrated recognition result modelling, in order for it to be used within a\nstring result recognition model with per-character alternatives. The stopping\nmethod and notes on its extension are described, and experimental evaluation is\nperformed on an open dataset MIDV-500. The method was compares with previously\npublished methods based on input observations clustering. The obtained results\nindicate that the stopping method based on the next integrated result modelling\nallows to achieve higher accuracy, even when compared with the best achievable\nconfiguration of the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:43:42 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Savelyev", "Boris", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1910.04236", "submitter": "Rafael Chaves", "authors": "Matheus Capela, Lucas C. C\\'eleri, Kavan Modi, Rafael Chaves", "title": "Monogamy of Temporal Correlations: Witnessing non-Markovianity Beyond\n  Data Processing", "comments": "9 pages + appendix, 2 figures", "journal-ref": "Phys. Rev. Research 2, 013350 (2020)", "doi": "10.1103/PhysRevResearch.2.013350", "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of natural phenomena via a Markov process --- a process for\nwhich the future is independent of the past, given the present--- is ubiquitous\nin many fields of science. Within this context, it is of foremost importance to\ndevelop ways to check from the available empirical data if the underlying\nmechanism is indeed Markovian. A paradigmatic example is given by data\nprocessing inequalities, the violation of which is an unambiguous proof of the\nnon-Markovianity of the process. Here, our aim is twofold. First we show the\nexistence of a monogamy-like type of constraints, beyond data processing,\nrespected by Markov chains. Second, to show a novel connection between the\nquantification of causality and the violation of both data processing and\nmonogamy inequalities. Apart from its foundational relevance in the study of\nstochastic processes we also consider the applicability of our results in a\ntypical quantum information setup, showing it can be useful to witness the\nnon-Markovianity arising in a sequence of quantum non-projective measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 20:30:31 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Capela", "Matheus", ""], ["C\u00e9leri", "Lucas C.", ""], ["Modi", "Kavan", ""], ["Chaves", "Rafael", ""]]}, {"id": "1910.04283", "submitter": "Victor Gabriel Capdeville da Silva", "authors": "Vitor G. C. da Silva (1), Kelly C. M. Gon\\c{c}alves (1) and Jo\\~ao B.\n  M. Pereira (1) ((1) Universidade Federal do Rio de Janeiro)", "title": "Bayesian factor models for multivariate categorical data obtained from\n  questionnaires", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is a flexible technique for assessment of multivariate\ndependence and codependence. Besides being an exploratory tool used to reduce\nthe dimensionality of multivariate data, it allows estimation of common factors\nthat often have an interesting theoretical interpretation in real problems.\nHowever, standard factor analysis is only applicable when the variables are\nscaled, which is often inappropriate, for example, in data obtained from\nquestionnaires in the field of psychology,where the variables are often\ncategorical. In this framework, we propose a factor model for the analysis of\nmultivariate ordered and non-ordered polychotomous data. The inference\nprocedure is done under the Bayesian approach via Markov chain Monte Carlo\nmethods. Two Monte-Carlo simulation studies are presented to investigate the\nperformance of this approach in terms of estimation bias, precision and\nassessment of the number of factors. We also illustrate the proposed method to\nanalyze participants' responses to the Motivational State Questionnaire\ndataset, developed to study emotions in laboratory and field settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:40:21 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 13:47:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["da Silva", "Vitor G. C.", "", "Universidade Federal do Rio de Janeiro"], ["Gon\u00e7alves", "Kelly C. M.", "", "Universidade Federal do Rio de Janeiro"], ["Pereira", "Jo\u00e3o B. M.", "", "Universidade Federal do Rio de Janeiro"]]}, {"id": "1910.04843", "submitter": "Chenguang Dai", "authors": "Chenguang Dai, Duo Chan, Peter Huybers, and Natesh Pillai", "title": "Late 19th-Century Navigational Uncertainties and Their Influence on Sea\n  Surface Temperature Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimates of historical changes in sea surface temperatures (SSTs)\nand their uncertainties are important for documenting and understanding\nhistorical changes in climate. A source of uncertainty that has not previously\nbeen quantified in historical SST estimates stems from position errors. A\nBayesian inference framework is proposed for quantifying errors in reported\npositions and their implications on SST estimates. The analysis framework is\napplied to data from the International Comprehensive Ocean-Atmosphere Data Set\n(ICOADS3.0) in 1885, a time when astronomical and chronometer estimation of\nposition was common, but predating the use of radio signals. Focus is upon a\nsubset of 943 ship tracks from ICOADS3.0 that report their position every two\nhours to a precision of 0.01{\\deg} longitude and latitude. These data are\ninterpreted as positions determined by dead reckoning that are periodically\nupdated by celestial correction techniques. The posterior medians of\nuncertainties in celestial correction are 33.1 km (0.30{\\deg} on the equator)\nin longitude and 24.4 km (0.22{\\deg}) in latitude, respectively. The posterior\nmedians of two-hourly dead reckoning uncertainties are 19.2% for ship speed and\n13.2{\\deg} for ship heading, leading to random position uncertainties with\nmedian 0.18{\\deg} (20 km on the equator) in longitude and 0.15{\\deg} (17 km) in\nlatitude. Reported ship tracks also contain systematic position uncertainties\nrelating to precursor dead-reckoning positions not being updated after\nobtaining celestial position estimates, indicating that more accurate positions\ncan be provided for SST observations. Finally, we translate position errors\ninto SST uncertainties by sampling an ensemble of SSTs from the Multi-scale\nUltra-high resolution Sea Surface Temperature (MURSST) data set.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 20:40:06 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 02:00:21 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dai", "Chenguang", ""], ["Chan", "Duo", ""], ["Huybers", "Peter", ""], ["Pillai", "Natesh", ""]]}, {"id": "1910.04906", "submitter": "Mustafa Bilgic", "authors": "Vinesh Kannan, Matthew A. Shapiro, Mustafa Bilgic", "title": "Hindsight Analysis of the Chicago Food Inspection Forecasting Model", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chicago Department of Public Health (CDPH) conducts routine food\ninspections of over 15,000 food establishments to ensure the health and safety\nof their patrons. In 2015, CDPH deployed a machine learning model to schedule\ninspections of establishments based on their likelihood to commit critical food\ncode violations. The City of Chicago released the training data and source code\nfor the model, allowing anyone to examine the model. We provide the first\nindependent analysis of the model, the data, the predictor variables, the\nperformance metrics, and the underlying assumptions. We present a summary of\nour findings, share lessons learned, and make recommendations to address some\nof the issues our analysis unearthed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:15:37 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kannan", "Vinesh", ""], ["Shapiro", "Matthew A.", ""], ["Bilgic", "Mustafa", ""]]}, {"id": "1910.04994", "submitter": "Subhradev Sen", "authors": "Suman K. Ghosh, Subhradev Sen", "title": "Modeling pages left blank in university examination: A resolution in\n  higher education process", "comments": "16 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trees are the main sources of paper production, in most of the cases, as far\nas the intellectual usages are concerned. However, our planet is lacking in\nthat particular natural resource due to rapid growth of population,\nurbanization, and increased pollution, more importantly non-judicial\nutilization of such kind. Indian education sectors (schools, colleges,\nuniversities) utilize a major part in consumption of papers as a classical\npractice for conducting examinations and other documentation activities. Our\nattempt in this article is to investigate and provide an optimal estimate of\nthe number of pages actually required in answer booklet in higher education\nsector. Truncated Poisson distribution is found to be the best fit for the data\non number of pages left blank in an answer booklet after conduction of semester\nend examinations. To predict the outcome based on various factors such as,\nlines per pages, words per line, types of examinations etc. suitable regression\nmodeling is performed. A real data set, collected over a period of one month,\nis been analyzed to illustrate the methods and conclusion is accomplished in\nthe direction of cost reduction, saving of papers, and in turn, logical uses of\nnatural resource to protect environmental interests.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:50:48 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Ghosh", "Suman K.", ""], ["Sen", "Subhradev", ""]]}, {"id": "1910.05077", "submitter": "Peter Klimek", "authors": "Peter Klimek, Michael Gyimesi, Herwig Ostermann, Stefan Thurner", "title": "A parameter-free population-dynamical approach to health workforce\n  supply forecasting of EU countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many countries face challenges like impending retirement waves, negative\npopulation growth, or a suboptimal distribution of resources across medical\nsectors and fields in supplying their healthcare systems with adequate\nstaffing. An increasing number of countries therefore employs quantitative\napproaches in health workforce supply forecasting. However, these models are\noften of limited usability as they either require extensive individual-level\ndata or become too simplistic to capture key demographic or epidemiological\nfactors. We propose a novel population-dynamical and stock-flow-consistent\napproach to health workforce supply forecasting complex enough to address\ndynamically changing behaviors while requiring only publicly available\ntimeseries data for complete calibration. We apply the model to 21 European\ncountries to forecast the supply of generalist and specialist physicians until\n2040. Compared to staffing levels required to keep the physician density\nconstant at 2016 levels, in many countries we find a significant trend toward\ndecreasing density for generalist physicians at the expense of increasing\ndensities for specialists. These trends are exacerbated in many Southern and\nEastern European countries by expectations of negative population growth. For\nthe example of Austria we generalize our approach to a multi-professional,\nmulti-regional and multi-sectoral model and find a suboptimal distribution in\nthe supply of contracted versus non-contracted physicians. It is of the utmost\nimportance to devise tools for decision makers to influence the allocation and\nsupply of physicians across fields and sectors to combat imbalances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:02:40 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Klimek", "Peter", ""], ["Gyimesi", "Michael", ""], ["Ostermann", "Herwig", ""], ["Thurner", "Stefan", ""]]}, {"id": "1910.05101", "submitter": "Thordis Thorarinsdottir", "authors": "Nina Schuhen, Thordis Thorarinsdottir, Alex Lenkoski", "title": "Rapid adjustment and post-processing of temperature forecast\n  trajectories", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3718", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern weather forecasts are commonly issued as consistent multi-day forecast\ntrajectories with a time resolution of 1-3 hours. Prior to issuing, statistical\npost-processing is routinely used to correct systematic errors and\nmisrepresentations of the forecast uncertainty. However, once the forecast has\nbeen issued, it is rarely updated before it is replaced in the next forecast\ncycle of the numerical weather prediction (NWP) model. This paper shows that\nthe error correlation structure within the forecast trajectory can be utilized\nto substantially improve the forecast between the NWP forecast cycles by\napplying additional post-processing steps each time new observations become\navailable. The proposed rapid adjustment is applied to temperature forecast\ntrajectories from the UK Met Office's convective-scale ensemble MOGREPS-UK.\nMOGREPS-UK is run four times daily and produces hourly forecasts for up to 36\nhours ahead. Our results indicate that the rapidly adjusted forecast from the\nprevious NWP forecast cycle outperforms the new forecast for the first few\nhours of the next cycle, or until the new forecast itself can be rapidly\nadjusted, suggesting a new strategy for updating the forecast cycle.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:11:50 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Schuhen", "Nina", ""], ["Thorarinsdottir", "Thordis", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1910.05121", "submitter": "Annette Kopp-Schneider", "authors": "Manuel Wiesenfarth, Annika Reinke, Bennett A. Landman, Manuel Jorge\n  Cardoso, Lena Maier-Hein, Annette Kopp-Schneider", "title": "Methods and open-source toolkit for analyzing and visualizing challenge\n  results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical challenges have become the de facto standard for benchmarking\nbiomedical image analysis algorithms. While the number of challenges is\nsteadily increasing, surprisingly little effort has been invested in ensuring\nhigh quality design, execution and reporting for these international\ncompetitions. Specifically, results analysis and visualization in the event of\nuncertainties have been given almost no attention in the literature. Given\nthese shortcomings, the contribution of this paper is two-fold: (1) We present\na set of methods to comprehensively analyze and visualize the results of\nsingle-task and multi-task challenges and apply them to a number of simulated\nand real-life challenges to demonstrate their specific strengths and\nweaknesses; (2) We release the open-source framework challengeR as part of this\nwork to enable fast and wide adoption of the methodology proposed in this\npaper. Our approach offers an intuitive way to gain important insights into the\nrelative and absolute performance of algorithms, which cannot be revealed by\ncommonly applied visualization techniques. This is demonstrated by the\nexperiments performed within this work. Our framework could thus become an\nimportant tool for analyzing and visualizing challenge results in the field of\nbiomedical image analysis and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:33:53 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 13:51:36 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Wiesenfarth", "Manuel", ""], ["Reinke", "Annika", ""], ["Landman", "Bennett A.", ""], ["Cardoso", "Manuel Jorge", ""], ["Maier-Hein", "Lena", ""], ["Kopp-Schneider", "Annette", ""]]}, {"id": "1910.05125", "submitter": "Bruce Cox", "authors": "Zachary T. Hornberger, Bruce A. Cox, and Raymond R. Hill", "title": "Effects of Aggregation Methodology on Uncertain Spatiotemporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large spatiotemporal demand datasets can prove intractable for location\noptimization problems, motivating the need to aggregate such data. However,\ndemand aggregation introduces error which impacts the results of the location\nstudy. We introduce and apply a framework for comparing both deterministic and\nstochastic aggregation methods using distance-based and volume-based\naggregation error metrics. In addition we introduce and apply weighted versions\nof these metrics to account for the reality that demand events are\nnon-homogeneous. These metrics are applied to a large, highly variable,\nspatiotemporal demand dataset of search and rescue events in the Pacific ocean.\nComparisons with these metrics between six quadrat aggregations of varying\nscales and two zonal distribution models using hierarchical clustering is\nconducted. We show that as quadrat fidelity increases the distance-based\naggregation error decreases, while the two deliberate zonal approaches further\nreduce this error while utilizing fewer zones. However, the higher fidelity\naggregations have a detrimental effect on volume error. In addition, by\nsplitting the search and rescue dataset into a training and test set we show\nthat stochastic aggregation of this highly variable spatiotemporal demand\nappears to be effective at simulating actual future demands.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 18:58:24 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Hornberger", "Zachary T.", ""], ["Cox", "Bruce A.", ""], ["Hill", "Raymond R.", ""]]}, {"id": "1910.05212", "submitter": "Jan Povala", "authors": "Jan Povala, Seppo Virtanen, Mark Girolami", "title": "Burglary in London: Insights from Statistical Heterogeneous Spatial\n  Point Processes", "comments": "Accepted at the Journal of the Royal Statistical Society Series C:\n  Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain operational insights regarding the crime of burglary in London we\nconsider the estimation of effects of covariates on the intensity of spatial\npoint patterns. By taking into account localised properties of criminal\nbehaviour, we propose a spatial extension to model-based clustering methods\nfrom the mixture modelling literature. The proposed Bayesian model is a finite\nmixture of Poisson generalised linear models such that each location is\nprobabilistically assigned to one of the clusters. Each cluster is\ncharacterised by the regression coefficients which we subsequently use to\ninterpret the localised effects of the covariates. Using a blocking structure\nof the study region, our approach allows specifying spatial dependence between\nnearby locations. We estimate the proposed model using Markov Chain Monte Carlo\nmethods and provide a Python implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:27:52 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:04:36 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 08:12:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Povala", "Jan", ""], ["Virtanen", "Seppo", ""], ["Girolami", "Mark", ""]]}, {"id": "1910.05219", "submitter": "Torsten Heinrich", "authors": "Jangho Yang and Torsten Heinrich and Julian Winkler and Fran\\c{c}ois\n  Lafond and Pantelis Koutroumpis and J. Doyne Farmer", "title": "Measuring productivity dispersion: a parametric approach using the\n  L\\'{e}vy alpha-stable distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Productivity levels and growth are extremely heterogeneous among firms. A\nvast literature has developed to explain the origins of productivity shocks,\ntheir dispersion, evolution and their relationship to the business cycle. We\nexamine in detail the distribution of labor productivity levels and growth, and\nobserve that they exhibit heavy tails. We propose to model these distributions\nusing the four parameter L\\'{e}vy stable distribution, a natural candidate\nderiving from the generalised Central Limit Theorem. We show that it is a\nbetter fit than several standard alternatives, and is remarkably consistent\nover time, countries and sectors. In all samples considered, the tail parameter\nis such that the theoretical variance of the distribution is infinite, so that\nthe sample standard deviation increases with sample size. We find a consistent\npositive skewness, a markedly different behaviour between the left and right\ntails, and a positive relationship between productivity and size. The\ndistributional approach allows us to test different measures of dispersion and\nfind that productivity dispersion has slightly decreased over the past decade.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:41:02 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Yang", "Jangho", ""], ["Heinrich", "Torsten", ""], ["Winkler", "Julian", ""], ["Lafond", "Fran\u00e7ois", ""], ["Koutroumpis", "Pantelis", ""], ["Farmer", "J. Doyne", ""]]}, {"id": "1910.05240", "submitter": "Madeline Ausdemore", "authors": "Cedric Neumann, Madeline A. Ausdemore", "title": "Defence Against the Modern Arts: the Curse of Statistics \"Score-based\n  likelihood ratios\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several decades, legal and scientific scholars have argued that\nconclusions from forensic examinations should be supported by statistical data\nand reported within a probabilistic framework. Multiple models have been\nproposed to quantify the probative value of forensic evidence. Unfortunately,\nseveral of these models rely on ad-hoc strategies that are not scientifically\nsound. The opacity of the technical jargon used to present these models and\ntheir results, and the complexity of the techniques involved make it very\ndifficult for the untrained user to separate the wheat from the chaff. This\nseries of papers is intended to help forensic scientists and lawyers recognise\nlimitations and issues in tools proposed to interpret the results of forensic\nexaminations. This paper focuses on tools that have been proposed to leverage\nthe use of similarity scores to assess the probative value of forensic\nfindings. We call this family of tools \"score-based likelihood ratios\". In this\npaper, we present the fundamental concepts on which these tools are built, we\ndescribe some specific members of this family of tools, and we explore their\nconvergence to the Bayes factor through an intuitive geometrical approach and\nthrough simulations. Finally, we discuss their validation and their potential\nusefulness as a decision-making tool in forensic science.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:18:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Neumann", "Cedric", ""], ["Ausdemore", "Madeline A.", ""]]}, {"id": "1910.05246", "submitter": "Barbara Pascal", "authors": "Barbara Pascal and Nelly Pustelnik and Patrice Abry", "title": "Strongly Convex Optimization for Joint Fractal Feature Estimation and\n  Texture Segmentation", "comments": "To appear in 2021 in Applied and Computational Harmonic Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work investigates the segmentation of textures by formulating it\nas a strongly convex optimization problem, aiming to favor piecewise constancy\nof fractal features (local variance and local regularity) widely used to model\nreal-world textures in numerous applications very different in nature. Two\nobjective functions combining these two features are compared, referred to as\njoint and coupled, promoting either independent or co-localized changes in\nlocal variance and regularity. To solve the resulting convex nonsmooth\noptimization problems, because the processing of large size images and\ndatabases are targeted, two categories of proximal algorithms (dual\nforward-backward and primal-dual), are devised and compared. An in-depth study\nof the objective functions, notably of their strong convexity, memory and\ncomputational costs, permits to propose significantly accelerated algorithms. A\nclass of synthetic models of piecewise fractal texture is constructed and\nstudied. They enable, by means of large-scale Monte-Carlo simulations, to\nquantify the benefits in texture segmentation of combining local regularity and\nlocal variance (as opposed to regularity only) while using strong-convexity\naccelerated primal-dual algorithms. Achieved results also permit to discuss the\ngains/costs in imposing co-localizations of changes in local regularity and\nlocal variance in the problem formulation. Finally, the potential of the\nproposed approaches is illustrated on real-world textures taken from a publicly\navailable and documented database.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:26:25 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 11:51:44 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 11:38:30 GMT"}, {"version": "v4", "created": "Sat, 3 Apr 2021 17:39:43 GMT"}, {"version": "v5", "created": "Sat, 10 Apr 2021 07:42:16 GMT"}, {"version": "v6", "created": "Fri, 16 Apr 2021 14:16:47 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Pascal", "Barbara", ""], ["Pustelnik", "Nelly", ""], ["Abry", "Patrice", ""]]}, {"id": "1910.05355", "submitter": "Federico Ferrari", "authors": "Federico Camerlenghi, Bianca Dumitrascu, Federico Ferrari, Barbara E.\n  Engelhardt and Stefano Favaro", "title": "Nonparametric Bayesian multi-armed bandits for single cell experiment\n  design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing cell type discovery under budget constraints is a\nfundamental challenge for the collection and analysis of single-cell\nRNA-sequencing (scRNA-seq) data. In this paper, we introduce a simple,\ncomputationally efficient, and scalable Bayesian nonparametric sequential\napproach to optimize the budget allocation when designing a large scale\nexperiment for the collection of scRNA-seq data for the purpose of, but not\nlimited to, creating cell atlases. Our approach relies on the following tools:\ni) a hierarchical Pitman-Yor prior that recapitulates biological assumptions\nregarding cellular differentiation, and ii) a Thompson sampling multi-armed\nbandit strategy that balances exploitation and exploration to prioritize\nexperiments across a sequence of trials. Posterior inference is performed by\nusing a sequential Monte Carlo approach, which allows us to fully exploit the\nsequential nature of our species sampling problem. We empirically show that our\napproach outperforms state-of-the-art methods and achieves near-Oracle\nperformance on simulated and scRNA-seq data alike. HPY-TS code is available at\nhttps://github.com/fedfer/HPYsinglecell.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:03:56 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 14:05:04 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Camerlenghi", "Federico", ""], ["Dumitrascu", "Bianca", ""], ["Ferrari", "Federico", ""], ["Engelhardt", "Barbara E.", ""], ["Favaro", "Stefano", ""]]}, {"id": "1910.05494", "submitter": "Sepideh Mosaferi", "authors": "Sepideh Mosaferi", "title": "Spatio-Temporal Mixed Models to Predict Coverage Error Rates at Local\n  Areas", "comments": "10 pages, 3 Figures, Presented as a poster in the Joint Statistical\n  Meeting 2017, July 29--August 3, 2017, Baltimore, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of the great efforts during the censuses, occurrence of some\nnonsampling errors such as coverage error is inevitable. Coverage error which\ncan be classified into two types of under-count and overcount occurs when there\nis no unique bijective (one-to-one) mapping between the individuals from the\ncensus count and the target population -- individuals who usually reside in the\ncountry (de jure residences). There are variety of reasons make the coverage\nerror happens including deficiencies in the census maps, errors in the field\noperations or disinclination of people for participation in the undercount\nsituation and multiple enumeration of individuals or those who do not belong to\nthe scope of the census in the overcount situation. A routine practice for\nestimating the net coverage error is subtracting the census count from the\nestimated true population, which obtained from a dual system (or\ncapture-recapture) technique. Estimated coverage error usually suffers from\nsignificant uncertainty of the direct estimate of true population or other\nerrors such as matching error. To rectify the above-mentioned problem and\npredict a more reliable coverage error rate, we propose a set of\nspatio-temporal mixed models. In an illustrative study on the 2010 census\ncoverage error rate of the U.S. counties with population more than 100,000, we\nselect the best mixed model for prediction by deviance information criteria\n(DIC) and conditional predictive ordinate (CPO). Our proposed approach for\npredicting coverage error rate and its measure of uncertainty is a full\nBayesian approach, which leads to a reasonable improvement over the direct\ncoverage error rate in terms of mean squared error (MSE) and confidence\ninterval (CI) as provided by the U.S. Census Bureau.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 05:05:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mosaferi", "Sepideh", ""]]}, {"id": "1910.05600", "submitter": "Youjin Lee", "authors": "Youjin Lee, Trang Q. Nguyen, and Elizabeth A. Stuart", "title": "Partially Pooled Propensity Score Models for Average Treatment Effect\n  Estimation with Multilevel Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference analyses often use existing observational data, which in\nmany cases has some clustering of individuals. In this paper we discuss\npropensity score weighting methods in a multilevel setting where within\nclusters individuals share unmeasured confounders that are related to treatment\nassignment and the potential outcomes. We focus in particular on settings where\nmodels with fixed cluster effects are either not feasible or not useful due to\nthe presence of a large number of small clusters. We found, both through\nnumerical experiments and theoretical derivations, that a strategy of grouping\nclusters with similar treatment prevalence and estimating propensity scores\nwithin such cluster groups is effective in reducing bias from unmeasured\ncluster-level covariates under mild conditions on the outcome model. We apply\nour proposed method in evaluating the effectiveness of center-based pre-school\nprogram participation on children's achievement at kindergarten, using the\nEarly Childhood Longitudinal Study, Kindergarten data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 17:15:18 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 22:05:18 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Lee", "Youjin", ""], ["Nguyen", "Trang Q.", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1910.05676", "submitter": "Zifeng Zhao", "authors": "Peng Shi and Zifeng Zhao", "title": "Regression for Copula-linked Compound Distributions with Applications in\n  Modeling Aggregate Insurance Claims", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In actuarial research, a task of particular interest and importance is to\npredict the loss cost for individual risks so that informative decisions are\nmade in various insurance operations such as underwriting, ratemaking, and\ncapital management. The loss cost is typically viewed to follow a compound\ndistribution where the summation of the severity variables is stopped by the\nfrequency variable. A challenging issue in modeling such outcome is to\naccommodate the potential dependence between the number of claims and the size\nof each individual claim. In this article, we introduce a novel regression\nframework for compound distributions that uses a copula to accommodate the\nassociation between the frequency and the severity variables, and thus allows\nfor arbitrary dependence between the two components. We further show that the\nnew model is very flexible and is easily modified to account for incomplete\ndata due to censoring or truncation. The flexibility of the proposed model is\nillustrated using both simulated and real data sets. In the analysis of\ngranular claims data from property insurance, we find substantive negative\nrelationship between the number and the size of insurance claims. In addition,\nwe demonstrate that ignoring the frequency-severity association could lead to\nbiased decision-making in insurance operations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:37:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shi", "Peng", ""], ["Zhao", "Zifeng", ""]]}, {"id": "1910.05794", "submitter": "Bertie Vidgen Dr", "authors": "Bertie Vidgen, Taha Yasseri, Helen Margetts", "title": "Islamophobes are not all the same! A study of far right actors on\n  Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Far-right actors are often purveyors of Islamophobic hate speech online,\nusing social media to spread divisive and prejudiced messages which can stir up\nintergroup tensions and conflict. Hateful content can inflict harm on targeted\nvictims, create a sense of fear amongst communities and stir up intergroup\ntensions and conflict. Accordingly, there is a pressing need to better\nunderstand at a granular level how Islamophobia manifests online and who\nproduces it. We investigate the dynamics of Islamophobia amongst followers of a\nprominent UK far right political party on Twitter, the British National Party.\nAnalysing a new data set of five million tweets, collected over a period of one\nyear, using a machine learning classifier and latent Markov modelling, we\nidentify seven types of Islamophobic far right actors, capturing qualitative,\nquantitative and temporal differences in their behaviour. Notably, we show that\na small number of users are responsible for most of the Islamophobia that we\nobserve. We then discuss the policy implications of this typology in the\ncontext of social media regulation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:39:00 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 07:18:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Vidgen", "Bertie", ""], ["Yasseri", "Taha", ""], ["Margetts", "Helen", ""]]}, {"id": "1910.05814", "submitter": "Samuel Melton", "authors": "Samuel Melton, Sharad Ramanathan", "title": "Discovering a sparse set of pairwise discriminating features in high\n  dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting an understanding of the underlying system from high dimensional\ndata is a growing problem in science. Discovering informative and meaningful\nfeatures is crucial for clustering, classification, and low dimensional data\nembedding. Here we propose to construct features based on their ability to\ndiscriminate between clusters of the data points. We define a class of problems\nin which linear separability of clusters is hidden in a low dimensional space.\nWe propose an unsupervised method to identify the subset of features that\ndefine a low dimensional subspace in which clustering can be conducted. This is\nachieved by averaging over discriminators trained on an ensemble of proposed\ncluster configurations. We then apply our method to single cell RNA-seq data\nfrom mouse gastrulation, and identify 27 key transcription factors (out of 409\ntotal), 18 of which are known to define cell states through their expression\nlevels. In this inferred subspace, we find clear signatures of known cell types\nthat eluded classification prior to discovery of the correct low dimensional\nsubspace.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 19:19:32 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 16:47:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Melton", "Samuel", ""], ["Ramanathan", "Sharad", ""]]}, {"id": "1910.05846", "submitter": "Emmanuel Afolabi Bakare E. A.", "authors": "Bakare E.A., Are E.B., Abolarin O.E., Osanyinlusi S.A., Ngwu Benitho,\n  and Ubaka Obiaderi N", "title": "Mathematical Modelling and Analysis of Transmission Dynamics of Lassa\n  fever", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, a periodically-forced seasonal non-autonomous system of a\nnon-linear ordinary differential equation is developed that captures the\ndynamics of Lassa fever transmission and seasonal variation in the birth of\nmastomys rodents where time was measured in days to capture seasonality. It was\nshown that the model is epidemiologically meaningful and mathematically\nwell-posed by using the results from the qualitative properties of the solution\nof the model. It was established that in order to eliminate Lassa fever\ndisease, treatments with Ribavirin must be provided early to reduce mortality\nand other preventive measures like an educational campaign, community hygiene,\nIsolation of infected humans, and culling/destruction of rodents must be\napplied to also reduce the morbidity of the disease. Finally, the obtained\nresults gave a primer framework for planning and designing cost-effective\nstrategies for good interventions in eliminating Lassa fever.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:20:12 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["A.", "Bakare E.", ""], ["B.", "Are E.", ""], ["E.", "Abolarin O.", ""], ["A.", "Osanyinlusi S.", ""], ["Benitho", "Ngwu", ""], ["N", "Ubaka Obiaderi", ""]]}, {"id": "1910.05847", "submitter": "Rui Meng", "authors": "Rui Meng, Soper Braden, Jan Nygard, Mari Nygrad, Herbert Lee", "title": "Hierarchical Hidden Markov Jump Processes for Cancer Screening Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov jump processes are an attractive approach for modeling clinical\ndisease progression data because they are explainable and capable of handling\nboth irregularly sampled and noisy data. Most applications in this context\nconsider time-homogeneous models due to their relative computational\nsimplicity. However, the time homogeneous assumption is too strong to\naccurately model the natural history of many diseases. Moreover, the population\nat risk is not homogeneous either, since disease exposure and susceptibility\ncan vary considerably. In this paper, we propose a piece-wise stationary\ntransition matrix to explain the heterogeneity in time. We propose a\nhierarchical structure for the heterogeneity in population, where prior\ninformation is considered to deal with unbalanced data. Moreover, an efficient,\nscalable EM algorithm is proposed for inference. We demonstrate the feasibility\nand superiority of our model on a cervical cancer screening dataset from the\nCancer Registry of Norway. Experiments show that our model outperforms\nstate-of-the-art recurrent neural network models in terms of prediction\naccuracy and significantly outperforms a standard hidden Markov jump process in\ngenerating Kaplan-Meier estimators.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:23:34 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Braden", "Soper", ""], ["Nygard", "Jan", ""], ["Nygrad", "Mari", ""], ["Lee", "Herbert", ""]]}, {"id": "1910.05851", "submitter": "Rui Meng", "authors": "Rui Meng, Braden Soper, Herbert Lee, Vincent X. Liu, John D. Greene,\n  Priyadip Ray", "title": "Nonstationary Multivariate Gaussian Processes for Electronic Health\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multivariate nonstationary Gaussian processes for jointly modeling\nmultiple clinical variables, where the key parameters, length-scales, standard\ndeviations and the correlations between the observed output, are all time\ndependent. We perform posterior inference via Hamiltonian Monte Carlo (HMC). We\nalso provide methods for obtaining computationally efficient gradient-based\nmaximum a posteriori (MAP) estimates. We validate our model on synthetic data\nas well as on electronic health records (EHR) data from Kaiser Permanente (KP).\nWe show that the proposed model provides better predictive performance over a\nstationary model as well as uncovers interesting latent correlation processes\nacross vitals which are potentially predictive of patient risk.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:37:08 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Soper", "Braden", ""], ["Lee", "Herbert", ""], ["Liu", "Vincent X.", ""], ["Greene", "John D.", ""], ["Ray", "Priyadip", ""]]}, {"id": "1910.05944", "submitter": "Georges Kariniotakis", "authors": "Kevin Bellinguer (PERSEE), Robin Girard (PERSEE), Guillaume Bontron,\n  Georges Kariniotakis (PERSEE)", "title": "Short-term photovoltaic generation forecasting using multiple\n  heterogenous sources of data", "comments": null, "journal-ref": "36th European PV Solar Energy Conference and Exhibition (EU\n  PVSEC), WIP Renewable Energies, Sep 2019, Marseille, France", "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable Energies (RES) penetration is progressing rapidly: in France, the\ninstalled capacity of photovoltaic (PV) power rose from 26MW in 2007 to 8GW in\n2017 [1]. Power generated by PV plants being highly dependent on variable\nweather conditions, this ever-growing pace is raising issues regarding grid\nstability and revenue optimization. To overcome these obstacles, PV forecasting\nbecame an area of intense research. In this paper, we propose a low complexity\nforecasting model able to operate with multiple heterogenous sources of data\n(power measurements, satellite images and Numerical Weather Predictions (NWP)).\nBeing non-parametric, this model can be extended to include inputs. The main\nstrength of the proposed model lies in its ability to automatically select the\noptimal sources of data according to the desired forecast horizon (from 15min\nto 6h ahead) thanks to a feature selection procedure. To take advantage of the\ngrowing number of PV plants, a Spatio-Temporal (ST) approach is implemented.\nThis approach considers the dependencies between spatially distributed plants.\nEach source has been studied incrementally so as to quantify their impact on\nforecast performances. This plurality of sources enhances the forecasting\nperformances up to 40% in terms of RMSE compared to a reference model. The\nevaluation process is carried out on nine PV plants from the Compagnie\nNationale du Rh{\\^o}ne (CNR).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:00:16 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Bellinguer", "Kevin", "", "PERSEE"], ["Girard", "Robin", "", "PERSEE"], ["Bontron", "Guillaume", "", "PERSEE"], ["Kariniotakis", "Georges", "", "PERSEE"]]}, {"id": "1910.06106", "submitter": "Elias Tuomaala", "authors": "Elias Tuomaala", "title": "The Bayesian Synthetic Control: Improved Counterfactual Estimation in\n  the Social Sciences through Probabilistic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists often study how a policy reform impacted a single targeted\ncountry. Increasingly, this is done with the synthetic control method (SCM).\nSCM models the country's counterfactual (non-reform or untreated) trajectory as\na weighted average of other countries' outcomes. The method struggles to\nquantify uncertainty; eg. it cannot produce confidence intervals. It is also\nsuspect to overfit. We propose an alternative method, the Bayesian synthetic\ncontrol (BSC), which lacks these flaws. Using MCMC sampling, we implement the\nmethod for two previously studied datasets. The proposed method outperforms SCM\nin a simple test of predictive accuracy and casts some doubt on significance of\nprior findings. The studied reforms are the German reunification of 1990 and\nthe California tobacco legislation of 1988. BSC borrows its causal model, the\nlinear latent factor model, from the SCM literature. Unlike SCM, BSC estimates\nthe latent factors explicitly through a dimensionality reduction. All\nuncertainty is captured in the posterior distribution so that, unlike for SCM,\ncredible intervals are easily derived. Further, BSC's reliability on the target\npanel dataset can be assessed through a posterior predictive check; SCM and its\nfrequentist derivatives use up the required information while testing\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:42:44 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Tuomaala", "Elias", ""]]}, {"id": "1910.06137", "submitter": "Marco Helbich", "authors": "Amit Birenboim, Martin Dijst, Floortje Scheepers, Maartje Poelman,\n  Marco Helbich", "title": "Wearables and location tracking technologies for mental-state sensing in\n  outdoor environments", "comments": null, "journal-ref": null, "doi": "10.1080/00330124.2018.1547978", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in commercial wearable devices are increasingly facilitating the\ncollection and analysis of everyday physiological data. This paper discusses\nthe theoretical and practical aspects of using such ambulatory devices for the\ndetection of episodic changes in physiological signals as a marker for mental\nstate in outdoor environments. A pilot study was conducted to evaluate the\nfeasibility of utilizing commercial wearables in combination with location\ntracking technologies. The study measured physiological signals for 15\nparticipants, including heart rate, heart-rate variability, and skin\nconductance. Participants' signals were recorded during an outdoor walk that\nwas tracked using a GPS logger. The walk was designed to pass through various\ntypes of environments including green, blue, and urban spaces as well as a more\nstressful road crossing. The data that was obtained was used to demonstrate how\nbiosensors information can be contextualized and enriched using location\ninformation. Significant episodic changes in physiological signals under\nreal-world conditions were detectable in the stressful road crossing, but not\nin the other types of environments. The article concludes that despite\nchallenges and limitations of current off-the-shelf wearables, the utilization\nof these devices offers novel opportunities for evaluating episodic changes in\nphysiological signals as a marker for mental state during everyday activities\nincluding in outdoor environments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:38:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Birenboim", "Amit", ""], ["Dijst", "Martin", ""], ["Scheepers", "Floortje", ""], ["Poelman", "Maartje", ""], ["Helbich", "Marco", ""]]}, {"id": "1910.06155", "submitter": "Michel Fornaciali", "authors": "Ligia Vizeu Barrozo, Michel Fornaciali, Carmen Diva Saldiva de\n  Andr\\'e, Guilherme Augusto Zimeo Morais, Giselle Mansur, William\n  Cabral-Miranda, Jo\\~ao Ricardo Sato, Edson Amaro J\\'unior", "title": "GeoSES -- um \\'Indice Socioecon\\^omico para Estudos de Sa\\'ude no Brasil", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: to define an index that summarizes the main dimensions of the\nsocioeconomic context for research purposes, evaluation and monitoring health\ninequalities. Methods: the index was created from the 2010 Brazilian\nDemographic Census, whose variables selection was guided by theoretical\nreferences for health studies, including seven socioeconomic dimensions:\neducation, mobility, poverty, wealth, income, segregation and deprivation of\nresources and services. The index was developed using principal component\nanalysis, and was evaluated for its construct, content and applicability\ncomponents. Results: GeoSES-BR dimensions showed good association with HDI-M\n(above 0.85). The model with the poverty dimension best explained the relative\nrisk of avoidable cause mortality in Brazil. In the intraurban scale, the model\nwith GeoSES-IM was the one that best explained the relative risk of mortality\nfrom circulatory system diseases. Conclusion: GeoSES showed significant\nexplanatory potential in the studied scales.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:49:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Barrozo", "Ligia Vizeu", ""], ["Fornaciali", "Michel", ""], ["de Andr\u00e9", "Carmen Diva Saldiva", ""], ["Morais", "Guilherme Augusto Zimeo", ""], ["Mansur", "Giselle", ""], ["Cabral-Miranda", "William", ""], ["Sato", "Jo\u00e3o Ricardo", ""], ["J\u00fanior", "Edson Amaro", ""]]}, {"id": "1910.06381", "submitter": "Jason Anastasopoulos", "authors": "L. Jason Anastasopoulos", "title": "Principled estimation of regression discontinuity designs", "comments": "First presented on August 30th, 2018 at the American Political\n  Science Association annual conference in Boston, Massachusetts", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity designs are frequently used to estimate the causal\neffect of election outcomes and policy interventions. In these contexts,\ntreatment effects are typically estimated with covariates included to improve\nefficiency. While including covariates improves precision asymptotically, in\npractice, treatment effects are estimated with a small number of observations,\nresulting in considerable fluctuations in treatment effect magnitude and\nprecision depending upon the covariates chosen. This practice thus incentivizes\nresearchers to select covariates which maximize treatment effect statistical\nsignificance rather than precision. Here, I propose a principled approach for\nestimating RDDs which provides a means of improving precision with covariates\nwhile minimizing adverse incentives. This is accomplished by integrating the\nadaptive LASSO, a machine learning method, into RDD estimation using an R\npackage developed for this purpose, adaptiveRDD. Using simulations, I show that\nthis method significantly improves treatment effect precision, particularly\nwhen estimating treatment effects with fewer than 200 observations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 03:18:08 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Anastasopoulos", "L. Jason", ""]]}, {"id": "1910.06449", "submitter": "David Cheng", "authors": "David Cheng, Rajeev Ayyagari, James Signorovitch", "title": "The Statistical Performance of Matching-Adjusted Indirect Comparisons", "comments": "36 pages, 3 figures; Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect comparisons of treatment-specific outcomes across separate studies\noften inform decision-making in the absence of head-to-head randomized\ncomparisons. Differences in baseline characteristics between study populations\nmay introduce confounding bias in such comparisons. Matching-adjusted indirect\ncomparison (MAIC) (Signorovitch et al., 2010) has been used to adjust for\ndifferences in observed baseline covariates when the individual patient-level\ndata (IPD) are available for only one study and aggregate data (AGD) are\navailable for the other study. The approach weights outcomes from the IPD using\nestimates of trial selection odds that balance baseline covariates between the\nIPD and AGD. With the increasing use of MAIC, there is a need for formal\nassessments of its statistical properties. In this paper we formulate\nidentification assumptions for causal estimands that justify MAIC estimators.\nWe then examine large sample properties and evaluate strategies for estimating\nstandard errors without the full IPD from both studies. The finite-sample bias\nof MAIC and the performance of confidence intervals based on different standard\nerror estimators are evaluated through simulations. The method is illustrated\nthrough an example comparing placebo arm and natural history outcomes in\nDuchenne muscular dystrophy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:14:46 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:47:08 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Cheng", "David", ""], ["Ayyagari", "Rajeev", ""], ["Signorovitch", "James", ""]]}, {"id": "1910.06497", "submitter": "Lata Kodali", "authors": "Lata Kodali, Srijan Sengupta, Leanna House, William H. Woodall", "title": "The Value of Summary Statistics for Anomaly Detection in\n  Temporally-Evolving Networks: A Performance Evaluation Study", "comments": "47 pages, 9 main figures, 17 main tables, 12 figures and tables in\n  appendix", "journal-ref": "Applied Stochastic Models in Business and Industry 36.6 (2020):\n  980-1013", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data has emerged as an active research area in statistics. Much of\nthe focus of ongoing research has been on static networks that represent a\nsingle snapshot or aggregated historical data unchanging over time. However,\nmost networks result from temporally-evolving systems that exhibit intrinsic\ndynamic behavior. Monitoring such temporally-varying networks to detect\nanomalous changes has applications in both social and physical sciences. In\nthis work, we perform an evaluation study of summary statistics for anomaly\ndetection in temporally-evolving networks by incorporating principles from\nstatistical process monitoring. In contrast to previous studies, we\ndeliberately incorporate temporal auto-correlation in our study. Other\nconsiderations in our comprehensive assessment include types and duration of\nanomaly, model type, and sparsity in temporally-evolving networks. We conclude\nthat summary statistics can be valuable tools for network monitoring and often\nperform better than more complicated statistics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:57:30 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 22:21:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kodali", "Lata", ""], ["Sengupta", "Srijan", ""], ["House", "Leanna", ""], ["Woodall", "William H.", ""]]}, {"id": "1910.06503", "submitter": "Qiang Xingzi", "authors": "Xingzi Qiang, Yanbo Zhu, Rui Xue", "title": "SVRPF: An Improved Particle Filter for a Nonlinear/non-Gaussian\n  Environment", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2947540", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a particle filter (PF) in nonlinear and non-Gaussian\nenvironments is often affected by particle degeneracy and impoverishment\nproblems. In this paper, these two problems are re-assessed using the concepts\nof importance region (IR) selection and particle density (PD), where IR\ndescribes the distribution region of particles, and PD describes the density of\nparticles in IR. Based on these two factors, a support vector regression PF\n(SVRPF) is proposed to overcome the problems from nonlinear and non-Gaussian\nenvironments, especially in regard to narrow observation noise. Furthermore,\nthe consistency of the SVRPF and Bayes' filtering is demonstrated. A numerical\nsimulation shows that the performance of the SVRPF is more stable than other\nfilter algorithms. Provided that other conditions are the same, when the\nobservation noise variance is 0.1 and 5, the root-mean-square errors of the\nSVRPF decrease by 0.5 and 0.03, respectively, compared with that of a general\nPF.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:15:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Qiang", "Xingzi", ""], ["Zhu", "Yanbo", ""], ["Xue", "Rui", ""]]}, {"id": "1910.06512", "submitter": "John Page", "authors": "John Paige, Geir-Arne Fuglstad, Andrea Riebler, Jon Wakefield", "title": "Design- and Model-Based Approaches to Small-Area Estimation in a Low and\n  Middle Income Country Context: Comparisons and Recommendations", "comments": "Main text: 35 pages, 5 figures, 2 tables. Supplementary materials: 63\n  pages, 5 figures, 21 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for rigorous and timely health and demographic summaries has\nprovided the impetus for an explosion in geographic studies, with a common\napproach being the production of pixel-level maps, particularly in low and\nmiddle income countries. In this context, household surveys are a major source\nof data, usually with a two-stage cluster design with stratification by region\nand urbanicity. Accurate estimates are of crucial interest for precision public\nhealth policy interventions, but many current studies take a cavalier approach\nto acknowledging the sampling design, while presenting results at a fine\ngeographic scale. In this paper we investigate the extent to which accounting\nfor sample design can affect predictions at the aggregate level, which is\nusually the target of inference. We describe a simulation study in which\nrealistic sampling frames are created for Kenya, based on population and\ndemographic information, with a survey design that mimics a Demographic Health\nSurvey (DHS). We compare the predictive performance of various commonly-used\nmodels. We also describe a cluster level model with a discrete spatial\nsmoothing prior that has not been previously used, but provides reliable\ninference. We find that including stratification and cluster level random\neffects can improve predictive performance. Spatially smoothed direct\n(weighted) estimates were robust to priors and survey design. Continuous\nspatial models performed well in the presence of fine scale variation; however,\nthese models require the most \"hand holding\". Subsequently, we examine how the\nmodels perform on real data; specifically we model the prevalence of secondary\neducation for women aged 20-29 using data from the 2014 Kenya DHS.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:48:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Paige", "John", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Wakefield", "Jon", ""]]}, {"id": "1910.06521", "submitter": "Chelsea Sidrane", "authors": "Chelsea Sidrane, Dylan J Fitzpatrick, Andrew Annex, Diane O'Donoghue,\n  Yarin Gal, Piotr Bili\\'nski", "title": "Machine Learning for Generalizable Prediction of Flood Susceptibility", "comments": "Will be presented at hadri.ai 2019, a workshop at NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flooding is a destructive and dangerous hazard and climate change appears to\nbe increasing the frequency of catastrophic flooding events around the world.\nPhysics-based flood models are costly to calibrate and are rarely generalizable\nacross different river basins, as model outputs are sensitive to site-specific\nparameters and human-regulated infrastructure. In contrast, statistical models\nimplicitly account for such factors through the data on which they are trained.\nSuch models trained primarily from remotely-sensed Earth observation data could\nreduce the need for extensive in-situ measurements. In this work, we develop\ngeneralizable, multi-basin models of river flooding susceptibility using\ngeographically-distributed data from the USGS stream gauge network. Machine\nlearning models are trained in a supervised framework to predict two measures\nof flood susceptibility from a mix of river basin attributes, impervious\nsurface cover information derived from satellite imagery, and historical\nrecords of rainfall and stream height. We report prediction performance of\nmultiple models using precision-recall curves, and compare with performance of\nnaive baselines. This work on multi-basin flood prediction represents a step in\nthe direction of making flood prediction accessible to all at-risk communities.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 04:31:39 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Sidrane", "Chelsea", ""], ["Fitzpatrick", "Dylan J", ""], ["Annex", "Andrew", ""], ["O'Donoghue", "Diane", ""], ["Gal", "Yarin", ""], ["Bili\u0144ski", "Piotr", ""]]}, {"id": "1910.06582", "submitter": "Roberto Galeazzi", "authors": "Pegah Barkhordari, Roberto Galeazzi, Alejandro de Miguel Tejada, Ilmar\n  F. Santos", "title": "Identification of Behavioural Models for Railway Turnouts Monitoring", "comments": "26 pages, 15 figures, submitted for evaluation to Elsevier journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces a low-complexity behavioural model to describe the\ndynamic response of railway turnouts due to the ballast and railpad components.\nThe behavioural model should serve as the basis for the future development of a\nsupervisory system for the continuous monitoring of turnouts. A fourth order\nlinear model is proposed based on spectral analysis of measured rail vertical\naccelerations gathered during a receptance test and it is then identified at\nseveral sections of the turnout applying the Eigensystem Realization Algorithm.\nThe predictviness and robustness of the behavioural models have been assessed\non a large data set of train passages differing for train type, speed and\nloading condition. Last, the need for a novel modeling method is argued in\nrelation to high-fidelity mechanistic models widely used in the railway\nengineering community.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:12:40 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Barkhordari", "Pegah", ""], ["Galeazzi", "Roberto", ""], ["Tejada", "Alejandro de Miguel", ""], ["Santos", "Ilmar F.", ""]]}, {"id": "1910.06596", "submitter": "Alex Lenkoski", "authors": "Alex Lenkoski and Fredrik Lohne Aanes", "title": "Sovereign Risk Indices and Bayesian Theory Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In economic applications, model averaging has found principal use examining\nthe validity of various theories related to observed heterogeneity in outcomes\nsuch as growth, development, and trade.Though often easy to articulate, these\ntheories are imperfectly captured quantitatively. A number of different proxies\nare often collected for a given theory and the uneven nature of this collection\nrequires care when employing model averaging. Furthermore, if valid, these\ntheories ought to be relevant outside of any single narrowly focused outcome\nequation. We propose a methodology which treats theories as represented by\nlatent indices, these latent processes controlled by model averaging on the\nproxy level. To achieve generalizability of the theory index our framework\nassumes a collection of outcome equations. We accommodate a flexible set of\ngeneralized additive models, enabling non-Gaussian outcomes to be included.\nFurthermore, selection of relevant theories also occurs on the outcome level,\nallowing for theories to be differentially valid. Our focus is on creating a\nset of theory-based indices directed at understanding a country's potential\nrisk of macroeconomic collapse. These Sovereign Risk Indices are calibrated\nacross a set of different \"collapse\" criteria, including default on sovereign\ndebt, heightened potential for high unemployment or inflation and dramatic\nswings in foreign exchange values. The goal of this exercise is to render a\nportable set of country/year theory indices which can find more general use in\nthe research community.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:52:36 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lenkoski", "Alex", ""], ["Aanes", "Fredrik Lohne", ""]]}, {"id": "1910.06640", "submitter": "Carlos Ruiz", "authors": "Andr\\'es M. Alonso, F. Javier Nogales and Carlos Ruiz", "title": "A Single Scalable LSTM Model for Short-Term Forecasting of Disaggregated\n  Electricity Loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most electricity systems worldwide are deploying advanced metering\ninfrastructures to collect relevant operational data. In particular, smart\nmeters allow tracking electricity load consumption at a very disaggregated\nlevel and at high frequency rates. This data opens the possibility of\ndeveloping new forecasting models with a potential positive impact in\nelectricity systems. We present a general methodology that is able to process\nand forecast a large number of smart meter time series. Instead of using\ntraditional and univariate approaches, we develop a single but complex\nrecurrent neural-network model with long short-term memory that can capture\nindividual consumption patterns and also consumptions from different\nhouseholds. The resulting model can accurately predict future loads\n(short-term) of individual consumers, even if these were not included in the\noriginal training set. This entails a great potential for large scale\napplications as once the single network is trained, accurate individual\nforecast for new consumers can be obtained at almost no computational cost. The\nproposed model is tested under a large set of numerical experiments by using a\nreal-world dataset with thousands of disaggregated electricity consumption time\nseries. Furthermore, we explore how geo-demographic segmentation of consumers\nmay impact the forecasting accuracy of the model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 10:33:34 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:42:47 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Alonso", "Andr\u00e9s M.", ""], ["Nogales", "F. Javier", ""], ["Ruiz", "Carlos", ""]]}, {"id": "1910.06870", "submitter": "Guanyu Hu", "authors": "Guanyu Hu, Fred Huffer, Ming-Hui Chen", "title": "New Development of Bayesian Variable Selection Criteria for Spatial\n  Point Process with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selecting important spatial-dependent variables under the nonhomogeneous\nspatial Poisson process model is an important topic of great current interest.\nIn this paper, we use the Deviance Information Criterion (DIC) and Logarithm of\nthe Pseudo Marginal Likelihood (LPML) for Bayesian variable selection under the\nnonhomogeneous spatial Poisson process model. We further derive the new Monte\nCarlo estimation formula for LPML in the spatial Poisson process setting.\nExtensive simulation studies are carried out to evaluate the empirical\nperformance of the proposed criteria. The proposed methodology is further\napplied to the analysis of two large data sets, the Earthquake Hazards Program\nof United States Geological Survey (USGS) earthquake data and the Forest of\nBarro Colorado Island (BCI) data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:30:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Hu", "Guanyu", ""], ["Huffer", "Fred", ""], ["Chen", "Ming-Hui", ""]]}, {"id": "1910.06897", "submitter": "Philip White", "authors": "Philip A. White and Alan E. Gelfand", "title": "Generalized Evolutionary Point Processes: Model Specifications and Model\n  Comparison", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability (2020+)", "doi": "10.1007/s11009-020-09797-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized evolutionary point processes offer a class of point process\nmodels that allows for either excitation or inhibition based upon the history\nof the process. In this regard, we propose modeling which comprises\ngeneralization of the nonlinear Hawkes process. Working within a Bayesian\nframework, model fitting is implemented through Markov chain Monte Carlo. This\nentails discussion of computation of the likelihood for such point patterns.\nFurthermore, for this class of models, we discuss strategies for model\ncomparison. Using simulation, we illustrate how well we can distinguish these\nmodels from point pattern specifications with conditionally independent event\ntimes, e.g., Poisson processes. Specifically, we demonstrate that these models\ncan correctly identify true relationships (i.e., excitation or\ninhibition/control). Then, we consider a novel extension of the log Gaussian\nCox process that incorporates evolutionary behavior and illustrate that our\nmodel comparison approach prefers the evolutionary log Gaussian Cox process\ncompared to simpler models. We also examine a real dataset consisting of\nviolent crime events from the 11th police district in Chicago from the year\n2018. This data exhibits strong daily seasonality and changes across the year.\nAfter we account for these data attributes, we find significant but mild\nself-excitation, implying that event occurrence increases the intensity of\nfuture events.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:12:13 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1910.06950", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, James S. Duncan", "title": "Jointly Discriminative and Generative Recurrent Neural Networks for\n  Learning from fMRI", "comments": "10th International Workshop on Machine Learning in Medical Imaging\n  (MLMI 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-32692-0_44", "report-no": null, "categories": "eess.IV cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) were designed for dealing with time-series\ndata and have recently been used for creating predictive models from functional\nmagnetic resonance imaging (fMRI) data. However, gathering large fMRI datasets\nfor learning is a difficult task. Furthermore, network interpretability is\nunclear. To address these issues, we utilize multitask learning and design a\nnovel RNN-based model that learns to discriminate between classes while\nsimultaneously learning to generate the fMRI time-series data. Employing the\nlong short-term memory (LSTM) structure, we develop a discriminative model\nbased on the hidden state and a generative model based on the cell state. The\naddition of the generative model constrains the network to learn functional\ncommunities represented by the LSTM nodes that are both consistent with the\ndata generation as well as useful for the classification task. We apply our\napproach to the classification of subjects with autism vs. healthy controls\nusing several datasets from the Autism Brain Imaging Data Exchange. Experiments\nshow that our jointly discriminative and generative model improves\nclassification learning while also producing robust and meaningful functional\ncommunities for better model understanding.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:43:45 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Zhuang", "Juntang", ""], ["Duncan", "James S.", ""]]}, {"id": "1910.07084", "submitter": "Johannes Bracher", "authors": "Johannes Bracher", "title": "An extended note on the multibin logarithmic score used in the FluSight\n  competitions", "comments": "This note elaborates on a letter published in PNAS: Bracher, J: On\n  the multibin logarithmic score used in the FluSight competitions. PNAS first\n  published September 26, 2019 https://doi.org/10.1073/pnas.1912147116", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the Centers for Disease Control and Prevention (CDC) have\norganized FluSight influenza forecasting competitions. To evaluate the\nparticipants' forecasts a multibin logarithmic score has been created, which is\na non-standard variant of the established logarithmic score. Unlike the\noriginal log score, the multibin version is not proper and may thus encourage\ndishonest forecasting. We explore the practical consequences this may have,\nusing forecasts from the 2016/17 FluSight competition for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:56:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bracher", "Johannes", ""]]}, {"id": "1910.07091", "submitter": "Ben Weidmann", "authors": "Ben Weidmann and Luke Miratrix", "title": "Lurking Inferential Monsters? Quantifying bias in non-experimental\n  evaluations of school programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines whether unobserved factors substantially bias education\nevaluations that rely on the Conditional Independence Assumption. We add 14 new\nwithin-study comparisons to the literature, all from primary schools in\nEngland. Across these 14 studies, we generate 42 estimates of selection bias\nusing a simple matching approach. A meta-analysis of the estimates suggests\nthat the distribution of underlying bias is centered around zero. The mean\nabsolute value of estimated bias is 0.03{\\sigma}, and none of the 42 estimates\nare larger than 0.11{\\sigma}. Results are similar for math, reading and writing\noutcomes. Overall, we find no evidence of substantial selection bias due to\nunobserved characteristics. These findings may not generalise easily to other\nsettings or to more radical educational interventions, but they do suggest that\nnon-experimental approaches could play a greater role than they currently do in\ngenerating reliable causal evidence for school education.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 22:51:24 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Weidmann", "Ben", ""], ["Miratrix", "Luke", ""]]}, {"id": "1910.07121", "submitter": "Cheng Lee", "authors": "C.K. Lee", "title": "Sampling by Reversing The Landmarking Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variations of the commonly applied landmark sampling are presented. These\nsamplings are \"forward\" landmarking such that each stack of data is created by\nfirst selecting landmarks and then including the subsequent observations of the\nselected landmarks. Unlike these forward landmarking samplings, a \"backward\" or\n\"reverse\" landmarking is proposed with a flexible \"progressive\" weighting on\nselecting different types of events and non-events. The backward landmark\nsample is compared with those forward landmark samples with a real world\nmortgage data. Results show that the backward landmark sample has smaller\nsampling errors than of those forward landmark samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:22:01 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Lee", "C. K.", ""]]}, {"id": "1910.07155", "submitter": "Tianbo Chen", "authors": "Tianbo Chen, Ying Sun, Ta-Hsin Li", "title": "A Semi-Parametric Estimation Method for the Quantile Spectrum with an\n  Application to Earthquake Classification Using Convolutional Neural Network", "comments": "24 pages, 3 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new estimation method is introduced for the quantile\nspectrum, which uses a parametric form of the autoregressive (AR) spectrum\ncoupled with nonparametric smoothing. The method begins with quantile\nperiodograms which are constructed by trigonometric quantile regression at\ndifferent quantile levels, to represent the serial dependence of time series at\nvarious quantiles. At each quantile level, we approximate the quantile spectrum\nby a function in the form of an ordinary AR spectrum. In this model, we first\ncompute what we call the quantile autocovariance function (QACF) by the inverse\nFourier transformation of the quantile periodogram at each quantile level.\nThen, we solve the Yule-Walker equations formed by the QACF to obtain the\nquantile partial autocorrelation function (QPACF) and the scale parameter.\nFinally, we smooth QPACF and the scale parameter across the quantile levels\nusing a nonparametric smoother, convert the smoothed QPACF to AR coefficients,\nand obtain the AR spectral density function. Numerical results show that the\nproposed method outperforms other conventional smoothing techniques. We take\nadvantage of the two-dimensional property of the estimators and train a\nconvolutional neural network (CNN) to classify smoothed quantile periodogram of\nearthquake data and achieve a higher accuracy than a similar classifier using\nordinary periodograms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:42:16 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chen", "Tianbo", ""], ["Sun", "Ying", ""], ["Li", "Ta-Hsin", ""]]}, {"id": "1910.07185", "submitter": "Guy Hawkins", "authors": "Laura Wall, David Gunawan, Scott D. Brown, Minh-Ngoc Tran, Robert\n  Kohn, Guy E. Hawkins", "title": "Identifying relationships between cognitive processes across tasks,\n  contexts, and time", "comments": "30 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly assumed that a specific testing occasion (task, design,\nprocedure, etc.) provides insights that generalise beyond that occasion. This\nassumption is infrequently carefully tested in data. We develop a statistically\nprincipled method to directly estimate the correlation between latent\ncomponents of cognitive processing across tasks, contexts, and time. This\nmethod simultaneously estimates individual-participant parameters of a\ncognitive model at each testing occasion, group-level parameters representing\nacross-participant parameter averages and variances, and across-task\ncorrelations. The approach provides a natural way to \"borrow\" strength across\ntesting occasions, which can increase the precision of parameter estimates\nacross all testing occasions. Two example applications demonstrate that the\nmethod is practical in standard designs. The examples, and a simulation study,\nalso provide evidence about the reliability and validity of parameter estimates\nfrom the linear ballistic accumulator model. We conclude by highlighting the\npotential of the parameter-correlation method to provide an \"assumption-light\"\ntool for estimating the relatedness of cognitive processes across tasks,\ncontexts, and time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 06:30:28 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 05:00:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wall", "Laura", ""], ["Gunawan", "David", ""], ["Brown", "Scott D.", ""], ["Tran", "Minh-Ngoc", ""], ["Kohn", "Robert", ""], ["Hawkins", "Guy E.", ""]]}, {"id": "1910.07213", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Youssef Esstafa (LMB), Bruno\n  Saussereau (LMB)", "title": "Estimating FARIMA models with uncorrelated but non-independent error\n  terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive the asymptotic properties of the least squares\nestimator (LSE) of fractionally integrated autoregressive moving-average\n(FARIMA) models under the assumption that the errors are uncorrelated but not\nnecessarily independent nor martingale differences. We relax considerably the\nindependence and even the martingale difference assumptions on the innovation\nprocess to extend the range of application of the FARIMA models. We propose a\nconsistent estimator of the asymptotic covariance matrix of the LSE which may\nbe very different from that obtained in the standard framework. A\nself-normalized approach to confidence interval construction for weak FARIMA\nmodel parameters is also presented. All our results are done under a mixing\nassumption on the noise. Finally, some simulation studies and an application to\nthe daily returns of stock market indices are presented to corroborate our\ntheoretical work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:25:55 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:35:10 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Esstafa", "Youssef", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1910.07438", "submitter": "Glen McGee", "authors": "Glen McGee, Marianthi-Anna Kioumourtzoglou, Marc G. Weisskopf,\n  Sebastien Haneuse and Brent A. Coull", "title": "On the Interplay Between Exposure Misclassification and Informative\n  Cluster Size", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12430", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the impact of exposure misclassification when cluster\nsize is potentially informative (i.e., related to outcomes) and when\nmisclassification is differential by cluster size. First, we show that\nmisclassification in an exposure related to cluster size can induce\ninformativeness when cluster size would otherwise be non-informative. Second,\nwe show that misclassification that is differential by informative cluster size\ncan not only attenuate estimates of exposure effects but even inflate or\nreverse the sign of estimates. To correct for bias in estimating marginal\nparameters, we propose two frameworks: (i) an observed likelihood approach for\njoint marginalized models of cluster size and outcomes and (ii) an expected\nestimating equations approach. Although we focus on estimating marginal\nparameters, a corollary is that the observed likelihood approach permits valid\ninference for conditional parameters as well. Using data from the Nurses Health\nStudy II, we compare the results of the proposed correction methods when\napplied to motivating data on the multigenerational effect of in-utero\ndiethylstilbestrol exposure on attention-deficit/hyperactivity disorder in\n106,198 children of 47,450 nurses.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:57:40 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["McGee", "Glen", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Weisskopf", "Marc G.", ""], ["Haneuse", "Sebastien", ""], ["Coull", "Brent A.", ""]]}, {"id": "1910.07447", "submitter": "Amanda Luby", "authors": "Amanda Luby, Anjali Mazumder, Brian Junker", "title": "Psychometric Analysis of Forensic Examiner Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic science often involves the comparison of crime-scene evidence to a\nknown-source sample to determine if the evidence and the reference sample came\nfrom the same source. Even as forensic analysis tools become increasingly\nobjective and automated, final source identifications are often left to\nindividual examiners' interpretation of the evidence. Each source\nidentification relies on judgements about the features and quality of the\ncrime-scene evidence that may vary from one examiner to the next. The current\napproach to characterizing uncertainty in examiners' decision-making has\nlargely centered around the calculation of error rates aggregated across\nexaminers and identification tasks, without taking into account these\nvariations in behavior. We propose a new approach using IRT and IRT-like models\nto account for differences among examiners and additionally account for the\nvarying difficulty among source identification tasks. In particular, we survey\nsome recent advances (Luby, 2019a) in the application of Bayesian psychometric\nmodels, including simple Rasch models as well as more elaborate decision tree\nmodels, to fingerprint examiner behavior.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:08:21 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Luby", "Amanda", ""], ["Mazumder", "Anjali", ""], ["Junker", "Brian", ""]]}, {"id": "1910.07452", "submitter": "Aureo de Paula", "authors": "Aureo de Paula, Imran Rasul, Pedro Souza", "title": "Identifying Network Ties from Panel Data: Theory and an Application to\n  Tax Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:19:25 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 21:00:01 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["de Paula", "Aureo", ""], ["Rasul", "Imran", ""], ["Souza", "Pedro", ""]]}, {"id": "1910.07712", "submitter": "Jie Peng", "authors": "Jilei Yang and Jie Peng", "title": "Estimating Spatially-Smoothed Fiber Orientation Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (D-MRI) is an in-vivo and\nnon-invasive imaging technology to probe anatomical architectures of biological\nsamples. The anatomy of white matter fiber tracts in the brain can be revealed\nto help understanding of the connectivity patterns among different brain\nregions. In this paper, we propose a novel Nearest-neighbor Adaptive Regression\nModel (NARM) for adaptive estimation of the fiber orientation distribution\n(FOD) function based on D-MRI data, where spatial homogeneity is used to\nimprove FOD estimation by incorporating neighborhood information. Specifically,\nwe formulate the FOD estimation problem as a weighted linear regression\nproblem, where the weights are chosen to account for spatial proximity and\npotential heterogeneity due to different fiber configurations. The weights are\nadaptively updated and a stopping rule based on nearest neighbor distance is\ndesigned to prevent over-smoothing. NARM is further extended to accommodate\nD-MRI data with multiple bvalues.\n  Comprehensive simulation results demonstrate that NARM leads to satisfactory\nFOD reconstructions and performs better than voxel-wise estimation as well as\ncompeting smoothing methods. By applying NARM to real 3T D-MRI datasets, we\ndemonstrate the effectiveness of NARM in recovering more realistic crossing\nfiber patterns and producing more coherent fiber tracking results, establishing\nthe practical value of NARM for analyzing D-MRI data and providing reliable\ninformation on brain structural connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:13:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Yang", "Jilei", ""], ["Peng", "Jie", ""]]}, {"id": "1910.07741", "submitter": "Vikash Singh", "authors": "Jianzhi Liu, Ziming Yang, Jesse E. Engelberg, Frankline S. Nsai, Serge\n  Bataliack, Vikash Singh", "title": "Intelligent Surveillance of World Health Organization (WHO) Integrated\n  Disease Surveillance and Response (IDSR) Data in Cameroon Using Multivariate\n  Cross-Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As developing countries continue to face challenges associated with\ninfectious diseases, the need to improve infrastructure to systematically\ncollect data which can be used to understand their outbreak patterns becomes\nmore critical. The World Health Organization (WHO) Integrated Disease\nSurveillance and Response (IDSR) strategy seeks to drive the systematic\ncollection of surveillance data to strengthen district-level reporting and to\ntranslate them into public health actions. Since the analysis of this\nsurveillance data at the central levels of government in many developing\nnations has traditionally not included advanced analytics, there are\nopportunities for the development and exploration of computational approaches\nthat can provide proactive insights and improve general health outcomes of\ninfectious disease outbreaks. We propose and demonstrate a multivariate time\nseries cross-correlation analysis as a foundational step towards gaining\ninsight on infectious disease patterns via the pairwise computation of weighted\ncross-correlation scores for a specified disease across different health\ndistricts using surveillance data from Cameroon. Following the computation of\nweighted cross-correlation scores, we apply an anomaly detection algorithm to\nassess how outbreak alarm patterns align in highly correlated health districts.\nWe demonstrate how multivariate cross-correlation analysis of weekly\nsurveillance data can provide insight into infectious disease incidence\npatterns in Cameroon by identifying highly correlated health districts for a\ngiven disease. We further demonstrate scenarios in which identification of\nhighly correlated districts aligns with alarms flagged using a standard anomaly\ndetection algorithm, hinting at the potential of end to end solutions combining\nanomaly detection algorithms for flagging alarms in combination with\nmultivariate cross-correlation analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:13:22 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Liu", "Jianzhi", ""], ["Yang", "Ziming", ""], ["Engelberg", "Jesse E.", ""], ["Nsai", "Frankline S.", ""], ["Bataliack", "Serge", ""], ["Singh", "Vikash", ""]]}, {"id": "1910.07748", "submitter": "Aniket Biswas", "authors": "Ardhendu Banerjee, Subrata Chakraborty and Aniket Biswas", "title": "Selection of link function in binary regression: A case-study with world\n  happiness report on immigration", "comments": "12 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection of appropriate link function for binary regression remains an\nimportant issue for data analysis and its influence on related inference. We\nprescribe a new data-driven methodology to search for the same, considering\nsome popular classification assessment metrics. A case-study with World\nHappiness report,2018 with special reference to immigration is presented for\ndemonstrating utility of the prescribed routine.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:36:45 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Banerjee", "Ardhendu", ""], ["Chakraborty", "Subrata", ""], ["Biswas", "Aniket", ""]]}, {"id": "1910.07825", "submitter": "Mar\\'ia Alonso-Pena", "authors": "Mar\\'ia Alonso-Pena, Jose Ameijeiras-Alonso, Rosa M. Crujeiras", "title": "Nonparametric tests for circular regression", "comments": "36 pages, 3 figures, 10 tables", "journal-ref": null, "doi": "10.1080/00949655.2020.1818243", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No matter the nature of the response and/or explanatory variables in a\nregression model, some basic issues such as the existence of an effect of the\npredictor on the response, or the assessment of a common shape across groups of\nobservations, must be solved prior to model fitting. This is also the case for\nregression models involving circular variables (supported on the unit\ncircumference). In that context, using kernel regression methods, this paper\nprovides a flexible alternative for constructing pilot estimators that allow to\nconstruct suitable statistics to perform no-effect tests and tests for equality\nand parallelism of regression curves. Finite sample performance of the proposed\nmethods is analyzed in a simulation study and illustrated with real data\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 11:23:50 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 10:10:24 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 09:00:32 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Alonso-Pena", "Mar\u00eda", ""], ["Ameijeiras-Alonso", "Jose", ""], ["Crujeiras", "Rosa M.", ""]]}, {"id": "1910.07965", "submitter": "Szymon Urbas", "authors": "Szymon Urbas, Chris Sherlock and Paul Metcalfe", "title": "Interim recruitment prediction for multi-centre clinical trials", "comments": "36 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for monitoring, modelling, and predicting\nthe recruitment to multi-centre clinical trials. The work is motivated by\noverly optimistic and narrow prediction intervals produced by existing\ntime-homogeneous recruitment models for multi-centre recruitment. We first\npresent two tests for detection of decay in recruitment rates, together with a\npower study. We then introduce a model based on the inhomogeneous Poisson\nprocess with monotonically decaying intensity, motivated by recruitment trends\nobserved in oncology trials. The general form of the model permits adaptation\nto any parametric curve-shape. A general method for constructing sensible\nparameter priors is provided and Bayesian model averaging is used for making\npredictions which account for the uncertainty in both the parameters and the\nmodel. The validity of the method and its robustness to misspecification are\ntested using simulated datasets. The new methodology is then applied to\noncology trial data, where we make interim accrual predictions, comparing them\nto those obtained by existing methods, and indicate where unexpected changes in\nthe accrual pattern occur.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 15:18:08 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 09:26:08 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 14:27:03 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 10:21:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Urbas", "Szymon", ""], ["Sherlock", "Chris", ""], ["Metcalfe", "Paul", ""]]}, {"id": "1910.08063", "submitter": "Georgios Karagiannis", "authors": "Bledar A. Konomi and Georgios Karagiannis", "title": "Bayesian analysis of multifidelity computer models with local features\n  and non-nested experimental designs: Application to the WRF model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-fidelity Bayesian emulator for the analysis of the Weather\nResearch and Forecasting (WRF) model when the available simulations are not\ngenerated based on hierarchically nested experimental design. The proposed\nprocedure, called Augmented Bayesian Treed Co-Kriging, extends the scope of\nco-kriging in two major ways. We introduce a binary treed partition latent\nprocess in the multifidelity setting to account for non-stationary and\npotential discontinuities in the model outputs at different fidelity levels.\nMoreover, we introduce an efficient imputation mechanism which allows the\npractical implementation of co-kriging when the experimental design is\nnon-hierarchically nested by enabling the specification of semi-conjugate\npriors. Our imputation strategy allows the design of an efficient RJ-MCMC\nimplementation that involves collapsed blocks and direct simulation from\nconditional distributions. We develop the Monte Carlo recursive emulator which\nprovides a Monte Carlo proxy for the full predictive distribution of the model\noutput at each fidelity level, in a computationally feasible manner. The\nperformance of our method is demonstrated on a benchmark example, and compared\nagainst existing methods. The proposed method is used for the analysis of a\nlarge-scale climate modeling application which involves the WRF model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:51:32 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Konomi", "Bledar A.", ""], ["Karagiannis", "Georgios", ""]]}, {"id": "1910.08112", "submitter": "Kevin Nguyen", "authors": "Kevin P. Nguyen, Cherise Chin Fatt, Alex Treacher, Cooper Mellema,\n  Madhukar H. Trivedi, Albert Montillo", "title": "Anatomically-Informed Data Augmentation for functional MRI with\n  Applications to Deep Learning", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning to build accurate predictive models from\nfunctional neuroimaging data is often hindered by limited dataset sizes. Though\ndata augmentation can help mitigate such training obstacles, most data\naugmentation methods have been developed for natural images as in computer\nvision tasks such as CIFAR, not for medical images. This work helps to fills in\nthis gap by proposing a method for generating new functional Magnetic Resonance\nImages (fMRI) with realistic brain morphology. This method is tested on a\nchallenging task of predicting antidepressant treatment response from\npre-treatment task-based fMRI and demonstrates a 26% improvement in performance\nin predicting response using augmented images. This improvement compares\nfavorably to state-of-the-art augmentation methods for natural images. Through\nan ablative test, augmentation is also shown to substantively improve\nperformance when applied before hyperparameter optimization. These results\nsuggest the optimal order of operations and support the role of data\naugmentation method for improving predictive performance in tasks using fMRI.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:55:10 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Nguyen", "Kevin P.", ""], ["Fatt", "Cherise Chin", ""], ["Treacher", "Alex", ""], ["Mellema", "Cooper", ""], ["Trivedi", "Madhukar H.", ""], ["Montillo", "Albert", ""]]}, {"id": "1910.08162", "submitter": "Ehsan Farahbakhsh", "authors": "Ehsan Farahbakhsh, Ardeshir Hezarkhani, Taymour Eslamkish, Abbas\n  Bahroudi, Rohitash Chandra", "title": "Three-dimensional weights of evidence modeling of a deep-seated porphyry\n  Cu deposit", "comments": "34 pages, 20 figures, 6 tables", "journal-ref": null, "doi": "10.1144/geochem2020-038", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the challenges in data acquisition and modeling at the stage of\ndetailed exploration, it is difficult to develop a prospectivity model,\nparticularly for disseminated ore deposits. Recently, the weights of evidence\n(WofE) method has demonstrated a high efficiency for modeling such deposits. In\nthis study, we propose a framework for creating a three-dimensional (3D)\nweights of evidence-based prospectivity model of the Nochoun porphyry Cu\ndeposit in the Urmia-Dokhtar magmatic arc of Iran. The input data include\nqualitative geological and quantitative geochemical information obtained from\nboreholes and field observations. We combine ordinary and fuzzy weights of\nevidence for integrating qualitative and quantitative exploration criteria in a\n3D space constrained by a metallogenic model of the study area for identifying\na deep-seated ore body. Ordinary weights of evidence are determined for\ngeological data, including lithology, alteration, rock type, and structure.\nMoreover, we determine the fuzzy weight of evidence for each class of\ncontinuous geochemical models created based on Fe, Mo, and Zn concentration\nvalues derived from boreholes. We integrate the input evidential models using\nWofE and create two prospectivity models (i.e., posterior and studentized\nposterior probability). We also determine anomalous voxels in the probability\nmodels using concentration-volume fractal models and validate them using\nprediction-volume plots. The modeling results indicate that the studentized\nposterior probability model is more efficient in identifying voxels\nrepresenting copper mineralized rock volumes. We provide open source software\nfor the proposed framework which can be used for exploring deep-seated ore\nbodies in other regions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:59:35 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 04:07:58 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 04:31:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Farahbakhsh", "Ehsan", ""], ["Hezarkhani", "Ardeshir", ""], ["Eslamkish", "Taymour", ""], ["Bahroudi", "Abbas", ""], ["Chandra", "Rohitash", ""]]}, {"id": "1910.08179", "submitter": "Christos Argyropoulos", "authors": "Cristian H Bologa, Vernon Shane Pankratz, Mark L Unruh, Maria Eleni\n  Roumelioti, Vallabh Shah, Saeed Kamran Shaffi, Soraya Arzhan, John Cook,\n  Christos Argyropoulos", "title": "High Performance Implementation of the Hierarchical Likelihood for\n  Generalized Linear Mixed Models. An Application to estimate the potassium\n  reference range in massive Electronic Health Records datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converting electronic health record (EHR) entries to useful clinical\ninferences requires one to address the poor scalability of existing\nimplementations of Generalized Linear Mixed Models (GLMM) for repeated\nmeasures. The major computational bottleneck concerns the numerical evaluation\nof integrals, which even for the simplest EHR analyses may involve millions of\ndimensions (one for each patient). The hierarchical likelihood (h-lik) approach\nto GLMMs is a framework for the estimation of GLMMs that is based on the\nLaplace Approximation (LA), which replaces integration with numerical\noptimization, and thus scales very well with dimensionality. We present a\nhigh-performance implementation of the h-lik for GLMMs in the R package TMB.\nUsing this approach, we examined the relation of serum potassium measurements\nand survival in the Cerner Real World Data (CRWD) EHR database. Analyzing this\ndata requires the evaluation of an integral in over 3 million dimensions. We\nalso assessed the scalability and accuracy of LA in smaller samples of 1 and\n10% size of the full dataset that were analyzed via the a) original,\ninterconnected Generalized Linear Models (iGLM), approach to h-lik, b) Adaptive\nGaussian Hermite (AGH) and c) the gold standard of Markov Chain Monte Carlo\n(MCMC) for multivariate integration. Random effects estimates generated by the\nLA were within 10% of the values obtained by the iGLMs, AGH and MCMC\ntechniques. The H-lik approach was 4-30 times faster than AGH and nearly 800\ntimes faster than MCMC. We found that the combination of the LA and AD offers a\ncomputationally efficient, numerically accurate approach for the analysis of\nextremely large, real world repeated measures data via the h-lik approach to\nGLMMs. The clinical inference from our analysis may guide choices of threatment\nthresholds for treating potassium disorders in the clinic.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 21:41:54 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 17:20:30 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 15:35:50 GMT"}, {"version": "v4", "created": "Sat, 17 Apr 2021 15:27:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bologa", "Cristian H", ""], ["Pankratz", "Vernon Shane", ""], ["Unruh", "Mark L", ""], ["Roumelioti", "Maria Eleni", ""], ["Shah", "Vallabh", ""], ["Shaffi", "Saeed Kamran", ""], ["Arzhan", "Soraya", ""], ["Cook", "John", ""], ["Argyropoulos", "Christos", ""]]}, {"id": "1910.08408", "submitter": "Alexander Matei", "authors": "Tristan Gally, Peter Groche, Florian Hoppe, Anja Kuttich, Alexander\n  Matei, Marc E. Pfetsch, Martin Rakowitsch, Stefan Ulbrich", "title": "Identification of Model Uncertainty via Optimal Design of Experiments\n  Applied to a Mechanical Press", "comments": null, "journal-ref": null, "doi": "10.1007/s11081-021-09600-8", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering applications almost all processes are described with the help\nof models. Especially forming machines heavily rely on mathematical models for\ncontrol and condition monitoring. Inaccuracies during the modeling,\nmanufacturing and assembly of these machines induce model uncertainty which\nimpairs the controller's performance. In this paper we propose an approach to\nidentify model uncertainty using parameter identification, optimal design of\nexperiments and hypothesis testing. The experimental setup is characterized by\noptimal sensor positions such that specific model parameters can be determined\nwith minimal variance. This allows for the computation of confidence regions in\nwhich the real parameters or the parameter estimates from different test sets\nhave to lie. We claim that inconsistencies in the estimated parameter values,\nconsidering their approximated confidence ellipsoids as well, cannot be\nexplained by data uncertainty but are indicators of model uncertainty. The\nproposed method is demonstrated using a component of the 3D Servo Press, a\nmulti-technology forming machine that combines spindles with eccentric servo\ndrives.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:27:53 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 14:21:35 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 13:45:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gally", "Tristan", ""], ["Groche", "Peter", ""], ["Hoppe", "Florian", ""], ["Kuttich", "Anja", ""], ["Matei", "Alexander", ""], ["Pfetsch", "Marc E.", ""], ["Rakowitsch", "Martin", ""], ["Ulbrich", "Stefan", ""]]}, {"id": "1910.08409", "submitter": "Georgios Karagiannis", "authors": "Georgios Karagiannis, Zhangshuan Hou, Maoyi Huang, and Guang Lin", "title": "Inverse modeling of hydrologic parameters in CLM4 via generalized\n  polynomial chaos in the Bayesian framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the applicability of generalized polynomial chaos (gPC)\nexpansion for land surface model parameter estimation is evaluated. We compute\nthe (posterior) distribution of the critical hydrological parameters that are\nsubject to great uncertainty in the community land model (CLM). The unknown\nparameters include those that have been identified as the most influential\nfactors on the simulations of surface and subsurface runoff, latent and\nsensible heat fluxes, and soil moisture in CLM4.0. We setup the inversion\nproblem this problem in the Bayesian framework in two steps: (i) build a\nsurrogate model expressing the input-output mapping, and (ii) compute the\nposterior distributions of the input parameters. Development of the surrogate\nmodel is done with a Bayesian procedure, based on the variable selection\nmethods that use gPC expansions. Our approach accounts for bases selection\nuncertainty and quantifies the importance of the gPC terms, and hence all the\ninput parameters, via the associated posterior probabilities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:30:03 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Karagiannis", "Georgios", ""], ["Hou", "Zhangshuan", ""], ["Huang", "Maoyi", ""], ["Lin", "Guang", ""]]}, {"id": "1910.08483", "submitter": "Agni Orfanoudaki", "authors": "Dimitris Bertsimas, Agni Orfanoudaki, Rory B. Weiner", "title": "Personalized Treatment for Coronary Artery Disease Patients: A Machine\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current clinical practice guidelines for managing Coronary Artery Disease\n(CAD) account for general cardiovascular risk factors. However, they do not\npresent a framework that considers personalized patient-specific\ncharacteristics. Using the electronic health records of 21,460 patients, we\ncreated data-driven models for personalized CAD management that significantly\nimprove health outcomes relative to the standard of care. We develop binary\nclassifiers to detect whether a patient will experience an adverse event due to\nCAD within a 10-year time frame. Combining the patients' medical history and\nclinical examination results, we achieve 81.5% AUC. For each treatment, we also\ncreate a series of regression models that are based on different supervised\nmachine learning algorithms. We are able to estimate with average R squared =\n0.801 the time from diagnosis to a potential adverse event (TAE) and gain\naccurate approximations of the counterfactual treatment effects. Leveraging\ncombinations of these models, we present ML4CAD, a novel personalized\nprescriptive algorithm. Considering the recommendations of multiple predictive\nmodels at once, ML4CAD identifies for every patient the therapy with the best\nexpected outcome using a voting mechanism. We evaluate its performance by\nmeasuring the prescription effectiveness and robustness under alternative\nground truths. We show that our methodology improves the expected TAE upon the\ncurrent baseline by 24.11%, increasing it from 4.56 to 5.66 years. The\nalgorithm performs particularly well for the male (24.3% improvement) and\nHispanic (58.41% improvement) subpopulations. Finally, we create an interactive\ninterface, providing physicians with an intuitive, accurate, readily\nimplementable, and effective tool.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:57:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Orfanoudaki", "Agni", ""], ["Weiner", "Rory B.", ""]]}, {"id": "1910.08501", "submitter": "Mariia Dmitrieva", "authors": "Mariia Dmitrieva, Keith E. Brown, Gary J. Heald, David M. Lane", "title": "Classification of spherical objects based on the form function of\n  acoustic echoes", "comments": null, "journal-ref": "Proc. of the 4th Underwater Acoustics Conference and Exhibition\n  (UACE), Skiathos, Greece, Sept 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to recognise an object is to study how the echo has been shaped\nduring the interaction with the target. Wideband sonar allows the study of the\nenergy distribution for a large range of frequencies. The frequency\ndistribution contains information about an object, including its inner\nstructure. This information is a key for automatic recognition. The scattering\nby a target can be quantitatively described by its Form Function. The Form\nFunction can be calculated based on the data of the initial pulse, reflected\npulse and parameters of a medium where the pulse is propagating. In this work\nspherical objects are classified based on their filler material - water or air.\nWe limit the study to spherical 2 layered targets immersed in water. The Form\nFunction is used as a descriptor and fed into a Neural Network classifier,\nMultilayer Perceptron (MLP). The performance of the classifier is compared with\nSupport Vector Machine (SVM) and the Form Function descriptor is examined in\ncontrast to the Time and Frequency Representation of the echo.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:51:30 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Dmitrieva", "Mariia", ""], ["Brown", "Keith E.", ""], ["Heald", "Gary J.", ""], ["Lane", "David M.", ""]]}, {"id": "1910.08509", "submitter": "Nikola Tuneski", "authors": "Ivan Hendrikx and Nikola Tuneski", "title": "The need for adequate sampling in a well-functioning market surveillance\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adequate sampling is essential for the well-functioning of a market\nsurveillance system. As small as possible statistically significant sample size\nis the main factor that determines the costs of market surveillance actions.\nThis paper studies various possibilities for calculation of the size of the\nsample with an emphasis on the method based on the binomial distribution.\nExamples, comparisons, and conclusions are provided.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:59:07 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 07:14:52 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Hendrikx", "Ivan", ""], ["Tuneski", "Nikola", ""]]}, {"id": "1910.08626", "submitter": "Tahar M Kechadi", "authors": "Fadoua Rafii and Tahar Kechadi", "title": "Collection of Historical Weather Data: Issues with Missing Values", "comments": "The 4th International Conference on Smart City Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather data collected from automated weather stations have become a crucial\ncomponent for making decisions in agriculture and in forestry. Over time,\nweather stations may become out-of-order or stopped for maintenance, and\ntherefore, during those periods, the data values will be missing.\nUnfortunately, this will cause huge problems when analysing the data. The main\naim of this study is to create high-quality historical weather datasets by\ndealing efficiently with missing values. In this paper, we present a set of\nmissing data imputation methods and study their effectiveness. These methods\nwere used based on different types of missing values. The experimental results\nshow that two the proposed methods are very promising and can be used at larger\nscale.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 14:42:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Rafii", "Fadoua", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1910.08699", "submitter": "Xin Ma", "authors": "Jie Xia, Xin Ma, Wenqing Wu, Baolian Huang, Wanpeng Li", "title": "Application of a new information priority accumulated grey model with\n  time power to predict short-term wind turbine capacity", "comments": null, "journal-ref": "Journal of Cleaner Production, Volume 244, 2020, 118573", "doi": "10.1016/j.jclepro.2019.118573", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind energy makes a significant contribution to global power generation.\nPredicting wind turbine capacity is becoming increasingly crucial for cleaner\nproduction. For this purpose, a new information priority accumulated grey model\nwith time power is proposed to predict short-term wind turbine capacity.\nFirstly, the computational formulas for the time response sequence and the\nprediction values are deduced by grey modeling technique and the definite\nintegral trapezoidal approximation formula. Secondly, an intelligent algorithm\nbased on particle swarm optimization is applied to determine the optimal\nnonlinear parameters of the novel model. Thirdly, three real numerical examples\nare given to examine the accuracy of the new model by comparing with six\nexisting prediction models. Finally, based on the wind turbine capacity from\n2007 to 2017, the proposed model is established to predict the total wind\nturbine capacity in Europe, North America, Asia, and the world. The numerical\nresults reveal that the novel model is superior to other forecasting models. It\nhas a great advantage for small samples with new characteristic behaviors.\nBesides, reasonable suggestions are put forward from the standpoint of the\npractitioners and governments, which has high potential to advance the\nsustainable improvement of clean energy production in the future.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 04:13:32 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xia", "Jie", ""], ["Ma", "Xin", ""], ["Wu", "Wenqing", ""], ["Huang", "Baolian", ""], ["Li", "Wanpeng", ""]]}, {"id": "1910.08750", "submitter": "Aditi Kathpalia", "authors": "Aditi Kathpalia and Nithin Nagaraj", "title": "Measuring Causality: The Science of Cause and Effect", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining and measuring cause-effect relationships is fundamental to most\nscientific studies of natural phenomena. The notion of causation is distinctly\ndifferent from correlation which only looks at association of trends or\npatterns in measurements. In this article, we review different notions of\ncausality and focus especially on measuring causality from time series data.\nCausality testing finds numerous applications in diverse disciplines such as\nneuroscience, econometrics, climatology, physics and artificial intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 11:15:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kathpalia", "Aditi", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1910.08857", "submitter": "Gwendolyn Eadie", "authors": "Gwendolyn Eadie, Arash Bahramian, Pauline Barmby, Radu Craiu, Derek\n  Bingham, Ren\\'ee Hlo\\v{z}ek, JJ Kavelaars, David Stenning, Samantha\n  Benincasa, Guillaume Thomas, Karun Thanjavur, Jo Bovy, Jan Cami, Ray\n  Carlberg, Sam Lawler, Adrian Liu, Henry Ngo, Mubdi Rahman, Michael Rupen", "title": "LRP2020: Astrostatistics in Canada", "comments": "White paper E017 submitted to the Canadian Long Range Plan LRP2020", "journal-ref": null, "doi": "10.5281/zenodo.3756019", "report-no": null, "categories": "astro-ph.IM physics.ed-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Abridged from Executive Summary) This white paper focuses on the\ninterdisciplinary fields of astrostatistics and astroinformatics, in which\nmodern statistical and computational methods are applied to and developed for\nastronomical data. Astrostatistics and astroinformatics have grown dramatically\nin the past ten years, with international organizations, societies,\nconferences, workshops, and summer schools becoming the norm. Canada's formal\nrole in astrostatistics and astroinformatics has been relatively limited, but\nthere is a great opportunity and necessity for growth in this area. We\nconducted a survey of astronomers in Canada to gain information on the training\nmechanisms through which we learn statistical methods and to identify areas for\nimprovement. In general, the results of our survey indicate that while\nastronomers see statistical methods as critically important for their research,\nthey lack focused training in this area and wish they had received more formal\ntraining during all stages of education and professional development. These\nfindings inform our recommendations for the LRP2020 on how to increase\ninterdisciplinary connections between astronomy and statistics at the\ninstitutional, national, and international levels over the next ten years. We\nrecommend specific, actionable ways to increase these connections, and discuss\nhow interdisciplinary work can benefit not only research but also astronomy's\nrole in training Highly Qualified Personnel (HQP) in Canada.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 23:14:57 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Eadie", "Gwendolyn", ""], ["Bahramian", "Arash", ""], ["Barmby", "Pauline", ""], ["Craiu", "Radu", ""], ["Bingham", "Derek", ""], ["Hlo\u017eek", "Ren\u00e9e", ""], ["Kavelaars", "JJ", ""], ["Stenning", "David", ""], ["Benincasa", "Samantha", ""], ["Thomas", "Guillaume", ""], ["Thanjavur", "Karun", ""], ["Bovy", "Jo", ""], ["Cami", "Jan", ""], ["Carlberg", "Ray", ""], ["Lawler", "Sam", ""], ["Liu", "Adrian", ""], ["Ngo", "Henry", ""], ["Rahman", "Mubdi", ""], ["Rupen", "Michael", ""]]}, {"id": "1910.08858", "submitter": "Sathya Ramesh", "authors": "Sathya Ramesh, Ragib Mostofa, Marco Bornstein, John Dobelman", "title": "Beating the House: Identifying Inefficiencies in Sports Betting Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inefficient markets allow investors to consistently outperform the market. To\ndemonstrate that inefficiencies exist in sports betting markets, we created a\nbetting algorithm that generates above market returns for the NFL, NBA, NCAAF,\nNCAAB, and WNBA betting markets. To formulate our betting strategy, we\ncollected and examined a novel dataset of bets, and created a non-parametric\nwin probability model to find positive expected value situations. As the United\nStates Supreme Court has recently repealed the federal ban on sports betting,\nresearch on sports betting markets is increasingly relevant for the growing\nsports betting industry.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 23:16:17 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 23:46:04 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Ramesh", "Sathya", ""], ["Mostofa", "Ragib", ""], ["Bornstein", "Marco", ""], ["Dobelman", "John", ""]]}, {"id": "1910.08936", "submitter": "Adil Yazigi", "authors": "Adil Yazigi, Antti Penttinen, Anna-Kaisa Ylitalo, Matti Maltamo,\n  Petteri Packalen, Lauri Meht\\\"atalo", "title": "Sequential Spatial Point Process Models for Spatio-Temporal Point\n  Processes: A Self-Interactive Model with Application to Forest Tree Data", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the spatial dynamics of a forest stand by using a special class of\nspatio-temporal point processes, the sequential spatial point process, where\nthe spatial dimension is parameterized and the time component is atomic. The\nsequential spatial point processes differ from spatial point processes in the\nsense that the realizations are ordered sequences of spatial locations and the\norder of points allows us to approximate the spatial evolutionary dynamics of\nthe process. This feature shall be useful to interpret the long-term dependence\nand the memory formed by the spatial history of the process. As an\nillustration, the sequence can represent the tree locations ordered with\nrespect to time, or to some given quantitative marks such as tree diameters. We\nderive a parametric sequential spatial point process model that is expressed in\nterms of self-interaction of the spatial points, and then the\nmaximum-likelihood-based inference is tractable. As an application, we apply\nthe model obtained to forest dataset collected from the Kiihtelysvaara site in\nEastern Finland. Potential applications in remote sensing of forests are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 09:36:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yazigi", "Adil", ""], ["Penttinen", "Antti", ""], ["Ylitalo", "Anna-Kaisa", ""], ["Maltamo", "Matti", ""], ["Packalen", "Petteri", ""], ["Meht\u00e4talo", "Lauri", ""]]}, {"id": "1910.09030", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri, Shaowu Yuchi, Shahrokh Shahpar, Geoffrey Parks", "title": "Supporting Multi-point Fan Design with Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the idea of turbomachinery active subspace performance maps,\nthis paper studies dimension reduction in turbomachinery 3D CFD simulations.\nFirst, we show that these subspaces exist across different blades---under the\nsame parametrization---largely independent of their Mach number or Reynolds\nnumber. This is demonstrated via a numerical study on three different blades.\nThen, in an attempt to reduce the computational cost of identifying a suitable\ndimension reducing subspace, we examine statistical sufficient dimension\nreduction methods, including sliced inverse regression, sliced average variance\nestimation, principal Hessian directions and contour regression. Unsatisfied by\nthese results, we evaluate a new idea based on polynomial variable\nprojection---a non-linear least squares problem. Our results using polynomial\nvariable projection clearly demonstrate that one can accurately identify\ndimension reducing subspaces for turbomachinery functionals at a fraction of\nthe cost associated with prior methods. We apply these subspaces to the problem\nof comparing design configurations across different flight points on a working\nline of a fan blade. We demonstrate how designs that offer a healthy compromise\nbetween performance at cruise and sea-level conditions can be easily found by\nvisually inspecting their subspaces.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:39:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Seshadri", "Pranay", ""], ["Yuchi", "Shaowu", ""], ["Shahpar", "Shahrokh", ""], ["Parks", "Geoffrey", ""]]}, {"id": "1910.09163", "submitter": "Zahra Razaee", "authors": "Zahra S. Razaee, Galen Wien-Cook, Mourad Tighiouart", "title": "A Nonparametric Bayesian Design for Drug Combination Cancer Trials", "comments": "22 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive design for early phase drug combination cancer trials\nwith the goal of estimating the maximum tolerated dose (MTD). A nonparametric\nBayesian model, using beta priors truncated to the set of partially ordered\ndose combinations, is used to describe the probability of dose limiting\ntoxicity (DLT). Dose allocation between successive cohorts of patients is\nestimated using a modified Continual Reassessment scheme. The updated\nprobabilities of DLT are calculated with a Gibbs sampler that employs a\nweighting mechanism to calibrate the influence of data versus the prior. At the\nend of the trial, we recommend one or more dose combinations as the MTD based\non our proposed algorithm. The design operating characteristics indicate that\nour method is comparable with existing methods. As an illustration, we apply\nour method to a phase I clinical trial of CB-839 and Gemcitabine.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:15:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Razaee", "Zahra S.", ""], ["Wien-Cook", "Galen", ""], ["Tighiouart", "Mourad", ""]]}, {"id": "1910.09461", "submitter": "Caroline S. Wagner", "authors": "Cong Cao, Jeroen Baas, Caroline S. Wagner, Koen Jonkers", "title": "Returning Scientists and the Emergence of Chinese Science System", "comments": "28 pages, 6 figures, new data on Chinese scientists and mobility", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chinese approach to developing a world-class science system includes a\nvigorous set of programmes to attract back Chinese researchers who have\noverseas training and work experience. No analysis is available to show the\nperformance of these mobile researchers. This article attempts to close part of\nthis gap. Using a novel bibliometric approach, we estimate the stocks of\noverseas Chinese and returnees from the perspective of their publication\nactivities, albeit with some limitations. We show that the share of overseas\nChinese scientists in the US is considerably larger than that in the EU. We\nalso show that Chinese returnees publish higher impact work, and continue to\npublish more and at the international level than domestic counterparts.\nReturnees not only tend to publish more, but they are instrumental in linking\nChina into the global network. Indeed, returnees actively co-publish with\nresearchers in their former host system, showing the importance of scientific\nsocial capital. Future research will examine the impact of length of stay,\namong other factors, on such impact and integration.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 15:46:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cao", "Cong", ""], ["Baas", "Jeroen", ""], ["Wagner", "Caroline S.", ""], ["Jonkers", "Koen", ""]]}, {"id": "1910.09570", "submitter": "Joseph Paul Cohen", "authors": "Shawn Tan and Guillaume Androz and Ahmad Chamseddine and Pierre\n  Fecteau and Aaron Courville and Yoshua Bengio and Joseph Paul Cohen", "title": "Icentia11K: An Unsupervised Representation Learning Dataset for\n  Arrhythmia Subtype Discovery", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release the largest public ECG dataset of continuous raw signals for\nrepresentation learning containing 11 thousand patients and 2 billion labelled\nbeats. Our goal is to enable semi-supervised ECG models to be made as well as\nto discover unknown subtypes of arrhythmia and anomalous ECG signal events. To\nthis end, we propose an unsupervised representation learning task, evaluated in\na semi-supervised fashion. We provide a set of baselines for different feature\nextractors that can be built upon. Additionally, we perform qualitative\nevaluations on results from PCA embeddings, where we identify some clustering\nof known subtypes indicating the potential for representation learning in\narrhythmia sub-type discovery.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:02:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Tan", "Shawn", ""], ["Androz", "Guillaume", ""], ["Chamseddine", "Ahmad", ""], ["Fecteau", "Pierre", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1910.09628", "submitter": "Jiarui Lu", "authors": "Jiarui Lu and Hongzhe Li", "title": "Hypothesis Testing in High-Dimensional Instrumental Variables Regression\n  with an Application to Genomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression and phenotype association can be affected by potential\nunmeasured confounders from multiple sources, leading to biased estimates of\nthe associations. Since genetic variants largely explain gene expression\nvariations, they can be used as instruments in studying the association between\ngene expressions and phenotype in the framework of high dimensional\ninstrumental variable (IV) regression. However, because the dimensions of both\ngenetic variants and gene expressions are often larger than the sample size,\nstatistical inferences such as hypothesis testing for such high dimensional IV\nmodels are not trivial and have not been investigated in literature. The\nproblem is more challenging since the instrumental variables (e.g., genetic\nvariants) have to be selected among a large set of genetic variants. This paper\nconsiders the problem of hypothesis testing for sparse IV regression models and\npresents methods for testing single regression coefficient and multiple testing\nof multiple coefficients, where the test statistic for each single coefficient\nis constructed based on an inverse regression. A multiple testing procedure is\ndeveloped for selecting variables and is shown to control the false discovery\nrate. Simulations are conducted to evaluate the performance of our proposed\nmethods. These methods are illustrated by an analysis of a yeast dataset in\norder to identify genes that are associated with growth in the presence of\nhydrogen peroxide.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:47:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Lu", "Jiarui", ""], ["Li", "Hongzhe", ""]]}, {"id": "1910.09922", "submitter": "Hannah Roberts", "authors": "Hannah Roberts, Ian Kellar, Mark Conner, Christopher Gidlow, Brian\n  Kelly, Mark Nieuwenhuijsen, Rosemary McEachan", "title": "Associations between park features, park satisfaction and park use in a\n  multi-ethnic deprived urban area", "comments": null, "journal-ref": null, "doi": "10.1016/j.ufug.2019.126485", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Parks are increasingly understood to be key community resources for public\nhealth, particularly for ethnic minority and low socioeconomic groups. At the\nsame time, research suggests parks are underutilised by these groups. In order\nto design effective interventions to promote health, the determinants of park\nuse for these groups must be understood. This study examines the associations\nbetween park features, park satisfaction and park use in a deprived and\nethnically diverse sample in Bradford, UK. 652 women from the Born in Bradford\ncohort completed a survey on park satisfaction and park use. Using a\nstandardised direct observation tool, 44 parks in the area were audited for\npresent park features. Features assessed were: access, recreational facilities,\namenities, natural features, significant natural features, non-natural\nfeatures, incivilities and usability. Size and proximity to the park were also\ncalculated. Multilevel linear regressions were performed to understand\nassociations between park features and (1) park satisfaction and (2) park use.\nInteractions between park features, ethnicity and socioeconomic status were\nexplored, and park satisfaction was tested as a mediator in the relationship\nbetween park features and park use. More amenities and greater usability were\nassociated with increased park satisfaction, while more incivilities were\nnegatively related to park satisfaction. Incivilities, access and proximity\nwere also negatively associated with park use. Ethnicity and socioeconomic\nstatus had no moderating role, and there was no evidence for park satisfaction\nas a mediator between park features and park use. Results suggest diverse park\nfeatures are associated with park satisfaction and park use, but this did not\nvary by ethnicity or socioeconomic status. The reduction of incivilities should\nbe prioritised where the aim is to encourage park satisfaction and park use.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:32:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Roberts", "Hannah", ""], ["Kellar", "Ian", ""], ["Conner", "Mark", ""], ["Gidlow", "Christopher", ""], ["Kelly", "Brian", ""], ["Nieuwenhuijsen", "Mark", ""], ["McEachan", "Rosemary", ""]]}, {"id": "1910.09937", "submitter": "Pascal Weber", "authors": "Pascal Weber, Georgios Arampatzis, Guido Novati, Siddhartha Verma,\n  Costas Papadimitriou and Petros Koumoutsakos", "title": "Optimal sensing for fish school identification", "comments": null, "journal-ref": "Biomimetics 2020, 5(1), 10", "doi": "10.3390/biomimetics5010010", "report-no": null, "categories": "physics.flu-dyn physics.bio-ph physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fish schooling implies an awareness of the swimmers for their companions. In\nflow mediated environments, in addition to visual cues, pressure and shear\nsensors on the fish body are critical for providing quantitative information\nthat assists the quantification of proximity to other swimmers. Here we examine\nthe distribution of sensors on the surface of an artificial swimmer so that it\ncan optimally identify a leading group of swimmers. We employ Bayesian\nexperimental design coupled with two-dimensional Navier Stokes equations for\nmultiple self-propelled swimmers. The follower tracks the school using\ninformation from its own surface pressure and shear stress. We demonstrate that\nthe optimal sensor distribution of the follower is qualitatively similar to the\ndistribution of neuromasts on fish. Our results show that it is possible to\nidentify accurately the center of mass and even the number of the leading\nswimmers using surface only information.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:57:26 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Weber", "Pascal", ""], ["Arampatzis", "Georgios", ""], ["Novati", "Guido", ""], ["Verma", "Siddhartha", ""], ["Papadimitriou", "Costas", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1910.10051", "submitter": "Amit Moscovich", "authors": "Ye Zhou, Amit Moscovich, Tamir Bendory, Alberto Bartesaghi", "title": "Unsupervised particle sorting for high-resolution single-particle\n  cryo-EM", "comments": "12 pages, 7 figures", "journal-ref": "Inverse Problems 36:4 (2020)", "doi": "10.1088/1361-6420/ab5ec8", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle cryo-Electron Microscopy (EM) has become a popular technique\nfor determining the structure of challenging biomolecules that are inaccessible\nto other technologies. Recent advances in automation, both in data collection\nand data processing, have significantly lowered the barrier for non-expert\nusers to successfully execute the structure determination workflow. Many\ncritical data processing steps, however, still require expert user intervention\nin order to converge to the correct high-resolution structure. In particular,\nstrategies to identify homogeneous populations of particles rely heavily on\nsubjective criteria that are not always consistent or reproducible among\ndifferent users. Here, we explore the use of unsupervised strategies for\nparticle sorting that are compatible with the autonomous operation of the image\nprocessing pipeline. More specifically, we show that particles can be\nsuccessfully sorted based on a simple statistical model for the distribution of\nscores assigned during refinement. This represents an important step towards\nthe development of automated workflows for protein structure determination\nusing single-particle cryo-EM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:45:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhou", "Ye", ""], ["Moscovich", "Amit", ""], ["Bendory", "Tamir", ""], ["Bartesaghi", "Alberto", ""]]}, {"id": "1910.10070", "submitter": "Harry Spearing", "authors": "Harry Spearing, Jonathan Tawn, David Irons, Tim Paulden, and Grace\n  Bennett", "title": "Ranking, and other Properties, of Elite Swimmers using Extreme Value\n  Theory", "comments": "28 pages, 11 figures, to be published in JRSS series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The International Swimming Federation (FINA) uses a very simple points system\nwith the aim to rank swimmers across all Olympic events. The points acquired is\na function of the ratio of the recorded time and the current world record for\nthat event. With some world records considered \"better\" than others however,\nbias is introduced between events, with some being much harder to attain points\nwhere the world record is hard to beat. A model based on extreme value theory\nwill be introduced, where swim-times are modelled through their rate of\noccurrence, and with the distribution of the best times following a generalised\nPareto distribution. Within this framework, the strength of a particular swim\nis judged based on its position compared to the whole distribution of\nswim-times, rather than just the world record. This model also accounts for the\ndate of the swim, as training methods improve over the years, as well as\nchanges in technology, such as full body suits. The parameters of the\ngeneralised Pareto distribution, for each of the 34 individual Olympic events,\nwill be shown to vary with a covariate, leading to a novel single unified\ndescription of swim quality over all events and time. This structure, which\nallows information to be shared across all strokes, distances, and genders,\nimproves the predictive power as well as the model robustness compared to\nequivalent independent models. A by-product of the model is that it is possible\nto estimate other features of interest, such as the ultimate possible time, the\ndistribution of new world records for any event, and to correct swim times for\nthe effect of full body suits. The methods will be illustrated using a dataset\nof the best 500 swim-times for each event in the period 2001-2018.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:13:43 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 11:46:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Spearing", "Harry", ""], ["Tawn", "Jonathan", ""], ["Irons", "David", ""], ["Paulden", "Tim", ""], ["Bennett", "Grace", ""]]}, {"id": "1910.10201", "submitter": "Viet Chi Tran", "authors": "Clotilde Lepers and Sylvain Billiard and Matthieu Porte and Sylvie\n  M\\'el\\'eard and Viet Chi Tran", "title": "Inference with selection, varying population size and evolving\n  population structure: Application of ABC to a forward-backward coalescent\n  process with interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic data are often used to infer demographic history and changes or\ndetect genes under selection. Inferential methods are commonly based on models\nmaking various strong assumptions: demography and population structures are\nsupposed \\textit{a priori} known, the evolution of the genetic composition of a\npopulation does not affect demography nor population structure, and there is no\nselection nor interaction between and within genetic strains. In this paper, we\npresent a stochastic birth-death model with competitive interactions and\nasexual reproduction. We develop an inferential procedure for ecological,\ndemographic and genetic parameters. We first show how genetic diversity and\ngenealogies are related to birth and death rates, and to how individuals\ncompete within and between strains. {This leads us to propose an original model\nof phylogenies, with trait structure and interactions, that allows multiple\nmerging}. Second, we develop an Approximate Bayesian Computation framework to\nuse our model for analyzing genetic data. We apply our procedure to simulated\ndata from a toy model, and to real data by analyzing the genetic diversity of\nmicrosatellites on Y-chromosomes sampled from Central Asia human populations in\norder to test whether different social organizations show significantly\ndifferent fertility.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 19:12:04 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 08:17:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lepers", "Clotilde", ""], ["Billiard", "Sylvain", ""], ["Porte", "Matthieu", ""], ["M\u00e9l\u00e9ard", "Sylvie", ""], ["Tran", "Viet Chi", ""]]}, {"id": "1910.10313", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Kyung Suk Lee, Sojung C. Park, Jae Youn Ahn", "title": "Double-Counting Problem of the Bonus-Malus System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bonus-malus system (BMS) is a widely used premium adjustment mechanism\nbased on policyholder's claim history. Most auto insurance BMSs assume that\npolicyholders in the same bonus-malus (BM) level share the same a posteriori\nrisk adjustment. This system reflects the policyholder's claim history in a\nrelatively simple manner. However, the typical system follows a single BM scale\nand is known to suffer from the double-counting problem: policyholders in the\nhigh-risk classes in terms of a priori characteristics are penalized too\nseverely (Taylor, 1997; Pitrebois et al., 2003). Thus, Pitrebois et al. (2003)\nproposed a new system with multiple BM scales based on the a priori\ncharacteristics. While this multiple-scale BMS removes the double-counting\nproblem, it loses the prime benefit of simplicity. Alternatively, we argue that\nthe double-counting problem can be viewed as an inefficiency of the\noptimization process. Furthermore, we show that the double-counting problem can\nbe resolved by fully optimizing the BMS setting, but retaining the traditional\nBMS format.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 01:52:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Oh", "Rosy", ""], ["Lee", "Kyung Suk", ""], ["Park", "Sojung C.", ""], ["Ahn", "Jae Youn", ""]]}, {"id": "1910.10375", "submitter": "Xiao Liu", "authors": "Xiao Liu, Kyongmin Yeo and Siyuan Lu", "title": "Statistical Modeling for Spatio-Temporal Data from Stochastic\n  Convection-Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a physical-statistical modeling approach for\nspatio-temporal data arising from a class of stochastic convection-diffusion\nprocesses. Such processes are widely found in scientific and engineering\napplications where fundamental physics imposes critical constraints on how data\ncan be modeled and how models should be interpreted. The idea of spectrum\ndecomposition is employed to approximate a physical spatio-temporal process by\nthe linear combination of spatial basis functions and a multivariate random\nprocess of spectral coefficients. Unlike existing approaches assuming\nspatially- and temporally-invariant convection-diffusion, this paper considers\na more general scenario with spatially-varying convection-diffusion and\nnonzero-mean source-sink. As a result, the temporal dynamics of spectral\ncoefficients is coupled with each other, which can be interpreted as the\nnon-linear energy redistribution across multiple scales from the perspective of\nphysics. Because of the spatially-varying convection-diffusion, the space-time\ncovariance is non-stationary in space. The theoretical results are integrated\ninto a hierarchical dynamical spatio-temporal model. The connection is\nestablished between the proposed model and the existing models based on\nIntegro-Difference Equations. Computational efficiency and scalability are also\ninvestigated to make the proposed approach practical. The advantages of the\nproposed methodology are demonstrated by numerical examples, a case study, and\ncomprehensive comparison studies. Computer code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 06:02:50 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 19:24:35 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 04:53:13 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 02:53:59 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Liu", "Xiao", ""], ["Yeo", "Kyongmin", ""], ["Lu", "Siyuan", ""]]}, {"id": "1910.10426", "submitter": "Linas Petkevicius", "authors": "Vilijandas Bagdonavicius, Linas Petkevicius", "title": "Multiple outlier detection tests for parametric models", "comments": null, "journal-ref": "Mathematics 8 (2020) 2156", "doi": "10.3390/math8122156", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple multiple outlier identification method for parametric\nlocation-scale and shape-scale models when the number of possible outliers is\nnot specified. The method is based on a result giving asymptotic properties of\nextreme z-scores. Robust estimators of model parameters are used defining\nz-scores. An extensive simulation study was done for comparing of the proposed\nmethod with existing methods. For the normal family, the method is compared\nwith the well known Davies-Gather, Rosner's, Hawking's and Bolshev's multiple\noutlier identification methods. The choice of an upper limit for the number of\npossible outliers in case of Rosner's test application is discussed. For other\nfamilies, the proposed method is compared with a method generalizing\nGather-Davies method. In most situations, the new method has the highest\noutlier identification power in terms of masking and swamping values. We also\ncreated R package outliersTests for proposed test.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:23:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bagdonavicius", "Vilijandas", ""], ["Petkevicius", "Linas", ""]]}, {"id": "1910.10432", "submitter": "Yunjiao Lu", "authors": "Yunjiao Lu, Pierre Hodara, Charles Kervrann, Alain Trubuil", "title": "Probabilistic reconstruction of truncated particle trajectories on a\n  closed surface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of dynamic processes in cell biology very often relies on the\nobservation in two dimensions of 3D biological processes. Consequently, the\ndata are partial and statistical methods and models are required to recover the\nparameters describing the dynamical processes. In the case of molecules moving\nover the 3D surface, such as proteins on walls of bacteria cell, a large\nportion of the 3D surface is not observed in 2D-time microscopy. It follows\nthat biomolecules may disappear for a period of time in a region of interest,\nand then reappear later. Assuming Brownian motion with drift, we address the\nmathematical problem of the reconstruction of biomolecules trajectories on a\ncylindrical surface. A subregion of the cylinder is typically recorded during\nthe observation period, and biomolecules may appear or disappear in any place\nof the 3D surface. The performance of the method is demonstrated on simulated\nparticle trajectories that mimic MreB protein dynamics observed in 2D\ntime-lapse fluorescence microscopy in rod-shaped bacteria.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:34:55 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 14:44:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Lu", "Yunjiao", ""], ["Hodara", "Pierre", ""], ["Kervrann", "Charles", ""], ["Trubuil", "Alain", ""]]}, {"id": "1910.10512", "submitter": "Saint-Clair Chabert-Liddell", "authors": "Saint-Clair Chabert-Liddell and Pierre Barbillon and Sophie Donnet and\n  Emmanuel Lazega", "title": "A Stochastic Block Model Approach for the Analysis of Multilevel\n  Networks: an Application to the Sociology of Organizations", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2021.107179", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multilevel network is defined as the junction of two interaction networks,\none level representing the interactions between individuals and the other the\ninteractions between organizations. The levels are linked by an affiliation\nrelationship, each individual belonging to a unique organization. A new\nStochastic Block Model is proposed as a unified probalistic framework tailored\nfor multilevel networks. This model contains latent blocks accounting for\nheterogeneity in the patterns of connection within each level and introducing\ndependencies between the levels. The sought connection patterns are not\nspecified a priori which makes this approach flexible. Variational methods are\nused for the model inference and an Integrated Classified Likelihood criterion\nis developed for choosing the number of blocks and also for deciding whether\nthe two levels are dependent or not. A comprehensive simulation study exhibits\nthe benefit of considering this approach, illustrates the robustness of the\nclustering and highlights the reliability of the criterion used for model\nselection. This approach is applied on a sociological dataset collected during\na television program trade fair, the inter-organizational level being the\neconomic network between companies and the inter-individual level being the\ninformal network between their representatives. It brings a synthetic\nrepresentation of the two networks unraveling their intertwined structure and\nconfirms the coopetition at stake.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 12:08:41 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:28:16 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 14:33:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chabert-Liddell", "Saint-Clair", ""], ["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""], ["Lazega", "Emmanuel", ""]]}, {"id": "1910.10581", "submitter": "Andrew R. McCluskey", "authors": "Andrew R. McCluskey, Thomas Arnold, Joshaniel F. K. Cooper, Tim Snow", "title": "A general approach to maximise information density in neutron\n  reflectometry analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft physics.comp-ph stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neutron and X-ray reflectometry are powerful techniques facilitating the\nstudy of the structure of interfacial materials. The analysis of these\ntechniques is ill-posed in nature requiring the application of a\nmodel-dependent methods. This can lead to the over- and under- analysis of\nexperimental data, when too many or too few parameters are allowed to vary in\nthe model. In this work, we outline a robust and generic framework for the\ndetermination of the set of free parameters that is capable of maximising the\nin-formation density of the model. This framework involves the determination of\nthe Bayesian evidence for each permutation of free parameters; and is applied\nto a simple phospholipid monolayer. We believe this framework should become an\nimportant component in reflectometry data analysis, and hope others more\nregularly consider the relative evidence for their analytical models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:33:59 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 10:58:14 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 11:04:56 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 19:52:13 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["McCluskey", "Andrew R.", ""], ["Arnold", "Thomas", ""], ["Cooper", "Joshaniel F. K.", ""], ["Snow", "Tim", ""]]}, {"id": "1910.10788", "submitter": "Maud Thomas", "authors": "Maud Thomas and Holger Rootz\\'en", "title": "Real-time prediction of severe influenza epidemics using Extreme Value\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, seasonal influenza epidemics cause hundreds of thousands of deaths\nworldwide and put high loads on health care systems. A main concern for\nresource planning is the risk of exceptionally severe epidemics. Taking\nadvantage of recent results on multivariate Generalized Pareto models in\nExtreme Value Statistics we develop methods for real-time prediction of the\nrisk that an ongoing influenza epidemic will be exceptionally severe and for\nreal-time detection of anomalous epidemics and use them for prediction and\ndetection of anomalies for influenza epidemics in France. Quality of\npredictions is assessed on observed and simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:03:55 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 09:43:10 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 07:43:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Thomas", "Maud", ""], ["Rootz\u00e9n", "Holger", ""]]}, {"id": "1910.10809", "submitter": "Laurent Barthes", "authors": "Mohamed Djallel Dilmi, Laurent Barth\\`es, C\\'ecile Mallet, Aymeric\n  Chazottes", "title": "Study of the impact of climate change on precipitation in Paris area\n  using method based on iterative multiscale dynamic time warping (IMS-DTW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the impact of climate change on precipitation is constrained by\nfinding a way to evaluate the evolution of precipitation variability over time.\nClassical approaches (feature-based) have shown their limitations for this\nissue due to the intermittent and irregular nature of precipitation. In this\nstudy, we present a novel variant of the Dynamic time warping method\nquantifying the dissimilarity between two rainfall time series based on shapes\ncomparisons, for clustering annual time series recorded at daily scale. This\nshape based approach considers the whole information (variability, trends and\nintermittency). We further labeled each cluster using a feature-based approach.\nWhile testing the proposed approach on the time series of Paris Montsouris, we\nfound that the precipitation variability increased over the years in Paris\narea.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:34:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Dilmi", "Mohamed Djallel", ""], ["Barth\u00e8s", "Laurent", ""], ["Mallet", "C\u00e9cile", ""], ["Chazottes", "Aymeric", ""]]}, {"id": "1910.10822", "submitter": "Erdem Varol", "authors": "Erdem Varol, Amin Nejatbakhsh", "title": "Wasserstein total variation filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we expand upon the theory of trend filtering by introducing\nthe use of the Wasserstein metric as a means to control the amount of\nspatiotemporal variation in filtered time series data. While trend filtering\nutilizes regularization to produce signal estimates that are piecewise linear,\nin the case of $\\ell_1$ regularization, or temporally smooth, in the case of\n$\\ell_2$ regularization, it ignores the topology of the spatial distribution of\nsignal. By incorporating the information about the underlying metric space of\nthe pixel layout, the Wasserstein metric is an attractive choice as a\nregularizer to undercover spatiotemporal trends in time series data. We\nintroduce a globally optimal algorithm for efficiently estimating the filtered\nsignal under a Wasserstein finite differences operator. The efficacy of the\nproposed algorithm in preserving spatiotemporal trends in time series video is\ndemonstrated in both simulated and fluorescent microscopy videos of the\nnematode caenorhabditis elegans and compared against standard trend filtering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 22:03:53 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Varol", "Erdem", ""], ["Nejatbakhsh", "Amin", ""]]}, {"id": "1910.10862", "submitter": "David Puelz", "authors": "David Puelz, Guillaume Basse, Avi Feller, Panos Toulis", "title": "A Graph-Theoretic Approach to Randomization Tests of Causal Effects\n  Under General Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference exists when a unit's outcome depends on another unit's treatment\nassignment. For example, intensive policing on one street could have a\nspillover effect on neighboring streets. Classical randomization tests\ntypically break down in this setting because many null hypotheses of interest\nare no longer sharp under interference. A promising alternative is to instead\nconstruct a conditional randomization test on a subset of units and assignments\nfor which a given null hypothesis is sharp. Finding these subsets is\nchallenging, however, and existing methods are limited to special cases or have\nlimited power. In this paper, we propose valid and easy-to-implement\nrandomization tests for a general class of null hypotheses under arbitrary\ninterference between units. Our key idea is to represent the hypothesis of\ninterest as a bipartite graph between units and assignments, and to find an\nappropriate biclique of this graph. Importantly, the null hypothesis is sharp\nwithin this biclique, enabling conditional randomization-based tests. We also\nconnect the size of the biclique to statistical power. Moreover, we can apply\noff-the-shelf graph clustering methods to find such bicliques efficiently and\nat scale. We illustrate our approach in settings with clustered interference\nand show advantages over methods designed specifically for that setting. We\nthen apply our method to a large-scale policing experiment in Medellin,\nColombia, where interference has a spatial structure.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:55:57 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:50:24 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 16:02:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Puelz", "David", ""], ["Basse", "Guillaume", ""], ["Feller", "Avi", ""], ["Toulis", "Panos", ""]]}, {"id": "1910.10938", "submitter": "Yuanyue Huang", "authors": "Yuanyue Huang, Haixiang Guo, Jing Yu, Shicheng Li, Zuozhi Zuo", "title": "Resilience for Landslide Geohazards and Promoting Strategies in the\n  Three Gorges Reservoir Area", "comments": "Dear readers of arxiv? these days I have been thinking about this\n  problem, and I found that, some points in this article should be changed,\n  which means that more experiments should be done. I want to widraw this paper\n  for a while", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, resilience is increasingly used as a concept for understanding\nnatural disaster systems. Landslide is one of the most frequent geohazards in\nthe Three Gorges Reservoir Area (TGRA).However, it is difficult to measure\nlocal disaster resilience, because of special geographical location in the TGRA\nand the special disaster landslide. Current approaches to disaster resilience\nevaluation are usually limited either by the qualitative method or properties\nof different disaster. Therefore, practical evaluating methods for the disaster\nresilience are needed. In this study, we developed an indicator system to\nevaluate landslides disaster resilience in the TGRE at the county level. It\nincludes two properties of inherent geological stress and external social\nresponse, which are summarized into physical stress and social forces. The\nevaluated disaster resilience can be simulated for promoting strategies with\nfuzzy cognitive map (FCM).\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:46:16 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 02:19:53 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Huang", "Yuanyue", ""], ["Guo", "Haixiang", ""], ["Yu", "Jing", ""], ["Li", "Shicheng", ""], ["Zuo", "Zuozhi", ""]]}, {"id": "1910.10993", "submitter": "Behnaz Pirzamanbein PhD", "authors": "Behnaz Pirzamanbein and Johan Lindstr\\\"om", "title": "Reconstruction of Past Human land-use from Pollen Data and Anthropogenic\n  land-cover Changes Scenarios", "comments": "5 Human land-use maps of Europe (1900 CE, 1725 CE, 1425 CE, 1000 BCE\n  and, 4000 BCE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate maps of past land cover and human land-use are necessary when\nstudying the impact of anthropogenic land-cover changes on climate. Ideally the\nmaps of past land cover would be separated into naturally occurring vegetation\nand human induced changes, allowing us to quantify the effect of human land-use\non past climate. Here we investigate the possibility of combining regional,\nfossil pollen based, land-cover reconstructions with, population based,\nestimates of past human land-use. By merging these two datasets and\ninterpolating the pollen based land-cover reconstructions we aim at obtaining\nmaps that provide both past natural land-cover and the anthropogenic land-cover\nchanges. We develop a Bayesian hierarchical model to handle the complex data,\nusing a latent Gaussian Markov random fields (GMRF) for the interpolation.\nEstimation of the model is based on a block updated Markov chain Monte Carlo\n(MCMC) algorithm. The sparse precision matrix of the GMRF together with an\nadaptive Metropolis adjusted Langevin step allows for fast inference.\nUncertainties in the land-use predictions are computed from the MCMC posterior\nsamples. The model uses the pollen based observations to reconstruct three\ncomposition of land cover; Coniferous forest, Broadleaved forest and\nUnforested/Open land. The unforested land is then further decomposed into\nnatural and human induced openness by inclusion of the estimates of past human\nland-use. The model is applied to five time periods - centred around 1900 CE,\n1725 CE, 1425 CE, 1000 and, 4000 BCE over Europe. The results suggest pollen\nbased observations can be used to recover past human land-use by adjusting the\npopulation based anthropogenic land-cover changes estimates.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:32:49 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pirzamanbein", "Behnaz", ""], ["Lindstr\u00f6m", "Johan", ""]]}, {"id": "1910.11374", "submitter": "Bruno Scalzo Dees", "authors": "Jean P. Chereau, Bruno Scalzo Dees, Danilo P. Mandic", "title": "Robust Principal Component Analysis Based On Maximum Correntropy Power\n  Iterations", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is recognised as a quintessential data\nanalysis technique when it comes to describing linear relationships between the\nfeatures of a dataset. However, the well-known sensitivity of PCA to\nnon-Gaussian samples and/or outliers often makes it unreliable in practice. To\nthis end, a robust formulation of PCA is derived based on the maximum\ncorrentropy criterion (MCC) so as to maximise the expected likelihood of\nGaussian distributed reconstruction errors. In this way, the proposed solution\nreduces to a generalised power iteration, whereby: (i) robust estimates of the\nprincipal components are obtained even in the presence of outliers; (ii) the\nnumber of principal components need not be specified in advance; and (iii) the\nentire set of principal components can be obtained, unlike existing approaches.\nThe advantages of the proposed maximum correntropy power iteration (MCPI) are\ndemonstrated through an intuitive numerical example.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:51:27 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Chereau", "Jean P.", ""], ["Dees", "Bruno Scalzo", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1910.11397", "submitter": "Stephen Lauer", "authors": "Stephen A. Lauer, Nicholas G. Reich, Laura B. Balzer", "title": "The covariate-adjusted residual estimator and its use in both randomized\n  trials and observational settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We often seek to estimate the causal effect of an exposure on a particular\noutcome in both randomized and observational settings. One such estimation\nmethod is the covariate-adjusted residuals estimator, which was designed for\nindividually or cluster randomized trials. In this manuscript, we study the\nproperties of this estimator and develop a new estimator that utilizes both\ncovariate adjustment and inverse probability weighting We support our\ntheoretical results with a simulation study and an application in an infectious\ndisease setting. The covariate-adjusted residuals estimator is an efficient and\nunbiased estimator of the average treatment effect in randomized trials;\nhowever, it is not guaranteed to be unbiased in observational studies. Our\nnovel estimator, the covariate-adjusted residuals estimator with inverse\nprobability weighting, is unbiased in randomized and observational settings,\nunder a reasonable set of assumptions. Furthermore, when these assumptions\nhold, it provides efficiency gains over inverse probability weighting in\nobservational studies. The covariate-adjusted residuals estimator is valid for\nuse in randomized trials, but should not be used in observational studies. The\ncovariate-adjusted residuals estimator with inverse probability weighting\nprovides an efficient alternative for use in randomized and observational\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 19:51:37 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Lauer", "Stephen A.", ""], ["Reich", "Nicholas G.", ""], ["Balzer", "Laura B.", ""]]}, {"id": "1910.11410", "submitter": "Richard Berk", "authors": "Richard A. Berk, Ayya A. Elzarka", "title": "Almost Politically Acceptable Criminal Justice Risk Assessment", "comments": "29 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In criminal justice risk forecasting, one can prove that it is impossible to\noptimize accuracy and fairness at the same time. One can also prove that it is\nimpossible optimize at once all of the usual group definitions of fairness. In\nthe policy arena, one is left with tradeoffs about which many stakeholders will\nadamantly disagree. In this paper, we offer a different approach. We do not\nseek perfectly accurate and fair risk assessments. We seek politically\nacceptable risk assessments. We describe and apply to data on 300,000 offenders\na machine learning approach that responds to many of the most visible charges\nof \"racial bias.\" Regardless of whether such claims are true, we adjust our\nprocedures to compensate. We begin by training the algorithm on White offenders\nonly and computing risk with test data separately for White offenders and Black\noffenders. Thus, the fitted algorithm structure is exactly the same for both\ngroups; the algorithm treats all offenders as if they are White. But because\nWhite and Black offenders can bring different predictors distributions to the\nwhite-trained algorithm, we provide additional adjustments if needed. Insofar\nare conventional machine learning procedures do not produce accuracy and\nfairness that some stakeholders require, it is possible to alter conventional\npractice to respond explicitly to many salient stakeholder claims even if they\nare unsupported by the facts. The results can be a politically acceptable risk\nassessment tools.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 20:13:57 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Berk", "Richard A.", ""], ["Elzarka", "Ayya A.", ""]]}, {"id": "1910.11474", "submitter": "Srijan Sengupta", "authors": "Jack Leitch, Kathleen A. Alexander, Srijan Sengupta", "title": "Toward epidemic thresholds on temporal networks: a review and open\n  questions", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological contact network models have emerged as an important tool in\nunderstanding and predicting the spread of infectious disease, due to their\ncapacity to engage individual heterogeneity that may underlie essential\ndynamics of a particular host-pathogen system. Just as fundamental are the\nchanges that real-world contact networks undergo over time, both independently\nof and in response to pathogen spreading. These dynamics play a central role in\ndetermining whether a disease will die out or become an epidemic within a\npopulation, known as the epidemic threshold. In this paper, we provide an\noverview of methods to predict the epidemic threshold for temporal contact\nnetwork models, and discuss areas that remain unexplored.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:04:14 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Leitch", "Jack", ""], ["Alexander", "Kathleen A.", ""], ["Sengupta", "Srijan", ""]]}, {"id": "1910.11518", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Beau David Barber, Berkley J. Walker", "title": "Calibration of inexact computer models with heteroscedastic errors", "comments": "51 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are commonly used to represent a wide range of real systems,\nbut they often involve some unknown parameters. Estimating the parameters by\ncollecting physical data becomes essential in many scientific fields, ranging\nfrom engineering to biology. However, most of the existing methods are\ndeveloped under the assumption that the physical data contains homoscedastic\nmeasurement errors. Motivated by an experiment of plant relative growth rates\nwhere replicates are available, we propose a new calibration method for inexact\ncomputer models with heteroscedastic measurement errors. Asymptotic properties\nof the parameter estimators are derived, and a goodness-of-fit test is\ndeveloped to detect the presence of heteroscedasticity. Numerical examples and\nempirical studies demonstrate that the proposed method not only yields accurate\nparameter estimation, but it also provides accurate predictions for physical\ndata in the presence of both heteroscedasticity and model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 04:17:04 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 23:32:32 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 09:52:17 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Sung", "Chih-Li", ""], ["Barber", "Beau David", ""], ["Walker", "Berkley J.", ""]]}, {"id": "1910.11524", "submitter": "Ronaldo Vigo", "authors": "Ronaldo Vigo", "title": "A Simple Descriptive Method & Standard for Comparing Pairs of Stacked\n  Bar Graphs", "comments": "Brief informal note, 5 pages, 1 figure, 5 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a plethora of research has been devoted to extoling the power and\nimportance of data visualization, research on the effectiveness of data\nvisualization methods from a human perceptual, and more generally, a cognitive\nstandpoint remains largely untapped. Indeed, the way that human observers\nperceive and judge graphic charts can determine the interpretation of the\ngraphed data. In this brief note we introduce a simple method for comparing\nstacked bar graphs based on a well-known result from cognitive science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 04:48:39 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Vigo", "Ronaldo", ""]]}, {"id": "1910.11743", "submitter": "The Tien Mai", "authors": "The Tien Mai and Paul Turner and Jukka Corander", "title": "Boosting heritability: estimating the genetic component of phenotypic\n  variation with multiple sample splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Heritability is a central measure in genetics quantifying how\nmuch of the variability observed in a trait is attributable to genetic\ndifferences. Existing methods for estimating heritability are most often based\non random-effect models, typically for computational reasons. The alternative\nof using a fixed-effect model has received much more limited attention in the\nliterature. Results: In this paper, we propose a generic strategy for\nheritability inference, termed as ``boosting heritability\", by combining the\nadvantageous features of different recent methods to produce an estimate of the\nheritability with a high-dimensional linear model. Boosting heritability uses\nin particular a multiple sample splitting strategy which leads in general to a\nstable and and accurate estimate. We use both simulated data and real\nantibiotic resistance data from a major human pathogen, Sptreptococcus\npneumoniae, to demonstrate the attractive features of our inference strategy.\nConclusions: Boosting is shown to offer a reliable and practically useful tool\nfor inference about heritability.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:14:08 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 09:14:21 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 16:21:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mai", "The Tien", ""], ["Turner", "Paul", ""], ["Corander", "Jukka", ""]]}, {"id": "1910.11826", "submitter": "Andrea Raffo", "authors": "Andrea Raffo and Silvia Biasotti", "title": "Weighted Quasi Interpolant Spline Approximations: Properties and\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1007/s11075-020-00989-4", "report-no": null, "categories": "math.NA cs.CG cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous representations are fundamental for modeling sampled data and\nperforming computations and numerical simulations directly on the model or its\nelements. To effectively and efficiently address the approximation of point\nclouds we propose the Weighted Quasi Interpolant Spline Approximation method\n(wQISA). We provide global and local bounds of the method and discuss how it\nstill preserves the shape properties of the classical quasi-interpolation\nscheme. This approach is particularly useful when the data noise can be\nrepresented as a probabilistic distribution: from the point of view of\nnonparametric regression, the wQISA estimator is robust to random\nperturbations, such as noise and outliers. Finally, we show the effectiveness\nof the method with several numerical simulations on real data, including curve\nfitting on images, surface approximation and simulation of rainfall\nprecipitations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:14:09 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 16:49:01 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 17:15:23 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Raffo", "Andrea", ""], ["Biasotti", "Silvia", ""]]}, {"id": "1910.11972", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Kernel Optimal Orthogonality Weighting: A Balancing Approach to\n  Estimating Effects of Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific questions require estimating the effects of continuous\ntreatments. Outcome modeling and weighted regression based on the generalized\npropensity score are the most commonly used methods to evaluate continuous\neffects. However, these techniques may be sensitive to model misspecification,\nextreme weights or both. In this paper, we propose Kernel Optimal Orthogonality\nWeighting (KOOW), a convex optimization-based method, for estimating the\neffects of continuous treatments. KOOW finds weights that minimize the\nworst-case penalized functional covariance between the continuous treatment and\nthe confounders. By minimizing this quantity, KOOW successfully provides\nweights that orthogonalize confounders and the continuous treatment, thus\nproviding optimal covariate balance, while controlling for extreme weights. We\nvaluate its comparative performance in a simulation study. Using data from the\nWomen's Health Initiative observational study, we apply KOOW to evaluate the\neffect of red meat consumption on blood pressure.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 01:03:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1910.11987", "submitter": "Tony Wong", "authors": "Tony E. Wong, Ying Cui, Dana L. Royer, Klaus Keller", "title": "A tighter constraint on Earth-system sensitivity from long-term\n  temperature and carbon-cycle observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The long-term temperature response to a given change in CO2 forcing, or\nEarth-system sensitivity (ESS), is a key parameter quantifying our\nunderstanding about the relationship between changes in Earth's radiative\nforcing and the resulting long-term Earth-system response. Current ESS\nestimates are subject to sizable uncertainties. Long-term carbon cycle models\ncan provide a useful avenue to constrain ESS, but previous efforts either use\nrather informal statistical approaches or focus on discrete paleoevents. Here,\nwe improve on previous ESS estimates by using a Bayesian approach to fuse\ndeep-time CO2 and temperature data over the last 420 Myrs with a long-term\ncarbon cycle model. Our median ESS estimate of 3.4 deg C (2.6-4.7 deg C; 5-95%\nrange) shows a narrower range than previous assessments. We show that weaker\nchemical weathering relative to the a priori model configuration via reduced\nweatherable land area yields better agreement with temperature records during\nthe Cretaceous. Research into improving the understanding about these\nweathering mechanisms hence provides potentially powerful avenues to further\nconstrain this fundamental Earth-system property.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 03:37:34 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 01:40:28 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 11:52:36 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 18:33:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wong", "Tony E.", ""], ["Cui", "Ying", ""], ["Royer", "Dana L.", ""], ["Keller", "Klaus", ""]]}, {"id": "1910.12058", "submitter": "Johnatan Cardona Jim\\'enez", "authors": "Johnatan Cardona Jim\\'enez, Carlos A. de B. Pereira, Victor Fossaluza", "title": "Assessing Dynamic Effects on a Bayesian Matrix-Variate Dynamic Linear\n  Model: an Application to fMRI Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a modeling procedure for fMRI data analysis using a\nBayesian Matrix-Variate Dynamic Linear Model (MVDLM). With this type of model,\nless complex than the more traditional temporal-spatial models, we are able to\ntake into account the temporal and -- at least locally -- the spatial\nstructures that are usually present in this type of data. Despite employing a\nvoxel-wise approach, every voxel in the brain is jointly modeled with its\nnearest neighbors, which are defined through a euclidian metric. MVDLM's have\nbeen widely used in applications where the interest lies in to perform\npredictions and/or analysis of covariance structures among time series. In this\ncontext, our interest is rather to assess the dynamic effects which are related\nto voxel activation. In order to do so, we develop three algorithms to simulate\nonline-trajectories related to the state parameter and with those curves or\nsimulated trajectories we compute a Monte Carlo evidence for voxel activation.\nThrough two practical examples and two different types of assessments, we show\nthat our method can be viewed for the practitioners as a reliable tool for fMRI\ndata analysis. Despite all the examples and analysis are illustrated just for a\nsingle subject analysis, we also describe how more general group analysis can\nbe implemented.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 12:59:05 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 16:14:51 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Jim\u00e9nez", "Johnatan Cardona", ""], ["Pereira", "Carlos A. de B.", ""], ["Fossaluza", "Victor", ""]]}, {"id": "1910.12090", "submitter": "Belhal Karimi", "authors": "Belhal Karimi and Marc Lavielle", "title": "Efficient Metropolis-Hastings Sampling for Nonlinear Mixed Effects\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate samples of the random effects from their conditional\ndistributions is fundamental for inference in mixed effects models. Random walk\nMetropolis is widely used to conduct such sampling, but such a method can\nconverge slowly for medium dimension problems, or when the joint structure of\nthe distributions to sample is complex. We propose a Metropolis Hastings (MH)\nalgorithm based on a multidimensional Gaussian proposal that takes into account\nthe joint conditional distribution of the random effects and does not require\nany tuning, in contrast with more sophisticated samplers such as the Metropolis\nAdjusted Langevin Algorithm or the No-U-Turn Sampler that involve costly tuning\nruns or intensive computation. Indeed, this distribution is automatically\nobtained thanks to a Laplace approximation of the original model. We show that\nsuch approximation is equivalent to linearizing the model in the case of\ncontinuous data. Numerical experiments based on real data highlight the very\ngood performances of the proposed method for continuous data model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:50:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Lavielle", "Marc", ""]]}, {"id": "1910.12128", "submitter": "Subhadeep Paul", "authors": "Selena Shuo Wang, Subhadeep Paul, Paul De Boeck", "title": "Joint Latent Space Model for Social Networks with Multivariate\n  Attributes", "comments": "A previous version of this paper (version 1) used a different\n  application problem and dataset, and also had a slightly different title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application problems in social, behavioral, and economic sciences,\nresearchers often have data on a social network among a group of individuals\nalong with high dimensional multivariate measurements for each individual. To\nanalyze such networked data structures, we propose a joint Attribute and Person\nLatent Space Model (APLSM) that summarizes information from the social network\nand the multiple attribute measurements in a person-attribute joint latent\nspace. We develop a Variational Bayesian Expectation-Maximization estimation\nalgorithm to estimate the posterior distribution of the attribute and person\nlocations in the joint latent space. This methodology allows for effective\nintegration, informative visualization, and prediction of social networks and\nhigh dimensional attribute measurements. Using APLSM, we explore the inner\nworkings of the French financial elites based on their social networks and\ntheir career, political views, and social status. We observe a division in the\nsocial circles of the French elites in accordance with the differences in their\nindividual characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 20:12:09 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:11:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Selena Shuo", ""], ["Paul", "Subhadeep", ""], ["De Boeck", "Paul", ""]]}, {"id": "1910.12168", "submitter": "Yicheng Li", "authors": "Yicheng Li and Adrian E. Raftery", "title": "Accounting for Smoking in Forecasting Mortality and Life Expectancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoking is one of the main risk factors that has affected human mortality and\nlife expectancy over the past century. Smoking accounts for a large part of the\nnonlinearities in the growth of life expectancy and of the geographic and sex\ndifferences in mortality. As Bongaarts (2006) and Janssen (2018) suggested,\naccounting for smoking could improve the quality of mortality forecasts due to\nthe predictable nature of the smoking epidemic. We propose a new Bayesian\nhierarchical model to forecast life expectancy at birth for both sexes and for\n69 countries with good data on smoking-related mortality. The main idea is to\nconvert the forecast of the non-smoking life expectancy at birth (i.e., life\nexpectancy at birth removing the smoking effect) into life expectancy forecast\nthrough the use of the age-specific smoking attributable fraction (ASSAF). We\nintroduce a new age-cohort model for the ASSAF and a Bayesian hierarchical\nmodel for non-smoking life expectancy at birth. The forecast performance of the\nproposed method is evaluated by out-of-sample validation compared with four\nother commonly used methods for life expectancy forecasting. Improvements in\nforecast accuracy and model calibration based on the new method are observed.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 03:26:21 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Yicheng", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1910.12174", "submitter": "Satoshi Morita", "authors": "Satoshi Morita, Peter M\\\"uller, Hiroyasu Abe", "title": "A Semi-parametric Bayesian Approach to Population Finding with\n  Time-to-Event and Toxicity Data in a Randomized Clinical Trial", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A utility-based Bayesian population finding (BaPoFi) method was proposed by\nMorita and M\\\"uller (2017, Biometrics, 1355-1365) to analyze data from a\nrandomized clinical trial with the aim of identifying good predictive baseline\ncovariates for optimizing the target population for a future study. The\napproach casts the population finding process as a formal decision problem\ntogether with a flexible probability model using a random forest to define a\nregression mean function. BaPoFi is constructed to handle a single continuous\nor binary outcome variable. In this paper, we develop BaPoFi-TTE as an\nextension of the earlier approach for clinically important cases of\ntime-to-event (TTE) data with censoring, and also accounting for a toxicity\noutcome. We model the association of TTE data with baseline covariates using a\nsemi-parametric failure time model with a P\\'olya tree prior for an unknown\nerror term and a random forest for a flexible regression mean function. We\ndefine a utility function that addresses a trade-off between efficacy and\ntoxicity as one of the important clinical considerations for population\nfinding. We examine the operating characteristics of the proposed method in\nextensive simulation studies. For illustration, we apply the proposed method to\ndata from a randomized oncology clinical trial. Concerns in a preliminary\nanalysis of the same data based on a parametric model motivated the proposed\nmore general approach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 03:56:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Morita", "Satoshi", ""], ["M\u00fcller", "Peter", ""], ["Abe", "Hiroyasu", ""]]}, {"id": "1910.12222", "submitter": "Belhal Karimi", "authors": "Belhal Karimi, Marc Lavielle, Eric Moulines", "title": "f-SAEM: A fast Stochastic Approximation of the EM algorithm for\n  nonlinear mixed effects models", "comments": "Accepted in CSDA 2020, 35 pages. arXiv admin note: text overlap with\n  arXiv:1910.12090", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate samples of the random effects from their conditional\ndistributions is fundamental for inference in mixed effects models. Random walk\nMetropolis is widely used to perform such sampling, but this method is known to\nconverge slowly for medium dimensional problems, or when the joint structure of\nthe distributions to sample is spatially heterogeneous. The main contribution\nconsists of an independent Metropolis-Hastings (MH) algorithm based on a\nmultidimensional Gaussian proposal that takes into account the joint\nconditional distribution of the random effects and does not require any tuning.\nIndeed, this distribution is automatically obtained thanks to a Laplace\napproximation of the incomplete data model. Such approximation is shown to be\nequivalent to linearizing the structural model in the case of continuous data.\nNumerical experiments based on simulated and real data illustrate the\nperformance of the proposed methods. For fitting nonlinear mixed effects\nmodels, the suggested MH algorithm is efficiently combined with a stochastic\napproximation version of the EM algorithm for maximum likelihood estimation of\nthe global parameters.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 09:56:54 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Lavielle", "Marc", ""], ["Moulines", "Eric", ""]]}, {"id": "1910.12267", "submitter": "Andreas Kryger Jensen", "authors": "Andreas Kryger Jensen and Theis Lange", "title": "A novel high-power test for continuous outcomes truncated by death", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient reported outcomes including quality of life (QoL) assessments are\nincreasingly being included as either primary or secondary outcomes in\nrandomized controlled trials. While making the outcomes more relevant for\npatients it entails a challenge in cases where death or a similar event makes\nthe outcome of interest undefined. A pragmatic - and much used - solution is to\nassign diseased patient with the lowest possible QoL score. This makes medical\nsense, but creates a statistical problem since traditional tests such as\nt-tests or Wilcox tests potentially looses large amounts of statistical power.\nIn this paper we propose a novel test that can keep the medical relevant\ncomposite outcome, but preserve full statistical power. The test is also\napplicable in other situations where a specific value (say 0 days alive outside\nhospitals) encodes a special meaning. The test is implemented in an R package\nwhich is available for download.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 14:23:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jensen", "Andreas Kryger", ""], ["Lange", "Theis", ""]]}, {"id": "1910.12337", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande and Katherine Evans", "title": "Expected Hypothetical Completion Probability", "comments": "This paper elaborates on work done for the NFL 2019 Big Data Bowl\n  contest. Manuscript accepted at the Journal of Quantitative Analysis in\n  Sports", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using high-resolution player tracking data made available by the National\nFootball League (NFL) for their 2019 Big Data Bowl competition, we introduce\nthe Expected Hypothetical Completion Probability (EHCP), a objective framework\nfor evaluating plays. At the heart of EHCP is the question \"on a given passing\nplay, did the quarterback throw the pass to the receiver who was most likely to\ncatch it?\" To answer this question, we first built a Bayesian non-parametric\ncatch probability model that automatically accounts for complex interactions\nbetween inputs like the receiver's speed and distances to the ball and nearest\ndefender. While building such a model is, in principle, straightforward, using\nit to reason about a hypothetical pass is challenging because many of the model\ninputs corresponding to a hypothetical are necessarily unobserved. To wit, it\nis impossible to observe how close an un-targeted receiver would be to his\nnearest defender had the pass been thrown to him instead of the receiver who\nwas actually targeted. To overcome this fundamental difficulty, we propose\nimputing the unobservable inputs and averaging our model predictions across\nthese imputations to derive EHCP. In this way, EHCP can track how the\ncompletion probability evolves for each receiver over the course of a play in a\nway that accounts for the uncertainty about missing inputs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:07:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Evans", "Katherine", ""]]}, {"id": "1910.12372", "submitter": "Saptarshi Roy", "authors": "Saptarshi Roy, Kaustav Chakraborty, Somnath Bhadra and Ayanendranath\n  Basu", "title": "Density Power Downweighting and Robust Inference: Some New Strategies", "comments": "33 Pages, 6 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving the robustness of the procedure has, at the present time, become\nalmost a default requirement for statistical data analysis. Since efficiency at\nthe model and robustness under misspecification of the model are often in\nconflict, it is important to choose such inference procedures which provide the\nbest compromise between these two concepts. Some minimum Bregman divergence\nestimators and related tests of hypothesis seem to be able to do well in this\nrespect, with the procedures based on the density power divergence providing\nthe existing standard. In this paper we propose a new family of Bregman\ndivergences which is a superfamily encompassing the density power divergence.\nThis paper describes the inference procedures resulting from this new family of\ndivergences, and makes a strong case for the utility of this divergence family\nin statistical inference.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 22:37:58 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Roy", "Saptarshi", ""], ["Chakraborty", "Kaustav", ""], ["Bhadra", "Somnath", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1910.12487", "submitter": "Marco Helbich", "authors": "Paulien Hagedoorn, Peter Groenewegen, Hannah Roberts, Marco Helbich", "title": "Is suicide mortality associated with neighbourhood social fragmentation\n  and deprivation? A Dutch register-based case-control study using\n  individualized neighbourhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Neighbourhood social fragmentation and socioeconomic deprivation\nseem to be associated with suicide mortality. However, results are\ninconclusive, which might be because dynamics in the social context are not\nwell-represented by administratively bounded neighbourhoods at baseline. We\nused individualized neighbourhoods to examine associations between suicide\nmortality, social fragmentation, and deprivation for the total population as\nwell as by sex and age group. Methods: Using a nested case-control design, all\nsuicides aged 18-64 years between 2007 and 2016 were selected from longitudinal\nDutch register data and matched with 10 random controls. Indices for social\nfragmentation and deprivation were calculated annually for 300, 600, and 1,000\nmetre circular buffers around each subject's residential address. Results:\nSuicide mortality was significantly higher in neighbourhoods with high\ndeprivation and social fragmentation. Accounting for individual characteristics\nlargely attenuated these associations. Suicide mortality remained significantly\nhigher for women living in highly fragmented neighbourhoods in the fully\nadjusted model. Age-stratified analyses indicate associations with\nneighbourhood fragmentation among women in older age groups (40-64 years) only.\nAmong men, suicide risk was lower in fragmented neighbourhoods for\n18-39-year-olds and for short-term residents. In deprived neighbourhoods, the\nsuicide risk was lower for 40-64-year-old men and long-term residents.\nAssociations between neighbourhood characteristics and suicide mortality were\ncomparable across buffer sizes. Conclusion: Our findings suggest that next to\nindividual characteristics, the social and economic context within which people\nlive may both enhance and buffer the risk of suicide.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 08:03:20 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Hagedoorn", "Paulien", ""], ["Groenewegen", "Peter", ""], ["Roberts", "Hannah", ""], ["Helbich", "Marco", ""]]}, {"id": "1910.12575", "submitter": "Gabriel Riutort Mayol", "authors": "Gabriel Riutort-Mayol, Virgilio G\\'omez-Rubio, Jos\\'e Luis Lerma,\n  Julio M. del Hoyo-Mel\\'endez", "title": "Correlated functional models with derivative information for modeling\n  MFS data on rock art paintings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microfading Spectrometry (MFS) is a method for assessing light sensitivity\ncolor (spectral) variations of cultural heritage objects. Each measured point\non the surface gives rise to a time-series of stochastic observations that\nrepresents color fading over time. Color degradation is expected to be\nnon-decreasing as a function of time and stabilize eventually. These properties\ncan be expressed in terms of the derivatives of the functions. In this work, we\npropose spatially correlated splines-based time-varying functions and their\nderivatives for modeling and predicting MFS data collected on the surface of\nrock art paintings. The correlation among the splines models is modeled using\nGaussian process priors over the spline coefficients across time-series. A\nmultivariate covariance function in a Gaussian process allows the use of\ntrichromatic image color variables jointly with spatial locations as inputs to\nevaluate the correlation among time-series, and demonstrated the colorimetric\nvariables as useful for predicting new color fading time-series. Furthermore,\nmodeling the derivative of the model and its sign demonstrated to be beneficial\nin terms of both predictive performance and application-specific\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 12:14:02 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Riutort-Mayol", "Gabriel", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Lerma", "Jos\u00e9 Luis", ""], ["del Hoyo-Mel\u00e9ndez", "Julio M.", ""]]}, {"id": "1910.12710", "submitter": "Sonia Khier", "authors": "Marc Vincent, Olivier Mathieu (GReD), Patrick Nolain, Cecilia Menac\\'e\n  (CHRU Montpellier), Sonia Khier (IMAG)", "title": "Population pharmacokinetics of levobupivacaine during a transversus\n  abdominis plane block in children", "comments": null, "journal-ref": "Therapeutic Drug Monitoring, Lippincott, Williams & Wilkins, 2019,\n  pp.1", "doi": "10.1097/FTD.0000000000000702", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND:Levobupivacaine is commonly used during transversus abdominis\nplane block in pediatric patients. However, the dosing regimen is still\nempirical, and the pharmacokinetic properties of levobupivacaine are not\nconsidered. Here, the pharmacokinetics of levobupivacaine during an\nultrasound-guided transversus abdominis plane block were evaluated to optimize\ndosing regimen, with regard to the between-subject variability and the volume\nof levobupivacaine injected.METHOD:The clinical trial (prospective, randomized,\ndouble-blind study protocol) was conducted in 40 children aged 1 to 5 years,\nwho were scheduled for inguinal surgery. Each patient received 0.4 mg/kg of\nlevobupivacaine with a volume of local anesthesia solution adjusted to 0.2\nmL/kg of 0.2% or 0.4 mL/kg of 0.1% levobupivacaine. Blood samples were\ncollected at 5, 15, 20, 25, 30, 45, 60, and 75 min following the block\ninjection. The population pharmacokinetic analysis was performed using the\nNONMEM software.RESULTS:From the pharmacokinetic parameters obtained, median\nCmax, tmax, and area under the concentration versus time curve were 0.315 mg/L,\n17 min, and 41 mg/L. min, respectively. Between-subject variability (BSV) of\nclearance was explained by weight. At the dose regimen of 0.4 mg/kg, none of\nthe infants showed signs of toxicity, but in 13 patients, transversus abdominis\nplane block failed. After analysis, BSV for absorption rate constant,\ndistribution volume, and clearance were 81%, 47%, and 41%, respectively.\nResidual unexplained variability was estimated to be 14%.CONCLUSION:For\nimproved efficiency in the pediatric population, the dose of levobupivacaine\nshould be greater than 0.4 mg/kg. Children's weight should be considered to\nanticipate any risk of toxicity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:30:48 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Vincent", "Marc", "", "GReD"], ["Mathieu", "Olivier", "", "GReD"], ["Nolain", "Patrick", "", "CHRU Montpellier"], ["Menac\u00e9", "Cecilia", "", "CHRU Montpellier"], ["Khier", "Sonia", "", "IMAG"]]}, {"id": "1910.12895", "submitter": "Azra Bihorac", "authors": "Shounak Datta, Tyler J. Loftus, Matthew M. Ruppert, Chris Giordano,\n  Lasith Adhikari, Ying-Chih Peng, Yuanfang Ren, Benjamin Shickel, Zheng Feng,\n  Gloria Lipori, Gilbert R. Upchurch Jr., Xiaolin Li, Parisa Rashidi, Tezcan\n  Ozrazgat-Baslanti, and Azra Bihorac", "title": "Added Value of Intraoperative Data for Predicting Postoperative\n  Complications: Development and Validation of a MySurgeryRisk Extension", "comments": "46 pages,8 figures, 7 tables version 2: corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To test the hypothesis that accuracy, discrimination, and precision in\npredicting postoperative complications improve when using both preoperative and\nintraoperative data input features versus preoperative data alone. Models that\npredict postoperative complications often ignore important intraoperative\nphysiological changes. Incorporation of intraoperative physiological data may\nimprove model performance. This retrospective cohort analysis included 52,529\ninpatient surgeries at a single institution during a 5 year period. Random\nforest machine learning models in the validated MySurgeryRisk platform made\npatient-level predictions for three postoperative complications and mortality\nduring hospital admission using electronic health record data and patient\nneighborhood characteristics. For each outcome, one model trained with\npreoperative data alone and one model trained with both preoperative and\nintraoperative data. Models were compared by accuracy, discrimination\n(expressed as AUROC), precision (expressed as AUPRC), and reclassification\nindices (NRI). Machine learning models incorporating both preoperative and\nintraoperative data had greater accuracy, discrimination, and precision than\nmodels using preoperative data alone for predicting all three postoperative\ncomplications (intensive care unit length of stay >48 hours, mechanical\nventilation >48 hours, and neurological complications including delirium) and\nin-hospital mortality (accuracy: 88% vs. 77%, AUROC: 0.93 vs. 0.87, AUPRC: 0.21\nvs. 0.15). Overall reclassification improvement was 2.9-10.0% for complications\nand 11.2% for in-hospital mortality. Incorporating both preoperative and\nintraoperative data significantly increased accuracy, discrimination, and\nprecision for machine learning models predicting postoperative complications.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:02:46 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 16:03:10 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Datta", "Shounak", ""], ["Loftus", "Tyler J.", ""], ["Ruppert", "Matthew M.", ""], ["Giordano", "Chris", ""], ["Adhikari", "Lasith", ""], ["Peng", "Ying-Chih", ""], ["Ren", "Yuanfang", ""], ["Shickel", "Benjamin", ""], ["Feng", "Zheng", ""], ["Lipori", "Gloria", ""], ["Upchurch", "Gilbert R.", "Jr."], ["Li", "Xiaolin", ""], ["Rashidi", "Parisa", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bihorac", "Azra", ""]]}, {"id": "1910.12927", "submitter": "Petr Plechac", "authors": "Petr Plech\\'a\\v{c}, Andrew Cooper, Benjamin Nagy, Artjoms \\v{S}ela", "title": "Reply to: Large-scale quantitative profiling of the Old English verse\n  tradition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Nature Human Behaviour 3/2019, an article was published entitled\n\"Large-scale quantitative profiling of the Old English verse tradition\" dealing\nwith (besides other things) the question of the authorship of the Old English\npoem Beowulf. The authors provide various textual measurements that they claim\npresent \"serious obstacles to those who would advocate for composite authorship\nor scribal recomposition\" (p. 565). In what follows we raise doubts about their\nmethods and address serious errors in both their data and their code. We show\nthat reliable stylometric methods actually identify significant stylistic\nheterogeneity in Beowulf. In what follows we discuss each method separately\nfollowing the order of the original article.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:35:55 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Plech\u00e1\u010d", "Petr", ""], ["Cooper", "Andrew", ""], ["Nagy", "Benjamin", ""], ["\u0160ela", "Artjoms", ""]]}, {"id": "1910.13080", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Masahiko Gosho, Ryota Ishii, Koji Oba and Toshi A.\n  Furukawa", "title": "Outlier detection and influence diagnostics in network meta-analysis", "comments": null, "journal-ref": "Res Synth Methods 2020;11(6):891-902", "doi": "10.1002/jrsm.1455", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis has been gaining prominence as an evidence synthesis\nmethod that enables the comprehensive synthesis and simultaneous comparison of\nmultiple treatments. In many network meta-analyses, some of the constituent\nstudies may have markedly different characteristics from the others, and may be\ninfluential enough to change the overall results. The inclusion of these\n\"outlying\" studies might lead to biases, yielding misleading results. In this\narticle, we propose effective methods for detecting outlying and influential\nstudies in a frequentist framework. In particular, we propose suitable\ninfluence measures for network meta-analysis models that involve missing\noutcomes and adjust the degree of freedoms appropriately. We propose three\ninfluential measures by a leave-one-trial-out cross-validation scheme: (1)\ncomparison-specific studentized residual, (2) relative change measure for\ncovariance matrix of the comparative effectiveness parameters, (3) relative\nchange measure for heterogeneity covariance matrix. We also propose (4) a\nmodel-based approach using a likelihood ratio statistic by a mean-shifted\noutlier detection model. We illustrate the effectiveness of the proposed\nmethods via applications to a network meta-analysis of antihypertensive drugs.\nUsing the four proposed methods, we could detect three potential influential\ntrials involving an obvious outlier that was retracted because of data\nfalsifications. We also demonstrate that the overall results of comparative\nefficacy estimates and the ranking of drugs were altered by omitting these\nthree influential studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 04:29:05 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Noma", "Hisashi", ""], ["Gosho", "Masahiko", ""], ["Ishii", "Ryota", ""], ["Oba", "Koji", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1910.13301", "submitter": "Zhenzhong Wang", "authors": "Zhenzhong Wang, Yundong Tu, Song Xi Chen", "title": "Analyzing China's Consumer Price Index Comparatively with that of United\n  States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a thorough analysis on the dynamic structures and\npredictability of China's Consumer Price Index (CPI-CN), with a comparison to\nthose of the United States. Despite the differences in the two leading\neconomies, both series can be well modeled by a class of Seasonal\nAutoregressive Integrated Moving Average Model with Covariates (S-ARIMAX). The\nCPI-CN series possess regular patterns of dynamics with stable annual cycles\nand strong Spring Festival effects, with fitting and forecasting errors largely\ncomparable to their US counterparts. Finally, for the CPI-CN, the diffusion\nindex (DI) approach offers improved predictions than the S-ARIMAX models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:43:40 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wang", "Zhenzhong", ""], ["Tu", "Yundong", ""], ["Chen", "Song Xi", ""]]}, {"id": "1910.13347", "submitter": "Laura Albert", "authors": "Laura A. Albert and John N. Angelis", "title": "Jump balls, rating falls, and elite status: A sensitivity analysis of\n  three quarterback rating statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quarterback performance can be difficult to rank, and much effort has been\nspent in creating new rating systems. However, the input statistics for such\nratings are subject to randomness and factors outside the quarterback's\ncontrol. To investigate this variance, we perform a sensitivity analysis of\nthree quarterback rating statistics: the Traditional 1971 rating by Smith, the\nBurke, and the Wages of Wins ratings. The comparisons are made at the team\nlevel for the 32 NFL teams from 2002-2015, thus giving each case an even 16\ngames. We compute quarterback ratings for each offense with 1-5 additional\ntouchdowns, 1-5 fewer interceptions, 1-5 additional sacks, and a 1-5 percent\nincrease in the passing completion rate. Our sensitivity analysis provides\ninsight into whether an elite passing team could seem mediocre or vice versa\nbased on random outcomes. The results indicate that the Traditional rating is\nthe most sensitive statistic with respect to touchdowns, interceptions, and\ncompletions, whereas the Burke rating is most sensitive to sacks. The analysis\nsuggests that team passing offense rankings are highly sensitive to aspects of\nfootball that are out of the quarterback's hands (e.g., deflected passes that\nlead to interceptions). Thus, on the margins, we show arguments about whether a\nspecific quarterback has entered the elite or remains mediocre are irrelevant.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:19:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Albert", "Laura A.", ""], ["Angelis", "John N.", ""]]}, {"id": "1910.13632", "submitter": "Gaoxiang Jia", "authors": "Gaoxiang Jia, Xinlei Wang, Qiwei Li, Wei Lu, Ximing Tang, Ignacio\n  Wistuba, and Yang Xie", "title": "RCRnorm: An integrated system of random-coefficient hierarchical\n  regression models for normalizing NanoString nCounter data", "comments": null, "journal-ref": "Ann. Appl. Stat. 13 (2019), no. 3, 1617--1647.\n  https://projecteuclid.org/euclid.aoas/1571277766", "doi": "10.1214/19-AOAS1249", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Formalin-fixed paraffin-embedded (FFPE) samples have great potential for\nbiomarker discovery, retrospective studies and diagnosis or prognosis of\ndiseases. Their application, however, is hindered by the unsatisfactory\nperformance of traditional gene expression profiling techniques on damaged\nRNAs. NanoString nCounter platform is well suited for profiling of FFPE samples\nand measures gene expression with high sensitivity which may greatly facilitate\nrealization of scientific and clinical values of FFPE samples. However,\nmethodological development for normalization, a critical step when analyzing\nthis type of data, is far behind. Existing methods designed for the platform\nuse information from different types of internal controls separately and rely\non an overly-simplified assumption that expression of housekeeping genes is\nconstant across samples for global scaling. Thus, these methods are not\noptimized for the nCounter system, not mentioning that they were not developed\nfor FFPE samples. We construct an integrated system of random-coefficient\nhierarchical regression models to capture main patterns and characteristics\nobserved from NanoString data of FFPE samples and develop a Bayesian approach\nto estimate parameters and normalize gene expression across samples. Our\nmethod, labeled RCRnorm, incorporates information from all aspects of the\nexperimental design and simultaneously removes biases from various sources. It\neliminates the unrealistic assumption on housekeeping genes and offers great\ninterpretability. Furthermore, it is applicable to freshly frozen or like\nsamples that can be generally viewed as a reduced case of FFPE samples.\nSimulation and applications showed the superior performance of RCRnorm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:38:40 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Jia", "Gaoxiang", ""], ["Wang", "Xinlei", ""], ["Li", "Qiwei", ""], ["Lu", "Wei", ""], ["Tang", "Ximing", ""], ["Wistuba", "Ignacio", ""], ["Xie", "Yang", ""]]}, {"id": "1910.13936", "submitter": "Tanzy Love", "authors": "Valeriia Sherina, Matthew N. McCall, and Tanzy M. T. Love", "title": "Fully Bayesian imputation model for non-random missing data in qPCR", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical approach to obtain differential gene expression\nof non-detects in quantitative real-time PCR (qPCR) experiments through\nBayesian hierarchical modeling. We propose to treat non-detects as non-random\nmissing data, model the missing data mechanism, and use this model to impute Ct\nvalues or obtain direct estimates of relevant model parameters. A typical\nlaboratory does not have the resources to perform experiments with a large\nnumber of replicates; therefore, we propose an approach that does not rely on\nlarge sample theory. We aim to demonstrate the possibilities that exist for\nanalyzing qPCR data in the presence of non-random missingness through the use\nof Bayesian estimation. Bayesian analysis typically allows for smaller data\nsets to be analyzed without losing power while retaining precision. The heart\nof Bayesian estimation is that everything that is known about a parameter\nbefore observing the data (the prior) is combined with the information from the\ndata itself (the likelihood), resulting in updated knowledge about the\nparameter (the posterior). In this work we introduce and describe our\nhierarchical model and chosen prior distributions, assess the model sensitivity\nto the choice of prior, perform convergence diagnostics for the Markov Chain\nMonte Carlo, and present the results of a real data application.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:43:52 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Sherina", "Valeriia", ""], ["McCall", "Matthew N.", ""], ["Love", "Tanzy M. T.", ""]]}, {"id": "1910.14026", "submitter": "Federico Orsini", "authors": "Federico Orsini, Massimiliano Gastaldi, Luca Mantecchini, Riccardo\n  Rossi", "title": "Neural networks trained with WiFi traces to predict airport passenger\n  behavior", "comments": "Post-print of paper presented at the 2019 6th International\n  Conference on Models and Technologies for Intelligent Transportation Systems\n  (MT-ITS)", "journal-ref": "2019 6th International Conference on Models and Technologies for\n  Intelligent Transportation Systems (MT-ITS)", "doi": "10.1109/MTITS.2019.8883365", "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of neural networks to predict airport passenger activity choices\ninside the terminal is presented in this paper. Three network architectures are\nproposed: Feedforward Neural Networks (FNN), Long Short-Term Memory (LSTM)\nnetworks, and a combination of the two. Inputs to these models are both static\n(passenger and trip characteristics) and dynamic (real-time passenger\ntracking). A real-world case study exemplifies the application of these models,\nusing anonymous WiFi traces collected at Bologna Airport to train the networks.\nThe performance of the models were evaluated according to the misclassification\nrate of passenger activity choices. In the LSTM approach, two different\nmulti-step forecasting strategies are tested. According to our findings, the\ndirect LSTM approach provides better results than the FNN, especially when the\nprediction horizon is relatively short (20 minutes or less).\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:11:38 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Orsini", "Federico", ""], ["Gastaldi", "Massimiliano", ""], ["Mantecchini", "Luca", ""], ["Rossi", "Riccardo", ""]]}, {"id": "1910.14072", "submitter": "Miguel Ib\\'a\\~nez Berganza", "authors": "Miguel Ib\\'a\\~nez-Berganza, Ambra Amico, Gian Luca Lancia, Federico\n  Maggiore, Bernardo Monechi, Vittorio Loreto", "title": "Unsupervised inference approach to facial attractiveness", "comments": "main article (10 pages, 4 figures) + supplementary information (22\n  pages, 10 figures). minor typos corrected. Federico Maggiore added as author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perception of facial beauty is a complex phenomenon depending on many,\ndetailed and global facial features influencing each other. In the machine\nlearning community this problem is typically tackled as a problem of supervised\ninference. However, it has been conjectured that this approach does not capture\nthe complexity of the phenomenon. A recent original experiment\n(Ib\\'a\\~nez-Berganza et al., Scientific Reports 9, 8364, 2019) allowed\ndifferent human subjects to navigate the face-space and ``sculpt'' their\npreferred modification of a reference facial portrait. Here we present an\nunsupervised inference study of the set of sculpted facial vectors in that\nexperiment. We first infer minimal, interpretable, and faithful probabilistic\nmodels (through Maximum Entropy and artificial neural networks) of the\npreferred facial variations, that capture the origin of the observed\ninter-subject diversity in the sculpted faces. The application of such\ngenerative models to the supervised classification of the gender of the\nsculpting subjects, reveals an astonishingly high prediction accuracy. This\nresult suggests that much relevant information regarding the subjects may\ninfluence (and be elicited from) her/his facial preference criteria, in\nagreement with the multiple motive theory of attractiveness proposed in\nprevious works.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:23:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 18:43:45 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 13:58:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ib\u00e1\u00f1ez-Berganza", "Miguel", ""], ["Amico", "Ambra", ""], ["Lancia", "Gian Luca", ""], ["Maggiore", "Federico", ""], ["Monechi", "Bernardo", ""], ["Loreto", "Vittorio", ""]]}, {"id": "1910.14195", "submitter": "Matthew Miller", "authors": "Matthew J. Miller, Matthew J. Cabral, Elizabeth C. Dickey, James M.\n  LeBeau and Brian J. Reich", "title": "Accounting for Location Measurement Error in Imaging Data with\n  Application to Atomic Resolution Images of Crystalline Materials", "comments": "29 pages including appendices, 4 figures, 13 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists use imaging to identify objects of interest and infer properties\nof these objects. The locations of these objects are often measured with error,\nwhich when ignored leads to biased parameter estimates and inflated variance.\nCurrent measurement error methods require an estimate or knowledge of the\nmeasurement error variance to correct these estimates, which may not be\navailable. Instead, we create a spatial Bayesian hierarchical model that treats\nthe locations as parameters, it using the image itself to incorporate\npositional uncertainty. We lower the computational burden by approximating the\nlikelihood using a non-contiguous block design around the object locations. We\napply this model in a materials science setting to study the relationship\nbetween the chemistry and displacement of hundreds of atom columns in crystal\nstructures directly imaged via scanning transmission electron microscopy.\nGreater knowledge of this relationship can lead to engineering materials with\nimproved properties of interest. We find strong evidence of a negative\nrelationship between atom column displacement and the intensity of neighboring\natom columns, which is related to the local chemistry. A simulation study shows\nour method corrects the bias in the parameter of interest and drastically\nimproves coverage in high noise scenarios compared to non-measurement error\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 00:41:19 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 13:02:50 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Miller", "Matthew J.", ""], ["Cabral", "Matthew J.", ""], ["Dickey", "Elizabeth C.", ""], ["LeBeau", "James M.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1910.14233", "submitter": "Debangan Dey", "authors": "Debangan Dey, Vadim Zipunnikov", "title": "Connecting population-level AUC and latent scale-invariant $R^2$ via\n  Semiparametric Gaussian Copula and rank correlations", "comments": "4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Area Under the Curve (AUC) is arguably the most popular measure of\nclassification accuracy. We use a semiparametric framework to introduce a\nlatent scale-invariant $R^2$, a novel measure of variation explained for an\nobserved binary outcome and an observed continuous predictor, and then directly\nlink the latent $R^2$ to AUC. This enables a mutually consistent simultaneous\nuse of AUC as a measure of classification accuracy and the latent $R^2$ as a\nscale-invariant measure of explained variation. Specifically, we employ\nSemiparametric Gaussian Copula (SGC) to model a joint dependence between\nobserved binary outcome and observed continuous predictor via the correlation\nof latent standard normal random variables. Under SGC, we show how, both\npopulation-level AUC and latent scale-invariant $R^2$, defined as a squared\nlatent correlation, can be estimated using any of the four rank statistics\ncalculated on binary-continuous pairs: Wilcoxon rank-sum, Kendall's Tau,\nSpearman's Rho, and Quadrant rank correlations. We then focus on three\nimplications and applications: i) we explicitly show that under SGC, the\npopulation-level AUC and the population-level latent $R^2$ are related via a\nmonotone function that depends on the population-level prevalence rate, ii) we\npropose Quadrant rank correlation as a robust semiparametric version of AUC;\niii) we demonstrate how, under complex-survey designs, Wilcoxon rank sum\nstatistics and Spearman and Quadrant rank correlations provide asymptotically\nconsistent estimators of the population-level AUC using only single-participant\nsurvey weights. We illustrate these applications using binary outcome of\nfive-year mortality and continuous predictors including Albumin, Systolic Blood\nPressure, and accelerometry-derived measures of total volume of physical\nactivity collected in 2003-2006 National Health and Nutrition Examination\nSurvey (NHANES) cohorts.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:25:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Dey", "Debangan", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1910.14330", "submitter": "Qing Yang", "authors": "Q. Yang, Y. Li and Y. Zhang", "title": "Change Point Detection for Nonparametric Regression under Strongly\n  Mixing Process", "comments": "34 pages, 20 figures, 1 tex file, 5 tex auxiliary files", "journal-ref": null, "doi": "10.1007/s00362-020-01196-y", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the estimation of the structural change point in\nthe nonparametric model with dependent observations. We introduce a\nmaximum-CUSUM-estimation procedure, where the CUSUM statistic is constructed\nbased on the sum-of-squares aggregation of the difference of the two\nNadaraya-Watson estimates using the observations before and after a specific\ntime point. Under some mild conditions, we prove that the statistic tends to\nzero almost surely if there is no change, and is larger than a threshold\nasymptotically almost surely otherwise, which helps us to obtain a\nthreshold-detection strategy. Furthermore, we demonstrate the strong\nconsistency of the change point estimator. In the simulation, we discuss the\nselection of the bandwidth and the threshold used in the estimation, and show\nthe robustness of our method in the long-memory scenario. We implement our\nmethod to the data of Nasdaq 100 index and find that the relation between the\nrealized volatility and the return exhibits several structural changes in\n2007-2009.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 09:33:52 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 03:47:34 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yang", "Q.", ""], ["Li", "Y.", ""], ["Zhang", "Y.", ""]]}, {"id": "1910.14502", "submitter": "Waleed Yousef", "authors": "Waleed A. Yousef", "title": "Assessment of Multiple-Biomarker Classifiers: fundamental principles and\n  a proposed strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple-biomarker classifier problem and its assessment are reviewed\nagainst the background of some fundamental principles from the field of\nstatistical pattern recognition, machine learning, or the recently so-called\n\"data science\". A narrow reading of that literature has led many authors to\nneglect the contribution to the total uncertainty of performance assessment\nfrom the finite training sample. Yet the latter is a fundamental indicator of\nthe stability of a classifier; thus its neglect may be contributing to the\nproblematic status of many studies. A three-level strategy is proposed for\nmoving forward in this field. The lowest level is that of construction, where\ncandidate features are selected and the choice of classifier architecture is\nmade. At that point, the effective dimensionality of the classifier is\nestimated and used to size the next level of analysis, a pilot study on\npreviously unseen cases. The total (training and testing) uncertainty resulting\nfrom the pilot study is, in turn, used to size the highest level of analysis, a\npivotal study with a target level of uncertainty. Some resources available in\nthe literature for implementing this approach are reviewed. Although the\nconcepts explained in the present article may be fundamental and\nstraightforward for many researchers in the machine learning community they are\nsubtle for many practitioners, for whom we provided a general advice for the\nbest practice in \\cite{Shi2010MAQCII} and elaborate here in the present paper.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:45:09 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Yousef", "Waleed A.", ""]]}, {"id": "1910.14563", "submitter": "Pandarasamy Arjunan", "authors": "Pandarasamy Arjunan, Kameshwar Poolla, Clayton Miller", "title": "EnergyStar++: Towards more accurate and explanatory building energy\n  benchmarking", "comments": null, "journal-ref": "Applied Energy, Volume 276, 15 October 2020, 115413", "doi": "10.1016/j.apenergy.2020.115413", "report-no": null, "categories": "stat.AP cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building energy performance benchmarking has been adopted widely in the USA\nand Canada through the Energy Star Portfolio Manager platform. Building\noperations and energy management professionals have long used a simple 1-100\nscore to understand how their building compares to its peers. This single\nnumber is easy to use, but is created by inaccurate linear regression (MLR)\nmodels. This paper proposes a methodology that enhances the existing Energy\nStar calculation method by increasing accuracy and providing additional model\noutput processing to help explain why a building is achieving a certain score.\nWe propose and test two new prediction models: multiple linear regression with\nfeature interactions (MLRi) and gradient boosted trees (GBT). Both models have\nbetter average accuracy than the baseline Energy Star models. The third order\nMLRi and GBT models achieve 4.9% and 24.9% increase in adjusted R2,\nrespectively, and 7.0% and 13.7% decrease in normalized root mean squared error\n(NRMSE), respectively, on average than MLR models for six building types. Even\nmore importantly, a set of techniques is developed to help determine which\nfactors most influence the score using SHAP values. The SHAP force\nvisualization in particular offers an accessible overview of the aspects of the\nbuilding that influence the score that non-technical users can readily\ninterpret. This methodology is tested on the 2012 Commercial Building Energy\nConsumption Survey (CBECS)(1,812 buildings) and public data sets from the\nenergy disclosure programs of New York City (11,131 buildings) and Seattle\n(2,073 buildings).\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 07:04:56 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 08:39:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Arjunan", "Pandarasamy", ""], ["Poolla", "Kameshwar", ""], ["Miller", "Clayton", ""]]}, {"id": "1910.14647", "submitter": "Kasper Kansanen", "authors": "Kasper Kansanen, Petteri Packalen, Matti Maltamo and Lauri Meht\\\"atalo", "title": "Horvitz-Thompson-like estimation with distance-based detection\n  probabilities for circular plot sampling of forests", "comments": "16 pages without the Appendix; 24 in total; Added a disclaimer on the\n  publication of this article in Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In circular plot sampling, trees within a given distance from the sample plot\nlocation constitute a sample, which is used to infer characteristics of\ninterest for the forest area. If the sample is collected using a technical\ndevice located at the sampling point, e.g. a terrestrial laser scanner, all\ntrees of the sample plot cannot be observed because they hide behind each\nother. We propose a Horvitz-Thompson-like estimator with distance-based\ndetection probabilities derived from stochastic geometry for estimation of\npopulation totals such as stem density and basal area in such situation. We\nshow that our estimator is unbiased for Poisson forests and give estimates of\nvariance and approximate confidence intervals for the estimator, unlike any\nprevious methods. We compare the estimator to two previously published\nbenchmark methods. The comparison is done through a simulation study where\nseveral plots are simulated either from field measured data or different marked\npoint processes. The simulations show that the estimator produces lower or\ncomparable error values than the other methods. In the sample plots based on\nthe field measured data the bias is relatively small - relative mean of errors\nfor stem density, for example, varying from 0.3 to 2.2 per cent, depending on\nthe detection condition - and the empirical coverage probabilities of the\napproximate confidence intervals are either similar to the nominal levels or\nconservative.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:36:16 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 07:14:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kansanen", "Kasper", ""], ["Packalen", "Petteri", ""], ["Maltamo", "Matti", ""], ["Meht\u00e4talo", "Lauri", ""]]}]