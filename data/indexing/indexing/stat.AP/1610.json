[{"id": "1610.00039", "submitter": "Jukka-Pekka Onnela", "authors": "Patrick Staples, M\\'elanie Prague, Victor De Gruttola, Jukka-Pekka\n  Onnela", "title": "Leveraging Contact Network Information in Clustered Observational\n  Studies of Contagion Processes", "comments": "Substantial revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an observational study, obtaining unbiased estimates of an exposure effect\nrequires adjusting for all potential confounders. When this condition is met,\nleveraging additional covariates related to the outcome may produce less\nvariable estimates of the effect of exposure. For contagion processes operating\non a contact network, transmission can only occur through ties that connect\nexposed and unexposed individuals; the outcome of such a process is known to\ndepend intimately on the structure of the network. In this paper, we\ninvestigate the use of contact network features as both confounders and\nefficiency covariates in exposure effect estimation. Using doubly-robust\naugmented generalized estimating equations (GEE), we estimate how gains in\nefficiency depend on the network structure and spread of the contagious agent\nor behavior. We apply this approach to estimate the effects of two distinct\nexposures, the proportion of leaders in a village and the proportion of\nhouseholds participating in a self-help program, for the spread of a\nmicrofinance program in a collection of villages in Karnataka, India. We\ncompare these results to simulated observational trials using a stochastic\ncompartmental contagion model on a collection of model-based contact networks\nand compare the bias and variance of the estimated exposure effects using an\nassortment of network covariate adjustment strategies.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 21:49:57 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 00:00:16 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Staples", "Patrick", ""], ["Prague", "M\u00e9lanie", ""], ["De Gruttola", "Victor", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1610.00124", "submitter": "Udaysinh T. Bhosale", "authors": "Udaysinh T. Bhosale and M. S. Santhanam", "title": "Signatures of bifurcation on quantum correlations: Case of the quantum\n  kicked top", "comments": "10 pages, 10 figures. Comments are welcome", "journal-ref": "Phys. Rev. E 95, 012216 (2017)", "doi": "10.1103/PhysRevE.95.012216", "report-no": null, "categories": "quant-ph math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum correlations reflect the quantumness of a system and are useful\nresources for quantum information and computational processes. The measures of\nquantum correlations do not have a classical analog and yet are influenced by\nthe classical dynamics. In this work, by modelling the quantum kicked top as a\nmulti-qubit system, the effect of classical bifurcations on the measures of\nquantum correlations such as quantum discord, geometric discord, Meyer and\nWallach $Q$ measure is studied. The quantum correlation measures change rapidly\nin the vicinity of a classical bifurcation point. If the classical system is\nlargely chaotic, time averages of the correlation measures are in good\nagreement with the values obtained by considering the appropriate random matrix\nensembles. The quantum correlations scale with the total spin of the system,\nrepresenting its semiclassical limit. In the vicinity of the trivial fixed\npoints of the kicked top, scaling function decays as a power-law. In the\nchaotic limit, for large total spin, quantum correlations saturate to a\nconstant, which we obtain analytically, based on random matrix theory, for the\n$Q$ measure. We also suggest that it can have experimental consequences.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 12:28:04 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 09:36:25 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bhosale", "Udaysinh T.", ""], ["Santhanam", "M. S.", ""]]}, {"id": "1610.00147", "submitter": "Maria DeYoreo", "authors": "Tracy Schifeling, Jerome P. Reiter, Maria DeYoreo", "title": "Data Fusion for Correcting Measurement Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in surveys, key items are subject to measurement errors. Given just the\ndata, it can be difficult to determine the distribution of this error process,\nand hence to obtain accurate inferences that involve the error-prone variables.\nIn some settings, however, analysts have access to a data source on different\nindividuals with high quality measurements of the error-prone survey items. We\npresent a data fusion framework for leveraging this information to improve\ninferences in the error-prone survey. The basic idea is to posit models about\nthe rates at which individuals make errors, coupled with models for the values\nreported when errors are made. This can avoid the unrealistic assumption of\nconditional independence typically used in data fusion. We apply the approach\non the reported values of educational attainments in the American Community\nSurvey, using the National Survey of College Graduates as the high quality data\nsource. In doing so, we account for the informative sampling design used to\nselect the National Survey of College Graduates. We also present a process for\nassessing the sensitivity of various analyses to different choices for the\nmeasurement error models. Supplemental material is available online.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 15:15:40 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Schifeling", "Tracy", ""], ["Reiter", "Jerome P.", ""], ["DeYoreo", "Maria", ""]]}, {"id": "1610.00195", "submitter": "Elizabeth Hou", "authors": "Elizabeth Hou, Earl Lawrence, Alfred O. Hero", "title": "Penalized Ensemble Kalman Filters for High Dimensional Non-linear\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is a data assimilation technique that uses\nan ensemble of models, updated with data, to track the time evolution of a\nusually non-linear system. It does so by using an empirical approximation to\nthe well-known Kalman filter. However, its performance can suffer when the\nensemble size is smaller than the state space, as is often necessary for\ncomputationally burdensome models. This scenario means that the empirical\nestimate of the state covariance is not full rank and possibly quite noisy. To\nsolve this problem in this high dimensional regime, we propose a\ncomputationally fast and easy to implement algorithm called the penalized\nensemble Kalman filter (PEnKF). Under certain conditions, it can be\ntheoretically proven that the PEnKF will be accurate (the estimation error will\nconverge to zero) despite having fewer ensemble members than state dimensions.\nFurther, as contrasted to localization methods, the proposed approach learns\nthe covariance structure associated with the dynamical system. These\ntheoretical results are supported with simulations of several non-linear and\nhigh dimensional systems.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 21:52:24 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 22:19:38 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 02:58:23 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hou", "Elizabeth", ""], ["Lawrence", "Earl", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1610.00336", "submitter": "Christopher E. Granade", "authors": "Christopher Granade and Christopher Ferrie and Ian Hincks and Steven\n  Casagrande and Thomas Alexander and Jonathan Gross and Michal Kononenko and\n  Yuval Sanders", "title": "QInfer: Statistical inference software for quantum applications", "comments": "19 pages, a full Users' Guide and illustrated examples describing the\n  QInfer software library", "journal-ref": null, "doi": "10.22331/q-2017-04-25-5", "report-no": null, "categories": "quant-ph physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Characterizing quantum systems through experimental data is critical to\napplications as diverse as metrology and quantum computing. Analyzing this\nexperimental data in a robust and reproducible manner is made challenging,\nhowever, by the lack of readily-available software for performing principled\nstatistical analysis. We improve the robustness and reproducibility of\ncharacterization by introducing an open-source library, QInfer, to address this\nneed. Our library makes it easy to analyze data from tomography, randomized\nbenchmarking, and Hamiltonian learning experiments either in post-processing,\nor online as data is acquired. QInfer also provides functionality for\npredicting the performance of proposed experimental protocols from simulated\nruns. By delivering easy-to-use characterization tools based on principled\nstatistical analysis, QInfer helps address many outstanding challenges facing\nquantum technology.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 19:01:09 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 04:50:29 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Granade", "Christopher", ""], ["Ferrie", "Christopher", ""], ["Hincks", "Ian", ""], ["Casagrande", "Steven", ""], ["Alexander", "Thomas", ""], ["Gross", "Jonathan", ""], ["Kononenko", "Michal", ""], ["Sanders", "Yuval", ""]]}, {"id": "1610.00580", "submitter": "Jacob Abernethy", "authors": "Jacob Abernethy (University of Michigan), Cyrus Anderson (University\n  of Michigan), Chengyu Dai (University of Michigan), Arya Farahi (University\n  of Michigan), Linh Nguyen (University of Michigan), Adam Rauh (University of\n  Michigan), Eric Schwartz (University of Michigan), Wenbo Shen (University of\n  Michigan), Guangsha Shi (University of Michigan), Jonathan Stroud (University\n  of Michigan), Xinyu Tan (University of Michigan), Jared Webb (University of\n  Michigan), Sheng Yang (University of Michigan)", "title": "Flint Water Crisis: Data-Driven Risk Assessment Via Residential Water\n  Testing", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery from the Flint Water Crisis has been hindered by uncertainty in both\nthe water testing process and the causes of contamination. In this work, we\ndevelop an ensemble of predictive models to assess the risk of lead\ncontamination in individual homes and neighborhoods. To train these models, we\nutilize a wide range of data sources, including voluntary residential water\ntests, historical records, and city infrastructure data. Additionally, we use\nour models to identify the most prominent factors that contribute to a high\nrisk of lead contamination. In this analysis, we find that lead service lines\nare not the only factor that is predictive of the risk of lead contamination of\nwater. These results could be used to guide the long-term recovery efforts in\nFlint, minimize the immediate damages, and improve resource-allocation\ndecisions for similar water infrastructure crises.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 14:31:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Abernethy", "Jacob", "", "University of Michigan"], ["Anderson", "Cyrus", "", "University\n  of Michigan"], ["Dai", "Chengyu", "", "University of Michigan"], ["Farahi", "Arya", "", "University\n  of Michigan"], ["Nguyen", "Linh", "", "University of Michigan"], ["Rauh", "Adam", "", "University of\n  Michigan"], ["Schwartz", "Eric", "", "University of Michigan"], ["Shen", "Wenbo", "", "University of\n  Michigan"], ["Shi", "Guangsha", "", "University of Michigan"], ["Stroud", "Jonathan", "", "University\n  of Michigan"], ["Tan", "Xinyu", "", "University of Michigan"], ["Webb", "Jared", "", "University of\n  Michigan"], ["Yang", "Sheng", "", "University of Michigan"]]}, {"id": "1610.01000", "submitter": "Lucie Montuelle", "authors": "Aur\\'elie Fischer (UPD7), Lucie Montuelle (UPD7), Mathilde Mougeot\n  (UPD7), Dominique Picard (UPD7)", "title": "Statistical learning for wind power : a modeling and stability study\n  towards forecasting", "comments": null, "journal-ref": "Wind Energy, Wiley, 2017, 20 (12), pp.2037 - 2047", "doi": "10.1002/we.2139", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on wind power modeling using machine learning techniques. We show on\nreal data provided by the wind energy company Ma{\\\"i}a Eolis, that parametric\nmodels, even following closely the physical equation relating wind production\nto wind speed are outperformed by intelligent learning algorithms. In\nparticular, the CART-Bagging algorithm gives very stable and promising results.\nBesides, as a step towards forecast, we quantify the impact of using\ndeteriorated wind measures on the performances. We show also on this\napplication that the default methodology to select a subset of predictors\nprovided in the standard random forest package can be refined, especially when\nthere exists among the predictors one variable which has a major impact.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:03:24 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 10:05:35 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Fischer", "Aur\u00e9lie", "", "UPD7"], ["Montuelle", "Lucie", "", "UPD7"], ["Mougeot", "Mathilde", "", "UPD7"], ["Picard", "Dominique", "", "UPD7"]]}, {"id": "1610.01215", "submitter": "Andre Python", "authors": "Andr\\'e Python, Janine Illian, Charlotte Jones-Todd and Marta\n  Blangiardo", "title": "A Bayesian Approach to Modelling Fine-Scale Spatial Dynamics of\n  Non-State Terrorism: World Study, 2002-2013", "comments": "typo in one reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To this day, terrorism persists as a worldwide threat, as exemplified by the\nongoing lethal attacks perpetrated by ISIS in Iraq, Syria, Al Qaeda in Yemen,\nand Boko Haram in Nigeria. In response, states deploy various counterterrorism\npolicies, the costs of which could be reduced through efficient preventive\nmeasures. Statistical models able to account for complex spatio-temporal\ndependencies have not yet been applied, despite their potential for providing\nguidance to explain and prevent terrorism. In an effort to address this\nshortcoming, we employ hierarchical models in a Bayesian context, where the\nspatial random field is represented by a stochastic partial differential\nequation. Our results confirm the contagious nature of the lethality of\nterrorism and the number of lethal terrorist attacks in both space and time.\nMoreover, the frequency of lethal attacks tends to be higher in richer areas,\nclose to large cities, and within democratic countries. In contrast, attacks\nare more likely to be lethal far away from large cities, at higher altitudes,\nin poorer areas, and in locations with higher ethnic diversity. We argue that,\non a local scale, the lethality of terrorism and the frequency of lethal\nattacks are driven by antagonistic mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 21:41:18 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 14:40:56 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 15:06:11 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Python", "Andr\u00e9", ""], ["Illian", "Janine", ""], ["Jones-Todd", "Charlotte", ""], ["Blangiardo", "Marta", ""]]}, {"id": "1610.01444", "submitter": "Davide Alinovi", "authors": "Davide Alinovi, Gianluigi Ferrari, Francesco Pisani, Riccardo Raheli", "title": "Markov Chain Modeling and Simulation of Breathing Patterns", "comments": "submitted for publication; 19 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of large video databases obtained from real patients with\nrespiratory disorders makes the design and optimization of video-based\nmonitoring systems quite critical. The purpose of this study is the development\nof suitable models and simulators of breathing behaviors and disorders, such as\nrespiratory pauses and apneas, in order to allow efficient design and test of\nvideo-based monitoring systems. More precisely, a novel Continuous-Time Markov\nChain (CTMC) statistical model of breathing patterns is presented. The\nRespiratory Rate (RR) pattern, estimated by measured vital signs of\nhospital-monitored patients, is approximated as a CTMC, whose states and\nparameters are selected through an appropriate statistical analysis. Then, two\nsimulators, software- and hardware-based, are proposed. After validation of the\nCTMC model, the proposed simulators are tested with previously developed\nvideo-based algorithms for the estimation of the RR and the detection of apnea\nevents. Examples of application to assess the performance of systems for\nvideo-based RR estimation and apnea detection are presented. The results, in\nterms of Kullback-Leibler divergence, show that realistic breathing patterns,\nincluding specific respiratory disorders, can be accurately described by the\nproposed model; moreover, the simulators are able to reproduce practical\nbreathing patterns for video analysis. The presented CTMC statistical model can\nbe strategic to describe realistic breathing patterns and devise simulators\nuseful to develop and test novel and effective video processing-based\nmonitoring systems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:32:21 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Alinovi", "Davide", ""], ["Ferrari", "Gianluigi", ""], ["Pisani", "Francesco", ""], ["Raheli", "Riccardo", ""]]}, {"id": "1610.01493", "submitter": "Gabriel Alvarez", "authors": "G. Alvarez, L. Fernandez, R. Salinas", "title": "Construction of hazard maps of Hantavirus contagion using Remote\n  Sensing, logistic regression and Artificial Neural Networks: case Araucan\\'ia\n  Region, Chile", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.bio-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, methods and computational results based on statistical\nanalysis and mathematical modelling, data collection in situ in order to make a\nhazard map of Hanta Virus infection in the region of Araucania, Chile are\npresented. The development of this work involves several elements such as\nLandsat satellite images, biological information regarding seropositivity of\nHanta Virus and information concerning positive cases of infection detected in\nthe region. All this information has been processed to find a function that\nmodels the danger of contagion in the region, through logistic regression\nanalysis and Artificial Neural Networks\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 15:58:20 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Alvarez", "G.", ""], ["Fernandez", "L.", ""], ["Salinas", "R.", ""]]}, {"id": "1610.01563", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas S. A. Wallis and Matthias Bethge", "title": "DeepGaze II: Reading fixations from deep features trained on object\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present DeepGaze II, a model that predicts where people look in\nimages. The model uses the features from the VGG-19 deep neural network trained\nto identify objects in images. Contrary to other saliency models that use deep\nfeatures, here we use the VGG features for saliency prediction with no\nadditional fine-tuning (rather, a few readout layers are trained on top of the\nVGG features to predict saliency). The model is therefore a strong test of\ntransfer learning. After conservative cross-validation, DeepGaze II explains\nabout 87% of the explainable information gain in the patterns of fixations and\nachieves top performance in area under the curve metrics on the MIT300 hold-out\nbenchmark. These results corroborate the finding from DeepGaze I (which\nexplained 56% of the explainable information gain), that deep features trained\non object recognition provide a versatile feature space for performing related\nvisual tasks. We explore the factors that contribute to this success and\npresent several informative image examples. A web service is available to\ncompute model predictions at http://deepgaze.bethgelab.org.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 18:47:28 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1610.01633", "submitter": "Alexandra Piryatinska Dr", "authors": "Boris Darkhovsky, Alexandra Piryatinska, Alexander Kaplan", "title": "Binary classification of multi-channel EEG records based on the\n  $\\epsilon$-complexity of continuous vector functions", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A methodology for binary classification of EEG records which correspond to\ndifferent mental states is proposed. This model-free methodology is based on\nour theory of the $\\epsilon$-complexity of continuous functions which is\nextended here (see Appendix) to the case of vector functions. This extension\npermits us to handle multichannel EEG recordings. The essence of the\nmethodology is to use the $\\epsilon$-complexity coefficients as features to\nclassify (using well known classifiers) different types of vector functions\nrepresenting EEG-records corresponding to different types of mental states. We\napply our methodology to the problem of classification of multichannel\nEEG-records related to a group of healthy adolescents and a group of\nadolescents with schizophrenia. We found that our methodology permits accurate\nclassification of the data in the four-dimensional feather space of the\n$\\epsilon$-complexity coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:24:17 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Darkhovsky", "Boris", ""], ["Piryatinska", "Alexandra", ""], ["Kaplan", "Alexander", ""]]}, {"id": "1610.01889", "submitter": "Dong Wang", "authors": "Dong Wang, Xialu Liu, Rong Chen", "title": "Factor Models for Matrix-Valued High-Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finance, economics and many other fields, observations in a matrix form\nare often observed over time. For example, many economic indicators are\nobtained in different countries over time. Various financial characteristics of\nmany companies are reported over time. Although it is natural to turn a matrix\nobservation into a long vector then use standard vector time series models or\nfactor analysis, it is often the case that the columns and rows of a matrix\nrepresent different sets of information that are closely interrelated in a very\nstructural way. We propose a novel factor model that maintains and utilizes the\nmatrix structure to achieve greater dimensional reduction as well as finding\nclearer and more interpretable factor structures. Estimation procedure and its\ntheoretical properties are investigated and demonstrated with simulated and\nreal examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 14:36:31 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 03:03:42 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Wang", "Dong", ""], ["Liu", "Xialu", ""], ["Chen", "Rong", ""]]}, {"id": "1610.02351", "submitter": "Lucas Janson", "authors": "Emmanuel Candes, Yingying Fan, Lucas Janson, Jinchi Lv", "title": "Panning for Gold: Model-X Knockoffs for High-dimensional Controlled\n  Variable Selection", "comments": "39 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many contemporary large-scale applications involve building interpretable\nmodels linking a large set of potential covariates to a response in a nonlinear\nfashion, such as when the response is binary. Although this modeling problem\nhas been extensively studied, it remains unclear how to effectively control the\nfraction of false discoveries even in high-dimensional logistic regression, not\nto mention general high-dimensional nonlinear models. To address such a\npractical problem, we propose a new framework of $model$-$X$ knockoffs, which\nreads from a different perspective the knockoff procedure (Barber and Cand\\`es,\n2015) originally designed for controlling the false discovery rate in linear\nmodels. Whereas the knockoffs procedure is constrained to homoscedastic linear\nmodels with $n\\ge p$, the key innovation here is that model-X knockoffs provide\nvalid inference from finite samples in settings in which the conditional\ndistribution of the response is arbitrary and completely unknown. Furthermore,\nthis holds no matter the number of covariates. Correct inference in such a\nbroad setting is achieved by constructing knockoff variables probabilistically\ninstead of geometrically. To do this, our approach requires the covariates be\nrandom (independent and identically distributed rows) with a distribution that\nis known, although we provide preliminary experimental evidence that our\nprocedure is robust to unknown/estimated distributions. To our knowledge, no\nother procedure solves the $controlled$ variable selection problem in such\ngenerality, but in the restricted settings where competitors exist, we\ndemonstrate the superior power of knockoffs through simulations. Finally, we\napply our procedure to data from a case-control study of Crohn's disease in the\nUnited Kingdom, making twice as many discoveries as the original analysis of\nthe same data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 17:18:02 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 18:25:01 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 05:37:10 GMT"}, {"version": "v4", "created": "Tue, 12 Dec 2017 14:57:10 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Candes", "Emmanuel", ""], ["Fan", "Yingying", ""], ["Janson", "Lucas", ""], ["Lv", "Jinchi", ""]]}, {"id": "1610.02508", "submitter": "Roxane Duroux", "authors": "Roxane Duroux, C\\'ecile Chauvel and John O'Quigley", "title": "Conditional survival given covariates and marginal survival", "comments": "18 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming some regression model, it is common to study the conditional\ndistribution of survival given covariates. Here, we consider the impact of\nfurther conditioning, specifically conditioning on a marginal survival\nfunction, known or estimated. We investigate to what purposes any such\ninformation can be used in a proportional or non-proportional hazards\nregression analysis of time on the covariates. It does not lead to any\nimprovement in efficiency when the form of the assumed proportional hazards\nmodel is valid. However, when the proportional hazards model is not valid, the\nusual partial likelihood estimator is not consistent and depends heavily on the\nunknown censoring mechanism. In this case we show that the conditional estimate\nthat we propose is consistent for a parameter that has a strong interpretation\nindependent of censoring. Simulations and examples are provided.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 10:25:10 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Duroux", "Roxane", ""], ["Chauvel", "C\u00e9cile", ""], ["O'Quigley", "John", ""]]}, {"id": "1610.02515", "submitter": "Roxane Duroux", "authors": "Roxane Duroux and John O'Quigley", "title": "Inference for changepoint survival models", "comments": "19 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-proportional hazards model where the regression coefficient\nis not constant but piecewise constant. Following Andersen and Gill (1982), we\nknow that a knowledge of the changepoint leads to a relatively straightforward\nestimation of the regression coefficients on either side of the changepoint.\nBetween adjacent changepoints, we place ourselves under the proportional\nhazards model. We can then maximize the partial likelihood to obtain a\nconsistent estimation of the regression coefficients. Difficulties occur when\nwe want to estimate these changepoints. We obtain a confidence region for the\nchangepoint, under a two-step regression model (Anderson and Senthilselvan,\n1982), based on the work of Davies (1977). Then we introduce a new estimation\nmethod using the standardized score process (Chauvel and O'Quigley, 2014),\nunder a model with multiple changepoints. In this context, rather than make use\nof the partial likelihood, we base inference on minimization of quadratic\nresiduals. Simulations and an example are provided.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 11:32:45 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Duroux", "Roxane", ""], ["O'Quigley", "John", ""]]}, {"id": "1610.02548", "submitter": "Zhana Kuncheva", "authors": "Zhana Kuncheva, Michelle L. Krishnan and Giovanni Montana", "title": "Exploring brain transcriptomic patterns: a topological analysis using\n  spatial expression networks", "comments": "8 pages, 4 figures, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the transcriptome architecture of the human brain is\nfundamental in gaining an understanding of brain function and disease. A number\nof recent studies have investigated patterns of brain gene expression obtained\nfrom an extensive anatomical coverage across the entire human brain using\nexperimental data generated by the Allen Human Brain Atlas (AHBA) project. In\nthis paper, we propose a new representation of a gene's transcription activity\nthat explicitly captures the pattern of spatial co-expression across different\nanatomical brain regions. For each gene, we define a Spatial Expression Network\n(SEN), a network quantifying co-expression patterns amongst several anatomical\nlocations. Network similarity measures are then employed to quantify the\ntopological resemblance between pairs of SENs and identify naturally occurring\nclusters. Using network-theoretical measures, three large clusters have been\ndetected featuring distinct topological properties. We then evaluate whether\ntopological diversity of the SENs reflects significant differences in\nbiological function through a gene ontology analysis. We report on evidence\nsuggesting that one of the three SEN clusters consists of genes specifically\ninvolved in the nervous system, including genes related to brain disorders,\nwhile the remaining two clusters are representative of immunity, transcription\nand translation. These findings are consistent with previous studies showing\nthat brain gene clusters are generally associated with one of these three major\nbiological processes.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 15:45:30 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Kuncheva", "Zhana", ""], ["Krishnan", "Michelle L.", ""], ["Montana", "Giovanni", ""]]}, {"id": "1610.02653", "submitter": "Ines Wilms", "authors": "Ines Wilms, Jeroen Rombouts, Christophe Croux", "title": "Lasso-based forecast combinations for forecasting realized variances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility forecasts are key inputs in financial analysis. While lasso based\nforecasts have shown to perform well in many applications, their use to obtain\nvolatility forecasts has not yet received much attention in the literature.\nLasso estimators produce parsimonious forecast models. Our forecast combination\napproach hedges against the risk of selecting a wrong degree of model\nparsimony. Apart from the standard lasso, we consider several lasso extensions\nthat account for the dynamic nature of the forecast model. We apply forecast\ncombined lasso estimators in a comprehensive forecasting exercise using\nrealized variance time series of ten major international stock market indices.\nWe find the lasso extended \"ordered lasso\" to give the most accurate realized\nvariance forecasts. Multivariate forecast models, accounting for volatility\nspillovers between different stock markets, outperform univariate forecast\nmodels for longer forecast horizons.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 10:19:57 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wilms", "Ines", ""], ["Rombouts", "Jeroen", ""], ["Croux", "Christophe", ""]]}, {"id": "1610.02708", "submitter": "Oscar Fontanelli", "authors": "Oscar Fontanelli and Pedro Miramontes and Germinal Cocho and Wentian\n  Li", "title": "Population patterns in World's administrative units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been an extended discussion concerning city population\ndistribution, little has been said about administrative units. Even though\nthere might be a correspondence between cities and administrative divisions,\nthey are conceptually different entities and the correspondence breaks as\nartificial divisions form and evolve. In this work we investigate the\npopulation distribution of second level administrative units for 150 countries\nand propose the Discrete Generalized Beta Distribution (DGBD) rank-size\nfunction to describe the data. After testing the goodness of fit of this two\nparameter function against power law, which is the most common model for city\npopulation, DGBD is a good statistical model for 73% of our data sets and\nbetter than power law in almost every case. Particularly, DGBD is better than\npower law for fitting country population data. The fitted parameters of this\nfunction allow us to construct a phenomenological characterization of countries\naccording to the way in which people are distributed inside them. We present a\ncomputational model to simulate the formation of administrative divisions and\ngive numerical evidence that DGBD arises from it. This model along with the\nDGBD function prove adequate to reproduce and describe local unit evolution and\nits effect on population distribution.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 19:31:44 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Fontanelli", "Oscar", ""], ["Miramontes", "Pedro", ""], ["Cocho", "Germinal", ""], ["Li", "Wentian", ""]]}, {"id": "1610.03225", "submitter": "Bijan Fallah", "authors": "Bijan Fallah", "title": "A cheap data assimilation approach for expensive numerical simulations", "comments": "18 Pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a very cheap Data Assimilation (DA) method, I show an alternative\napproach to classical DA for numerical climate models which produce a large\namount of \"big data\". The problematic features of state-of-the-art high\nresolution Regional Climate Models are highlighted. One of the shortcomings is\nthe sensitivity of such models to the slightly different initial and boundary\nconditions which could be corrected by assimilating scattered observational\ndata. This method might help to reduce the bias of numerical models based on\navailable observations within the model domain, especially for the\ntime-averaged observations and the long-term simulations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 07:52:11 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Fallah", "Bijan", ""]]}, {"id": "1610.03687", "submitter": "Juho Kopra", "authors": "Juho Kopra, Juha Karvanen, Tommi H\\\"ark\\\"anen", "title": "Bayesian models for data missing not at random in health examination\n  surveys", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological surveys, data missing not at random (MNAR) due to survey\nnonresponse may potentially lead to a bias in the risk factor estimates. We\npropose an approach based on Bayesian data augmentation and survival modelling\nto reduce the nonresponse bias. The approach requires additional information\nbased on follow-up data. We present a case study of smoking prevalence using\nFINRISK data collected between 1972 and 2007 with a follow-up to the end of\n2012 and compare it to other commonly applied missing at random (MAR)\nimputation approaches. A simulation experiment is carried out to study the\nvalidity of the approaches. Our approach appears to reduce the nonresponse bias\nsubstantially, where as MAR imputation was not successful in bias reduction.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 12:31:20 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 11:56:34 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Kopra", "Juho", ""], ["Karvanen", "Juha", ""], ["H\u00e4rk\u00e4nen", "Tommi", ""]]}, {"id": "1610.03724", "submitter": "Lucas Fievet", "authors": "Lucas Fievet and Didier Sornette", "title": "Decision trees unearth return sign correlation in the S&P 500", "comments": "32 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technical trading rules and linear regressive models are often used by\npractitioners to find trends in financial data. However, these models are\nunsuited to find non-linearly separable patterns. We propose a decision tree\nforecasting model that has the flexibility to capture arbitrary patterns. To\nillustrate, we construct a binary Markov process with a deterministic component\nthat cannot be predicted with an autoregressive process. A simulation study\nconfirms the robustness of the trees and limitation of the autoregressive\nmodel. Finally, adjusting for multiple testing, we show that some tree based\nstrategies achieve trading performance significant at the 99% confidence level\non the S&P 500 over the past 20 years. The best strategy breaks even with the\nbuy-and-hold strategy at 21 bps in transaction costs per round trip. A\nfour-factor regression analysis shows significant intercept and correlation\nwith the market. The return anomalies are strongest during the bursts of the\ndotcom bubble, financial crisis, and European debt crisis. The correlation of\nthe return signs during these periods confirms the theoretical model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:22:05 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 17:16:19 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fievet", "Lucas", ""], ["Sornette", "Didier", ""]]}, {"id": "1610.03809", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Boris A. Gutman, Joshua Faskowitz, Neda Jahanshad, Paul\n  M. Thompson", "title": "A Continuous Model of Cortical Connectivity", "comments": "Accepted at MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a continuous model for structural brain connectivity based on the\nPoisson point process. The model treats each streamline curve in a tractography\nas an observed event in connectome space, here a product space of cortical\nwhite matter boundaries. We approximate the model parameter via kernel density\nestimation. To deal with the heavy computational burden, we develop a fast\nparameter estimation method by pre-computing associated Legendre products of\nthe data, leveraging properties of the spherical heat kernel. We show how our\napproach can be used to assess the quality of cortical parcellations with\nrespect to connectivty. We further present empirical results that suggest the\ndiscrete connectomes derived from our model have substantially higher\ntest-retest reliability compared to standard methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 18:11:46 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 22:33:37 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Moyer", "Daniel", ""], ["Gutman", "Boris A.", ""], ["Faskowitz", "Joshua", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1610.03917", "submitter": "Alex Deng", "authors": "Alex Deng, Pengchuan Zhang, Shouyuan Chen, Dong Woo Kim, Jiannan Lu", "title": "Concise Summarization of Heterogeneous Treatment Effect Using Total\n  Variation Regularized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled experiment has long been accepted as the golden\nstandard for establishing causal link and estimating causal effect in various\nscientific fields. Average treatment effect is often used to summarize the\neffect estimation, even though treatment effects are commonly believed to be\nvarying among individuals. In the recent decade with the availability of \"big\ndata\", more and more experiments have large sample size and increasingly rich\nside information that enable and require experimenters to discover and\nunderstand heterogeneous treatment effect (HTE). There are two aspects in HTE\nunderstanding, one is to predict the effect conditioned on a given set of side\ninformation or a given individual, the other is to interpret the HTE structure\nand summarize it in a memorable way. The former aspect can be treated as a\nregression problem, and the latter aspect focuses on concise summarization and\ninterpretation. In this paper we propose a method that can achieve both at the\nsame time. This method can be formulated as a convex optimization problem, for\nwhich we provide stable and scalable implementation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 02:36:20 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Deng", "Alex", ""], ["Zhang", "Pengchuan", ""], ["Chen", "Shouyuan", ""], ["Kim", "Dong Woo", ""], ["Lu", "Jiannan", ""]]}, {"id": "1610.04107", "submitter": "Yoann Altmann", "authors": "Yoann Altmann, Aurora Maccarone, Aongus McCarthy, Gregory Newstadt,\n  Gerald S. Buller, Steve McLaughlin and Alfred Hero", "title": "Robust spectral unmixing of sparse multispectral Lidar waveforms using\n  gamma Markov random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian spectral unmixing algorithm to analyse\nremote scenes sensed via sparse multispectral Lidar measurements. To a first\napproximation, in the presence of a target, each Lidar waveform consists of a\nmain peak, whose position depends on the target distance and whose amplitude\ndepends on the wavelength of the laser source considered (i.e, on the target\nreflectivity). Besides, these temporal responses are usually assumed to be\ncorrupted by Poisson noise in the low photon count regime. When considering\nmultiple wavelengths, it becomes possible to use spectral information in order\nto identify and quantify the main materials in the scene, in addition to\nestimation of the Lidar-based range profiles. Due to its anomaly detection\ncapability, the proposed hierarchical Bayesian model, coupled with an efficient\nMarkov chain Monte Carlo algorithm, allows robust estimation of depth images\ntogether with abundance and outlier maps associated with the observed 3D scene.\nThe proposed methodology is illustrated via experiments conducted with real\nmultispectral Lidar data acquired in a controlled environment. The results\ndemonstrate the possibility to unmix spectral responses constructed from\nextremely sparse photon counts (less than 10 photons per pixel and band).\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 14:49:00 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 14:33:14 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Altmann", "Yoann", ""], ["Maccarone", "Aurora", ""], ["McCarthy", "Aongus", ""], ["Newstadt", "Gregory", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Steve", ""], ["Hero", "Alfred", ""]]}, {"id": "1610.04196", "submitter": "Herman Carstens", "authors": "Herman Carstens, Xiaohua Xia, Sarma Yadavalli", "title": "Low-Cost Energy Meter Calibration Method for Measurement and\n  Verification", "comments": "17 pages, 5 figures", "journal-ref": "Applied Energy, Volume 188, 15 February 2017, Pages 563-575", "doi": "10.1016/j.apenergy.2016.12.028", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy meters need to be calibrated for use in Measurement and Verification\n(M&V) projects. However, calibration can be prohibitively expensive and affect\nproject feasibility negatively. This study presents a novel low-cost in-situ\nmeter data calibration technique using a relatively low accuracy commercial\nenergy meter as a calibrator. Calibration is achieved by combining two machine\nlearning tools: the SIMulation EXtrapolation (SIMEX) Measurement Error Model,\nand Bayesian regression. The model is trained or calibrated on half-hourly\nbuilding energy data for 24 hours. Measurements are then compared to the true\nvalues over the following months to verify the method. Results show that the\nhybrid method significantly improves parameter estimates and goodness of fit\nwhen compared to Ordinary Least Squares regression or standard SIMEX. This\nstudy also addresses the effect of mismeasurement in energy monitoring, and\nimplements a powerful technique for mitigating the bias that arises because of\nit. Meters calibrated by the technique presented have satisfactory accuracy for\nmost M&V applications, at a significantly lower cost.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 18:39:26 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 12:21:39 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Carstens", "Herman", ""], ["Xia", "Xiaohua", ""], ["Yadavalli", "Sarma", ""]]}, {"id": "1610.04297", "submitter": "Leigh Roberts Dr", "authors": "Leigh A Roberts", "title": "Distribution free testing of grouped Bernoulli trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Khmaladze has shown how to `rotate' one empirical process to\nanother. This paper is the first to apply this transform when successive data\npoints are generated by a single distributional family, but with covariates\nvarying over the sample. The application is to Bernoulli trials, and new\nresults show how group sizes rotated are related to the number of parameters,\nand explore the impact of different types of data generating processes. The\nutility of the rotation is clear: goodness of fit tests after rotation to a\ndistribution free process are easily computed, show excellent convergence\nproperties, and exhibit high power to reject incorrect null hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 00:15:38 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 22:00:39 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Roberts", "Leigh A", ""]]}, {"id": "1610.04371", "submitter": "Jean-Stephane Bailly", "authors": "Ibrahim Fayad (UMR TETIS), Nicolas Baghdadi (UMR TETIS), St\\'ephane\n  Guitet (INRA, UMR AMAP), Jean-St\\'ephane Bailly (LISAH), Bruno H\\'erault\n  (ECOFOG), Val\\'ery Gond, Mahmoud Hajj, Dinh Ho Tong Minh (UMR TETIS)", "title": "Aboveground biomass mapping in French Guiana by combining remote\n  sensing, forest inventories and environmental data", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation, Elsevier, 2016, 52, pp.502 - 514", "doi": "10.1016/j.jag.2016.07.015", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping forest aboveground biomass (AGB) has become an important task,\nparticularly for the reporting of carbon stocks and changes. AGB can be mapped\nusing synthetic aperture radar data (SAR) or passive optical data. However,\nthese data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and\n\\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical\nforests. Studies have mapped the rough variations in AGB by combining optical\nand environmental data at regional and global scales. Nevertheless, these maps\ncannot represent local variations in AGB in tropical forests. In this paper, we\nhypothesize that the problem of misrepresenting local variations in AGB and AGB\nestimation with good precision occurs because of both methodological limits\n(signal saturation or dilution bias) and a lack of adequate calibration data in\nthis range of AGB values. We test this hypothesis by developing a calibrated\nregression model to predict variations in high AGB values (mean\n\\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for\nspatial extrapolation with data from the optical geoscience laser altimeter\nsystem (GLAS), forest inventories, radar, optics, and environmental variables\nfor spatial inter-and extrapolation. Given their higher point count, GLAS data\nallow a wider coverage of AGB values. We find that the metrics from GLAS\nfootprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3\nMg/ha) with no bias for high values. First, predictive models, including\nremote-sensing, environmental variables and spatial correlation functions,\nallow us to obtain \"wall-to-wall\" AGB maps over French Guiana with an RMSE for\nthe in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We\nconclude that a calibrated regression model based on GLAS with dependent\nenvironmental data can produce good AGB predictions even for high AGB values if\nthe calibration data fit the AGB range. We also demonstrate that small temporal\nand spatial mismatches between field data and GLAS footprints are not a problem\nfor regional and global calibrated regression models because field data aim to\npredict large and deep tendencies in AGB variations from environmental\ngradients and do not aim to represent high but stochastic and temporally\nlimited variations from forest dynamics. Thus, we advocate including a greater\nvariety of data, even if less precise and shifted, to better represent high AGB\nvalues in global models and to improve the fitting of these models for high\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 08:53:48 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Fayad", "Ibrahim", "", "UMR TETIS"], ["Baghdadi", "Nicolas", "", "UMR TETIS"], ["Guitet", "St\u00e9phane", "", "INRA, UMR AMAP"], ["Bailly", "Jean-St\u00e9phane", "", "LISAH"], ["H\u00e9rault", "Bruno", "", "ECOFOG"], ["Gond", "Val\u00e9ry", "", "UMR TETIS"], ["Hajj", "Mahmoud", "", "UMR TETIS"], ["Minh", "Dinh Ho Tong", "", "UMR TETIS"]]}, {"id": "1610.04804", "submitter": "Zhen Han", "authors": "Zhen Han and Alyson Wilson", "title": "Dynamic Stacked Generalization for Node Classification on Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 00:47:21 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Han", "Zhen", ""], ["Wilson", "Alyson", ""]]}, {"id": "1610.05046", "submitter": "Jolanda Kossakowski", "authors": "Jolanda J Kossakowski, Marijke CM Gordijn, Harriette Riese, Lourens J\n  Waldorp", "title": "Mean Field Dynamics of Graphs II: Assessing the Risk for the Development\n  of Phase Transitions in Empirical Data", "comments": "The paper is changed and has a different title, thus we submitted the\n  new version as a new preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Psychological disorders like major depressive disorder can be seen as complex\ndynamical systems. By looking at symptom activation patterns, we can\ninvestigate the dynamic behaviour of individuals to see whether or not they are\nat risk for sudden changes (phase transitions). Here, we show how a mean field\napproximation is used to reduce a dynamic multidimensional system to\none-dimensional system to analyse the dynamics. Using maximum likelihood\nestimation, we can estimate the parameter of interest which, in combination\nwith a bifurcation diagram, reflects the risk that someone has for experiencing\na transition. After validating the proposed method with simulated data, we\napply this method to three empirical examples, where we validate our method\nusing data that contains a transition, and where we show its use in a clinical\nand general sample. Results show an increased risk for a transition when the\ntransition actually occurred, and that members of both the clinical and general\nsample were not susceptible to transitions from a healthy to a depressed mood,\nor vice versa. We conclude that the mean field approximation is valid to assess\nthe risk for a transition, and could in the future aid clinical therapists in\nthe treatment of depressed patient.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 11:24:12 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 08:52:44 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 12:59:35 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Kossakowski", "Jolanda J", ""], ["Gordijn", "Marijke CM", ""], ["Riese", "Harriette", ""], ["Waldorp", "Lourens J", ""]]}, {"id": "1610.05057", "submitter": "Benjamin Kaehler", "authors": "Benjamin D Kaehler", "title": "Full Reconstruction of Non-Stationary Strand-Symmetric Models on Rooted\n  Phylogenies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the evolutionary relationship among species is of fundamental\nimportance to the biological sciences. The location of the root in any\nphylogenetic tree is critical as it gives an order to evolutionary events. None\nof the popular models of nucleotide evolution used in likelihood or Bayesian\nmethods are able to infer the location of the root without exogenous\ninformation. It is known that the most general Markov models of nucleotide\nsubstitution can also not identify the location of the root or be fitted to\nmultiple sequence alignments with less than three sequences. We prove that the\nlocation of the root and the full model can be identified and statistically\nconsistently estimated for a non-stationary, strand-symmetric substitution\nmodel given a multiple sequence alignment with two or more sequences. We also\ngeneralise earlier work to provide a practical means of overcoming the\ncomputationally intractable problem of labelling hidden states in a\nphylogenetic model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 11:43:53 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 00:20:24 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kaehler", "Benjamin D", ""]]}, {"id": "1610.05105", "submitter": "Jolanda Kossakowski", "authors": "Lourens J. Waldorp and Jolanda J. Kossakowski", "title": "Mean field dynamics of graphs I: Evolution of probabilistic cellular\n  automata for random and small-world graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CG physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It was recently shown how graphs can be used to provide descriptions of\npsychopathologies, where symptoms of, say, depression, affect each other and\ncertain configurations determine whether someone could fall into a sudden\ndepression. To analyse changes over time and characterise possible future\nbehaviour is rather difficult for large graphs. We describe the dynamics of\nnetworks using one-dimensional discrete time dynamical systems theory obtained\nfrom a mean field approach to (elementary) probabilistic cellular automata\n(PCA). Often the mean field approach is used on a regular graph (a grid or\ntorus) where each node has the same number of edges and the same probability of\nbecoming active. We show that we can use variations of the mean field of the\ngrid to describe the dynamics of the PCA on a random and small-world graph.\nBifurcation diagrams for the mean field of the grid, random, and small-world\ngraphs indicate possible phase transitions for certain parameter settings.\nExtensive simulations indicate for different graph sizes (number of nodes) that\nthe mean field approximation is accurate. The mean field approach allows us to\nprovide possible explanations of 'jumping' behaviour in depression.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 13:33:39 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 09:15:45 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Waldorp", "Lourens J.", ""], ["Kossakowski", "Jolanda J.", ""]]}, {"id": "1610.05183", "submitter": "Georgios Giasemidis Dr", "authors": "Stephen Haben and Georgios Giasemidis", "title": "A hybrid model of kernel density estimation and quantile regression for\n  GEFCom2014 probabilistic load forecasting", "comments": "9 pages, 1 figure, minor differences to published version. Method\n  achieved top 5 in GEFCom 2014", "journal-ref": "International Journal of Forecasting, 32 (2016) 1017--1022", "doi": "10.1016/j.ijforecast.2015.11.004", "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for generating probabilistic forecasts by combining kernel\ndensity estimation (KDE) and quantile regression techniques, as part of the\nprobabilistic load forecasting track of the Global Energy Forecasting\nCompetition 2014. The KDE method is initially implemented with a time-decay\nparameter. We later improve this method by conditioning on the temperature or\nthe period of the week variables to provide more accurate forecasts. Secondly,\nwe develop a simple but effective quantile regression forecast. The novel\naspects of our methodology are two-fold. First, we introduce symmetry into the\ntime-decay parameter of the kernel density estimation based forecast. Secondly\nwe combine three probabilistic forecasts with different weights for different\nperiods of the month.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 16:04:21 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Haben", "Stephen", ""], ["Giasemidis", "Georgios", ""]]}, {"id": "1610.05948", "submitter": "Dhananjay Ram", "authors": "Dhananjay Ram, Debasis Kundu, Rajesh M. Hegde", "title": "A Bayesian Approach to Estimation of Speaker Normalization Parameters", "comments": "23 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a Bayesian approach to speaker normalization is proposed to\ncompensate for the degradation in performance of a speaker independent speech\nrecognition system. The speaker normalization method proposed herein uses the\ntechnique of vocal tract length normalization (VTLN). The VTLN parameters are\nestimated using a novel Bayesian approach which utilizes the Gibbs sampler, a\nspecial type of Markov Chain Monte Carlo method. Additionally the\nhyperparameters are estimated using maximum likelihood approach. This model is\nused assuming that human vocal tract can be modeled as a tube of uniform cross\nsection. It captures the variation in length of the vocal tract of different\nspeakers more effectively, than the linear model used in literature. The work\nhas also investigated different methods like minimization of Mean Square Error\n(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both\nsingle pass and two pass approaches are then used to build a VTLN based speech\nrecognizer. Experimental results on recognition of vowels and Hindi phrases\nfrom a medium vocabulary indicate that the Bayesian method improves the\nperformance by a considerable margin.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:16:46 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ram", "Dhananjay", ""], ["Kundu", "Debasis", ""], ["Hegde", "Rajesh M.", ""]]}, {"id": "1610.06110", "submitter": "Mustafa Mohamad", "authors": "Han Kyul Joo, Mustafa A. Mohamad, Themistoklis P. Sapsis", "title": "Heavy-tailed response of structural systems subjected to stochastic\n  excitation containing extreme forcing events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the complex, heavy-tailed probability distribution functions\n(pdf) describing the response and its local extrema for structural systems\nsubjected to random forcing that includes extreme events. Our approach is based\non the recent probabilistic decomposition-synthesis technique, where we\ndecouple rare events regimes from the background fluctuations. The result of\nthe analysis has the form of a semi-analytical approximation formula for the\npdf of the response (displacement, velocity, and acceleration) and the pdf of\nthe local extrema. For special limiting cases (lightly damped or heavily damped\nsystems) our analysis provides fully analytical approximations. We also\ndemonstrate how the method can be applied to high dimensional structural\nsystems through a two-degrees-of-freedom structural system undergoing rare\nevents due to intermittent forcing. The derived formulas can be evaluated with\nvery small computational cost and are shown to accurately capture the\ncomplicated heavy-tailed and asymmetrical features in the probability\ndistribution many standard deviations away from the mean, through comparisons\nwith expensive Monte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 17:13:37 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 00:20:37 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 21:36:15 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Joo", "Han Kyul", ""], ["Mohamad", "Mustafa A.", ""], ["Sapsis", "Themistoklis P.", ""]]}, {"id": "1610.06154", "submitter": "Pierre Masselot", "authors": "Pierre Masselot, Sophie Dabo-Niang, Fateh Chebana, Taha B.M.J. Ouarda", "title": "Streamflow forecasting using functional regression", "comments": "30 pages, 12 figures", "journal-ref": "Journal of Hydrology 2016 538:754-66", "doi": "10.1016/j.jhydrol.2016.04.048", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streamflow, as a natural phenomenon, is continuous in time and so are the\nmeteorological variables which influence its variability. In practice, it can\nbe of interest to forecast the whole flow curve instead of points (daily or\nhourly). To this end, this paper introduces the functional linear models and\nadapts it to hydrological forecasting. More precisely, functional linear models\nare regression models based on curves instead of single values. They allow to\nconsider the whole process instead of a limited number of time points or\nfeatures. We apply these models to analyse the flow volume and the whole\nstreamflow curve during a given period by using precipitations curves. The\nfunctional model is shown to lead to encouraging results. The potential of\nfunctional linear models to detect special features that would have been hard\nto see otherwise is pointed out. The functional model is also compared to the\nartificial neural network approach and the advantages and disadvantages of both\nmodels are discussed. Finally, future research directions involving the\nfunctional model in hydrology are presented.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 18:57:20 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Masselot", "Pierre", ""], ["Dabo-Niang", "Sophie", ""], ["Chebana", "Fateh", ""], ["Ouarda", "Taha B. M. J.", ""]]}, {"id": "1610.06195", "submitter": "Leonid Bogachev", "authors": "J\\'anos Gyarmati-Szab\\'o, Leonid V. Bogachev and Haibo Chen", "title": "Nonstationary POT modelling of air pollution concentrations: Statistical\n  analysis of the traffic and meteorological impact", "comments": "21 pages, 2 figures, 3 tables. Accepted for publication in\n  \"Environmetrics\"", "journal-ref": null, "doi": "10.1002/env.2449", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the occurrence, level and duration of high air pollution\nconcentrations exceeding a given critical level enables researchers to study\nthe health impact of road traffic on local air quality and to inform public\npolicy action. Precise estimates of the probabilities of occurrence and level\nof extreme concentrations are formidable due to the combination of complex\nphysical and chemical processes involved. This underpins the need for\ndeveloping sophisticated extreme value models, in particular allowing for\nnon-stationarity of environmental time series. In this paper, extremes of\nnitrogen oxide (NO), nitrogen dioxide (NO$_2$) and ozone (O$_3$) concentrations\nare investigated using two models. Model I is based on an extended\npeaks-over-threshold (POT) approach developed by A. C. Davison and R. L. Smith,\nwhereby the parameters of the underlying generalized Pareto distribution (GPD)\nare treated as functions of covariates (i.e., traffic and meteorological\nfactors). The new Model II resolves the lack of threshold stability in the\nDavison--Smith model by constructing a special functional form for the GPD\nparameters. For each of the models, the effects of trafic and meteorological\nfactors on the frequency and size of extreme values are estimated using Markov\nchain Monte Carlo methods. Finally, appropriate goodness-of-fit tests and model\nselection criteria confirm that Model II significantly outperforms Model I in\nestimation and forecasting of extremes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 18:38:57 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 08:55:49 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gyarmati-Szab\u00f3", "J\u00e1nos", ""], ["Bogachev", "Leonid V.", ""], ["Chen", "Haibo", ""]]}, {"id": "1610.06242", "submitter": "Christopher Marks", "authors": "Jytte Klausen, Christopher Marks, Tauhid Zaman", "title": "Finding Online Extremists in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online extremists in social networks pose a new form of threat to the general\npublic. These extremists range from cyberbullies who harass innocent users to\nterrorist organizations such as the Islamic State of Iraq and Syria (ISIS) that\nuse social networks to recruit and incite violence. Currently social networks\nsuspend the accounts of such extremists in response to user complaints. The\nchallenge is that these extremist users simply create new accounts and continue\ntheir activities. In this work we present a new set of operational capabilities\nto deal with the threat posed by online extremists in social networks.\n  Using data from several hundred thousand extremist accounts on Twitter, we\ndevelop a behavioral model for these users, in particular what their accounts\nlook like and who they connect with. This model is used to identify new\nextremist accounts by predicting if they will be suspended for extremist\nactivity. We also use this model to track existing extremist users as they\ncreate new accounts by identifying if two accounts belong to the same user.\nFinally, we present a model for searching the social network to efficiently\nfind suspended users' new accounts based on a variant of the classic Polya's\nurn setup. We find a simple characterization of the optimal search policy for\nthis model under fairly general conditions. Our urn model and main theoretical\nresults generalize easily to search problems in other fields.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 22:23:27 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Klausen", "Jytte", ""], ["Marks", "Christopher", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1610.06462", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael Gutmann, Aki Vehtari, Pekka Marttinen", "title": "Gaussian process modeling in approximate Bayesian computation to\n  estimate horizontal gene transfer in bacteria", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) can be used for model fitting when the\nlikelihood function is intractable but simulating from the model is feasible.\nHowever, even a single evaluation of a complex model may take several hours,\nlimiting the number of model evaluations available. Modelling the discrepancy\nbetween the simulated and observed data using a Gaussian process (GP) can be\nused to reduce the number of model evaluations required by ABC, but the\nsensitivity of this approach to a specific GP formulation has not yet been\nthoroughly investigated. We begin with a comprehensive empirical evaluation of\nusing GPs in ABC, including various transformations of the discrepancies and\ntwo novel GP formulations. Our results indicate the choice of GP may\nsignificantly affect the accuracy of the estimated posterior distribution.\nSelection of an appropriate GP model is thus important. We formulate expected\nutility to measure the accuracy of classifying discrepancies below or above the\nABC threshold, and show that it can be used to automate the GP model selection\nstep. Finally, based on the understanding gained with toy examples, we fit a\npopulation genetic model for bacteria, providing insight into horizontal gene\ntransfer events within the population and from external origins.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:39:15 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:57:22 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 12:24:27 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1610.06551", "submitter": "Yanning Shen", "authors": "Yanning Shen, Brian Baingana, Georgios B. Giannakis", "title": "Nonlinear Structural Vector Autoregressive Models for Inferring\n  Effective Brain Network Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models (SEMs) and vector autoregressive models (VARMs)\nare two broad families of approaches that have been shown useful in effective\nbrain connectivity studies. While VARMs postulate that a given region of\ninterest in the brain is directionally connected to another one by virtue of\ntime-lagged influences, SEMs assert that causal dependencies arise due to\ncontemporaneous effects, and may even be adopted when nodal measurements are\nnot necessarily multivariate time series. To unify these complementary\nperspectives, linear structural vector autoregressive models (SVARMs) that\nleverage both contemporaneous and time-lagged nodal data have recently been put\nforth. Albeit simple and tractable, linear SVARMs are quite limited since they\nare incapable of modeling nonlinear dependencies between neuronal time series.\nTo this end, the overarching goal of the present paper is to considerably\nbroaden the span of linear SVARMs by capturing nonlinearities through kernels,\nwhich have recently emerged as a powerful nonlinear modeling framework in\ncanonical machine learning tasks, e.g., regression, classification, and\ndimensionality reduction. The merits of kernel-based methods are extended here\nto the task of learning the effective brain connectivity, and an efficient\nregularized estimator is put forth to leverage the edge sparsity inherent to\nreal-world complex networks. Judicious kernel choice from a preselected\ndictionary of kernels is also addressed using a data-driven approach. Extensive\nnumerical tests on ECoG data captured through a study on epileptic seizures\ndemonstrate that it is possible to unveil previously unknown causal links\nbetween brain regions of interest.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:37:46 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Shen", "Yanning", ""], ["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1610.06599", "submitter": "Adam Rahman", "authors": "Adam Rahman and Wayne Oldford", "title": "Euclidean distance matrix completion and point configurations from the\n  minimal spanning tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a special case of the Euclidean distance matrix\ncompletion problem (edmcp) of interest in statistical data analysis where only\nthe minimal spanning tree distances are given and the matrix completion must\npreserve the minimal spanning tree. Two solutions are proposed, one an\nadaptation of a more general method based on a dissimilarity parameterized\nformulation, the other an entirely novel method which constructs the point\nconfiguration directly through a guided random search. These methods as well as\nthree standard edcmp methods are described and compared experimentally on real\nand synthetic data. It is found that the constructive method given by the\nguided random search algorithm clearly outperforms all others considered here.\nNotably, standard methods including the adaptation force peculiar, and\ngenerally unwanted, geometric structure on the point configurations their\ncompletions produce.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 20:41:07 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rahman", "Adam", ""], ["Oldford", "Wayne", ""]]}, {"id": "1610.06640", "submitter": "Andriy Olenko", "authors": "A. Olenko, K. T. Wong, H. Mir, and H. Al-Nashash", "title": "A Generalized Correlation Index for Quantifying Signal Morphological\n  Similarity", "comments": "2 two-column pages, 3 figures", "journal-ref": null, "doi": "10.1049/el.2016.2974", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical applications, the similarity between a signal measured from an\ninjured subject and a reference signal measured from a normal subject can be\nused to quantify the injury severity. This paper proposes a generalization of\nthe adaptive signed correlation index (ASCI) to account for specific signal\nfeatures of interest and extend the trichotomization of conventional ASCI to an\narbitrary number of levels. In the context of spinal cord injury assessment, a\ncomputational example is presented to illustrate the enhanced resolution of the\nproposed measure and its ability to offer a more refined measure of the level\nof injury.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 01:25:00 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Olenko", "A.", ""], ["Wong", "K. T.", ""], ["Mir", "H.", ""], ["Al-Nashash", "H.", ""]]}, {"id": "1610.06731", "submitter": "Evgeny Burnaev", "authors": "Alexey Zaytsev and Evgeny Burnaev", "title": "Minimax Error of Interpolation and Optimal Design of Experiments for\n  Variable Fidelity Data", "comments": "25 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering problems often involve data sources of variable fidelity with\ndifferent costs of obtaining an observation. In particular, one can use both a\ncheap low fidelity function (e.g. a computational experiment with a CFD code)\nand an expensive high fidelity function (e.g. a wind tunnel experiment) to\ngenerate a data sample in order to construct a regression model of a high\nfidelity function. The key question in this setting is how the sizes of the\nhigh and low fidelity data samples should be selected in order to stay within a\ngiven computational budget and maximize accuracy of the regression model prior\nto committing resources on data acquisition.\n  In this paper we obtain minimax interpolation errors for single and variable\nfidelity scenarios for a multivariate Gaussian process regression. Evaluation\nof the minimax errors allows us to identify cases when the variable fidelity\ndata provides better interpolation accuracy than the exclusively high fidelity\ndata for the same computational budget.\n  These results allow us to calculate the optimal shares of variable fidelity\ndata samples under the given computational budget constraint. Real and\nsynthetic data experiments suggest that using the obtained optimal shares often\noutperforms natural heuristics in terms of the regression accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 10:24:08 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:02:57 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 10:58:19 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Zaytsev", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1610.06953", "submitter": "Th\\'eo Michelot", "authors": "Th\\'eo Michelot, Roland Langrock, Sophie Bestley, Ian D. Jonsen,\n  Theoni Photopoulou, Toby A. Patterson", "title": "Estimation and simulation of foraging trips in land-based marine\n  predators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behaviour of colony-based marine predators is the focus of much research\nglobally. Large telemetry and tracking data sets have been collected for this\ngroup of animals, and are accompanied by many theoretical studies of optimal\nforaging strategies. However, relatively few studies have detailed statistical\nmethods for inferring behaviours in central place foraging trips. In this paper\nwe describe an approach based on hidden Markov models, which splits foraging\ntrips into segments labelled as \"outbound\", \"search\", \"forage\", and \"inbound\".\nBy structuring the hidden Markov model transition matrix appropriately, the\nmodel naturally handles the sequence of behaviours within a foraging trip.\nAdditionally, by structuring the model in this way, we are able to develop\nrealistic simulations from the fitted model. We demonstrate our approach on\ndata from southern elephant seals (Mirounga leonina) tagged on Kerguelen Island\nin the Southern Ocean. We discuss the differences between our 4-state model and\nthe widely used 2-state model, and the advantages and disadvantages of\nemploying a more complex model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 09:51:28 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 10:29:28 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 13:48:45 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Michelot", "Th\u00e9o", ""], ["Langrock", "Roland", ""], ["Bestley", "Sophie", ""], ["Jonsen", "Ian D.", ""], ["Photopoulou", "Theoni", ""], ["Patterson", "Toby A.", ""]]}, {"id": "1610.07024", "submitter": "Purba Das", "authors": "Purba Das, Ananya Lahiri and Sourish Das", "title": "Understanding Sea Ice Melting via Functional Data Analysis", "comments": "9 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we considered the problem of sea ice cover is melting.\nConsidering the `satellite passive microwave remote sensing data' as functional\ndata, we studied daily observation of sea ice cover of each year as a smooth\ncontinuous function of time. We investigated the mean function for the sea ice\narea for following decades and computed the corresponding $95\\%$ bootstrap\nconfidence interval for the both Arctic and Antarctic Oceans. We found the mean\nfunction for the sea ice area dropped statistically significantly in recent\ndecades for the Arctic Ocean. However, no such statistical evidence was found\nfor the Antarctic ocean. Essentially, the mean function for sea ice area in the\nAntarctic Ocean is unchanged. Additional evidence of the melting of sea ice\narea in the Arctic Ocean is provided by three types of phase curve (namely,\nArea vs. Velocity, Area vs. Acceleration, and Velocity Vs. Acceleration). In\nthe Arctic Ocean, during the summer, the current decades is observing the size\nof the sea ice area about $30\\%$ less, than what it used to be during the first\ndecade. In this article, we have taken a distribution-free approach for our\nanalysis, except the data generating process, belongs to the Hilbert space.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 09:53:36 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Das", "Purba", ""], ["Lahiri", "Ananya", ""], ["Das", "Sourish", ""]]}, {"id": "1610.07278", "submitter": "Oscar Garc\\'ia", "authors": "Oscar Garc\\'ia", "title": "Cohort aggregation modelling for complex forest stands: Spruce-aspen\n  mixtures in British Columbia", "comments": "Accepted manuscript, to appear in Ecological Modelling", "journal-ref": "Ecological Modelling 343: 109-122, 2017", "doi": "10.1016/j.ecolmodel.2016.10.020", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-species growth models are needed as a synthesis of ecological knowledge\nand for guiding forest management. Individual-tree models have been commonly\nused, but the difficulties of reliably scaling from the individual to the stand\nlevel are often underestimated. Emergent properties and statistical issues\nlimit their effectiveness. A more holistic modelling of aggregates at the whole\nstand level is a potentially attractive alternative. This work explores\nmethodology for developing biologically consistent dynamic mixture models where\nthe state is described by aggregate stand-level variables for species or\nage/size cohorts. The methods are demonstrated and tested with a two-cohort\nmodel for spruce-aspen mixtures named SAM. The models combine single-species\nsubmodels and submodels for resource partitioning among the cohorts. The\npartitioning allows for differences in competitive strength among species and\nsize classes, and for complementarity effects. Height growth reduction in\nsuppressed cohorts is also modelled. SAM fits well the available data, and\nexhibits behaviors consistent with current ecological knowledge. The general\nframework can be applied to any number of cohorts, and should be useful as a\nbasis for modelling other mixed-species or uneven-aged stands.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 04:43:40 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 03:03:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Garc\u00eda", "Oscar", ""]]}, {"id": "1610.07485", "submitter": "Aureli Alabert", "authors": "Carme Font and Merc\\`e Farr\\'e and Aureli Alabert", "title": "Relating Diversity and Human Appropriation from Land Cover Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to describe the relation between indicators of landscape\ndiversity and the human appropriation of the net primary production in a given\nregion. These quantities are viewed as functions of the vector of proportions\nof the different land covers, which is in turn treated as a random vector whose\nvalues depend on the particular small terrain cell that is observed.\n  We illustrate the method assuming first that the vector of proportions\nfollows a uniform distribution on the simplex. We then consider as starting\npoint a raw dataset of observed proportions for each cell, for which we must\nfirst obtain an estimate of its theoretical probability distribution, and\nsecondly generate a sample of large size from it. We apply this procedure to\nreal historical data of the Mallorca Island in three different moments of time.\n  Our main goal is to compute the mean value of the landscape diversity as a\nfunction of the level of human appropriation. This function is related to the\nso-called Energy-Species hypothesis and to the Intermediate Disturbance\nHypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:40:40 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Font", "Carme", ""], ["Farr\u00e9", "Merc\u00e8", ""], ["Alabert", "Aureli", ""]]}, {"id": "1610.07524", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova", "title": "Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments", "comments": "FATML 2016 conference paper. A long version of the paper available on\n  the author's website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:23:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chouldechova", "Alexandra", ""]]}, {"id": "1610.07670", "submitter": "Bai Jiang", "authors": "Bai Jiang, Xiaolin Shi, Hongwei Shang, Zhigeng Geng, Alyssa Glass", "title": "A Framework for Network AB Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing, also known as controlled experiment, bucket testing or splitting\ntesting, has been widely used for evaluating a new feature, service or product\nin the data-driven decision processes of online websites. The goal of A/B\ntesting is to estimate or test the difference between the treatment effects of\nthe old and new variations. It is a well-studied two-sample comparison problem\nif each user's response is influenced by her treatment only. However, in many\napplications of A/B testing, especially those in HIVE of Yahoo and other social\nnetworks of Microsoft, Facebook, LinkedIn, Twitter and Google, users in the\nsocial networks influence their friends via underlying social interactions, and\nthe conventional A/B testing methods fail to work. This paper considers the\nnetwork A/B testing problem and provide a general framework consisting of five\nsteps: data sampling, probabilistic model, parameter inference, computing\naverage treatment effect and hypothesis test. The framework performs well for\nnetwork A/B testing in simulation studies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 22:24:09 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Jiang", "Bai", ""], ["Shi", "Xiaolin", ""], ["Shang", "Hongwei", ""], ["Geng", "Zhigeng", ""], ["Glass", "Alyssa", ""]]}, {"id": "1610.07683", "submitter": "Ming Yuan", "authors": "Shulei Wang and Ming Yuan", "title": "Combined Hypothesis Testing on Graphs with Applications to Gene Set\n  Enrichment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by gene set enrichment analysis, we investigate the problem of\ncombined hypothesis testing on a graph. We introduce a general framework to\neffectively use the structural information of the underlying graph when testing\nmultivariate means. A new testing procedure is proposed within this framework.\nWe show that the test is optimal in that it can consistently detect departure\nfrom the collective null at a rate that no other test could improve, for almost\nall graphs. We also provide general performance bounds for the proposed test\nunder any specific graph, and illustrate their utility through several common\ntypes of graphs. Numerical experiments are presented to further demonstrate the\nmerits of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 23:51:07 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wang", "Shulei", ""], ["Yuan", "Ming", ""]]}, {"id": "1610.07684", "submitter": "Yuxiao Wang", "authors": "Yuxiao Wang, Chee-Ming Ting and Hernando Ombao", "title": "Exploratory Analysis of High Dimensional Time Series with Applications\n  to Multichannel Electroencephalograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the the major hurdle of high dimensionality in EEG\nanalysis by extracting the optimal lower dimensional representations. Using our\napproach, connectivity between regions in a high-dimensional brain network is\ncharacterized through the connectivity between region-specific factors. The\nproposed approach is motivated by our observation that electroencephalograms\n(EEGs) from channels within each region exhibit a high degree of\nmulticollinearity and synchrony. These observations suggest that it would be\nsensible to extract summary factors for each region. We consider the general\napproach for deriving summary factors which are solutions to the criterion of\nsquared error reconstruction. In this work, we focus on two special cases of\nlinear auto encoder and decoder. In the first approach, the factors are\ncharacterized as instantaneous linear mixing of the observed high dimensional\ntime series. In the second approach, the factors signals are linear filtered\nversions of the original signal which is more general than an instantaneous\nmixing. This exploratory analysis is the starting point to the multi-scale\nfactor analysis model where the concatenated factors from all regions are\nrepresented by vector auto-regressive model that captures the connectivity in\nhigh dimensional signals. We performed evaluations on the two approaches via\nsimulations under different conditions. The simulation results provide insights\non the performance and application scope of the methods. We also performed\nexploratory analysis of EEG recorded over several epochs during resting state.\nFinally, we implemented these exploratory methods in a Matlab toolbox XHiDiTS\navailable from https://goo.gl/uXc8ei .\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 23:53:03 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wang", "Yuxiao", ""], ["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""]]}, {"id": "1610.07748", "submitter": "Nikolay Doudchenko", "authors": "Nikolay Doudchenko and Guido W. Imbens", "title": "Balancing, Regression, Difference-In-Differences and Synthetic Control\n  Methods: A Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH), see also\nAbadie and Gardeazabal [2003], Abadie et al. [2014], develop the synthetic\ncontrol procedure for estimating the effect of a treatment, in the presence of\na single treated unit and a number of control units, with pre-treatment\noutcomes observed for all units. The method constructs a set of weights such\nthat selected covariates and pre-treatment outcomes of the treated unit are\napproximately matched by a weighted average of control units (the synthetic\ncontrol). The weights are restricted to be nonnegative and sum to one, which is\nimportant because it allows the procedure to obtain unique weights even when\nthe number of lagged outcomes is modest relative to the number of control\nunits, a common setting in applications. In the current paper we propose a\ngeneralization that allows the weights to be negative, and their sum to differ\nfrom one, and that allows for a permanent additive difference between the\ntreated unit and the controls, similar to difference-in-difference procedures.\nThe weights directly minimize the distance between the lagged outcomes for the\ntreated and the control units, using regularization methods to deal with a\npotentially large number of possible control units.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:00:36 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 02:55:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Doudchenko", "Nikolay", ""], ["Imbens", "Guido W.", ""]]}, {"id": "1610.08088", "submitter": "Art Owen", "authors": "K. Gao and A. B. Owen", "title": "Estimation and Inference for Very Large Linear Mixed Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models with large imbalanced crossed random effects structures\npose severe computational problems for maximum likelihood estimation and for\nBayesian analysis. The costs can grow as fast as $N^{3/2}$ when there are N\nobservations. Such problems arise in any setting where the underlying factors\nsatisfy a many to many relationship (instead of a nested one) and in electronic\ncommerce applications, the N can be quite large. Methods that do not account\nfor the correlation structure can greatly underestimate uncertainty. We propose\na method of moments approach that takes account of the correlation structure\nand that can be computed at O(N) cost. The method of moments is very amenable\nto parallel computation and it does not require parametric distributional\nassumptions, tuning parameters or convergence diagnostics. For the regression\ncoefficients, we give conditions for consistency and asymptotic normality as\nwell as a consistent variance estimate. For the variance components, we give\nconditions for consistency and we use consistent estimates of a mildly\nconservative variance estimate. All of these computations can be done in O(N)\nwork. We illustrate the algorithm with some data from Stitch Fix where the\ncrossed random effects correspond to clients and items.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:38:24 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 23:28:55 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gao", "K.", ""], ["Owen", "A. B.", ""]]}, {"id": "1610.08139", "submitter": "Mihir Arjunwadkar", "authors": "Mihir Arjunwadkar, Akanksha Kashikar, and Manjari Bagchi", "title": "Neutron stars in the light of SKA: Data, statistics, and science", "comments": "To appear in Journal of Astrophysics and Astronomy (JOAA) special\n  issue on \"Science with the SKA: an Indian perspective\"", "journal-ref": null, "doi": "10.1007/s12036-016-9410-0", "report-no": null, "categories": "astro-ph.IM astro-ph.HE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Square Kilometre Array (SKA), when it becomes functional, is expected to\nenrich neutron star (NS) catalogues by at least an order of magnitude over\ntheir current state. This includes the discovery of new NS objects leading to\nbetter sampling of under-represented NS categories, precision measurements of\nintrinsic properties such as spin period and magnetic field, as also data on\nrelated phenomena such as microstructure, nulling, glitching, etc. This will\npresent a unique opportunity to seek answers to interesting and fundamental\nquestions about the extreme physics underlying these exotic objects in the\nuniverse. In this paper, we first present a meta-analysis (from a\nmethodological viewpoint) of statistical analyses performed using existing NS\ndata, with a two-fold goal: First, this should bring out how statistical models\nand methods are shaped and dictated by the science problem being addressed.\nSecond, it is hoped that these analyses will provide useful starting points for\ndeeper analyses involving richer data from SKA whenever it becomes available.\nWe also describe a few other areas of NS science which we believe will benefit\nfrom SKA which are of interest to the Indian NS community.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 01:31:55 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Arjunwadkar", "Mihir", ""], ["Kashikar", "Akanksha", ""], ["Bagchi", "Manjari", ""]]}, {"id": "1610.08246", "submitter": "Adway Mitra", "authors": "Adway Mitra, Ashwin K. Seshadri", "title": "Exploring Spatial Coherence in Inter-annual Changes and Annual Extremes\n  of Rainfall over India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasts of monsoon rainfall for India are made at national scale. But there\nis spatial coherence and heterogeneity that is relevant to forecasting. This\npaper considers year-to-year rainfall change and annual extremes at\nsub-national scales. We use Data Mining techniques to gridded rain-gauge data\nfor 1901-2011 to characterize coherence and heterogeneity and identify\nspatially homogeneous clusters. We study the direction of change in rainfall\nbetween years (Phase), and extreme annual rainfall at both grid level and\nnational level. Grid-level Phase is found to be spatially coherent, and\nsignificantly correlated with all-India mean rainfall (AIMR) phase. Grid-level\nextreme-rainfall years are not strongly associated with corresponding extremes\nin AIMR, although in extreme AIMR years local extremes of the same type occur\nwith higher spatial coherence. Years of extremes in AIMR entail widespread\nphase of the corresponding sign. Furthermore, local extremes and phase are\nfound to frequently co-occur in spatially contiguous clusters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:23:28 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Mitra", "Adway", ""], ["Seshadri", "Ashwin K.", ""]]}, {"id": "1610.08386", "submitter": "Ra\\'ul Torres", "authors": "Ra\\'ul Torres and Elena Di Bernardino and Henry Laniado and Rosa E.\n  Lillo", "title": "On the estimation of extreme directional multivariate quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate extreme value theory (MEVT), the focus is on analysis outside\nof the observable sampling zone, which implies that the region of interest is\nassociated to high risk levels. This work provides tools to include directional\nnotions into the MEVT, giving the opportunity to characterize the recently\nintroduced directional multivariate quantiles (DMQ) at high levels. Then, an\nout-sample estimation method for these quantiles is given. A bootstrap\nprocedure carries out the estimation of the tuning parameter in this\nmultivariate framework and helps with the estimation of the DMQ. Asymptotic\nnormality for the proposed estimator is provided and the methodology is\nillustrated with simulated data-sets. Finally, a real-life application to a\nfinancial case is also performed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 15:42:13 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 21:28:57 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 11:01:28 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 16:58:28 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2018 12:59:42 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Torres", "Ra\u00fal", ""], ["Di Bernardino", "Elena", ""], ["Laniado", "Henry", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1610.08450", "submitter": "Dimitrije Markovic", "authors": "Dimitrije Markovi\\'c, Borjana Val\\v{c}i\\'c, and Neboj\\v{s}a\n  Male\\v{s}evi\\'c", "title": "Body movement to sound interface with vector autoregressive hierarchical\n  hidden Markov models", "comments": "12 pages, 7 figures, a pre-submission draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interfacing a kinetic action of a person to an action of a machine system is\nan important research topic in many application areas. One of the key factors\nfor intimate human-machine interaction is the ability of the control algorithm\nto detect and classify different user commands with shortest possible latency,\nthus making a highly correlated link between cause and effect. In our research,\nwe focused on the task of mapping user kinematic actions into sound samples.\nThe presented methodology relies on the wireless sensor nodes equipped with\ninertial measurement units and the real-time algorithm dedicated for early\ndetection and classification of a variety of movements/gestures performed by a\nuser. The core algorithm is based on the approximate Bayesian inference of\nVector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where\nmodels database is derived from the set of motion gestures. The performance of\nthe algorithm was compared with an online version of the K-nearest neighbours\n(KNN) algorithm, where we used offline expert based classification as the\nbenchmark. In almost all of the evaluation metrics (e.g. confusion matrix,\nrecall and precision scores) the VAR-HHMM algorithm outperformed KNN.\nFurthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement\nonset detection compared with the offline standard. The proposed concept,\nalthough envisioned for movement-to-sound application, could be implemented in\nother human-machine interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:27:27 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Markovi\u0107", "Dimitrije", ""], ["Val\u010di\u0107", "Borjana", ""], ["Male\u0161evi\u0107", "Neboj\u0161a", ""]]}, {"id": "1610.08665", "submitter": "Gianluca Frasso", "authors": "Gianluca Frasso and Paul H.C. Eilers", "title": "Direct semi-parametric estimation of the state price density implied in\n  option prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for direct semi-parametric estimation of the State Price\nDensity (SPD) implied in quoted option prices. We treat the observed prices as\nexpected values of possible pay-offs at maturity, weighted by the unknown\nprobability density function. We model the logarithm of the latter as a smooth\nfunction while matching the expected values of the potential pay-offs with the\nobserved prices. This leads to a special case of the penalized composite link\nmodel. Our estimates do not rely on any parametric assumption on the underlying\nasset price dynamics and are consistent with no-arbitrage conditions. The model\nshows excellent performance in simulations and in application to real data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 09:03:58 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 14:58:15 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 08:09:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Frasso", "Gianluca", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1610.08718", "submitter": "Manuel Oviedo de la Fuente", "authors": "Manuel Oviedo de la Fuente, Manuel Febrero Bande, Mar\\'ia Pilar\n  Mu\\~noz and \\`Angela Dom\\'inguez", "title": "Predicting seasonal influenza transmission using Regression Models with\n  Temporal Dependence", "comments": "25 pages, 2 figures, 10 tables, R code in fda.usc package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we use meteorological information in Galicia (Spain) to\npropose a novel approach to predict the incidence of influenza. Our approach\nextends the GLS methods in the multivariate framework to functional regression\nmodels with dependent errors. A simulation study shows that the GLS estimators\nrender better estimations of the parameters associated with the regression\nmodel and obtain extremely good results from the predictive point of view. Thus\nthey improve the classical linear approach. It proposes an iterative version of\nthe GLS estimator (called iGLS) that can help to model complicated dependence\nstructures, uses the distance correlation measure $\\mathcal{R}$ to select\nrelevant information to predict influenza rate and applies the GLS procedure to\nthe prediction of the influenza rate using readily available functional\nvariables. These kinds of models are extremely useful to health managers in\nallocating resources in advance for an epidemic outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 11:41:29 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["de la Fuente", "Manuel Oviedo", ""], ["Bande", "Manuel Febrero", ""], ["Mu\u00f1oz", "Mar\u00eda Pilar", ""], ["Dom\u00ednguez", "\u00c0ngela", ""]]}, {"id": "1610.08974", "submitter": "Yunfan Tang", "authors": "Yunfan Tang, Li Ma and Dan L. Niclolae", "title": "A phylogenetic scan test on Dirichlet-tree multinomial model for\n  microbiome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the phylogenetic scan test (PhyloScan) for\ninvestigating cross-group differences in microbiome compositions using the\nDirichlet-tree multinomial (DTM) model. DTM models the microbiome data through\na cascade of independent local DMs on the internal nodes of the phylogenetic\ntree. Each of the local DMs captures the count distributions of a certain\nnumber of operational taxonomic units at a given resolution. Since\ndistributional differences tend to occur in clusters along evolutionary\nlineages, we design a scan statistic over the phylogenetic tree to allow nodes\nto borrow signal strength from their parents and children. We also derive a\nformula to bound the tail probability of the scan statistic, and verify its\naccuracy through simulations. The PhyloScan procedure is applied to the\nAmerican Gut dataset to identify taxa associated with diet habits. Empirical\nstudies performed on this dataset show that PhyloScan achieves higher testing\npower in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 19:59:22 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 14:49:53 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Tang", "Yunfan", ""], ["Ma", "Li", ""], ["Niclolae", "Dan L.", ""]]}, {"id": "1610.09041", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami and Yoshiki Yamagata", "title": "Estimation of gridded population and GDP scenarios with spatially\n  explicit statistical downscaling", "comments": "This manuscript is submit to Environmental Research Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study downscales the population and gross domestic product (GDP)\nscenarios given under Shared Socioeconomic Pathways (SSPs) into 0.5-degree\ngrids. Our downscale approach has the following features: (i) it explicitly\nconsiders spatial and socioeconomic interactions among cities; (ii) it utilizes\nauxiliary variables, including, road network and land cover; (iii) it\nendogenously estimates influence from each factor by a model ensemble approach;\n(iv) it allows us controlling urban shrinkage/dispersion depending on SSPs. It\nis confirmed that our downscaling results are consistent with scenario\nassumptions (e.g., concentration in SSP1 and dispersion in SSP3). Besides,\nwhile existing grid-level scenario tends to have overly-smoothed population\ndistributions in non-urban areas, ours does not suffer from the problem, and\ncaptures difference in urban and non-urban areas in a more reasonable manner.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 00:30:51 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 05:57:04 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Murakami", "Daisuke", ""], ["Yamagata", "Yoshiki", ""]]}, {"id": "1610.09108", "submitter": "Jonas Haslbeck", "authors": "Jonas Haslbeck and Lourens J Waldorp", "title": "How well do Network Models predict Observations? On the Importance of\n  Predictability in Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are an increasingly popular way to abstract complex\npsychological phenomena. While the study of the structure of network models has\nled to many important insights, little attention is paid to how well they\npredict observations. This is despite the fact that predictability is crucial\nfor judging the practical relevance of edges: for instance in clinical\npractice, predictability of a symptom indicates whether a an intervention on\nthat symptom through the symptom network is promising. We close this\nmethodological gap by introducing nodewise predictability, which quantifies how\nwell a given node can be predicted by all other nodes it is connected to in the\nnetwork. In addition, we provide fully reproducible code examples of how to\ncompute and visualize nodewise predictability both for cross-sectional and\ntime-series data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 08:02:22 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 06:28:52 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 13:24:34 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Haslbeck", "Jonas", ""], ["Waldorp", "Lourens J", ""]]}, {"id": "1610.09294", "submitter": "Silvia Montagna", "authors": "Pantelis Samartsidis, Silvia Montagna, Thomas E. Nichols, Timothy D.\n  Johnson", "title": "The coordinate-based meta-analysis of neuroimaging data", "comments": null, "journal-ref": "Statist. Sci. Volume 32, Number 4 (2017), 580-599", "doi": "10.1214/17-STS624", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging meta-analysis is an area of growing interest in statistics. The\nspecial characteristics of neuroimaging data render classical meta-analysis\nmethods inapplicable and therefore new methods have been developed. We review\nexisting methodologies, explaining the benefits and drawbacks of each. A\ndemonstration on a real dataset of emotion studies is included. We discuss some\nstill-open problems in the field to highlight the need for future research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:12:38 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 15:31:20 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Samartsidis", "Pantelis", ""], ["Montagna", "Silvia", ""], ["Nichols", "Thomas E.", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "1610.09388", "submitter": "Tobias M\\\"utze", "authors": "Tobias M\\\"utze, Frank Konietschke, Axel Munk, Tim Friede", "title": "A studentized permutation test for three-arm trials in the 'gold\n  standard' design", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7176", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 'gold standard' design for three-arm trials refers to trials with an\nactive control and a placebo control in addition to the experimental treatment\ngroup. This trial design is recommended when being ethically justifiable and it\nallows the simultaneous comparison of experimental treatment, active control,\nand placebo. Parametric testing methods have been studied plentifully over the\npast years. However, these methods often tend to be liberal or conservative\nwhen distributional assumptions are not met particularly with small sample\nsizes. In this article, we introduce a studentized permutation test for testing\nnon-inferiority and superiority of the experimental treatment compared to the\nactive control in three-arm trials in the `gold standard' design. The\nperformance of the studentized permutation test for finite sample sizes is\nassessed in a Monte-Carlo simulation study under various parameter\nconstellations. Emphasis is put on whether the studentized permutation test\nmeets the target significance level. For comparison purposes, commonly used\nWald-type tests are included in the simulation study. The simulation study\nshows that the presented studentized permutation test for assessing\nnon-inferiority in three-arm trials in the 'gold standard' design outperforms\nits competitors for count data. The methods discussed in this paper are\nimplemented in the R package ThreeArmedTrials which is available on the\ncomprehensive R archive network (CRAN).\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 20:13:40 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["M\u00fctze", "Tobias", ""], ["Konietschke", "Frank", ""], ["Munk", "Axel", ""], ["Friede", "Tim", ""]]}, {"id": "1610.09485", "submitter": "David Watson", "authors": "David Watson and Luciano Floridi", "title": "Crowdsourced science: sociotechnical epistemology in the e-research\n  paradigm", "comments": "Synthese, October 2016", "journal-ref": null, "doi": "10.1007/s11229-016-1238-2", "report-no": null, "categories": "stat.AP cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent years have seen a surge in online collaboration between experts and\namateurs on scientific research. In this article, we analyse the\nepistemological implications of these crowdsourced projects, with a focus on\nZooniverse, the world's largest citizen science web portal. We use quantitative\nmethods to evaluate the platform's success in producing large volumes of\nobservation statements and high impact scientific discoveries relative to more\nconventional means of data processing. Through empirical evidence, Bayesian\nreasoning, and conceptual analysis, we show how information and communication\ntechnologies enhance the reliability, scalability, and connectivity of\ncrowdsourced e-research, giving online citizen science projects powerful\nepistemic advantages over more traditional modes of scientific investigation.\nThese results highlight the essential role played by technologically mediated\nsocial interaction in contemporary knowledge production. We conclude by calling\nfor an explicitly sociotechnical turn in the philosophy of science that\ncombines insights from statistics and logic to analyse the latest developments\nin scientific research.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 10:51:14 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Watson", "David", ""], ["Floridi", "Luciano", ""]]}, {"id": "1610.09750", "submitter": "Vadim Sokolov", "authors": "Eric Jacquier and Nicholas Polson and Vadim Sokolov", "title": "Sequential Bayesian Learning for Merton's Jump Model with Stochastic\n  Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump stochastic volatility models are central to financial econometrics for\nvolatility forecasting, portfolio risk management, and derivatives pricing.\nMarkov Chain Monte Carlo (MCMC) algorithms are computationally unfeasible for\nthe sequential learning of volatility state variables and parameters, whereby\nthe investor must update all posterior and predictive densities as new\ninformation arrives. We develop a particle filtering and learning algorithm to\nsample posterior distribution in Merton's jump stochastic volatility. This\nallows to filter spot volatilities and jump times, together with sequentially\nupdating (learning) of jump and volatility parameters. We illustrate our\nmethodology on Google's stock return. We conclude with directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 00:53:08 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Jacquier", "Eric", ""], ["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1610.09780", "submitter": "Rebecca Steorts", "authors": "Giacomo Zanella, Brenda Betancourt, Hanna Wallach, Jeffrey Miller,\n  Abbas Zaidi, and Rebecca C. Steorts", "title": "Flexible Models for Microclustering with Application to Entity\n  Resolution", "comments": "15 pages, 3 figures, 1 table, to appear NIPS 2016. arXiv admin note:\n  text overlap with arXiv:1512.00792", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some applications, this\nassumption is inappropriate. For example, when performing entity resolution,\nthe size of each cluster should be unrelated to the size of the data set, and\neach cluster should contain a negligible fraction of the total number of data\npoints. These applications require models that yield clusters whose sizes grow\nsublinearly with the size of the data set. We address this requirement by\ndefining the microclustering property and introducing a new class of models\nthat can exhibit this property. We compare models within this class to two\ncommonly used clustering models using four entity-resolution data sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:00:11 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Zanella", "Giacomo", ""], ["Betancourt", "Brenda", ""], ["Wallach", "Hanna", ""], ["Miller", "Jeffrey", ""], ["Zaidi", "Abbas", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1610.09787", "submitter": "Dustin Tran", "authors": "Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang,\n  David M. Blei", "title": "Edward: A library for probabilistic modeling, inference, and criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.PL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is a powerful approach for analyzing empirical\ninformation. We describe Edward, a library for probabilistic modeling. Edward's\ndesign reflects an iterative process pioneered by George Box: build a model of\na phenomenon, make inferences about the model given data, and criticize the\nmodel's fit to the data. Edward supports a broad class of probabilistic models,\nefficient algorithms for inference, and many techniques for model criticism.\nThe library builds on top of TensorFlow to support distributed training and\nhardware such as GPUs. Edward enables the development of complex probabilistic\nmodels and their algorithms at a massive scale.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:56:13 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 06:47:13 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 01:37:04 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tran", "Dustin", ""], ["Kucukelbir", "Alp", ""], ["Dieng", "Adji B.", ""], ["Rudolph", "Maja", ""], ["Liang", "Dawen", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09878", "submitter": "Tobias M\\\"utze", "authors": "Tobias M\\\"utze and Tim Friede", "title": "Blinded sample size re-estimation in three-arm trials with 'gold\n  standard' design", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7356", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample size of a clinical trial relies on information about nuisance\nparameters such as the outcome variance. When no or only limited information is\navailable, it has been proposed to include an internal pilot study in the\ndesign of the trial. Based on the results of the internal pilot study, the\ninitially planned sample size can be adjusted. In this paper, we study blinded\nsample size re-estimation in the 'gold standard' design for normally\ndistributed outcomes. The 'gold standard' design is a three-arm clinical trial\ndesign which includes an active and a placebo control in addition to an\nexperimental treatment. We compare several sample size re-estimation procedures\nin a simulation study assessing operating characteristics including power and\ntype I error. We find that sample size re-estimation based on the popular\none-sample variance estimator results in overpowered trials. Moreover, sample\nsize re-estimation based on unbiased variance estimators such as the Xing-Ganju\nvariance estimator results in underpowered trials, as it is expected since an\noverestimation of the variance and thus the sample size is in general required\nfor the re-estimation procedure to eventually meet the target power. Moreover,\nwe propose an inflation factor for the sample size re-estimation with the\nXing-Ganju variance estimator and show that this approach results in adequately\npowered trials. Due to favorable features of Xing-Ganju variance estimator such\nas unbiasedness and a distribution independent of the group means, the\ninflation factor does not depend on the nuisance parameter and, therefore, can\nbe calculated prior to a trial.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 11:47:29 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 18:52:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["M\u00fctze", "Tobias", ""], ["Friede", "Tim", ""]]}, {"id": "1610.09926", "submitter": "Guy Harling", "authors": "Guy Harling, Rui Wang, Jukka-Pekka Onnela, Victor De Gruttola", "title": "Leveraging contact network structure in the design of cluster randomized\n  trials", "comments": "33 pages, original text submitted for review, Clinical Trials, first\n  published online October 24, 2016", "journal-ref": "Clinical Trials, 2017, 14 (1), 37-47", "doi": "10.1177/1740774516673355", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In settings where proof-of-principle trials have succeeded but\nthe effectiveness of different forms of implementation remains uncertain,\ntrials that not only generate information about intervention effects but also\nprovide public health benefit would be useful. Cluster randomized trials (CRT)\ncapture both direct and indirect intervention effects; the latter depends\nheavily on contact networks within and across clusters. We propose a novel\nclass of connectivity-informed trial designs that leverages information about\nsuch networks in order to improve public health impact and preserve ability to\ndetect intervention effects.\n  Methods: We consider CRTs in which the order of enrollment is based on the\ntotal number of ties between individuals across clusters (based either on the\ntotal number of inter-cluster connections or on connections only to untreated\nclusters). We include options analogous both to traditional Parallel and\nStepped Wedge designs. We also allow for control clusters to be \"held-back\"\nfrom re-randomization for some period. We investigate the performance epidemic\ncontrol and power to detect vaccine effect performance of these designs by\nsimulating vaccination trials during an SEIR-type epidemic using a\nnetwork-structured agent-based model.\n  Results: In our simulations, connectivity-informed designs have lower peak\ninfectiousness than comparable traditional designs and reduce cumulative\nincidence by 20%, but with little impact on time to end of epidemic and reduced\npower to detect differences in incidence across clusters. However even a brief\n\"holdback\" period restores most of the power lost compared to traditional\napproaches.\n  Conclusion: Incorporating information about cluster connectivity in design of\nCRTs can increase their public health impact, especially in acute outbreak\nsettings, with modest cost in power to detect an effective intervention.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 08:34:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Harling", "Guy", ""], ["Wang", "Rui", ""], ["Onnela", "Jukka-Pekka", ""], ["De Gruttola", "Victor", ""]]}, {"id": "1610.09929", "submitter": "Rakshith Jagannath", "authors": "Rakshith Jagannath, Radha Krishna Ganti and Neelesh S Upadhye", "title": "Maximal Packing with Interference Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of scheduling a maximal set of\ntransmitters subjected to an interference constraint across all the nodes.\nGiven a set of nodes, the problem reduces to finding the maximum cardinality of\na subset of nodes that can concurrently transmit without violating interference\nconstraints. The resulting packing problem is a binary optimization problem and\nis NP hard. We propose a semi-definite relaxation (SDR) for this problem and\nprovide bounds on the relaxation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 14:04:00 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Jagannath", "Rakshith", ""], ["Ganti", "Radha Krishna", ""], ["Upadhye", "Neelesh S", ""]]}, {"id": "1610.10040", "submitter": "Shahin Tavakoli", "authors": "Shahin Tavakoli and Davide Pigoli and John A. D. Aston and John S.\n  Coleman", "title": "A Spatial Modeling Approach for Linguistic Object Data: Analysing\n  dialect sound variations across Great Britain", "comments": "18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialect variation is of considerable interest in linguistics and other social\nsciences. However, traditionally it has been studied using proxies\n(transcriptions) rather than acoustic recordings directly. We introduce novel\nstatistical techniques to analyse geolocalised speech recordings and to explore\nthe spatial variation of pronunciations continuously over the region of\ninterest, as opposed to traditional isoglosses, which provide a discrete\npartition of the region. Data of this type require an explicit modeling of the\nvariation in the mean and the covariance. Usual Euclidean metrics are not\nappropriate, and we therefore introduce the concept of $d$-covariance, which\nallows consistent estimation both in space and at individual locations. We then\npropose spatial smoothing for these objects which accounts for the possibly non\nconvex geometry of the domain of interest. We apply the proposed method to data\nfrom the spoken part of the British National Corpus, deposited at the British\nLibrary, London, and we produce maps of the dialect variation over Great\nBritain. In addition, the methods allow for acoustic reconstruction across the\ndomain of interest, allowing researchers to listen to the statistical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:05:47 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 16:11:07 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 11:34:28 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 20:29:12 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tavakoli", "Shahin", ""], ["Pigoli", "Davide", ""], ["Aston", "John A. D.", ""], ["Coleman", "John S.", ""]]}]