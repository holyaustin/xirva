[{"id": "1012.0073", "submitter": "Richard Barker", "authors": "Richard J. Barker and William A. Link", "title": "Posterior model probabilities computed from model-specific Gibbs output", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible jump Markov chain Monte Carlo (RJMCMC) extends ordinary MCMC\nmethods for use in Bayesian multimodel inference. We show that RJMCMC can be\nimplemented as Gibbs sampling with alternating updates of a model indicator and\na vector-valued \"palette\" of parameters denoted $\\bm \\psi$. Like an artist uses\nthe palette to mix dabs of color for specific needs, we create model-specific\nparameters from the set available in $\\bm \\psi$. This description not only\nremoves some of the mystery of RJMCMC, but also provides a basis for fitting\nmodels one at a time using ordinary MCMC and computing model weights or Bayes\nfactors by post-processing the Monte Carlo output. We illustrate our procedure\nusing several examples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 00:44:09 GMT"}, {"version": "v2", "created": "Thu, 26 May 2011 01:50:19 GMT"}], "update_date": "2011-05-27", "authors_parsed": [["Barker", "Richard J.", ""], ["Link", "William A.", ""]]}, {"id": "1012.0269", "submitter": "Pierre Lafaye de Micheaux", "authors": "C\\'ecile Bordier and Michel Dojat and Pierre Lafaye de Micheaux", "title": "Temporal and Spatial Independent Component Analysis for fMRI data sets\n  embedded in a R package", "comments": "Submitted to JSS", "journal-ref": "Journal of Statistical Software, Vol. 44, Issue 9, Oct 2011", "doi": null, "report-no": null, "categories": "stat.CO physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For statistical analysis of functional Magnetic Resonance Imaging (fMRI) data\nsets, we propose a data-driven approach based on Independent Component Analysis\n(ICA) implemented in a new version of the AnalyzeFMRI R package. For fMRI data\nsets, spatial dimension being much greater than temporal dimension, spatial ICA\nis the tractable approach generally proposed. However, for some neuroscientific\napplications, temporal independence of source signals can be assumed and\ntemporal ICA becomes then an attracting exploratory technique. In this work, we\nuse a classical linear algebra result ensuring the tractability of temporal\nICA. We report several experiments on synthetic data and real MRI data sets\nthat demonstrate the potential interest of our R package.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 18:32:31 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Bordier", "C\u00e9cile", ""], ["Dojat", "Michel", ""], ["de Micheaux", "Pierre Lafaye", ""]]}, {"id": "1012.0497", "submitter": "Nicolas Barbey", "authors": "Nicolas Barbey, Marc Sauvage, Jean-Luc Starck, Roland Ottensamer,\n  Pierre Chanial", "title": "Feasibility and performances of compressed-sensing and sparse map-making\n  with Herschel/PACS data", "comments": "11 pages, 6 figures, 5 tables, peer-reviewed article", "journal-ref": null, "doi": "10.1051/0004-6361/201015779", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Herschel Space Observatory of ESA was launched in May 2009 and is in\noperation since. From its distant orbit around L2 it needs to transmit a huge\nquantity of information through a very limited bandwidth. This is especially\ntrue for the PACS imaging camera which needs to compress its data far more than\nwhat can be achieved with lossless compression. This is currently solved by\nincluding lossy averaging and rounding steps on board. Recently, a new theory\ncalled compressed-sensing emerged from the statistics community. This theory\nmakes use of the sparsity of natural (or astrophysical) images to optimize the\nacquisition scheme of the data needed to estimate those images. Thus, it can\nlead to high compression factors.\n  A previous article by Bobin et al. (2008) showed how the new theory could be\napplied to simulated Herschel/PACS data to solve the compression requirement of\nthe instrument. In this article, we show that compressed-sensing theory can\nindeed be successfully applied to actual Herschel/PACS data and give\nsignificant improvements over the standard pipeline. In order to fully use the\nredundancy present in the data, we perform full sky map estimation and\ndecompression at the same time, which cannot be done in most other compression\nmethods. We also demonstrate that the various artifacts affecting the data\n(pink noise, glitches, whose behavior is a priori not well compatible with\ncompressed-sensing) can be handled as well in this new framework. Finally, we\nmake a comparison between the methods from the compressed-sensing scheme and\ndata acquired with the standard compression scheme. We discuss improvements\nthat can be made on ground for the creation of sky maps from the data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 17:00:29 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Barbey", "Nicolas", ""], ["Sauvage", "Marc", ""], ["Starck", "Jean-Luc", ""], ["Ottensamer", "Roland", ""], ["Chanial", "Pierre", ""]]}, {"id": "1012.1047", "submitter": "Luis Carvalho", "authors": "Luis Carvalho", "title": "A Bayesian Statistical Approach for Inference on Static\n  Origin-Destination Matrices", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of static OD matrix estimation from a formal\nstatistical viewpoint. We adopt a novel Bayesian framework to develop a class\nof models that explicitly cast trip configurations in the study region as\nrandom variables. As a consequence, classical solutions from growth factor,\ngravity, and maximum entropy models are identified to specific estimators under\nthe proposed models. We show that each of these solutions usually account for\nonly a small fraction of the posterior probability mass in the ensemble and we\nthen contend that the uncertainty in the inference should be propagated to\nlater analyses or next-stage models. We also propose alternative, more robust\nestimators and devise Markov chain Monte Carlo sampling schemes to obtain them\nand perform other types of inference. We present several examples showcasing\nthe proposed models and approach, and highlight how other sources of data can\nbe incorporated in the model and inference in a principled, non-heuristic way.\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 23:00:55 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 00:28:23 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Carvalho", "Luis", ""]]}, {"id": "1012.1297", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "LASSO Methods for Gaussian Instrumental Variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,\nsqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate\noptimal instruments in linear instrumental variables (IV) models with many\ninstruments in the canonical Gaussian case. The methods apply even when the\nnumber of instruments is much larger than the sample size. We derive asymptotic\ndistributions for the resulting IV estimators and provide conditions under\nwhich these sparsity-based IV estimators are asymptotically oracle-efficient.\nIn simulation experiments, a sparsity-based IV estimator with a data-driven\npenalty performs well compared to recently advocated many-instrument-robust\nprocedures. We illustrate the procedure in an empirical example using the\nAngrist and Krueger (1991) schooling data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 20:04:51 GMT"}, {"version": "v2", "created": "Wed, 23 Feb 2011 23:39:28 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1012.1833", "submitter": "Ethan Anderes", "authors": "Ethan Anderes, Lloyd Knox, Alexander van Engelen", "title": "Mapping gravitational lensing of the CMB using local likelihoods", "comments": null, "journal-ref": "Phys.Rev.D83:043523,2011; Phys.Rev.D83:069905,2011", "doi": "10.1103/PhysRevD.83.043523 10.1103/PhysRevD.83.069905", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new estimation method for mapping the gravitational lensing\npotential from observed CMB intensity and polarization fields. Our method uses\nBayesian techniques to estimate the average curvature of the potential over\nsmall local regions. These local curvatures are then used to construct an\nestimate of a low pass filter of the gravitational potential. By utilizing\nBayesian/likelihood methods one can easily overcome problems with missing\nand/or non-uniform pixels and problems with partial sky observations (E and B\nmode mixing, for example). Moreover, our methods are local in nature which\nallow us to easily model spatially varying beams and are highly parallelizable.\nWe note that our estimates do not rely on the typical Taylor approximation\nwhich is used to construct estimates of the gravitational potential by Fourier\ncoupling. We present our methodology with a flat sky simulation under nearly\nideal experimental conditions with a noise level of 1 $\\mu K$-arcmin for the\ntemperature field, $\\sqrt{2}$ $\\mu K$-arcmin for the polarization fields, with\nan instrumental beam full width at half maximum (FWHM) of 0.25 arcmin.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 19:29:52 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Anderes", "Ethan", ""], ["Knox", "Lloyd", ""], ["van Engelen", "Alexander", ""]]}, {"id": "1012.1879", "submitter": "Janos Gyarmati-Szabo", "authors": "Janos Gyarmati-Szabo, Leonid V. Bogachev, Haibo Chen", "title": "Multiple change-point Poisson model for threshold exceedances of air\n  pollution concentrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian multiple change-point model is proposed to analyse violations of\nair quality standards by pollutants such as nitrogen oxides (NO2 and NO) and\ncarbon monoxide (CO). The model is built on the assumption that the occurrence\nof threshold exceedances may be described by a non-homogeneous Poisson process\nwith a step rate function. Unlike earlier approaches, our model is not\nrestricted by a predetermined number of change-points, nor does it involve any\ncovariates. Possible short-range correlations in the exceedance data (e.g., due\nto chemical and meteorological factors) are removed via declusterisation. The\nunknown rate function is estimated using a reversible jump MCMC sampling\nalgorithm adapted from Green (1995), which allows for transitions between\nparameter subspaces of varying dimension. This technique is applied to the\n17-year (1993-2009) daily NO2, NO and CO concentration data in the City of\nLeeds, UK. The results are validated by running the MCMC estimator on simulated\ndata replicated via a posterior estimate of the rate function. The findings are\ninterpreted and discussed in relation to some known traffic control actions.\nThe proposed methodology may be useful in the air quality management context by\nproviding quantitative objective means to measure the efficacy of pollution\ncontrol programmes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 21:41:02 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Gyarmati-Szabo", "Janos", ""], ["Bogachev", "Leonid V.", ""], ["Chen", "Haibo", ""]]}, {"id": "1012.2155", "submitter": "Hugo Gabriel Eyherabide", "authors": "Hugo Gabriel Eyherabide and In\\'es Samengo", "title": "Time and category information in pattern-based codes", "comments": "Free access at\n  http://www.frontiersin.org/computational_neuroscience/10.3389/fncom.2010.00145/abstract", "journal-ref": "Frontiers in Computational Neuroscience 4:145 (2010)", "doi": "10.3389/fncom.2010.00145", "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory stimuli are usually composed of different features (the what)\nappearing at irregular times (the when). Neural responses often use spike\npatterns to represent sensory information. The what is hypothesized to be\nencoded in the identity of the elicited patterns (the pattern categories), and\nthe when, in the time positions of patterns (the pattern timing). However, this\nstandard view is oversimplified. In the real world, the what and the when might\nnot be separable concepts, for instance, if they are correlated in the\nstimulus. In addition, neuronal dynamics can condition the pattern timing to be\ncorrelated with the pattern categories. Hence, timing and categories of\npatterns may not constitute independent channels of information. In this paper,\nwe assess the role of spike patterns in the neural code, irrespective of the\nnature of the patterns. We first define information-theoretical quantities that\nallow us to quantify the information encoded by different aspects of the neural\nresponse. We also introduce the notion of synergy/redundancy between time\npositions and categories of patterns. We subsequently establish the relation\nbetween the what and the when in the stimulus with the timing and the\ncategories of patterns. To that aim, we quantify the mutual information between\ndifferent aspects of the stimulus and different aspects of the response. This\nformal framework allows us to determine the precise conditions under which the\nstandard view holds, as well as the departures from this simple case. Finally,\nwe study the capability of different response aspects to represent the what and\nthe when in the neural response.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 01:29:10 GMT"}], "update_date": "2010-12-13", "authors_parsed": [["Eyherabide", "Hugo Gabriel", ""], ["Samengo", "In\u00e9s", ""]]}, {"id": "1012.4122", "submitter": "Amber Tomas", "authors": "Amber Tomas and Krista J. Gile", "title": "The Effect of Differential Recruitment, Non-response and Non-recruitment\n  on Estimators for Respondent-Driven Sampling", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling is a widely-used network sampling technique,\ndesigned to sample from hard-to-reach populations. Estimation from the\nresulting samples is an area of active research, with software available to\ncompute at least four estimators of a population proportion. Each estimator is\nclaimed to address deficiencies in previous estimators, however those claims\nare often unsubstantiated. In this study we provide a simulation-based\ncomparison of five existing estimators, focussing on sampling conditions which\na recent estimator is designed to address. We find no estimator consistently\nout-performs all others, and highlight sampling conditions in which each is to\nbe preferred.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 21:51:26 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Tomas", "Amber", ""], ["Gile", "Krista J.", ""]]}, {"id": "1012.4185", "submitter": "Andrew C. Thomas", "authors": "Andrew C. Thomas, Stephen E. Fienberg", "title": "Exploring the Consequences of IED Deployment with a Generalized Linear\n  Model Implementation of the Canadian Traveller Problem", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of improvised explosive devices (IEDs) along major roadways\nhas been a favoured strategy of insurgents in recent war zones, both for the\nability to cause damage to targets along roadways at minimal cost, but also as\na means of controlling the flow of traffic and causing additional expense to\nopposing forces. Among other related approaches (which we discuss), the\nadversarial problem has an analogue in the Canadian Traveller Problem, wherein\na stretch of road is blocked with some independent probability, and the state\nof the road is only discovered once the traveller reaches one of the\nintersections that bound this stretch of road. We discuss the implementation of\nideas from social network analysis, namely the notion of \"betweenness\ncentrality\", and how this can be adapted to the notion of deployment of IEDs\nwith the aid of Generalized Linear Models (GLMs): namely, how we can model the\nprobability of an IED deployment in terms of the increased effort due to\nCanadian betweenness, how we can include expert judgement on the probability of\na deployment, and how we can extend the approach to estimation and updating\nover several time steps.\n", "versions": [{"version": "v1", "created": "Sun, 19 Dec 2010 16:53:07 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Thomas", "Andrew C.", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1012.4676", "submitter": "Garfield Brown", "authors": "Garfield Brown and Winston Buckley", "title": "Discrimination for Two Way Models with Insurance Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review and apply several approaches to model selection for\nanalysis of variance models which are used in a credibility and insurance\ncontext. The reversible jump algorithm is employed for model selection, where\nposterior model probabilities are computed. We then apply this method to\ninsurance data from workers' compensation insurance schemes. The reversible\njump results are compared with the Deviance Information Criterion, and are\nshown to be consistent.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 14:40:55 GMT"}], "update_date": "2010-12-22", "authors_parsed": [["Brown", "Garfield", ""], ["Buckley", "Winston", ""]]}, {"id": "1012.4702", "submitter": "Garfield Brown", "authors": "Garfield Brown and Steve Brooks and Winston Buckley", "title": "Experience Rating with Poisson Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixture Poisson model for claims counts in which the number of\ncomponents in the mixture are estimated by reversible jump MCMC methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 15:48:01 GMT"}], "update_date": "2010-12-22", "authors_parsed": [["Brown", "Garfield", ""], ["Brooks", "Steve", ""], ["Buckley", "Winston", ""]]}, {"id": "1012.4726", "submitter": "Rosemary Braun", "authors": "Rosemary Braun and Kenneth Buetow", "title": "Pathways of Distinction Analysis: a new technique for multi-SNP analysis\n  of GWAS data", "comments": "Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Genome-wide association studies have become increasingly common due to\nadvances in technology and have permitted the identification of differences in\nsingle nucleotide polymorphism (SNP) alleles that are associated with diseases.\nHowever, while typical GWAS analysis techniques treat markers individually,\ncomplex diseases are unlikely to have a single causative gene. There is thus a\npressing need for multi-SNP analysis methods that can reveal system-level\ndifferences in cases and controls. Here, we present a novel multi-SNP GWAS\nanalysis method called Pathways of Distinction Analysis (PoDA). The method uses\nGWAS data and known pathway-gene and gene-SNP associations to identify pathways\nthat permit, ideally, the distinction of cases from controls. The technique is\nbased upon the hypothesis that if a pathway is related to disease risk, cases\nwill appear more similar to other cases than to controls for the SNPs\nassociated with that pathway. By systematically applying the method to all\npathways of potential interest, we can identify those for which the hypothesis\nholds true, i.e., pathways containing SNPs for which the samples exhibit\ngreater within-class similarity than across classes. Importantly, PoDA improves\non existing single-SNP and SNP-set enrichment analyses in that it does not\nrequire the SNPs in a pathway to exhibit independent main effects. This permits\nPoDA to reveal pathways in which epistatic interactions drives risk. In this\npaper, we detail the PoDA method and apply it to two GWA studies: one of breast\ncancer, and the other of liver cancer. The results obtained strongly suggest\nthat there exist pathway-wide genomic differences that contribute to disease\nsusceptibility. PoDA thus provides an analytical tool that is complementary to\nexisting techniques and has the power to enrich our understanding of disease\ngenomics at the systems-level.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 16:50:59 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2011 22:09:54 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Braun", "Rosemary", ""], ["Buetow", "Kenneth", ""]]}, {"id": "1012.4769", "submitter": "Michael Braun", "authors": "Michael Braun and Andr\\'e Bonfrer", "title": "Scalable Inference of Customer Similarities from Interactions Data using\n  Dirichlet Processes", "comments": null, "journal-ref": "Marketing Science 30:3 513-531 2011", "doi": "10.1287/mksc.1110.0640", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the sociological theory of homophily, people who are similar to one\nanother are more likely to interact with one another. Marketers often have\naccess to data on interactions among customers from which, with homophily as a\nguiding principle, inferences could be made about the underlying similarities.\nHowever, larger networks face a quadratic explosion in the number of potential\ninteractions that need to be modeled. This scalability problem renders\nprobability models of social interactions computationally infeasible for all\nbut the smallest networks. In this paper we develop a probabilistic framework\nfor modeling customer interactions that is both grounded in the theory of\nhomophily, and is flexible enough to account for random variation in who\ninteracts with whom. In particular, we present a novel Bayesian nonparametric\napproach, using Dirichlet processes, to moderate the scalability problems that\nmarketing researchers encounter when working with networked data. We find that\nthis framework is a powerful way to draw insights into latent similarities of\ncustomers, and we discuss how marketers can apply these insights to\nsegmentation and targeting activities.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 19:18:49 GMT"}], "update_date": "2011-07-06", "authors_parsed": [["Braun", "Michael", ""], ["Bonfrer", "Andr\u00e9", ""]]}, {"id": "1012.5095", "submitter": "Max Little", "authors": "Max A. Little, Nick S. Jones", "title": "Generalized Methods and Solvers for Noise Removal from Piecewise\n  Constant Signals", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": "10.1098/rspa.2010.0674", "report-no": null, "categories": "physics.data-an math.NA q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing noise from piecewise constant (PWC) signals, is a challenging signal\nprocessing problem arising in many practical contexts. For example, in\nexploration geosciences, noisy drill hole records need separating into\nstratigraphic zones, and in biophysics, jumps between molecular dwell states\nneed extracting from noisy fluorescence microscopy signals. Many PWC denoising\nmethods exist, including total variation regularization, mean shift clustering,\nstepwise jump placement, running medians, convex clustering shrinkage and\nbilateral filtering; conventional linear signal processing methods are\nfundamentally unsuited however. This paper shows that most of these methods are\nassociated with a special case of a generalized functional, minimized to\nachieve PWC denoising. The minimizer can be obtained by diverse solver\nalgorithms, including stepwise jump placement, convex programming, finite\ndifferences, iterated running medians, least angle regression, regularization\npath following, and coordinate descent. We introduce novel PWC denoising\nmethods, which, for example, combine global mean shift clustering with local\ntotal variation smoothing. Head-to-head comparisons between these methods are\nperformed on synthetic data, revealing that our new methods have a useful role\nto play. Finally, overlaps between the methods of this paper and others such as\nwavelet shrinkage, hidden Markov models, and piecewise smooth filtering are\ntouched on.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 20:55:09 GMT"}, {"version": "v2", "created": "Tue, 4 Jan 2011 09:19:13 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Little", "Max A.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1012.6033", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Large-scale interval and point estimates from an empirical Bayes\n  extension of confidence posteriors", "comments": null, "journal-ref": "Bickel, D. R. (2012). Empirical Bayes Interval Estimates that are\n  Conditionally Equal to Unadjusted Confidence Intervals or to Default Prior\n  Credibility Intervals. Statistical Applications in Genetics and Molecular\n  Biology, 11 (article 3)", "doi": "10.1515/1544-6115.1765", "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed approach extends the confidence posterior distribution to the\nsemi-parametric empirical Bayes setting. Whereas the Bayesian posterior is\ndefined in terms of a prior distribution conditional on the observed data, the\nconfidence posterior is defined such that the probability that the parameter\nvalue lies in any fixed subset of parameter space, given the observed data, is\nequal to the coverage rate of the corresponding confidence interval. A\nconfidence posterior that has correct frequentist coverage at each fixed\nparameter value is combined with the estimated local false discovery rate to\nyield a parameter distribution from which interval and point estimates are\nderived within the framework of minimizing expected loss. The point estimates\nexhibit suitable shrinkage toward the null hypothesis value, making them\npractical for automatically ranking features in order of priority. The\ncorresponding confidence intervals are also shrunken and tend to be much\nshorter than their fixed-parameter counterparts, as illustrated with gene\nexpression data. Further, simulations confirm a theoretical argument that the\nshrunken confidence intervals cover the parameter at a higher-than-nominal\nfrequency.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 20:47:55 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Bickel", "David R.", ""]]}]