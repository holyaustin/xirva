[{"id": "1710.00001", "submitter": "Gavin Whitaker", "authors": "Gavin A. Whitaker, Ricardo Silva, Daniel Edwards, Ioannis Kosmidis", "title": "A Bayesian inference approach for determining player abilities in\n  football", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of determining a football player's ability for a given\nevent type, for example, scoring a goal. We propose an interpretable Bayesian\nmodel which is fit using variational inference methods. We implement a Poisson\nmodel to capture occurrences of event types, from which we infer player\nabilities. Our approach also allows the visualisation of differences between\nplayers, for a specific ability, through the marginal posterior variational\ndensities. We then use these inferred player abilities to extend the Bayesian\nhierarchical model of Baio and Blangiardo (2010) which captures a team's\nscoring rate (the rate at which they score goals). We apply the resulting\nscheme to the English Premier League, capturing player abilities over the\n2013/2014 season, before using output from the hierarchical model to predict\nwhether over or under 2.5 goals will be scored in a given game in the 2014/2015\nseason. This validates our model as a way of providing insights into team\nformation and the individual success of sports teams.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 13:47:58 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 11:29:26 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Silva", "Ricardo", ""], ["Edwards", "Daniel", ""], ["Kosmidis", "Ioannis", ""]]}, {"id": "1710.00153", "submitter": "Minjie Fan", "authors": "Minjie Fan, Debashis Paul, Thomas C. M. Lee, Tomoko Matsuo", "title": "A Multi-Resolution Model for Non-Gaussian Random Fields on a Sphere with\n  Application to Ionospheric Electrostatic Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields have been one of the most popular tools for analyzing\nspatial data. However, many geophysical and environmental processes often\ndisplay non-Gaussian characteristics. In this paper, we propose a new class of\nspatial models for non-Gaussian random fields on a sphere based on a\nmulti-resolution analysis. Using a special wavelet frame, named spherical\nneedlets, as building blocks, the proposed model is constructed in the form of\na sparse random effects model. The spatial localization of needlets, together\nwith carefully chosen random coefficients, ensure the model to be non-Gaussian\nand isotropic. The model can also be expanded to include a spatially varying\nvariance profile. The special formulation of the model enables us to develop\nefficient estimation and prediction procedures, in which an adaptive MCMC\nalgorithm is used. We investigate the accuracy of parameter estimation of the\nproposed model, and compare its predictive performance with that of two\nGaussian models by extensive numerical experiments. Practical utility of the\nproposed model is demonstrated through an application of the methodology to a\ndata set of high-latitude ionospheric electrostatic potentials, generated from\nthe LFM-MIX model of the magnetosphere-ionosphere system.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 06:02:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Fan", "Minjie", ""], ["Paul", "Debashis", ""], ["Lee", "Thomas C. M.", ""], ["Matsuo", "Tomoko", ""]]}, {"id": "1710.00173", "submitter": "Sadegh Movahed", "authors": "A. Vafaei Sadr, S. M. S. Movahed, M. Farhang, C. Ringeval, F. R.\n  Bouchet", "title": "Multi-Scale Pipeline for the Search of String-Induced CMB Anisotropies", "comments": "13 pages, 5 figures, 1 table, Comments are welcome", "journal-ref": "Monthly Notices of the Royal Astronomical Society 475.1 (2017):\n  1010-1022", "doi": "10.1093/mnras/stx3126", "report-no": null, "categories": "astro-ph.CO astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-scale edge-detection algorithm to search for the\nGott-Kaiser-Stebbins imprints of a cosmic string (CS) network on the Cosmic\nMicrowave Background (CMB) anisotropies. Curvelet decomposition and extended\nCanny algorithm are used to enhance the string detectability. Various\nstatistical tools are then applied to quantify the deviation of CMB maps having\na cosmic string contribution with respect to pure Gaussian anisotropies of\ninflationary origin. These statistical measures include the one-point\nprobability density function, the weighted two-point correlation function\n(TPCF) of the anisotropies, the unweighted TPCF of the peaks and of the\nup-crossing map, as well as their cross-correlation. We use this algorithm on a\nhundred of simulated Nambu-Goto CMB flat sky maps, covering approximately\n$10\\%$ of the sky, and for different string tensions $G\\mu$. On noiseless sky\nmaps with an angular resolution of $0.9'$, we show that our pipeline detects\nCSs with $G\\mu$ as low as $G\\mu\\gtrsim 4.3\\times 10^{-10}$. At the same\nresolution, but with a noise level typical to a CMB-S4 phase II experiment, the\ndetection threshold would be to $G\\mu\\gtrsim 1.2 \\times 10^{-7}$.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 09:43:56 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Sadr", "A. Vafaei", ""], ["Movahed", "S. M. S.", ""], ["Farhang", "M.", ""], ["Ringeval", "C.", ""], ["Bouchet", "F. R.", ""]]}, {"id": "1710.00324", "submitter": "Swasti Khuntia", "authors": "Swasti R. Khuntia, Jose L. Rueda, and Mart A. M. M. van der Meijden", "title": "Mutual Information based Bayesian Analysis of Power System Reliability", "comments": "6 pages, 9 figures, 2 tables, PowerTech, 2015 IEEE Eindhoven", "journal-ref": null, "doi": "10.1109/PTC.2015.7232592", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at assessing the power system reliability by estimating loss\nof load (LOL) index using mutual information based Bayesian approach.\nReliability analysis is a key component in the design, analysis and tuning of\ncomplex structure like electrical power system. Consideration is given to rare\nevents while constructing the Bayesian network, which provides reliable\nestimates of probability distribution function of LOL with lesser computing\neffort. Also, the ranking of load components due to loss of load is evaluated.\nThe RBTS and IEEE RTS-24 systems are used as test cases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 09:40:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Khuntia", "Swasti R.", ""], ["Rueda", "Jose L.", ""], ["van der Meijden", "Mart A. M. M.", ""]]}, {"id": "1710.00559", "submitter": "Christoph Bandt", "authors": "Christoph Bandt", "title": "Crude EEG parameter provides sleep medicine with well-defined continuous\n  hypnograms", "comments": "23 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate EEG data, one can count local maxima and minima on a fine scale,\nin a sliding window analysis. This straightforward calculation, which\nsimplifies and improves previous work on permutation entropy, directly defines\na good proxy for brain activity in an EEG channel during an epoch of 30\nseconds. Different channels and persons can be compared when they are measured\nwith the same device and prefiltering options. This could lead to a rigorously\ndefined and suitably standardized biomarker of cortex activity, like blood\npressure or laboratory values. Applied to sleep EEG, the algorithm yields\nhypnograms with continuous scale which show amazing coincidence with sleep\nstage annotation by trained experts. Although produced by a crude method,\ncontinuous hypnograms provide a lot of details. For example, sleep depth\nusually decreases from evening to morning even within the same annotated sleep\nstage, except for REM phases where mean sleep depth is rather constant, but\ndifferent in frontal and parietal channels. The diagnostic potential of the\nmethod is demonstrated with two hypnograms of narcoleptic patients. In all 10\nsubjects, infra-slow oscillations of activity with a wavelength between 30s and\ntwo minutes were clearly seen, particularly strong at the onset of sleep and in\nS2 phases. The suggested method needs to be checked and improved. In its\npresent form it seems already an appropriate tool for screening long-term EEG\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 09:55:45 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Bandt", "Christoph", ""]]}, {"id": "1710.00575", "submitter": "Pablo Morales-\\'Alvarez", "authors": "Pablo Morales-Alvarez and Adrian Perez-Suay and Rafael Molina and\n  Gustau Camps-Valls", "title": "Remote Sensing Image Classification with Large Scale Gaussian Processes", "comments": "11 pages, 6 figures, Accepted for publication in IEEE Transactions on\n  Geoscience and Remote Sensing; added the IEEE copyright statement", "journal-ref": null, "doi": "10.1109/TGRS.2017.2758922", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current remote sensing image classification problems have to deal with an\nunprecedented amount of heterogeneous and complex data sources. Upcoming\nmissions will soon provide large data streams that will make land cover/use\nclassification difficult. Machine learning classifiers can help at this, and\nmany methods are currently available. A popular kernel classifier is the\nGaussian process classifier (GPC), since it approaches the classification\nproblem with a solid probabilistic treatment, thus yielding confidence\nintervals for the predictions as well as very competitive results to\nstate-of-the-art neural networks and support vector machines. However, its\ncomputational cost is prohibitive for large scale applications, and constitutes\nthe main obstacle precluding wide adoption. This paper tackles this problem by\nintroducing two novel efficient methodologies for Gaussian Process (GP)\nclassification. We first include the standard random Fourier features\napproximation into GPC, which largely decreases its computational cost and\npermits large scale remote sensing image classification. In addition, we\npropose a model which avoids randomly sampling a number of Fourier frequencies,\nand alternatively learns the optimal ones within a variational Bayes approach.\nThe performance of the proposed methods is illustrated in complex problems of\ncloud detection from multispectral imagery and infrared sounding data.\nExcellent empirical results support the proposal in both computational cost and\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 10:51:47 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 10:40:11 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Morales-Alvarez", "Pablo", ""], ["Perez-Suay", "Adrian", ""], ["Molina", "Rafael", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1710.00578", "submitter": "Jack Baker", "authors": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package sgmcmc; which can be used for Bayesian\ninference on problems with large datasets using stochastic gradient Markov\nchain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC)\nmethods, such as Metropolis-Hastings, are known to run prohibitively slowly as\nthe dataset size increases. SGMCMC solves this issue by only using a subset of\ndata at each iteration. SGMCMC requires calculating gradients of the log\nlikelihood and log priors, which can be time consuming and error prone to\nperform by hand. The sgmcmc package calculates these gradients itself using\nautomatic differentiation, making the implementation of these methods much\neasier. To do this, the package uses the software library TensorFlow, which has\na variety of statistical distributions and mathematical operations as standard,\nmeaning a wide class of models can be built using this framework. SGMCMC has\nbecome widely adopted in the machine learning literature, but less so in the\nstatistics community. We believe this may be partly due to lack of software;\nthis package aims to bridge this gap.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 11:01:53 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 10:13:42 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 12:01:23 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Baker", "Jack", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B.", ""], ["Nemeth", "Christopher", ""]]}, {"id": "1710.00862", "submitter": "Chao Gao", "authors": "Chao Gao and John Lafferty", "title": "Testing for Global Network Structure Using Small Subgraph Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing for community structure in networks using\nrelations between the observed frequencies of small subgraphs. We propose a\nsimple test for the existence of communities based only on the frequencies of\nthree-node subgraphs. The test statistic is shown to be asymptotically normal\nunder a null assumption of no community structure, and to have power\napproaching one under a composite alternative hypothesis of a degree-corrected\nstochastic block model. We also derive a version of the test that applies to\nmultivariate Gaussian data. Our approach achieves near-optimal detection rates\nfor the presence of community structure, in regimes where the signal-to-noise\nis too weak to explicitly estimate the communities themselves, using existing\ncomputationally efficient algorithms. We demonstrate how the method can be\neffective for detecting structure in social networks, citation networks for\nscientific articles, and correlations of stock returns between companies on the\nS\\&P 500.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:39:20 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 04:57:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gao", "Chao", ""], ["Lafferty", "John", ""]]}, {"id": "1710.00875", "submitter": "Daniela Castro Camilo", "authors": "Daniela Castro-Camilo and Rapha\\\"el Huser", "title": "Local likelihood estimation of complex tail dependence structures,\n  applied to U.S. precipitation extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To disentangle the complex non-stationary dependence structure of\nprecipitation extremes over the entire contiguous U.S., we propose a flexible\nlocal approach based on factor copula models. Our sub-asymptotic spatial\nmodeling framework yields non-trivial tail dependence structures, with a\nweakening dependence strength as events become more extreme, a feature commonly\nobserved with precipitation data but not accounted for in classical asymptotic\nextreme-value models. To estimate the local extremal behavior, we fit the\nproposed model in small regional neighborhoods to high threshold exceedances,\nunder the assumption of local stationarity, which allows us to gain in\nflexibility. Adopting a local censored likelihood approach, inference is made\non a fine spatial grid, and local estimation is performed by taking advantage\nof distributed computing resources and the embarrassingly parallel nature of\nthis estimation procedure. The local model is efficiently fitted at all grid\npoints, and uncertainty is measured using a block bootstrap procedure. An\nextensive simulation study shows that our approach can adequately capture\ncomplex, non-stationary dependencies, while our study of U.S. winter\nprecipitation data reveals interesting differences in local tail structures\nover space, which has important implications on regional risk assessment of\nextreme precipitation events.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 19:19:50 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 20:10:19 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 06:35:41 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Castro-Camilo", "Daniela", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1710.00894", "submitter": "Pariya Behrouzi", "authors": "P. Behrouzi and E.C. Wit", "title": "Detecting Epistatic Selection with Partially Observed Genotype Data\n  Using Copula Graphical Models", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.MN q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recombinant Inbred Lines derived from divergent parental lines can display\nextensive segregation distortion and long-range linkage disequilibrium (LD)\nbetween distant loci. These genomic signatures are consistent with epistatic\nselection during inbreeding. Epistatic interactions affect growth and fertility\ntraits or even cause complete lethality. Detecting epistasis is challenging as\nmultiple testing approaches are under-powered and true long-range LD is\ndifficult to distinguish from drift.\n  Here we develop a method for reconstructing an underlying network of genomic\nsignatures of high-dimensional epistatic selection from multi-locus genotype\ndata. The network captures the conditionally dependent short- and long-range LD\nstructure and thus reveals \"aberrant\" marker-marker associations that are due\nto epistatic selection rather than gametic linkage. The network estimation\nrelies on penalized Gaussian copula graphical models, which accounts for a\nlarge number of markers p and a small number of individuals n.\n  A multi-core implementation of our algorithm makes it feasible to estimate\nthe graph in high-dimensions also in the presence of significant portions of\nmissing data. We demonstrate the efficiency of the proposed method on simulated\ndatasets as well as on genotyping data in A.thaliana and maize. In addition, we\nimplemented the method in the R package netgwas which is freely available at\nhttps://CRAN.R-project.org/package=netgwas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:18:51 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 01:21:34 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Behrouzi", "P.", ""], ["Wit", "E. C.", ""]]}, {"id": "1710.01054", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Bastien Chopard, Jonas L\\\"att, Frank Dubois, Karim\n  Zouaoui Boudjeltia and Antonietta Mira", "title": "Parameter estimation of platelets deposition: Approximate Bayesian\n  computation with high performance computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.CB q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show the existing clinical tests to detect\nCardio/cerebrovascular diseases (CVD) are ineffectual as they do not consider\ndifferent stages of platelet activation or the molecular dynamics involved in\nplatelet interactions. Further they are also incapable to consider\ninter-individual variability. A physical description of platelets deposition\nwas introduced recently in Chopard et. al. [2017], by integrating fundamental\nunderstandings of how platelets interact in a numerical model, parameterized by\nfive parameters. These parameters specify the deposition process and are\nrelevant for a biomedical understanding of the phenomena. One of the main\nintuition is that these parameters are precisely the information needed for a\npathological test identifying CVD captured and that they capture the\ninter-individual variability. Following this intuition, here we devise a\nBayesian inferential scheme for estimation of these parameters. As the\nlikelihood function of the numerical model is intractable due to the complex\nstochastic nature of the model, we use a likelihood-free inference scheme\napproximate Bayesian computation (ABC) to calibrate the parameters in a\ndata-driven manner. As ABC requires the generation of many pseudo-data by\nexpensive simulation runs, we use a high performance computing (HPC) framework\nfor ABC to make the inference possible for this model. We illustrate that our\nmean posterior prediction of platelet deposition pattern matches the\nexperimental dataset closely with a tight posterior prediction error margin for\na collective dataset of 7 volunteers. The present approach can be used to build\na new generation of personalized platelet functionality tests for CVD\ndetection, using numerical modeling of platelet deposition, Bayesian\nuncertainty quantification and High performance computing.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 09:52:18 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 12:39:28 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Chopard", "Bastien", ""], ["L\u00e4tt", "Jonas", ""], ["Dubois", "Frank", ""], ["Boudjeltia", "Karim Zouaoui", ""], ["Mira", "Antonietta", ""]]}, {"id": "1710.01063", "submitter": "Pariya Behrouzi", "authors": "Pariya Behrouzi and Ernst C. Wit", "title": "De novo construction of polyploid linkage maps using discrete graphical\n  models", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.MN q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linkage maps are used to identify the location of genes responsible for\ntraits and diseases. New sequencing techniques have created opportunities to\nsubstantially increase the density of genetic markers. Such revolutionary\nadvances in technology have given rise to new challenges, such as creating\nhigh-density linkage maps. Current multiple testing approaches based on\npairwise recombination fractions are underpowered in the high-dimensional\nsetting and do not extend easily to polyploid species. We propose to construct\nlinkage maps using graphical models either via a sparse Gaussian copula or a\nnonparanormal skeptic approach. Linkage groups (LGs), typically chromosomes,\nand the order of markers in each LG are determined by inferring the conditional\nindependence relationships among large numbers of markers in the genome.\nThrough simulations, we illustrate the utility of our map construction method\nand compare its performance with other available methods, both when the data\nare clean and contain no missing observations and when data contain genotyping\nerrors and are incomplete. We apply the proposed method to two genotype\ndatasets: barley and potato from diploid and polypoid populations,\nrespectively. Our comprehensive map construction method makes full use of the\ndosage SNP data to reconstruct linkage map for any bi-parental diploid and\npolyploid species. We have implemented the method in the R package netgwas.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 10:30:51 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 10:31:25 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 00:52:29 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 14:32:47 GMT"}, {"version": "v5", "created": "Mon, 2 Apr 2018 22:36:52 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Behrouzi", "Pariya", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1710.01236", "submitter": "Pariya Behrouzi", "authors": "Pariya Behrouzi, Danny Arends, and Ernst C. Wit", "title": "netgwas: An R Package for Network-Based Genome-Wide Association Studies", "comments": "32 pages, 9 figures; due to the limitation \"The abstract field cannot\n  be longer than 1,920 characters\", the abstract appearing here is slightly\n  shorter than that in the PDF file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.BM q-bio.GN q-bio.MN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are powerful tools for modeling and making statistical\ninferences regarding complex associations among variables in multivariate data.\nIn this paper we introduce the R package netgwas, which is designed based on\nundirected graphical models to accomplish three important and interrelated\ngoals in genetics: constructing linkage map, reconstructing linkage\ndisequilibrium (LD) networks from multi-loci genotype data, and detecting\nhigh-dimensional genotype-phenotype networks. The netgwas package deals with\nspecies with any chromosome copy number in a unified way, unlike other\nsoftware. It implements recent improvements in both linkage map construction\n(Behrouzi and Wit, 2018), and reconstructing conditional independence network\nfor non-Gaussian continuous data, discrete data, and mixed\ndiscrete-and-continuous data (Behrouzi and Wit, 2017). Such datasets routinely\noccur in genetics and genomics such as genotype data, and genotype-phenotype\ndata. We demonstrate the value of our package functionality by applying it to\nvarious multivariate example datasets taken from the literature. We show, in\nparticular, that our package allows a more realistic analysis of data, as it\nadjusts for the effect of all other variables while performing pairwise\nassociations. This feature controls for spurious associations between variables\nthat can arise from classical multiple testing approach. This paper includes a\nbrief overview of the statistical methods which have been implemented in the\npackage. The main body of the paper explains how to use the package. The\npackage uses a parallelization strategy on multi-core processors to speed-up\ncomputations for large datasets. In addition, it contains several functions for\nsimulation and visualization. The netgwas package is freely available at\nhttps://cran.r-project.org/web/packages/netgwas\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 16:03:00 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 10:43:56 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 15:07:42 GMT"}, {"version": "v4", "created": "Tue, 1 May 2018 12:37:59 GMT"}, {"version": "v5", "created": "Thu, 25 Apr 2019 22:07:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Behrouzi", "Pariya", ""], ["Arends", "Danny", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1710.01369", "submitter": "Brenda Betancourt", "authors": "Brenda Betancourt, Abel Rodr\\'iguez, Naomi Boyd", "title": "Bayesian Fused Lasso regression for dynamic binary networks", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1341323", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multinomial logistic regression model for link prediction in a\ntime series of directed binary networks. To account for the dynamic nature of\nthe data we employ a dynamic model for the model parameters that is strongly\nconnected with the fused lasso penalty. In addition to promoting sparseness,\nthis prior allows us to explore the presence of change points in the structure\nof the network. We introduce fast computational algorithms for estimation and\nprediction using both optimization and Bayesian approaches. The performance of\nthe model is illustrated using simulated data and data from a financial trading\nnetwork in the NYMEX natural gas futures market. Supplementary material\ncontaining the trading network data set and code to implement the algorithms is\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 20:07:44 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Betancourt", "Brenda", ""], ["Rodr\u00edguez", "Abel", ""], ["Boyd", "Naomi", ""]]}, {"id": "1710.01398", "submitter": "Brenda Betancourt", "authors": "Brenda Betancourt, Abel Rodr\\'iguez, Naomi Boyd", "title": "Investigating Competition in Financial Markets: A Sparse Autologistic\n  Model for Dynamic Network Data", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2017.1357684", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a sparse autologistic model for investigating the impact of\ndiversification and disintermediation strategies in the evolution of financial\ntrading networks. In order to induce sparsity in the model estimates and\naddress substantive questions about the underlying processes the model includes\nan $L^1$ regularization penalty. This makes implementation feasible for complex\ndynamic networks in which the number of parameters is considerably greater than\nthe number of observations over time. We use the model to characterize trader\nbehavior in the NYMEX natural gas futures market, where we find that\ndisintermediation and not diversification or momentum tend to drive market\nmicrostructure.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 22:02:40 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Betancourt", "Brenda", ""], ["Rodr\u00edguez", "Abel", ""], ["Boyd", "Naomi", ""]]}, {"id": "1710.01415", "submitter": "Brenda Betancourt", "authors": "Brenda Betancourt, Abel Rodr\\'iguez, Naomi Boyd", "title": "Modelling and prediction of financial trading networks: An application\n  to the NYMEX natural gas futures market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years there has been a growing interest in using financial\ntrading networks to understand the microstructure of financial markets. Most of\nthe methodologies developed so far for this purpose have been based on the\nstudy of descriptive summaries of the networks such as the average node degree\nand the clustering coefficient. In contrast, this paper develops novel\nstatistical methods for modeling sequences of financial trading networks. Our\napproach uses a stochastic blockmodel to describe the structure of the network\nduring each period, and then links multiple time periods using a hidden Markov\nmodel. This structure allows us to identify events that affect the structure of\nthe market and make accurate short-term prediction of future transactions. The\nmethodology is illustrated using data from the NYMEX natural gas futures market\nfrom January 2005 to December 2008.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 22:44:30 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Betancourt", "Brenda", ""], ["Rodr\u00edguez", "Abel", ""], ["Boyd", "Naomi", ""]]}, {"id": "1710.01416", "submitter": "Saed Khawaldeh", "authors": "Vu Hoang Minh, Tajwar Abrar Aleef, Usama Pervaiz, Yeman Brhane Hagos,\n  Saed Khawaldeh", "title": "Smoothness-based Edge Detection using Low-SNR Camera for Robot\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the emerging advancement in the branch of autonomous robotics, the ability\nof a robot to efficiently localize and construct maps of its surrounding is\ncrucial. This paper deals with utilizing thermal-infrared cameras, as opposed\nto conventional cameras as the primary sensor to capture images of the robot's\nsurroundings. For localization, the images need to be further processed before\nfeeding them to a navigational system. The main motivation of this paper was to\ndevelop an edge detection methodology capable of utilizing the low-SNR poor\noutput from such a thermal camera and effectively detect smooth edges of the\nsurrounding environment. The enhanced edge detector proposed in this paper\ntakes the raw image from the thermal sensor, denoises the images, applies Canny\nedge detection followed by CSS method. The edges are ranked to remove any noise\nand only edges of the highest rank are kept. Then, the broken edges are linked\nby computing edge metrics and a smooth edge of the surrounding is displayed in\na binary image. Several comparisons are also made in the paper between the\nproposed technique and the existing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 22:48:41 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Minh", "Vu Hoang", ""], ["Aleef", "Tajwar Abrar", ""], ["Pervaiz", "Usama", ""], ["Hagos", "Yeman Brhane", ""], ["Khawaldeh", "Saed", ""]]}, {"id": "1710.01441", "submitter": "Atsushi Iwasaki", "authors": "Atsushi Iwasaki", "title": "Analysis of NIST SP800-22 focusing on randomness of each sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NIST SP800-22 is a randomness test set applied for a set of sequences.\nAlthough SP800-22 widely used, a rational criterion throughout all test items\nhas not been shown. The main reason is that the dependency of test items has\nnot been perfectly clear. In this paper, a certain scalar is computed for each\nsequence throughout all test items and make the histogram of the scalar. By\ncomparing the histogram and the theoretical distribution under some\nassumptions, the dependency is visually shown. In addition, an algorithmic\nmethod to derive \"minimum set\" using the histogram is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 02:27:41 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Iwasaki", "Atsushi", ""]]}, {"id": "1710.01470", "submitter": "Saeid Rezakhah", "authors": "H. Ghasemi, S. Rezakhah and N. Modarresi", "title": "Multi-scale Invariant Fields: Estimation and Prediction", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the concept of multi-selfsimilar random field we study multi-scale\ninvariant (MSI) fields which have component-wise discrete scale invariant\nproperty. Assuming scale parameters as $\\lambda_i>1$, $i=1,\\ldots,d$ and the\nparameter space as $(1, \\infty)^d$, the first scale rectangle is referred to\nthe rectangle $ (1, \\lambda_1)\\times \\ldots \\times (1, \\lambda_d)$. Applying\ncertain component-wise geometric sampling of MSI field, the harmonic-like\nrepresentation and spectral density of the sampled MSI field are characterized.\nFurthermore, the covariance function and spectral density of the sampled Markov\nMSI field are presented by the variances and covariances of samples inside\nfirst scale rectangle. As an example of MSI field, a two-dimensional simple\nfractional Brownian sheet (sfBs) is demonstrated. Also real data of the\nprecipitation in some area of Brisbane in Australia for two days (25 and 26\nJanuary 2013) are examined. We show that precipitation on this area has MSI\nproperty and estimate it as a simple MSI field with stationary increments\ninside scale intervals.\n  This structure enables us to predict the precipitation in surface and time.\nWe apply the mean absolute percentage error as a measure for the accuracy of\nthe predictions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 06:01:44 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 08:58:05 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 13:58:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ghasemi", "H.", ""], ["Rezakhah", "S.", ""], ["Modarresi", "N.", ""]]}, {"id": "1710.01490", "submitter": "Mohamed Laib", "authors": "Mohamed Laib, Jean Golay, Luciano Telesca, Mikhail Kanevski", "title": "Multifractal analysis of the time series of daily means of wind speed in\n  complex regions", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.02.024", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we applied the multifractal detrended fluctuation analysis to\nthe daily means of wind speed measured by 119 weather stations distributed over\nthe territory of Switzerland. The analysis was focused on the inner time\nfluctuations of wind speed, which could be more linked with the local\nconditions of the highly varying topography of Switzerland. Our findings point\nout to a persistent behaviour of all the measured wind speed series (indicated\nby a Hurst exponent significantly larger than 0.5), and to a high\nmultifractality degree indicating a relative dominance of the large\nfluctuations in the dynamics of wind speed, especially in the Swiss plateau,\nwhich is comprised between the Jura and Alp mountain ranges. The study\nrepresents a contribution to the understanding of the dynamical mechanisms of\nwind speed variability in mountainous regions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 07:46:53 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Laib", "Mohamed", ""], ["Golay", "Jean", ""], ["Telesca", "Luciano", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1710.01581", "submitter": "Billy Pik Lik Lau", "authors": "Billy Pik Lik Lau, Nipun Wijerathne, Benny Kai Kiat Ng and and Chau\n  Yuen", "title": "Sensor Fusion for Public Space Utilization Monitoring in a Smart City", "comments": null, "journal-ref": null, "doi": "10.1109/JIOT.2017.2748987", "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public space utilization is crucial for urban developers to understand how\nefficient a place is being occupied in order to improve existing or future\ninfrastructures. In a smart cities approach, implementing public space\nmonitoring with Internet-of-Things (IoT) sensors appear to be a viable\nsolution. However, choice of sensors often is a challenging problem and often\nlinked with scalability, coverage, energy consumption, accuracy, and privacy.\nTo get the most from low cost sensor with aforementioned design in mind, we\nproposed data processing modules for capturing public space utilization with\nRenewable Wireless Sensor Network (RWSN) platform using pyroelectric infrared\n(PIR) and analog sound sensor. We first proposed a calibration process to\nremove false alarm of PIR sensor due to the impact of weather and environment.\nWe then demonstrate how the sounds sensor can be processed to provide various\ninsight of a public space. Lastly, we fused both sensors and study a particular\npublic space utilization based on one month data to unveil its usage.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 01:35:16 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 15:35:08 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Lau", "Billy Pik Lik", ""], ["Wijerathne", "Nipun", ""], ["Ng", "Benny Kai Kiat", ""], ["Yuen", "and Chau", ""]]}, {"id": "1710.01662", "submitter": "Colin  Gillespie", "authors": "Colin S Gillespie", "title": "Estimating the number of casualties in the American Indian war: a\n  Bayesian analysis using the power law distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The American Indian war lasted over one hundred years, and is a major event\nin the history of North America. As expected, since the war commenced in late\neighteenth century, casualty records surrounding this conflict contain numerous\nsources of error, such as rounding and counting. Additionally, while major\nbattles such as the Battle of the Little Bighorn were recorded, many smaller\nskirmishes were completely omitted from the records. Over the last few decades,\nit has been observed that the number of casualties in major conflicts follows a\npower law distribution. This paper places this observation within the Bayesian\nparadigm, enabling modelling of different error sources, allowing inferences to\nbe made about the overall casualty numbers in the American Indian war.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 15:38:53 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Gillespie", "Colin S", ""]]}, {"id": "1710.01701", "submitter": "Dhruv Shah", "authors": "Dhruv Shah, Sebastian Scherer", "title": "Robust Localization of an Arbitrary Distribution of Radioactive Sources\n  for Aerial Inspection", "comments": "15 pages, 10 figures. Accepted for presentation in Waste Management\n  Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiation source detection has seen various applications in the past decade,\nranging from the detection of dirty bombs in public places to scanning critical\nnuclear facilities for leakage or flaws, and in the autonomous inspection of\nnuclear sites. Despite the success in detecting single point sources or a small\nnumber of spatially separated point sources, most of the existing algorithms\nfail to localize sources in complex scenarios with a large number of point\nsources or non-trivial distributions & bulk sources. Even in simpler\nenvironments, most existing algorithms are not scalable to larger regions\nand/or higher dimensional spaces. For effective autonomous inspection, we not\nonly need to estimate the positions of the sources, but also the number,\ndistribution, and intensities of each of them. In this paper, we present a\nnovel algorithm for the robust localization of an arbitrary distribution of\nradiation sources using multi-layer sequential Monte Carlo methods coupled with\nsuitable clustering algorithms. We achieve near-perfect accuracy, in terms of\nF1-scores (> 0.95), while allowing the algorithm to scale, both to large\nregions in space and to higher dimensional spaces (5 tested).\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 17:09:46 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 22:54:31 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 20:06:00 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Shah", "Dhruv", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1710.01898", "submitter": "Ansgar Steland", "authors": "Ansgar Steland and Yuan-Tsung Chang", "title": "Jackknife variance estimation for common mean estimators under ordered\n  variances and general two-sample statistics", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samples with a common mean but possibly different, ordered variances arise in\nvarious fields such as interlaboratory experiments, field studies or the\nanalysis of sensor data. Estimators for the common mean under ordered variances\ntypically employ random weights, which depend on the sample means and the\nunbiased variance estimators. They take different forms when the sample\nestimators are in agreement with the order constraints or not, which\ncomplicates even basic analyses such as estimating their variance. We propose\nto use the jackknife, whose consistency is established for general smooth\ntwo--sample statistics induced by continuously G\\^ateux or Fr\\'echet\ndifferentiable functionals, and, more generally, asymptotically linear\ntwo--sample statistics, allowing us to study a large class of common mean\nestimators. Further, it is shown that the common mean estimators under\nconsideration satisfy a central limit theorem (CLT). We investigate the\naccuracy of the resulting confidence intervals by simulations and illustrate\nthe approach by analyzing several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 07:08:18 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 09:14:58 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Steland", "Ansgar", ""], ["Chang", "Yuan-Tsung", ""]]}, {"id": "1710.02091", "submitter": "Paul Sharkey", "authors": "Paul Sharkey and Hugo C. Winter", "title": "A Bayesian spatial hierarchical model for extreme precipitation in Great\n  Britain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intense precipitation events are commonly known to be associated with an\nincreased risk of flooding. As a result of the societal and infrastructural\nrisks linked with flooding, extremes of precipitation require careful\nmodelling. Extreme value analysis is typically used to model large\nprecipitation events, though a site-by-site analysis tends to produce spatially\ninconsistent risk estimates. In reality, one would expect neighbouring\nlocations to have more similar risk estimates than locations separated by large\ndistances. We present an approach, in the Bayesian hierarchical modelling\nframework, that imposes a spatial structure on the parameters of a generalised\nPareto distribution. In particular, we look at the clear benefits of this\napproach in improving spatial consistency of return level estimates and\nincreasing precision of these estimates. Unlike many previous approaches that\npool information over locations, we account for the spatial dependence of the\ndata in our confidence intervals. We implement this model for gridded\nprecipitation data over Great Britain.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:13:43 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Sharkey", "Paul", ""], ["Winter", "Hugo C.", ""]]}, {"id": "1710.02219", "submitter": "S. Stanley Young", "authors": "Karl E. Peace, JingJing Yin, Haresh Rochani, Sarbesh Pandeya, S.\n  Stanley Young", "title": "The reliability of a nutritional meta-analysis study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many researchers have studied the relationship between diet and\nhealth. There are papers showing an association between the consumption of\nsugar-sweetened beverages and Type 2 diabetes. Many meta-analyses use\nindividual studies that do not adjust for multiple testing or multiple modeling\nand thus provide biased estimates of effect. Hence the claims reported in a\nmeta-analysis paper may be unreliable if the primary papers do not ensure\nunbiased estimates of effect. Objective: Determine the statistical reliability\nof 10 papers and indirectly the reliability of the meta-analysis study. Method:\nTen primary papers used in a meta-analysis paper and counted the numbers of\noutcomes, predictors, and covariates. We estimated the size of the potential\nanalysis search space available to the authors of these papers; i.e. the number\nof comparisons and models available. Since we noticed that there were\ndifferences between predictors and covariates cited in the abstract and in the\ntext, we applied this formula to information found in the abstracts, Space A,\nas well as the text, Space T, of each primary paper. Results: The median and\nrange of the number of comparisons possible across the primary papers are 6.5\nand (2-12,288) for abstracts, and 196,608 and (3,072-117,117,952) the texts.\nNote that the median of 6.5 for Space A is misleading as each primary study has\n60-165 foods not mentioned in the abstract. Conclusion: Given that testing is\nat the 0.05 level and the number of comparisons is very large, nominal\nstatistical significance is very weak support for a claim. The claims in these\npapers are not statistically supported and hence are unreliable. Thus, the\nclaims of the meta-analysis paper lack evidentiary confirmation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:08:15 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Peace", "Karl E.", ""], ["Yin", "JingJing", ""], ["Rochani", "Haresh", ""], ["Pandeya", "Sarbesh", ""], ["Young", "S. Stanley", ""]]}, {"id": "1710.02469", "submitter": "Ryan Sun", "authors": "Ryan Sun and Xihong Lin", "title": "Set-Based Tests for Genetic Association Using the Generalized Berk-Jones\n  Statistic", "comments": "Corrected typos in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the effects of groups of Single Nucleotide Polymorphisms (SNPs), as\nin a gene, genetic pathway, or network, can provide novel insight into complex\ndiseases, above that which can be gleaned from studying SNPs individually.\nCommon challenges in set-based genetic association testing include weak effect\nsizes, correlation between SNPs in a SNP-set, and scarcity of signals, with\nsingle-SNP effects often ranging from extremely sparse to moderately sparse in\nnumber. Motivated by these challenges, we propose the Generalized Berk-Jones\n(GBJ) test for the association between a SNP-set and outcome. The GBJ extends\nthe Berk-Jones (BJ) statistic by accounting for correlation among SNPs, and it\nprovides advantages over the Generalized Higher Criticism (GHC) test when\nsignals in a SNP-set are moderately sparse. We also provide an analytic p-value\ncalculation procedure for SNP-sets of any finite size. Using this p-value\ncalculation, we illustrate that the rejection region for GBJ can be described\nas a compromise of those for BJ and GHC. We develop an omnibus statistic as\nwell, and we show that this omnibus test is robust to the degree of signal\nsparsity. An additional advantage of our method is the ability to conduct\ninference using individual SNP summary statistics from a genome-wide\nassociation study. We evaluate the finite sample performance of the GBJ though\nsimulation studies and application to gene-level association analysis of breast\ncancer risk.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 16:04:24 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 01:54:38 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Sun", "Ryan", ""], ["Lin", "Xihong", ""]]}, {"id": "1710.02520", "submitter": "Ana Helena Tavares", "authors": "Ana Helena Tavares and Jakob Raymaekers and Peter Rousseeuw and Raquel\n  M. Silva and Carlos A.C. Bastos and Armando Pinho and Paula Brito and Vera\n  Afreixo", "title": "Comparing reverse complementary genomic words based on their distance\n  distributions and frequencies", "comments": "Post-print of a paper accepted to publication in \"Interdisciplinary\n  Sciences: Computational Life Sciences\" (ISSN: 1913-2751, ESSN: 1867-1462)", "journal-ref": "Interdisciplinary Sciences: Computational Life Sciences, 2018,\n  Vol. 10, 1-11", "doi": "10.1007/s12539-017-0273-0", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study reverse complementary genomic word pairs in the human\nDNA, by comparing both the distance distribution and the frequency of a word to\nthose of its reverse complement. Several measures of dissimilarity between\ndistance distributions are considered, and it is found that the peak\ndissimilarity works best in this setting. We report the existence of reverse\ncomplementary word pairs with very dissimilar distance distributions, as well\nas word pairs with very similar distance distributions even when both\ndistributions are irregular and contain strong peaks. The association between\ndistribution dissimilarity and frequency discrepancy is explored also, and it\nis speculated that symmetric pairs combining low and high values of each\nmeasure may uncover features of interest. Taken together, our results suggest\nthat some asymmetries in the human genome go far beyond Chargaff's rules. This\nstudy uses both the complete human genome and its repeat-masked version.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 12:06:44 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Tavares", "Ana Helena", ""], ["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter", ""], ["Silva", "Raquel M.", ""], ["Bastos", "Carlos A. C.", ""], ["Pinho", "Armando", ""], ["Brito", "Paula", ""], ["Afreixo", "Vera", ""]]}, {"id": "1710.02669", "submitter": "Jerzy Rydlewski", "authors": "Daniel Kosiorowski, Dominik Mielczarek, Jerzy P. Rydlewski", "title": "Aggregated moving functional median in robust prediction of hierarchical\n  functional time series - an application to forecasting web portal users\n  behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new nonparametric and robust method of forecasting\nhierarchical functional time series is presented. The method is compared with\nHyndman and Shang's method with respect to their unbiasedness, effectiveness,\nrobustness, and computational complexity. Taking into account results of the\nanalytical, simulation and empirical studies, we come to the conclusion that\nour proposal is superior over the proposal of Hyndman and Shang with respect to\nsome statistical criteria and especially with respect to robustness and\ncomputational complexity. An empirical usefulness of our method is presented on\nexample of management of a certain web portal divided into four subservices. An\nextensive simulation study involving hierarchical systems consisted of FAR(1)\nprocesses and Wiener processes has been conducted as well.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 11:00:46 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 10:59:19 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy P.", ""]]}, {"id": "1710.02690", "submitter": "Rebecca Steorts", "authors": "Beidi Chen, Anshumali Shrivastava, Rebecca C. Steorts", "title": "Unique Entity Estimation with Application to the Syrian Conflict", "comments": "35 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution identifies and removes duplicate entities in large, noisy\ndatabases and has grown in both usage and new developments as a result of\nincreased data availability. Nevertheless, entity resolution has tradeoffs\nregarding assumptions of the data generation process, error rates, and\ncomputational scalability that make it a difficult task for real applications.\nIn this paper, we focus on a related problem of unique entity estimation, which\nis the task of estimating the unique number of entities and associated standard\nerrors in a data set with duplicate entities. Unique entity estimation shares\nmany fundamental challenges of entity resolution, namely, that the\ncomputational cost of all-to-all entity comparisons is intractable for large\ndatabases. To circumvent this computational barrier, we propose an efficient\n(near-linear time) estimation algorithm based on locality sensitive hashing.\nOur estimator, under realistic assumptions, is unbiased and has provably low\nvariance compared to existing random sampling based approaches. In addition, we\nempirically show its superiority over the state-of-the-art estimators on three\nreal applications. The motivation for our work is to derive an accurate\nestimate of the documented, identifiable deaths in the ongoing Syrian conflict.\nOur methodology, when applied to the Syrian data set, provides an estimate of\n$191,874 \\pm 1772$ documented, identifiable deaths, which is very close to the\nHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides\nan example of challenges and efforts involved in solving a real, noisy\nchallenging problem where modeling assumptions may not hold.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 14:30:20 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1710.02773", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "Baseline Mixture Models for Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous mixtures of distributions are widely employed in the statistical\nliterature as models for phenomena with highly divergent outcomes; in\nparticular, many familiar heavy-tailed distributions arise naturally as\nmixtures of light-tailed distributions (e.g., Gaussians), and play an important\nrole in applications as diverse as modeling of extreme values and robust\ninference. In the case of social networks, continuous mixtures of graph\ndistributions can likewise be employed to model social processes with\nheterogeneous outcomes, or as robust priors for network inference. Here, we\nintroduce some simple families of network models based on continuous mixtures\nof baseline distributions. While analytically and computationally tractable,\nthese models allow more flexible modeling of cross-graph heterogeneity than is\npossible with conventional baseline (e.g., Bernoulli or $U|man$ distributions).\nWe illustrate the utility of these baseline mixture models with application to\nproblems of multiple-network ERGMs, network evolution, and efficient network\ninference. Our results underscore the potential ubiquity of network processes\nwith nontrivial mixture behavior in natural settings, and raise some\npotentially disturbing questions regarding the adequacy of current network data\ncollection practices.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 03:00:24 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "1710.02824", "submitter": "Lisandro Kaunitz", "authors": "Lisandro Kaunitz, Shenjun Zhong, Javier Kreiner", "title": "Beating the bookies with their own numbers - and how the online sports\n  betting market is rigged", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online sports gambling industry employs teams of data analysts to build\nforecast models that turn the odds at sports games in their favour. While\nseveral betting strategies have been proposed to beat bookmakers, from expert\nprediction models and arbitrage strategies to odds bias exploitation, their\nreturns have been inconsistent and it remains to be shown that a betting\nstrategy can outperform the online sports betting market. We designed a\nstrategy to beat football bookmakers with their own numbers. Instead of\nbuilding a forecasting model to compete with bookmakers predictions, we\nexploited the probability information implicit in the odds publicly available\nin the marketplace to find bets with mispriced odds. Our strategy proved\nprofitable in a 10-year historical simulation using closing odds, a 6-month\nhistorical simulation using minute to minute odds, and a 5-month period during\nwhich we staked real money with the bookmakers (we made code, data and models\npublicly available). Our results demonstrate that the football betting market\nis inefficient - bookmakers can be consistently beaten across thousands of\ngames in both simulated environments and real-life betting. We provide a\ndetailed description of our betting experience to illustrate how the sports\ngambling industry compensates these market inefficiencies with discriminatory\npractices against successful clients.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 11:44:35 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 04:09:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kaunitz", "Lisandro", ""], ["Zhong", "Shenjun", ""], ["Kreiner", "Javier", ""]]}, {"id": "1710.02961", "submitter": "Markus Hainy", "authors": "Xing Ju Lee, Markus Hainy, James P. McKeone, Christopher C. Drovandi,\n  Anthony N. Pettitt", "title": "ABC model selection for spatial extremes models applied to South\n  Australian maximum temperature data", "comments": "Changes to previous version: slightly altered abstract; some of the\n  footnotes were eliminated and incorporated into the main text and the table\n  captions; new section B.1 contains content of former footnote from Section\n  4.3", "journal-ref": "Computational Statistics & Data Analysis 128 (2018), 128-144", "doi": "10.1016/j.csda.2018.06.019", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are a common choice for modelling spatial extreme data\nas they arise naturally as the infinite-dimensional generalisation of\nmultivariate extreme value theory. Statistical inference for such models is\ncomplicated by the intractability of the multivariate density function.\nNonparametric, composite likelihood-based, and Bayesian approaches have been\nproposed to address this difficulty. More recently, a simulation-based approach\nusing approximate Bayesian computation (ABC) has been employed for estimating\nparameters of max-stable models. ABC algorithms rely on the evaluation of\ndiscrepancies between model simulations and the observed data rather than\nexplicit evaluations of computationally expensive or intractable likelihood\nfunctions. The use of an ABC method to perform model selection for max-stable\nmodels is explored. Three max-stable models are regarded: the extremal-t model\nwith either a Whittle-Mat\\'ern or a powered exponential covariance function,\nand the Brown-Resnick model with power variogram. In addition, the non-extremal\nStudent-t copula model with a Whittle-Mat\\'ern or a powered exponential\ncovariance function is also considered. The method is applied to annual maximum\ntemperature data from 25 weather stations dispersed around South Australia.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 06:56:04 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 07:04:54 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 03:36:18 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2018 06:31:13 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Lee", "Xing Ju", ""], ["Hainy", "Markus", ""], ["McKeone", "James P.", ""], ["Drovandi", "Christopher C.", ""], ["Pettitt", "Anthony N.", ""]]}, {"id": "1710.02976", "submitter": "Marco Iglesias", "authors": "Lia De Simon, Marco Iglesias, Benjamin Jones, Christopher Wood", "title": "Quantifying uncertainty in thermal properties of walls by means of\n  Bayesian inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a computational framework to statistically infer thermophysical\nproperties of any given wall from in-situ measurements of air temperature and\nsurface heat fluxes. The proposed framework uses these measurements, within a\nBayesian calibration approach, to sequentially infer input parameters of a\none-dimensional heat diffusion model that describes the thermal performance of\nthe wall. These inputs include spatially-variable functions that characterise\nthe thermal conductivity and the volumetric heat capacity of the wall. We\nencode our computational framework in an algorithm that sequentially updates\nour probabilistic knowledge of the thermophysical properties as new\nmeasurements become available, and thus enables an on-the-fly uncertainty\nquantification of these properties. In addition, the proposed algorithm enables\nus to investigate the effect of the discretisation of the underlying heat\ndiffusion model on the accuracy of estimates of thermophysical properties and\nthe corresponding predictive distributions of heat flux. By means of\nvirtual/synthetic and real experiments we show the capabilities of the proposed\napproach to (i) characterise heterogenous thermophysical properties associated\nwith, for example, unknown cavities and insulators; (ii) obtain rapid and\naccurate uncertainty estimates of effective thermal properties (e.g. thermal\ntransmittance); and (iii) accurately compute an statistical description of the\nthermal performance of the wall which is, in turn, crucial in evaluating\npossible retrofit measures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 07:43:11 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 14:35:50 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["De Simon", "Lia", ""], ["Iglesias", "Marco", ""], ["Jones", "Benjamin", ""], ["Wood", "Christopher", ""]]}, {"id": "1710.03181", "submitter": "Marco Antonio Aquino Lopez", "authors": "Marco A Aquino-L\\'opez and Maarten Blaauw and J Andr\\'es Christen and\n  Nicole K. Sanderson", "title": "Bayesian analysis of 210Pb dating", "comments": "22 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many studies of environmental change of the past few centuries, 210Pb\ndating is used to obtain chronologies for sedimentary sequences. One of the\nmost commonly used approaches to estimate the ages of depths in a sequence is\nto assume a constant rate of supply (CRS) or influx of `unsupported' 210Pb from\nthe atmosphere, together with a constant or varying amount of `supported'\n210Pb. Current 210Pb dating models do not use a proper statistical framework\nand thus provide poor estimates of errors. Here we develop a new model for\n210Pb dating, where both ages and values of supported and unsupported 210Pb\nform part of the parameters. We apply our model to a case study from Canada as\nwell as to some simulated examples. Our model can extend beyond the current CRS\napproach, deal with asymmetric errors and mix 210Pb with other types of dating,\nthus obtaining more robust, realistic and statistically better defined\nestimates.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 16:29:19 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Aquino-L\u00f3pez", "Marco A", ""], ["Blaauw", "Maarten", ""], ["Christen", "J Andr\u00e9s", ""], ["Sanderson", "Nicole K.", ""]]}, {"id": "1710.03222", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Christoph Bergmeir, Slawek Smyl", "title": "Forecasting Across Time Series Databases using Recurrent Neural Networks\n  on Groups of Similar Series: A Clustering Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Big Data, nowadays in many applications databases\ncontaining large quantities of similar time series are available. Forecasting\ntime series in these domains with traditional univariate forecasting procedures\nleaves great potentials for producing accurate forecasts untapped. Recurrent\nneural networks (RNNs), and in particular Long Short-Term Memory (LSTM)\nnetworks, have proven recently that they are able to outperform\nstate-of-the-art univariate time series forecasting methods in this context\nwhen trained across all available time series. However, if the time series\ndatabase is heterogeneous, accuracy may degenerate, so that on the way towards\nfully automatic forecasting methods in this space, a notion of similarity\nbetween the time series needs to be built into the methods. To this end, we\npresent a prediction model that can be used with different types of RNN models\non subgroups of similar time series, which are identified by time series\nclustering techniques. We assess our proposed methodology using LSTM networks,\na widely popular RNN variant. Our method achieves competitive results on\nbenchmarking datasets under competition evaluation procedures. In particular,\nin terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM\nmodel and outperforms all other methods on the CIF2016 forecasting competition\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 04:08:15 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 08:03:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bandara", "Kasun", ""], ["Bergmeir", "Christoph", ""], ["Smyl", "Slawek", ""]]}, {"id": "1710.03268", "submitter": "Matthew K. Breitenstein Ph.D.", "authors": "Alena Orlenko, Jason H. Moore, Patryk Orzechowski, Randal S. Olson,\n  Junmei Cairns, Pedro J. Caraballo, Richard M. Weinshilboum, Liewei Wang,\n  Matthew K. Breitenstein", "title": "Considerations of automated machine learning in clinical metabolic\n  profiling: Altered homocysteine plasma concentration associated with\n  metformin exposure", "comments": "Manuscript - containing supplementary information - accepted\n  (9/15/2017) for publication within Pacific Symposium on Biocomputing 2018\n  <https://psb.stanford.edu/psb-online>. Original supplementary information\n  includes an additional 6 pages of content (18 pages total) and 8 figures (13\n  figures total)", "journal-ref": "Pacific Symposium on Biocomputing, 2018 (Vol. 23)", "doi": null, "report-no": null, "categories": "q-bio.MN q-bio.PE q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the maturation of metabolomics science and proliferation of biobanks,\nclinical metabolic profiling is an increasingly opportunistic frontier for\nadvancing translational clinical research. Automated Machine Learning (AutoML)\napproaches provide exciting opportunity to guide feature selection in agnostic\nmetabolic profiling endeavors, where potentially thousands of independent data\npoints must be evaluated. In previous research, AutoML using high-dimensional\ndata of varying types has been demonstrably robust, outperforming traditional\napproaches. However, considerations for application in clinical metabolic\nprofiling remain to be evaluated. Particularly, regarding the robustness of\nAutoML to identify and adjust for common clinical confounders. In this study,\nwe present a focused case study regarding AutoML considerations for using the\nTree-Based Optimization Tool (TPOT) in metabolic profiling of exposure to\nmetformin in a biobank cohort. First, we propose a tandem rank-accuracy measure\nto guide agnostic feature selection and corresponding threshold determination\nin clinical metabolic profiling endeavors. Second, while AutoML, using default\nparameters, demonstrated potential to lack sensitivity to low-effect\nconfounding clinical covariates, we demonstrated residual training and\nadjustment of metabolite features as an easily applicable approach to ensure\nAutoML adjustment for potential confounding characteristics. Finally, we\npresent increased homocysteine with long-term exposure to metformin as a\npotentially novel, non-replicated metabolite association suggested by TPOT; an\nassociation not identified in parallel clinical metabolic profiling endeavors.\nWhile considerations are recommended, including adjustment approaches for\nclinical confounders, AutoML presents an exciting tool to enhance clinical\nmetabolic profiling and advance translational research endeavors.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:19:57 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Orlenko", "Alena", ""], ["Moore", "Jason H.", ""], ["Orzechowski", "Patryk", ""], ["Olson", "Randal S.", ""], ["Cairns", "Junmei", ""], ["Caraballo", "Pedro J.", ""], ["Weinshilboum", "Richard M.", ""], ["Wang", "Liewei", ""], ["Breitenstein", "Matthew K.", ""]]}, {"id": "1710.03296", "submitter": "Youjin Lee", "authors": "Youjin Lee, Elizabeth L. Ogburn", "title": "Testing for Network and Spatial Autocorrelation", "comments": null, "journal-ref": "International Conference on Network Science (2020) 91-104", "doi": "10.1007/978-3-030-38965-9_7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for dependence has been a well-established component of spatial\nstatistical analyses for decades. In particular, several popular test\nstatistics have desirable properties for testing for the presence of spatial\nautocorrelation in continuous variables. In this paper we propose two\ncontributions to the literature on tests for autocorrelation. First, we propose\na new test for autocorrelation in categorical variables. While some methods\ncurrently exist for assessing spatial autocorrelation in categorical variables,\nthe most popular method is unwieldy, somewhat ad hoc, and fails to provide\ngrounds for a single omnibus test. Second, we discuss the importance of testing\nfor autocorrelation in data sampled from the nodes of a network, motivated by\nsocial network applications. We demonstrate that our proposed statistic for\ncategorical variables can both be used in the spatial and network setting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:11:02 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 13:29:00 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 13:03:19 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 23:45:27 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lee", "Youjin", ""], ["Ogburn", "Elizabeth L.", ""]]}, {"id": "1710.03298", "submitter": "Ignacio Alvarez-Castro", "authors": "Natalia da Silva and Ignacio Alvarez-Castro", "title": "Clicks and Cliques. Exploring the Soul of the Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we analyze 26 communities across the United States with the\nobjective to understand what attaches people to their community and how this\nattachment differs among communities. How different are attached people from\nunattached? What attaches people to their community? How different are the\ncommunities? What are key drivers behind emotional attachment? To address these\nquestions, graphical, supervised and unsupervised learning tools were used and\ninformation from the Census Bureau and the Knight Foundation were combined.\nUsing the same pre-processed variables as Knight (2010) most likely will drive\nthe results towards the same conclusions than the Knight foundation, so this\npaper does not use those variables.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:15:33 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["da Silva", "Natalia", ""], ["Alvarez-Castro", "Ignacio", ""]]}, {"id": "1710.03436", "submitter": "Susan Cheng", "authors": "Joseph Antonelli, Brian Claggett, Mir Henglin, Jeramie D. Watrous, Kim\n  A. Lehmann, Pavel Hushcha, Olga Demler, Samia Mora, Teemu Niiranen, Alexandre\n  C. Pereira, Mohit Jain, Susan Cheng", "title": "Statistical Methods and Workflow for Analyzing Human Metabolomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput metabolomics investigations, when conducted in large human\ncohorts, represent a potentially powerful tool for elucidating the biochemical\ndiversity and mechanisms underlying human health and disease. Large-scale\nmetabolomics data, generated using targeted or nontargeted platforms, are\nincreasingly more common. Appropriate statistical analysis of these complex\nhigh-dimensional data is critical for extracting meaningful results from such\nlarge-scale human metabolomics studies. Herein, we consider the main\nstatistical analytical approaches that have been employed in human metabolomics\nstudies. Based on the lessons learned and collective experience to date in the\nfield, we propose a step-by-step framework for pursuing statistical analyses of\nhuman metabolomics data. We discuss the range of options and potential\napproaches that may be employed at each stage of data management, analysis, and\ninterpretation, and offer guidance on analytical considerations that are\nimportant for implementing an analysis workflow. Certain pervasive analytical\nchallenges facing human metabolomics warrant ongoing research. Addressing these\nchallenges will allow for more standardization in the field and lead to\nanalytical advances in metabolomics investigations with the potential to\nelucidate novel mechanisms underlying human health and disease.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:02:51 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:28:27 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Antonelli", "Joseph", ""], ["Claggett", "Brian", ""], ["Henglin", "Mir", ""], ["Watrous", "Jeramie D.", ""], ["Lehmann", "Kim A.", ""], ["Hushcha", "Pavel", ""], ["Demler", "Olga", ""], ["Mora", "Samia", ""], ["Niiranen", "Teemu", ""], ["Pereira", "Alexandre C.", ""], ["Jain", "Mohit", ""], ["Cheng", "Susan", ""]]}, {"id": "1710.03443", "submitter": "Susan Cheng", "authors": "Brian L. Claggett, Joseph Antonelli, Mir Henglin, Jeramie D. Watrous,\n  Kim A. Lehmann, Gabriel Musso, Andrew Correia, Sivani Jonnalagadda, Olga V.\n  Demler, Ramachandran S. Vasan, Martin G. Larson, Mohit Jain, Susan Cheng", "title": "Quantitative Comparison of Statistical Methods for Analyzing Human\n  Metabolomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Emerging technologies now allow for mass spectrometry based\nprofiling of up to thousands of small molecule metabolites (metabolomics) in an\nincreasing number of biosamples. While offering great promise for revealing\ninsight into the pathogenesis of human disease, standard approaches have yet to\nbe established for statistically analyzing increasingly complex,\nhigh-dimensional human metabolomics data in relation to clinical phenotypes\nincluding disease outcomes. To determine optimal statistical approaches for\nmetabolomics analysis, we sought to formally compare traditional statistical as\nwell as newer statistical learning methods across a range of metabolomics\ndataset types. Results. In simulated and experimental metabolomics data derived\nfrom large population-based human cohorts, we observed that with an increasing\nnumber of study subjects, univariate compared to multivariate methods resulted\nin a higher false discovery rate due to substantial correlations among\nmetabolites. In scenarios wherein the number of assayed metabolites increases,\nas in the application of nontargeted versus targeted metabolomics measures,\nmultivariate methods performed especially favorably across a range of\nstatistical operating characteristics. In nontargeted metabolomics datasets\nthat included thousands of metabolite measures, sparse multivariate models\ndemonstrated greater selectivity and lower potential for spurious\nrelationships. Conclusion. When the number of metabolites was similar to or\nexceeded the number of study subjects, as is common with nontargeted\nmetabolomics analysis of relatively small sized cohorts, sparse multivariate\nmodels exhibited the most robust statistical power with more consistent\nresults. These findings have important implications for the analysis of\nmetabolomics studies of human disease.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:23:00 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:28:21 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Claggett", "Brian L.", ""], ["Antonelli", "Joseph", ""], ["Henglin", "Mir", ""], ["Watrous", "Jeramie D.", ""], ["Lehmann", "Kim A.", ""], ["Musso", "Gabriel", ""], ["Correia", "Andrew", ""], ["Jonnalagadda", "Sivani", ""], ["Demler", "Olga V.", ""], ["Vasan", "Ramachandran S.", ""], ["Larson", "Martin G.", ""], ["Jain", "Mohit", ""], ["Cheng", "Susan", ""]]}, {"id": "1710.03526", "submitter": "Pablo Dorta-Gonzalez", "authors": "M\\'onica Clavel, Jes\\'us Arteaga-Ortiz, Rub\\'en Fern\\'andez-Ortiz,\n  Pablo Dorta-Gonz\\'alez", "title": "Measuring the gradualist approach to internationalization", "comments": "18 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is to fill a gap in the literature on\ninternationalization, in relation to the absence of objective and measurable\nperformance indicators on the process of how firms sequentially enter external\nmarkets. To that end, this research develops a quantitative tool that can be\nused as a performance indicator of gradualness for firms entering external\nmarkets at a sectoral level. The performance indicator is based on firms'\nexport volume, number of years of exporting, geographic areas targeted for\nexport, and when exports were initiated for each area. Additionally, the\nindicator is tested empirically in the Spanish wine sector. The main\ncontribution of this study is the creation of an international priority index\nwhich serves as a valuable and reliable tool because of its potential use in\nother industry sectors and geographic areas, allowing us to analyze how\ngeographically differentiated internationalization strategies develop.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 12:00:06 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Clavel", "M\u00f3nica", ""], ["Arteaga-Ortiz", "Jes\u00fas", ""], ["Fern\u00e1ndez-Ortiz", "Rub\u00e9n", ""], ["Dorta-Gonz\u00e1lez", "Pablo", ""]]}, {"id": "1710.03704", "submitter": "Sen Hu", "authors": "Sen Hu, Adrian O'Hagan, Thomas Brendan Murphy", "title": "Motor Insurance Accidental Damage Claims Modeling with Factor Collapsing\n  and Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accidental damage is a typical component of motor insurance claim. Modeling\nof this nature generally involves analysis of past claim history and different\ncharacteristics of the insured objects and the policyholders. Generalized\nlinear models (GLMs) have become the industry's standard approach for pricing\nand modeling risks of this nature. However, the GLM approach utilizes a single\n\"best\" model on which loss predictions are based, which ignores the uncertainty\namong the competing models and variable selection. An additional characteristic\nof motor insurance data sets is the presence of many categorical variables,\nwithin which the number of levels is high. In particular, not all levels of\nsuch variables may be statistically significant and rather some subsets of the\nlevels may be merged to give a smaller overall number of levels for improved\nmodel parsimony and interpretability. A method is proposed for assessing the\noptimal manner in which to collapse a factor with many levels into one with a\nsmaller number of levels, then Bayesian model averaging (BMA) is used to blend\nmodel predictions from all reasonable models to account for factor collapsing\nuncertainty. This method will be computationally intensive due to the number of\nfactors being collapsed as well as the possibly large number of levels within\nfactors. Hence a stochastic optimisation is proposed to quickly find the best\ncollapsing cases across the model space.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 16:21:42 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Hu", "Sen", ""], ["O'Hagan", "Adrian", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1710.03786", "submitter": "Brett McClintock", "authors": "Brett T. McClintock and Theo Michelot", "title": "momentuHMM: R package for generalized hidden Markov models of animal\n  movement", "comments": null, "journal-ref": "Methods in Ecology and Evolution 2018, Vol. 9, No. 6, 1518-1530", "doi": "10.1111/2041-210X.12995", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time hidden Markov models (HMMs) have become an immensely popular\ntool for inferring latent animal behaviors from telemetry data. Here we\nintroduce an open-source R package, momentuHMM, that addresses many of the\ndeficiencies in existing HMM software. Features include: 1) data pre-processing\nand visualization; 2) user-specified probability distributions for an unlimited\nnumber of data streams and latent behavior states; 3) biased and correlated\nrandom walk movement models, including \"activity centers\" associated with\nattractive or repulsive forces; 4) user-specified design matrices and\nconstraints for covariate modelling of parameters using formulas familiar to\nmost R users; 5) multiple imputation methods that account for measurement error\nand temporally-irregular or missing data; 6) seamless integration of\nspatio-temporal covariate raster data; 7) cosinor and spline models for\ncyclical and other complicated patterns; 8) model checking and selection; and\n9) simulation. momentuHMM considerably extends the capabilities of existing HMM\nsoftware while accounting for common challenges associated with telemetery\ndata. It therefore facilitates more realistic hypothesis-driven animal movement\nanalyses that have hitherto been largely inaccessible to non-statisticians.\nWhile motivated by telemetry data, the package can be used for analyzing any\ntype of data that is amenable to HMMs. Practitioners interested in additional\nfeatures are encouraged to contact the authors.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 19:03:53 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 23:51:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["McClintock", "Brett T.", ""], ["Michelot", "Theo", ""]]}, {"id": "1710.03866", "submitter": "Derek Smith", "authors": "Derek K Smith, Loren E Smith, Brett Kroncke, Frederic T Billings, Jens\n  Meiler, Jeffrey Blume", "title": "An Empirical Bayes Approach to Regularization Using Previously Published\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript proposes a novel empirical Bayes technique for regularizing\nregression coefficients in predictive models. When predictions from a\npreviously published model are available, this empirical Bayes method provides\na natural mathematical framework for shrinking coefficients toward the\nestimates implied by the body of existing research rather than the shrinkage\ntoward zero provided by traditional L1 and L2 penalization schemes. The method\nis applied to two different prediction problems. The first involves the\nconstruction of a model for predicting whether a single nucleotide polymorphism\n(SNP) of the KCNQ1 gene will result in dysfunction of the corresponding voltage\ngated ion channel. The second involves the prediction of preoperative serum\ncreatinine change in patients undergoing cardiac surgery.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 00:41:01 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Smith", "Derek K", ""], ["Smith", "Loren E", ""], ["Kroncke", "Brett", ""], ["Billings", "Frederic T", ""], ["Meiler", "Jens", ""], ["Blume", "Jeffrey", ""]]}, {"id": "1710.04030", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Zhiyong Zhou, Anders Garpebring, Jun Yu", "title": "Sparsity estimation in compressive sensing with application to MR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of compressive sensing (CS) asserts that an unknown signal\n$\\mathbf{x} \\in \\mathbb{C}^N$ can be accurately recovered from $m$ measurements\nwith $m\\ll N$ provided that $\\mathbf{x}$ is sparse. Most of the recovery\nalgorithms need the sparsity $s=\\lVert\\mathbf{x}\\rVert_0$ as an input. However,\ngenerally $s$ is unknown, and directly estimating the sparsity has been an open\nproblem. In this study, an estimator of sparsity is proposed by using Bayesian\nhierarchical model. Its statistical properties such as unbiasedness and\nasymptotic normality are proved. In the simulation study and real data study,\nmagnetic resonance image data is used as input signal, which becomes sparse\nafter sparsified transformation. The results from the simulation study confirm\nthe theoretical properties of the estimator. In practice, the estimate from a\nreal MR image can be used for recovering future MR images under the framework\nof CS if they are believed to have the same sparsity level after\nsparsification.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:13:42 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Wang", "Jianfeng", ""], ["Zhou", "Zhiyong", ""], ["Garpebring", "Anders", ""], ["Yu", "Jun", ""]]}, {"id": "1710.04105", "submitter": "Yetkin Tua\\c{c}", "authors": "Yetkin Tua\\c{c} and Olcay Arslan", "title": "Variable Selection in Restricted Linear Regression Models", "comments": "12 pages, 4 figures 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of prior information in the linear regression is well known to\nprovide more efficient estimators of regression coefficients. The methods of\nnon-stochastic restricted regression estimation proposed by Theil and\nGoldberger (1961) are preferred when prior information is available. In this\nstudy, we will consider parameter estimation and the variable selection in\nnon-stochastic restricted linear regression model, using least absolute\nshrinkage and selection operator (LASSO) method introduced by Tibshirani\n(1996). A small simulation study and real data example are provided to\nillustrate the performance of the proposed method for dealing with the variable\nselection and the parameter estimation in restricted linear regression models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:10:06 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Tua\u00e7", "Yetkin", ""], ["Arslan", "Olcay", ""]]}, {"id": "1710.04234", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, Toby Dylan Hocking, Fran\\c{c}ois Laviolette", "title": "Maximum Margin Interval Trees", "comments": "Accepted for presentation at the 31st Conference on Neural\n  Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a regression function using censored or interval-valued output data\nis an important problem in fields such as genomics and medicine. The goal is to\nlearn a real-valued prediction function, and the training output labels\nindicate an interval of possible values. Whereas most existing algorithms for\nthis task are linear models, in this paper we investigate learning nonlinear\ntree models. We propose to learn a tree by minimizing a margin-based\ndiscriminative objective function, and we provide a dynamic programming\nalgorithm for computing the optimal solution in log-linear time. We show\nempirically that this algorithm achieves state-of-the-art speed and prediction\naccuracy in a benchmark of several data sets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 18:02:38 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 16:48:57 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Drouin", "Alexandre", ""], ["Hocking", "Toby Dylan", ""], ["Laviolette", "Fran\u00e7ois", ""]]}, {"id": "1710.04484", "submitter": "Colleen Farrelly", "authors": "Colleen M. Farrelly", "title": "Dimensionality Reduction Ensembles", "comments": "12 pages, 1 table, 8 figures; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning has had many successes in supervised learning, but it has\nbeen rare in unsupervised learning and dimensionality reduction. This study\nexplores dimensionality reduction ensembles, using principal component analysis\nand manifold learning techniques to capture linear, nonlinear, local, and\nglobal features in the original dataset. Dimensionality reduction ensembles are\ntested first on simulation data and then on two real medical datasets using\nrandom forest classifiers; results suggest the efficacy of this approach, with\naccuracies approaching that of the full dataset. Limitations include\ncomputational cost of some algorithms with strong performance, which may be\nameliorated through distributed computing and the development of more efficient\nversions of these algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:38:47 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Farrelly", "Colleen M.", ""]]}, {"id": "1710.04687", "submitter": "Susan Cheng", "authors": "Mir Henglin, Teemu Niiranen, Jeramie D. Watrous, Kim A. Lehmann,\n  Joseph Antonelli, Brian L. Claggett, Emmanuella J. Demosthenes, Beatrice von\n  Jeinsen, Olga Demler, Ramachandran S. Vasan, Martin G. Larson, Mohit Jain,\n  Susan Cheng", "title": "A Single Visualization Technique for Displaying Multiple\n  Metabolite-Phenotype Associations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More advanced visualization tools are needed to assist with the analyses and\ninterpretation of human metabolomics data, which are rapidly increasing in\nquantity and complexity. Using a dataset of several hundred bioactive lipid\nmetabolites profiled in a cohort of over 1400 individuals sampled from a\npopulation-based community study, we performed a comprehensive set of\nassociation analyses relating all metabolites with eight demographic and\ncardiometabolic traits and outcomes. We then compared existing graphical\napproaches with an adapted rain plot approach to display the results of these\nanalyses. The rain plot combines the features of a raindrop plot and a parallel\nheatmap approach to succinctly convey, in a single visualization, the results\nof relating complex metabolomics data with multiple phenotypes. This approach\ncomplements existing tools, particularly by facilitating comparisons between\nindividual metabolites and across a range of pre-specified clinical outcomes.\nWe anticipate that this single visualization technique may be further extended\nand applied to alternate study designs using different types of molecular\nphenotyping data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 18:54:18 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Henglin", "Mir", ""], ["Niiranen", "Teemu", ""], ["Watrous", "Jeramie D.", ""], ["Lehmann", "Kim A.", ""], ["Antonelli", "Joseph", ""], ["Claggett", "Brian L.", ""], ["Demosthenes", "Emmanuella J.", ""], ["von Jeinsen", "Beatrice", ""], ["Demler", "Olga", ""], ["Vasan", "Ramachandran S.", ""], ["Larson", "Martin G.", ""], ["Jain", "Mohit", ""], ["Cheng", "Susan", ""]]}, {"id": "1710.04749", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman", "title": "Explaining Aviation Safety Incidents Using Deep Temporal Multiple\n  Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although aviation accidents are rare, safety incidents occur more frequently\nand require a careful analysis to detect and mitigate risks in a timely manner.\nAnalyzing safety incidents using operational data and producing event-based\nexplanations is invaluable to airline companies as well as to governing\norganizations such as the Federal Aviation Administration (FAA) in the United\nStates. However, this task is challenging because of the complexity involved in\nmining multi-dimensional heterogeneous time series data, the lack of\ntime-step-wise annotation of events in a flight, and the lack of scalable tools\nto perform analysis over a large number of events. In this work, we propose a\nprecursor mining algorithm that identifies events in the multidimensional time\nseries that are correlated with the safety incident. Precursors are valuable to\nsystems health and safety monitoring and in explaining and forecasting safety\nincidents. Current methods suffer from poor scalability to high dimensional\ntime series data and are inefficient in capturing temporal behavior. We propose\nan approach by combining multiple-instance learning (MIL) and deep recurrent\nneural networks (DRNN) to take advantage of MIL's ability to learn using weakly\nsupervised data and DRNN's ability to model temporal behavior. We describe the\nalgorithm, the data, the intuition behind taking a MIL approach, and a\ncomparative analysis of the proposed algorithm with baseline models. We also\ndiscuss the application to a real-world aviation safety problem using data from\na commercial airline company and discuss the model's abilities and\nshortcomings, with some final remarks about possible deployment directions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 23:42:00 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 05:16:08 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""]]}, {"id": "1710.05008", "submitter": "Justin Strait", "authors": "Justin Strait, Oksana Chkrebtii, Sebastian Kurtek", "title": "Automatic Detection and Uncertainty Quantification of Landmarks on\n  Elastic Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population quantity of interest in statistical shape analysis is the\nlocation of landmarks, which are points that aid in reconstructing and\nrepresenting shapes of objects. We provide an automated, model-based approach\nto inferring landmarks given a sample of shape data. The model is formulated\nbased on a linear reconstruction of the shape, passing through the specified\npoints, and a Bayesian inferential approach is described for estimating unknown\nlandmark locations. The question of how many landmarks to select is addressed\nin two different ways: (1) by defining a criterion-based approach, and (2)\njoint estimation of the number of landmarks along with their locations.\nEfficient methods for posterior sampling are also discussed. We motivate our\napproach using several simulated examples, as well as data obtained from\napplications in computer vision and biology; additionally, we explore\nplacements and associated uncertainty in landmarks for various substructures\nextracted from magnetic resonance image slices.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:13:24 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Strait", "Justin", ""], ["Chkrebtii", "Oksana", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1710.05284", "submitter": "Andrew Karl", "authors": "Jennifer E. Broatch, Andrew T. Karl", "title": "Multivariate Generalized Linear Mixed Models for Joint Estimation of\n  Sporting Outcomes", "comments": "To appear in the Italian Journal of Applied Statistics", "journal-ref": "Italian Journal of Applied Statistics (2018), Vol.30, No.2,\n  pp189-211", "doi": "10.26398/IJAS.0030-008", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores improvements in prediction accuracy and inference\ncapability when allowing for potential correlation in team-level random effects\nacross multiple game-level responses from different assumed distributions.\nFirst-order and fully exponential Laplace approximations are used to fit\nnormal-binary and Poisson-binary multivariate generalized linear mixed models\nwith non-nested random effects structures. We have built these models into the\nR package mvglmmRank, which is used to explore several seasons of American\ncollege football and basketball data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 06:40:00 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Broatch", "Jennifer E.", ""], ["Karl", "Andrew T.", ""]]}, {"id": "1710.05394", "submitter": "Dileep Kalathil", "authors": "Shahana Ibrahim, Dileep Kalathil, Rene O. Sanchez and Pravin Varaiya", "title": "Estimating Phase Duration for SPaT Messages", "comments": "9 Pages, 13 Figures, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A SPaT (Signal Phase and Timing) message describes for each lane the current\nphase at a signalized intersection together with an estimate of the residual\ntime of that phase. Accurate SPaT messages can be used to construct a speed\nprofile for a vehicle that reduces its fuel consumption as it approaches or\nleaves an intersection. This paper presents SPaT estimation algorithms at an\nintersection with a semi-actuated signal, using real-time signal phase\nmeasurements. The algorithms are evaluated using high-resolution data from two\nintersections in Montgomery County, MD. The algorithms can be readily\nimplemented at signal controllers. The study supports three findings. First,\nreal-time information dramatically improves the accuracy of the prediction of\nthe residual time compared with prediction based on historical data alone.\nSecond, as time increases the prediction of the residual time may increase or\ndecrease. Third, as drivers differently weight errors in predicting `end of\ngreen' and `end of red', drivers on two different approaches may prefer\ndifferent estimates of the residual time of the same phase.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 20:15:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 20:54:43 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Ibrahim", "Shahana", ""], ["Kalathil", "Dileep", ""], ["Sanchez", "Rene O.", ""], ["Varaiya", "Pravin", ""]]}, {"id": "1710.05406", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli", "title": "Variable selection for (realistic) stochastic blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic blockmodels provide a convenient representation of relations\nbetween communities of nodes in a network. However, they imply a notion of\nstochastic equivalence that is often unrealistic for real networks, and they\ncomprise large number of parameters that can make them hardly interpretable. We\ndiscuss two extensions of stochastic blockmodels, and a recently proposed\nvariable selection approach based on penalized inference, which allows to infer\na sparse reduced graph summarizing relations between communities. We compare\nthis approach with maximum likelihood estimation on two datasets on\nface-to-face interactions in a French primary school and on bill cosponsorships\nin the Italian Parliament.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 21:43:46 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Signorelli", "Mirko", ""]]}, {"id": "1710.05513", "submitter": "Ziping Zhao", "authors": "Ziping Zhao and Daniel P. Palomar", "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction\n  Model", "comments": "5 pages, 3 figures, to appear in Proc. of the 2017 5th IEEE Global\n  Conference on Signal and Information Processing (GlobalSIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econometrics and finance, the vector error correction model (VECM) is an\nimportant time series model for cointegration analysis, which is used to\nestimate the long-run equilibrium variable relationships. The traditional\nanalysis and estimation methodologies assume the underlying Gaussian\ndistribution but, in practice, heavy-tailed data and outliers can lead to the\ninapplicability of these methods. In this paper, we propose a robust model\nestimation method based on the Cauchy distribution to tackle this issue. In\naddition, sparse cointegration relations are considered to realize feature\nselection and dimension reduction. An efficient algorithm based on the\nmajorization-minimization (MM) method is applied to solve the proposed\nnonconvex problem. The performance of this algorithm is shown through numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 05:38:27 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhao", "Ziping", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1710.05878", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison", "title": "A response to: \"NIST experts urge caution in use of courtroom evidence\n  presentation method\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A press release from the National Institute of Standards and Technology\n(NIST)could potentially impede progress toward improving the analysis of\nforensic evidence and the presentation of forensic analysis results in courts\nin the United States and around the world. \"NIST experts urge caution in use of\ncourtroom evidence presentation method\" was released on October 12, 2017, and\nwas picked up by the phys.org news service. It argues that, except in\nexceptional cases, the results of forensic analyses should not be reported as\n\"likelihood ratios\". The press release, and the journal article by NIST\nresearchers Steven P. Lund & Harri Iyer on which it is based, identifies some\nlegitimate points of concern, but makes a strawman argument and reaches an\nunjustified conclusion that throws the baby out with the bathwater.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 17:18:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""]]}, {"id": "1710.05963", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron, Sophie Dede", "title": "Asymptotic distribution of least squares estimators for linear models\n  with dependent errors : regular designs", "comments": "31 pages", "journal-ref": "Mathematical Methods of Statistics, 2018, Vol. 27, No. 4, pp.\n  268-293", "doi": "10.3103/S1066530718040026", "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the usual linear regression model in the case\nwhere the error process is assumed strictly stationary. We use a result from\nHannan, who proved a Central Limit Theorem for the usual least squares\nestimator under general conditions on the design and on the error process. We\nshow that for a large class of designs, the asymptotic covariance matrix is as\nsimple as the independent and identically distributed case. We then estimate\nthe covariance matrix using an estimator of the spectral density whose\nconsistency is proved under very mild conditions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 19:25:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 13:46:30 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 16:35:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dede", "Sophie", ""]]}, {"id": "1710.06047", "submitter": "Justin Silverman", "authors": "Justin D. Silverman and Rachel K. Silverman", "title": "The Bayesian Sorting Hat: A Decision-Theoretic Approach to\n  Size-Constrained Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Size-constrained clustering (SCC) refers to the dual problem of using\nobservations to determine latent cluster structure while at the same time\nassigning observations to the unknown clusters subject to an analyst defined\nconstraint on cluster sizes. While several approaches have been proposed, SCC\nremains a difficult problem due to the combinatorial dependency between\nobservations introduced by the size-constraints. Here we reformulate SCC as a\ndecision problem and introduce a novel loss function to capture various types\nof size constraints. As opposed to prior work, our approach is uniquely suited\nto situations in which size constraints reflect and external limitation or\ndesire rather than an internal feature of the data generation process. To\ndemonstrate our approach, we develop a Bayesian mixture model for clustering\nrespondents using both simulated and real categorical survey data. Our\nmotivation for the development of this decision theoretic approach to SCC was\nto determine optimal team assignments for a Harry Potter themed scavenger hunt\nbased on categorical survey data from participants.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 01:32:28 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Silverman", "Justin D.", ""], ["Silverman", "Rachel K.", ""]]}, {"id": "1710.06366", "submitter": "Vinny Davies", "authors": "Vinny Davies, William T. Harvey, Richard Reeve and Dirk Husmeier", "title": "Improving the identification of antigenic sites in the H1N1 Influenza\n  virus through accounting for the experimental structure in a sparse\n  hierarchical Bayesian model", "comments": null, "journal-ref": "J. R. Stat. Soc. C (2019)", "doi": "10.1111/rssc.12338", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how genetic changes allow emerging virus strains to escape the\nprotection afforded by vaccination is vital for the maintenance of effective\nvaccines. In the current work, we use structural and phylogenetic differences\nbetween pairs of virus strains to identify important antigenic sites on the\nsurface of the influenza A(H1N1) virus through the prediction of\nhaemagglutination inhibition (HI) assay, pairwise measures of the antigenic\nsimilarity of virus strains. We propose a sparse hierarchical Bayesian model\nthat can deal with the pairwise structure and inherent experimental variability\nin the H1N1 data through the introduction of latent variables. The latent\nvariables represent the underlying HI assay measurement of any given pair of\nvirus strains and help account for the fact that for any HI assay measurement\nbetween the same pair of virus strains, the difference in the viral sequence\nremains the same. Through accurately representing the structure of the H1N1\ndata, the model is able to select virus sites which are antigenic, while its\nlatent structure achieves the computational efficiency required to deal with\nlarge virus sequence data, as typically available for the influenza virus. In\naddition to the latent variable model, we also propose a new method, block\nintegrated Widely Applicable Information Criterion (biWAIC), for selecting\nbetween competing models. We show how this allows us to effectively select the\nrandom effects when used with the proposed model and apply both methods to an\nA(H1N1) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 10:43:36 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Davies", "Vinny", ""], ["Harvey", "William T.", ""], ["Reeve", "Richard", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1710.06551", "submitter": "Erik Schlicht", "authors": "Erik J. Schlicht", "title": "Exploiting oddsmaker bias to improve the prediction of NFL outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting the outcome of sporting events has been a goal for many\ngroups who seek to maximize profit. What makes this challenging is that the\noutcome of an event can be influenced by many factors that dynamically change\nacross time. Oddsmakers attempt to estimate these factors by using both\nalgorithmic and subjective methods to set the spread. However, it is well-known\nthat both human and algorithmic decision-making can be biased, so this paper\nexplores if oddsmaker biases can be used in an exploitative manner, in order to\nimprove the prediction of NFL game outcomes. Real-world gambling data was used\nto train and test different predictive models under varying assumptions. The\nresults show that methods that leverage oddsmaker biases in an exploitative\nmanner perform best under the conditions tested in this paper. These findings\nsuggest that leveraging human and algorithmic decision biases in an\nexploitative manner may be useful for predicting the outcomes of competitive\nevents, and could lead to increased profit for those who have financial\ninterest in the outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 01:41:11 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 23:37:32 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Schlicht", "Erik J.", ""]]}, {"id": "1710.06611", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli and Luisa Cutillo", "title": "On community structure validation in real networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community structure is a commonly observed feature of real networks. The term\nrefers to the presence in a network of groups of nodes (communities) that\nfeature high internal connectivity, but are poorly connected between each\nother. Whereas the issue of community detection has been addressed in several\nworks, the problem of validating a partition of nodes as a good community\nstructure for a real network has received considerably less attention and\nremains an open issue. We propose a set of indices for community structure\nvalidation of network partitions, which rely on concepts from network\nenrichment analysis. The proposed indices allow to compare the adequacy of\ndifferent partitions of nodes as community structures. Moreover, they can be\nemployed to assess whether two networks share the same or similar community\nstructures, and to evaluate the performance of different network clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:11:37 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 11:16:48 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 02:02:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Signorelli", "Mirko", ""], ["Cutillo", "Luisa", ""]]}, {"id": "1710.06613", "submitter": "Torstein Fjeldstad", "authors": "Torstein Fjeldstad and Henning Omre", "title": "Bayesian inversion of convolved hidden Markov models with applications\n  in reservoir prediction", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient assessment of convolved hidden Markov models is discussed. The\nbottom-layer is defined as an unobservable categorical first-order Markov\nchain, while the middle-layer is assumed to be a Gaussian spatial variable\nconditional on the bottom-layer. Hence, this layer appear as a Gaussian mixture\nspatial variable unconditionally. We observe the top-layer as a convolution of\nthe middle-layer with Gaussian errors. Focus is on assessment of the\ncategorical and Gaussian mixture variables given the observations, and we\noperate in a Bayesian inversion framework. The model is defined to make\ninversion of subsurface seismic AVO data into lithology/fluid classes and to\nassess the associated elastic material properties. Due to the spatial coupling\nin the likelihood functions, evaluation of the posterior normalizing constant\nis computationally demanding, and brute-force, single-site updating Markov\nchain Monte Carlo algorithms converges far too slow to be useful. We construct\ntwo classes of approximate posterior models which we assess analytically and\nefficiently using the recursive Forward-Backward algorithm. These approximate\nposterior densities are used as proposal densities in an independent proposal\nMarkov chain Monte Carlo algorithm, to assess the correct posterior model. A\nset of synthetic realistic examples are presented. The proposed approximations\nprovides efficient proposal densities which results in acceptance probabilities\nin the range 0.10-0.50 in the Markov chain Monte Carlo algorithm. A case study\nof lithology/fluid seismic inversion is presented. The lithology/fluid classes\nand the elastic material properties can be reliably predicted.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:16:35 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Fjeldstad", "Torstein", ""], ["Omre", "Henning", ""]]}, {"id": "1710.06618", "submitter": "Nil Kamal Hazra", "authors": "M. Finkelstein, N.K. Hazra, and J.H. Cha", "title": "On Optimal Operational Sequence of Components in a Warm Standby System", "comments": "The proof of one of the theorems is erroneous. Apart from this, there\n  are some other technical issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an open problem of optimal operational sequence for the\n$1$-out-of-$n$ system with warm standby. Using the virtual age concept and the\ncumulative exposure model, we show that the components should be activated in\naccordance with the increasing sequence of their lifetimes. Lifetimes of the\ncomponents and the system are compared with respect to the stochastic\nprecedence order. Only specific cases of this optimal problem were considered\nin the literature previously.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:31:58 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 04:00:51 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Finkelstein", "M.", ""], ["Hazra", "N. K.", ""], ["Cha", "J. H.", ""]]}, {"id": "1710.06891", "submitter": "Iavor Bojinov", "authors": "Iavor Bojinov, Natesh Pillai and Donald Rubin", "title": "Diagnosing missing always at random in multivariate data", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asz061", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for analyzing multivariate data sets with missing values require\nstrong, often unassessable, assumptions. The most common of these is that the\nmechanism that created the missing data is ignorable - a twofold assumption\ndependent on the mode of inference. The first part, which is the focus here,\nunder the Bayesian and direct-likelihood paradigms, requires that the missing\ndata are missing at random; in contrast, the frequentist-likelihood paradigm\ndemands that the missing data mechanism always produces missing at random data,\na condition known as missing always at random. Under certain regularity\nconditions, assuming missing always at random leads to an assumption that can\nbe tested using the observed data alone namely, the missing data indicators\nonly depend on fully observed variables. Here, we propose three different\ndiagnostic tests that not only indicate when this assumption is incorrect but\nalso suggest which variables are the most likely culprits. Although missing\nalways at random is not a necessary condition to ensure validity under the\nBayesian and direct-likelihood paradigms, it is sufficient, and evidence for\nits violation should encourage the careful statistician to conduct targeted\nsensitivity analyses.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 18:40:24 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:45:10 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 12:54:28 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bojinov", "Iavor", ""], ["Pillai", "Natesh", ""], ["Rubin", "Donald", ""]]}, {"id": "1710.07066", "submitter": "Filippo Elba", "authors": "Filippo Elba, Lisa Gnaulati, Fabio Voeller", "title": "Reti bayesiane per lo studio del fenomeno degli incidenti stradali tra i\n  giovani in Toscana", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to analyse adolescents' road accidents in Tuscany. The\nanalysis is based on the Database Edit of Osservatorio di Epidemiologia della\nToscana. Complexity and heterogeneity of Edit's data represet an interesting\nscope to apply Machine Learning methods. In particular, in this paper is\nproposed an analysis based on a Bayesian probabilistic network, used to\ndiscover relationships between adolescents' characteristics and behaviours that\nare more often associated with an audacious driving style. The probabilistic\nnetwork developed by this study can be considered a useful starting point for\nfollow up reasearches, aiming to develop a causal network, a tool to limit this\nphenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 10:19:54 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Elba", "Filippo", ""], ["Gnaulati", "Lisa", ""], ["Voeller", "Fabio", ""]]}, {"id": "1710.07186", "submitter": "Saed Khawaldeh", "authors": "Vu Hoang Minh, Tajwar Abrar Aleef, Usama Pervaiz, Yeman Brhane Hagos,\n  Saed Khawaldeh", "title": "A Universal Simulation Platform for Flexible Systems", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a universal simulation platform for simulating systems\nundergoing duress. In other words, this paper introduces a total simulation\npackage which includes a number of methods of simulating the flexibility of a\ngiven system. This platform includes detailed procedures for simulating a\nflexible link by a numerical method called finite difference method. In order\nto verify the effectiveness of the proposed process, two examples are covered\nin different situations to discuss the importance of boundary control and mesh\nselection in the way of ensuring the stability of the system. In addition, a\ngraphical user interface (GUI) application called the SimuFlex is designed\nhaving a selection of methods that the user can choose along with the\nparameters of the controllers that can be easily manipulated from the GUI.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 22:04:50 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Minh", "Vu Hoang", ""], ["Aleef", "Tajwar Abrar", ""], ["Pervaiz", "Usama", ""], ["Hagos", "Yeman Brhane", ""], ["Khawaldeh", "Saed", ""]]}, {"id": "1710.07679", "submitter": "Majnu John", "authors": "Aparna John, Toshikazu Ikuta, Janina D Ferbinteanu, Majnu John", "title": "Nonparametrically estimating dynamic bivariate correlation using\n  visibility graph algorithm", "comments": null, "journal-ref": null, "doi": "10.3390/e22060617", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic conditional correlation (DCC) is a method that estimates the\ncorrelation between two time series across time. Although used primarily in\nfinance so far, DCC has been proposed recently as a model-based estimation\nmethod for quantifying functional connectivity during fMRI experiments. DCC\ncould also be used to estimate the dynamic correlation between other types of\ntime series such as local field potentials (LFP's) or spike trains recorded\nfrom distinct brain areas. DCC has very nice properties compared to other\nexisting methods, but its applications for neuroscience are currently limited\nbecause of non-optimal performance in the presence of outliers. To address this\nissue, we developed a robust nonparametric version of DCC, based on an\nadaptation of the weighted visibility graph algorithm which converts a time\nseries into a weighted graph. The modified DCC demonstrated better performance\nin the analysis of empirical data sets: one fMRI data set collected from a\nhuman subject performing a Stroop task; and one LFP data set recorded from an\nawake rat in resting state. Nonparametric DCC has the potential of enlarging\nthe spectrum of analytical tools designed to assess the dynamic coupling and\nuncoupling of activity among brain areas.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 19:23:38 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["John", "Aparna", ""], ["Ikuta", "Toshikazu", ""], ["Ferbinteanu", "Janina D", ""], ["John", "Majnu", ""]]}, {"id": "1710.07763", "submitter": "Kunjin Chen", "authors": "Kunjin Chen, Kunlong Chen", "title": "Modeling the Annual Growth Rate of Electricity Consumption of China in\n  the 21st Century: Trends and Prediction", "comments": "2016 US-China Green Energy Summit on Developing Energy Internets to\n  Combat Climate Change", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the annual growth rate of electricity consumption in China in\nthe first 15 years of the 21st century is modeled using multiple linear\nregression. Historical data and trends of gross domestic product, fixed assets\ninvestment and share of the secondary sector in China's GDP are used to account\nfor the observed trend and fluctuation of electricity consumption in recent\nyears. A comparison between the proposed method and the predictions given by\nChina Electricity Council is performed, showing that the proposed method is\ngood at capturing the trend of the electricity consumption of China under\ncomplex social and economic background. The electricity consumptions for 2016\nand 2017 are also predicted, which is helpful for energy planners and\npolicymakers for future challenges.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 06:41:53 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chen", "Kunjin", ""], ["Chen", "Kunlong", ""]]}, {"id": "1710.07880", "submitter": "Anastasios Matzavinos", "authors": "Karen Larson, Clark Bowman, Zhizhong Chen, Panagiotis Hadjidoukas,\n  Costas Papadimitriou, Petros Koumoutsakos, Anastasios Matzavinos", "title": "Data-driven prediction and origin identification of epidemics in\n  population networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective intervention strategies for epidemics rely on the identification of\ntheir origin and on the robustness of the predictions made by network disease\nmodels. We introduce a Bayesian uncertainty quantification framework to infer\nmodel parameters for a disease spreading on a network of communities from\nlimited, noisy observations; the state-of-the-art computational framework\ncompensates for the model complexity by exploiting massively parallel computing\narchitectures. Using noisy, synthetic data, we show the potential of the\napproach to perform robust model fitting and additionally demonstrate that we\ncan effectively identify the disease origin via Bayesian model selection. As\ndisease-related data are increasingly available, the proposed framework has\nbroad practical relevance for the prediction and management of epidemics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 03:24:20 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 04:53:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Larson", "Karen", ""], ["Bowman", "Clark", ""], ["Chen", "Zhizhong", ""], ["Hadjidoukas", "Panagiotis", ""], ["Papadimitriou", "Costas", ""], ["Koumoutsakos", "Petros", ""], ["Matzavinos", "Anastasios", ""]]}, {"id": "1710.07978", "submitter": "Andrea Arnold", "authors": "Andrea Arnold, Alun L. Lloyd", "title": "An approach to periodic, time-varying parameter estimation using\n  nonlinear filtering", "comments": "24 pages, 11 figures", "journal-ref": "Inverse Problems 34 (2018) 105005", "doi": "10.1088/1361-6420/aad3e0", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems arising in biological applications are subject to periodic\nforcing. In these systems the forcing parameter is not only time-varying but\nalso known to have a periodic structure. We present an approach to estimating\nperiodic, time-varying parameters that imposes periodic structure by treating\nthe time-varying parameter as a piecewise function with unknown coefficients.\nThis method allows the resulting parameter estimate more flexibility in shape\nthan prescribing a specific functional form (e.g., sinusoidal) to model its\nbehavior, while still maintaining periodicity. We employ nonlinear filtering,\nmore specifically, a version of the augmented ensemble Kalman filter (EnKF), to\nestimate the unknown coefficients comprising the piecewise approximation of the\nperiodic, time-varying parameter. This allows for straightforward comparison of\nthe proposed method with an EnKF-based parameter tracking algorithm, where\nperiodicity is not guaranteed.\n  We demonstrate the effectiveness of the proposed approach on two biological\nexamples: a synthetic example with data generated from the nonlinear\nFitzHugh-Nagumo system, modeling the excitability of a nerve cell, to estimate\nthe external voltage parameter, and a case study using reported measles\nincidence data from three locations during the pre-vaccine era to estimate the\nseasonal transmission parameter. The formulation of the proposed approach also\nallows for simultaneous estimation of initial conditions and other static\nsystem parameters, such as the reporting probability of measles cases, which is\nvital for predicting under-reported incidence data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 17:07:14 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 22:38:22 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Arnold", "Andrea", ""], ["Lloyd", "Alun L.", ""]]}, {"id": "1710.08076", "submitter": "Eitan Levin", "authors": "Eitan Levin, Tamir Bendory, Nicolas Boumal, Joe Kileel and Amit Singer", "title": "3D ab initio modeling in cryo-EM by autocorrelation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-Particle Reconstruction (SPR) in Cryo-Electron Microscopy (cryo-EM) is\nthe task of estimating the 3D structure of a molecule from a set of noisy 2D\nprojections, taken from unknown viewing directions. Many algorithms for SPR\nstart from an initial reference molecule, and alternate between refining the\nestimated viewing angles given the molecule, and refining the molecule given\nthe viewing angles. This scheme is called iterative refinement. Reliance on an\ninitial, user-chosen reference introduces model bias, and poor initialization\ncan lead to slow convergence. Furthermore, since no ground truth is available\nfor an unsolved molecule, it is difficult to validate the obtained results.\nThis creates the need for high quality ab initio models that can be quickly\nobtained from experimental data with minimal priors, and which can also be used\nfor validation. We propose a procedure to obtain such an ab initio model\ndirectly from raw data using Kam's autocorrelation method. Kam's method has\nbeen known since 1980, but it leads to an underdetermined system, with missing\northogonal matrices. Until now, this system has been solved only for special\ncases, such as highly symmetric molecules or molecules for which a homologous\nstructure was already available. In this paper, we show that knowledge of just\ntwo clean projections is sufficient to guarantee a unique solution to the\nsystem. This system is solved by an optimization-based heuristic. For the first\ntime, we are then able to obtain a low-resolution ab initio model of an\nasymmetric molecule directly from raw data, without 2D class averaging and\nwithout tilting. Numerical results are presented on both synthetic and\nexperimental data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 03:03:27 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 09:58:22 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Levin", "Eitan", ""], ["Bendory", "Tamir", ""], ["Boumal", "Nicolas", ""], ["Kileel", "Joe", ""], ["Singer", "Amit", ""]]}, {"id": "1710.08112", "submitter": "Augustin Touron", "authors": "Augustin Touron (UP11, EDF R\\&D)", "title": "Modeling rainfalls using a seasonal hidden markov model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reach the supply/demand balance, electricity providers need to\npredict the demand and production of electricity at different time scales. This\nimplies the need of modeling weather variables such as temperature, wind speed,\nsolar radiation and precipitation. This work is dedicated to a new daily\nrainfall generator at a single site. It is based on a seasonal hidden Markov\nmodel with mixtures of exponential distributions as emission laws. The\nparameters of the exponential distributions include a periodic component in\norder to account for the seasonal behaviour of rainfall. We show that under\nmild assumptions , the maximum likelihood estimator is strongly consistent,\nwhich is a new result for such models. The model is able to produce arbitrarily\nlong daily rainfall simulations that reproduce closely different features of\nobserved time series, including seasonality, rainfall occurrence , daily\ndistributions of rainfall, dry and rainy spells. The model was fitted and\nvalidated on data from several weather stations across Germany. We show that it\nis possible to give a physical interpretation to the estimated states.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 07:02:33 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Touron", "Augustin", "", "UP11, EDF R\\&D"]]}, {"id": "1710.08144", "submitter": "Rasmus Henningsson", "authors": "Rasmus Henningsson (1,2) and Magnus Fontes (1,2,3,4) ((1) The Centre\n  for Mathematical Sciences, Lund University, Sweden, (2) The International\n  Group for Data Analysis, Institut Pasteur, Paris, France, (3) The Center for\n  Genomic Medicine, Rigshospitalet, Copenhagen, Denmark, (4) Persimune, The\n  Centre of Excellence for Personalized Medicine, Copenhagen, Denmark)", "title": "SMSSVD - SubMatrix Selection Singular Value Decomposition", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput biomedical measurements normally capture multiple overlaid\nbiologically relevant signals and often also signals representing different\ntypes of technical artefacts like e.g. batch effects. Signal identification and\ndecomposition are accordingly main objectives in statistical biomedical\nmodeling and data analysis. Existing methods, aimed at signal reconstruction\nand deconvolution, in general, are either supervised, contain parameters that\nneed to be estimated or present other types of ad hoc features. We here\nintroduce SubMatrix Selection SingularValue Decomposition (SMSSVD), a\nparameter-free unsupervised signal decomposition and dimension reduction\nmethod, designed to reduce noise, adaptively for each low-rank-signal in a\ngiven data matrix, and represent the signals in the data in a way that enable\nunbiased exploratory analysis and reconstruction of multiple overlaid signals,\nincluding identifying groups of variables that drive different signals.\n  The Submatrix Selection Singular Value Decomposition (SMSSVD) method produces\na denoised signal decomposition from a given data matrix. The SMSSVD method\nguarantees orthogonality between signal components in a straightforward manner\nand it is designed to make automation possible. We illustrate SMSSVD by\napplying it to several real and synthetic datasets and compare its performance\nto golden standard methods like PCA (Principal Component Analysis) and SPC\n(Sparse Principal Components, using Lasso constraints). The SMSSVD is\ncomputationally efficient and despite being a parameter-free method, in\ngeneral, outperforms existing statistical learning methods.\n  A Julia implementation of SMSSVD is openly available on GitHub\n(https://github.com/rasmushenningsson/SMSSVD.jl).\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 08:35:12 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Henningsson", "Rasmus", ""], ["Fontes", "Magnus", ""]]}, {"id": "1710.08171", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "A hierarchical Bayesian model for measuring individual-level and\n  group-level numerical representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A popular method for indexing numerical representations is to compute an\nindividual estimate of a response time effect, such as the SNARC effect or the\nnumerical distance effect. Classically, this is done by estimating individual\nlinear regression slopes and then either pooling the slopes to obtain a\ngroup-level slope estimate, or using the individual slopes as predictors of\nother phenomena. In this paper, I develop a hierarchical Bayesian model for\nsimultaneously estimating group-level and individual-level slope parameters. I\nshow examples of using this modeling framework to assess two common effects in\nnumerical cognition: the SNARC effect and the numerical distance effect.\nFinally, I demonstrate that the Bayesian approach can result in better\nmeasurement fidelity than the classical approach, especially with small\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 09:55:22 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1710.08266", "submitter": "Jean-Michel Loubes", "authors": "Thomas Epelbaum (IPHT), Fabrice Gamboa (IMT), Jean-Michel Loubes\n  (IMT), Jessica Martin", "title": "Deep Learning applied to Road Traffic Speed forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to\nforecast a regression model for time dependent data. These algorithm's are\ndesigned to handle Floating Car Data (FCD) historic speeds to predict road\ntraffic data. For this we aggregate the speeds into the network inputs in an\ninnovative way. We compare the RMSE thus obtained with the results of a simpler\nphysical model, and show that the latter achieves better RMSE accuracy. We also\npropose a new indicator, which evaluates the algorithms improvement when\ncompared to a benchmark prediction. We conclude by questioning the interest of\nusing deep learning methods for this specific regression task.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:27:38 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Epelbaum", "Thomas", "", "IPHT"], ["Gamboa", "Fabrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Martin", "Jessica", ""]]}, {"id": "1710.08269", "submitter": "Yin Song", "authors": "Yin Song, Farouk S. Nathoo, Arif Babul", "title": "A Potts-Mixture Spatiotemporal Joint Model for Combined MEG and EEG Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new methodology for determining the location and dynamics of\nbrain activity from combined magnetoencephalography (MEG) and\nelectroencephalography (EEG) data. The resulting inverse problem is ill-posed\nand is one of the most difficult problems in neuroimaging data analysis. In our\ndevelopment we propose a solution that combines the data from three different\nmodalities, MRI, MEG, and EEG, together. We propose a new Bayesian spatial\nfinite mixture model that builds on the mesostate-space model developed by\nDaunizeau and Friston (2007). Our new model incorporates two major extensions:\n(i) We combine EEG and MEG data together and formulate a joint model for\ndealing with the two modalities simultaneously; (ii) we incorporate the Potts\nmodel to represent the spatial dependence in an allocation process that\npartitions the cortical surface into a small number of latent states termed\nmesostates. The cortical surface is obtained from MRI. We formulate the new\nspatiotemporal model and derive an efficient procedure for simultaneous point\nestimation and model selection based on the iterated conditional modes\nalgorithm combined with local polynomial smoothing. The proposed method results\nin a novel estimator for the number of mixture components and is able to select\nactive brain regions which correspond to active variables in a high-dimensional\ndynamic linear model. The methodology is investigated using synthetic data and\nsimulation studies and then demonstrated on an application examining the neural\nresponse to the perception of scrambled faces. R software implementing the\nmethodology along with several sample datasets are available at the following\nGitHub repository https://github.com/v2south/PottsMix.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:03:53 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 05:34:56 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 03:47:36 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 18:58:14 GMT"}, {"version": "v5", "created": "Thu, 11 Oct 2018 23:19:31 GMT"}, {"version": "v6", "created": "Sat, 20 Oct 2018 02:08:56 GMT"}, {"version": "v7", "created": "Sun, 3 Mar 2019 03:54:24 GMT"}, {"version": "v8", "created": "Sat, 20 Jul 2019 21:38:26 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Song", "Yin", ""], ["Nathoo", "Farouk S.", ""], ["Babul", "Arif", ""]]}, {"id": "1710.08349", "submitter": "Roland Matsouaka", "authors": "Folefac Atem and Roland A. Matsouaka", "title": "Linear regression model with a randomly censored predictor:Estimation\n  procedures", "comments": "21 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider linear regression model estimation where the covariate of\ninterest is randomly censored. Under a non-informative censoring mechanism, one\nmay obtain valid estimates by deleting censored observations. However, this\ncomes at a cost of lost information and decreased efficiency, especially under\nheavy censoring. Other methods for dealing with censored covariates, such as\nignoring censoring or replacing censored observations with a fixed number,\noften lead to severely biased results and are of limited practicality.\nParametric methods based on maximum likelihood estimation as well as\nsemiparametric and non-parametric methods have been successfully used in linear\nregression estimation with censored covariates where censoring is due to a\nlimit of detection.\n  In this paper, we adapt some of these methods to handle randomly censored\ncovariates and compare them under different scenarios to recently-developed\nsemiparametric and nonparametric methods for randomly censored covariates.\nSpecifically, we consider both dependent and independent randomly censored\nmechanisms as well as the impact of using a non-parametric algorithm on the\ndistribution of the randomly censored covariate. Through extensive simulation\nstudies, we compare the performance of these methods under different scenarios.\nFinally, we illustrate and compare the methods using the Framingham Health\nStudy data to assess the association between low-density lipoprotein (LDL) in\noffspring and parental age at onset of a clinically-diagnosed cardiovascular\nevent.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 15:50:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Atem", "Folefac", ""], ["Matsouaka", "Roland A.", ""]]}, {"id": "1710.08412", "submitter": "Meng-Ta Chung", "authors": "Meng-Ta Chung and Matthew S. Johnson", "title": "An MCMC Algorithm for Estimating the Reduced RUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RRUM is a model that is frequently seen in language assessment studies.\nThe objective of this research is to advance an MCMC algorithm for the Bayesian\nRRUM. The algorithm starts with estimating correlated attributes. Using a\nsaturated model and a binary decimal conversion, the algorithm transforms\npossible attribute patterns to a Multinomial distribution. Along with the\nlikelihood of an attribute pattern, a Dirichlet distribution is used as the\nprior to sample from the posterior. The Dirichlet distribution is constructed\nusing Gamma distributions. Correlated attributes of examinees are generated\nusing the inverse transform sampling. Model parameters are estimated using the\nMetropolis within Gibbs sampler sequentially. Two simulation studies are\nconducted to evaluate the performance of the algorithm. The first simulation\nuses a complete and balanced Q-matrix that measures 5 attributes. Comprised of\n28 items and 9 attributes, the Q-matrix for the second simulation is incomplete\nand imbalanced. The empirical study uses the ECPE data obtained from the CDM R\npackage. Parameter estimates from the MCMC algorithm and from the CDM R package\nare presented and compared. The algorithm developed in this research is\nimplemented in R.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 16:26:52 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Chung", "Meng-Ta", ""], ["Johnson", "Matthew S.", ""]]}, {"id": "1710.08437", "submitter": "Pinchao Zhang", "authors": "Pinchao Zhang, Zhen (Sean) Qian", "title": "User-centric interdependent urban systems: using time-of-day electricity\n  usage data to predict morning roadway congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban systems are interdependent as individuals' daily activities engage\nusing those urban systems at certain time of day and locations. There may exist\nclear spatial and temporal correlations among usage patterns across all urban\nsystems. This paper explores such a correlation among energy usage and roadway\ncongestion. We propose a general framework to predict congestion starting time\nand congestion duration in the morning using the time-of-day electricity use\ndata from anonymous households with no personally identifiable information. We\nshow that using time-of-day electricity data from midnight to early morning\nfrom 322 households in the City of Austin, can make reliable prediction of\ncongestion starting time of several highway segments, at the time as early as\n2am. This predictor significantly outperforms a time-series predictor that uses\nonly real-time travel time data up to 6am. We found that 8 out of the 10\ntypical electricity use patterns have statistically significant affects on\nmorning congestion on highways in Austin. Some patterns have negative effects,\nrepresented by an early spike of electricity use followed by a drastic drop\nthat could imply early departure from home. Others have positive effects,\nrepresented by a late night spike of electricity use possible implying late\nnight activities that can lead to late morning departure from home.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 18:03:22 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Zhang", "Pinchao", "", "Sean"], ["Zhen", "", "", "Sean"], ["Qian", "", ""]]}, {"id": "1710.08508", "submitter": "Meng Li", "authors": "Meng Li and Armin Schwartzman", "title": "Standardization of multivariate Gaussian mixture models and background\n  adjustment of PET images in brain oncology", "comments": null, "journal-ref": null, "doi": "10.1214/18-AOAS1149", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain oncology, it is routine to evaluate the progress or remission of the\ndisease based on the differences between a pre-treatment and a post-treatment\nPositron Emission Tomography (PET) scan. Background adjustment is necessary to\nreduce confounding by tissue-dependent changes not related to the disease. When\nmodeling the voxel intensities for the two scans as a bivariate Gaussian\nmixture, background adjustment translates into standardizing the mixture at\neach voxel, while tumor lesions present themselves as outliers to be detected.\nIn this paper, we address the question of how to standardize the mixture to a\nstandard multivariate normal distribution, so that the outliers (i.e., tumor\nlesions) can be detected using a statistical test. We show theoretically and\nnumerically that the tail distribution of the standardized scores is favorably\nclose to standard normal in a wide range of scenarios while being conservative\nat the tails, validating voxelwise hypothesis testing based on standardized\nscores. To address standardization in spatially heterogeneous image data, we\npropose a spatial and robust multivariate expectation-maximization (EM)\nalgorithm, where prior class membership probabilities are provided by\ntransformation of spatial probability template maps and the estimation of the\nclass mean and covariances are robust to outliers. Simulations in both\nunivariate and bivariate cases suggest that standardized scores with soft\nassignment have tail probabilities that are either very close to or more\nconservative than standard normal. The proposed methods are applied to a real\ndata set from a PET phantom experiment, yet they are generic and can be used in\nother contexts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:13:50 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 19:40:27 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Meng", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1710.08511", "submitter": "Denisa Roberts", "authors": "Lucas Roberts and Denisa Roberts", "title": "An Expectation Maximization Framework for Yule-Simon Preferential\n  Attachment Models", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop an Expectation Maximization(EM) algorithm to\nestimate the parameter of a Yule-Simon distribution. The Yule-Simon\ndistribution exhibits the \"rich get richer\" effect whereby an 80-20 type of\nrule tends to dominate. These distributions are ubiquitous in industrial\nsettings. The EM algorithm presented provides both frequentist and Bayesian\nestimates of the $\\lambda$ parameter. By placing the estimation method within\nthe EM framework we are able to derive Standard errors of the resulting\nestimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and\nstudy the rate of convergence. An explicit, closed form solution for the rate\nof convergence of the algorithm is given. Applications including graph node\ndegree distribution estimation are listed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:33:31 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 23:11:55 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 14:10:43 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2020 20:09:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Roberts", "Lucas", ""], ["Roberts", "Denisa", ""]]}, {"id": "1710.08519", "submitter": "Hossein Seifoory", "authors": "Hossein Seifoory and Marc. M. Dignam", "title": "Squeezed state evolution and entanglement in lossy coupled resonator\n  optical waveguides", "comments": "10 pages, 6 figures", "journal-ref": "Phys. Rev. A 97, 023840 (2018)", "doi": "10.1103/PhysRevA.97.023840", "report-no": null, "categories": "quant-ph physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate theoretically the temporal evolution of a squeezed state in\nlossy coupled-cavity systems. We present a general formalism based upon the\ntight binding approximation and apply this to a two-cavity system as well as to\na coupled resonator optical waveguide in a photonic crystal. We derive\nanalytical expressions for the number of photons and the quadrature noise in\neach cavity as a function of time when the initial excited state is a squeezed\nstate in one of the cavities. We also analytically evaluate the time dependant\ncross correlation between the photons in different cavities to evaluate the\ndegree of quantum entanglement. We demonstrate the effects of loss on the\nproperties of the coupled-cavity systems and derive approximate analytic\nexpressions for the maximum photon number, maximum squeezing and maximum\nentanglement for cavities far from the initially excited cavity in a lossless\ncoupled resonator optical waveguide.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 22:00:01 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Seifoory", "Hossein", ""], ["Dignam", "Marc. M.", ""]]}, {"id": "1710.08553", "submitter": "Oscar Alberto Quijano Xacur", "authors": "Oscar Alberto Quijano Xacur and Jos\\'e Garrido", "title": "Bayesian Credibility for GLMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We revisit the classical credibility results of Jewell and B\\\"uhlmann to\nobtain credibility premiums for a GLM using a modern Bayesian approach. Here\nthe prior distributions can be chosen without restrictions to be conjugate to\nthe response distribution. It can even come from out-of-sample information if\nthe actuary prefers.\n  Then we use the relative entropy between the \"true\" and the estimated models\nas a loss function, without restricting credibility premiums to be linear. A\nnumerical illustration on real data shows the feasibility of the approach, now\nthat computing power is cheap, and simulations software readily available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 23:58:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 01:25:23 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 01:44:23 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Xacur", "Oscar Alberto Quijano", ""], ["Garrido", "Jos\u00e9", ""]]}, {"id": "1710.08561", "submitter": "Ildoo Kim", "authors": "Eunice J. Kim and Ildoo Kim", "title": "Approximate analytic solution of the potential flow around a rectangle", "comments": "v2 (year 2019, 12 pages, 2 figures) v1 (year 2015, 13 pages, 5\n  figures)", "journal-ref": "American Journal of Physics 88, pp. 25-30 (2020)", "doi": "10.1119/10.0000264", "report-no": null, "categories": "physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In undergraduate classes, the potential flow that goes around a circular\ncylinder is designed for complemental understanding of mathematical technique\nto handle the Laplace equation with Neumann boundary conditions and the\nphysical concept of the multipolar expansion. The simplicity of the standard\nproblem is suited for the introductory level, however, it has a drawback. The\ndiscussion of higher order multipoles is often missed because the exact\nanalytic solution contains only the dipole term. In this article, we present a\nmodified problem of the potential flow around a rectangle as an advanced\nproblem. Although the exact solution of this case is intractable, the\napproximate solution can be obtained by the discretization and the optimization\nusing multiple linear regression. The suggested problem is expected to deepen\nthe students' insight on the concept of multipoles and also provides an\nopportunity to discuss the formalism of the regression analysis, which in many\nphysics curricula is lacking even though it has a significant importance in\nexperimental physics.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 00:51:45 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 00:54:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kim", "Eunice J.", ""], ["Kim", "Ildoo", ""]]}, {"id": "1710.08583", "submitter": "Abdollah Safari", "authors": "Abdollah Safari, Rachel MacKay Altman and Thomas M. Loughin", "title": "Display advertising: Estimating conversion probability efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of online display advertising is to entice users to \"convert\" (i.e.,\ntake a pre-defined action such as making a purchase) after clicking on the ad.\nAn important measure of the value of an ad is the probability of conversion.\nThe focus of this paper is the development of a computationally efficient,\naccurate, and precise estimator of conversion probability. The challenges\nassociated with this estimation problem are the delays in observing conversions\nand the size of the data set (both number of observations and number of\npredictors). Two models have previously been considered as a basis for\nestimation: A logistic regression model and a joint model for observed\nconversion statuses and delay times. Fitting the former is simple, but ignoring\nthe delays in conversion leads to an under-estimate of conversion probability.\nOn the other hand, the latter is less biased but computationally expensive to\nfit. Our proposed estimator is a compromise between these two estimators. We\napply our results to a data set from Criteo, a commerce marketing company that\npersonalizes online display advertisements for users.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 02:46:23 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Safari", "Abdollah", ""], ["Altman", "Rachel MacKay", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "1710.08747", "submitter": "Yousra Bekhti", "authors": "Yousra Bekhti, Felix Lucka, Joseph Salmon, and Alexandre Gramfort", "title": "A hierarchical Bayesian perspective on majorization-minimization for\n  non-convex sparse regression: application to M/EEG source imaging", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aac9b3", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorization-minimization (MM) is a standard iterative optimization technique\nwhich consists in minimizing a sequence of convex surrogate functionals. MM\napproaches have been particularly successful to tackle inverse problems and\nstatistical machine learning problems where the regularization term is a\nsparsity-promoting concave function. However, due to non-convexity, the\nsolution found by MM depends on its initialization. Uniform initialization is\nthe most natural and often employed strategy as it boils down to penalizing all\ncoefficients equally in the first MM iteration. Yet, this arbitrary choice can\nlead to unsatisfactory results in severely under-determined inverse problems\nsuch as source imaging with magneto- and electro-encephalography (M/EEG). The\nframework of hierarchical Bayesian modeling (HBM) is an alternative approach to\nencode sparsity. This work shows that for certain hierarchical models, a simple\nalternating scheme to compute fully Bayesian maximum a posteriori (MAP)\nestimates leads to the exact same sequence of updates as a standard MM strategy\n(cf. the Adaptive Lasso). With this parallel outlined, we show how to improve\nupon these MM techniques by probing the multimodal posterior density using\nMarkov Chain Monte-Carlo (MCMC) techniques. Firstly, we show that these samples\ncan provide well-informed initializations that help MM schemes to reach better\nlocal minima. Secondly, we demonstrate how it can reveal the different modes of\nthe posterior distribution in order to explore and quantify the inherent\nuncertainty and ambiguity of such ill-posed inference procedure. In the context\nof M/EEG, each mode corresponds to a plausible configuration of neural sources,\nwhich is crucial for data interpretation, especially in clinical contexts.\nResults on both simulations and real datasets show how the number or the type\nof sensors affect the uncertainties on the estimates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 12:56:14 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 08:55:22 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 16:59:21 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Bekhti", "Yousra", ""], ["Lucka", "Felix", ""], ["Salmon", "Joseph", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1710.09009", "submitter": "Fabian Dunker", "authors": "Fabian Dunker, Stephan Klasen, Tatyana Krivobokova", "title": "Asymptotic Distribution and Simultaneous Confidence Bands for Ratios of\n  Quantile Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ratio of medians or other suitable quantiles of two distributions is widely\nused in medical research to compare treatment and control groups or in\neconomics to compare various economic variables when repeated cross-sectional\ndata are available. Inspired by the so-called growth incidence curves\nintroduced in poverty research, we argue that the ratio of quantile functions\nis a more appropriate and informative tool to compare two distributions. We\npresent an estimator for the ratio of quantile functions and develop\ncorresponding simultaneous confidence bands, which allow to assess significance\nof certain features of the quantile functions ratio. Derived simultaneous\nconfidence bands rely on the asymptotic distribution of the quantile functions\nratio and do not require re-sampling techniques. The performance of the\nsimultaneous confidence bands is demonstrated in simulations. Analysis of the\nexpenditure data from Uganda in years 1999, 2002 and 2005 illustrates the\nrelevance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:28:58 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Dunker", "Fabian", ""], ["Klasen", "Stephan", ""], ["Krivobokova", "Tatyana", ""]]}, {"id": "1710.09095", "submitter": "Sophie Achard", "authors": "Sophie Achard (1), Ir\\`ene Gannaz (2), Marianne Clausel, Fran\\c{c}ois\n  Roueff (3) ((1) GIPSA-CICS, (2) ICJ, (3) LTCI)", "title": "New results on approximate Hilbert pairs of wavelet filters with common\n  factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the design of wavelet filters based on the Thiran\ncommon-factor approach proposed in Selesnick [2001]. This approach aims at\nbuilding finite impulseresponse filters of a Hilbert-pair of wavelets serving\nas real and imaginary part of a complexwavelet. Unfortunately it is not\npossible to construct wavelets which are both finitelysupported and analytic.\nThe wavelet filters constructed using the common-factor approachare then\napproximately analytic. Thus, it is of interest to control their analyticity.\nThepurpose of this paper is to first provide precise and explicit expressions\nas well as easilyexploitable bounds for quantifying the analytic approximation\nof this complex wavelet.Then, we prove the existence of such filters enjoying\nthe classical perfect reconstructionconditions, with arbitrarily many vanishing\nmoments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 07:19:59 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Achard", "Sophie", "", "GIPSA-CICS"], ["Gannaz", "Ir\u00e8ne", "", "ICJ"], ["Clausel", "Marianne", "", "LTCI"], ["Roueff", "Fran\u00e7ois", "", "LTCI"]]}, {"id": "1710.09202", "submitter": "Nil Kamal Hazra", "authors": "Mithu Rani Kuiti, Nil Kamal Hazra and Maxim Finkelstein", "title": "On Component Redundancy Versus System Redundancy for a $k$-out-of-$n$\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precedence order is a natural type of comparison for random variables in\nnumerous engineering applications (e.g., for the stress-strength modeling). In\nthis note, we show that, for a $k$-out-of-$n$ system, redundancy at the\ncomponent level is superior to that at the system level with respect to the\nstochastic precedence order. Cases of active and cold redundancy are\nconsidered. Similar results for other stochastic orders were intensively\ndiscussed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 12:52:11 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Kuiti", "Mithu Rani", ""], ["Hazra", "Nil Kamal", ""], ["Finkelstein", "Maxim", ""]]}, {"id": "1710.09308", "submitter": "Frederik Vissing Mikkelsen", "authors": "Frederik Vissing Mikkelsen and Niels Richard Hansen", "title": "Learning Large Scale Ordinary Differential Equation Systems", "comments": "55 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning large scale nonlinear ordinary differential equation (ODE) systems\nfrom data is known to be computationally and statistically challenging. We\npresent a framework together with the adaptive integral matching (AIM)\nalgorithm for learning polynomial or rational ODE systems with a sparse network\nstructure. The framework allows for time course data sampled from multiple\nenvironments representing e.g. different interventions or perturbations of the\nsystem. The algorithm AIM combines an initial penalised integral matching step\nwith an adapted least squares step based on solving the ODE numerically. The R\npackage episode implements AIM together with several other algorithms and is\navailable from CRAN. It is shown that AIM achieves state-of-the-art network\nrecovery for the in silico phosphoprotein abundance data from the eighth DREAM\nchallenge with an AUROC of 0.74, and it is demonstrated via a range of\nnumerical examples that AIM has good statistical properties while being\ncomputationally feasible even for large systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 15:38:43 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 06:13:15 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Mikkelsen", "Frederik Vissing", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1710.09422", "submitter": "Jessie Jamieson", "authors": "Robert A. Bridges, Jessie D. Jamieson, and Joel W. Reed", "title": "Setting the threshold for high throughput detectors: A mathematical\n  approach for ensembles of dynamic, heterogeneous, probabilistic anomaly\n  detectors", "comments": "11 pages, 5 figures. Proceedings of IEEE Big Data Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection (AD) has garnered ample attention in security research, as\nsuch algorithms complement existing signature-based methods but promise\ndetection of never-before-seen attacks. Cyber operations manage a high volume\nof heterogeneous log data; hence, AD in such operations involves multiple\n(e.g., per IP, per data type) ensembles of detectors modeling heterogeneous\ncharacteristics (e.g., rate, size, type) often with adaptive online models\nproducing alerts in near real time. Because of high data volume, setting the\nthreshold for each detector in such a system is an essential yet underdeveloped\nconfiguration issue that, if slightly mistuned, can leave the system useless,\neither producing a myriad of alerts and flooding downstream systems, or giving\nnone. In this work, we build on the foundations of Ferragut et al. to provide a\nset of rigorous results for understanding the relationship between threshold\nvalues and alert quantities, and we propose an algorithm for setting the\nthreshold in practice. Specifically, we give an algorithm for setting the\nthreshold of multiple, heterogeneous, possibly dynamic detectors completely a\npriori, in principle. Indeed, if the underlying distribution of the incoming\ndata is known (closely estimated), the algorithm provides provably manageable\nthresholds. If the distribution is unknown (e.g., has changed over time) our\nanalysis reveals how the model distribution differs from the actual\ndistribution, indicating a period of model refitting is necessary. We provide\nempirical experiments showing the efficacy of the capability by regulating the\nalert rate of a system with $\\approx$2,500 adaptive detectors scoring over 1.5M\nevents in 5 hours. Further, we demonstrate on the real network data and\ndetection framework of Harshaw et al. the alternative case, showing how the\ninability to regulate alerts indicates the detection model is a bad fit to the\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 18:53:48 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Bridges", "Robert A.", ""], ["Jamieson", "Jessie D.", ""], ["Reed", "Joel W.", ""]]}, {"id": "1710.09791", "submitter": "Joakim And\\'en", "authors": "Joakim And\\'en and Amit Singer", "title": "Structural Variability from Noisy Tomographic Projections", "comments": "52 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cryo-electron microscopy, the 3D electric potentials of an ensemble of\nmolecules are projected along arbitrary viewing directions to yield noisy 2D\nimages. The volume maps representing these potentials typically exhibit a great\ndeal of structural variability, which is described by their 3D covariance\nmatrix. Typically, this covariance matrix is approximately low-rank and can be\nused to cluster the volumes or estimate the intrinsic geometry of the\nconformation space. We formulate the estimation of this covariance matrix as a\nlinear inverse problem, yielding a consistent least-squares estimator. For $n$\nimages of size $N$-by-$N$ pixels, we propose an algorithm for calculating this\ncovariance estimator with computational complexity\n$\\mathcal{O}(nN^4+\\sqrt{\\kappa}N^6 \\log N)$, where the condition number\n$\\kappa$ is empirically in the range $10$--$200$. Its efficiency relies on the\nobservation that the normal equations are equivalent to a deconvolution problem\nin 6D. This is then solved by the conjugate gradient method with an appropriate\ncirculant preconditioner. The result is the first computationally efficient\nalgorithm for consistent estimation of 3D covariance from noisy projections. It\nalso compares favorably in runtime with respect to previously proposed\nnon-consistent estimators. Motivated by the recent success of eigenvalue\nshrinkage procedures for high-dimensional covariance matrices, we introduce a\nshrinkage procedure that improves accuracy at lower signal-to-noise ratios. We\nevaluate our methods on simulated datasets and achieve classification results\ncomparable to state-of-the-art methods in shorter running time. We also present\nresults on clustering volumes in an experimental dataset, illustrating the\npower of the proposed algorithm for practical determination of structural\nvariability.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 15:59:05 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 18:46:59 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1710.09793", "submitter": "Timothy Coleman", "authors": "Tim Coleman, Lucas Mentch, Daniel Fink, Frank La Sorte, Giles Hooker,\n  Wesley Hochachka, David Winkler", "title": "Statistical Inference on Tree Swallow Migrations with Random Forests", "comments": "23 pages, 7 figures. Work between Cornell Lab of Ornithology and\n  University of Pittsburgh Department of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird species' migratory patterns have typically been studied through\nindividual observations and historical records. In recent years however, the\neBird citizen science project, which solicits observations from thousands of\nbird watchers around the world, has opened the door for a data-driven approach\nto understanding the large-scale geographical movements. Here, we focus on the\nNorth American Tree Swallow (\\textit{Tachycineta bicolor}) occurrence patterns\nthroughout the eastern United States. Migratory departure dates for this\nspecies are widely believed by both ornithologists and casual observers to vary\nsubstantially across years, but the reasons for this are largely unknown. In\nthis work, we present evidence that maximum daily temperature is a major factor\ninfluencing Tree Swallow occurrence. Because it is generally understood that\nspecies occurrence is a function of many complex, high-order interactions\nbetween ecological covariates, we utilize the flexible modeling approach\noffered by random forests. Making use of recent asymptotic results, we provide\nformal hypothesis tests for predictive significance various covariates and also\ndevelop and implement a permutation-based approach for formally assessing\ninterannual variations by treating the prediction surfaces generated by random\nforests as functional data. Each of these tests suggest that maximum daily\ntemperature has a significant effect on migration patterns.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:29:42 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 00:57:30 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 17:27:07 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2019 17:52:06 GMT"}, {"version": "v5", "created": "Fri, 8 Nov 2019 16:14:07 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Coleman", "Tim", ""], ["Mentch", "Lucas", ""], ["Fink", "Daniel", ""], ["La Sorte", "Frank", ""], ["Hooker", "Giles", ""], ["Hochachka", "Wesley", ""], ["Winkler", "David", ""]]}, {"id": "1710.09821", "submitter": "Megan L. Gelsinger", "authors": "Megan L. Gelsinger, Laura L. Tupper and David S. Matteson", "title": "Cell Line Classification Using Electric Cell-substrate Impedance Sensing\n  (ECIS)", "comments": "40 pages, 10 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cell line classification using multivariate time series data\nobtained from electric cell-substrate impedance sensing (ECIS) technology. The\nECIS device, which monitors the attachment and spreading of mammalian cells in\nreal time through the collection of electrical impedance data, has historically\nbeen used to study one cell line at a time. However, we show that if applied to\ndata from multiple cell lines, ECIS can be used to classify unknown or\npotentially mislabeled cells, which may help to mitigate the current crisis of\nreproducibility in the biological literature. We assess a range of approaches\nto this new problem, testing different classification methods and deriving a\ndictionary of 29 features to characterize ECIS data. Our analysis also makes\nuse of simultaneous multi-frequency ECIS data, where previous studies have\nfocused on only one frequency. In classification tests on fifteen mammalian\ncell lines, we obtain very high out-of-sample accuracy. These preliminary\nfindings provide a baseline for future large-scale studies in this field.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:36:53 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 15:55:39 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 16:41:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gelsinger", "Megan L.", ""], ["Tupper", "Laura L.", ""], ["Matteson", "David S.", ""]]}, {"id": "1710.09870", "submitter": "Sahil Agarwal", "authors": "Sahil Agarwal and John S. Wettlaufer", "title": "Exoplanet Atmosphere Retrieval from Multifractal Analysis of Secondary\n  Eclipse Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extend a data-based model-free multifractal method of exoplanet detection\nto probe exoplanetary atmospheres. Whereas the transmission spectrum is studied\nduring the primary eclipse, we analyze the emission spectrum during the\nsecondary eclipse, thereby probing the atmospheric limb. In addition to the\nspectral structure of exoplanet atmospheres, the approach provides information\nto study phenomena such as atmospheric flows, tidal-locking behavior, and the\ndayside-nightside redistribution of energy. The approach is demonstrated using\nSpitzer data for exoplanet HD189733b. The central advantage of the method is\nthe lack of model assumptions in the detection and observational schemes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:01:58 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 04:07:56 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 17:37:53 GMT"}, {"version": "v4", "created": "Sat, 22 May 2021 01:56:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Agarwal", "Sahil", ""], ["Wettlaufer", "John S.", ""]]}, {"id": "1710.09890", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou", "title": "Bayesian Nonparametric Models for Biomedical Data Analysis", "comments": "PhD thesis, UT Austin, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we develop nonparametric Bayesian models for biomedical\ndata analysis. In particular, we focus on inference for tumor heterogeneity and\ninference for missing data. First, we present a Bayesian feature allocation\nmodel for tumor subclone reconstruction using mutation pairs. The key\ninnovation lies in the use of short reads mapped to pairs of proximal single\nnucleotide variants (SNVs). In contrast, most existing methods use only\nmarginal reads for unpaired SNVs. In the same context of using mutation pairs,\nin order to recover the phylogenetic relationship of subclones, we then develop\na Bayesian treed feature allocation model. In contrast to commonly used feature\nallocation models, we allow the latent features to be dependent, using a tree\nstructure to introduce dependence. Finally, we propose a nonparametric Bayesian\napproach to monotone missing data in longitudinal studies with non-ignorable\nmissingness. In contrast to most existing methods, our method allows for\nincorporating information from auxiliary covariates and is able to capture\ncomplex structures among the response, missingness and auxiliary covariates.\nOur models are validated through simulation studies and are applied to\nreal-world biomedical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:39:36 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zhou", "Tianjian", ""]]}, {"id": "1710.10023", "submitter": "Jos Boesten", "authors": "Jos J.T.I. Boesten", "title": "Estimating the coefficients of variation of Freundlich parameters with\n  weighted least squares analysis", "comments": "25 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Freundlich isotherm has been used widely to describe sorption of solutes\nto soils for many decades. The Freundlich parameters are often estimated using\nunweighted least squares (ULS) analysis after log-log transformation.\nEstimating the accuracy of these parameters (characterized by their coefficient\nof variation, CV) is an essential element of the use of these parameters.\nAccurate CVs can be derived with weighted least squares (WLS), but only if\nproper weights are assigned to the residuals in the fitting procedure. This\nwork presents the derivation of an analytical approximation of these weights\nwhich were found to decrease with increasing concentration, increasing\nFreundlich exponent, and increasing CVs of the initial and equilibrium\nconcentrations. Monte-Carlo simulations for a wide range of Freundlich systems\nbased on known values of the CVs of the initial and equilibrium concentrations\nconfirmed the accuracy of this analytical approximation. Unfortunately, in\npractice, the CVs of the initial and equilibrium concentrations are unknown a\npriori. Simulations showed that the accuracy of the estimated CVs of the\nFreundlich parameters was distinctly lower if the CVs of the initial and\nequilibrium concentrations were estimated from the isotherm data. However, this\naccuracy was still considerably better than when using ULS. It is recommended\nto use this analytical approximation whenever these CVs are relevant for the\nfurther use and interpretation of the estimated Freundlich parameters.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 08:17:57 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Boesten", "Jos J. T. I.", ""]]}, {"id": "1710.10024", "submitter": "Mehdi Shafiei", "authors": "Mehdi Shafiei, Ghavameddin Nourbakhsh, Ali Arefi, Gerard Ledwich,\n  Houman Pezeshki", "title": "Single Iteration Conditional Based DSE Considering Spatial and Temporal\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of distribution network calls for advancement in\ndistribution system state estimation (DSSE) to monitor the operating conditions\nmore accurately. Sufficient number of measurements is imperative for a reliable\nand accurate state estimation. The limitation on the measurement devices is\ngenerally tackled with using the so-called pseudo measured data. However, the\nerrors in pseudo data by cur-rent techniques are quite high leading to a poor\nDSSE. As customer loads in distribution networks show high cross-correlation in\nvarious locations and over successive time steps, it is plausible that\ndeploying the spatial-temporal dependencies can improve the pseudo data\naccuracy and estimation. Although, the role of spatial dependency in DSSE has\nbeen addressed in the literature, one can hardly find an efficient DSSE\nframework capable of incorporating temporal dependencies present in customer\nloads. Consequently, to obtain a more efficient and accurate state estimation,\nwe propose a new non-iterative DSSE framework to involve spatial-temporal\ndependencies together. The spatial-temporal dependencies are modeled by\nconditional multivariate complex Gaussian distributions and are studied for\nboth static and real-time state estimations, where information at preceding\ntime steps are employed to increase the accuracy of DSSE. The efficiency of the\nproposed approach is verified based on quality and accuracy indices, standard\ndeviation and computational time. Two balanced medium voltage (MV) and one\nunbalanced low voltage (LV) distribution case studies are used for evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 08:26:08 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 22:23:27 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 01:30:11 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2018 01:28:04 GMT"}, {"version": "v5", "created": "Tue, 21 Aug 2018 22:17:52 GMT"}, {"version": "v6", "created": "Mon, 17 Sep 2018 00:51:30 GMT"}, {"version": "v7", "created": "Thu, 8 Nov 2018 00:51:36 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Shafiei", "Mehdi", ""], ["Nourbakhsh", "Ghavameddin", ""], ["Arefi", "Ali", ""], ["Ledwich", "Gerard", ""], ["Pezeshki", "Houman", ""]]}, {"id": "1710.10161", "submitter": "Joeseph Smith", "authors": "Joeseph P. Smith, Andrew D. Gronewold", "title": "Development and analysis of a Bayesian water balance model for large\n  lake systems", "comments": "Final version for ArXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water balance models (WBMs) are often employed to understand regional\nhydrologic cycles over various time scales. Most WBMs, however, are\nphysically-based, and few employ state-of-the-art statistical methods to\nreconcile independent input measurement uncertainty and bias. Further, few WBMs\nexist for large lakes, and most large lake WBMs perform additive accounting,\nwith minimal consideration towards input data uncertainty. Here, we introduce a\nframework for improving a previously developed large lake statistical water\nbalance model (L2SWBM). Focusing on the water balances of Lakes Superior and\nMichigan-Huron, we demonstrate our new analytical framework, identifying\nL2SWBMs from 26 alternatives that adequately close the water balance of the\nlakes with satisfactory computation times compared with the prototype model. We\nexpect our new framework will be used to develop water balance models for other\nlakes around the world.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 12:49:05 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 16:07:24 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:10:25 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 21:29:33 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Smith", "Joeseph P.", ""], ["Gronewold", "Andrew D.", ""]]}, {"id": "1710.10279", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, John Choi, Bijan Pesaran, Demba Ba, and Vahid Tarokh", "title": "Wavelet Shrinkage and Thresholding based Robust Classification for Brain\n  Computer Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A macaque monkey is trained to perform two different kinds of tasks, memory\naided and visually aided. In each task, the monkey saccades to eight possible\ntarget locations. A classifier is proposed for direction decoding and task\ndecoding based on local field potentials (LFP) collected from the prefrontal\ncortex. The LFP time-series data is modeled in a nonparametric regression\nframework, as a function corrupted by Gaussian noise. It is shown that if the\nfunction belongs to Besov bodies, then using the proposed wavelet shrinkage and\nthresholding based classifier is robust and consistent. The classifier is then\napplied to the LFP data to achieve high decoding performance. The proposed\nclassifier is also quite general and can be applied for the classification of\nother types of time-series data as well, not necessarily brain data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 18:04:51 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 18:16:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Banerjee", "Taposh", ""], ["Choi", "John", ""], ["Pesaran", "Bijan", ""], ["Ba", "Demba", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1710.10319", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Veronica Vinciotti, Ernst C. Wit", "title": "Identifying overlapping terrorist cells from the Noordin Top actor-event\n  network", "comments": "24 pages, 5 figures; related R package (manet) available on CRAN", "journal-ref": "Annals of Applied Statistics 2020, Vol. 14, No. 3, 1516-1534", "doi": "10.1214/20-AOAS1358", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor-event data are common in sociological settings, whereby one registers\nthe pattern of attendance of a group of social actors to a number of events. We\nfocus on 79 members of the Noordin Top terrorist network, who were monitored\nattending 45 events. The attendance or non-attendance of the terrorist to\nevents defines the social fabric, such as group coherence and social\ncommunities. The aim of the analysis of such data is to learn about the\naffiliation structure. Actor-event data is often transformed to actor-actor\ndata in order to be further analysed by network models, such as stochastic\nblock models. This transformation and such analyses lead to a natural loss of\ninformation, particularly when one is interested in identifying, possibly\noverlapping, subgroups or communities of actors on the basis of their\nattendances to events. In this paper we propose an actor-event model for\noverlapping communities of terrorists, which simplifies interpretation of the\nnetwork. We propose a mixture model with overlapping clusters for the analysis\nof the binary actor-event network data, called {\\tt manet}, and develop a\nBayesian procedure for inference. After a simulation study, we show how this\nanalysis of the terrorist network has clear interpretative advantages over the\nmore traditional approaches of affiliation network analysis.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 20:02:41 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 13:28:34 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Ranciati", "Saverio", ""], ["Vinciotti", "Veronica", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1710.10346", "submitter": "Yury Garcia", "authors": "Yury E. Garc\\'ia, Oksana A. Chkrebtii, Marcos A. Capistr\\'an, Daniel\n  E. Noyola", "title": "Inference for stochastic kinetic models from multiple data sources for\n  joint estimation of infection dynamics from aggregate reports and virological\n  data", "comments": "This paper was previously submitted as new one accidentally. (ID\n  1903.10905)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza and respiratory syncytial virus (RSV) are the leading etiological\nagents of seasonal acute respiratory infections (ARI) around the world. Medical\ndoctors typically base the diagnosis of ARI on patients' symptoms alone, and do\nnot always conduct virological tests necessary to identify individual viruses,\nwhich limits the ability to study the interaction between multiple pathogens\nand make public health recommendations. We consider a stochastic kinetic model\n(SKM) for two interacting ARI pathogens circulating in a large population and\nan empirically motivated background process for infections with other pathogens\ncausing similar symptoms. An extended marginal sampling approach based on the\nLinear Noise Approximation to the SKM integrates multiple data sources and\nadditional model components. We infer the parameters defining the pathogens'\ndynamics and interaction within a Bayesian hierarchical model and explore the\nposterior trajectories of infections for each illness based on aggregate\ninfection reports from six epidemic seasons collected by the state health\ndepartment, and a subset of virological tests from a sentinel program at a\ngeneral hospital in San Luis Potos\\'i, M\\'exico. We interpret the results based\non real and simulated data and make recommendations for future data collection\nstrategies. Supplementary materials and software are provided online.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 21:53:58 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:29:12 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 20:28:24 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Garc\u00eda", "Yury E.", ""], ["Chkrebtii", "Oksana A.", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Noyola", "Daniel E.", ""]]}, {"id": "1710.10351", "submitter": "Andrew Brown", "authors": "D. Andrew Brown, Christopher S. McMahan, Russell T. Shinohara, Kristin\n  A. Linn", "title": "Bayesian Spatial Binary Regression for Label Fusion in Structural\n  Neuroimaging", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many analyses of neuroimaging data involve studying one or more regions of\ninterest (ROIs) in a brain image. In order to do so, each ROI must first be\nidentified. Since every brain is unique, the location, size, and shape of each\nROI varies across subjects. Thus, each ROI in a brain image must either be\nmanually identified or (semi-) automatically delineated, a task referred to as\nsegmentation. Automatic segmentation often involves mapping a previously\nmanually segmented image to a new brain image and propagating the labels to\nobtain an estimate of where each ROI is located in the new image. A more recent\napproach to this problem is to propagate labels from multiple manually\nsegmented atlases and combine the results using a process known as label\nfusion. To date, most label fusion algorithms either employ voting procedures\nor impose prior structure and subsequently find the maximum a posteriori\nestimator (i.e., the posterior mode) through optimization. We propose using a\nfully Bayesian spatial regression model for label fusion that facilitates\ndirect incorporation of covariate information while making accessible the\nentire posterior distribution. We discuss the implementation of our model via\nMarkov chain Monte Carlo and illustrate the procedure through both simulation\nand application to segmentation of the hippocampus, an anatomical structure\nknown to be associated with Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 22:24:24 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 14:14:14 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Brown", "D. Andrew", ""], ["McMahan", "Christopher S.", ""], ["Shinohara", "Russell T.", ""], ["Linn", "Kristin A.", ""]]}, {"id": "1710.10383", "submitter": "Zhonghua Liu", "authors": "Zhonghua Liu and Xihong Lin", "title": "A Geometric Perspective on the Power of Principal Component Association\n  Tests in Multiple Phenotype Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint analysis of multiple phenotypes can increase statistical power in\ngenetic association studies. Principal component analysis, as a popular\ndimension reduction method, especially when the number of phenotypes is\nhigh-dimensional, has been proposed to analyze multiple correlated phenotypes.\nIt has been empirically observed that the first PC, which summarizes the\nlargest amount of variance, can be less powerful than higher order PCs and\nother commonly used methods in detecting genetic association signals. In this\npaper, we investigate the properties of PCA-based multiple phenotype analysis\nfrom a geometric perspective by introducing a novel concept called principal\nangle. A particular PC is powerful if its principal angle is $0^o$ and is\npowerless if its principal angle is $90^o$. Without prior knowledge about the\ntrue principal angle, each PC can be powerless. We propose linear, non-linear\nand data-adaptive omnibus tests by combining PCs. We show that the omnibus PC\ntest is robust and powerful in a wide range of scenarios. We study the\nproperties of the proposed methods using power analysis and eigen-analysis. The\nsubtle differences and close connections between these combined PC methods are\nillustrated graphically in terms of their rejection boundaries. Our proposed\ntests have convex acceptance regions and hence are admissible. The $p$-values\nfor the proposed tests can be efficiently calculated analytically and the\nproposed tests have been implemented in a publicly available R package {\\it\nMPAT}. We conduct simulation studies in both low and high dimensional settings\nwith various signal vectors and correlation structures. We apply the proposed\ntests to the joint analysis of metabolic syndrome related phenotypes with data\nsets collected from four international consortia to demonstrate the\neffectiveness of the proposed combined PC testing procedures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 03:53:09 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 03:17:22 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Liu", "Zhonghua", ""], ["Lin", "Xihong", ""]]}, {"id": "1710.10555", "submitter": "Wenying Ji", "authors": "Wenying Ji, Simaan M. AbouRizk, Osmar R. Zaiane, Yitong Li", "title": "Complexity Analysis Approach for Prefabricated Construction Products\n  Using Uncertain Data Clustering", "comments": null, "journal-ref": null, "doi": "10.1061/(ASCE)CO.1943-7862.0001520", "report-no": null, "categories": "cs.DB cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an uncertain data clustering approach to quantitatively\nanalyze the complexity of prefabricated construction components through the\nintegration of quality performance-based measures with associated engineering\ndesign information. The proposed model is constructed in three steps, which (1)\nmeasure prefabricated construction product complexity (hereafter referred to as\nproduct complexity) by introducing a Bayesian-based nonconforming quality\nperformance indicator; (2) score each type of product complexity by developing\na Hellinger distance-based distribution similarity measurement; and (3) cluster\nproducts into homogeneous complexity groups by using the agglomerative\nhierarchical clustering technique. An illustrative example is provided to\ndemonstrate the proposed approach, and a case study of an industrial company in\nEdmonton, Canada, is conducted to validate the feasibility and applicability of\nthe proposed model. This research inventively defines and investigates product\ncomplexity from the perspective of product quality performance with design\ninformation associated. The research outcomes provide simplified,\ninterpretable, and informative insights for practitioners to better analyze and\nmanage product complexity. In addition to this practical contribution, a novel\nhierarchical clustering technique is devised. This technique is capable of\nclustering uncertain data (i.e., beta distributions) with lower computational\ncomplexity and has the potential to be generalized to cluster all types of\nuncertain data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 03:30:36 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 17:45:03 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ji", "Wenying", ""], ["AbouRizk", "Simaan M.", ""], ["Zaiane", "Osmar R.", ""], ["Li", "Yitong", ""]]}, {"id": "1710.10641", "submitter": "Qifan Yang", "authors": "Qifan Yang, Gennady V. Roshchupkin, Wiro J. Niessen, Sarah E. Medland,\n  Alyssa H. Zhu, Paul M. Thompson, Neda Jahanshad", "title": "A Fast, Accurate Two-Step Linear Mixed Model for Genetic Analysis\n  Applied to Repeat MRI Measurements", "comments": "2017 Neural Information Processing Systems (NeurIPS) BigNeuro\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale biobanks are being collected around the world in efforts to\nbetter understand human health and risk factors for disease. They often survey\nhundreds of thousands of individuals, combining questionnaires with clinical,\ngenetic, demographic, and imaging assessments; some of this data may be\ncollected longitudinally. Genetic associations analysis of such datasets\nrequires methods to properly handle relatedness, population structure and other\ntypes of biases introduced by confounders. Most popular and accurate approaches\nrely on linear mixed model (LMM) algorithms, which are iterative and\ncomputational complexity of each iteration scales by the square of the sample\nsize, slowing the pace of discoveries (up to several days for single trait\nanalysis), and, furthermore, limiting the use of repeat phenotypic\nmeasurements. Here, we describe our new, non-iterative, much faster and\naccurate Two-Step Linear Mixed Model (Two-Step LMM) approach, that has a\ncomputational complexity that scales linearly with sample size. We show that\nthe first step retains accurate estimates of the heritability (the proportion\nof the trait variance explained by additive genetic factors), even when\nincreasingly complex genetic relationships between individuals are modeled.\nSecond step provides a faster framework to obtain the effect sizes of\ncovariates in regression model. We applied Two-Step LMM to real data from the\nUK Biobank, which recently released genotyping information and processed MRI\ndata from 9,725 individuals. We used the left and right hippocampus volume (HV)\nas repeated measures, and observed increased and more accurate heritability\nestimation, consistent with simulations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 16:24:40 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 13:07:09 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 01:42:45 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 19:01:43 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yang", "Qifan", ""], ["Roshchupkin", "Gennady V.", ""], ["Niessen", "Wiro J.", ""], ["Medland", "Sarah E.", ""], ["Zhu", "Alyssa H.", ""], ["Thompson", "Paul M.", ""], ["Jahanshad", "Neda", ""]]}, {"id": "1710.10692", "submitter": "Wenhao Li", "authors": "Wenhao Li, Bolong Wang, Tianxiang Shen, Ronghua Zhu, Dehui Wang", "title": "Research on ruin probability of risk model based on AR(1) series", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this text, we establish the risk model based on AR(1) series and propose\nthe basic model which has a dependent structure under intensity of claim\nnumber. Considering some properties of the risk model, we take advantage of\nnewton iteration method to figure out the adjustment coefficient and estimate\nthe exponential upper bound of ruin probability. This is significant to refine\nthe research of ruin theory. As a result, our theory will help develop\ninsurance industry stably.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 21:05:32 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Li", "Wenhao", ""], ["Wang", "Bolong", ""], ["Shen", "Tianxiang", ""], ["Zhu", "Ronghua", ""], ["Wang", "Dehui", ""]]}, {"id": "1710.10728", "submitter": "Chengyu Liu", "authors": "Chengyu Liu, Wei Wang", "title": "Contextual Regression: An Accurate and Conveniently Interpretable\n  Nonlinear Model for Mining Discovery from Scientific Data", "comments": "18 pages of Main Article, 30 pages of Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms such as linear regression, SVM and neural network\nhave played an increasingly important role in the process of scientific\ndiscovery. However, none of them is both interpretable and accurate on\nnonlinear datasets. Here we present contextual regression, a method that joins\nthese two desirable properties together using a hybrid architecture of neural\nnetwork embedding and dot product layer. We demonstrate its high prediction\naccuracy and sensitivity through the task of predictive feature selection on a\nsimulated dataset and the application of predicting open chromatin sites in the\nhuman genome. On the simulated data, our method achieved high fidelity recovery\nof feature contributions under random noise levels up to 200%. On the open\nchromatin dataset, the application of our method not only outperformed the\nstate of the art method in terms of accuracy, but also unveiled two previously\nunfound open chromatin related histone marks. Our method can fill the blank of\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 00:39:47 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Liu", "Chengyu", ""], ["Wang", "Wei", ""]]}, {"id": "1710.10742", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei", "title": "Implicit Causal Models for Genome-wide Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in probabilistic generative models has accelerated, developing\nricher models with neural architectures, implicit densities, and with scalable\nalgorithms for their Bayesian inference. However, there has been limited\nprogress in models that capture causal relationships, for example, how\nindividual genetic factors cause major human diseases. In this work, we focus\non two challenges in particular: How do we build richer causal models, which\ncan capture highly nonlinear relationships and interactions between multiple\ncauses? How do we adjust for latent confounders, which are variables\ninfluencing both cause and effect and which prevent learning of causal\nrelationships? To address these challenges, we synthesize ideas from causality\nand modern probabilistic modeling. For the first, we describe implicit causal\nmodels, a class of causal models that leverages neural architectures with an\nimplicit density. For the second, we describe an implicit causal model that\nadjusts for confounders by sharing strength across examples. In experiments, we\nscale Bayesian inference on up to a billion genetic measurements. We achieve\nstate of the art accuracy for identifying causal factors: we significantly\noutperform existing genetics methods by an absolute difference of 15-45.3%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:05:10 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1710.10745", "submitter": "Xing He", "authors": "Xing He, Lei Chu, Robert C. Qiu, Qian Ai, Zenan Ling, Jian Zhang", "title": "Invisible Units Detection and Estimation Based on Random Matrix Theory", "comments": "10 pages,Accepted by IEEE Transaction on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invisible units mainly refer to small-scale units that are not monitored by,\nand thus are not visible to utilities. Integration of these invisible units\ninto power systems does significantly affect the way in which a distribution\ngrid is planned and operated. This paper, based on random matrix theory (RMT),\nproposes a statistical, data-driven framework to handle the massive grid data,\nin contrast to its deterministic, model-based counterpart. Combining the\nRMT-based data-mining framework with conventional techniques, some heuristics\nare derived as the solution to the invisible units detection and estimation\ntask: linear eigenvalue statistic indicators (LESs) are suggested as the main\ningredients of the solution; according to the statistical properties of LESs,\nthe hypothesis testing is formulated to conduct change point detection in the\nhigh-dimensional space. The proposed method is promising for anomaly detection\nand pertinent to current distribution networks---it is capable of detecting\ninvisible power usage and fraudulent behavior while even being able to locate\nthe suspect's location. Case studies, using both simulated data and actual\ndata, validate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:34:01 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 04:50:29 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["He", "Xing", ""], ["Chu", "Lei", ""], ["Qiu", "Robert C.", ""], ["Ai", "Qian", ""], ["Ling", "Zenan", ""], ["Zhang", "Jian", ""]]}, {"id": "1710.11044", "submitter": "Dominik Paprotny", "authors": "Dominik Paprotny, Antonia Sebastian, Oswaldo Morales-N\\'apoles,\n  Sebastiaan N. Jonkman", "title": "Trends in European flood risk over the past 150 years", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flood risk changes in time and is influenced by both natural and\nsocio-economic trends and interactions. In Europe, previous studies of\nhistorical flood losses corrected for demographic and economic growth\n(\"normalized\") have been limited in temporal and spatial extent, leading to an\nincomplete representation in trends of losses over time. In this study we\nutilize a gridded reconstruction of flood exposure in 37 European countries and\na new database of damaging floods since 1870. Our results indicate that since\n1870 there has been an increase in annually inundated area and number of\npersons affected, contrasted by a substantial decrease in flood fatalities,\nafter correcting for change in flood exposure. For more recent decades we also\nfound a considerable decline in financial losses per year. We estimate,\nhowever, that there is large underreporting of smaller floods beyond most\nrecent years, and show that underreporting has a substantial impact on observed\ntrends.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:22:59 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Paprotny", "Dominik", ""], ["Sebastian", "Antonia", ""], ["Morales-N\u00e1poles", "Oswaldo", ""], ["Jonkman", "Sebastiaan N.", ""]]}, {"id": "1710.11065", "submitter": "Jose Garrido", "authors": "Zied Ben Salah, Jos\\'e Garrido", "title": "On Fair Reinsurance Premiums; Capital Injections in a Perturbed Risk\n  Model", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a risk model where deficits after ruin are covered by a new type\nof reinsurance contract that provides capital injections. To allow the\ninsurance company's survival after ruin, the reinsurer injects capital only at\nruin times caused by jumps larger than a chosen retention level. Otherwise\ncapital must be raised from the shareholders for small deficits. The problem\nhere is to determine adequate reinsurance premiums. It seems fair to base the\nnet reinsurance premium on the discounted expected value of any future capital\ninjections. Inspired by the results of Huzak et al. (2004) and Ben Salah (2014)\non successive ruin events, we show that an explicit formula for these\nreinsurance premiums exists in a setting where aggregate claims are modeled by\na subordinator and a Brownian perturbation. Here ruin events are due either to\nBrownian oscillations or jumps and reinsurance capital injections only apply in\nthe latter case. The results are illustrated explicitly for two specific risk\nmodels and in some numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:56:46 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 16:54:48 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 09:11:26 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 12:15:21 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Salah", "Zied Ben", ""], ["Garrido", "Jos\u00e9", ""]]}, {"id": "1710.11283", "submitter": "Ken Sze-Wai Wong", "authors": "Jessica Foo, Lek-Heng Lim, Ken Sze-Wai Wong", "title": "Macroeconomics and FinTech: Uncovering Latent Macroeconomic Effects on\n  Peer-to-Peer Lending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech)\ntrend that is displacing traditional retail banking. Studies on P2P lending\nhave focused on predicting individual interest rates or default probabilities.\nHowever, the relationship between aggregated P2P interest rates and the general\neconomy will be of interest to investors and borrowers as the P2P credit market\nmatures. We show that the variation in P2P interest rates across grade types\nare determined by three macroeconomic latent factors formed by Canonical\nCorrelation Analysis (CCA) - macro default, investor uncertainty, and the\nfundamental value of the market. However, the variation in P2P interest rates\nacross term types cannot be explained by the general economy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 00:49:04 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Foo", "Jessica", ""], ["Lim", "Lek-Heng", ""], ["Wong", "Ken Sze-Wai", ""]]}, {"id": "1710.11292", "submitter": "Dalia Chakrabarty Dr.", "authors": "Kangrui Wang, and Dalia Chakrabarty", "title": "Correlation between Multivariate Datasets, from Inter-Graph Distance\n  computed using Graphical Models Learnt With Uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for simultaneous Bayesian learning of the correlation\nmatrix and graphical model of a multivariate dataset, along with uncertainties\nin each, to subsequently compute distance between the learnt graphical models\nof a pair of datasets, using a new metric that approximates an\nuncertainty-normalised Hellinger distance between the posterior probabilities\nof the graphical models given the respective dataset; correlation between the\npair of datasets is then computed as a corresponding affinity measure. We\nachieve a closed-form likelihood of the between-columns correlation matrix by\nmarginalising over the between-row matrices. This between-columns correlation\nis updated first, given the data, and the graph is then updated, given the\npartial correlation matrix that is computed given the updated correlation,\nallowing for learning of the 95$\\%$ Highest Probability Density credible\nregions of the correlation matrix and graphical model of the data. Difference\nmade to the learnt graphical model, by acknowledgement of measurement noise, is\ndemonstrated on a small simulated dataset, while the large human\ndisease-symptom network--with $>8,000$ nodes--is learnt using real data. Data\non vino-chemical attributes of Portuguese red and white wine samples are\nemployed to learn with-uncertainty graphical model of each dataset, and\nsubsequently, the distance between these learnt graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 01:48:23 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 23:13:41 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 02:52:35 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1710.11297", "submitter": "Yang Cao", "authors": "Y. Cao, S. Zhu, Y. Xie, J. Key, J. Kacher, R. R. Unocic, C. M. Rouleau", "title": "Sequential Adaptive Detection for In-Situ Transmission Electron\n  Microscopy (TEM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new efficient online algorithms for detecting transient sparse\nsignals in TEM video sequences, by adopting the recently developed framework\nfor sequential detection jointly with online convex optimization [1]. We cast\nthe problem as detecting an unknown sparse mean shift of Gaussian observations,\nand develop adaptive CUSUM and adaptive SSRS procedures, which are based on\nlikelihood ratio statistics with post-change mean vector being online maximum\nlikelihood estimators with $\\ell_1$. We demonstrate the meritorious performance\nof our algorithms for TEM imaging using real data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 02:04:21 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Cao", "Y.", ""], ["Zhu", "S.", ""], ["Xie", "Y.", ""], ["Key", "J.", ""], ["Kacher", "J.", ""], ["Unocic", "R. R.", ""], ["Rouleau", "C. M.", ""]]}, {"id": "1710.11534", "submitter": "Juan Pablo P\\'erez Monsalve", "authors": "Juan Pablo P\\'erez Monsalve and Freddy H. Mar\\'in Sanchez", "title": "Parameter Estimation in Mean Reversion Processes with Periodic\n  Functional Tendency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the procedure to estimate the parameters in mean\nreversion processes with functional tendency defined by a periodic continuous\ndeterministic function, expressed as a series of truncated Fourier. Two phases\nof estimation are defined, in the first phase through Gaussian techniques using\nthe Euler-Maruyama discretization, we obtain the maximum likelihood function,\nthat will allow us to find estimators of the external parameters and an\nestimation of the expected value of the process. In the second phase, a\nreestimate of the periodic functional tendency with it's parameters of phase\nand amplitude is carried out, this will allow, improve the initial estimation.\nSome experimental result using simulated data sets are graphically illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 15:38:57 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Monsalve", "Juan Pablo P\u00e9rez", ""], ["Sanchez", "Freddy H. Mar\u00edn", ""]]}]