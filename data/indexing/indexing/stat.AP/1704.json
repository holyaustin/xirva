[{"id": "1704.00040", "submitter": "Yulong Huang", "authors": "Yulong Huang and Yonggang Zhang", "title": "Robust Student's t based Stochastic Cubature Filter for Nonlinear\n  Systems with Heavy-tailed Process and Measurement Noises", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new robust Student's t based stochastic cubature filter\n(RSTSCF) is proposed for nonlinear state-space model with heavy-tailed process\nand measurement noises. The heart of the RSTSCF is a stochastic Student's t\nspherical radial cubature rule (SSTSRCR), which is derived based on the\nthird-degree unbiased spherical rule and the proposed third-degree unbiased\nradial rule. The existing stochastic integration rule is a special case of the\nproposed SSTSRCR when the degrees of freedom parameter tends to infinity. The\nproposed filter is applied to a manoeuvring bearings-only tracking example,\nwhere an agile target is tracked and the bearing is observed in clutter.\nSimulation results show that the proposed RSTSCF can achieve higher estimation\naccuracy than the existing Gaussian approximate filter, Gaussian sum filter,\nHuber-based nonlinear Kalman filter, maximum correntropy criterion based Kalman\nfilter, and robust Student's t based nonlinear filters, and is computationally\nmuch more efficient than the existing particle filter.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 20:03:08 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Huang", "Yulong", ""], ["Zhang", "Yonggang", ""]]}, {"id": "1704.00076", "submitter": "C\\'eline L\\'evy-Leduc", "authors": "M. Perrot-Dock\\`es, C. L\\'evy-Leduc, J. Chiquet, L. Sansonnet, M.\n  Br\\'eg\\`ere, M.-P. \\'Etienne, S. Robin and G. Genta-Jouve", "title": "A multivariate variable selection approach for analyzing LC-MS\n  metabolomics data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omic data are characterized by the presence of strong dependence structures\nthat result either from data acquisition or from some underlying biological\nprocesses. In metabolomics, for instance, data resulting from Liquid\nChromatography-Mass Spectrometry (LC-MS) -- a technique which gives access to a\nlarge coverage of metabolites -- exhibit such patterns. These data sets are\ntypically used to find the metabolites characterizing a phenotype of interest\nassociated with the samples. However, applying some statistical procedures that\ndo not adjust the variable selection step to the dependence pattern may result\nin a loss of power and the selection of spurious variables. The goal of this\npaper is to propose a variable selection procedure in the multivariate linear\nmodel that accounts for the dependence structure of the multiple outputs which\nmay lead in the LC-MS framework to the selection of more relevant metabolites.\nWe propose a novel Lasso-based approach in the multivariate framework of the\ngeneral linear model taking into account the dependence structure by using\nvarious modelings of the covariance matrix of the residuals. Our numerical\nexperiments show that including the estimation of the covariance matrix of the\nresiduals in the Lasso criterion dramatically improves the variable selection\nperformance. Our approach is also successfully applied to a LC-MS data set made\nof African copals samples for which it is able to provide a small list of\nmetabolites without altering the phenotype discrimination. Our methodology is\nimplemented in the R package MultiVarSel which is available from the CRAN\n(Comprehensive R Archive Network).\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 22:29:59 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Perrot-Dock\u00e8s", "M.", ""], ["L\u00e9vy-Leduc", "C.", ""], ["Chiquet", "J.", ""], ["Sansonnet", "L.", ""], ["Br\u00e9g\u00e8re", "M.", ""], ["\u00c9tienne", "M. -P.", ""], ["Robin", "S.", ""], ["Genta-Jouve", "G.", ""]]}, {"id": "1704.00197", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis", "title": "iWinRNFL: A Simple, Interpretable & Well-Calibrated In-Game Win\n  Probability Model for NFL", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last few sports seasons a lot of discussion has been generated for\nthe several, high-profile, \"comebacks\" that were observed in almost all sports.\nThe Cavaliers won the championship after being down 3-1 in the 2016 NBA finals'\nseries against the Golden State Warriors, which was exactly the case for\nChicago Cubs and the World Series. The Patriots won the Super Bowl in 2016 even\nthough they were trailing by 25 points late in the third quarter, while FC\nBarcelona in the top-16 round of the 2016-17 Champions League scored 3 goals\nduring the last 7 minutes of the game (including stoppage time) against PSG to\nadvance in the tournament. This has brought the robustness and accuracy of the\nvarious probabilistic prediction models under high scrutiny. Many of these\nmodels are proprietary, which makes it hard to evaluate. In this paper, we\nbuild a simple and open, yet robust and well-calibrated, in-game probability\nmodel for predicting the winner in an NFL (iWinRNFL) game. In particular, we\nbuild a logistic regression model that utilizes a set of 10 variables to\npredict the running win probability for the home team. We train our model using\ndetailed play-by-play data from the last 7 NFL seasons obtained through the\nleague's API. Our results indicate that in 75% of the cases iWinRNFL provides\nan accurate winner projection, as compared to a 63% accuracy of a baseline\npre-game win probability model. Most importantly the probabilities that\niWinRNFL provides are well-calibrated. Finally, we have also evaluated more\ncomplex, non-linear, models using the same set of features, without any\nsignificant improvement in performance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 16:33:39 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 14:49:44 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 14:49:05 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Pelechrinis", "Konstantinos", ""]]}, {"id": "1704.00224", "submitter": "Mohammad Tariqul Islam", "authors": "Mohammad Tariqul Islam, Ishmam Zabir, Sk. Tanvir Ahamed, Md. Tahmid\n  Yasar, Celia Shahnaz, Shaikh Anowarul Fattah", "title": "A Time-Frequency Domain Approach of Heart Rate Estimation From\n  Photoplethysmographic (PPG) Signal", "comments": "14 pages, 10 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.bspc.2017.03.020", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective- Heart rate monitoring using wrist type Photoplethysmographic (PPG)\nsignals is getting popularity because of construction simplicity and low cost\nof wearable devices. The task becomes very difficult due to the presence of\nvarious motion artifacts. The objective is to develop algorithms to reduce the\neffect of motion artifacts and thus obtain accurate heart rate estimation.\nMethods- Proposed heart rate estimation scheme utilizes both time and frequency\ndomain analyses. Unlike conventional single stage adaptive filter, multi-stage\ncascaded adaptive filtering is introduced by using three channel accelerometer\ndata to reduce the effect of motion artifacts. Both recursive least squares\n(RLS) and least mean squares (LMS) adaptive filters are tested. Moreover,\nsingular spectrum analysis (SSA) is employed to obtain improved spectral peak\ntracking. The outputs from the filter block and SSA operation are logically\ncombined and used for spectral domain heart rate estimation. Finally, a\ntracking algorithm is incorporated considering neighbouring estimates. Results-\nThe proposed method provides an average absolute error of 1.16 beat per minute\n(BPM) with a standard deviation of 1.74 BPM while tested on publicly available\ndatabase consisting of recordings from 12 subjects during physical activities.\nConclusion- It is found that the proposed method provides consistently better\nheart rate estimation performance in comparison to that recently reported by\nTROIKA, JOSS and SPECTRAP methods. Significance- The proposed method offers\nvery low estimation error and a smooth heart rate tracking with simple\nalgorithmic approach and thus feasible for implementing in wearable devices to\nmonitor heart rate for fitness and clinical purpose.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 20:24:52 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 17:25:33 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Islam", "Mohammad Tariqul", ""], ["Zabir", "Ishmam", ""], ["Ahamed", "Sk. Tanvir", ""], ["Yasar", "Md. Tahmid", ""], ["Shahnaz", "Celia", ""], ["Fattah", "Shaikh Anowarul", ""]]}, {"id": "1704.00240", "submitter": "Mami Kajita Iwata", "authors": "Mami Kajita and Seiji Kajita", "title": "Crime Prediction by Data-Driven Green's Function method", "comments": "22 pages, 3 figure", "journal-ref": "International Journal of Forecasting, 2019", "doi": "10.1016/j.ijforecast.2019.06.005", "report-no": null, "categories": "stat.AP cs.CE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm that forecasts cascading events, by employing a\nGreen's function scheme on the basis of the self-exciting point process model.\nThis method is applied to open data of 10 types of crimes happened in Chicago.\nIt shows a good prediction accuracy superior to or comparable to the standard\nmethods which are the expectation-maximization method and prospective hotspot\nmaps method. We find a cascade influence of the crimes that has a long-time,\nlogarithmic tail; this result is consistent with an earlier study on\nburglaries. This long-tail feature cannot be reproduced by the other standard\nmethods. In addition, a merit of the Green's function method is the low\ncomputational cost in the case of high density of events and/or large amount of\nthe training data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 00:26:20 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 10:14:48 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 06:20:59 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2019 20:10:55 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kajita", "Mami", ""], ["Kajita", "Seiji", ""]]}, {"id": "1704.00351", "submitter": "Nina Golyandina E.", "authors": "Theodore Alexandrov, Nina Golyandina, David Holloway, Alex Shlemov,\n  and Alexander Spirov", "title": "Two-exponential models of gene expression patterns for noisy\n  experimental data", "comments": null, "journal-ref": "Journal of Computational Biology (2018) Vol. 25, No. 11, p.\n  1220-1230", "doi": "10.1089/cmb.2017.0063", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Spatial pattern formation of the primary anterior-posterior\nmorphogenetic gradient of the transcription factor Bicoid (Bcd) has been\nstudied experimentally and computationally for many years. Bcd specifies\npositional information for the downstream segmentation genes, affecting the fly\nbody plan. More recently, a number of researchers have focused on the\npatterning dynamics of the underlying bcd mRNA gradient, which is translated\ninto Bcd protein. New, more accurate techniques for visualizing bcd mRNA need\nto be combined with quantitative signal extraction techniques to reconstruct\nthe bcd mRNA distribution.\n  Results: Here, we present a robust technique for quantifying gradients with a\ntwo-exponential model. This approach: 1) has natural, biologically relevant\nparameters; and 2) is invariant to linear transformations of the data which can\narise due to variation in experimental conditions (e.g. microscope settings,\nnon-specific background signal). This allows us to quantify bcd mRNA gradient\nvariability from embryo to embryo (important for studying the robustness of\ndevelopmental regulatory networks); sort out atypical gradients; and classify\nembryos to developmental stage by quantitative gradient parameters.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 19:09:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Alexandrov", "Theodore", ""], ["Golyandina", "Nina", ""], ["Holloway", "David", ""], ["Shlemov", "Alex", ""], ["Spirov", "Alexander", ""]]}, {"id": "1704.00352", "submitter": "Dongmeng Liu", "authors": "Dongmeng Liu and Jinko Graham", "title": "Simple Measures of Individual Cluster-Membership Certainty for Hard\n  Partitional Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two probability-like measures of individual cluster-membership\ncertainty which can be applied to a hard partition of the sample such as that\nobtained from the Partitioning Around Medoids (PAM) algorithm, hierarchical\nclustering or k-means clustering. One measure extends the individual silhouette\nwidths and the other is obtained directly from the pairwise dissimilarities in\nthe sample. Unlike the classic silhouette, however, the measures behave like\nprobabilities and can be used to investigate an individual's tendency to belong\nto a cluster. We also suggest two possible ways to evaluate the hard partition.\nWe evaluate the performance of both measures in individuals with ambiguous\ncluster membership, using simulated binary datasets that have been partitioned\nby the PAM algorithm or continuous datasets that have been partitioned by\nhierarchical clustering and k-means clustering. For comparison, we also present\nresults from soft clustering algorithms such as soft analysis clustering\n(FANNY) and two model-based clustering methods. Our proposed measures perform\ncomparably to the posterior-probability estimators from either FANNY or the\nmodel-based clustering methods. We also illustrate the proposed measures by\napplying them to Fisher's classic iris data set.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 19:12:26 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 22:54:10 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Liu", "Dongmeng", ""], ["Graham", "Jinko", ""]]}, {"id": "1704.00436", "submitter": "Santosh Nannuru", "authors": "Santosh Nannuru, Kay L. Gemba, Peter Gerstoft, William S. Hodgkiss,\n  Christoph Mecklenbr\\\"auker", "title": "Sparse Bayesian learning with uncertainty models and multiple\n  dictionaries", "comments": "11 pages, 8 figures", "journal-ref": "Elsevier Signal Processing, Volume 159, June 2019, Pages 159-170", "doi": "10.1016/j.sigpro.2019.02.003", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Bayesian learning (SBL) has emerged as a fast and competitive method\nto perform sparse processing. The SBL algorithm, which is developed using a\nBayesian framework, approximately solves a non-convex optimization problem\nusing fixed point updates. It provides comparable performance and is\nsignificantly faster than convex optimization techniques used in sparse\nprocessing. We propose a signal model which accounts for dictionary mismatch\nand the presence of errors in the weight vector at low signal-to-noise ratios.\nA fixed point update equation is derived which incorporates the statistics of\nmismatch and weight errors. We also process observations from multiple\ndictionaries. Noise variances are estimated using stochastic maximum\nlikelihood. The derived update equations are studied quantitatively using\nbeamforming simulations applied to direction-of-arrival (DoA). Performance of\nSBL using single- and multi-frequency observations, and in the presence of\naliasing, is evaluated. SwellEx-96 experimental data demonstrates qualitatively\nthe advantages of SBL.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 06:05:44 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 23:05:05 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Nannuru", "Santosh", ""], ["Gemba", "Kay L.", ""], ["Gerstoft", "Peter", ""], ["Hodgkiss", "William S.", ""], ["Mecklenbr\u00e4uker", "Christoph", ""]]}, {"id": "1704.00522", "submitter": "Sean Yiu", "authors": "Sean Yiu, Vernon T. Farewell, Brian D. M. Tom", "title": "Clustered multi-state models with observation-level random effects,\n  mover-stayer effects and dynamic covariates: Modelling transition intensities\n  and sojourn times in a study of psoriatic arthritis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In psoriatic arthritis, it is important to understand the joint activity\n(represented by swelling and pain) and damage processes because both are\nrelated to severe physical disability. This paper aims to provide a\ncomprehensive investigation in to both processes occurring over time, in\nparticular their relationship, by specifying a joint multi-state model at the\nindividual hand joint-level, which also accounts for many of their important\nfeatures. As there are multiple hand joints, such an analysis will be based on\nthe use of clustered multi-state models. Here we consider an observation-level\nrandom effects structure with dynamic covariates and allow for the possibility\nthat a subpopulation of patients are at minimal risk of damage. Such an\nanalysis is found to provide further understanding of the activity-damage\nrelationship beyond that provided by previous analyses. Consideration is also\ngiven to the modelling of mean sojourn times and jump probabilities. In\nparticular, a novel model parameterization which allows easily interpretable\ncovariate effects to act on these quantities is proposed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:50:03 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Yiu", "Sean", ""], ["Farewell", "Vernon T.", ""], ["Tom", "Brian D. M.", ""]]}, {"id": "1704.00543", "submitter": "Jouni Helske", "authors": "Satu Helske, Jouni Helske", "title": "Mixture Hidden Markov Models for Sequence Data: The seqHMM Package in R", "comments": "33 pages, 8 figures", "journal-ref": "Journal of Statistical Software, 88(3), 1 - 32 (2019)", "doi": "10.18637/jss.v088.i03", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence analysis is being more and more widely used for the analysis of\nsocial sequences and other multivariate categorical time series data. However,\nit is often complex to describe, visualize, and compare large sequence data,\nespecially when there are multiple parallel sequences per subject. Hidden\n(latent) Markov models (HMMs) are able to detect underlying latent structures\nand they can be used in various longitudinal settings: to account for\nmeasurement error, to detect unobservable states, or to compress information\nacross several types of observations. Extending to mixture hidden Markov models\n(MHMMs) allows clustering data into homogeneous subsets, with or without\nexternal covariates.\n  The seqHMM package in R is designed for the efficient modeling of sequences\nand other categorical time series data containing one or multiple subjects with\none or multiple interdependent sequences using HMMs and MHMMs. Also other\nrestricted variants of the MHMM can be fitted, e.g., latent class models,\nMarkov models, mixture Markov models, or even ordinary multinomial regression\nmodels with suitable parameterization of the HMM. Good graphical presentations\nof data and models are useful during the whole analysis process from the first\nglimpse at the data to model fitting and presentation of results. The package\nprovides easy options for plotting parallel sequence data, and proposes\nvisualizing HMMs as directed graphs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 12:09:19 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 17:46:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Helske", "Satu", ""], ["Helske", "Jouni", ""]]}, {"id": "1704.00575", "submitter": "Emiliano Diaz", "authors": "Emiliano Diaz", "title": "Sparse mean localization by information theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse feature selection is necessary when we fit statistical models, we have\naccess to a large group of features, don't know which are relevant, but assume\nthat most are not. Alternatively, when the number of features is larger than\nthe available data the model becomes over parametrized and the sparse feature\nselection task involves selecting the most informative variables for the model.\nWhen the model is a simple location model and the number of relevant features\ndoes not grow with the total number of features, sparse feature selection\ncorresponds to sparse mean estimation. We deal with a simplified mean\nestimation problem consisting of an additive model with gaussian noise and mean\nthat is in a restricted, finite hypothesis space. This restriction simplifies\nthe mean estimation problem into a selection problem of combinatorial nature.\nAlthough the hypothesis space is finite, its size is exponential in the\ndimension of the mean. In limited data settings and when the size of the\nhypothesis space depends on the amount of data or on the dimension of the data,\nchoosing an approximation set of hypotheses is a desirable approach. Choosing a\nset of hypotheses instead of a single one implies replacing the bias-variance\ntrade off with a resolution-stability trade off. Generalization capacity\nprovides a resolution selection criterion based on allowing the learning\nalgorithm to communicate the largest amount of information in the data to the\nlearner without error. In this work the theory of approximation set coding and\ngeneralization capacity is explored in order to understand this approach. We\nthen apply the generalization capacity criterion to the simplified sparse mean\nestimation problem and detail an importance sampling algorithm which at once\nsolves the difficulty posed by large hypothesis spaces and the slow convergence\nof uniform sampling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:35:17 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Diaz", "Emiliano", ""]]}, {"id": "1704.00583", "submitter": "Shael Brown", "authors": "Shael Brown", "title": "A PageRank Model for Player Performance Assessment in Basketball, Soccer\n  and Hockey", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sports of soccer, hockey and basketball the most commonly used\nstatistics for player performance assessment are divided into two categories:\noffensive statistics and defensive statistics. However, qualitative assessments\nof playmaking (for example making \"smart\" passes) are difficult to quantify. It\nwould be advantageous to have available a single statistic that can emphasize\nthe flow of a game, rewarding those players who initiate and contribute to\nsuccessful plays more. In this paper we will examine a model based on Google's\nPageRank. Other papers have explored ranking teams, coaches, and captains but\nhere we construct ratings and rankings for individual members on both teams\nthat emphasizes initiating and partaking in successful plays and forcing\ndefensive turnovers. For a soccer/hockey/basketball game, our model assigns a\nnode for each of the n players who play in the game and a \"goal node\". Arcs\nbetween player nodes indicate sport-specific situations (including passes,\nturnovers, scoring, fouls, out-of-bounds, play-stoppages, turnovers, missed\nshots, defensive plays etc.), tailored for each sport. As well, some additional\narcs are added in to ensure that the associated matrix is primitive and hence\nthere is a unique PageRank vector. The PageRank vector of the associated matrix\nis used to rank the players of the game. To illustrate the model, data was\ntaken from nine NBA games played between 2014-2016. Many of the top-ranked\nplayers (in the model) in a given game had some of the most impressive\ntraditional stat-lines. However, from the model there were surprises where some\nplayers who had impressive stat-lines had lower ranks, and others who had less\nimpressive stat-lines had higher ranks. Overall, the model's ranking and\nratings reflect more the flow of the game compared to traditional sports\nstatistics.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 16:03:40 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Brown", "Shael", ""]]}, {"id": "1704.00587", "submitter": "Salima El", "authors": "Salima El Kolei (ENSAI), Fr\\'ed\\'eric Patras (JAD)", "title": "Analysis, detection and correction of misspecified discrete time state\n  space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misspecifications (i.e. errors on the parameters) of state space models lead\nto incorrect inference of the hidden states. This paper studies weakly\nnonlin-ear state space models with additive Gaussian noises and proposes a\nmethod for detecting and correcting misspecifications. The latter induce a\nbiased estimator of the hidden state but also happen to induce correlation on\ninnovations and other residues. This property is used to find a well-defined\nobjective function for which an optimisation routine is applied to recover the\ntrue parameters of the model. It is argued that this method can consistently\nestimate the bias on the parameter. We demonstrate the algorithm on various\nmodels of increasing complexity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:49:25 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Kolei", "Salima El", "", "ENSAI"], ["Patras", "Fr\u00e9d\u00e9ric", "", "JAD"]]}, {"id": "1704.00588", "submitter": "Emiliano Diaz", "authors": "Emiliano Diaz", "title": "Causality and surrogate variable analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression depends on thousands of factors and we usually only have\naccess to tens or hundreds of observations of gene expression levels meaning we\nare in a high-dimensional setting. Additionally we don't always observe or care\nabout all the factors. However, many different gene expression levels depend on\na set of common factors. By observing the joint variance of the gene expression\nlevels together with the observed primary variables (those we care about)\nSurrogate Variable Analysis (SVA) seeks to estimate the remaining unobserved\nfactors. The ultimate goal is to assess whether the primary variable (or\nvector) has a significant effect on the different gene expression levels, but\nwithout estimating unobserved factors first the various regression models and\nhypothesis tests are dependent which complicates significance analysis. In this\nwork we define a class of additive gene expression structural equation models\n(SEMs) which are convenient for modeling gene expression data and which\nprovides a useful framework to understand the various steps of the SVA\nmethodology. We justify the use of this class from a modeling viewpoint but\nalso from a causality viewpoint by exploring the independence and causality\nproperties of this class and comparing to the biologically driven data\nassumptions. For this we use some of the theory that has been developed\nelsewhere on graphical models and causality. We then give a detailed\ndescription of the SVA methodology and its implementation in the R package sva\nreferring each step to different parts of the additive gene expression SEM\ndefined previously.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:51:50 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Diaz", "Emiliano", ""]]}, {"id": "1704.00665", "submitter": "Toshiki Sato", "authors": "Toshiki Sato, Yuichi Takano, Takanobu Nakahara", "title": "Investigating consumers' store-choice behavior via hierarchical variable\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a store-choice model for investigating\nconsumers' store-choice behavior based on scanner panel data. Our store-choice\nmodel enables us to evaluate the effects of the consumer/product attributes not\nonly on the consumer's store choice but also on his/her purchase quantity.\nMoreover, we adopt a mixed-integer optimization (MIO) approach to selecting the\nbest set of explanatory variables with which to construct a store-choice model.\nWe devise two MIO models for hierarchical variable selection in which the\nhierarchical structure of product categories is used to enhance the reliability\nand computational efficiency of the variable selection. We assess the\neffectiveness of our MIO models through computational experiments on actual\nscanner panel data. These experiments are focused on the consumer's choice\namong three types of stores in Japan: convenience stores, drugstores, and\ngrocery supermarkets. The computational results demonstrate that our method has\nseveral advantages over the common methods for variable selection, namely, the\nstepwise method and $L_1$-regularized regression. Furthermore, our analysis\nreveals that convenience stores tend to be chosen because of accessibility,\ndrugstores are chosen for the purchase of specific products at low prices, and\ngrocery supermarkets are chosen for health food products by women with\nfamilies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 16:25:19 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sato", "Toshiki", ""], ["Takano", "Yuichi", ""], ["Nakahara", "Takanobu", ""]]}, {"id": "1704.00823", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande, Abraham J. Wyner", "title": "A Hierarchical Bayesian Model of Pitch Framing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of high-resolution pitch tracking data (PITCHf/x), many in\nthe sabermetrics community have attempted to quantify a Major League Baseball\ncatcher's ability to \"frame\" a pitch (i.e. increase the chance that a pitch is\ncalled as a strike). Especially in the last three years, there has been an\nexplosion of interest in the \"art of pitch framing\" in the popular press as\nwell as signs that teams are considering framing when making roster decisions.\n  We introduce a Bayesian hierarchical model to estimate each umpire's\nprobability of calling a strike, adjusting for pitch participants, pitch\nlocation, and contextual information like the count. Using our model, we can\nestimate each catcher's effect on an umpire's chance of calling a strike.We are\nthen able to translate these estimated effects into average runs saved across a\nseason. We also introduce a new metric, analogous to Jensen, Shirley, and\nWyner's Spatially Aggregate Fielding Evaluation metric, which provides a more\nhonest assessment of the impact of framing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 21:56:29 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 21:32:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Wyner", "Abraham J.", ""]]}, {"id": "1704.00829", "submitter": "Emiliano Diaz", "authors": "Emiliano Diaz", "title": "Online deforestation detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deforestation detection using satellite images can make an important\ncontribution to forest management. Current approaches can be broadly divided\ninto those that compare two images taken at similar periods of the year and\nthose that monitor changes by using multiple images taken during the growing\nseason. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that\nbuilds on the latter category by implementing a year-long, continuous,\ntime-series based approach to monitoring images. This algorithm was developed\nfor 30m resolution, 16-day frequency reflectance data from the Landsat\nsatellite. In this work we adapt the algorithm to 1km, 16-day frequency\nreflectance data from the modis sensor aboard the Terra satellite. The CMFDA\nalgorithm is composed of two submodels which are fitted on a pixel-by-pixel\nbasis. The first estimates the amount of surface reflectance as a function of\nthe day of the year. The second estimates the occurrence of a deforestation\nevent by comparing the last few predicted and real reflectance values. For this\ncomparison, the reflectance observations for six different bands are first\ncombined into a forest index. Real and predicted values of the forest index are\nthen compared and high absolute differences for consecutive observation dates\nare flagged as deforestation events. Our adapted algorithm also uses the two\nmodel framework. However, since the modis 13A2 dataset used, includes\nreflectance data for different spectral bands than those included in the\nLandsat dataset, we cannot construct the forest index. Instead we propose two\ncontrasting approaches: a multivariate and an index approach similar to that of\nCMFDA.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 22:40:48 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Diaz", "Emiliano", ""]]}, {"id": "1704.00953", "submitter": "Daniel Kraus", "authors": "Matthias Fischer, Daniel Kraus, Marius Pfeuffer, Claudia Czado", "title": "Stress Testing German Industry Sectors: Results from a Vine Copula Based\n  Quantile Regression", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring interdependence between probabilities of default (PDs) in different\nindustry sectors of an economy plays a crucial role in financial stress\ntesting. Thereby, regression approaches may be employed to model the impact of\nstressed industry sectors as covariates on other response sectors. We identify\nvine copula based quantile regression as an eligible tool for conducting such\nstress tests as this method has good robustness properties, takes into account\npotential nonlinearities of conditional quantile functions and ensures that no\nquantile crossing effects occur. We illustrate its performance by a data set of\nsector specific PDs for the German economy. Empirical results are provided for\na rough and a fine-grained industry sector classification scheme. Amongst\nothers, we confirm that a stressed automobile industry has a severe impact on\nthe German economy as a whole at different quantile levels whereas e.g., for a\nstressed financial sector the impact is rather moderate. Moreover, the vine\ncopula based quantile regression approach is benchmarked against both classical\nlinear quantile regression and expectile regression in order to illustrate its\nmethodological effectiveness in the scenarios evaluated.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 10:59:25 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 14:15:27 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Fischer", "Matthias", ""], ["Kraus", "Daniel", ""], ["Pfeuffer", "Marius", ""], ["Czado", "Claudia", ""]]}, {"id": "1704.00959", "submitter": "Christian Hennig", "authors": "Daniel M\\\"ullensiefen, Christian Hennig, Hedie Howells", "title": "Using clustering of rankings to explain brand preferences with\n  personality and socio-demographic variables", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary aim of market segmentation is to identify relevant groups of\nconsumers that can be addressed efficiently by marketing or advertising\ncampaigns. This paper addresses the issue whether consumer groups can be\nidentified from background variables that are not brand-related and how much\npersonality vs. socio-demographic variables contribute to the identification of\nconsumer clusters. This is done by clustering aggregated preferences for 25\nbrands across 5 different product categories, and by relating socio-demographic\nand personality variables to the clusters using logistic regression and random\nforests over a range of different numbers of clusters. Results indicate that\nsome personality variables contribute significantly to the identification of\nconsumer groups in one sample. However, these results were not replicated on a\nsecond sample that was more heterogeneous in terms of socio-demographic\ncharacteristics and not representative of the brands' target audience.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 11:26:40 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["M\u00fcllensiefen", "Daniel", ""], ["Hennig", "Christian", ""], ["Howells", "Hedie", ""]]}, {"id": "1704.01207", "submitter": "Bo Chen", "authors": "Bo Chen, Radu V. Craiu and Lei Sun", "title": "Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in\n  Genetic Association Study", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-chromosome is often excluded from the so called `whole-genome' association\nstudies due to its intrinsic difference between males and females. One\nparticular analytical challenge is the unknown status of X-inactivation, where\none of the two X-chromosome variants in females may be randomly selected to be\nsilenced. In the absence of biological evidence in favour of one specific\nmodel, we consider a Bayesian model averaging framework that offers a\nprincipled way to account for the inherent model uncertainty, providing model\naveraging-based posterior density intervals and Bayes factors. We examine the\ninferential properties of the proposed methods via extensive simulation\nstudies, and we apply the methods to a genetic association study of an\nintestinal disease occurring in about twenty percent of Cystic Fibrosis\npatients. Compared with the results previously reported assuming the presence\nof inactivation, we show that the proposed Bayesian methods provide more\nfeature-rich quantities that are useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 22:34:49 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 18:05:00 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Bo", ""], ["Craiu", "Radu V.", ""], ["Sun", "Lei", ""]]}, {"id": "1704.01220", "submitter": "Parvez Ahammad", "authors": "Qingzhu Gao, Prasenjit Dey, and Parvez Ahammad", "title": "Perceived Performance of Webpages In the Wild: Insights from Large-scale\n  Crowdsourcing of Above-the-Fold QoE", "comments": "6 pages, 5 figures, submitted to ACM SIGCOMM 2nd Workshop on\n  QoE-based Analysis and Management of Data Communication Networks\n  (Internet-QoE 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clearly, no one likes webpages with poor quality of experience (QoE). Being\nperceived as slow or fast is a key element in the overall perceived QoE of web\napplications. While extensive effort has been put into optimizing web\napplications (both in industry and academia), not a lot of work exists in\ncharacterizing what aspects of webpage loading process truly influence human\nend-user's perception of the \"Speed\" of a page. In this paper we present\n\"SpeedPerception\", a large-scale web performance crowdsourcing framework\nfocused on understanding the perceived loading performance of above-the-fold\n(ATF) webpage content. Our end goal is to create free open-source benchmarking\ndatasets to advance the systematic analysis of how humans perceive webpage\nloading process. In Phase-1 of our \"SpeedPerception\" study using Internet\nRetailer Top 500 (IR 500) websites\n(https://github.com/pahammad/speedperception), we found that commonly used\nnavigation metrics such as \"onLoad\" and \"Time To First Byte (TTFB)\" fail (less\nthan 60% match) to represent majority human perception when comparing the speed\nof two webpages. We present a simple 3-variable-based machine learning model\nthat explains the majority end-user choices better (with $87 \\pm 2\\%$\naccuracy). In addition, our results suggest that the time needed by end-users\nto evaluate relative perceived speed of webpage is far less than the time of\nits \"visualComplete\" event.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:47:41 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Gao", "Qingzhu", ""], ["Dey", "Prasenjit", ""], ["Ahammad", "Parvez", ""]]}, {"id": "1704.01469", "submitter": "Thomas Nichols", "authors": "Thomas E. Nichols", "title": "Notes on Creating a Standardized Version of DVARS", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By constructing a sampling distribution for DVARS we can create a\nstandardized version of DVARS that should be more similar across scanners and\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 15:02:26 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Nichols", "Thomas E.", ""]]}, {"id": "1704.01932", "submitter": "Emiliano Diaz", "authors": "Emiliano D\\'iaz", "title": "Estimaci\\'on de la inicial de referencia utilizando simulaci\\'on", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method proposed by Bernardo and Smith [2000] to approximate reference\npriors by simulation was analyzed with the objective of improving the procedure\nin order to obtain consistent estimators and to allow the estimation of\nasymptotic probability intervals. In this sense, the variance of Bernardo's\nestimator was derived and was used to construct probability intervals that\npermitted the expression of the estimation error as a function of sample size.\nAdditionally a variance reduction technique (common random numbers) were\nexplored as a means to obtain more precise estimations with smaller sample\nsizes. These technique was found to considerably reduce estimation error for\nsome of the examples explored. In other cases the use of the technique resulted\nin zero estimation error given that the estimator does not depend on the\nsample.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 14:25:10 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["D\u00edaz", "Emiliano", ""]]}, {"id": "1704.01978", "submitter": "Marina Valdora", "authors": "Julieta Molina, Mariela Sued, Marina Valdora", "title": "Models for the Propensity Score that Contemplate the Positivity\n  Assumption and their Application to Missing Data and Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models are often assumed to fit propensity scores, which\nare used to compute inverse probability weighted (IPW) estimators. In order to\nderive the asymptotic properties of IPW estimators, the propensity score is\nsupposed to be bounded away from cero. This condition is known in the\nliterature as strict positivity (or positivity assumption) and, in practice,\nwhen it does not hold, IPW estimators are very unstable and have a large\nvariability. Although strict positivity is often assumed, it is not upheld when\nsome of the covariates are continuous. In this work, we attempt to conciliate\nbetween the strict positivity condition and the theory of generalized linear\nmodels by incorporating an extra parameter, which results in an explicit lower\nbound for the propensity scores.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 18:10:45 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 12:15:20 GMT"}, {"version": "v3", "created": "Sun, 23 Apr 2017 12:31:11 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Molina", "Julieta", ""], ["Sued", "Mariela", ""], ["Valdora", "Marina", ""]]}, {"id": "1704.02069", "submitter": "Eric Lock", "authors": "Adam Kaplan and Eric F. Lock", "title": "Prediction with Dimension Reduction of Multiple Molecular Data Sources\n  for Patient Survival", "comments": "11 pages, 9 figures", "journal-ref": "Cancer Informatics, 16: 1-11, 2017", "doi": "10.1177/1176935117718517", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling from high-dimensional genomic data is often preceded by a\ndimension reduction step, such as principal components analysis (PCA). However,\nthe application of PCA is not straightforward for multi-source data, wherein\nmultiple sources of 'omics data measure different but related biological\ncomponents. In this article we utilize recent advances in the dimension\nreduction of multi-source data for predictive modeling. In particular, we apply\nexploratory results from Joint and Individual Variation Explained (JIVE), an\nextension of PCA for multi-source data, for prediction of differing response\ntypes. We conduct illustrative simulations to illustrate the practical\nadvantages and interpretability of our approach. As an application example we\nconsider predicting survival for Glioblastoma Multiforme (GBM) patients from\nthree data sources measuring mRNA expression, miRNA expression, and DNA\nmethylation. We also introduce a method to estimate JIVE scores for new samples\nthat were not used in the initial dimension reduction, and study its\ntheoretical properties; this method is implemented in the R package R.JIVE on\nCRAN, in the function 'jive.predict'.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 02:01:32 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 22:13:32 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Kaplan", "Adam", ""], ["Lock", "Eric F.", ""]]}, {"id": "1704.02283", "submitter": "Joshua Whitlinger", "authors": "Joshua Whitlinger, Edward L Boone, Ryad Ghanam", "title": "A Bayesian Estimation for the Fractional Order of the Differential\n  Equation that Models Transport in Unconventional Hydrocarbon Reservoirs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of natural gas from the earth has been shown to be governed by\ndifferential equations concerning flow through a porous material. Recently,\nmodels such as fractional differential equations have been developed to model\nthis phenomenon. One key issue with these models is estimating the fraction of\nthe differential equation. Traditional methods such as maximum likelihood,\nleast squares and even method of moments are not available to estimate this\nparameter as traditional calculus methods do not apply. We develop a Bayesian\napproach to estimate the fraction of the order of the differential equation\nthat models transport in unconventional hydrocarbon reservoirs. In this paper,\nwe use this approach to adequately quantify the uncertainties associated with\nthe error and predictions. A simulation study is presented as well to assess\nthe utility of the modeling approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 16:32:57 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 18:15:13 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 13:55:18 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Whitlinger", "Joshua", ""], ["Boone", "Edward L", ""], ["Ghanam", "Ryad", ""]]}, {"id": "1704.02664", "submitter": "Dhruv Madeka", "authors": "Dhruv Madeka", "title": "Accurate Prediction of Electoral Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel methods for predicting the outcome of large elections. Our\nfirst algorithm uses a diffusion process to model the time uncertainty inherent\nin polls taken with substantial calendar time left to the election. Our second\nmodel uses Online Learning along with a novel ex-ante scoring function to\ncombine different forecasters along with our first model. We evaluate different\ndensity based scoring functions that can be used to better judge the efficacy\nof forecasters. We also propose scoring functions which take into account the\nentire density of the forecast rather than just a point estimate of the value.\nFinally, we consider this framework as a way to improve and judge different\nmodels performing a prediction on the same task.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 21:52:18 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 03:15:54 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Madeka", "Dhruv", ""]]}, {"id": "1704.02739", "submitter": "Yunqi Bu", "authors": "Yunqi Bu and Johannes Lederer", "title": "Integrating Additional Knowledge Into Estimation of Graphical Models", "comments": "16 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of graphical models, we typically have more information than\njust the samples themselves. A prime example is the estimation of brain\nconnectivity networks based on fMRI data, where in addition to the samples\nthemselves, the spatial positions of the measurements are readily available.\nWith particular regard for this application, we are thus interested in ways to\nincorporate additional knowledge most effectively into graph estimation. Our\napproach to this is to make neighborhood selection receptive to additional\nknowledge by strengthening the role of the tuning parameters. We demonstrate\nthat this concept (i) can improve reproducibility, (ii) is computationally\nconvenient and efficient, and (iii) carries a lucid Bayesian interpretation. We\nspecifically show that the approach provides effective estimations of brain\nconnectivity graphs from fMRI data. However, providing a general scheme for the\ninclusion of additional knowledge, our concept is expected to have applications\nin a wide range of domains.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 07:33:54 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 04:26:11 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bu", "Yunqi", ""], ["Lederer", "Johannes", ""]]}, {"id": "1704.03070", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz", "title": "Online Nonparametric Anomaly Detection based on Geometric Entropy\n  Minimization", "comments": "to appear in IEEE International Symposium on Information Theory\n  (ISIT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online and nonparametric detection of abrupt and persistent\nanomalies, such as a change in the regular system dynamics at a time instance\ndue to an anomalous event (e.g., a failure, a malicious activity). Combining\nthe simplicity of the nonparametric Geometric Entropy Minimization (GEM) method\nwith the timely detection capability of the Cumulative Sum (CUSUM) algorithm we\npropose a computationally efficient online anomaly detection method that is\napplicable to high-dimensional datasets, and at the same time achieve a\nnear-optimum average detection delay performance for a given false alarm\nconstraint. We provide new insights to both GEM and CUSUM, including new\nasymptotic analysis for GEM, which enables soft decisions for outlier\ndetection, and a novel interpretation of CUSUM in terms of the discrepancy\ntheory, which helps us generalize it to the nonparametric GEM statistic. We\nnumerically show, using both simulated and real datasets, that the proposed\nnonparametric algorithm attains a close performance to the clairvoyant\nparametric CUSUM test.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 22:26:08 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 16:46:31 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Yilmaz", "Yasin", ""]]}, {"id": "1704.03177", "submitter": "Sezen Cekic", "authors": "Sezen Cekic, Didier Grandjean and Olivier Renaud", "title": "Time, Frequency & Time-Varying Causality Measures in Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a systematic methodological review and objective\ncriticism of existing methods enabling the derivation of time-varying\nGranger-causality statistics in neuroscience. The increasing interest and the\nhuge number of publications related to this topic calls for this systematic\nreview which describes the very complex methodological aspects. The capacity to\ndescribe the causal links between signals recorded at different brain locations\nduring a neuroscience experiment is of primary interest for neuroscientists,\nwho often have very precise prior hypotheses about the relationships between\nrecorded brain signals that arise at a specific time and in a specific\nfrequency band. The ability to compute a time-varying frequency-specific\ncausality statistic is therefore essential. Two steps are necessary to achieve\nthis: the first consists of finding a statistic that can be interpreted and\nthat directly answers the question of interest. The second concerns the model\nthat underlies the causality statistic and that has this time-frequency\nspecific causality interpretation. In this article, we will review\nGranger-causality statistics with their spectral and time-varying extensions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 07:40:07 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Cekic", "Sezen", ""], ["Grandjean", "Didier", ""], ["Renaud", "Olivier", ""]]}, {"id": "1704.03239", "submitter": "Gregor Kastner", "authors": "Gregor Kastner and Florian Huber", "title": "Sparse Bayesian vector autoregressions in huge dimensions", "comments": null, "journal-ref": "Journal of Forecasting (2020)", "doi": "10.1002/for.2680", "report-no": null, "categories": "stat.CO econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian vector autoregressive (VAR) model with multivariate\nstochastic volatility that is capable of handling vast dimensional information\nsets. Three features are introduced to permit reliable estimation of the model.\nFirst, we assume that the reduced-form errors in the VAR feature a factor\nstochastic volatility structure, allowing for conditional equation-by-equation\nestimation. Second, we apply recently developed global-local shrinkage priors\nto the VAR coefficients to cure the curse of dimensionality. Third, we utilize\nrecent innovations to efficiently sample from high-dimensional multivariate\nGaussian distributions. This makes simulation-based fully Bayesian inference\nfeasible when the dimensionality is large but the time series length is\nmoderate. We demonstrate the merits of our approach in an extensive simulation\nstudy and apply the model to US macroeconomic data to evaluate its forecasting\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 11:14:41 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 16:04:35 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 07:29:27 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kastner", "Gregor", ""], ["Huber", "Florian", ""]]}, {"id": "1704.03346", "submitter": "Caifa Zhou", "authors": "Yang Gu, Caifa Zhou, Andreas Wieser, Zhimin Zhou", "title": "Pedestrian Positioning Using WiFi Fingerprints and a Foot-mounted\n  Inertial Sensor", "comments": "12 figures. Accepted by European Navigation Conference 2017, Lausanne", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foot-mounted inertial positioning (FMIP) and fingerprinting based WiFi indoor\npositioning (FWIP) are two promising solutions for indoor positioning. However,\nFMIP suffers from accumulative positioning errors in the long term while FWIP\ninvolves a very labor-intensive offline training phase. A new approach\ncombining the two solutions is proposed in this paper, which can limit the\nerror growth in FMIP and is free of any offline site survey phase. This\napproach is realized in the framework of a particle filter, where each particle\ndenotes a potential trajectory of the user and is weighted according to its\nconsistency in signal strength space. Compared with the traditional Gaussian\nprocess based approaches, the proposed one has less computational cost and is\nfree from any prior information in the position domain, such as the positions\nof access points, received signal strengths at certain positions and so on. An\nexperiment is carried out to demonstrate the performance of the proposed\napproach compared to the traditional Gaussian process based approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:07:39 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Gu", "Yang", ""], ["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""], ["Zhou", "Zhimin", ""]]}, {"id": "1704.03360", "submitter": "Jonathan C. Mattingly", "authors": "Sachet Bangia, Christy Vaughn Graves, Gregory Herschlag, Han Sung\n  Kang, Justin Luo, Jonathan C. Mattingly and Robert Ravier", "title": "Redistricting: Drawing the Line", "comments": "Corrected typos from previous version; added new plots showing\n  stability; corrected error in EG plots and analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods to evaluate whether a political districting accurately\nrepresents the will of the people. To explore and showcase our ideas, we\nconcentrate on the congressional districts for the U.S. House of\nrepresentatives and use the state of North Carolina and its redistrictings\nsince the 2010 census. Using a Monte Carlo algorithm, we randomly generate over\n24,000 redistrictings that are non-partisan and adhere to criteria from\nproposed legislation. Applying historical voting data to these random\nredistrictings, we find that the number of democratic and republican\nrepresentatives elected varies drastically depending on how districts are\ndrawn. Some results are more common, and we gain a clear range of expected\nelection outcomes. Using the statistics of our generated redistrictings, we\ncritique the particular congressional districtings used in the 2012 and 2016 NC\nelections as well as a districting proposed by a bipartisan redistricting\ncommission. We find that the 2012 and 2016 districtings are highly atypical and\nnot representative of the will of the people. On the other hand, our results\nindicate that a plan produced by a bipartisan panel of retired judges is highly\ntypical and representative. Since our analyses are based on an ensemble of\nreasonable redistrictings of North Carolina, they provide a baseline for a\ngiven election which incorporates the geometry of the state's population\ndistribution.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 13:48:50 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 14:13:47 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bangia", "Sachet", ""], ["Graves", "Christy Vaughn", ""], ["Herschlag", "Gregory", ""], ["Kang", "Han Sung", ""], ["Luo", "Justin", ""], ["Mattingly", "Jonathan C.", ""], ["Ravier", "Robert", ""]]}, {"id": "1704.03380", "submitter": "Leonardo Bennun LB", "authors": "Leonardo Bennun", "title": "An ad-hoc modified Likelihood Function Applied to Optimization of Data\n  Analysis in Atomic Spectroscopy", "comments": "10 pages 4 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an ad-hoc construction of the Likelihood Function,\nin order to develop a data analysis procedure, to be applied in atomic and\nnuclear spectral analysis. The classical Likelihood Function was modified\ntaking into account the underlying statistics of the phenomena studied, by the\ninspection of the residues of the fitting, which should behave with specific\nstatistical properties. This new formulation was analytically developed, but\nthe sought parameter should be evaluated numerically, since it cannot be\nobtained as a function of each one of the independent variables. For this\nsimple numerical evaluation, along with the acquired data, we also should\nprocess many sets of external data, with specific properties - This new data\nshould be uncorrelated with the acquired signal. The developed statistical\nmethod was evaluated using computer simulated spectra. The numerical\nestimations of the calculated parameter applying this method, indicate an\nimprovement over accuracy and precision, being one order of magnitude better\nthan those produced by least squares approaches. We still have to evaluate the\nimprovement produced by this method over Detection and Quantitation Limits, in\nTXRF spectral analysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:54:30 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Bennun", "Leonardo", ""]]}, {"id": "1704.03458", "submitter": "Jinsung Yoon", "authors": "J. Yoon, W. R. Zame, A. Banerjee, M. Cadeiras, A. M. Alaa, M. van der\n  Schaar", "title": "Personalized Survival Predictions for Cardiac Transplantation via Trees\n  of Predictors", "comments": "Main manuscript: 20 pages, Supplementary materials: 13 pages, 5\n  figures, 3 tables. Submitted to Science Translational Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the limited pool of donor organs, accurate predictions of survival on\nthe wait list and post transplantation are crucial for cardiac transplantation\ndecisions and policy. However, current clinical risk scores do not yield\naccurate predictions. We develop a new methodology (ToPs, Trees of Predictors)\nbuilt on the principle that specific predictors should be used for specific\nclusters within the target population. ToPs discovers these specific clusters\nof patients and the specific predictor that perform best for each cluster. In\ncomparison with current clinical risk scoring systems, our method provides\nsignificant improvements in the prediction of survival time on the wait list\nand post transplantation. For example, in terms of 3 month survival for\npatients who were on the US patient wait list in the period 1985 to 2015, our\nmethod achieves AUC of 0.847, the best commonly used clinical risk score\n(MAGGIC) achieves 0.630. In terms of 3 month survival/mortality predictions (in\ncomparison to MAGGIC), holding specificity at 80.0 percents, our algorithm\ncorrectly predicts survival for 1,228 (26.0 percents more patients out of 4,723\nwho actually survived, holding sensitivity at 80.0 percents, our algorithm\ncorrectly predicts mortality for 839 (33.0 percents) more patients out of 2,542\nwho did not survive. Our method achieves similar improvements for other time\nhorizons and for predictions post transplantation. Therefore, we offer a more\naccurate, personalized approach to survival analysis that can benefit patients,\nclinicians and policymakers in making clinical decisions and setting clinical\npolicy. Because risk prediction is widely used in diagnostic and prognostic\nclinical decision making across diseases and clinical specialties, the\nimplications of our methods are far reaching.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:06:08 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Yoon", "J.", ""], ["Zame", "W. R.", ""], ["Banerjee", "A.", ""], ["Cadeiras", "M.", ""], ["Alaa", "A. M.", ""], ["van der Schaar", "M.", ""]]}, {"id": "1704.03731", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich and Markus Pauly", "title": "MATS: Inference for potentially Singular and Heteroscedastic MANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many experiments in the life sciences, several endpoints are recorded per\nsubject. The analysis of such multivariate data is usually based on MANOVA\nmodels assuming multivariate normality and covariance homogeneity. These\nassumptions, however, are often not met in practice. Furthermore, test\nstatistics should be invariant under scale transformations of the data, since\nthe endpoints may be measured on different scales. In the context of\nhigh-dimensional data, Srivastava and Kubokawa (2013) proposed such a test\nstatistic for a specific one-way model, which, however, relies on the\nassumption of a common non-singular covariance matrix. We modify and extend\nthis test statistic to factorial MANOVA designs, incorporating general\nheteroscedastic models. In particular, our only distributional assumption is\nthe existence of the group-wise covariance matrices, which may even be\nsingular. We base inference on quantiles of resampling distributions, and\nderive confidence regions and ellipsoids based on these quantiles. In a\nsimulation study, we extensively analyze the behavior of these procedures.\nFinally, the methods are applied to a data set containing information on the\n2016 presidential elections in the USA with unequal and singular empirical\ncovariance matrices.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 12:43:29 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:03:05 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Friedrich", "Sarah", ""], ["Pauly", "Markus", ""]]}, {"id": "1704.03876", "submitter": "Bruno Sudret", "authors": "C. Mai, K. Konakli and B. Sudret", "title": "Seismic fragility curves for structures using non-parametric\n  representations", "comments": "arXiv admin note: substantial text overlap with arXiv:1403.5481", "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-003", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fragility curves are commonly used in civil engineering to assess the\nvulnerability of structures to earthquakes. The probability of failure\nassociated with a prescribed criterion (e.g. the maximal inter-storey drift of\na building exceeding a certain threshold) is represented as a function of the\nintensity of the earthquake ground motion (e.g. peak ground acceleration or\nspectral acceleration). The classical approach relies on assuming a lognormal\nshape of the fragility curves; it is thus parametric. In this paper, we\nintroduce two non-parametric approaches to establish the fragility curves\nwithout employing the above assumption, namely binned Monte Carlo simulation\nand kernel density estimation. As an illustration, we compute the fragility\ncurves for a three-storey steel frame using a large number of synthetic ground\nmotions. The curves obtained with the non-parametric approaches are compared\nwith respective curves based on the lognormal assumption. A similar comparison\nis presented for a case when a limited number of recorded ground motions is\navailable. It is found that the accuracy of the lognormal curves depends on the\nground motion intensity measure, the failure criterion and most importantly, on\nthe employed method for estimating the parameters of the lognormal shape.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 14:50:41 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Mai", "C.", ""], ["Konakli", "K.", ""], ["Sudret", "B.", ""]]}, {"id": "1704.04087", "submitter": "Moritz Berger Dr.", "authors": "Moritz Berger and Matthias Schmid", "title": "Semiparametric Regression for Discrete Time-to-Event Data", "comments": "35 pages, 2 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-to-event models are a popular tool to analyse data where the outcome\nvariable is the time to the occurrence of a specific event of interest. Here we\nfocus on the analysis of time-to-event outcomes that are either intrisically\ndiscrete or grouped versions of continuous event times. In the literature,\nthere exists a variety of regression methods for such data. This tutorial\nprovides an introduction to how these models can be applied using open source\nstatistical software. In particular, we consider semiparametric extensions\ncomprising the use of smooth nonlinear functions and tree-based methods. All\nmethods are illustrated by data on the duration of unemployment of U.S.\ncitizens.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 12:30:27 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Berger", "Moritz", ""], ["Schmid", "Matthias", ""]]}, {"id": "1704.04355", "submitter": "Andrew Francis", "authors": "Guilherme S. Rodrigues, Andrew R. Francis, Scott A. Sisson, Mark M.\n  Tanaka", "title": "Inferences on the acquisition of multidrug resistance in\n  \\emph{Mycobacterium tuberculosis} using molecular epidemiological data", "comments": "32 pages, 6 figures. This manuscript will appear as a chapter in the\n  Handbook of Approximate Bayesian Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the rates of drug resistance acquisition in a natural\npopulation using molecular epidemiological data from Bolivia. First, we study\nthe rate of direct acquisition of double resistance from the double sensitive\nstate within patients and compare it to the rates of evolution to single\nresistance. In particular, we address whether or not double resistance can\nevolve directly from a double sensitive state within a given host. Second, we\naim to understand whether the differences in mutation rates to rifampicin and\nisoniazid resistance translate to the epidemiological scale. Third, we estimate\nthe proportion of MDR TB cases that are due to the transmission of MDR strains\ncompared to acquisition of resistance through evolution. To address these\nproblems we develop a model of TB transmission in which we track the evolution\nof resistance to two drugs and the evolution of VNTR loci. However, the\navailable data is incomplete, in that it is recorded only {for a fraction of\nthe population and} at a single point in time. The likelihood function induced\nby the proposed model is computationally prohibitive to evaluate and\naccordingly impractical to work with directly. We therefore approach\nstatistical inference using approximate Bayesian computation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 05:53:58 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Rodrigues", "Guilherme S.", ""], ["Francis", "Andrew R.", ""], ["Sisson", "Scott A.", ""], ["Tanaka", "Mark M.", ""]]}, {"id": "1704.04376", "submitter": "Guillaume Bouleux", "authors": "Guillaume Bouleux and R\\'emy Boyer", "title": "Sparse-Based Estimation Performance for Partially Known Overcomplete\n  Large-Systems", "comments": "10 pages, 5 figures, Journal of Signal Processing", "journal-ref": null, "doi": "10.1016/j.sigpro.2017.04.010", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume the direct sum <A> o <B> for the signal subspace. As a result of\npost- measurement, a number of operational contexts presuppose the a priori\nknowledge of the LB -dimensional \"interfering\" subspace <B> and the goal is to\nestimate the LA am- plitudes corresponding to subspace <A>. Taking into account\nthe knowledge of the orthogonal \"interfering\" subspace <B>\\perp, the Bayesian\nestimation lower bound is de-\nrivedfortheLA-sparsevectorinthedoublyasymptoticscenario,i.e. N,LA,LB -> \\infty\nwith a finite asymptotic ratio. By jointly exploiting the Compressed Sensing\n(CS) and the Random Matrix Theory (RMT) frameworks, closed-form expressions for\nthe lower bound on the estimation of the non-zero entries of a sparse vector of\ninterest are derived and studied. The derived closed-form expressions enjoy\nseveral interesting features: (i) a simple interpretable expression, (ii) a\nvery low computational cost especially in the doubly asymptotic scenario, (iii)\nan accurate prediction of the mean-square-error (MSE) of popular sparse-based\nestimators and (iv) the lower bound remains true for any amplitudes vector\npriors. Finally, several idealized scenarios are compared to the derived bound\nfor a common output signal-to-noise-ratio (SNR) which shows the in- terest of\nthe joint estimation/rejection methodology derived herein.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 09:47:48 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Bouleux", "Guillaume", ""], ["Boyer", "R\u00e9my", ""]]}, {"id": "1704.04839", "submitter": "Li Ma", "authors": "Jacopo Soriano and Li Ma", "title": "Mixture modeling on related samples by $\\psi$-stick breaking and kernel\n  perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great interest recently in applying nonparametric kernel\nmixtures in a hierarchical manner to model multiple related data samples\njointly. In such settings several data features are commonly present: (i) the\nrelated samples often share some, if not all, of the mixture components but\nwith differing weights, (ii) only some, not all, of the mixture components vary\nacross the samples, and (iii) often the shared mixture components across\nsamples are not aligned perfectly in terms of their location and spread, but\nrather display small misalignments either due to systematic cross-sample\ndifference or more often due to uncontrolled, extraneous causes. Properly\nincorporating these features in mixture modeling will enhance the efficiency of\ninference, whereas ignoring them not only reduces efficiency but can jeopardize\nthe validity of the inference due to issues such as confounding. We introduce\ntwo techniques for incorporating these features in modeling related data\nsamples using kernel mixtures. The first technique, called $\\psi$-stick\nbreaking, is a joint generative process for the mixing weights through the\nbreaking of both a stick shared by all the samples for the components that do\nnot vary in size across samples and an idiosyncratic stick for each sample for\nthose components that do vary in size. The second technique is to imbue random\nperturbation into the kernels, thereby accounting for cross-sample\nmisalignment. These techniques can be used either separately or together in\nboth parametric and nonparametric kernel mixtures. We derive efficient Bayesian\ninference recipes based on MCMC sampling for models featuring these techniques,\nand illustrate their work through both simulated data and a real flow cytometry\ndata set in prediction/estimation, cross-sample calibration, and testing\nmulti-sample differences.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 00:58:37 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Soriano", "Jacopo", ""], ["Ma", "Li", ""]]}, {"id": "1704.04979", "submitter": "Vahid Moosavi", "authors": "Vahid Moosavi", "title": "Urban Data Streams and Machine Learning: A Case of Swiss Real Estate\n  Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how using publicly available data streams and machine\nlearning algorithms one can develop practical data driven services with no\ninput from domain experts as a form of prior knowledge. We report the initial\nsteps toward development of a real estate portal in Switzerland. Based on\ncontinuous web crawling of publicly available real estate advertisements and\nusing building data from Open Street Map, we developed a system, where we\nroughly estimate the rental and sale price indexes of 1.7 million buildings\nacross the country. In addition to these rough estimates, we developed a web\nbased API for accurate automated valuation of rental prices of individual\nproperties and spatial sensitivity analysis of rental market. We tested several\nestablished function approximation methods against the test data to check the\nquality of the rental price estimations and based on our experiments, Random\nForest gives very reasonable results with the median absolute relative error of\n6.57 percent, which is comparable with the state of the art in the industry. We\nargue that while recently there have been successful cases of real estate\nportals, which are based on Big Data, majority of the existing solutions are\nexpensive, limited to certain users and mostly with non-transparent underlying\nsystems. As an alternative we discuss, how using the crawled data sets and\nother open data sets provided from different institutes it is easily possible\nto develop data driven services for spatial and temporal sensitivity analysis\nin the real estate market to be used for different stakeholders. We believe\nthat this kind of digital literacy can disrupt many other existing business\nconcepts across many domains.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 09:28:55 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Moosavi", "Vahid", ""]]}, {"id": "1704.05028", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio, Alessio Pollice, and Francesca Fedele", "title": "Distributions-oriented wind forecast verification by a hidden Markov\n  model for multivariate circular-linear data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winds from the North-West quadrant and lack of precipitation are known to\nlead to an increase of PM10 concentrations over a residential neighborhood in\nthe city of Taranto (Italy). In 2012 the local government prescribed a\nreduction of industrial emissions by 10% every time such meteorological\nconditions are forecasted 72 hours in advance. Wind forecasting is addressed\nusing the Weather Research and Forecasting (WRF) atmospheric simulation system\nby the Regional Environmental Protection Agency. In the context of\ndistributions-oriented forecast verification, we propose a comprehensive\nmodel-based inferential approach to investigate the ability of the WRF system\nto forecast the local wind speed and direction allowing different performances\nfor unknown weather regimes. Ground-observed and WRF-forecasted wind speed and\ndirection at a relevant location are jointly modeled as a 4-dimensional time\nseries with an unknown finite number of states characterized by homogeneous\ndistributional behavior. The proposed model relies on a mixture of joint\nprojected and skew normal distributions with time-dependent states, where the\ntemporal evolution of the state membership follows a first order Markov\nprocess. Parameter estimates, including the number of states, are obtained by a\nBayesian MCMC-based method. Results provide useful insights on the performance\nof WRF forecasts in relation to different combinations of wind speed and\ndirection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:39:30 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Pollice", "Alessio", ""], ["Fedele", "Francesca", ""]]}, {"id": "1704.05335", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB), Lo\\\"ic Denis (LHC), Sonia Tabti (GREYC,\n  LTCI), Florence Tupin (LTCI)", "title": "MuLoG, or How to apply Gaussian denoisers to multi-channel SAR speckle\n  reduction?", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2713946", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle reduction is a longstanding topic in synthetic aperture radar (SAR)\nimaging. Since most current and planned SAR imaging satellites operate in\npolarimetric, interferometric or tomographic modes, SAR images are\nmulti-channel and speckle reduction techniques must jointly process all\nchannels to recover polarimetric and interferometric information. The\ndistinctive nature of SAR signal (complex-valued, corrupted by multiplicative\nfluctuations) calls for the development of specialized methods for speckle\nreduction. Image denoising is a very active topic in image processing with a\nwide variety of approaches and many denoising algorithms available, almost\nalways designed for additive Gaussian noise suppression. This paper proposes a\ngeneral scheme, called MuLoG (MUlti-channel LOgarithm with Gaussian denoising),\nto include such Gaussian denoisers within a multi-channel SAR speckle reduction\ntechnique. A new family of speckle reduction algorithms can thus be obtained,\nbenefiting from the ongoing progress in Gaussian denoising, and offering\nseveral speckle reduction results often displaying method-specific artifacts\nthat can be dismissed by comparison between results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:32:37 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"], ["Denis", "Lo\u00efc", "", "LHC"], ["Tabti", "Sonia", "", "GREYC,\n  LTCI"], ["Tupin", "Florence", "", "LTCI"]]}, {"id": "1704.05356", "submitter": "Nicolas Pr\\'ollochs", "authors": "Nicolas Pr\\\"ollochs, Stefan Feuerriegel, Dirk Neumann", "title": "Understanding Negations in Information Processing: Learning from\n  Replicating Human Behavior", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information systems experience an ever-growing volume of unstructured data,\nparticularly in the form of textual materials. This represents a rich source of\ninformation from which one can create value for people, organizations and\nbusinesses. For instance, recommender systems can benefit from automatically\nunderstanding preferences based on user reviews or social media. However, it is\ndifficult for computer programs to correctly infer meaning from narrative\ncontent. One major challenge is negations that invert the interpretation of\nwords and sentences. As a remedy, this paper proposes a novel learning strategy\nto detect negations: we apply reinforcement learning to find a policy that\nreplicates the human perception of negations based on an exogenous response,\nsuch as a user rating for reviews. Our method yields several benefits, as it\neliminates the former need for expensive and subjective manual labeling in an\nintermediate stage. Moreover, the inferred policy can be used to derive\nstatistical inferences and implications regarding how humans process and act on\nnegations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:27:46 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Pr\u00f6llochs", "Nicolas", ""], ["Feuerriegel", "Stefan", ""], ["Neumann", "Dirk", ""]]}, {"id": "1704.05390", "submitter": "Kirsty Rhodes", "authors": "Kirsty Rhodes, David Mawdsley, Rebecca Turner, Hayley Jones, Jelena\n  Savovic and Julian Higgins", "title": "Label-invariant models for the analysis of meta-epidemiological data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich meta-epidemiological data sets have been collected to explore\nassociations between intervention effect estimates and study-level\ncharacteristics. Welton et al. proposed models for the analysis of\nmeta-epidemiological data, but these models are restrictive because they force\nheterogeneity among studies with a particular characteristic to be at least as\nlarge as that among studies without the characteristic. In this paper we\npresent alternative models that are invariant to the labels defining the two\ncategories of studies. To exemplify the methods, we use a collection of\nmeta-analyses in which the Cochrane Risk of Bias tool has been implemented. We\nfirst investigate the influence of small trial sample sizes (less than 100\nparticipants), before investigating the influence of multiple methodological\nflaws (inadequate or unclear sequence generation, allocation concealment and\nblinding). We fit both the Welton et al. model and our proposed label-invariant\nmodel and compare the results. Estimates of mean bias associated with the trial\ncharacteristics and of between-trial variances are not very sensitive to the\nchoice of model. Results from fitting a univariable model show that\nheterogeneity variance is, on average, 88% greater among trials with less than\n100 participants. Based on a multivariable model, heterogeneity variance is, on\naverage, 25% greater among trials with inadequate/unclear sequence generation,\n51% greater among trials with inadequate/unclear blinding, and 23% lower among\ntrials with inadequate/unclear allocation concealment, though the 95% intervals\nfor these ratios are very wide. Our proposed label-invariant models for\nmeta-epidemiological data analysis facilitate investigations of between-study\nheterogeneity attributable to certain study characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:22:24 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 17:01:03 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rhodes", "Kirsty", ""], ["Mawdsley", "David", ""], ["Turner", "Rebecca", ""], ["Jones", "Hayley", ""], ["Savovic", "Jelena", ""], ["Higgins", "Julian", ""]]}, {"id": "1704.05452", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Livia Casciola-Rosen, Ami A. Shah, Antony Rosen, Scott\n  Zeger", "title": "Estimating AutoAntibody Signatures to Detect Autoimmune Disease Patient\n  Subsets", "comments": "Appendix containing algorithmic details and extra figures can be\n  found here:\n  http://zhenkewu.com/assets/pdfs/papers/wu-2017-gel-preprocessing_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoimmune diseases are characterized by highly specific immune responses\nagainst molecules in self-tissues. Different autoimmune diseases are\ncharacterized by distinct immune responses, making autoantibodies useful for\ndiagnosis and prediction. In many diseases, the targets of autoantibodies are\nincompletely defined. Although the technologies for autoantibody discovery have\nadvanced dramatically over the past decade, each of these techniques generates\nhundreds of possibilities, which are onerous and expensive to validate. We set\nout to establish a method to greatly simplify autoantibody discovery, using a\npre-filtering step to define subgroups with similar specificities based on\nmigration of radiolabeled, immunoprecipitated proteins on sodium dodecyl\nsulfate (SDS) gels and autoradiography [$\\textbf{G}$el\n$\\textbf{E}$lectrophoresis and band detection on $\\textbf{A}$utoradiograms\n(GEA)]. Human recognition of patterns is not optimal when the patterns are\ncomplex or scattered across many samples. Multiple sources of errors -\nincluding irrelevant intensity differences and warping of gels - have\nchallenged automation of pattern discovery from autoradiograms.\n  In this paper, we address these limitations using a Bayesian hierarchical\nmodel with shrinkage priors for pattern alignment and spatial dewarping. The\nBayesian model combines information from multiple gel sets and corrects spatial\nwarping for coherent estimation of autoantibody signatures defined by presence\nor absence of a grid of landmark proteins. We show the pre-processing creates\nmore clearly separated clusters and improves the accuracy of autoantibody\nsubset detection via hierarchical clustering. Finally, we demonstrate the\nutility of the proposed methods with GEA data from scleroderma patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 05:02:15 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 05:06:12 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Wu", "Zhenke", ""], ["Casciola-Rosen", "Livia", ""], ["Shah", "Ami A.", ""], ["Rosen", "Antony", ""], ["Zeger", "Scott", ""]]}, {"id": "1704.05630", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "M. Dolores Ruiz-Medina and J. \\'Alvarez-Li\\'ebana", "title": "Classical and bayesian componentwise predictors for non-compact\n  correlated ARH(1) processes", "comments": "33 pages: 6 figures are included. In press, accepted manuscript", "journal-ref": "REVSTAT 2017", "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A special class of standard Gaussian Autoregressive Hilbertian processes of\norder one (Gaussian ARH(1) processes), with bounded linear autocorrelation\noperator, which does not satisfy the usual Hilbert-Schmidt assumption, is\nconsidered. To compensate the slow decay of the diagonal coefficients of the\nautocorrelation operator, a faster decay velocity of the eigenvalues of the\ntrace autocovariance operator of the innovation process is assumed. As usual,\nthe eigenvectors of the autocovariance operator of the ARH(1) process are\nconsidered for projection, since, here, they are assumed to be known. Diagonal\ncomponentwise classical and bayesian estimation of the autocorrelation operator\nis studied for prediction. The asymptotic efficiency and equivalence of both\nestimators is proved, as well as of their associated componentwise ARH(1)\nplugin predictors. A simulation study is undertaken to illustrate the\ntheoretical results derived.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 06:56:54 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:59:40 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ruiz-Medina", "M. Dolores", ""], ["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1704.05694", "submitter": "John M. O' Toole", "authors": "John M. O' Toole and Geraldine B. Boylan", "title": "NEURAL: quantitative features for newborn EEG using Matlab", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: For newborn infants in critical care, continuous monitoring of\nbrain function can help identify infants at-risk of brain injury. Quantitative\nfeatures allow a consistent and reproducible approach to EEG analysis, but only\nwhen all implementation aspects are clearly defined.\n  Methods: We detail quantitative features frequently used in neonatal EEG\nanalysis and present a Matlab software package together with exact\nimplementation details for all features. The feature set includes stationary\nfeatures that capture amplitude and frequency characteristics and features of\ninter-hemispheric connectivity. The software, a Neonatal Eeg featURe set in\nmAtLab (NEURAL), is open source and freely available. The software also\nincludes a pre-processing stage with a basic artefact removal procedure.\n  Conclusions: NEURAL provides a common platform for quantitative analysis of\nneonatal EEG. This will support reproducible research and enable comparisons\nacross independent studies. These features present summary measures of the EEG\nthat can also be used in automated methods to determine brain development and\nhealth of the newborn in critical care.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 11:20:38 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Toole", "John M. O'", ""], ["Boylan", "Geraldine B.", ""]]}, {"id": "1704.05767", "submitter": "Soraia Pereira", "authors": "Soraia Pereira, Feridun Turkman, Luis Correia", "title": "Spatio-temporal analysis of regional unemployment rates: A comparison of\n  model based approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to analyze the methodologies that can be used to estimate the\ntotal number of unemployed, as well as the unemployment rates for 28 regions of\nPortugal, designated as NUTS III regions, using model based approaches as\ncompared to the direct estimation methods currently employed by INE (National\nStatistical Institute of Portugal). Model based methods, often known as small\narea estimation methods (Rao, 2003), \"borrow strength\" from neighbouring\nregions and in doing so, aim to compensate for the small sample sizes often\nobserved in these areas. Consequently, it is generally accepted that model\nbased methods tend to produce estimates which have lesser variation. Other\nbenefit in employing model based methods is the possibility of including\nauxiliary information in the form of variables of interest and latent random\nstructures. This study focuses on the application of Bayesian hierarchical\nmodels to the Portuguese Labor Force Survey data from the 1st quarter of 2011\nto the 4th quarter of 2013. Three different data modeling strategies are\nconsidered and compared: Modeling of the total unemployed through Poisson,\nBinomial and Negative Binomial models; modeling of rates using a Beta model;\nand modeling of the three states of the labor market (employed, unemployed and\ninactive) by a Multinomial model. The implementation of these models is based\non the \\textit{Integrated Nested Laplace Approximation} (INLA) approach, except\nfor the Multinomial model which is implemented based on the method of Monte\nCarlo Markov Chain (MCMC). Finally, a comparison of the performance of these\nmodels, as well as the comparison of the results with those obtained by direct\nestimation methods at NUTS III level are given.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:13:05 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Pereira", "Soraia", ""], ["Turkman", "Feridun", ""], ["Correia", "Luis", ""]]}, {"id": "1704.06000", "submitter": "Wassim Suleiman", "authors": "Wassim Suleiman, Pouyan Parvazi, Marius Pesavento, Abdelhak M. Zoubir", "title": "Non-Coherent Direction-of-Arrival Estimation Using Partly Calibrated\n  Arrays", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, direction-of-arrival (DOA) estimation using non-coherent\nprocessing for partly calibrated arrays composed of multiple subarrays is\nconsidered. The subarrays are assumed to compute locally the sample covariance\nmatrices of their measurements and communicate them to the processing center. A\nsufficient condition for the unique identifiability of the sources in the\naforementioned non-coherent processing scheme is presented. We prove that,\nunder mild conditions, with the non-coherent system of subarrays, it is\npossible to identify more sources than identifiable by each individual\nsubarray. This property of non-coherent processing has not been investigated\nbefore. We derive the Maximum Likelihood estimator (MLE) for DOA estimation at\nthe processing center using the sample covariance matrices received from the\nsubarrays. Moreover, the Cramer-Rao Bound (CRB) for our measurement model is\nderived and is used to assess the presented DOA estimators. The behaviour of\nthe CRB at high signal-to-noise ratio (SNR) is analyzed. In contrast to\ncoherent processing, we prove that the CRB approaches zero at high SNR only if\nat least one subarray can identify the sources individually.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:04:08 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Suleiman", "Wassim", ""], ["Parvazi", "Pouyan", ""], ["Pesavento", "Marius", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1704.06074", "submitter": "Luca Pallotta", "authors": "Augusto Aubry and Antonio De Maio and Luca Pallotta", "title": "A Geometric Approach to Covariance Matrix Estimation and its\n  Applications to Radar Problems", "comments": "submitted for journal publication", "journal-ref": null, "doi": "10.1109/TSP.2017.2757913", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of disturbance covariance matrix estimators for radar signal\nprocessing applications is introduced following a geometric paradigm. Each\nestimator is associated with a given unitary invariant norm and performs the\nsample covariance matrix projection into a specific set of structured\ncovariance matrices. Regardless of the considered norm, an efficient solution\ntechnique to handle the resulting constrained optimization problem is\ndeveloped. Specifically, it is shown that the new family of distribution-free\nestimators shares a shrinkagetype form; besides, the eigenvalues estimate just\nrequires the solution of a one-dimensional convex problem whose objective\nfunction depends on the considered unitary norm. For the two most common norm\ninstances, i.e., Frobenius and spectral, very efficient algorithms are\ndeveloped to solve the aforementioned one-dimensional optimization leading to\nalmost closed form covariance estimates. At the analysis stage, the performance\nof the new estimators is assessed in terms of achievable Signal to Interference\nplus Noise Ratio (SINR) both for a spatial and a Doppler processing assuming\ndifferent data statistical characterizations. The results show that interesting\nSINR improvements with respect to some counterparts available in the open\nliterature can be achieved especially in training starved regimes.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 10:09:24 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Aubry", "Augusto", ""], ["De Maio", "Antonio", ""], ["Pallotta", "Luca", ""]]}, {"id": "1704.06112", "submitter": "Ross Gore", "authors": "Carlos Miguel Lemos, Ross Gore, F. LeRon Shults", "title": "Exploratory and Confirmatory Factor Analyses of Religiosity. A\n  Four-Factor Conceptual Model", "comments": null, "journal-ref": "Lemos, Carlos Miguel, et al. \"Dimensionality and factorial\n  invariance of religiosity among Christians and the religiously unaffiliated.\"\n  PloS one 14.5 (2019): e0216352", "doi": "10.1371/journal.pone.0216352", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an exploratory and confirmatory factor analysis of the\nInternational Social Survey Programme Religion Cumulation (1991-1998-2008) data\nset, to identify the factors of individual religiosity and their interrelations\nin quantitative terms. The exploratory factor analysis was performed using data\nfrom the first two waves (1991 and 1998), and led to the identification of four\nstrongly correlated and reliable factors which we labeled Religious formation,\nSupernatural beliefs, Belief in God, and Religious practice. The confirmatory\nfactor analysis was run using data from 2008, and led to the confirmation of\nthis four-factor structure with very good fit measures. We also ran a set of\nstructural equation models in an attempt to determine the causality links\nbetween these four factors. It was found that for the models which provide the\nbest fit Belief in God does not cause Supernatural beliefs, Religious practice\ncan cause Belief in God and that there are multiple paths leading to Belief in\nGod, most of which include Religious formation as a source. The exploratory\nfactor analysis also led to the identification of other factors related to\ntraditional values, confidence in institutions and influence of religious\nleaders on politics, but these were found to have lower reliability or\ninsufficient number of items to meet the acceptance criteria, and thus were not\nincluded in the confirmatory factor analysis and the investigation of causal\nlinks. The results obtained in this work have important material implications\nfor the conceptualization of \"religiosity,\" and important methodological\nimplications for the scientific study of religion.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 12:37:15 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 12:56:06 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 12:05:26 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Lemos", "Carlos Miguel", ""], ["Gore", "Ross", ""], ["Shults", "F. LeRon", ""]]}, {"id": "1704.06408", "submitter": "Matineh Shaker", "authors": "Matineh Shaker, Deniz Erdogmus, Jennifer Dy, Sylvain Bouix", "title": "Subject-Specific Abnormal Region Detection in Traumatic Brain Injury\n  Using Sparse Model Selection on High Dimensional Diffusion Data", "comments": null, "journal-ref": "Medical Image Analysis 37 (2017) 56-65", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate a multivariate Gaussian distribution of\ndiffusion tensor features in a set of brain regions based on a small sample of\nhealthy individuals, and use this distribution to identify imaging\nabnormalities in subjects with mild traumatic brain injury. The multivariate\nmodel receives a {\\em apriori} knowledge in the form of a neighborhood graph\nimposed on the precision matrix, which models brain region interactions, and an\nadditional $L_1$ sparsity constraint. The model is then estimated using the\ngraphical LASSO algorithm and the Mahalanobis distance of healthy and TBI\nsubjects to the distribution mean is used to evaluate the discriminatory power\nof the model. Our experiments show that the addition of the {\\em apriori}\nneighborhood graph results in significant improvements in classification\nperformance compared to a model which does not take into account the brain\nregion interactions or one which uses a fully connected prior graph. In\naddition, we describe a method, using our model, to detect the regions that\ncontribute the most to the overall abnormality of the DTI profile of a\nsubject's brain.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 06:13:05 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Shaker", "Matineh", ""], ["Erdogmus", "Deniz", ""], ["Dy", "Jennifer", ""], ["Bouix", "Sylvain", ""]]}, {"id": "1704.06491", "submitter": "Kirsty Rhodes", "authors": "Kirsty Rhodes, Rebecca Turner, Jelena Savovi\\'c, Hayley Jones, David\n  Mawdsley, Julian Higgins", "title": "Between-trial heterogeneity in meta-analyses may be partially explained\n  by reported design characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We investigated the associations between risk of bias judgments\nfrom Cochrane reviews for sequence generation, allocation concealment and\nblinding and between-trial heterogeneity.\n  Study Design and Setting: Bayesian hierarchical models were fitted to binary\ndata from 117 meta-analyses, to estimate the ratio {\\lambda} by which\nheterogeneity changes for trials at high/unclear risk of bias, compared to\ntrials at low risk of bias. We estimated the proportion of between-trial\nheterogeneity in each meta-analysis that could be explained by the bias\nassociated with specific design characteristics.\n  Results: Univariable analyses showed that heterogeneity variances were, on\naverage, increased among trials at high/unclear risk of bias for sequence\ngeneration ({\\lambda} 1.14, 95% interval: 0.57 to 2.30) and blinding ({\\lambda}\n1.74, 95% interval: 0.85 to 3.47). Trials at high/unclear risk of bias for\nallocation concealment were on average less heterogeneous ({\\lambda} 0.75, 95%\ninterval: 0.35 to 1.61). Multivariable analyses showed that a median of 37%\n(95% interval: 0% to 71%) heterogeneity variance could be explained by trials\nat high/unclear risk of bias for sequence generation, allocation concealment\nand/or blinding. All 95% intervals for changes in heterogeneity were wide and\nincluded the null of no difference.\n  Conclusion: Our interpretation of the results is limited by imprecise\nestimates. There is some indication that between-trial heterogeneity could be\npartially explained by reported design characteristics, and hence adjustment\nfor bias could potentially improve accuracy of meta-analysis results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:41:37 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 17:12:51 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rhodes", "Kirsty", ""], ["Turner", "Rebecca", ""], ["Savovi\u0107", "Jelena", ""], ["Jones", "Hayley", ""], ["Mawdsley", "David", ""], ["Higgins", "Julian", ""]]}, {"id": "1704.06492", "submitter": "Duncan Lee", "authors": "Duncan Lee", "title": "A spatio-temporal process-convolution model for quantifying health\n  inequalities in respiratory prescription rates in Scotland", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rates of respiratory prescriptions vary by GP surgery across Scotland,\nsuggesting there are sizeable health inequalities in respiratory ill health\nacross the country. The aim of this paper is to estimate the magnitude, spatial\npattern and drivers of this spatial variation. Monthly data on respiratory\nprescriptions are available at the GP surgery level, which creates an\ninteresting methodological challenge as these data are not the classical\ngeostatistical, areal unit or point process data types. A novel\nprocess-convolution model is proposed, which extends existing methods by being\nan adaptive smoother via a random weighting scheme and using a tapering\nfunction to reduce the computational burden. The results show that particulate\nair pollution, poverty and ethnicity all drive the health inequalities, while\nthere are additional regional inequalities in rates after covariate adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:43:19 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Lee", "Duncan", ""]]}, {"id": "1704.06572", "submitter": "Bruno Remillard", "authors": "Jonathan A. Ch\\'avez-Casillas and Robert J. Elliott and Bruno\n  R\\'emillard and Anatoliy V. Swishchuk", "title": "A level-1 Limit Order book with time dependent arrival rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple stochastic model for the dynamics of a limit order book,\nextending the recent work of Cont and de Larrard (2013), where the price\ndynamics are endogenous, resulting from market transactions. We also show that\nthe conditional diffusion limit of the price process is the so-called Brownian\nmeander.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 14:42:43 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Ch\u00e1vez-Casillas", "Jonathan A.", ""], ["Elliott", "Robert J.", ""], ["R\u00e9millard", "Bruno", ""], ["Swishchuk", "Anatoliy V.", ""]]}, {"id": "1704.06613", "submitter": "Ba Chu", "authors": "Ba Chu", "title": "Composite Quasi-Likelihood Estimation of Dynamic Panels with\n  Group-Specific Heterogeneity and Spatially Dependent Errors", "comments": "158 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method to estimate large panel data\nerror-correction models with stationary/non-stationary covariates and spatially\ndependent errors, which allows for known/unknown group-specific patterns of\nslope heterogeneity. Analysis is based on composite quasi-likelihood (CQL)\nmaximization which performs estimation and classification simultaneously. The\nproposed CQL estimator remains unbiased in the presence of misspecification of\nthe unobserved individual/group-specific fixed effects; therefore, neither\ninstrumental variables nor bias corrections/reductions are required. This\nestimator also achieves the `oracle' property as the estimation errors of group\nmemberships have no effect on the asymptotic distributions of the\ngroup-specific slope parameters estimates. Classification and estimation\ninvolve a large-scale non-convex mixed-integer programming problem, which can\nthen be solved via a new algorithm based on DC (Difference-of-Convex functions)\nprogramming - the DCA (DC Algorithm). Simulations confirm good finite-sample\nproperties of the proposed estimator. An empirical application and a software\npackage to implement this method are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:05:26 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 18:49:10 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 20:00:59 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Chu", "Ba", ""]]}, {"id": "1704.07141", "submitter": "Caitlin Bucj Prof", "authors": "Caitlin E Buck and Miguel Juarez", "title": "Bayesian radiocarbon modelling for beginners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to freely available, tailored software, Bayesian statistics is fast\nbecoming the dominant paradigm in archaeological chronology construction. Such\nsoftware provides users with powerful tools for Bayesian inference for\nchronological models with little need to undertake formal study of statistical\nmodelling or computer programming. This runs the risk that it is reduced to the\nstatus of a black-box which is not sensible given the power and complexity of\nthe modelling tools it implements. In this paper we seek to offer intuitive\ninsight to ensure that readers from the archaeological research community who\nuse Bayesian chronological modelling software will be better able to make well\neducated choices about the tools and techniques they adopt. Our hope is that\nthey will then be both better informed about their own research designs and\nbetter prepared to offer constructively critical assessments of the modelling\nundertaken by others.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:58:41 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Buck", "Caitlin E", ""], ["Juarez", "Miguel", ""]]}, {"id": "1704.07152", "submitter": "Khalil Said", "authors": "V\\'eronique Maume-Deschamps (1), Didier Rulli\\`ere (2), Khalil Said\n  ((1) ICJ (2) SAF)", "title": "Asymptotic multivariate expectiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [16], a new family of vector-valued risk measures called multivariate\nexpectiles is introduced. In this paper, we focus on the asymptotic behavior of\nthese measures in a multivariate regular variations context. For models with\nequivalent tails, we propose an estimator of these multivariate asymptotic\nexpectiles, in the Fr{\\'e}chet attraction domain case, with asymptotic\nindependence, or in the comonotonic case.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:35:18 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 10:09:44 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Maume-Deschamps", "V\u00e9ronique", "", "ICJ"], ["Rulli\u00e8re", "Didier", "", "SAF"], ["Said", "Khalil", ""]]}, {"id": "1704.07342", "submitter": "Wilfrid Kendall", "authors": "Clair Barnes, Wilfrid Stephen Kendall", "title": "Perches, Post-holes and Grids", "comments": "12 pages (PDF), 1 table, 14 figures. Added reference to data\n  repository holding zipfile and Rmarkdown scripts", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"Planning in the Early Medieval Landscape\" project (PEML)\n<http://humanities.exeter.ac.uk/archaeology/research/projects/planningintheearlymedievallandscape/>,\nfunded by the Leverhulme Trust, has organized and collated a substantial\nquantity of images, and has used this as evidence to support the hypothesis\nthat Anglo-Saxon building construction was based on grid-like planning\nstructures based on fixed modules or quanta of measurement. We report on the\ndevelopment of some statistical contributions to the debate concerning this\nhypothesis. In practice the PEML images correspond to data arising in a wide\nvariety of different forms. It does not seem feasible to produce a single\nautomatic method which can be applied uniformly to all such images; even the\ninitial chore of cleaning up an image (removing extraneous material such as\nlegends and physical features which do not bear on the planning hypothesis)\ntypically presents a separate and demanding challenge for each different image.\nMoreover care must be taken, even in the relatively straightforward cases of\nclearly defined ground-plans (for example for large ecclesiastical buildings of\nthe period), to consider exactly what measurements might be relevant. We report\non pilot statistical analyses concerning three different situations. These\nestablish not only the presence of underlying structure (which indeed is often\nvisually obvious), but also provide an account of the numerical evidence\nsupporting the deduction that such structure is present. We contend that\nstatistical methodology thus contributes to the larger historical debate and\nprovides useful input to the wide and varied range of evidence that has to be\ndebated.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:33:14 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 15:16:56 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Barnes", "Clair", ""], ["Kendall", "Wilfrid Stephen", ""]]}, {"id": "1704.07349", "submitter": "Durba Bhattacharya ms", "authors": "Durba Bhattacharya and Sourabh Bhattacharya", "title": "A Non-Gaussian, Nonparametric Structure for Gene-Gene and\n  Gene-Environment Interactions in Case-Control Studies Based on Hierarchies of\n  Dirichlet Processes", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly clear that complex interactions among genes and\nenvironmental factors play crucial roles in triggering complex diseases. Thus,\nunderstanding such interactions is vital, which is possible only through\nstatistical models that adequately account for such intricate, albeit unknown,\ndependence structures. Bhattacharya & Bhattacharya (2016b) attempt such\nmodeling, relating finite mixtures composed of Dirichlet processes that\nrepresent unknown number of genetic sub-populations through a hierarchical\nmatrix-normal structure that incorporates gene-gene interactions, and possible\nmutations, induced by environmental variables. However, the product dependence\nstructure implied by their matrix-normal model seems to be too simple to be\nappropriate for general complex, realistic situations. In this article, we\npropose and develop a novel nonparametric Bayesian model for case-control\ngenotype data using hierarchies of Dirichlet processes that offers a more\nrealistic and nonparametric dependence structure between the genes, induced by\nthe environmental variables. In this regard, we propose a novel and highly\nparallelisable MCMC algorithm that is rendered quite efficient by the\ncombination of modern parallel computing technology, effective Gibbs sampling\nsteps, retrospective sampling and Transformation based Markov Chain Monte Carlo\n(TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect\nthe roles of genes and environment in case-control studies. We apply our ideas\nto 5 biologically realistic case-control genotype datasets simulated under\ndistinct set-ups, and obtain encouraging results in each case. We finally apply\nour ideas to a real, myocardial infarction dataset, and obtain interesting\nresults on gene-gene and gene-environment interaction, while broadly agreeing\nwith the results reported in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:42:47 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 11:36:23 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bhattacharya", "Durba", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1704.07358", "submitter": "Liang-Hsuan Tai", "authors": "Liang-Hsuan Tai, Anuj Srivastava, Kyle A. Gallivan", "title": "Trend and Variable-Phase Seasonality Estimation from Functional Data", "comments": "18 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating trend and seasonal variation in time-series data\nhas been studied over several decades, although mostly using single time\nseries. This paper studies the problem of estimating these components from\nfunctional data, i.e. multiple time series, in situations where seasonal\neffects exhibit arbitrary time warpings or phase variability across different\nobservations. Rather than ignoring the phase variability, or using an\noff-the-shelf alignment method to remove phase, we take a model-based approach\nand seek MLEs of the trend and the seasonal effects, while performing\nalignments over the seasonal effects at the same time. The MLEs of trend,\nseasonality, and phase are computed using a coordinate-descent based\noptimization method. We use bootstrap replication for computing confidence\nbands and for testing hypothesis about the estimated components. We also\nutilize log-likelihood for selecting the trend subspace, and for comparisons\nwith other candidate models. This framework is demonstrated using experiments\ninvolving synthetic data and three real data (Berkeley Growth Velocity, U.S.\nelectricity price, and USD exchange fluctuation).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:53:54 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Tai", "Liang-Hsuan", ""], ["Srivastava", "Anuj", ""], ["Gallivan", "Kyle A.", ""]]}, {"id": "1704.07606", "submitter": "Amanda Lenzi", "authors": "Amanda Lenzi, Ingelin Steinsland and Pierre Pinson", "title": "Benefits of spatio-temporal modelling for short term wind power\n  forecasting at both individual and aggregated levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The share of wind energy in total installed power capacity has grown rapidly\nin recent years around the world. Producing accurate and reliable forecasts of\nwind power production, together with a quantification of the uncertainty, is\nessential to optimally integrate wind energy into power systems. We build\nspatio-temporal models for wind power generation and obtain full probabilistic\nforecasts from 15 minutes to 5 hours ahead. Detailed analysis of the forecast\nperformances on the individual wind farms and aggregated wind power are\nprovided. We show that it is possible to improve the results of forecasting\naggregated wind power by utilizing spatio-temporal correlations among\nindividual wind farms. Furthermore, spatio-temporal models have the advantage\nof being able to produce spatially out-of-sample forecasts. We evaluate the\npredictions on a data set from wind farms in western Denmark and compare the\nspatio-temporal model with an autoregressive model containing a common\nautoregressive parameter for all wind farms, identifying the specific cases\nwhen it is important to have a spatio-temporal model instead of a temporal one.\nThis case study demonstrates that it is possible to obtain fast and accurate\nforecasts of wind power generation at wind farms where data is available, but\nalso at a larger portfolio including wind farms at new locations. The results\nand the methodologies are relevant for wind power forecasts across the globe as\nwell as for spatial-temporal modelling in general.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 09:36:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lenzi", "Amanda", ""], ["Steinsland", "Ingelin", ""], ["Pinson", "Pierre", ""]]}, {"id": "1704.07638", "submitter": "Nicolas Haverkamp", "authors": "Nicolas Haverkamp and Andre Beauducel", "title": "Violation of the sphericity assumption and its effect on Type-I error\n  rates in repeated measures ANOVA and multi-level linear models (MLM)", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to investigate the effects of violations of the sphericity\nassumption on Type I error rates for different methodical approaches of\nrepeated measures analysis using a simulation approach. In contrast to previous\nsimulation studies on this topic, up to nine measurement occasions were\nconsidered. Therefore, two populations representing the conditions of a\nviolation vs. a non-violation of the sphericity assumption without any\nbetween-group effect or within-subject effect were created and 5,000 random\nsamples of each population were drawn. Finally, the mean Type I error rates for\nMultilevel linear models (MLM) with an unstructured covariance matrix (MLM-UN),\nMLM with compound-symmetry (MLM-CS) and for repeated measures analysis of\nvariance (rANOVA) models (without correction, with\nGreenhouse-Geisser-correction, and Huynh-Feldt-correction) were computed. To\nexamine the effect of both the sample size and the number of measurement\noccasions, sample sizes of n = 20, 40, 60, 80, and 100 were considered as well\nas measurement occasions of m = 3, 6 and 9. For MLM-UN, the results illustrate\na massive progressive bias for small sample sizes (n =20) and m = 6 or more\nmeasurement occasions. This effect could not be found in previous simulation\nstudies with a smaller number of measurement occasions. The mean Type I error\nrates for rANOVA with Greenhouse-Geisser-correction demonstrate a small\nconservative bias if sphericity was not violated, sample sizes were small (n =\n20), and m = 6 or more measurement occasions were conducted. The results plead\nfor a use of rANOVA with Huynh-Feldt-correction, especially when the sphericity\nassumption is violated, the sample size is rather small and the number of\nmeasurement occasions is large. MLM-UN may be used when the sphericity\nassumption is violated and when sample sizes are large.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 11:27:56 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Haverkamp", "Nicolas", ""], ["Beauducel", "Andre", ""]]}, {"id": "1704.07639", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison, Ewald Enzinger, Cuiling Zhang", "title": "Reply to Hicks et al 2017, Reply to Morrison et al 2016 Refining the\n  relevant population in forensic voice comparison, Reply to Hicks et al 2015\n  The importance of distinguishing info from evidence/observations when\n  formulating propositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present letter to the editor is one in a series of publications\ndiscussing the formulation of hypotheses (propositions) for the evaluation of\nstrength of forensic evidence. In particular, the discussion focusses on the\nissue of what information may be used to define the relevant population\nspecified as part of the different-speaker hypothesis in forensic voice\ncomparison. The previous publications in the series are: Hicks et al. 2015\n<http://dx.doi.org/10.1016/j.scijus.2015.06.008>; Morrison et al. (2016)\n<http://dx.doi.org/10.1016/j.scijus.2016.07.002>; Hicks et al. (2017)\n<http://dx.doi.org/10.1016/j.scijus.2017.04.005>. The latter letter to the\neditor mostly resolves the apparent disagreement between the two groups of\nauthors. We briefly discuss one outstanding point of apparent disagreement, and\nattempt to correct a misinterpretation of our earlier remarks. We believe that\nat this point there is no actual disagreement, and that both groups of authors\nare calling for greater collaboration in order to reduce the likelihood of\nfuture misunderstandings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 11:28:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""], ["Enzinger", "Ewald", ""], ["Zhang", "Cuiling", ""]]}, {"id": "1704.07645", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Bayesian nonparametric estimation of survival functions with\n  multiple-samples information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real problems, dependence structures more general than\nexchangeability are required. For instance, in some settings partial\nexchangeability is a more reasonable assumption. For this reason, vectors of\ndependent Bayesian nonparametric priors have recently gained popularity. They\nprovide flexible models which are tractable from a computational and\ntheoretical point of view. In this paper, we focus on their use for estimating\nsurvival functions with multiple-samples information. Our methodology allows to\nmodel the dependence among survival times of different groups of observations\nand extend previous work to an arbitrary dimension . Theoretical results about\nthe posterior behaviour of the underlying dependent vector of completely random\nmeasures are provided. The performance of the model is tested on a simulated\ndataset arising from a distributional Clayton copula.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 11:52:11 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:52:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1704.07736", "submitter": "Toshiki Sato", "authors": "Toshiki Sato, Yuichi Takano", "title": "Smoothness-constrained model for nonparametric item response theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the nonparametric item response theory (NIRT)\nfor estimating item characteristic curves (ICCs) and latent abilities of\nexaminees on educational and psychological tests. In contrast to parametric\nmodels, NIRT models can estimate various forms of ICCs under mild shape\nrestrictions, such as the constraints of monotone homogeneity and double\nmonotonicity. However, NIRT models frequently suffer from estimation\ninstability because of the great flexibility of nonparametric ICCs, especially\nwhen there is only a small amount of item-response data. To improve the\nestimation accuracy, we propose a novel NIRT model constrained by monotone\nhomogeneity and smoothness based on ordered latent classes. Our smoothness\nconstraints avoid overfitting of nonparametric ICCs by keeping them close to\nlogistic curves. We also implement a tailored expectation--maximization\nalgorithm to calibrate our smoothness-constrained NIRT model efficiently. We\nconducted computational experiments to assess the effectiveness of our\nsmoothness-constrained model in comparison with the common two-parameter\nlogistic model and the monotone-homogeneity model. The computational results\ndemonstrate that our model obtained more accurate estimation results than did\nthe two-parameter logistic model when the latent abilities of examinees for\nsome test items followed bimodal distributions. Moreover, our model\noutperformed the monotone-homogeneity model because of the effect of the\nsmoothness constraints.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:16:14 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Sato", "Toshiki", ""], ["Takano", "Yuichi", ""]]}, {"id": "1704.07787", "submitter": "Eliot Abrams", "authors": "Eliot Abrams, George Gui, Ali Hortacsu", "title": "Finding Exogenous Variation in Data", "comments": "20 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider the classic problem of recovering exogenous variation from an\nendogenous regressor. Two-stage least squares recovers exogenous variation\nthrough presuming the existence of an instrumental variable. We rely instead on\nthe assumption that the regressor is a mixture of exogenous and endogenous\nobservations--say as the result of temporary natural experiments. With this\nassumption, we propose an alternative two-stage method based on\nnonparametrically estimating a mixture model to recover a subset of the\nexogenous observations. We demonstrate that our method recovers exogenous\nobservations in simulation and can be used to find pricing experiments hidden\nin grocery store scanner data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:03:32 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 18:31:44 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 02:50:31 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Abrams", "Eliot", ""], ["Gui", "George", ""], ["Hortacsu", "Ali", ""]]}, {"id": "1704.07814", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Vladim\\'ir Hol\\'y and Karel \\v{S}afr", "title": "Disaggregating Input-Output Tables by the Multidimensional RAS Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unknown input-output table can be estimated by the RAS method when only\nits row and column sums are known and some initial structure is assumed. The\nRAS approach can also be utilized for disaggregation of an annual national\ntable to more detailed tables such as regional, quarterly and domestic/imported\ntables. However, the regular RAS method does not ensure that the sums of\ndisaggregated tables are equal to the total table. For this problem, we propose\nto use the multidimensional RAS method which besides input and output totals\nalso ensures regional, quarterly and domestic/imported totals. Our analysis of\nthe Czech industry shows that the multidimensional RAS method increases the\naccuracy of table estimation as well as accuracy of input-output applications\nsuch as the Leontief inverse, the regional Isard's model and the quarterly\nvalue added.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:47:04 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 18:45:20 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 20:06:52 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hol\u00fd", "Vladim\u00edr", ""], ["\u0160afr", "Karel", ""]]}, {"id": "1704.07904", "submitter": "Curtis Storlie", "authors": "Curtis B. Storlie, Terry M. Therneau, Rickey E. Carter, Nicholas Chia,\n  John R. Bergquist, Jeanne M. Huddleston, Santiago Romero-Brufau", "title": "Prediction and Inference with Missing Data in Patient Alert Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Bedside Patient Rescue (BPR) project, the goal of which is\nrisk prediction of adverse events for non-ICU patients using ~200 variables\n(vitals, lab results, assessments, ...). There are several missing predictor\nvalues for most patients, which in the health sciences is the norm, rather than\nthe exception. A Bayesian approach is presented that addresses many of the\nshortcomings to standard approaches to missing predictors: (i) treatment of the\nuncertainty due to imputation is straight-forward in the Bayesian paradigm,\n(ii) the predictor distribution is flexibly modeled as an infinite normal\nmixture with latent variables to explicitly account for discrete predictors\n(i.e., as in multivariate probit regression models), and (iii) certain missing\nnot at random situations can be handled effectively by allowing the indicator\nof missingness into the predictor distribution only to inform the distribution\nof the missing variables. The proposed approach also has the benefit of\nproviding a distribution for the prediction, including the uncertainty inherent\nin the imputation. Therefore, we can ask questions such as: is it possible this\nindividual is at high risk but we are missing too much information to know for\nsure? How much would we reduce the uncertainty in our risk prediction by\nobtaining a particular missing value? This approach is applied to the BPR\nproblem resulting in excellent predictive capability to identify deteriorating\npatients.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 21:02:07 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Storlie", "Curtis B.", ""], ["Therneau", "Terry M.", ""], ["Carter", "Rickey E.", ""], ["Chia", "Nicholas", ""], ["Bergquist", "John R.", ""], ["Huddleston", "Jeanne M.", ""], ["Romero-Brufau", "Santiago", ""]]}, {"id": "1704.07969", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang, Amit Singer", "title": "Anisotropic twicing for single particle reconstruction using\n  autocorrelation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.BM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing phase problem in X-ray crystallography is commonly solved using\nthe technique of molecular replacement, which borrows phases from a previously\nsolved homologous structure, and appends them to the measured Fourier\nmagnitudes of the diffraction patterns of the unknown structure. More recently,\nmolecular replacement has been proposed for solving the missing orthogonal\nmatrices problem arising in Kam's autocorrelation analysis for single particle\nreconstruction using X-ray free electron lasers and cryo-EM. In classical\nmolecular replacement, it is common to estimate the magnitudes of the unknown\nstructure as twice the measured magnitudes minus the magnitudes of the\nhomologous structure, a procedure known as `twicing'. Mathematically, this is\nequivalent to finding an unbiased estimator for a complex-valued scalar. We\ngeneralize this scheme for the case of estimating real or complex valued\nmatrices arising in single particle autocorrelation analysis. We name this\napproach \"Anisotropic Twicing\" because unlike the scalar case, the unbiased\nestimator is not obtained by a simple magnitude isotropic correction. We\ncompare the performance of the least squares, twicing and anisotropic twicing\nestimators on synthetic and experimental datasets. We demonstrate 3D homology\nmodeling in cryo-EM directly from experimental data without iterative\nrefinement or class averaging, for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 04:47:01 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1704.07971", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Jason D. Lee", "title": "A Flexible Framework for Hypothesis Testing in High-dimensions", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing in the linear regression model is a fundamental\nstatistical problem. We consider linear regression in the high-dimensional\nregime where the number of parameters exceeds the number of samples ($p> n$).\nIn order to make informative inference, we assume that the model is\napproximately sparse, that is the effect of covariates on the response can be\nwell approximated by conditioning on a relatively small number of covariates\nwhose identities are unknown. We develop a framework for testing very general\nhypotheses regarding the model parameters. Our framework encompasses testing\nwhether the parameter lies in a convex cone, testing the signal strength, and\ntesting arbitrary functionals of the parameter. We show that the proposed\nprocedure controls the type I error, and also analyze the power of the\nprocedure. Our numerical experiments confirm our theoretical findings and\ndemonstrate that we control false positive rate (type I error) near the nominal\nlevel, and have high power. By duality between hypotheses testing and\nconfidence intervals, the proposed framework can be used to obtain valid\nconfidence intervals for various functionals of the model parameters. For\nlinear functionals, the length of confidence intervals is shown to be minimax\nrate optimal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 05:01:16 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:35:33 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 08:10:47 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 06:11:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1704.07989", "submitter": "Jiayi Hou", "authors": "Jiayi Hou, Anthony Paravati, Ronghui Xu, James Murphy", "title": "High-Dimensional Variable Selection and Prediction under Competing Risks\n  with Application to SEER-Medicare Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risk analysis considers event times due to multiple causes, or of\nmore than one event types. Commonly used regression models for such data\ninclude 1) cause-specific hazards model, which focuses on modeling one type of\nevent while acknowledging other event types simultaneously; and 2)\nsubdistribution hazards model, which links the covariate effects directly to\nthe cumulative incidence function. Their use and in particular statistical\nproperties in the presence of high-dimensional predictors are largely\nunexplored. Motivated by an analysis using the linked SEER-Medicare database\nfor the purposes of predicting cancer versus non-cancer mortality for patients\nwith prostate cancer, we study the accuracy of prediction and variable\nselection of existing statistical learning methods under both models using\nextensive simulation experiments, including different approaches to choosing\npenalty parameters in each method. We then apply the optimal approaches to the\nanalysis of the SEER-Medicare data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 07:10:49 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Hou", "Jiayi", ""], ["Paravati", "Anthony", ""], ["Xu", "Ronghui", ""], ["Murphy", "James", ""]]}, {"id": "1704.08248", "submitter": "Sarit Agami", "authors": "Robert J. Adler, Sarit Agami, and Pratyush Pranav", "title": "Modeling and replicating statistical topology, and evidence for CMB\n  non-homogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the banner of `Big Data', the detection and classification of structure\nin extremely large, high dimensional, data sets, is, one of the central\nstatistical challenges of our times. Among the most intriguing approaches to\nthis challenge is `TDA', or `Topological Data Analysis', one of the primary\naims of which is providing non-metric, but topologically informative,\npre-analyses of data sets which make later, more quantitative analyses\nfeasible. While TDA rests on strong mathematical foundations from Topology, in\napplications it has faced challenges due to an inability to handle issues of\nstatistical reliability and robustness and, most importantly, in an inability\nto make scientific claims with verifiable levels of statistical confidence. We\npropose a methodology for the parametric representation, estimation, and\nreplication of persistence diagrams, the main diagnostic tool of TDA. The power\nof the methodology lies in the fact that even if only one persistence diagram\nis available for analysis -- the typical case for big data applications --\nreplications can be generated to allow for conventional statistical hypothesis\ntesting. The methodology is conceptually simple and computationally practical,\nand provides a broadly effective statistical procedure for persistence diagram\nTDA analysis. We demonstrate the basic ideas on a toy example, and the power of\nthe approach in a novel and revealing analysis of CMB non-homogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:57:42 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Adler", "Robert J.", ""], ["Agami", "Sarit", ""], ["Pranav", "Pratyush", ""]]}, {"id": "1704.08275", "submitter": "Hari Iyer", "authors": "Steven P. Lund and Hari K. Iyer", "title": "Likelihood Ratio as Weight of Forensic Evidence: A Closer Look", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.07598", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forensic science community has increasingly sought quantitative methods\nfor conveying the weight of evidence. Experts from many forensic laboratories\nsummarize their findings in terms of a likelihood ratio. Several proponents of\nthis approach have argued that Bayesian reasoning proves it to be normative. We\nfind this likelihood ratio paradigm to be unsupported by arguments of Bayesian\ndecision theory, which applies only to personal decision making and not to the\ntransfer of information from an expert to a separate decision maker. We further\nargue that decision theory does not exempt the presentation of a likelihood\nratio from uncertainty characterization, which is required to assess the\nfitness for purpose of any transferred quantity. We propose the concept of a\nlattice of assumptions leading to an uncertainty pyramid as a framework for\nassessing the uncertainty in an evaluation of a likelihood ratio. We\ndemonstrate the use of these concepts with illustrative examples regarding the\nrefractive index of glass and automated comparison scores for fingerprints.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 18:13:10 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Lund", "Steven P.", ""], ["Iyer", "Hari K.", ""]]}, {"id": "1704.08299", "submitter": "Tate Twinam", "authors": "Martin Saavedra and Tate Twinam", "title": "A Machine Learning Approach to Improving Occupational Income Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical studies of labor markets frequently lack data on individual\nincome. The occupational income score (OCCSCORE) is often used as an\nalternative measure of labor market outcomes. We consider the consequences of\nusing OCCSCORE when researchers are interested in earnings regressions. We\nestimate race and gender earnings gaps in modern decennial Censuses as well as\nthe 1915 Iowa State Census. Using OCCSCORE biases results towards zero and can\nresult in estimated gaps of the wrong sign. We use a machine learning approach\nto construct a new adjusted score based on industry, occupation, and\ndemographics. The new income score provides estimates closer to earnings\nregressions. Lastly, we consider the consequences for estimates of\nintergenerational mobility elasticities.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 02:55:33 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 02:43:51 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 18:21:47 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Saavedra", "Martin", ""], ["Twinam", "Tate", ""]]}, {"id": "1704.08479", "submitter": "Ya'acov Ritov", "authors": "Ya'acov Ritov", "title": "The utility of a Bayesian analysis of complex models and the study of\n  archeological ${}^{14}$C data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a critical introduction to the complex statistical models\nused in ${}^{14}$C dating. The emphasis is on the estimation of the transit\ntime between a sequence of archeological layers. Although a frequentist\nestimation of the parameters is relatively simple, confidence intervals\nconstructions are not standard as the models are not regular. I argue that that\nthe Bayesian paradigm is a natural approach to these models. It is simple, and\ngives immediate solutions to credible sets, with natural interpretation and\nsimple construction. Indeed it is the standard tool of ${}^{14}$C analysis.\nHowever and necessarily, the Bayesian approach is based on technical\nassumptions that may dominate the scientific conclusion in a hard to predict\nway. I exemplify the discussion in two ways. Firstly, I simulate toy models.\nSecondly, I analyze a particular data set from the Iron Age period in Tel\nRehov. These data are important to the debate on the absolute time of the Iron\nAge I/IIA transition in the Levant, and in particular to the feasibility of the\nBible story about the United Monarchy of David and Solomon. Our conclusion is\nthat the data in question cannot resolve this debate.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 08:45:50 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Ritov", "Ya'acov", ""]]}, {"id": "1704.08615", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas S. A. Wallis, Matthias Bethge", "title": "Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics", "comments": "published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dozens of new models on fixation prediction are published every year and\ncompared on open benchmarks such as MIT300 and LSUN. However, progress in the\nfield can be difficult to judge because models are compared using a variety of\ninconsistent metrics. Here we show that no single saliency map can perform well\nunder all metrics. Instead, we propose a principled approach to solve the\nbenchmarking problem by separating the notions of saliency models, maps and\nmetrics. Inspired by Bayesian decision theory, we define a saliency model to be\na probabilistic model of fixation density prediction and a saliency map to be a\nmetric-specific prediction derived from the model density which maximizes the\nexpected performance on that metric given the model density. We derive these\noptimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,\nNSS, CC, SIM, KL-Div) and show that they can be computed analytically or\napproximated with high precision. We show that this leads to consistent\nrankings in all metrics and avoids the penalties of using one saliency map for\nall metrics. Our method allows researchers to have their model compete on many\ndifferent metrics with state-of-the-art in those metrics: \"good\" models will\nperform well in all metrics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:07:42 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 13:31:14 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1704.08851", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald and Moritz Grosse-Wentrup", "title": "The right tool for the right question --- beyond the encoding versus\n  decoding dichotomy", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major questions that neuroimaging studies attempt to answer:\nFirst, how are sensory stimuli represented in the brain (which we term the\nstimulus-based setting)? And, second, how does the brain generate cognition\n(termed the response-based setting)? There has been a lively debate in the\nneuroimaging community whether encoding and decoding models can provide\ninsights into these questions. In this commentary, we construct two simple and\nanalytically tractable examples to demonstrate that while an encoding model\nanalysis helps with the former, neither model is appropriate to satisfactorily\nanswer the latter question. Consequently, we argue that if we want to\nunderstand how the brain generates cognition, we need to move beyond the\nencoding versus decoding dichotomy and instead discuss and develop tools that\nare specifically tailored to our endeavour.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:56:47 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Grosse-Wentrup", "Moritz", ""]]}]