[{"id": "1302.0360", "submitter": "William Rey Dr.", "authors": "William Rey", "title": "On Weighted Low-Rank Approximation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main interest is the low-rank approximation of a matrix in R^m.n under a\nweighted Frobenius norm. This norm associates a weight to each of the (m x n)\nmatrix entries. We conjecture that the number of approximations is at most\nmin(m, n).\n  We also investigate how the approximations depend on the weight-values.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 10:19:51 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Rey", "William", ""]]}, {"id": "1302.0883", "submitter": "Michael Scheuerer", "authors": "Michael Scheuerer and Luca B\\\"uermann", "title": "Spatially adaptive post-processing of ensemble forecasts for temperature", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series C 63 (2014)\n  405-422", "doi": "10.1111/rssc.12040", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the non-homogeneous Gaussian regression (NGR)\nmodel by Gneiting et al. (2005) that yields locally calibrated probabilistic\nforecasts of tem- perature, based on the output of an ensemble prediction\nsystem (EPS). Our method represents the mean of the predictive distributions as\na sum of short-term averages of local temperatures and EPS-driven terms. For\nthe spatial interpolation of temperature averages and local forecast\nuncertainty parameters we use a Gaussian random field model with an\nintrinsically stationary component that captures large scale fluctuations and a\nlocation-dependent nugget effect that accounts for small scale variability.\nBased on the dynamical forecasts by the COSMO-DE-EPS and observational data\nover Germany we evaluate the performance of our method and and compare it with\nother post-processing approaches such as geostatistical model averaging. Our\nmethod yields locally calibrated and sharp probabilistic forecasts and compares\nfavorably with other approaches. It is reasonably simple, computationally\nefficient, and therefore suitable for operational usage in the post-processing\nof temperature ensemble forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 21:57:45 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Scheuerer", "Michael", ""], ["B\u00fcermann", "Luca", ""]]}, {"id": "1302.0885", "submitter": "Vassilis Kekatos", "authors": "Georgios B. Giannakis, Vassilis Kekatos, Nikolaos Gatsis, Seung-Jun\n  Kim, Hao Zhu, Bruce F. Wollenberg", "title": "Monitoring and Optimization for Power Grids: A Signal Processing\n  Perspective", "comments": "Accepted for publication in the IEEE Signal Processing Magazine", "journal-ref": null, "doi": "10.1109/MSP.2013.2245726", "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smart grid vision is to revitalize the electric power network by\nleveraging the proven sensing, communication, control, and machine learning\ntechnologies to address pressing issues related to security, stability,\nenvironmental impact, market diversity, and novel power technologies.\nSignificant effort and investment have been committed to architect the\nnecessary infrastructure by installing advanced metering systems and\nestablishing data communication networks throughout the grid. Signal processing\nmethodologies are expected to play a major role in this context by providing\nintelligent algorithms that fully exploit such pervasive sensing and control\ncapabilities to realize the vision and manifold anticipated benefits of the\nsmart grid. In this feature article, analytical background and relevance of\nsignal processing tools to power systems are delineated, while introducing\nmajor challenges and opportunities for the future grid engineering. From grid\ninformatics to inference for monitoring and optimization tools, energy-related\nissues are shown to offer a fertile ground for signal processing growth whose\ntime has come.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 22:01:53 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Giannakis", "Georgios B.", ""], ["Kekatos", "Vassilis", ""], ["Gatsis", "Nikolaos", ""], ["Kim", "Seung-Jun", ""], ["Zhu", "Hao", ""], ["Wollenberg", "Bruce F.", ""]]}, {"id": "1302.0893", "submitter": "Michael Scheuerer", "authors": "Michael Scheuerer", "title": "Probabilistic Quantitative Precipitation Forecasting Using Ensemble\n  Model Output Statistics", "comments": null, "journal-ref": "Quarterly Journal of the Royal Meteorological Society 140 (2014)\n  1086-1096", "doi": "10.1002/qj.2183", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical post-processing of dynamical forecast ensembles is an essential\ncomponent of weather forecasting. In this article, we present a post-processing\nmethod that generates full predictive probability distributions for\nprecipitation accumulations based on ensemble model output statistics (EMOS).\nWe model precipitation amounts by a generalized extreme value distribution that\nis left-censored at zero. This distribution permits modelling precipitation on\nthe original scale without prior transformation of the data. A closed form\nexpression for its continuous rank probability score can be derived and permits\ncomputationally efficient model fitting. We discuss an extension of our\napproach that incorporates further statistics characterizing the spatial\nvariability of precipitation amounts in the vicinity of the location of\ninterest. The proposed EMOS method is applied to daily 18-h forecasts of 6-h\naccumulated precipitation over Germany in 2011 using the COSMO-DE ensemble\nprediction system operated by the German Meteorological Service. It yields\ncalibrated and sharp predictive distributions and compares favourably with\nextended logistic regression and Bayesian model averaging which are state of\nthe art approaches for precipitation post-processing. The incorporation of\nneighbourhood information further improves predictive performance and turns out\nto be a useful strategy to account for displacement errors of the dynamical\nforecasts in a probabilistic forecasting framework.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 22:35:27 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Scheuerer", "Michael", ""]]}, {"id": "1302.0926", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Liao, Xiaofeng Shi", "title": "Risks of Large Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating and assessing the risk of a large portfolio is an important topic\nin financial econometrics and risk management. The risk is often estimated by a\nsubstitution of a good estimator of the volatility matrix. However, the\naccuracy of such a risk estimator for large portfolios is largely unknown, and\na simple inequality in the previous literature gives an infeasible upper bound\nfor the estimation error. In addition, numerical studies illustrate that this\nupper bound is very crude. In this paper, we propose factor-based risk\nestimators under a large amount of assets, and introduce a high-confidence\nlevel upper bound (H-CLUB) to assess the accuracy of the risk estimation. The\nH-CLUB is constructed based on three different estimates of the volatility\nmatrix: sample covariance, approximate factor model with known factors, and\nunknown factors (POET, Fan, Liao and Mincheva, 2013). For the first time in the\nliterature, we derive the limiting distribution of the estimated risks in high\ndimensionality. Our numerical results demonstrate that the proposed upper\nbounds significantly outperform the traditional crude bounds, and provide\ninsightful assessment of the estimation of the portfolio risks. In addition,\nour simulated results quantify the relative error in the risk estimation, which\nis usually negligible using 3-month daily data. Finally, the proposed methods\nare applied to an empirical study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 03:02:39 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""], ["Shi", "Xiaofeng", ""]]}, {"id": "1302.0977", "submitter": "Antonio Parisi", "authors": "Brunero Liseo and Antonio Parisi", "title": "Bayesian inference for the multivariate skew-normal model: a Population\n  Monte Carlo approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist and likelihood methods of inference based on the multivariate\nskew-normal model encounter several technical difficulties with this model. In\nspite of the popularity of this class of densities, there are no broadly\nsatisfactory solutions for estimation and testing problems. A general\npopulation Monte Carlo algorithm is proposed which: 1) exploits the latent\nstructure stochastic representation of skew-normal random variables to provide\na full Bayesian analysis of the model and 2) accounts for the presence of\nconstraints in the parameter space. The proposed approach can be defined as\nweakly informative, since the prior distribution approximates the actual\nreference prior for the shape parameter vector. Results are compared with the\nexisting classical solutions and the practical implementation of the algorithm\nis illustrated via a simulation study and a real data example. A generalization\nto the matrix variate regression model with skew-normal error is also\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:56:47 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Liseo", "Brunero", ""], ["Parisi", "Antonio", ""]]}, {"id": "1302.1281", "submitter": "Pierre Weiss", "authors": "Nicolas Chauffert (LNAO), Philippe Ciuiu (LNAO), Jonas Kahn, Pierre\n  Weiss (ITAV, UMR CNRS 5219)", "title": "Travelling salesman-based compressive sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing theory indicates that selecting a few measurements\nindependently at random is a near optimal strategy to sense sparse or\ncompressible signals. This is infeasible in practice for many acquisition\ndevices that acquire samples along continuous trajectories (e.g., radial,\nspiral, ...). Examples include magnetic resonance imaging (MRI) or\nradiointerferometry. In this paper, we propose to generate continuous sampling\ntrajectories by drawing a small set of measurements independently and joining\nthem using a travelling salesman problem solver. Our contribution lies in the\ntheoretical derivation of the appropriate probability density of the initial\ndrawings. Preliminary computational results show that this strategy is as\nefficient as independent drawings while being implementable on real acquisition\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 07:39:01 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2013 21:05:23 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Chauffert", "Nicolas", "", "LNAO"], ["Ciuiu", "Philippe", "", "LNAO"], ["Kahn", "Jonas", "", "ITAV, UMR CNRS 5219"], ["Weiss", "Pierre", "", "ITAV, UMR CNRS 5219"]]}, {"id": "1302.1382", "submitter": "Jacob Levman", "authors": "Jacob Levman", "title": "Test Sensitivity in the Computer-Aided Detection of Breast Cancer from\n  Clinical Mammographic Screening: a Meta-analysis", "comments": "Results from this article have been published in Radiology: Levman J,\n  Clinical Mammographic Screening: Cross-Sectional and Longitudinal Studies\n  Demonstrate Benefits from Computer-Aided Detection, Radiology. Published\n  online February 4th, 2013. via:\n  http://radiology.rsna.org/content/266/1/123/reply#radiology_el_256741", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To assess evaluative methodologies for comparative measurements\nof test sensitivity in clinical mammographic screening trials of computer-aided\ndetection (CAD) technologies. Materials and Methods: This meta-analysis was\nperformed by analytically reviewing the relevant literature on the clinical\napplication of computer-aided detection (CAD) technologies as part of a breast\ncancer screening program based on x-ray mammography. Each clinical study's\nmethod for measuring the CAD system's improvement in test sensitivity is\nexamined in this meta-analysis. The impact of the chosen sensitivity\nmeasurement on the study's conclusions are analyzed. Results: This\nmeta-analysis demonstrates that some studies have inappropriately compared\nsensitivity measurements between control groups and CAD enabled groups. The\ninappropriate comparison of control groups and CAD enabled groups can lead to\nan underestimation of the benefits of the clinical application of\ncomputer-aided detection technologies. Conclusions: The potential for the\nsensitivity measurement issues raised in this meta-analysis to alter the\nconclusions of multiple existing large clinical studies is discussed. Two large\nscale studies are substantially affected by the analysis provided in this study\nand this meta-analysis demonstrates that computer-aided detection systems are\nsuccessfully assisting in the breast cancer screening process.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 14:25:28 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Levman", "Jacob", ""]]}, {"id": "1302.1793", "submitter": "Elizabeth Talbott", "authors": "Elizabeth Talbott, George Karabatsos, Jaime Zurheide", "title": "Informant Discrepancies and the Heritability of Antisocial Behavior: A\n  Meta-Analysis", "comments": "72 pages, 3 figures, 2 tables; As of 2.15.2013 this manuscript is\n  under review for publication. Please do not cite without written permission\n  from the lead author. Thank you.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antisocial behavior, which includes both aggressive and delinquent\nactivities, is the opposite of prosocial behavior. Researchers have studied the\nheritability of antisocial behavior among twin and non-twin sibling pairs from\nbehavioral ratings made by parents, teachers, observers, and youth. Through a\nmeta-analysis, we examined longitudinal and cross sectional research in the\nbehavioral genetics of antisocial behavior, consisting of 42 studies, of which\n38 were studies of twin pairs, 3 were studies of twins and non-twin siblings,\nand 1 was a study of adoptees. These studies provided n = 89 heritability (h2)\neffect size estimates from a total of 94,517 sibling pairs who ranged in age\nfrom 1.5 to 18 years; studies provided data for 29 moderators (predictors). We\nemployed a random-effects meta-analysis model to achieve three goals: (a)\nperform statistical inference of the overall heritability distribution in the\nunderlying population of studies, (b) identify significant study level\nmoderators (predictors) of heritability, and (c) examine how the heritability\ndistribution varied as a function of age and type of informant, particularly in\nlongitudinal research. The meta-analysis indicated a bimodal overall\nheritability distribution, indicating two clusters of moderate and high\nheritability values, respectively; identified four moderators that predicted\nsignificant changes in mean heritability; and indicated differential patterns\nof median h2 and variance (interquartile ranges) across informants and ages. We\nargue for a cross-perspective, cross-setting model for selecting informants in\nbehavioral genetic research, that is flexible and sensitive to changes in\nantisocial behavior over time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 16:13:44 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Talbott", "Elizabeth", ""], ["Karabatsos", "George", ""], ["Zurheide", "Jaime", ""]]}, {"id": "1302.2027", "submitter": "Carlo Lancia", "authors": "Maria Virginia Caccavale, Antonio Iovanella, Carlo Lancia, Guglielmo\n  Lulli, Benedetto Scoppola", "title": "On the statistical description of the inbound air traffic over Heathrow\n  airport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model to describe the inbound air traffic over a congested hub.\nWe show that this model gives a very accurate description of the traffic by the\ncomparison of our theoretical distribution of the queue with the actual\ndistribution observed over Heathrow airport. We discuss also the robustness of\nour model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 13:23:45 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Caccavale", "Maria Virginia", ""], ["Iovanella", "Antonio", ""], ["Lancia", "Carlo", ""], ["Lulli", "Guglielmo", ""], ["Scoppola", "Benedetto", ""]]}, {"id": "1302.2267", "submitter": "Anna Gloria Bille' Ph.D. Candidate", "authors": "A.G. Bill\\'e and G. Arbia", "title": "Spatial Discrete Choice and Spatial Limited Dependent Variable Models: a\n  review with an emphasis on the use in Regional Health Economics", "comments": "38 pages", "journal-ref": "Journal of Economic Survey 2019", "doi": "10.1111/joes.12333", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite spatial econometrics is now considered a consolidated discipline,\nonly in recent years we have experienced an increasing attention to the\npossibility of applying it to the field of discrete choices (e.g. Smirnov, 2010\nfor a recent review) and limited dependent variable models. In particular, only\na small number of papers introduced the above-mentioned models in Health\nEconomics. The main purpose of the present paper is to review the different\nmethodological solutions in spatial discrete choice models as they appeared in\nseveral applied fields by placing an emphasis on the health economics\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 20:29:40 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Bill\u00e9", "A. G.", ""], ["Arbia", "G.", ""]]}, {"id": "1302.2525", "submitter": "Henk Van Elst", "authors": "Henk van Elst (parcIT GmbH, K\\\"oln, Germany)", "title": "Foundations of Descriptive and Inferential Statistics", "comments": "176 pages, 25 *.eps figures, LaTeX2e, hyperlinked references. Third\n  thorough revision, further extended list of references, inclusion of relevant\n  R commands", "journal-ref": null, "doi": "10.13140/RG.2.1.2112.3044", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These lecture notes were written with the aim to provide an accessible though\ntechnically solid introduction to the logic of systematical analyses of\nstatistical data to both undergraduate and postgraduate students, in particular\nin the Social Sciences, Economics, and the Financial Services. They may also\nserve as a general reference for the application of quantitative--empirical\nresearch methods. In an attempt to encourage the adoption of an\ninterdisciplinary perspective on quantitative problems arising in practice, the\nnotes cover the four broad topics (i) descriptive statistical processing of raw\ndata, (ii) elementary probability theory, (iii) the operationalisation of\none-dimensional latent statistical variables according to Likert's widely used\nscaling approach, and (iv) null hypothesis significance testing within the\nfrequentist approach to probability theory concerning (a) distributional\ndifferences of variables between subgroups of a target population, and (b)\nstatistical associations between two variables. The relevance of effect sizes\nfor making inferences is emphasised. These lecture notes are fully hyperlinked,\nthus providing a direct route to original scientific papers as well as to\ninteresting biographical information. They also list many commands for running\nstatistical functions and data analysis routines in the software packages R,\nSPSS, EXCEL and OpenOffice. The immediate involvement in actual data analysis\npractices is strongly recommended.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 16:32:55 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2013 06:02:00 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2015 15:15:20 GMT"}, {"version": "v4", "created": "Fri, 30 Aug 2019 06:24:33 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["van Elst", "Henk", "", "parcIT GmbH, K\u00f6ln, Germany"]]}, {"id": "1302.2712", "submitter": "John Paisley", "authors": "Yue Huang, John Paisley, Qin Lin, Xinghao Ding, Xueyang Fu and\n  Xiao-ping Zhang", "title": "Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2360122", "report-no": null, "categories": "cs.CV physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric model for reconstructing magnetic\nresonance images (MRI) from highly undersampled k-space data. We perform\ndictionary learning as part of the image reconstruction process. To this end,\nwe use the beta process as a nonparametric dictionary learning prior for\nrepresenting an image patch as a sparse combination of dictionary elements. The\nsize of the dictionary and the patch-specific sparsity pattern are inferred\nfrom the data, in addition to other dictionary learning variables. Dictionary\nlearning is performed directly on the compressed image, and so is tailored to\nthe MRI being considered. In addition, we investigate a total variation penalty\nterm in combination with the dictionary learning model, and show how the\ndenoising property of dictionary learning removes dependence on regularization\nparameters in the noisy setting. We derive a stochastic optimization algorithm\nbased on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use the\nalternating direction method of multipliers (ADMM) for efficiently performing\ntotal variation minimization. We present empirical results on several MRI,\nwhich show that the proposed regularization framework can improve\nreconstruction accuracy over other methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 06:17:02 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 20:02:45 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 11:23:53 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Huang", "Yue", ""], ["Paisley", "John", ""], ["Lin", "Qin", ""], ["Ding", "Xinghao", ""], ["Fu", "Xueyang", ""], ["Zhang", "Xiao-ping", ""]]}, {"id": "1302.3446", "submitter": "Xin Yuan", "authors": "Xin Yuan, Jianbo Yang, Patrick Llull, Xuejun Liao, Guillermo Sapiro,\n  David J. Brady and Lawrence Carin", "title": "Adaptive Temporal Compressive Sensing for Video", "comments": "IEEE Interonal International Conference on Image Processing\n  (ICIP),2013", "journal-ref": null, "doi": "10.1109/ICIP.2013.6738004", "report-no": null, "categories": "stat.AP cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the concept of adaptive temporal compressive sensing\n(CS) for video. We propose a CS algorithm to adapt the compression ratio based\non the scene's temporal complexity, computed from the compressed data, without\ncompromising the quality of the reconstructed video. The temporal adaptivity is\nmanifested by manipulating the integration time of the camera, opening the\npossibility to real-time implementation. The proposed algorithm is a\ngeneralized temporal CS approach that can be incorporated with a diverse set of\nexisting hardware systems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 15:54:25 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 04:40:52 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2013 23:25:59 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Yuan", "Xin", ""], ["Yang", "Jianbo", ""], ["Llull", "Patrick", ""], ["Liao", "Xuejun", ""], ["Sapiro", "Guillermo", ""], ["Brady", "David J.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1302.3463", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Locally epistatic genomic relationship matrices for genomic association,\n  prediction and selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount and complexity of genetic information increases it is necessary\nthat we explore some efficient ways of handling these data. This study takes\nthe \"divide and conquer\" approach for analyzing high dimensional genomic data.\nOur aims include reducing the dimensionality of the problem that has to be\ndealt one at a time, improving the performance and interpretability of the\nmodels. We propose using the inherent structures in the genome; to divide the\nbigger problem into manageable parts. In plant and animal breeding studies a\ndistinction is made between the commercial value (additive + epistatic genetic\neffects) and the breeding value (additive genetic effects) of an individual\nsince it is expected that some of the epistatic genetic effects will be lost\ndue to recombination. In this paper, we argue that the breeder can take\nadvantage of some of the epistatic marker effects in regions of low\nrecombination. The models introduced here aim to estimate local epistatic line\nheritability by using the genetic map information and combine the local\nadditive and epistatic effects. To this end, we have used semi-parametric mixed\nmodels with multiple local genomic relationship matrices with hierarchical\ntesting designs and lasso post-processing for sparsity in the final model and\nspeed. Our models produce good predictive performance along with genetic\nassociation information.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 16:46:59 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 15:27:42 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2013 16:00:35 GMT"}, {"version": "v4", "created": "Thu, 18 Apr 2013 12:43:25 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2013 15:25:07 GMT"}, {"version": "v6", "created": "Wed, 14 Aug 2013 15:40:18 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1302.3564", "submitter": "Enrique F. Castillo", "authors": "Enrique F. Castillo, Cristina Solares, Patricia Gomez", "title": "Tail Sensitivity Analysis in Bayesian Networks", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-133-140", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an efficient method for simulating the tails of a target\nvariable Z=h(X) which depends on a set of basic variables X=(X_1, ..., X_n). To\nthis aim, variables X_i, i=1, ..., n are sequentially simulated in such a\nmanner that Z=h(x_1, ..., x_i-1, X_i, ..., X_n) is guaranteed to be in the tail\nof Z. When this method is difficult to apply, an alternative method is\nproposed, which leads to a low rejection proportion of sample values, when\ncompared with the Monte Carlo method. Both methods are shown to be very useful\nto perform a sensitivity analysis of Bayesian networks, when very large\nconfidence intervals for the marginal/conditional probabilities are required,\nas in reliability or risk analysis. The methods are shown to behave best when\nall scores coincide. The required modifications for this to occur are\ndiscussed. The methods are illustrated with several examples and one example of\napplication to a real case is used to illustrate the whole process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:12:46 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Castillo", "Enrique F.", ""], ["Solares", "Cristina", ""], ["Gomez", "Patricia", ""]]}, {"id": "1302.3590", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey, Laura Martignon", "title": "Bayesian Learning of Loglinear Models for Neural Connectivity", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-373-380", "categories": "cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian approach to learning the connectivity\nstructure of a group of neurons from data on configuration frequencies. A major\nobjective of the research is to provide statistical tools for detecting changes\nin firing patterns with changing stimuli. Our framework is not restricted to\nthe well-understood case of pair interactions, but generalizes the Boltzmann\nmachine model to allow for higher order interactions. The paper applies a\nMarkov Chain Monte Carlo Model Composition (MC3) algorithm to search over\nconnectivity structures and uses Laplace's method to approximate posterior\nprobabilities of structures. Performance of the methods was tested on synthetic\ndata. The models were also applied to data obtained by Vaadia on multi-unit\nrecordings of several neurons in the visual cortex of a rhesus monkey in two\ndifferent attentional states. Results confirmed the experimenters' conjecture\nthat different attentional states were associated with different interaction\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:15:20 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""], ["Martignon", "Laura", ""]]}, {"id": "1302.3697", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann and Werner Marx", "title": "How to evaluate individual researchers working in the natural and life\n  sciences meaningfully? A proposal of methods based on percentiles of\n  citations", "comments": "Accepted for publication in Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although bibliometrics has been a separate research field for many years,\nthere is still no uniformity in the way bibliometric analyses are applied to\nindividual researchers. Therefore, this study aims to set up proposals how to\nevaluate individual researchers working in the natural and life sciences. 2005\nsaw the introduction of the h index, which gives information about a\nresearcher's productivity and the impact of his or her publications in a single\nnumber (h is the number of publications with at least h citations); however, it\nis not possible to cover the multidimensional complexity of research\nperformance and to undertake inter-personal comparisons with this number. This\nstudy therefore includes recommendations for a set of indicators to be used for\nevaluating researchers. Our proposals relate to the selection of data on which\nan evaluation is based, the analysis of the data and the presentation of the\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 07:55:41 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 07:22:28 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Bornmann", "Lutz", ""], ["Marx", "Werner", ""]]}, {"id": "1302.3790", "submitter": "Maria Rodrigo-Domingo", "authors": "Maria Rodrigo-Domingo, Rasmus Waagepetersen, Julie St{\\o}ve B{\\o}dker,\n  Steffen Falgreen, Malene Krag Kjeldsen, Hans Erik Johnsen, Karen Dybk{\\ae}r,\n  Martin B{\\o}gsted", "title": "Reproducible probe-level analysis of the Affymetrix Exon 1.0 ST array\n  with R/Bioconductor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of different transcripts of a gene across samples can be\nanalysed by whole-transcriptome microarrays. Reproducing results from published\nmicroarray data represents a challenge due to the vast amounts of data and the\nlarge variety of pre-processing and filtering steps employed before the actual\nanalysis is carried out. To guarantee a firm basis for methodological\ndevelopment where results with new methods are compared with previous results\nit is crucial to ensure that all analyses are completely reproducible for other\nresearchers. We here give a detailed workflow on how to perform reproducible\nanalysis of the GeneChip Human Exon 1.0 ST Array at probe and probeset level\nsolely in R/Bioconductor, choosing packages based on their simplicity of use.\nTo exemplify the use of the proposed workflow we analyse differential splicing\nand differential gene expression in a publicly available dataset using various\nstatistical methods. We believe this study will provide other researchers with\nan easy way of accessing gene expression data at different annotation levels\nand with the sufficient details needed for developing their own tools for\nreproducible analysis of the GeneChip Human Exon 1.0 ST Array.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 16:12:03 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 18:33:30 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Rodrigo-Domingo", "Maria", ""], ["Waagepetersen", "Rasmus", ""], ["B\u00f8dker", "Julie St\u00f8ve", ""], ["Falgreen", "Steffen", ""], ["Kjeldsen", "Malene Krag", ""], ["Johnsen", "Hans Erik", ""], ["Dybk\u00e6r", "Karen", ""], ["B\u00f8gsted", "Martin", ""]]}, {"id": "1302.3925", "submitter": "Danail Obreschkow Dr", "authors": "Wolfgang Riemer, Dietrich Stoyan, Danail Obreschkow", "title": "Cuboidal Dice and Gibbs Distributions", "comments": "10 pages, 2 figures, 4 tables, Metrika, April (2013)", "journal-ref": null, "doi": "10.1007/s00184-013-0435-y", "report-no": null, "categories": "math-ph math.MP math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the face-probabilities of a cuboidal die, i.e. a die with different\nside-lengths? This paper introduces a model for these probabilities based on a\nGibbs distribution. Experimental data produced in this work and drawn from the\nliterature support the Gibbs model. The experiments also reveal that the\nphysical conditions, such as the quality of the surface onto which the dice are\ndropped, can affect the face-probabilities. In the Gibbs model, those\nvariations are condensed in a single parameter, adjustable to the physical\nconditions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 03:53:28 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 05:53:28 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Riemer", "Wolfgang", ""], ["Stoyan", "Dietrich", ""], ["Obreschkow", "Danail", ""]]}, {"id": "1302.4049", "submitter": "Gyorgy Terdik DR", "authors": "Gyorgy Terdik", "title": "Angular Spectra for non-Gaussian Isotropic Fields", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.CO math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmic Microwave Background (CMB) Anisotropies is a subject of intensive\nresearch in several fields of sciences. In this paper we start a systematic\ndevelopment of basic notions and theory in statistics according to the\napplication for CMB. The main result of this paper is the necessary and\nsufficient condition for isotropy of a non-Gaussian field in terms of spectra.\nClear formulae for bi-, tri- and polyspectra and bi-, tri-, and higher order\ncovariances are also given. Keywords: Bispectrum, Trispectrum, Angular\npoly-Spectra, Cosmic microwave background radiation; Gaussianity; spherical\nrandom fields\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2013 10:11:27 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 15:01:28 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Terdik", "Gyorgy", ""]]}, {"id": "1302.4118", "submitter": "Shunqiao Sun Shunqiao Sun", "authors": "Shunqiao Sun, Athina P. Petropulu and Waheed U. Bajwa", "title": "Target Estimation in Colocated MIMO Radar via Matrix Completion", "comments": "5 pages, ICASSP 2013", "journal-ref": "Proc. IEEE Intl. Conf. Acoustics, Speech, and Signal Processing,\n  Vancouver, Canada, May 26-31, 2013, pp. 4144-4148", "doi": "10.1109/ICASSP.2013.6638439", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a colocated MIMO radar scenario, in which the receive antennas\nforward their measurements to a fusion center. Based on the received data, the\nfusion center formulates a matrix which is then used for target parameter\nestimation. When the receive antennas sample the target returns at Nyquist\nrate, and assuming that there are more receive antennas than targets, the data\nmatrix at the fusion center is low-rank. When each receive antenna sends to the\nfusion center only a small number of samples, along with the sample index, the\nreceive data matrix has missing elements, corresponding to the samples that\nwere not forwarded. Under certain conditions, matrix completion techniques can\nbe applied to recover the full receive data matrix, which can then be used in\nconjunction with array processing techniques, e.g., MUSIC, to obtain target\ninformation. Numerical results indicate that good target recovery can be\nachieved with occupancy of the receive data matrix as low as 50%.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2013 20:42:13 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2013 02:01:51 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Sun", "Shunqiao", ""], ["Petropulu", "Athina P.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1302.4373", "submitter": "Juemin Yang", "authors": "Juemin Yang, Ani Eloyan, Anita Barber, Mary Beth Nebel, Stewart\n  Mostofsky, James J. Pekar, Ciprian Crainiceanu and Brian Caffo", "title": "Homotopic Group ICA for Multi-Subject Brain Imaging Data", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a computational technique for\nrevealing latent factors that underlie sets of measurements or signals. It has\nbecome a standard technique in functional neuroimaging. In functional\nneuroimaging, so called group ICA (gICA) seeks to identify and quantify\nnetworks of correlated regions across subjects. This paper reports on the\ndevelopment of a new group ICA approach, Homotopic Group ICA (H-gICA), for\nblind source separation of resting state functional magnetic resonance imaging\n(fMRI) data. Resting state brain functional homotopy is the similarity of\nspontaneous fluctuations between bilaterally symmetrically opposing regions\n(i.e. those symmetric with respect to the mid-sagittal plane) (Zuo et al.,\n2010). The approach we proposed improves network estimates by leveraging this\nknown brain functional homotopy. H-gICA increases the potential for network\ndiscovery, effectively by averaging information across hemispheres. It is\ntheoretically proven to be identical to standard group ICA when the true\nsources are both perfectly homotopic and noise-free, while simulation studies\nand data explorations demonstrate its benefits in the presence of noise.\nMoreover, compared to commonly applied group ICA algorithms, the structure of\nthe H-gICA input data leads to significant improvement in computational\nefficiency. A simulation study comfirms its effectiveness in homotopic,\nnon-homotopic and mixed settings, as well as on the landmark ADHD-200 dataset.\nFrom a relatively small subset of data, several brain networks were found\nincluding: the visual, the default mode and auditory networks, as well as\nothers. These were shown to be more contiguous and clearly delineated than the\ncorresponding ordinary group ICA. Finally, in addition to improving network\nestimation, H-gICA facilitates the investigation of functional homotopy via\nICA-based networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:05:46 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Yang", "Juemin", ""], ["Eloyan", "Ani", ""], ["Barber", "Anita", ""], ["Nebel", "Mary Beth", ""], ["Mostofsky", "Stewart", ""], ["Pekar", "James J.", ""], ["Crainiceanu", "Ciprian", ""], ["Caffo", "Brian", ""]]}, {"id": "1302.4404", "submitter": "Steffen Lauritzen", "authors": "R. G. Cowell and T. Graversen and S. Lauritzen and J. Mortera", "title": "Analysis of Forensic DNA Mixtures with Artefacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA is now routinely used in criminal investigations and court cases,\nalthough DNA samples taken at crime scenes are of varying quality and therefore\npresent challenging problems for their interpretation. We present a statistical\nmodel for the quantitative peak information obtained from an electropherogram\n(EPG) of a forensic DNA sample and illustrate its potential use for the\nanalysis of criminal cases. In contrast to most previously used methods, we\ndirectly model the peak height information and incorporates important artefacts\nassociated with the production of the EPG. Our model has a number of unknown\nparameters, and we show that these can be estimated by the method of maximum\nlikelihood in the presence of multiple unknown contributors, and their\napproximate standard errors calculated; the computations exploit a Bayesian\nnetwork representation of the model. A case example from a UK trial, as\nreported in the literature, is used to illustrate the efficacy and use of the\nmodel, both in finding likelihood ratios to quantify the strength of evidence,\nand in the deconvolution of mixtures for the purpose of finding likely profiles\nof one or more unknown contributors to a DNA sample. Our model is readily\nextended to simultaneous analysis of more than one mixture as illustrated in a\ncase example. We show that combination of evidence from several samples may\ngive an evidential strength close to that of a single source trace and thus\nmodelling of peak height information provides for a potentially very efficient\nmixture analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 20:01:09 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 19:05:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Cowell", "R. G.", ""], ["Graversen", "T.", ""], ["Lauritzen", "S.", ""], ["Mortera", "J.", ""]]}, {"id": "1302.4631", "submitter": "Daniel Heersink", "authors": "Daniel K. Heersink and Reinhard Furrer and Mike A. Mooney", "title": "Intelligent Compaction and Quality Assurance of Roller Measurement\n  Values utilizing Backfitting and Multiresolution Scale Space Analysis", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern earthwork compaction rollers collect location and compaction\ninformation as they traverse a compaction site. These roller measurement values\npresent a challenging spatio-temporal statistical problem that requires careful\nimplementation of a proper stochastic model and estimation procedure. Heersink\nand Furrer (2013) proposed a sequential, spatial mixed-effects model and a\nsequential, spatial backfitting routine for estimation of the modeling terms\nfor such data. The estimated fields produced from this backfitting procedure\nare analyzed using a multiresolution scale space analysis developed by\nHolmstr\u007fom et al. (2011). This image analysis is proposed as a viable solution\nto improved intelligent compaction and quality assurance of the compaction\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 15:04:15 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2013 17:05:48 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2013 15:18:59 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Heersink", "Daniel K.", ""], ["Furrer", "Reinhard", ""], ["Mooney", "Mike A.", ""]]}, {"id": "1302.4659", "submitter": "Daniel Heersink", "authors": "Daniel K. Heersink and Reinhard Furrer and Mike A. Mooney", "title": "Spatial Backfitting of Roller Measurement Values from a Florida Test Bed", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern earthwork compaction rollers collect location and compaction\ninformation as they traverse a compaction site. These data are indirectly\nobserved through non-linear measurement operators, inherently multivariate with\ncomplex correlation structures, and collected in huge quantities. The nature of\nsuch data was investigated at a large, atypically compacted test bed in\nFlorida, USA. Exploratory analysis of this data through detrending and\nempirical semivariogram estimation is performed. A second analysis using a\nsequential, spatial backfitting algorithm is used to investigate the importance\nof driving direction of the roller.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 16:16:53 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 09:39:14 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Heersink", "Daniel K.", ""], ["Furrer", "Reinhard", ""], ["Mooney", "Mike A.", ""]]}, {"id": "1302.4735", "submitter": "Brian Macdonald", "authors": "Brian Macdonald and William Pulleyblank", "title": "Realignment in the NHL, MLB, the NFL, and the NBA", "comments": "20 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports leagues consist of conferences subdivided into divisions. Teams play a\nnumber of games within their divisions and fewer games against teams in\ndifferent divisions and conferences. Usually, a league structure remains stable\nfrom one season to the next. However, structures change when growth or\ncontraction occurs, and realignment of the four major professional sports\nleagues in North America has occurred more than twenty-five times since 1967.\nIn this paper, we describe a method for realigning sports leagues that is\nflexible, adaptive, and that enables construction of schedules that minimize\ntravel while satisfying other criteria. We do not build schedules; we develop\nleague structures which support the subsequent construction of efficient\nschedules. Our initial focus is the NHL, which has an urgent need for\nrealignment following the recent move of the Atlanta Thrashers to Winnipeg, but\nour methods can be adapted to virtually any situation. We examine a variety of\nscenarios for the NHL, and apply our methods to the NBA, MLB, and NFL. We find\nthe biggest improvements for MLB and the NFL, where adopting the best solutions\nwould reduce league travel by about 20%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 00:50:35 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Macdonald", "Brian", ""], ["Pulleyblank", "William", ""]]}, {"id": "1302.4774", "submitter": "Chih-Chun Chen", "authors": "Chih-Chun Chen", "title": "A theoretical framework for conducting multi-level studies of complex\n  social systems with agent-based models and empirical data", "comments": "50 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formal but intuitive framework is introduced to bridge the gap between data\nobtained from empirical studies and that generated by agent-based models. This\nis based on three key tenets. Firstly, a simulation can be given multiple\nformal descriptions corresponding to static and dynamic properties at different\nlevels of observation. These can be easily mapped to empirically observed\nphenomena and data obtained from them. Secondly, an agent-based model generates\na set of closed systems, and computational simulation is the means by which we\nsample from this set. Thirdly, properties at different levels and statistical\nrelationships between them can be used to classify simulations as those that\ninstantiate a more sophisticated set of constraints. These can be validated\nwith models obtained from statistical models of empirical data (for example,\nstructural equation or multi-level models) and hence provide more stringent\ncriteria for validating the agent-based model itself.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 23:06:58 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Chen", "Chih-Chun", ""]]}, {"id": "1302.4857", "submitter": "Michele Piana", "authors": "Federico Benvenuto, Richard Schwartz, Michele Piana, Anna Maria\n  Massone", "title": "Expectation Maximization for Hard X-ray Count Modulation Profiles", "comments": null, "journal-ref": null, "doi": "10.1051/0004-6361/201321295", "report-no": null, "categories": "astro-ph.IM astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the image reconstruction problem when the\nmeasured data are solar hard X-ray modulation profiles obtained from the Reuven\nRamaty High Energy Solar Spectroscopic Imager (RHESSI)} instrument. Our goal is\nto demonstrate that a statistical iterative method classically applied to the\nimage deconvolution problem is very effective when utilized for the analysis of\ncount modulation profiles in solar hard X-ray imaging based on Rotating\nModulation Collimators. The algorithm described in this paper solves the\nmaximum likelihood problem iteratively and encoding a positivity constraint\ninto the iterative optimization scheme. The result is therefore a classical\nExpectation Maximization method this time applied not to an image deconvolution\nproblem but to image reconstruction from count modulation profiles. The\ntechnical reason that makes our implementation particularly effective in this\napplication is the use of a very reliable stopping rule which is able to\nregularize the solution providing, at the same time, a very satisfactory\nCash-statistic (C-statistic). The method is applied to both reproduce synthetic\nflaring configurations and reconstruct images from experimental data\ncorresponding to three real events. In this second case, the performance of\nExpectation Maximization, when compared to Pixon image reconstruction, shows a\ncomparable accuracy and a notably reduced computational burden; when compared\nto CLEAN, shows a better fidelity with respect to the measurements with a\ncomparable computational effectiveness. If optimally stopped, Expectation\nMaximization represents a very reliable method for image reconstruction in the\nRHESSI context when count modulation profiles are used as input data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 10:04:24 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Benvenuto", "Federico", ""], ["Schwartz", "Richard", ""], ["Piana", "Michele", ""], ["Massone", "Anna Maria", ""]]}, {"id": "1302.5186", "submitter": "Ana Georgina Flesia MS", "authors": "Javier Gimenez, Jorge Martinez, Ana Georgina Flesia", "title": "Unsupervised edge map scoring: a statistical complexity approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Statistical Complexity Measure (SCM) to qualify edge maps\nwithout Ground Truth (GT) knowledge. The measure is the product of two indices,\nan \\emph{Equilibrium} index $\\mathcal{E}$ obtained by projecting the edge map\ninto a family of edge patterns, and an \\emph{Entropy} index $\\mathcal{H}$,\ndefined as a function of the Kolmogorov Smirnov (KS) statistic.\n  This new measure can be used for performance characterization which includes:\n(i)~the specific evaluation of an algorithm (intra-technique process) in order\nto identify its best parameters, and (ii)~the comparison of different\nalgorithms (inter-technique process) in order to classify them according to\ntheir quality.\n  Results made over images of the South Florida and Berkeley databases show\nthat our approach significantly improves over Pratt's Figure of Merit (PFoM)\nwhich is the objective reference-based edge map evaluation standard, as it\ntakes into account more features in its evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 05:56:41 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 18:28:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Gimenez", "Javier", ""], ["Martinez", "Jorge", ""], ["Flesia", "Ana Georgina", ""]]}, {"id": "1302.5554", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas, Fr\\'ed\\'eric Lavancier, Souleymane Kadri-Harouna", "title": "Self-similar prior and wavelet bases for hidden incompressible turbulent\n  motion", "comments": "SIAM Journal on Imaging Sciences, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV cs.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with the ill-posed inverse problem of estimating\nturbulent flows from the observation of an image sequence. From a Bayesian\nperspective, a divergence-free isotropic fractional Brownian motion (fBm) is\nchosen as a prior model for instantaneous turbulent velocity fields. This\nself-similar prior characterizes accurately second-order statistics of velocity\nfields in incompressible isotropic turbulence. Nevertheless, the associated\nmaximum a posteriori involves a fractional Laplacian operator which is delicate\nto implement in practice. To deal with this issue, we propose to decompose the\ndivergent-free fBm on well-chosen wavelet bases. As a first alternative, we\npropose to design wavelets as whitening filters. We show that these filters are\nfractional Laplacian wavelets composed with the Leray projector. As a second\nalternative, we use a divergence-free wavelet basis, which takes implicitly\ninto account the incompressibility constraint arising from physics. Although\nthe latter decomposition involves correlated wavelet coefficients, we are able\nto handle this dependence in practice. Based on these two wavelet\ndecompositions, we finally provide effective and efficient algorithms to\napproach the maximum a posteriori. An intensive numerical evaluation proves the\nrelevance of the proposed wavelet-based self-similar priors.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 11:28:07 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 07:43:12 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["Kadri-Harouna", "Souleymane", ""]]}, {"id": "1302.5556", "submitter": "Anubha Gupta", "authors": "Anubha Gupta and ShivDutt Joshi", "title": "DCT and Eigenvectors of Covariance of 1st and 2nd order Discrete\n  fractional Brownian motion", "comments": "27 pages, Submitted to transactions on information theory, January\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes connection between discrete cosine transform (DCT) and\n1st and 2nd order discrete-time fractional Brownian motion process. It is\nproved that the eigenvectors of the auto-covariance matrix of a 1st and 2nd\norder discrete-time fractional Brownian motion can be approximated by DCT basis\nvectors in the asymptotic sense. Perturbation in eigenvectors from DCT basis\nvectors is modeled using the analytic perturbation theory of linear operators.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 11:34:56 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Gupta", "Anubha", ""], ["Joshi", "ShivDutt", ""]]}, {"id": "1302.5624", "submitter": "Dennis Prangle", "authors": "Dennis Prangle, Paul Fearnhead, Murray P. Cox, Patrick J. Biggs, Nigel\n  P. French", "title": "Semi-automatic selection of summary statistics for ABC model choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central statistical goal is to choose between alternative explanatory\nmodels of data. In many modern applications, such as population genetics, it is\nnot possible to apply standard methods based on evaluating the likelihood\nfunctions of the models, as these are numerically intractable. Approximate\nBayesian computation (ABC) is a commonly used alternative for such situations.\nABC simulates data x for many parameter values under each model, which is\ncompared to the observed data xobs. More weight is placed on models under which\nS(x) is close to S(xobs), where S maps data to a vector of summary statistics.\nPrevious work has shown the choice of S is crucial to the efficiency and\naccuracy of ABC. This paper provides a method to select good summary statistics\nfor model choice. It uses a preliminary step, simulating many x values from all\nmodels and fitting regressions to this with the model as response. The\nresulting model weight estimators are used as S in an ABC analysis. Theoretical\nresults are given to justify this as approximating low dimensional sufficient\nstatistics. A substantive application is presented: choosing between competing\ncoalescent models of demographic growth for Campylobacter jejuni in New Zealand\nusing multi-locus sequence typing data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 15:48:31 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Prangle", "Dennis", ""], ["Fearnhead", "Paul", ""], ["Cox", "Murray P.", ""], ["Biggs", "Patrick J.", ""], ["French", "Nigel P.", ""]]}, {"id": "1302.5721", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, F. DuBois Bowman, Paul J. Laurienti", "title": "Analyzing complex functional brain networks: fusing statistics and\n  network science to understand the brain", "comments": "Statistics Surveys, In Press", "journal-ref": "Statistics Surveys (2013) 7, 1-36", "doi": "10.1214/13-SS103", "report-no": null, "categories": "stat.ME q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex functional brain network analyses have exploded over the last eight\nyears, gaining traction due to their profound clinical implications. The\napplication of network science (an interdisciplinary offshoot of graph theory)\nhas facilitated these analyses and enabled examining the brain as an integrated\nsystem that produces complex behaviors. While the field of statistics has been\nintegral in advancing activation analyses and some connectivity analyses in\nfunctional neuroimaging research, it has yet to play a commensurate role in\ncomplex network analyses. Fusing novel statistical methods with network-based\nfunctional neuroimage analysis will engender powerful analytical tools that\nwill aid in our understanding of normal brain function as well as alterations\ndue to various brain disorders. Here we survey widely used statistical and\nnetwork science tools for analyzing fMRI network data and discuss the\nchallenges faced in filling some of the remaining methodological gaps. When\napplied and interpreted correctly, the fusion of network scientific and\nstatistical methods has a chance to revolutionize the understanding of brain\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 21:50:37 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 14:02:12 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2013 20:40:21 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Simpson", "Sean L.", ""], ["Bowman", "F. DuBois", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1302.5743", "submitter": "Jim Gaffney", "authors": "Jim A Gaffney, Dan Clark, Vijay Sonnad, Stephen B Libby", "title": "Bayesian inference of inaccuracies in radiation transport physics from\n  inertial confinement fusion experiments", "comments": null, "journal-ref": "High Energy Density Physics 9:3 457 (2013)", "doi": "10.1016/j.hedp.2013.04.012", "report-no": "LLNL-JRNL-617033", "categories": "physics.plasm-ph physics.atom-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First principles microphysics models are essential to the design and analysis\nof high energy density physics experiments. Using experimental data to\ninvestigate the underlying physics is also essential, particularly when\nsimulations and experiments are not consistent with each other. This is a\ndifficult task, due to the large number of physical models that play a role,\nand due to the complex (and as a result, noisy) nature of the experiments. This\nresults in a large number of parameters that make any inference a daunting\ntask; it is also very important to consistently treat both experimental and\nprior understanding of the problem. In this paper we present a Bayesian method\nthat includes both these effects, and allows the inference of a set of\nmodifiers which have been constructed to give information about microphysics\nmodels from experimental data. We pay particular attention to radiation\ntransport models. The inference takes into account a large set of experimental\nparameters and an estimate of the prior knowledge through a modified $\\chi^{2}$\nfunction, which is minimised using an efficient genetic algorithm. Both factors\nplay an essential role in our analysis. We find that although there is evidence\nof inaccuracies in off-line calculations of X ray drive intensity and Ge $L$\nshell absorption, modifications to radiation transport are unable to reconcile\ndifferences between 1D HYDRA simulations and the experiment.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 00:20:34 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Gaffney", "Jim A", ""], ["Clark", "Dan", ""], ["Sonnad", "Vijay", ""], ["Libby", "Stephen B", ""]]}, {"id": "1302.5745", "submitter": "Jim Gaffney", "authors": "Jim A Gaffney, Dan Clark, Vijay Sonnad, Stephen B Libby", "title": "Development of a Bayesian method for the analysis of inertial\n  confinement fusion experiments on the NIF", "comments": null, "journal-ref": null, "doi": "10.1088/0029-5515/53/7/073032", "report-no": "LLNL-JRNL-614352", "categories": "physics.plasm-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex nature of inertial confinement fusion (ICF) experiments results\nin a very large number of experimental parameters that are only known with\nlimited reliability. These parameters, combined with the myriad physical models\nthat govern target evolution, make the reliable extraction of physics from\nexperimental campaigns very difficult. We develop an inference method that\nallows all important experimental parameters, and previous knowledge, to be\ntaken into account when investigating underlying microphysics models. The\nresult is framed as a modified $\\chi^{2}$ analysis which is easy to implement\nin existing analyses, and quite portable. We present a first application to a\nrecent convergent ablator experiment performed at the NIF, and investigate the\neffect of variations in all physical dimensions of the target (very difficult\nto do using other methods). We show that for well characterised targets in\nwhich dimensions vary at the 0.5% level there is little effect, but 3%\nvariations change the results of inferences dramatically. Our Bayesian method\nallows particular inference results to be associated with prior errors in\nmicrophysics models; in our example, tuning the carbon opacity to match\nexperimental data (i.e., ignoring prior knowledge) is equivalent to an assumed\nprior error of 400% in the tabop opacity tables. This large error is\nunreasonable, underlining the importance of including prior knowledge in the\nanalysis of these experiments.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 00:23:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Gaffney", "Jim A", ""], ["Clark", "Dan", ""], ["Sonnad", "Vijay", ""], ["Libby", "Stephen B", ""]]}, {"id": "1302.5762", "submitter": "Yue Wu", "authors": "Yue Wu and Brian Tracey and Premkumar Natarajan and Joseph P. Noonan", "title": "Probabilistic Non-Local Means", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1109/LSP.2013.2263135", "report-no": null, "categories": "cs.CV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a so-called probabilistic non-local means (PNLM)\nmethod for image denoising. Our main contributions are: 1) we point out defects\nof the weight function used in the classic NLM; 2) we successfully derive all\ntheoretical statistics of patch-wise differences for Gaussian noise; and 3) we\nemploy this prior information and formulate the probabilistic weights truly\nreflecting the similarity between two noisy patches. The probabilistic nature\nof the new weight function also provides a theoretical basis to choose\nthresholds rejecting dissimilar patches for fast computations. Our simulation\nresults indicate the PNLM outperforms the classic NLM and many NLM recent\nvariants in terms of peak signal noise ratio (PSNR) and structural similarity\n(SSIM) index. Encouraging improvements are also found when we replace the NLM\nweights with the probabilistic weights in tested NLM variants.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 04:48:14 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Wu", "Yue", ""], ["Tracey", "Brian", ""], ["Natarajan", "Premkumar", ""], ["Noonan", "Joseph P.", ""]]}, {"id": "1302.5847", "submitter": "Bruno Ribeiro", "authors": "Fabricio Murai, Bruno Ribeiro, Don Towsley, Krista Gile", "title": "Characterizing Branching Processes from Sampled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Branching processes model the evolution of populations of agents that\nrandomly generate offsprings. These processes, more patently Galton-Watson\nprocesses, are widely used to model biological, social, cognitive, and\ntechnological phenomena, such as the diffusion of ideas, knowledge, chain\nletters, viruses, and the evolution of humans through their Y-chromosome DNA or\nmitochondrial RNA. A practical challenge of modeling real phenomena using a\nGalton-Watson process is the offspring distribution, which must be measured\nfrom the population. In most cases, however, directly measuring the offspring\ndistribution is unrealistic due to lack of resources or the death of agents. So\nfar, researchers have relied on informed guesses to guide their choice of\noffspring distribution. In this work we propose two methods to estimate the\noffspring distribution from real sampled data. Using a small sampled fraction\nof the agents and instrumented with the identity of the ancestors of the\nsampled agents, we show that accurate offspring distribution estimates can be\nobtained by sampling as little as 14% of the population.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 21:49:53 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Murai", "Fabricio", ""], ["Ribeiro", "Bruno", ""], ["Towsley", "Don", ""], ["Gile", "Krista", ""]]}, {"id": "1302.5849", "submitter": "Giovanni Montana", "authors": "M. Silver, P. Chen, L. Ruoying, C.Y. Cheng, T.Y. Wong, E. Tai, Y.Y.\n  Teo, G. Montana", "title": "Pathways-driven Sparse Regression Identifies Pathways and Genes\n  Associated with High-density Lipoprotein Cholesterol in Two Asian Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches to analysing data in genome-wide association studies\n(GWAS) ignore any potential functional relationships between genetic markers.\nIn contrast gene pathways analysis uses prior information on functional\nstructure within the genome to identify pathways associated with a trait of\ninterest. In a second step, important single nucleotide polymorphisms (SNPs) or\ngenes may be identified within associated pathways. Most pathways methods begin\nby testing SNPs one at a time, and so fail to capitalise on the potential\nadvantages inherent in a multi-SNP, joint modelling approach. Here we describe\na dual-level, sparse regression model for the simultaneous identification of\npathways, genes and SNPs associated with a quantitative trait. Our method takes\naccount of various factors specific to the joint modelling of pathways with\ngenome-wide data, including widespread correlation between genetic predictors,\nand the fact that variants may overlap multiple pathways. We use a resampling\nstrategy that exploits finite sample variability to provide robust rankings for\npathways, SNPs and genes. We test our method through simulation, and use it to\nperform pathways-driven SNP selection in a search for pathways, genes and SNPs\nassociated with variation in serum high-density lipoprotein cholesterol (HDLC)\nlevels in two separate GWAS cohorts of Asian adults. By comparing results from\nboth cohorts we identify a number of candidate pathways including those\nassociated with cardiomyopathy, and T cell receptor and PPAR signalling.\nHighlighted genes include those associated with the L-type calcium channel,\nadenylate cyclase, integrin, laminin, MAPK signalling and immune function.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 22:10:01 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Silver", "M.", ""], ["Chen", "P.", ""], ["Ruoying", "L.", ""], ["Cheng", "C. Y.", ""], ["Wong", "T. Y.", ""], ["Tai", "E.", ""], ["Teo", "Y. Y.", ""], ["Montana", "G.", ""]]}, {"id": "1302.5857", "submitter": "Giovanni Montana", "authors": "Maurice Berk and Cheryl Hemingway and Michael Levin and Giovanni\n  Montana", "title": "Longitudinal analysis of gene expression profiles using functional\n  mixed-effects models", "comments": null, "journal-ref": "Advanced Statistical Methods for the Analysis of Large Data-Sets.\n  Studies in Theoretical and Applied Statistics 2012, pp 57-67", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many longitudinal microarray studies, the gene expression levels in a\nrandom sample are observed repeatedly over time under two or more conditions.\nThe resulting time courses are generally very short, high-dimensional, and may\nhave missing values. Moreover, for every gene, a certain amount of variability\nin the temporal profiles, among biological replicates, is generally observed.\nWe propose a functional mixed-effects model for estimating the temporal pattern\nof each gene, which is assumed to be a smooth function. A statistical test\nbased on the distance between the fitted curves is then carried out to detect\ndifferential expression. A simulation procedure for assessing the statistical\npower of our model is also suggested. We evaluate the model performance using\nboth simulations and a real data set investigating the human host response to\nBCG exposure.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 23:31:58 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Berk", "Maurice", ""], ["Hemingway", "Cheryl", ""], ["Levin", "Michael", ""], ["Montana", "Giovanni", ""]]}, {"id": "1302.6498", "submitter": "Frederic Pascal", "authors": "F. Pascal and L. Bombrun and J.Y. Tourneret and Y. Berthoumieu", "title": "Parameter Estimation For Multivariate Generalized Gaussian Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2282909", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its heavy-tailed and fully parametric form, the multivariate\ngeneralized Gaussian distribution (MGGD) has been receiving much attention for\nmodeling extreme events in signal and image processing applications.\nConsidering the estimation issue of the MGGD parameters, the main contribution\nof this paper is to prove that the maximum likelihood estimator (MLE) of the\nscatter matrix exists and is unique up to a scalar factor, for a given shape\nparameter \\beta\\in(0,1). Moreover, an estimation algorithm based on a\nNewton-Raphson recursion is proposed for computing the MLE of MGGD parameters.\nVarious experiments conducted on synthetic and real data are presented to\nillustrate the theoretical derivations in terms of number of iterations and\nnumber of samples for different values of the shape parameter. The main\nconclusion of this work is that the parameters of MGGDs can be estimated using\nthe maximum likelihood principle with good performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 17:09:25 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 15:15:05 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pascal", "F.", ""], ["Bombrun", "L.", ""], ["Tourneret", "J. Y.", ""], ["Berthoumieu", "Y.", ""]]}, {"id": "1302.6595", "submitter": "Ratnadip Adhikari", "authors": "Ratnadip Adhikari, R. K. Agrawal", "title": "Combining Multiple Time Series Models Through A Robust Weighted\n  Mechanism", "comments": "6 pages, 3 figures, 2 tables, conference", "journal-ref": "International Conference on Recent Advances in Information\n  Technology, 2012", "doi": "10.1109/RAIT.2012.6194621", "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvement of time series forecasting accuracy through combining multiple\nmodels is an important as well as a dynamic area of research. As a result,\nvarious forecasts combination methods have been developed in literature.\nHowever, most of them are based on simple linear ensemble strategies and hence\nignore the possible relationships between two or more participating models. In\nthis paper, we propose a robust weighted nonlinear ensemble technique which\nconsiders the individual forecasts from different models as well as the\ncorrelations among them while combining. The proposed ensemble is constructed\nusing three well-known forecasting models and is tested for three real-world\ntime series. A comparison is made among the proposed scheme and three other\nwidely used linear combination methods, in terms of the obtained forecast\nerrors. This comparison shows that our ensemble scheme provides significantly\nlower forecast errors than each individual model as well as each of the four\nlinear combination methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 21:01:42 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Adhikari", "Ratnadip", ""], ["Agrawal", "R. K.", ""]]}, {"id": "1302.6625", "submitter": "Paul McNicholas", "authors": "Ryan P. Browne, Paul D. McNicholas and Christopher J. Findlay", "title": "A Partial EM Algorithm for Clustering White Breads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of new products for consumer markets has undergone a major\ntransformation over the last 50 years. Traditionally, inventors would create a\nnew product that they thought might address a perceived need of consumers. Such\nproducts tended to be developed to meet the inventors own perception and not\nnecessarily that of consumers. The social consequence of a top-down approach to\nproduct development has been a large failure rate in new product introduction.\nBy surveying potential customers, a refined target is created that guides\ndevelopers and reduces the failure rate. Today, however, the proliferation of\nproducts and the emergence of consumer choice has resulted in the\nidentification of segments within the market. Understanding your target market\ntypically involves conducting a product category assessment, where 12 to 30\ncommercial products are tested with consumers to create a preference map. Every\nconsumer gets to test every product in a complete-block design; however, many\nclasses of products do not lend themselves to such approaches because only a\nfew samples can be evaluated before `fatigue' sets in. We consider an analysis\nof incomplete balanced-incomplete-block data on 12 different types of white\nbread. A latent Gaussian mixture model is used for this analysis, with a\npartial expectation-maximization (PEM) algorithm developed for parameter\nestimation. This PEM algorithm circumvents the need for a traditional E-step,\nby performing a partial E-step that reduces the Kullback-Leibler divergence\nbetween the conditional distribution of the missing data and the distribution\nof the missing data given the observed data. The results of the white bread\nanalysis are discussed and some mathematical details are given in an appendix.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 23:23:27 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""], ["Findlay", "Christopher J.", ""]]}, {"id": "1302.6906", "submitter": "Jacob G. Foster", "authors": "Jacob G. Foster, Andrey Rzhetsky, James A. Evans", "title": "Tradition and Innovation in Scientists' Research Strategies", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What factors affect a scientist's choice of research problem? Qualitative\nresearch in the history, philosophy, and sociology of science suggests that\nthis choice is shaped by an \"essential tension\" between the professional demand\nfor productivity and a conflicting drive toward risky innovation. We examine\nthis tension empirically in the context of biomedical chemistry. We use complex\nnetworks to represent the evolving state of scientific knowledge, as expressed\nin publications. We then define research strategies relative to these networks.\nScientists can introduce novel chemicals or chemical relationships--or delve\ndeeper into known ones. They can consolidate existing knowledge clusters, or\nbridge distant ones. Analyzing such choices in aggregate, we find that the\ndistribution of strategies remains remarkably stable, even as chemical\nknowledge grows dramatically. High-risk strategies, which explore new chemical\nrelationships, are less prevalent in the literature, reflecting a growing focus\non established knowledge at the expense of new opportunities. Research\nfollowing a risky strategy is more likely to be ignored but also more likely to\nachieve high impact and recognition. While the outcome of a risky strategy has\na higher expected reward than the outcome of a conservative strategy, the\nadditional reward is insufficient to compensate for the additional risk. By\nstudying the winners of 137 different prizes in biomedicine and chemistry, we\nshow that the occasional \"gamble\" for extraordinary impact is the most\nplausible explanation for observed levels of risk-taking. Our empirical\ndemonstration and unpacking of the \"essential tension\" suggests policy\ninterventions that may foster more innovative research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 16:23:31 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Foster", "Jacob G.", ""], ["Rzhetsky", "Andrey", ""], ["Evans", "James A.", ""]]}, {"id": "1302.7056", "submitter": "Wesam Elshamy", "authors": "Wesam Elshamy, Doina Caragea, William Hsu", "title": "KSU KDD: Word Sense Induction by Clustering in Topic Space", "comments": null, "journal-ref": "Proceedings of the 5th International Workshop on Semantic\n  Evaluation, pages 367-370, Uppsala, Sweden, July 2010. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our language-independent unsupervised word sense induction\nsystem. This system only uses topic features to cluster different word senses\nin their global context topic space. Using unlabeled data, this system trains a\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\ndistribution of the test instances. By clustering these topics distributions in\ntheir topic space we cluster them into different senses. Our hypothesis is that\ncloseness in topic space reflects similarity between different word senses.\nThis system participated in SemEval-2 word sense induction and disambiguation\ntask and achieved the second highest V-measure score among all other systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 02:10:38 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Elshamy", "Wesam", ""], ["Caragea", "Doina", ""], ["Hsu", "William", ""]]}, {"id": "1302.7088", "submitter": "Wesam Elshamy", "authors": "Wesam Elshamy", "title": "Continuous-time Infinite Dynamic Topic Models", "comments": "Ph.D. dissertation, Kansas State University, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are probabilistic models for discovering topical themes in\ncollections of documents. In real world applications, these models provide us\nwith the means of organizing what would otherwise be unstructured collections.\nThey can help us cluster a huge collection into different topics or find a\nsubset of the collection that resembles the topical theme found in an article\nat hand.\n  The first wave of topic models developed were able to discover the prevailing\ntopics in a big collection of documents spanning a period of time. It was later\nrealized that these time-invariant models were not capable of modeling 1) the\ntime varying number of topics they discover and 2) the time changing structure\nof these topics. Few models were developed to address this two deficiencies.\nThe online-hierarchical Dirichlet process models the documents with a time\nvarying number of topics. It varies the structure of the topics over time as\nwell. However, it relies on document order, not timestamps to evolve the model\nover time. The continuous-time dynamic topic model evolves topic structure in\ncontinuous-time. However, it uses a fixed number of topics over time.\n  In this dissertation, I present a model, the continuous-time infinite dynamic\ntopic model, that combines the advantages of these two models 1) the\nonline-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic\nmodel. More specifically, the model I present is a probabilistic topic model\nthat does the following: 1) it changes the number of topics over continuous\ntime, and 2) it changes the topic structure over continuous-time.\n  I compared the model I developed with the two other models with different\nsetting values. The results obtained were favorable to my model and showed the\nneed for having a model that has a continuous-time varying number of topics and\ntopic structure.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 05:30:41 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Elshamy", "Wesam", ""]]}]