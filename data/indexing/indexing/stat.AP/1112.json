[{"id": "1112.0210", "submitter": "Karol Wawrzyniak K.W.", "authors": "Karol Wawrzyniak, Wojciech Wislicki", "title": "Mesoscopic approach to minority games in herd regime", "comments": "arXiv admin note: substantial text overlap with arXiv:0907.3231", "journal-ref": null, "doi": "10.1016/j.physa.2011.11.041", "report-no": null, "categories": "nlin.AO cs.MA math.DS q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minority games in efficient regime. By incorporating the utility\nfunction and aggregating agents with similar strategies we develop an effective\nmesoscale notion of state of the game. Using this approach, the game can be\nrepresented as a Markov process with substantially reduced number of states\nwith explicitly computable probabilities. For any payoff, the finiteness of the\nnumber of states is proved. Interesting features of an extensive random\nvariable, called aggregated demand, viz. its strong inhomogeneity and presence\nof patterns in time, can be easily interpreted. Using Markov theory and\nquenched disorder approach, we can explain important macroscopic\ncharacteristics of the game: behavior of variance per capita and predictability\nof the aggregated demand. We prove that in case of linear payoff many\nattractors in the state space are possible.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 21:29:40 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Wawrzyniak", "Karol", ""], ["Wislicki", "Wojciech", ""]]}, {"id": "1112.0504", "submitter": "Kalyani Krishnamurthy", "authors": "Kalyani Krishnamurthy, Rebecca Willett and Maxim Raginsky", "title": "Target Detection Performance Bounds in Compressive Imaging", "comments": "Submitted to the EURASIP Journal on Advances in Signal Processing", "journal-ref": null, "doi": "10.1186/1687-6180-2012-205", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes computationally efficient approaches and associated\ntheoretical performance guarantees for the detection of known targets and\nanomalies from few projection measurements of the underlying signals. The\nproposed approaches accommodate signals of different strengths contaminated by\na colored Gaussian background, and perform detection without reconstructing the\nunderlying signals from the observations. The theoretical performance bounds of\nthe target detector highlight fundamental tradeoffs among the number of\nmeasurements collected, amount of background signal present, signal-to-noise\nratio, and similarity among potential targets coming from a known dictionary.\nThe anomaly detector is designed to control the number of false discoveries.\nThe proposed approach does not depend on a known sparse representation of\ntargets; rather, the theoretical performance bounds exploit the structure of a\nknown dictionary of targets and the distance preservation property of the\nmeasurement matrix. Simulation experiments illustrate the practicality and\neffectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 16:54:40 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2012 16:28:52 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Krishnamurthy", "Kalyani", ""], ["Willett", "Rebecca", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1112.0929", "submitter": "Arthur Charpentier", "authors": "Mathieu Boudreault and Arthur Charpentier", "title": "Multivariate integer-valued autoregressive models applied to earthquake\n  counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various situations in the insurance industry, in finance, in epidemiology,\netc., one needs to represent the joint evolution of the number of occurrences\nof an event. In this paper, we present a multivariate integer-valued\nautoregressive (MINAR) model, derive its properties and apply the model to\nearthquake occurrences across various pairs of tectonic plates. The model is an\nextension of Pedelis & Karlis (2011) where cross autocorrelation (spatial\ncontagion in a seismic context) is considered. We fit various bivariate count\nmodels and find that for many contiguous tectonic plates, spatial contagion is\nsignificant in both directions. Furthermore, ignoring cross autocorrelation can\nunderestimate the potential for high numbers of occurrences over the\nshort-term. Our overall findings seem to further confirm Parsons & Velasco\n(2001).\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 14:11:03 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Boudreault", "Mathieu", ""], ["Charpentier", "Arthur", ""]]}, {"id": "1112.1023", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong", "title": "Weighted KS Statistics for Inference on Conditional Moment Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes confidence regions for the identified set in conditional\nmoment inequality models using Kolmogorov-Smirnov statistics with a truncated\ninverse variance weighting with increasing truncation points. The new weighting\ndiffers from those proposed in the literature in two important ways. First,\nconfidence regions based on KS tests with the weighting function I propose\nconverge to the identified set at a faster rate than existing procedures based\non bounded weight functions in a broad class of models. This provides a\ntheoretical justification for inverse variance weighting in this context, and\ncontrasts with analogous results for conditional moment equalities in which\noptimal weighting only affects the asymptotic variance. Second, the new\nweighting changes the asymptotic behavior, including the rate of convergence,\nof the KS statistic itself, requiring a new asymptotic theory in choosing the\ncritical value, which I provide. To make these comparisons, I derive rates of\nconvergence for the confidence regions I propose along with new results for\nrates of convergence of existing estimators under a general set of conditions.\nA series of examples illustrates the broad applicability of the conditions. A\nmonte carlo study examines the finite sample behavior of the confidence\nregions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 19:00:26 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Armstrong", "Timothy B.", ""]]}, {"id": "1112.1024", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong", "title": "Asymptotically Exact Inference in Conditional Moment Inequality Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives the rate of convergence and asymptotic distribution for a\nclass of Kolmogorov-Smirnov style test statistics for conditional moment\ninequality models for parameters on the boundary of the identified set under\ngeneral conditions. In contrast to other moment inequality settings, the rate\nof convergence is faster than root-$n$, and the asymptotic distribution depends\nentirely on nonbinding moments. The results require the development of new\ntechniques that draw a connection between moment selection, irregular\nidentification, bandwidth selection and nonstandard M-estimation. Using these\nresults, I propose tests that are more powerful than existing approaches for\nchoosing critical values for this test statistic. I quantify the power\nimprovement by showing that the new tests can detect alternatives that converge\nto points on the identified set at a faster rate than those detected by\nexisting approaches. A monte carlo study confirms that the tests and the\nasymptotic approximations they use perform well in finite samples. In an\napplication to a regression of prescription drug expenditures on income with\ninterval data from the Health and Retirement Study, confidence regions based on\nthe new tests are substantially tighter than those based on existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 19:01:16 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Armstrong", "Timothy B.", ""]]}, {"id": "1112.1047", "submitter": "Chris. J. Oates", "authors": "Chris. J. Oates, Sach Mukherjee", "title": "Network inference and biological dynamics", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS532 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1209-1235", "doi": "10.1214/11-AOAS532", "report-no": "IMS-AOAS-AOAS532", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network inference approaches are now widely used in biological applications\nto probe regulatory relationships between molecular components such as genes or\nproteins. Many methods have been proposed for this setting, but the connections\nand differences between their statistical formulations have received less\nattention. In this paper, we show how a broad class of statistical network\ninference methods, including a number of existing approaches, can be described\nin terms of variable selection for the linear model. This reveals some subtle\nbut important differences between the methods, including the treatment of time\nintervals in discretely observed data. In developing a general formulation, we\nalso explore the relationship between single-cell stochastic dynamics and\nnetwork inference on averages over cells. This clarifies the link between\nbiochemical networks as they operate at the cellular level and network\ninference as carried out on data that are averages over populations of cells.\nWe present empirical results, comparing thirty-two network inference methods\nthat are instances of the general formulation we describe, using two published\ndynamical models. Our investigation sheds light on the applicability and\nlimitations of network inference and provides guidance for practitioners and\nsuggestions for experimental design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 20:17:34 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 07:10:56 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Oates", "Chris. J.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1112.1187", "submitter": "Andres Almansa", "authors": "Neus Sabater (CMLA), Andr\\'es Almansa (LTCI), Jean-Michel Morel (CMLA)", "title": "Meaningful Matches in Stereovision", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence 99,\n  Preprints (2011) 1-12", "journal-ref": null, "doi": "10.1109/TPAMI.2011.207", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a statistical method to decide whether two blocks in a\npair of of images match reliably. The method ensures that the selected block\nmatches are unlikely to have occurred \"just by chance.\" The new approach is\nbased on the definition of a simple but faithful statistical \"background model\"\nfor image blocks learned from the image itself. A theorem guarantees that under\nthis model not more than a fixed number of wrong matches occurs (on average)\nfor the whole image. This fixed number (the number of false alarms) is the only\nmethod parameter. Furthermore, the number of false alarms associated with each\nmatch measures its reliability. This \"a contrario\" block-matching method,\nhowever, cannot rule out false matches due to the presence of periodic objects\nin the images. But it is successfully complemented by a parameterless\n\"self-similarity threshold.\" Experimental evidence shows that the proposed\nmethod also detects occlusions and incoherent motions due to vehicles and\npedestrians in non simultaneous stereo.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 08:06:45 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Sabater", "Neus", "", "CMLA"], ["Almansa", "Andr\u00e9s", "", "LTCI"], ["Morel", "Jean-Michel", "", "CMLA"]]}, {"id": "1112.1526", "submitter": "Miguel A. Martinez-Beneito", "authors": "Miguel A. Martinez-Beneito, Gonzalo Garc\\'ia-Donato, Diego Salmer\\'on", "title": "A Bayesian Joinpoint regression model with an unknown number of\n  break-points", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS471 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 2150-2168", "doi": "10.1214/11-AOAS471", "report-no": "IMS-AOAS-AOAS471", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joinpoint regression is used to determine the number of segments needed to\nadequately explain the relationship between two variables. This methodology can\nbe widely applied to real problems, but we focus on epidemiological data, the\nmain goal being to uncover changes in the mortality time trend of a specific\ndisease under study. Traditionally, Joinpoint regression problems have paid\nlittle or no attention to the quantification of uncertainty in the estimation\nof the number of change-points. In this context, we found a satisfactory way to\nhandle the problem in the Bayesian methodology. Nevertheless, this novel\napproach involves significant difficulties (both theoretical and practical)\nsince it implicitly entails a model selection (or testing) problem. In this\nstudy we face these challenges through (i) a novel reparameterization of the\nmodel, (ii) a conscientious definition of the prior distributions used and\n(iii) an encompassing approach which allows the use of MCMC simulation-based\ntechniques to derive the results. The resulting methodology is flexible enough\nto make it possible to consider mortality counts (for epidemiological\napplications) as Poisson variables. The methodology is applied to the study of\nannual breast cancer mortality during the period 1980--2007 in Castell\\'{o}n, a\nprovince in Spain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 11:31:25 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Martinez-Beneito", "Miguel A.", ""], ["Garc\u00eda-Donato", "Gonzalo", ""], ["Salmer\u00f3n", "Diego", ""]]}, {"id": "1112.1527", "submitter": "Konstantinos Themelis", "authors": "Konstantinos E. Themelis, Fr\\'ed\\'eric Schmidt, Olga Sykioti,\n  Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas, Ioannis A. Daglis", "title": "On the unmixing of MEx/OMEGA hyperspectral data", "comments": null, "journal-ref": "Planetary and Space Science, 2011", "doi": "10.1016/j.pss.2011.11.015", "report-no": null, "categories": "astro-ph.IM astro-ph.EP physics.space-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a comparative study of three different types of\nestimators used for supervised linear unmixing of two MEx/OMEGA hyperspectral\ncubes. The algorithms take into account the constraints of the abundance\nfractions, in order to get physically interpretable results. Abundance maps\nshow that the Bayesian maximum a posteriori probability (MAP) estimator\nproposed in Themelis and Rontogiannis (2008) outperforms the other two schemes,\noffering a compromise between complexity and estimation performance. Thus, the\nMAP estimator is a candidate algorithm to perform ice and minerals detection on\nlarge hyperspectral datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 11:33:23 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Themelis", "Konstantinos E.", ""], ["Schmidt", "Fr\u00e9d\u00e9ric", ""], ["Sykioti", "Olga", ""], ["Rontogiannis", "Athanasios A.", ""], ["Koutroumbas", "Konstantinos D.", ""], ["Daglis", "Ioannis A.", ""]]}, {"id": "1112.1541", "submitter": "Danny Pfeffermann", "authors": "Danny Pfeffermann, Victoria Landsman", "title": "Are private schools better than public schools? Appraisal for Ireland by\n  methods for observational studies", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS456 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 1726-1751", "doi": "10.1214/11-AOAS456", "report-no": "IMS-AOAS-AOAS456", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies the assignment of units to treatments is not under\ncontrol. Consequently, the estimation and comparison of treatment effects based\non the empirical distribution of the responses can be biased since the units\nexposed to the various treatments could differ in important unknown\npretreatment characteristics, which are related to the response. An important\nexample studied in this article is the question of whether private schools\noffer better quality of education than public schools. In order to address this\nquestion, we use data collected in the year 2000 by OECD for the Programme for\nInternational Student Assessment (PISA). Focusing for illustration on scores in\nmathematics of 15-year-old pupils in Ireland, we find that the raw average\nscore of pupils in private schools is higher than of pupils in public schools.\nHowever, application of a newly proposed method for observational studies\nsuggests that the less able pupils tend to enroll in public schools, such that\ntheir lower scores are not necessarily an indication of bad quality of the\npublic schools. Indeed, when comparing the average score in the two types of\nschools after adjusting for the enrollment effects, we find quite surprisingly\nthat public schools perform better on average. This outcome is supported by the\nmethods of instrumental variables and latent variables, commonly used by\neconometricians for analyzing and evaluating social programs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 12:22:21 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Pfeffermann", "Danny", ""], ["Landsman", "Victoria", ""]]}, {"id": "1112.1669", "submitter": "Mikhail Simkin", "authors": "M. V. Simkin", "title": "Monkeys get a silver in Abstract Art Olympics", "comments": "Significance, Web exclusive articles, December 6, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiment shows that art students prefer abstract art to monkey art in about\ntwo-third of the cases. Since the number is above 50%, some argue that abstract\nart is different and better than animal art. I compare this result with figure\nskating competitions, where on average 73% of judges prefer gold medalist to\nsilver medalist. This means that the difference between abstract artists and\nanimal artists is less than the difference between gold and silver medalists.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 19:44:43 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Simkin", "M. V.", ""]]}, {"id": "1112.2251", "submitter": "Michalis Vafopoulos n", "authors": "Vafopoulos Michalis and Oikonomou Michael", "title": "Recommendation systems: a joint analysis of technical aspects with\n  marketing implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2010, Web users ordered, only in Amazon, 73 items per second and massively\ncontribute reviews about their consuming experience. As the Web matures and\nbecomes social and participatory, collaborative filters are the basic\ncomplement in searching online information about people, events and products.\nIn Web 2.0, what connected consumers create is not simply content (e.g.\nreviews) but context. This new contextual framework of consumption emerges\nthrough the aggregation and collaborative filtering of personal preferences\nabout goods in the Web in massive scale. More importantly, facilitates\nconnected consumers to search and navigate the complex Web more effectively and\namplifies incentives for quality. The objective of the present article is to\njointly review the basic stylized facts of relevant research in recommendation\nsystems in computer and marketing studies in order to share some common\ninsights. After providing a comprehensive definition of goods and Users in the\nWeb, we describe a classification of recommendation systems based on two\nfamilies of criteria: how recommendations are formed and input data\navailability. The classification is presented under a common minimal matrix\nnotation and is used as a bridge to related issues in the business and\nmarketing literature. We focus our analysis in the fields of one-to-one\nmarketing, network-based marketing Web merchandising and atmospherics and their\nimplications in the processes of personalization and adaptation in the Web.\nMarket basket analysis is investigated in context of recommendation systems.\nDiscussion on further research refers to the business implications and\ntechnological challenges of recommendation systems.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 04:53:28 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Michalis", "Vafopoulos", ""], ["Michael", "Oikonomou", ""]]}, {"id": "1112.2516", "submitter": "Jesper Schneider jws", "authors": "Jesper W. Schneider", "title": "Caveats for using statistical significance tests in research assessments", "comments": "Accepted version for Journal of Informetrics", "journal-ref": null, "doi": "10.1016/j.joi.2012.08.005", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper raises concerns about the advantages of using statistical\nsignificance tests in research assessments as has recently been suggested in\nthe debate about proper normalization procedures for citation indicators.\nStatistical significance tests are highly controversial and numerous criticisms\nhave been leveled against their use. Based on examples from articles by\nproponents of the use statistical significance tests in research assessments,\nwe address some of the numerous problems with such tests. The issues\nspecifically discussed are the ritual practice of such tests, their dichotomous\napplication in decision making, the difference between statistical and\nsubstantive significance, the implausibility of most null hypotheses, the\ncrucial assumption of randomness, as well as the utility of standard errors and\nconfidence intervals for inferential purposes. We argue that applying\nstatistical significance tests and mechanically adhering to their results is\nhighly problematic and detrimental to critical thinking. We claim that the use\nof such tests do not provide any advantages in relation to citation indicators,\ninterpretations of them, or the decision making processes based upon them. On\nthe contrary their use may be harmful. Like many other critics, we generally\nbelieve that statistical significance tests are over- and misused in the social\nsciences including scientometrics and we encourage a reform on these matters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 11:57:12 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2012 07:15:27 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Schneider", "Jesper W.", ""]]}, {"id": "1112.2889", "submitter": "Juan Jimenez", "authors": "I. Garcia and J. Jimenez", "title": "Estimating financial risk using piecewise Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational method for measuring financial risk by estimating\nthe Value at Risk and Expected Shortfall from financial series. We have made\ntwo assumptions: First, that the predictive distributions of the values of an\nasset are conditioned by information on the way in which the variable evolves\nfrom similar conditions, and secondly, that the underlying random processes can\nbe described using piecewise Gaussian processes. The performance of the method\nwas evaluated by using it to estimate VaR and ES for a daily data series taken\nfrom the S&P500 index and applying a backtesting procedure recommended by the\nBasel Committee on Banking Supervision. The results indicated a satisfactory\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 13:48:23 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Garcia", "I.", ""], ["Jimenez", "J.", ""]]}, {"id": "1112.3250", "submitter": "Richard B. Chandler", "authors": "Richard B. Chandler, J. Andrew Royle", "title": "Spatially explicit models for inference about density in unmarked or\n  partially marked populations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS610 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 936-954", "doi": "10.1214/12-AOAS610", "report-no": "IMS-AOAS-AOAS610", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed spatial capture-recapture (SCR) models represent a major\nadvance over traditional capture-recapture (CR) models because they yield\nexplicit estimates of animal density instead of population size within an\nunknown area. Furthermore, unlike nonspatial CR methods, SCR models account for\nheterogeneity in capture probability arising from the juxtaposition of animal\nactivity centers and sample locations. Although the utility of SCR methods is\ngaining recognition, the requirement that all individuals can be uniquely\nidentified excludes their use in many contexts. In this paper, we develop\nmodels for situations in which individual recognition is not possible, thereby\nallowing SCR concepts to be applied in studies of unmarked or partially marked\npopulations. The data required for our model are spatially referenced counts\nmade on one or more sample occasions at a collection of closely spaced sample\nunits such that individuals can be encountered at multiple locations. Our\napproach includes a spatial point process for the animal activity centers and\nuses the spatial correlation in counts as information about the number and\nlocation of the activity centers. Camera-traps, hair snares, track plates,\nsound recordings, and even point counts can yield spatially correlated count\ndata, and thus our model is widely applicable. A simulation study demonstrated\nthat while the posterior mean exhibits frequentist bias on the order of 5-10%\nin small samples, the posterior mode is an accurate point estimator as long as\nadequate spatial correlation is present. Marking a subset of the population\nsubstantially increases posterior precision and is recommended whenever\npossible. We applied our model to avian point count data collected on an\nunmarked population of the northern parula (Parula americana) and obtained a\ndensity estimate (posterior mode) of 0.38 (95% CI: 0.19-1.64) birds/ha. Our\npaper challenges sampling and analytical conventions in ecology by\ndemonstrating that neither spatial independence nor individual recognition is\nneeded to estimate population density - rather, spatial dependence can be\ninformative about individual distribution and density.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 15:35:05 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 09:25:37 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Chandler", "Richard B.", ""], ["Royle", "J. Andrew", ""]]}, {"id": "1112.3329", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela, Tommi Vatanen, Eric Malmi, Tapani Raiko, Timo Aaltonen\n  and Yoshikazu Nagai", "title": "Semi-Supervised Anomaly Detection - Towards Model-Independent Searches\n  of New Physics", "comments": "Proceedings of ACAT 2011 conference (Uxbridge, UK), 9 pages, 4\n  figures", "journal-ref": null, "doi": "10.1088/1742-6596/368/1/012032", "report-no": null, "categories": "physics.data-an hep-ex stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification algorithms used in high energy physics fall under the\ncategory of supervised machine learning. Such methods require a training set\ncontaining both signal and background events and are prone to classification\nerrors should this training data be systematically inaccurate for example due\nto the assumed MC model. To complement such model-dependent searches, we\npropose an algorithm based on semi-supervised anomaly detection techniques,\nwhich does not require a MC training sample for the signal data. We first model\nthe background using a multivariate Gaussian mixture model. We then search for\ndeviations from this model by fitting to the observations a mixture of the\nbackground model and a number of additional Gaussians. This allows us to\nperform pattern recognition of any anomalous excess over the background. We\nshow by a comparison to neural network classifiers that such an approach is a\nlot more robust against misspecification of the signal MC than supervised\nclassification. In cases where there is an unexpected signal, a neural network\nmight fail to correctly identify it, while anomaly detection does not suffer\nfrom such a limitation. On the other hand, when there are no systematic errors\nin the training data, both methods perform comparably.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 20:32:34 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 12:43:16 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2012 20:45:26 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Kuusela", "Mikael", ""], ["Vatanen", "Tommi", ""], ["Malmi", "Eric", ""], ["Raiko", "Tapani", ""], ["Aaltonen", "Timo", ""], ["Nagai", "Yoshikazu", ""]]}, {"id": "1112.3627", "submitter": "Mikhail Simkin", "authors": "M. V. Simkin", "title": "Mathematical proof of fraud in Russian elections unsound", "comments": null, "journal-ref": "Significance, Web Exclusive Articles, December 15, 2011", "doi": null, "report-no": null, "categories": "physics.pop-ph physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Washigton Post had published allegations, that results of Russian\nelections \"violate Gauss's groundbreaking work on statistics.\" I show that\nthese allegations lack scientific basis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 19:59:55 GMT"}], "update_date": "2012-01-13", "authors_parsed": [["Simkin", "M. V.", ""]]}, {"id": "1112.3652", "submitter": "Stefano Andreon", "authors": "S. Andreon (INAF-OABrera)", "title": "Understanding better (some) astronomical data using Bayesian methods", "comments": "Largely based on an invited talk at ISI 2011 - 58th World Statistics\n  Congress, Dublin. To appear as chapter 2 of the upcoming \"Astrostatistical\n  Challenges for the New Astronomy\" book (ed. J. Hilbe) for Springer Series on\n  Astrostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.SR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current analysis of astronomical data are confronted with the daunting task\nof modeling the awkward features of astronomical data, among which\nheteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data\ncollection (selection effects), data structure, non-uniform populations (often\ncalled Malmquist bias), non-Gaussian data, and upper/lower limits. This chapter\nshows, by examples, how modeling all these features using Bayesian methods. In\nshort, one just need to formalize, using maths, the logical link between the\ninvolved quantities, how the data arise and what we already known on the\nquantities we want to study. The posterior probability distribution summarizes\nwhat we known on the studied quantities after the data, and we should not be\nafraid about their actual numerical computation, because it is left to\n(special) Monte Carlo programs such as JAGS. As examples, we show how to\npredict the mass of a new object disposing of a calibrating sample, how to\nconstraint cosmological parameters from supernovae data and how to check if the\nfitted data are in tension with the adopted fitting model. Examples are given\nwith their coding. These examples can be easily used as template for completely\ndifferent analysis, on totally unrelated astronomical objects, requiring to\nmodel the same awkward data features.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 10:43:27 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Andreon", "S.", "", "INAF-OABrera"]]}, {"id": "1112.3944", "submitter": "Ge Wang", "authors": "J.S. Yang, M.W. Vannier, F. Wang, Y. Deng, F.R. Ou, J.R. Bennett, Y.\n  Liu, G. Wang", "title": "Scientific Productivity, Research Funding, Race and Ethnicity", "comments": "12 pages, 5 tables, and 46 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent study by Ginther et al., the probability of receiving a U.S.\nNational Institutes of Health (NIH) RO1 award was related to the applicant's\nrace/ethnicity. The results indicate black/African-American applicants were 10%\nless likely than white peers to receive an award, after controlling for\nbackground and qualifications. It has generated a widespread debate regarding\nthe unfairness of the NIH grant review process and its correction. In this\npaper, the work by Ginther et al. was augmented by pairing analysis,\naxiomatically-individualized productivity and normalized funding success\nmeasurement. Although there are racial differences in R01 grant success rates,\nnormalized figures of merit for funding success explain the discrepancy. The\nsuggested \"leverage points for policy intervention\" are in question and require\ndeeper and more thorough investigations. Further adjustments in policies to\nremove racial disparity should be made more systematically for equal\nopportunity, rather than being limited to the NIH review process.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 20:27:59 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Yang", "J. S.", ""], ["Vannier", "M. W.", ""], ["Wang", "F.", ""], ["Deng", "Y.", ""], ["Ou", "F. R.", ""], ["Bennett", "J. R.", ""], ["Liu", "Y.", ""], ["Wang", "G.", ""]]}, {"id": "1112.4180", "submitter": "Sandra Plancade", "authors": "Sandra Plancade and Yves Rozenholc and Eiliv Lund", "title": "Generalization of the normal-exponential model: exploration of a more\n  accurate parametrisation for the signal distribution on Illumina BeadArrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Illumina BeadArray technology includes negative control features\nthat allow a precise estimation of the background noise. As an alternative to\nthe background subtraction proposed in BeadStudio which leads to an important\nloss of information by generating negative values, a background correction\nmethod modeling the observed intensities as the sum of the exponentially\ndistributed signal and normally distributed noise has been developed.\nNevertheless, Wang and Ye (2011) display a kernel-based estimator of the signal\ndistribution on Illumina BeadArrays and suggest that a gamma distribution would\nrepresent a better modeling of the signal density. Hence, the\nnormal-exponential modeling may not be appropriate for Illumina data and\nbackground corrections derived from this model may lead to wrong estimation.\nResults: We propose a more flexible modeling based on a gamma distributed\nsignal and a normal distributed background noise and develop the associated\nbackground correction. Our model proves to be markedly more accurate to model\nIllumina BeadArrays: on the one hand, this model offers a more correct fit of\nthe observed intensities. On the other hand, the comparison of the operating\ncharacteristics of several background correction procedures on spike-in and on\nnormal-gamma simulated data shows high similarities, reinforcing the validation\nof the normal-gamma modeling. The performance of the background corrections\nbased on the normal-gamma and normal-exponential models are compared on two\ndilution data sets. Surprisingly, we observe that the implementation of a more\naccurate parametrisation in the model-based background correction does not\nincrease the sensitivity. These results may be explained by the operating\ncharacteristics of the estimators: the normal-gamma background correction\noffers an improvement in terms of bias, but at the cost of a loss in precision.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 19:53:10 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 20:08:53 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Plancade", "Sandra", ""], ["Rozenholc", "Yves", ""], ["Lund", "Eiliv", ""]]}, {"id": "1112.4372", "submitter": "Geert Barentsen", "authors": "Geert Barentsen, Rainer Arlt, Hans-Erich Fr\\\"ohlich", "title": "Estimating meteor rates using Bayesian inference", "comments": "WGN, Journal of the International Meteor Organization, 39:5 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for estimating the true meteor rate \\lambda\\ from a small number of\nobserved meteors n is derived. We employ Bayesian inference with a Poissonian\nlikelihood function. We discuss the choice of a suitable prior and propose the\nadoption of Jeffreys prior, P(\\lambda)=\\lambda^{-0.5}, which yields an\nexpectation value E(\\lambda) = n+0.5 for any n \\geq 0. We update the ZHR meteor\nactivity formula accordingly, and explain how 68%- and 95%-confidence intervals\ncan be computed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 15:35:28 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Barentsen", "Geert", ""], ["Arlt", "Rainer", ""], ["Fr\u00f6hlich", "Hans-Erich", ""]]}, {"id": "1112.4534", "submitter": "Cristin Buescu", "authors": "Cristin Buescu, Michael Taksar and Fatoumata J. Kon\\'e", "title": "An application of the method of moments to volatility estimation using\n  daily high, low, opening and closing prices", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.PR q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the expectation of the range of an arithmetic Brownian motion and the\nmethod of moments on the daily high, low, opening and closing prices to\nestimate the volatility of the stock price. The daily price jump at the opening\nis considered to be the result of the unobserved evolution of an after-hours\nvirtual trading day.The annualized volatility is used to calculate\nBlack-Scholes prices for European options, and a trading strategy is devised to\nprofit when these prices differ flagrantly from the market prices.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 00:04:36 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["Buescu", "Cristin", ""], ["Taksar", "Michael", ""], ["Kon\u00e9", "Fatoumata J.", ""]]}, {"id": "1112.4675", "submitter": "Siew Li  Tan", "authors": "Siew Li Tan and David J. Nott", "title": "Variational approximation for mixtures of linear mixed models", "comments": "36 pages, 5 figures, 2 tables, submitted to JCGS", "journal-ref": "Journal of Computational and Graphical Statistics. Volume 23,\n  Issue 2, 2014, pages 564-585", "doi": "10.1080/10618600.2012.761138", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of linear mixed models (MLMMs) are useful for clustering grouped\ndata and can be estimated by likelihood maximization through the EM algorithm.\nThe conventional approach to determining a suitable number of components is to\ncompare different mixture models using penalized log-likelihood criteria such\nas BIC.We propose fitting MLMMs with variational methods which can perform\nparameter estimation and model selection simultaneously. A variational\napproximation is described where the variational lower bound and parameter\nupdates are in closed form, allowing fast evaluation. A new variational greedy\nalgorithm is developed for model selection and learning of the mixture\ncomponents. This approach allows an automatic initialization of the algorithm\nand returns a plausible number of mixture components automatically. In cases of\nweak identifiability of certain model parameters, we use hierarchical centering\nto reparametrize the model and show empirically that there is a gain in\nefficiency by variational algorithms similar to that in MCMC algorithms.\nRelated to this, we prove that the approximate rate of convergence of\nvariational algorithms by Gaussian approximation is equal to that of the\ncorresponding Gibbs sampler which suggests that reparametrizations can lead to\nimproved convergence in variational algorithms as well.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 12:59:09 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 09:28:56 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Tan", "Siew Li", ""], ["Nott", "David J.", ""]]}, {"id": "1112.5006", "submitter": "Gilles Guillot", "authors": "Gilles Guillot, Sabrina Renaud, Ronan Ledevin, Joahn Michaux, Julien\n  Claude", "title": "A Unifying Model for the Analysis of Phenotypic, Genetic and Geographic\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of evolutionary units (species, populations) requires integrating\nseveral kinds of data such as genetic or phenotypic markers or spatial\ninformation, in order to get a comprehensive view concerning the\ndifferentiation of the units. We propose a statistical model with a double\noriginal advantage: (i) it incorporates information about the spatial\ndistribution of the samples, with the aim to increase inference power and to\nrelate more explicitly observed patterns to geography; and (ii) it allows one\nto analyze genetic and phenotypic data within a unified model and inference\nframework, thus opening the way to robust comparisons between markers and\npossibly combined analyzes. We show from simulated data as well are real data\nfrom the literature that our method estimates parameters accurately and\nimproves alternative approaches in many situations. The interest of this method\nis exemplified using an intricate case of inter- and intra-species\ndifferentiation based on an original data-set of georeferenced genetic and\nmorphometric markers obtained on {\\em Myodes} voles from Sweden. A computer\nprogram is made available as an extension of the R package Geneland.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 12:34:40 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Guillot", "Gilles", ""], ["Renaud", "Sabrina", ""], ["Ledevin", "Ronan", ""], ["Michaux", "Joahn", ""], ["Claude", "Julien", ""]]}, {"id": "1112.5794", "submitter": "Jie Hao", "authors": "Jie Hao, William Astle, Maria De Iorio, and Timothy Ebbels", "title": "BATMAN-an R package for the automated quantification of metabolites from\n  NMR spectra using a Bayesian Model", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: NMR spectra are widely used in metabolomics to obtain metabolite\nprofiles in complex biological mixtures. Common methods used to assign and\nestimate concentrations of metabolites involve either an expert manual peak\nfitting or extra pre-processing steps, such as peak alignment and binning. Peak\nfitting is very time consuming and is subject to human error. Conversely,\nalignment and binning can introduce artefacts and limit immediate biological\ninterpretation of models. Results: We present the Bayesian AuTomated Metabolite\nAnalyser for NMR spectra (BATMAN), an R package which deconvolutes peaks from\n1-dimensional NMR spectra, automatically assigns them to specific metabolites\nfrom a target list and obtains concentration estimates. The Bayesian model\nincorporates information on charac-teristic peak patterns of metabolites and is\nable to account for shifts in the position of peaks commonly seen in NMR\nspectra of biological samples. It applies a Markov Chain Monte Carlo (MCMC)\nalgorithm to sample from a joint posterior distribution of the model parameters\nand obtains concentration estimates with reduced error compared with\nconventional numerical integration and comparable to manual deconvolution by\nexperienced spectroscopists. Availability:\nhttp://www1.imperial.ac.uk/medicine/people/t.ebbels/ Contact:\nt.ebbels@imperial.ac.uk\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2011 20:30:40 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 12:30:09 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Hao", "Jie", ""], ["Astle", "William", ""], ["De Iorio", "Maria", ""], ["Ebbels", "Timothy", ""]]}, {"id": "1112.5802", "submitter": "Teng Guo", "authors": "Teng Guo, Lingyi Hu", "title": "Economic Determinants of Happiness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scholars have recently begun to dispute the assumed link between\nindividual wellbeing and economic conditions and the extent to which the latter\nmatters (Easterlin, 1995; Stevenson and Wolfers 2008; Tella and MacCulloch\n2008). This dilemma is empirically demonstrated in the Latin America Public\nOpinion Project (LAPOP, 2011), which surveyed North and Latin America in terms\nof perceived life satisfaction. Higher measures found in the less developed\ncountries of Brazil, Costa Rica, and Panama than in North America pose an\nintriguing quandary to traditional economic theory. In light of this\npredicament this paper aims to construct a sensible measure of the national\nhappiness level for the United States on a year by year basis; and regress this\nagainst indicators of the national economy to provide insight into this\npuzzling enigma between national happiness and economic forces\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2011 23:52:15 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Guo", "Teng", ""], ["Hu", "Lingyi", ""]]}, {"id": "1112.5966", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "Calculation of the mean duration and age of onset of a chronic disease\n  and application to dementia in Germany", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper descibes a new method of calculating the mean duration and mean\nage of onset of a chronic disease from incidence and mortality rates. It is\nbased on an ordinary differential equation resulting from a simple compartment\nmodel. Applicability of the method is demonstrated in data about dementia in\nGermany.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 14:23:55 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1112.6015", "submitter": "Suleiman Khan", "authors": "Suleiman A. Khan, Ali Faisal, John Patric Mpindi, Juuso A. Parkkinen,\n  Tuomo Kalliokoski, Antti Poso, Olli P. Kallioniemi, Krister Wennerberg,\n  Samuel Kaski", "title": "Comprehensive data-driven analysis of the impact of chemoinformatic\n  structure on the genome-wide biological response profiles of cancer cells to\n  1159 drugs", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detailed and systematic understanding of the biological effects of millions\nof available compounds on living cells is a significant challenge. As most\ncompounds impact multiple targets and pathways, traditional methods for\nanalyzing structure-function relationships are not comprehensive enough.\nTherefore more advanced integrative models are needed for predicting biological\neffects elicited by specific chemical features. As a step towards creating such\ncomputational links we developed a data-driven chemical systems biology\napproach to comprehensively study the relationship of 76 structural\n3D-descriptors (VolSurf, chemical space) of 1159 drugs with the gene expression\nresponses (biological space) they elicited in three cancer cell lines. The\nanalysis covering 11350 genes was based on data from the Connectivity Map. We\ndecomposed these biological response profiles into components, each linked to a\ncharacteristic chemical descriptor profile. The integrated quantitative\nanalysis of the chemical and biological spaces was more informative about\nprotein-target based drug similarity than either dataset separately. We\nidentified ten major components that link distinct VolSurf features across\nmultiple compounds to specific biological activity types. For example,\ncomponent 2 (hydrophobic properties) strongly links to DNA damage response,\nwhile component 3 (hydrogen bonding) connects to metabolic stress. Individual\nstructural and biological features were often linked to one cell line only,\nsuch as leukemia cells (HL-60) specifically responding to cardiac glycosides.\nIn summary, our approach identified specific chemical structures shared across\nmultiple drugs causing distinct biological responses. The decoding of such\nsystematic chemical-biological relationships is necessary to build better\nmodels of drug effects, including unidentified types of molecular properties\nwith strong biological effects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 20:40:19 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Khan", "Suleiman A.", ""], ["Faisal", "Ali", ""], ["Mpindi", "John Patric", ""], ["Parkkinen", "Juuso A.", ""], ["Kalliokoski", "Tuomo", ""], ["Poso", "Antti", ""], ["Kallioniemi", "Olli P.", ""], ["Wennerberg", "Krister", ""], ["Kaski", "Samuel", ""]]}, {"id": "1112.6151", "submitter": "Robert J. Adler", "authors": "Robert J. Adler, Eliran Subag, Jonathan E. Taylor", "title": "Rotation and scale space random fields and the Gaussian kinematic\n  formula", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1055 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 6, 2910-2942", "doi": "10.1214/12-AOS1055", "report-no": "IMS-AOS-AOS1055", "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new approach, along with extensions, to results in two important\npapers of Worsley, Siegmund and coworkers closely tied to the statistical\nanalysis of fMRI (functional magnetic resonance imaging) brain data. These\npapers studied approximations for the exceedence probabilities of scale and\nrotation space random fields, the latter playing an important role in the\nstatistical analysis of fMRI data. The techniques used there came either from\nthe Euler characteristic heuristic or via tube formulae, and to a large extent\nwere carefully attuned to the specific examples of the paper. This paper treats\nthe same problem, but via calculations based on the so-called Gaussian\nkinematic formula. This allows for extensions of the Worsley-Siegmund results\nto a wide class of non-Gaussian cases. In addition, it allows one to obtain\nresults for rotation space random fields in any dimension via reasonably\nstraightforward Riemannian geometric calculations. Previously only the\ntwo-dimensional case could be covered, and then only via computer algebra. By\nadopting this more structured approach to this particular problem, a solution\npath for other, related problems becomes clearer.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2011 17:13:01 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 08:31:42 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Adler", "Robert J.", ""], ["Subag", "Eliran", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1112.6390", "submitter": "Reason Machete", "authors": "Reason Lesego Machete", "title": "Early Warning with Calibrated and Sharper Probabilistic Forecasts", "comments": "23 pages, 3 figures. Accepted for publication in Journal of\n  Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a nonlinear model, a probabilistic forecast may be obtained by Monte\nCarlo simulations. At a given forecast horizon, Monte Carlo simulations yield\nsets of discrete forecasts, which can be converted to density forecasts. The\nresulting density forecasts will inevitably be downgraded by model\nmis-specification. In order to enhance the quality of the density forecasts,\none can mix them with the unconditional density. This paper examines the value\nof combining conditional density forecasts with the unconditional density. The\nfindings have positive implications for issuing early warnings in different\ndisciplines including economics and meteorology, but UK inflation forecasts are\nconsidered as an example.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 19:31:48 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2012 16:29:12 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2012 14:38:23 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Machete", "Reason Lesego", ""]]}, {"id": "1112.6424", "submitter": "Peter Waddell", "authors": "Peter J. Waddell, Jorge Ramos and Xi Tan", "title": "Homo denisova, Correspondence Spectral Analysis, Finite Sites Reticulate\n  Hierarchical Coalescent Models and the Ron Jeremy Hypothesis", "comments": "43 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows how to fit reticulate finite and infinite sites sequence\nspectra to aligned data from five modern human genomes (San, Yoruba, French,\nHan and Papuan) plus two archaic humans (Denisovan and Neanderthal), to better\ninfer demographic parameters. These include interbreeding between distinct\nlineages. Major improvements in the fit of the sequence spectrum are made with\nsuccessively more complicated models. Findings include some evidence of a male\nbiased gene flow from the Denisova lineage to Papuan ancestors and possibly\neven more archaic gene flow. It is unclear if there is evidence for more than\none Neanderthal interbreeding, as the evidence suggesting this largely\ndisappears when a finite sites model is fitted.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 20:50:13 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Waddell", "Peter J.", ""], ["Ramos", "Jorge", ""], ["Tan", "Xi", ""]]}]