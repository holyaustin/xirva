[{"id": "1801.00274", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio and Giovanna Jona Lasinio and Alessio Pollice\n  and Giulia Capotorti and Lorenzo Teodonio and Giulio Genova and Carlo Blasi", "title": "A Hierarchical Multivariate Spatio-Temporal Model for Large Clustered\n  Climate data with Annual Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multivariate hierarchical space-time model to describe the joint\nseries of monthly extreme temperatures and amounts of rainfall. Data are\navailable for 360 monitoring stations over 60 years, with missing data\naffecting almost all series. Model components account for spatio-temporal\ndependence with annual cycles, dependence on covariates and between responses.\nThe very large amount of data is tackled modeling the spatio-temporal\ndependence by the nearest neighbor Gaussian process. Response multivariate\ndependencies are described using the linear model of coregionalization, while\nannual cycles are assessed by a circular representation of time. The proposed\napproach allows imputation of missing values and easy interpolation of climate\nsurfaces at the national level. The motivation behind is the characterization\nof the so called ecoregions over the Italian territory. Ecoregions delineate\nbroad and discrete ecologically homogeneous areas of similar potential as\nregards the climate, physiography, hydrography, vegetation and wildlife, and\nprovide a geographic framework for interpreting ecological processes,\ndisturbance regimes, vegetation patterns and dynamics. To now, the two main\nItalian macro-ecoregions are hierarchically arranged into 35 zones. The current\nclimatic characterization of Italian ecoregions is based on data and\nbioclimatic indices for the period 1955-1985 and requires an appropriate\nupdate.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 12:13:45 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Lasinio", "Giovanna Jona", ""], ["Pollice", "Alessio", ""], ["Capotorti", "Giulia", ""], ["Teodonio", "Lorenzo", ""], ["Genova", "Giulio", ""], ["Blasi", "Carlo", ""]]}, {"id": "1801.00344", "submitter": "Gen Li", "authors": "Gen Li, Jianhua Z. Huang, Haipeng Shen", "title": "To Wait or Not to Wait: Two-way Functional Hazards Model for\n  Understanding Waiting in Call Centers", "comments": "to appear in JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telephone call centers offer a convenient communication channel between\nbusinesses and their customers. Efficient management of call centers needs\naccurate modeling of customer waiting behavior, which contains important\ninformation about customer patience (how long a customer is willing to wait)\nand service quality (how long a customer needs to wait to get served). Hazard\nfunctions offer dynamic characterization of customer waiting behavior, and\nprovide critical inputs for agent scheduling. Motivated by this application, we\ndevelop a two-way functional hazards (tF-Hazards) model to study customer\nwaiting behavior as a function of two timescales, waiting duration and the time\nof day that a customer calls in. The model stems from a two-way piecewise\nconstant hazard function, and imposes low-rank structure and smoothness on the\nhazard rates to enhance interpretability. We exploit an alternating direction\nmethod of multipliers (ADMM) algorithm to optimize a penalized likelihood\nfunction of the model. We carefully analyze the data from a US bank call\ncenter, and provide informative insights about customer patience and service\nquality patterns along waiting time and across different times of a day. The\nfindings provide primitive inputs for call center agent staffing and\nscheduling, as well as for call center practitioners to understand the effect\nof system protocols on customer waiting behavior.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 19:45:27 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Li", "Gen", ""], ["Huang", "Jianhua Z.", ""], ["Shen", "Haipeng", ""]]}, {"id": "1801.00434", "submitter": "Shuvashree Mondal", "authors": "Shuvashree Mondal, Debasis Kundu", "title": "Inference on Weibull Parameters Under a Balanced Two Sample Type-II\n  Progressive Censoring Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The progressive censoring scheme has received considerable amount of\nattention in the last fifteen years. During the last few years joint\nprogressive censoring scheme has gained some popularity. Recently, the authors\nMondal and Kundu (\"A new two sample Type-II progressive censoring scheme\",\narXiv:1609.05805) introduced a balanced two sample Type-II progressive\ncensoring scheme and provided the exact inference when the two populations are\nexponentially distributed. In this article we consider the case when the two\npopulations follow Weibull distributions with the common shape parameter and\ndifferent scale parameters. We obtain the maximum likelihood estimators of the\nunknown parameters. It is observed that the maximum likelihood estimators\ncannot be obtained in explicit forms, hence, we propose approximate maximum\nlikelihood estimators, which can be obtained in explicit forms. We construct\nthe asymptotic and bootstrap confidence intervals of the population parameters.\nFurther we derive an exact joint confidence region of the unknown parameters.\nWe propose an objective function based on the expected volume of this\nconfidence set and using that we obtain the optimum progressive censoring\nscheme. Extensive simulations have been performed to see the performances of\nthe proposed method, and one real data set has been analyzed for illustrative\npurposes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 11:59:36 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Mondal", "Shuvashree", ""], ["Kundu", "Debasis", ""]]}, {"id": "1801.00528", "submitter": "Ronald Rivest", "authors": "Ronald L. Rivest", "title": "Bayesian Tabulation Audits: Explained and Extended", "comments": "49 pages, 3 figures Version 2 of the paper replaces use of the bare\n  Dirichlet model with the Dirichlet-multinomial, which is more standard and\n  works better in edge cases, such as when the sample is very large", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabulation audits for an election provide statistical evidence that a\nreported contest outcome is \"correct\" (meaning that the tabulation of votes was\nproperly performed), or else the tabulation audit determines the correct\noutcome.\n  Stark proposed risk-limiting tabulation audits for this purpose; such audits\nare effective and are beginning to be used in practice.\n  We expand the study of election audits based on Bayesian methods, first\nintroduced by Rivest and Shen in 2012. (The risk-limiting audits proposed by\nStark are \"frequentist\" rather than Bayesian in character.)\n  We first provide a simplified presentation of Bayesian tabulation audits. A\nBayesian tabulation audit begins by drawing a random sample of the votes in\nthat contest, and tallying those votes. It then considers what effect\nstatistical variations of this tally have on the contest outcome. If such\nvariations almost always yield the previously-reported outcome, the audit\nterminates, accepting the reported outcome. Otherwise the audit is repeated\nwith an enlarged sample.\n  Bayesian audits are attractive because they work with any method for\ndetermining the winner (such as ranked-choice voting).\n  We then show how Bayesian audits may be extended to handle more complex\nsituations, such as auditing contests that \\emph{span multiple jurisdictions},\nor are otherwise \"stratified.\"\n  We highlight the auditing of such multiple-jurisdiction contests where some\nof the jurisdictions have an electronic cast vote record (CVR) for each cast\npaper vote, while the others do not. Complex situations such as this may arise\nnaturally when some counties in a state have upgraded to new equipment, while\nothers have not. Bayesian audits are able to handle such situations in a\nstraightforward manner.\n  We also discuss the benefits and relevant considerations for using Bayesian\naudits in practice.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 00:00:15 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 23:41:18 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Rivest", "Ronald L.", ""]]}, {"id": "1801.00641", "submitter": "Xiao-Pu Han", "authors": "Pan Liu, Xiao-Pu Han, Linyuan L\\\"u", "title": "Triangle-mapping Analysis on Spatial Competition and Cooperation of\n  Chinese Cities", "comments": "The 43rd Annual Conference of the IEEE Industrial Electronics Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we empirically analyze the spatial distribution of Chinese\ncities using a method based on triangle transition. This method uses a regular\ntriangle mapping from the observed cities and its three neighboring cities to\nanalyze their distribution of mapping positions. We find that obvious\ncenter-gathering tendency for the relationship between cities and its nearest\nthree cities, indicating the spatial competition between cities. Moreover, we\nobserved the competitive trends between neighboring cities with similar\neconomic volume, and the remarkable cooperative tendency between neighboring\ncities with large difference on economy. The threshold of the ratio of the two\ncities' economic volume on the transition from competition to cooperation is\nabout 1.2. These findings are helpful in the understanding of the cities\neconomic relationship, especially in the study of competition and cooperation\nbetween cities.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 13:38:00 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Liu", "Pan", ""], ["Han", "Xiao-Pu", ""], ["L\u00fc", "Linyuan", ""]]}, {"id": "1801.00727", "submitter": "David Heckerman", "authors": "David Heckerman", "title": "Accounting for hidden common causes when inferring cause and effect from\n  observational data", "comments": "Presented at the NIPS workshop on causal inference (NIPS 2017), Long\n  Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal relationships from observation data is difficult, in large\npart, due to the presence of hidden common causes. In some cases, where just\nthe right patterns of conditional independence and dependence lie in the\ndata---for example, Y-structures---it is possible to identify cause and effect.\nIn other cases, the analyst deliberately makes an uncertain assumption that\nhidden common causes are absent, and infers putative causal relationships to be\ntested in a randomized trial. Here, we consider a third approach, where there\nare sufficient clues in the data such that hidden common causes can be\ninferred.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:04:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 23:29:07 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Heckerman", "David", ""]]}, {"id": "1801.00736", "submitter": "Manuel Oviedo de la Fuente", "authors": "Manuel Febrero-Bande, Wenceslao Gonz\\'alez-Manteiga and Manuel Oviedo\n  de la Fuente", "title": "Variable selection in Functional Additive Regression Models", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of variable selection in regression models\nin the case of functional variables that may be mixed with other type of\nvariables (scalar, multivariate, directional, etc.). Our proposal begins with a\nsimple null model and sequentially selects a new variable to be incorporated\ninto the model based on the use of distance correlation proposed by\n\\cite{Szekely2007}. For the sake of simplicity, this paper only uses additive\nmodels. However, the proposed algorithm may assess the type of contribution\n(linear, non linear, ...) of each variable. The algorithm has shown quite\npromising results when applied to simulations and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:28:34 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 16:55:42 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Febrero-Bande", "Manuel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["de la Fuente", "Manuel Oviedo", ""]]}, {"id": "1801.00775", "submitter": "Alexander McLain", "authors": "Alexander C. McLain, Rajeshwari Sundaram, Marie Thoma, and Germaine M.\n  Buck Louis", "title": "Cautionary note on \"Semiparametric modeling of grouped current duration\n  data with preferential reporting'\"", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is designed to clarify a few points about the article\n\"Semiparametric modeling of grouped current duration data with preferential\nreporting\" by McLain, Sundaram, Thoma and Louis in Statistics in Medicine\n(McLain et al., 2014, hereafter MSTL) regarding using the methods under right\ncensoring. In simulation studies, it has been found that bias can occur when\nright censoring is present. Current duration data normally does not have\ncensored values, but censoring can be induced at a value, say tau, after which\nthe data values are thought to be unreliable. As noted in MSTL, some right\ncensored data require an assumption on the parametric form of the data beyond\n{\\tau}. While this assumption was given in MSTL, the implications of the\nassumption were not sufficiently explored. Here we present simulations and\nevaluate the methods of MSTL under type I censoring, give some settings under\nwhich the method works well even in presence of censoring, state when the model\nis correctly specified and discuss the reasons of the bias.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 18:59:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["McLain", "Alexander C.", ""], ["Sundaram", "Rajeshwari", ""], ["Thoma", "Marie", ""], ["Louis", "Germaine M. Buck", ""]]}, {"id": "1801.00959", "submitter": "Mahdi Doostparast", "authors": "Mohammad Doostparast and Mahdi Doostparast", "title": "Prediction of corrosions in Gas and Oil pipelines based on the theory of\n  records", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions of corrosions in pipelines are valuable. Based on the available\ndata sets, it is critical and useful, for example in preventive maintenance.\nThis paper deals with this problem by two powerful statistical tools, i.e.\ntheory of records and non-homogeneous Poisson process, for modeling location of\ncorrosions. These methods may be used for comparing performances of different\npipelines via a quantitative approach. For illustration purposes, we applied\nthe obtained results for a real pipeline in order to prediction the next\ncorrosions based on the available data. Finally, some concluding results and\nfurther remarks are given.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 11:19:03 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Doostparast", "Mohammad", ""], ["Doostparast", "Mahdi", ""]]}, {"id": "1801.01242", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin, Adrian Wills, Brett Ninness", "title": "Sparse Bayesian ARX models with flexible noise distributions", "comments": "17 pages, 4 figures. Accepted for publication in the Proceedings of\n  the 18th IFAC Symposium on System Identification (SYSID). Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating linear dynamic system models\nwhen the observations are corrupted by random disturbances with nonstandard\ndistributions. The paper is particularly motivated by applications where sensor\nimperfections involve significant contribution of outliers or wrap-around\nissues resulting in multi-modal distributions such as commonly encountered in\nrobotics applications. As will be illustrated, these nonstandard measurement\nerrors can dramatically compromise the effectiveness of standard estimation\nmethods, while a computational Bayesian approach developed here is demonstrated\nto be equally effective as standard methods in standard measurement noise\nscenarios, but dramatically more effective in nonstandard measurement noise\ndistribution scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 04:30:22 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:08:53 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 11:23:00 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Dahlin", "Johan", ""], ["Wills", "Adrian", ""], ["Ninness", "Brett", ""]]}, {"id": "1801.01285", "submitter": "Carel F.W. Peeters", "authors": "Bernet S. Kato and Carel F.W. Peeters", "title": "Inequality Constrained Multilevel Models", "comments": "20 pages. Postprint of Chapter 13 in: H. Hoijtink, I. Klugkist, &\n  P.A. Boelen (Eds.). \"Bayesian Evaluation of Informative Hypotheses.\" New\n  York: Springer, 2008: pp. 273-295", "journal-ref": null, "doi": "10.1007/978-0-387-09612-4_13", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel or hierarchical data structures can occur in many areas of\nresearch, including economics, psychology, sociology, agriculture, medicine,\nand public health. Over the last 25 years, there has been increasing interest\nin developing suitable techniques for the statistical analysis of multilevel\ndata, and this has resulted in a broad class of models known under the generic\nname of multilevel models. Generally, multilevel models are useful for\nexploring how relationships vary across higher-level units taking into account\nthe within and between cluster variations. Research scientists often have\nsubstantive theories in mind when evaluating data with statistical models.\nSubstantive theories often involve inequality constraints among the parameters\nto translate a theory into a model. This chapter shows how the inequality\nconstrained multilevel linear model can be given a Bayesian formulation, how\nthe model parameters can be estimated using a so-called augmented Gibbs\nsampler, and how posterior probabilities can be computed to assist the\nresearcher in model selection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:19:51 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kato", "Bernet S.", ""], ["Peeters", "Carel F. W.", ""]]}, {"id": "1801.01303", "submitter": "Pavel Hrab\\'ak", "authors": "Marek Buk\\'a\\v{c}ek and Pavel Hrab\\'ak and Milan Krb\\'alek", "title": "Microscopic Travel Time Analysis of Bottleneck Experiments", "comments": "To appear in Transportmetrica A:\n  http://www.tandfonline.com/doi/abs/10.1080/23249935.2017.1419423", "journal-ref": null, "doi": "10.1080/23249935.2017.1419423", "report-no": null, "categories": "physics.soc-ph cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution provides a microscopic experimental study of pedestrian\nmotion in front of the bottleneck. Identification of individual pedestrians in\nconducted experiments enables to explain the high variance of travel time by\nheterogeneity of the crowd. Some pedestrians are able to push effectively\nthrough the crowd, some get trapped in the crowd for significantly longer time.\nThis ability to push through the crowd is associated with the slope of\nindividual linear model of the dependency of the travel time on the number of\npedestrians in front of the bottleneck. Further detailed study of the origin of\nsuch ability is carried out by means of the route choice, i.e. strategy whether\nto bypass the crowd or to walk directly through it. The study has revealed that\nthe ability to push through the crowd is a combination of aggressiveness in\nconflicts and willingness to overtake the crowd.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 10:54:39 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Buk\u00e1\u010dek", "Marek", ""], ["Hrab\u00e1k", "Pavel", ""], ["Krb\u00e1lek", "Milan", ""]]}, {"id": "1801.01326", "submitter": "Yan Wang", "authors": "Yan Wang, Lewi Stone", "title": "Understanding the connections between species distribution models", "comments": "25 pages, 4 figures, 5 tables, and 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for accurately predicting species distributions have become essential\ntools for many ecological and conservation problems. For many species,\npresence-background (presence-only) data is the most commonly available type of\nspatial data. A number of important methods have been proposed to model\npresence-background (PB) data, and there have been debates on the connection\nbetween these seemingly disparate methods. The paper begins by studying the\nclose relationship between the LI (Lancaster & Imbens, 1996) and LK (Lele &\nKeim, 2006) models, which were among the first developed methods for analysing\nPB data. The second part of the paper identifies close connections between the\nLK and point process models, as well as the equivalence between the Scaled\nBinomial (SB), Expectation-Maximization (EM), partial likelihood based Lele\n(2009) and LI methods, many of which have not been noted in the literature. We\nclarify that all these methods are the same in their ability to estimate the\nrelative probability (or intensity) of presence from PB data; and the absolute\nprobability of presence, when extra information of the species' prevalence is\nknown. A new unified constrained LK (CLK) method is also proposed as a\ngeneralisation of the better known existing approaches, with less theory\ninvolved and greater ease of implementation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 12:37:25 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wang", "Yan", ""], ["Stone", "Lewi", ""]]}, {"id": "1801.01441", "submitter": "Pieter Segaert", "authors": "K. Segaert, S.J.E. Lucas, C.V. Burley, Pieter Segaert, A. E. Milner,\n  M. Ryan, L. Wheeldon", "title": "Higher physical fitness levels are associated with less language decline\n  in healthy ageing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthy ageing is associated with decline in cognitive abilities such as\nlanguage. Aerobic fitness has been shown to ameliorate decline in some\ncognitive domains, but the potential benefits for language have not been\nexamined. In a cross-sectional sample, we investigated the relationship between\naerobic fitness and tip-of-the-tongue states. These are among the most frequent\ncognitive failures in healthy older adults and occur when a speaker knows a\nword but is unable to produce it. We found that healthy older adults indeed\nexperience more tip-of-the-tongue states than young adults. Importantly, higher\naerobic fitness levels decrease the probability of experiencing\ntip-of-the-tongue states in healthy older adults. Fitness-related differences\nin word finding abilities are observed over and above effects of age. This is\nthe first demonstration of a link between aerobic fitness and language\nfunctioning in healthy older adults.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:56:59 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 13:33:01 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Segaert", "K.", ""], ["Lucas", "S. J. E.", ""], ["Burley", "C. V.", ""], ["Segaert", "Pieter", ""], ["Milner", "A. E.", ""], ["Ryan", "M.", ""], ["Wheeldon", "L.", ""]]}, {"id": "1801.01467", "submitter": "Hussain Kazmi", "authors": "Hussain Kazmi, Fahad Mehmood, Stefan Lodeweyckx, Johan Driesen", "title": "Deep Reinforcement Learning based Optimal Control of Hot Water Systems", "comments": null, "journal-ref": "Energy, Volume 144, 2018", "doi": "10.1016/j.energy.2017.12.019", "report-no": null, "categories": "cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption for hot water production is a major draw in high\nefficiency buildings. Optimizing this has typically been approached from a\nthermodynamics perspective, decoupled from occupant influence. Furthermore,\noptimization usually presupposes existence of a detailed dynamics model for the\nhot water system. These assumptions lead to suboptimal energy efficiency in the\nreal world. In this paper, we present a novel reinforcement learning based\nmethodology which optimizes hot water production. The proposed methodology is\ncompletely generalizable, and does not require an offline step or human domain\nknowledge to build a model for the hot water vessel or the heating element.\nOccupant preferences too are learnt on the fly. The proposed system is applied\nto a set of 32 houses in the Netherlands where it reduces energy consumption\nfor hot water production by roughly 20% with no loss of occupant comfort.\nExtrapolating, this translates to absolute savings of roughly 200 kWh for a\nsingle household on an annual basis. This performance can be replicated to any\ndomestic hot water system and optimization objective, given that the fairly\nminimal requirements on sensor data are met. With millions of hot water systems\noperational worldwide, the proposed framework has the potential to reduce\nenergy consumption in existing and new systems on a multi Gigawatt-hour scale\nin the years to come.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 17:37:50 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kazmi", "Hussain", ""], ["Mehmood", "Fahad", ""], ["Lodeweyckx", "Stefan", ""], ["Driesen", "Johan", ""]]}, {"id": "1801.01529", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Tsuyoshi Hamada, Shuji Ogino and Molin Wang", "title": "A novel calibration framework for survival analysis when a binary\n  covariate is measured at sparse time points", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxy063", "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goals in clinical and cohort studies often include evaluation of the\nassociation of a time-dependent binary treatment or exposure with a survival\noutcome. Recently, several impactful studies targeted the association between\naspirin-taking and survival following colorectal cancer diagnosis. Due to\nsurgery, aspirin-taking value is zero at baseline and may change its value to\none at some time point. Estimating this association is complicated by having\nonly intermittent measurements on aspirin-taking. Naive, commonly-used, methods\ncan lead to substantial bias. We present a class of calibration models for the\ndistribution of the time of status change of the binary covariate. Estimates\nobtained from these models are then incorporated into the proportional hazard\npartial likelihood in a natural way. We develop nonparametric, semiparametric\nand parametric calibration models, and derive asymptotic theory for the methods\nthat we implement in the aspirin and colorectal cancer study. Our methodology\nallows to include additional baseline variables in the calibration models for\nthe status change time of the binary covariate. We further develop a risk-set\ncalibration approach that is more useful in settings in which the association\nbetween the binary covariate and survival is strong.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 19:55:32 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nevo", "Daniel", ""], ["Hamada", "Tsuyoshi", ""], ["Ogino", "Shuji", ""], ["Wang", "Molin", ""]]}, {"id": "1801.01535", "submitter": "Zhongyang Zhao", "authors": "Zhongyang Zhao, Chang Fu, Caisheng Wang, Carol Miller", "title": "Improvement to the Prediction of Fuel Cost Distributions Using ARIMA\n  Model", "comments": "Accepted by IEEE PES 2018 General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of a validated, realistic fuel cost model is a prerequisite to\nthe development and validation of new optimization methods and control tools.\nThis paper uses an autoregressive integrated moving average (ARIMA) model with\nhistorical fuel cost data in development of a three-step-ahead fuel cost\ndistribution prediction. First, the data features of Form EIA-923 are explored\nand the natural gas fuel costs of Texas generating facilities are used to\ndevelop and validate the forecasting algorithm for the Texas example.\nFurthermore, the spot price associated with the natural gas hub in Texas is\nutilized to enhance the fuel cost prediction. The forecasted data is fit to a\nnormal distribution and the Kullback-Leibler divergence is employed to evaluate\nthe difference between the real fuel cost distributions and the estimated\ndistributions. The comparative evaluation suggests the proposed forecasting\nalgorithm is effective in general and is worth pursuing further.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 20:12:23 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:22:48 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Zhao", "Zhongyang", ""], ["Fu", "Chang", ""], ["Wang", "Caisheng", ""], ["Miller", "Carol", ""]]}, {"id": "1801.01538", "submitter": "Samuel Jackson PhD", "authors": "Samuel E. Jackson, Ian Vernon, Junli Liu, Keith Lindsey", "title": "Understanding Hormonal Crosstalk in Arabidopsis Root Development via\n  Emulation and History Matching", "comments": "54 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in plant developmental biology is to understand how plant\ngrowth is coordinated by interacting hormones and genes. To meet this\nchallenge, it is important to not only use experimental data, but also\nformulate a mathematical model. For the mathematical model to best describe the\ntrue biological system, it is necessary to understand the parameter space of\nthe model, along with the links between the model, the parameter space and\nexperimental observations. We develop sequential history matching methodology,\nusing Bayesian emulation, to gain substantial insight into biological model\nparameter spaces. This is achieved by finding sets of acceptable parameters in\naccordance with successive sets of physical observations. These methods are\nthen applied to a complex hormonal crosstalk model for Arabidopsis root growth.\nIn this application, we demonstrate how an initial set of 22 observed trends\nreduce the volume of the set of acceptable inputs to a proportion of 6.1 x\n10^(-7) of the original space. Additional sets of biologically relevant\nexperimental data, each of size 5, reduce the size of this space by a further\nthree and two orders of magnitude respectively. Hence, we provide insight into\nthe constraints placed upon the model structure by, and the biological\nconsequences of, measuring subsets of observations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 20:31:48 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 20:15:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jackson", "Samuel E.", ""], ["Vernon", "Ian", ""], ["Liu", "Junli", ""], ["Lindsey", "Keith", ""]]}, {"id": "1801.01660", "submitter": "Thomas Muehlenstaedt", "authors": "Thomas Muehlenstaedt", "title": "Control charts in a multi product environment: An application study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article an SPC case study is presented. It consists of monitoring a\nmanufacturing process used for different products of similar kind. So far, each\nof these products is monitored individually. However, if there is e.g. a\nquality problem with one joint component, this signal will be distributed over\ndifferent control charts. Therefor a standardization is introduced, similar to\nshort run SPC methods. Then control chart methods are applied to detect process\ndeviations. If there are observations out of control, a next step is taken and\nprocess information are analysed in order to derive possible root causes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 08:04:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Muehlenstaedt", "Thomas", ""]]}, {"id": "1801.01669", "submitter": "Xin Shi", "authors": "Xin Shi, Robert Qiu, Xing He, Zenan Ling, Haosen Yang, Lei Chu", "title": "Early Anomaly Detection and Location in Distribution Network: A\n  Data-Driven Approach", "comments": "10 pages, submitted to IET Generation, Transmission and Distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement data collected from the supervisory control and data\nacquisition (SCADA) system installed in distribution network can reflect the\noperational state of the network effectively. In this paper, a random matrix\ntheory (RMT) based approach is developed for early anomaly detection and\nlocalization by using the data. For every feeder in the distribution network, a\ncorresponding data matrix is formed. Based on the Marchenko-Pastur Law for the\nempirical spectral analysis of covariance `signal+noise' matrix, the linear\neigenvalue statistics are introduced to indicate the anomaly, and the outliers\nand their corresponding eigenvectors are analyzed for locating the anomaly. As\nfor the low observability feeders in the distribution network, an increasing\ndata dimension algorithm is designed for the formulated low-dimensional\nmatrices being more accurately analyzed. The developed approach can detect and\nlocalize the anomaly at an early stage, and it is robust to random disturbance\nand measurement error. Cases on Matpower simulation data and real SCADA data\ncorroborate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 08:32:17 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 08:41:38 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 03:45:44 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 16:24:30 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Shi", "Xin", ""], ["Qiu", "Robert", ""], ["He", "Xing", ""], ["Ling", "Zenan", ""], ["Yang", "Haosen", ""], ["Chu", "Lei", ""]]}, {"id": "1801.01697", "submitter": "Manuel Rizzo", "authors": "Domenico Cucina, Manuel Rizzo and Eugen Ursu", "title": "Multiple changepoint detection for periodic autoregressive models with\n  an application to river flow analysis", "comments": "22 pages, 12 figures", "journal-ref": "Stochastic Environmental Research and Risk Assessment (2019)", "doi": "10.1007/s00477-019-01692-0", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In river flow analysis and forecasting there are some key elements to\nconsider in order to obtain reliable results. For example, seasonality is often\naccounted for in statistical models because climatic oscillations occurring\nevery year have an obvious impact on river flow. Further sources of alteration\ncould be caused by changes in reservoir management, instrumentation or even\nunexpected shifts in climatic conditions. When these changes are ignored the\nstatistical results can be strongly misleading. This paper develops an\nautomatic procedure to estimate number and locations of changepoints in\nPeriodic AutoRegressive models. These latter have been extensively used for\nmodelling seasonality in hydrology, climatology, economics and electrical\nengineering, but there are very few papers devoted also to changepoints\ndetection, moreover being limited to changes in mean or variance. In our\nproposal we allow the model structure as a whole to change, and estimation is\nperformed by optimizing an objective function derived from the Information\nCriterion using Genetic Algorithms. The proposed methodology is brought out\nthrough the example of three river flows, for which we built models with\npossible changepoints and evaluated their forecasting accuracy by means of Root\nMean Square Error, Mean Absolute Error and Mean Absolute Percentage Error. The\nlast years of data sets have been omitted from the selection and estimation\nprocedure and were then used to forecast. Comparisons with literature on river\nflow forecasting confirms the efficiency of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:39:30 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Cucina", "Domenico", ""], ["Rizzo", "Manuel", ""], ["Ursu", "Eugen", ""]]}, {"id": "1801.01792", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Mat\\'u\\v{s} Maciak, Ostap Okhrin, and Michal Pe\\v{s}ta", "title": "Dynamic and granular loss reserving with copulae", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intensive research sprang up for stochastic methods in insurance during\nthe past years. To meet all future claims rising from policies, it is requisite\nto quantify the outstanding loss liabilities. Loss reserving methods based on\naggregated data from run-off triangles are predominantly used to calculate the\nclaims reserves. Conventional reserving techniques have some disadvantages:\nloss of information from the policy and the claim's development due to the\naggregation, zero or negative cells in the triangle; usually small number of\nobservations in the triangle; only few observations for recent accident years;\nand sensitivity to the most recent paid claims.\n  To overcome these dilemmas, granular loss reserving methods for individual\nclaim-by-claim data will be derived. Reserves' estimation is a crucial part of\nthe risk valuation process, which is now a front burner in economics. Since\nthere is a growing demand for prediction of total reserves for different types\nof claims or even multiple lines of business, a time-varying copula framework\nfor granular reserving will be established.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 15:25:42 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Maciak", "Mat\u00fa\u0161", ""], ["Okhrin", "Ostap", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1801.01797", "submitter": "Johan Segers", "authors": "Fran\\c{c}ois Portier and Johan Segers", "title": "Monte Carlo integration with a growing number of control variates", "comments": "22 pages. Numerical experiments in earlier version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Monte Carlo integration with variance reduction by\nmeans of control variates can be implemented by the ordinary least squares\nestimator for the intercept in a multiple linear regression model. A central\nlimit theorem is established for the integration error if the number of control\nvariates tends to infinity. The integration error is scaled by the standard\ndeviation of the error term in the regression model. If the linear span of the\ncontrol variates is dense in a function space that contains the integrand, the\nintegration error tends to zero at a rate which is faster than the square root\nof the number of Monte Carlo replicates. Depending on the situation, increasing\nthe number of control variates may or may not be computationally more efficient\nthan increasing the Monte Carlo sample size.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 15:47:21 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 20:15:27 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 13:55:19 GMT"}, {"version": "v4", "created": "Wed, 9 Oct 2019 08:41:47 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Portier", "Fran\u00e7ois", ""], ["Segers", "Johan", ""]]}, {"id": "1801.01829", "submitter": "Uwe Saint-Mont", "authors": "Uwe Saint-Mont", "title": "On the Logic (plus some history and philosophy) of Statistical Tests and\n  Scientific Investigation", "comments": "22 pages, 1 table, just a few essential formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every scientific endeavour consists of (at least) two components: A\nhypothesis on the one hand and data on the other. There is always a more or\nless abstract level - some theory, a set of concepts, certain relations of\nideas - and a concrete level, i.e., empirical evidence, experiments or some\nobservations which constitute matters of fact.\n  The focus of this contribution is on elementary models connecting both levels\nthat have been very popular in psychological research - statistical tests.\nGoing from simple to complex we will examine four paradigms of statistical\ntesting (Fisher, Likelihood, Bayes, Neyman & Pearson) and an elegant\ncontemporary treatment.\n  In a nutshell, testing is an easy problem that has a straightforward\nmathematical solution. However, it is rather surprising that the statistical\nmainstream has pursued a different line of argument. The application of the\nlatter theory in psychology and other fields has brought some progress but has\nalso impaired scientific thinking.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 16:47:44 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Saint-Mont", "Uwe", ""]]}, {"id": "1801.01869", "submitter": "Matjaz Perc", "authors": "Haroldo V. Ribeiro, Luiz G. A. Alves, Alvaro F. Martins, Ervin K.\n  Lenzi, Matjaz Perc", "title": "The dynamical structure of political corruption networks", "comments": "20 pages, 5 figures; accepted for publication in Journal of Complex\n  Networks", "journal-ref": "J. Complex Netw. 6, 989-1003 (2018)", "doi": "10.1093/comnet/cny002", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corruptive behaviour in politics limits economic growth, embezzles public\nfunds, and promotes socio-economic inequality in modern democracies. We analyse\nwell-documented political corruption scandals in Brazil over the past 27 years,\nfocusing on the dynamical structure of networks where two individuals are\nconnected if they were involved in the same scandal. Our research reveals that\ncorruption runs in small groups that rarely comprise more than eight people, in\nnetworks that have hubs and a modular structure that encompasses more than one\ncorruption scandal. We observe abrupt changes in the size of the largest\nconnected component and in the degree distribution, which are due to the\ncoalescence of different modules when new scandals come to light or when\ngovernments change. We show further that the dynamical structure of political\ncorruption networks can be used for successfully predicting partners in future\nscandals. We discuss the important role of network science in detecting and\nmitigating political corruption.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 18:47:27 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ribeiro", "Haroldo V.", ""], ["Alves", "Luiz G. A.", ""], ["Martins", "Alvaro F.", ""], ["Lenzi", "Ervin K.", ""], ["Perc", "Matjaz", ""]]}, {"id": "1801.02078", "submitter": "Andrew Finley Dr.", "authors": "Daniel Taylor-Rodriguez, Andrew O. Finley, Abhirup Datta, Chad\n  Babcock, Hans-Erik Andersen, Bruce D. Cook, Douglas C. Morton, Sudipto\n  Banerjee", "title": "Spatial Factor Models for High-Dimensional and Large Spatial Data: An\n  Application in Forest Variable Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gathering information about forest variables is an expensive and arduous\nactivity. As such, directly collecting the data required to produce\nhigh-resolution maps over large spatial domains is infeasible. Next generation\ncollection initiatives of remotely sensed Light Detection and Ranging (LiDAR)\ndata are specifically aimed at producing complete-coverage maps over large\nspatial domains. Given that LiDAR data and forest characteristics are often\nstrongly correlated, it is possible to make use of the former to model,\npredict, and map forest variables over regions of interest. This entails\ndealing with the high-dimensional ($\\sim$$10^2$) spatially dependent LiDAR\noutcomes over a large number of locations (~10^5-10^6). With this in mind, we\ndevelop the Spatial Factor Nearest Neighbor Gaussian Process (SF-NNGP) model,\nand embed it in a two-stage approach that connects the spatial structure found\nin LiDAR signals with forest variables. We provide a simulation experiment that\ndemonstrates inferential and predictive performance of the SF-NNGP, and use the\ntwo-stage modeling strategy to generate complete-coverage maps of forest\nvariables with associated uncertainty over a large region of boreal forests in\ninterior Alaska.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 19:56:23 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 18:57:19 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Taylor-Rodriguez", "Daniel", ""], ["Finley", "Andrew O.", ""], ["Datta", "Abhirup", ""], ["Babcock", "Chad", ""], ["Andersen", "Hans-Erik", ""], ["Cook", "Bruce D.", ""], ["Morton", "Douglas C.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1801.02205", "submitter": "Danilo Delpini", "authors": "Danilo Delpini, Stefano Battiston, Guido Caldarelli, Massimo Riccaboni", "title": "The Network of U.S. Mutual Fund Investments: Diversification, Similarity\n  and Fragility throughout the Global Financial Crisis", "comments": "27 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network theory proved recently to be useful in the quantification of many\nproperties of financial systems. The analysis of the structure of investment\nportfolios is a major application since their eventual correlation and overlap\nimpact the actual risk diversification by individual investors. We investigate\nthe bipartite network of US mutual fund portfolios and their assets. We follow\nits evolution during the Global Financial Crisis and analyse the interplay\nbetween diversification, as understood in classical portfolio theory, and\nsimilarity of the investments of different funds. We show that, on average,\nportfolios have become more diversified and less similar during the crisis.\nHowever, we also find that large overlap is far more likely than expected from\nmodels of random allocation of investments. This indicates the existence of\nstrong correlations between fund portfolio strategies. We introduce a\nsimplified model of propagation of financial shocks, that we exploit to show\nthat a systemic risk component origins from the similarity of portfolios. The\nnetwork is still vulnerable after crisis because of this effect, despite the\nincrease in the diversification of portfolios. Our results indicate that\ndiversification may even increase systemic risk when funds diversify in the\nsame way. Diversification and similarity can play antagonistic roles and the\ntrade-off between the two should be taken into account to properly assess\nsystemic risk.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 16:09:27 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Delpini", "Danilo", ""], ["Battiston", "Stefano", ""], ["Caldarelli", "Guido", ""], ["Riccaboni", "Massimo", ""]]}, {"id": "1801.02388", "submitter": "Wouter Van den Broek", "authors": "Wouter Van den Broek, Bryan W. Reed, Armand B\\'ech\\'e, Abner Velazco,\n  Johan Verbeeck, Christoph T. Koch", "title": "Various Compressed Sensing Set-Ups Evaluated Against Shannon Sampling\n  Under Constraint of Constant Illumination", "comments": "18 pages, 13 figures. New Monte Carlo simulations in Figure 13\n  showing the behavior of the single-pixel camera under various magnitudes of\n  read-out noise", "journal-ref": "IEEE Transactions on Computational Imaging, 2019", "doi": "10.1109/TCI.2019.2894950", "report-no": null, "categories": "physics.ins-det physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the constraint of constant illumination, an information criterion is\nformulated for the Fisher information that compressed sensing measurements in\noptical and transmission electron microscopy contain about the underlying\nparameters. Since this approach requires prior knowledge of the signal's\nsupport in the sparse basis, we develop a heuristic quantity, the detective\nquantum efficiency (DQE), that tracks this information criterion well without\nthis knowledge. It is shown that for the investigated choice of sensing\nmatrices, and in the absence of read-out noise, i.e. with only Poisson noise\npresent, compressed sensing does not raise the amount of Fisher information in\nthe recordings above that of Shannon sampling. Furthermore, enabled by the\nDQE's analytical tractability, the experimental designs are optimized by\nfinding out the optimal fraction of on-pixels as a function of dose and\nread-out noise. Finally, we introduce a regularization and demonstrate, through\nsimulations and experiment, that it yields reconstructions attaining minimum\nmean squared error at experimental settings predicted by the DQE as optimal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 11:32:31 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 09:04:48 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 15:20:42 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Broek", "Wouter Van den", ""], ["Reed", "Bryan W.", ""], ["B\u00e9ch\u00e9", "Armand", ""], ["Velazco", "Abner", ""], ["Verbeeck", "Johan", ""], ["Koch", "Christoph T.", ""]]}, {"id": "1801.02858", "submitter": "Seth Flaxman", "authors": "Seth Flaxman and Michael Chirico and Pau Pereira and Charles Loeffler", "title": "Scalable high-resolution forecasting of sparse spatiotemporal events\n  with kernel methods: a winning solution to the NIJ \"Real-Time Crime\n  Forecasting Challenge\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic spatiotemporal event forecasting method, which we\ndeveloped for the National Institute of Justice's (NIJ) Real-Time Crime\nForecasting Challenge. Our method is a spatiotemporal forecasting model\ncombining scalable randomized Reproducing Kernel Hilbert Space (RKHS) methods\nfor approximating Gaussian processes with autoregressive smoothing kernels in a\nregularized supervised learning framework. While the smoothing kernels capture\nthe two main approaches in current use in the field of crime forecasting,\nkernel density estimation (KDE) and self-exciting point process (SEPP) models,\nthe RKHS component of the model can be understood as an approximation to the\npopular log-Gaussian Cox Process model. For inference, we discretize the\nspatiotemporal point pattern and learn a log-intensity function using the\nPoisson likelihood and highly efficient gradient-based optimization methods.\nModel hyperparameters including quality of RKHS approximation, spatial and\ntemporal kernel lengthscales, number of autoregressive lags, bandwidths for\nsmoothing kernels, as well as cell shape, size, and rotation, were learned\nusing crossvalidation. Resulting predictions significantly exceeded baseline\nKDE estimates and SEPP models for sparse events.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 10:04:17 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 13:00:24 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 11:38:24 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2019 08:10:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Flaxman", "Seth", ""], ["Chirico", "Michael", ""], ["Pereira", "Pau", ""], ["Loeffler", "Charles", ""]]}, {"id": "1801.03050", "submitter": "V\\'ictor Gallego", "authors": "V\\'ictor Gallego, Pablo Su\\'arez-Garc\\'ia, Pablo Angulo, David\n  G\\'omez-Ullate", "title": "Assessing the effect of advertising expenditures upon sales: a Bayesian\n  structural time series model", "comments": "Published at Applied Stochastic Models in Business and Industry,\n  https://onlinelibrary.wiley.com/doi/full/10.1002/asmb.2460", "journal-ref": "Appl Stochastic Models Bus Ind. 2019; 1-13", "doi": "10.1002/asmb.2460", "report-no": null, "categories": "stat.ML econ.EM q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust implementation of the Nerlove--Arrow model using a\nBayesian structural time series model to explain the relationship between\nadvertising expenditures of a country-wide fast-food franchise network with its\nweekly sales. Thanks to the flexibility and modularity of the model, it is well\nsuited to generalization to other markets or situations. Its Bayesian nature\nfacilitates incorporating \\emph{a priori} information (the manager's views),\nwhich can be updated with relevant data. This aspect of the model will be used\nto present a strategy of budget scheduling across time and channels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:39:51 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 08:15:51 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 08:36:35 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gallego", "V\u00edctor", ""], ["Su\u00e1rez-Garc\u00eda", "Pablo", ""], ["Angulo", "Pablo", ""], ["G\u00f3mez-Ullate", "David", ""]]}, {"id": "1801.03093", "submitter": "Saeed Zamanzad Gavidel PhD Candidate", "authors": "Saeed Z.Gavidel, Jeremy L. Rickli", "title": "End-of-Use Core Triage in Extreme Scenarios Based on a Threshold\n  Approach", "comments": "6 pages, 4 Figures, This is a Conference paper submitted to\n  Industrial and Systems Engineering Research Conference (ISERC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remanufacturing is a significant factor in securing sustainability through a\ncircular economy. Sorting plays a significant role in remanufacturing\npre-processing inspections. Its significance can increase when remanufacturing\nfacilities encounter extreme situations, such as abnormally huge core arrivals.\nOur main objective in this work is switching from less efficient to a more\nefficient model and to characterize extreme behavior of core arrival in\nremanufacturing and applying the developed model to triage cores. Central\ntendency core flow models are not sufficient to handle extreme situations,\nhowever, complementary Extreme Value (EV) approaches have shown to improve\nmodel efficiency. Extreme core flows to remanufacturing facilities are rare but\nstill likely and can adversely affect remanufacturing business operations. In\nthis investigation, extreme end-of-use core flow is modelled by a threshold\napproach using the Generalized Pareto Distribution (GPD). It is shown that GPD\nhas better performance than its maxima-block GEV counterpart from practical and\ndata efficiency perspectives. The model is validated by a synthesized big\ndataset, tested by sophisticated statistical Anderson Darling (AD) test, and is\napplied to a case of extreme flow to a valve shop in order to predict\nprobability of over-capacity arrivals that is critical in remanufacturing\nbusiness management. Finally, the GPD model combined with triage strategies is\nused to initiate investigations into the efficacy of different triage methods\nin remanufacturing operations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:44:25 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Gavidel", "Saeed Z.", ""], ["Rickli", "Jeremy L.", ""]]}, {"id": "1801.03147", "submitter": "Yaoyuan Vincent Tan", "authors": "Yaoyuan V. Tan and Carol A.C. Flannagan and Michael R. Elliott", "title": "\"Robust-squared\" Imputation Models Using BART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples of \"doubly robust\" estimator for missing data include augmented\ninverse probability weighting (AIPWT) models (Robins et al., 1994) and\npenalized splines of propensity prediction (PSPP) models (Zhang and Little,\n2009). Doubly-robust estimators have the property that, if either the response\npropensity or the mean is modeled correctly, a consistent estimator of the\npopulation mean is obtained. However, doubly-robust estimators can perform\npoorly when modest misspecification is present in both models (Kang and\nSchafer, 2007). Here we consider extensions of the AIPWT and PSPP models that\nuse Bayesian Additive Regression Trees (BART; Chipman et al., 2010) to provide\nhighly robust propensity and mean model estimation. We term these\n\"robust-squared\" in the sense that the propensity score, the means, or both can\nbe estimated with minimal model misspecification, and applied to the\ndoubly-robust estimator. We consider their behavior via simulations where\npropensities and/or mean models are misspecified. We apply our proposed method\nto impute missing instantaneous velocity (delta-v) values from the 2014\nNational Automotive Sampling System Crashworthiness Data System dataset and\nmissing Blood Alcohol Concentration values from the 2015 Fatality Analysis\nReporting System dataset. We found that BART applied to PSPP and AIPWT,\nprovides a more robust and efficient estimate compared to PSPP and AIPWT, with\nthe BART-estimated propensity score combined with PSPP providing the most\nefficient estimator with close to nominal coverage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:35:20 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tan", "Yaoyuan V.", ""], ["Flannagan", "Carol A. C.", ""], ["Elliott", "Michael R.", ""]]}, {"id": "1801.03400", "submitter": "Anna Broido", "authors": "Anna D. Broido and Aaron Clauset", "title": "Scale-free networks are rare", "comments": "14 pages, 9 figures, 2 tables, 5 appendices", "journal-ref": "Nature Communications 10, 1017 (2019)", "doi": "10.1038/s41467-019-08746-5", "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central claim in modern network science is that real-world networks are\ntypically \"scale free,\" meaning that the fraction of nodes with degree $k$\nfollows a power law, decaying like $k^{-\\alpha}$, often with $2 < \\alpha < 3$.\nHowever, empirical evidence for this belief derives from a relatively small\nnumber of real-world networks. We test the universality of scale-free structure\nby applying state-of-the-art statistical tools to a large corpus of nearly 1000\nnetwork data sets drawn from social, biological, technological, and\ninformational sources. We fit the power-law model to each degree distribution,\ntest its statistical plausibility, and compare it via a likelihood ratio test\nto alternative, non-scale-free models, e.g., the log-normal. Across domains, we\nfind that scale-free networks are rare, with only 4% exhibiting the\nstrongest-possible evidence of scale-free structure and 52% exhibiting the\nweakest-possible evidence. Furthermore, evidence of scale-free structure is not\nuniformly distributed across sources: social networks are at best weakly scale\nfree, while a handful of technological and biological networks can be called\nstrongly scale free. These results undermine the universality of scale-free\nnetworks and reveal that real-world networks exhibit a rich structural\ndiversity that will likely require new ideas and mechanisms to explain.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 04:20:08 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Broido", "Anna D.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1801.03783", "submitter": "Gregory Herschlag", "authors": "Gregory Herschlag and Han Sung Kang and Justin Luo and Christy Vaughn\n  Graves and Sachet Bangia and Robert Ravier and Jonathan C. Mattingly", "title": "Quantifying Gerrymandering in North Carolina", "comments": "This is a revised and expanded version of arxiv:1704.03360, entitled\n  \"Redistricting: Drawing the Line.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an ensemble of redistricting plans, we evaluate whether a given\npolitical districting faithfully represents the geo-political landscape.\nRedistricting plans are sampled by a Monte Carlo algorithm from a probability\ndistribution that adheres to realistic and non-partisan criteria. Using the\nsampled redistricting plans and historical voting data, we produce an ensemble\nof elections that reveal geo-political structure within the state. We showcase\nour methods on the two most recent districtings of NC for the U.S. House of\nRepresentatives, as well as a plan drawn by a bipartisan redistricting panel.\nWe find the two state enacted plans are highly atypical outliers whereas the\nbipartisan plan accurately represents the ensemble both in partisan outcome and\nin the fine scale structure of district-level results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 05:55:41 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Herschlag", "Gregory", ""], ["Kang", "Han Sung", ""], ["Luo", "Justin", ""], ["Graves", "Christy Vaughn", ""], ["Bangia", "Sachet", ""], ["Ravier", "Robert", ""], ["Mattingly", "Jonathan C.", ""]]}, {"id": "1801.04064", "submitter": "Rui She", "authors": "Rui She, Shanyun Liu, and Pingyi Fan", "title": "State Variation Mining: On Information Divergence with Message\n  Importance in Big Data", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information transfer which reveals the state variation of variables usually\nplays a vital role in big data analytics and processing. In fact, the measures\nfor information transfer could reflect the system change by use of the variable\ndistributions, similar to KL divergence and Renyi divergence. Furthermore, in\nterms of the information transfer in big data, small probability events usually\ndominate the importance of the total message to some degree. Therefore, it is\nsignificant to design an information transfer measure based on the message\nimportance which emphasizes the small probability events. In this paper, we\npropose a message importance transfer measure (MITM) and investigate its\ncharacteristics and applications on three aspects. First, the message\nimportance transfer capacity based on MITM is presented to offer an upper bound\nfor the information transfer process with disturbance. Then, we extend the MITM\nto the continuous case and discuss the robustness by using it to measuring\ninformation distance. Finally, we utilize the MITM to guide the queue length\nselection in the caching operation of mobile edge computing.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 05:57:29 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 09:23:15 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2018 04:47:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["She", "Rui", ""], ["Liu", "Shanyun", ""], ["Fan", "Pingyi", ""]]}, {"id": "1801.04110", "submitter": "Jin Wang", "authors": "Jin Wang, Ph.D., Javier Cabrera, Ph.D., Kwok-Leung Tsui, Ph.D., Hainan\n  Guo, Ph.D., Monique Bakker, Ph.D., John B. Kostis, M.D", "title": "Clinical and Non-clinical Effects on Surgery Duration: Statistical\n  Modeling and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgery duration is usually used as an input to the operation room (OR)\nallocation and surgery scheduling problems. A good estimation of surgery\nduration benefits the operation planning in ORs. In contrast, we would like to\ninvestigate whether the allocation decisions in turn influence surgery\nduration. Using almost two years of data from a large hospital in China, we\nfind evidence in support of our conjecture. Surgery duration decreases with the\nnumber of surgeries a surgeon performs in a day. Numerically, surgery duration\nwill decrease by 10 minutes on average if a surgeon performs one more surgery.\nFurthermore, we find a non-linear relationship between surgery duration and the\nnumber of surgeries allocated to an OR. Also, a surgery's duration is affected\nby its position in a sequence of surgeries performed by one surgeon. In\naddition, surgeons exhibit different patterns on the effects of surgery type\nand position. Since the findings are obtained from a particular data set, We do\nnot claim the generalizability. Instead, the analysis in this paper provides\ninsights into surgery duration study in ORs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:50:22 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Wang", "Jin", ""], ["D.", "Ph.", ""], ["Cabrera", "Javier", ""], ["D.", "Ph.", ""], ["Tsui", "Kwok-Leung", ""], ["D.", "Ph.", ""], ["Guo", "Hainan", ""], ["D.", "Ph.", ""], ["Bakker", "Monique", ""], ["D.", "Ph.", ""], ["Kostis", "John B.", ""], ["D", "M.", ""]]}, {"id": "1801.04159", "submitter": "Lucas Maystre", "authors": "Ali Batuhan Yard{\\i}m, Victor Kristof, Lucas Maystre, Matthias\n  Grossglauser", "title": "Can Who-Edits-What Predict Edit Survival?", "comments": "Accepted at KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219979", "report-no": null, "categories": "stat.AP cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of contributors to online peer-production systems grows, it\nbecomes increasingly important to predict whether the edits that users make\nwill eventually be beneficial to the project. Existing solutions either rely on\na user reputation system or consist of a highly specialized predictor that is\ntailored to a specific peer-production system. In this work, we explore a\ndifferent point in the solution space that goes beyond user reputation but does\nnot involve any content-based feature of the edits. We view each edit as a game\nbetween the editor and the component of the project. We posit that the\nprobability that an edit is accepted is a function of the editor's skill, of\nthe difficulty of editing the component and of a user-component interaction\nterm. Our model is broadly applicable, as it only requires observing data about\nwho makes an edit, what the edit affects and whether the edit survives or not.\nWe apply our model on Wikipedia and the Linux kernel, two examples of\nlarge-scale peer-production systems, and we seek to understand whether it can\neffectively predict edit survival: in both cases, we provide a positive answer.\nOur approach significantly outperforms those based solely on user reputation\nand bridges the gap with specialized predictors that use content-based\nfeatures. It is simple to implement, computationally inexpensive, and in\naddition it enables us to discover interesting structure in the data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 13:26:57 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 07:37:47 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Yard\u0131m", "Ali Batuhan", ""], ["Kristof", "Victor", ""], ["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1801.04212", "submitter": "Mor Absa Loum", "authors": "Mor Absa Loum (LM-Orsay), Marie-Anne Poursat (LM-Orsay), Abdourahmane\n  Sow, Amadou Sall, Cheikh Loucoubar (G4-IPD), Elisabeth Gassiat (LM-Orsay)", "title": "Multinomial logistic model for coinfection diagnosis between arbovirus\n  and malaria in Kedougou", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tropical regions, populations continue to suffer morbidity and mortality\nfrom malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are\nall endemic due to the climate and its geographical position. The\nco-circulation of malaria parasites and arboviruses can explain the observation\nof coinfected cases. Indeed there is strong resemblance in symptoms between\nthese diseases making problematic targeted medical care of coinfected cases.\nThis is due to the fact that the origin of illness is not obviously known. Some\ncases could be immunized against one or the other of the pathogens, immunity\ntypically acquired with factors like age and exposure as usual for endemic\narea. Then, coinfection needs to be better diagnosed. Using data collected from\npatients in Kedougou region, from 2009 to 2013, we adjusted a multinomial\nlogistic model and selected relevant variables in explaining coinfection\nstatus. We observed specific sets of variables explaining each of the diseases\nexclusively and the coinfection. We tested the independence between arboviral\nand malaria infections and derived coinfection probabilities from the model\nfitting. In case of a coinfection probability greater than a threshold value to\nbe calibrated on the data, duration of illness above 3 days and age above 10\nyears-old are mostly indicative of arboviral disease while body temperature\nhigher than 40{\\textdegree}C and presence of nausea or vomiting symptoms during\nthe rainy season are mostly indicative of malaria disease.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:05:56 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:47:17 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Loum", "Mor Absa", "", "LM-Orsay"], ["Poursat", "Marie-Anne", "", "LM-Orsay"], ["Sow", "Abdourahmane", "", "G4-IPD"], ["Sall", "Amadou", "", "G4-IPD"], ["Loucoubar", "Cheikh", "", "G4-IPD"], ["Gassiat", "Elisabeth", "", "LM-Orsay"]]}, {"id": "1801.04587", "submitter": "Sarah Tan", "authors": "Sarah Tan, Susanna Makela, Daliah Heller, Kevin Konty, Sharon Balter,\n  Tian Zheng, James H. Stark", "title": "A Bayesian Evidence Synthesis Approach to Estimate Disease Prevalence in\n  Hard-To-Reach Populations: Hepatitis C in New York City", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods to estimate the prevalence of chronic hepatitis C (HCV) in\nNew York City (NYC) are limited in scope and fail to assess hard-to-reach\nsubpopulations with highest risk such as injecting drug users (IDUs). To\naddress these limitations, we employ a Bayesian multi-parameter evidence\nsynthesis model to systematically combine multiple sources of data, account for\nbias in certain data sources, and provide unbiased HCV prevalence estimates\nwith associated uncertainty. Our approach improves on previous estimates by\nexplicitly accounting for injecting drug use and including data from high-risk\nsubpopulations such as the incarcerated, and is more inclusive, utilizing ten\nNYC data sources. In addition, we derive two new equations to allow age at\nfirst injecting drug use data for former and current IDUs to be incorporated\ninto the Bayesian evidence synthesis, a first for this type of model. Our\nestimated overall HCV prevalence as of 2012 among NYC adults aged 20-59 years\nis 2.78% (95% CI 2.61-2.94%), which represents between 124,900 and 140,000\nchronic HCV cases. These estimates suggest that HCV prevalence in NYC is higher\nthan previously indicated from household surveys (2.2%) and the surveillance\nsystem (2.37%), and that HCV transmission is increasing among young injecting\nadults in NYC. An ancillary benefit from our results is an estimate of current\nIDUs aged 20-59 in NYC: 0.58% or 27,600 individuals.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 17:31:33 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Tan", "Sarah", ""], ["Makela", "Susanna", ""], ["Heller", "Daliah", ""], ["Konty", "Kevin", ""], ["Balter", "Sharon", ""], ["Zheng", "Tian", ""], ["Stark", "James H.", ""]]}, {"id": "1801.04756", "submitter": "Tze Siong Lau", "authors": "Tze Siong Lau, Wee Peng Tay, Venugopal V. Veeravalli", "title": "A Binning Approach to Quickest Change Detection with Unknown Post-Change\n  Distribution", "comments": "Double-column 13-page version sent to IEEE. Transaction on Signal\n  Processing. Supplementary material included", "journal-ref": null, "doi": "10.1109/TSP.2018.2881666", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickest detection of a change in distribution is considered\nunder the assumption that the pre-change distribution is known, and the\npost-change distribution is only known to belong to a family of distributions\ndistinguishable from a discretized version of the pre-change distribution. A\nsequential change detection procedure is proposed that partitions the sample\nspace into a finite number of bins, and monitors the number of samples falling\ninto each of these bins to detect the change. A test statistic that\napproximates the generalized likelihood ratio test is developed. It is shown\nthat the proposed test statistic can be efficiently computed using a recursive\nupdate scheme, and a procedure for choosing the number of bins in the scheme is\nprovided. Various asymptotic properties of the test statistic are derived to\noffer insights into its performance trade-off between average detection delay\nand average run length to a false alarm. Testing on synthetic and real data\ndemonstrates that our approach is comparable or better in performance to\nexisting non-parametric change detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 11:53:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 03:18:49 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 14:59:17 GMT"}, {"version": "v4", "created": "Fri, 2 Nov 2018 12:32:55 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Lau", "Tze Siong", ""], ["Tay", "Wee Peng", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1801.05049", "submitter": "Blakeley McShane", "authors": "Jennifer L. Tackett and Blakeley B. McShane", "title": "Conceptualizing and Evaluating Replication Across Domains of Behavioral\n  Research", "comments": null, "journal-ref": "Behavioral and Brain Sciences 2018, Vol. 41, e120, 40-41", "doi": "10.1017/S0140525X17001972", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the authors' conceptualization of replication, in particular the\nfalse dichotomy of direct versus conceptual replication intrinsic to it, and\nsuggest a broader one that better generalizes to other domains of psychological\nresearch. We also discuss their approach to the evaluation of replication\nresults and suggest moving beyond their dichotomous statistical paradigms and\nemploying hierarchical / meta-analytic statistical models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 22:13:12 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tackett", "Jennifer L.", ""], ["McShane", "Blakeley B.", ""]]}, {"id": "1801.05062", "submitter": "Xinyuan Zhang", "authors": "Xinyuan Zhang, Ricardo Henao, Zhe Gan, Yitong Li, Lawrence Carin", "title": "Multi-Label Learning from Medical Plain Text with Convolutional Residual\n  Models", "comments": "Machine Learning for Healthcare 2018 spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting diagnoses from Electronic Health Records (EHRs) is an important\nmedical application of multi-label learning. We propose a convolutional\nresidual model for multi-label classification from doctor notes in EHR data. A\ngiven patient may have multiple diagnoses, and therefore multi-label learning\nis required. We employ a Convolutional Neural Network (CNN) to encode plain\ntext into a fixed-length sentence embedding vector. Since diagnoses are\ntypically correlated, a deep residual network is employed on top of the CNN\nencoder, to capture label (diagnosis) dependencies and incorporate information\ndirectly from the encoded sentence vector. A real EHR dataset is considered,\nand we compare the proposed model with several well-known baselines, to predict\ndiagnoses based on doctor notes. Experimental results demonstrate the\nsuperiority of the proposed convolutional residual model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 22:59:17 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 19:36:06 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Zhang", "Xinyuan", ""], ["Henao", "Ricardo", ""], ["Gan", "Zhe", ""], ["Li", "Yitong", ""], ["Carin", "Lawrence", ""]]}, {"id": "1801.05244", "submitter": "Silvia Polettini", "authors": "Cinzia Carota, Maurizio Filippone, Silvia Polettini", "title": "Assessing Bayesian Nonparametric Log-Linear Models: an application to\n  Disclosure Risk estimation", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for identification of models with good predictive\nperformances in the family of Bayesian log-linear mixed models with Dirichlet\nprocess random effects. Such a problem arises in many different applications;\nhere we consider it in the context of disclosure risk estimation, an\nincreasingly relevant issue raised by the increasing demand for data collected\nunder a pledge of confidentiality. Two different criteria are proposed and\njointly used via a two-stage selection procedure, in a M-open view. The first\nstage is devoted to identifying a path of search; then, at the second, a small\nnumber of nonparametric models is evaluated through an application-specific\nscore based Bayesian information criterion. We test our method on a variety of\ncontingency tables based on microdata samples from the US Census Bureau and the\nItalian National Security Administration, treated here as populations, and\ncarefully discuss its features. This leads us to a journey around different\nforms and sources of bias along which we show that (i) while based on the so\ncalled \"score+search\" paradigm, our method is by construction well protected\nfrom the selection-induced bias, and (ii) models with good performances are\ninvariably characterized by an extraordinarily simple structure of fixed\neffects. The complexity of model selection - a very challenging and difficult\ntask in a strictly parametric context with large and sparse tables - is\ntherefore significantly defused by our approach. An attractive collateral\nresult of our analysis are fruitful new ideas about modeling in small area\nestimation problems, where interest is in total counts over cells with a small\nnumber of observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 13:20:31 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Carota", "Cinzia", ""], ["Filippone", "Maurizio", ""], ["Polettini", "Silvia", ""]]}, {"id": "1801.05327", "submitter": "Pedro Ramos", "authors": "Pedro Luiz Ramos, Francisco Louzada, Eduardo Ramos, Sanku Dey", "title": "The Frechet distribution: Estimation and Application an Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of estimating the parameters of the\nFr\\'echet distribution from both frequentist and Bayesian points of view. First\nwe briefly describe different frequentist approaches, namely, maximum\nlikelihood, method of moments, percentile estimators, L-moments, ordinary and\nweighted least squares, maximum product of spacings, maximum goodness-of-fit\nestimators and compare them with respect to mean relative estimates, mean\nsquared errors and the 95\\% coverage probability of the asymptotic confidence\nintervals using extensive numerical simulations. Next, we consider the Bayesian\ninference approach using reference priors. The Metropolis-Hasting algorithm is\nused to draw Markov Chain Monte Carlo samples, and they have in turn been used\nto compute the Bayes estimates and also to construct the corresponding credible\nintervals. Five real data sets related to the minimum flow of water on\nPiracicaba river in Brazil are used to illustrate the applicability of the\ndiscussed procedures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 15:52:30 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ramos", "Pedro Luiz", ""], ["Louzada", "Francisco", ""], ["Ramos", "Eduardo", ""], ["Dey", "Sanku", ""]]}, {"id": "1801.05465", "submitter": "Roberto Vila Gabriel", "authors": "Roberto Vila, Jeremias Le\\~ao, Helton Saulo, Mirza Nabeed and Manoel\n  Santos-Neto", "title": "On a bimodal Birnbaum-Saunders distribution with applications to\n  lifetime data", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": "10.1214/19-BJPS448", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Birnbaum-Saunders distribution is a flexible and useful model which has\nbeen used in several fields. In this paper, a new bimodal version of this\ndistribution based on the alpha-skew-normal distribution is established. We\ndiscuss some of its mathematical and inferential properties. We consider\nlikelihood-based methods to estimate the model parameters. We carry out a Monte\nCarlo simulation study to evaluate the performance of the maximum likelihood\nestimators. For illustrative purposes, three real data sets are analyzed. The\nresults indicated that the proposed model outperformed some existing models in\nthe literature, in special, a recent bimodal extension of the Birnbaum-Saunders\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 19:57:59 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 19:55:10 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Vila", "Roberto", ""], ["Le\u00e3o", "Jeremias", ""], ["Saulo", "Helton", ""], ["Nabeed", "Mirza", ""], ["Santos-Neto", "Manoel", ""]]}, {"id": "1801.05725", "submitter": "Donald Williams Mr.", "authors": "Donald R. Williams, Juho Piironen, Aki Vehtari and Philippe Rast", "title": "Bayesian Estimation of Gaussian Graphical Models with Predictive\n  Covariance Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian graphical models are used for determining conditional relationships\nbetween variables. This is accomplished by identifying off-diagonal elements in\nthe inverse-covariance matrix that are non-zero. When the ratio of variables\n(p) to observations (n) approaches one, the maximum likelihood estimator of the\ncovariance matrix becomes unstable and requires shrinkage estimation. Whereas\nseveral classical (frequentist) methods have been introduced to address this\nissue, fully Bayesian methods remain relatively uncommon in practice and\nmethodological literatures. Here we introduce a Bayesian method for estimating\nsparse matrices, in which conditional relationships are determined with\nprojection predictive selection. With this method, that uses Kullback-Leibler\ndivergence and cross-validation for neighborhood selection, we reconstruct the\ninverse-covariance matrix in both low and high-dimensional settings. Through\nsimulation and applied examples, we characterized performance compared to\nseveral Bayesian methods and the graphical lasso, in addition to TIGER that\nsimilarly estimates the inverse-covariance matrix with regression. Our results\ndemonstrate that projection predictive selection not only has superior\nperformance compared to selecting the most probable model and Bayesian model\naveraging, particularly for high-dimensional data, but also compared to the the\nBayesian and classical glasso methods. Further, we show that estimating the\ninverse-covariance matrix with multiple regression is often more accurate, with\nrespect to various loss functions, and efficient than direct estimation. In\nlow-dimensional settings, we demonstrate that projection predictive selection\nalso provides competitive performance. We have implemented the projection\npredictive method for covariance selection in the R package GGMprojpred\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:06:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 14:57:17 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 02:40:23 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 15:40:31 GMT"}, {"version": "v5", "created": "Sat, 4 Aug 2018 20:00:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Williams", "Donald R.", ""], ["Piironen", "Juho", ""], ["Vehtari", "Aki", ""], ["Rast", "Philippe", ""]]}, {"id": "1801.05738", "submitter": "Haroldo Ribeiro", "authors": "Max Jauregui, Luciano Zunino, Ervin K. Lenzi, Renio S. Mendes, Haroldo\n  V. Ribeiro", "title": "Characterization of Time Series Via R\\'enyi Complexity-Entropy Curves", "comments": "19 pages, 6 figures; accepted for publication in Physica A", "journal-ref": "Physica A 498, 74 (2018)", "doi": "10.1016/j.physa.2018.01.026", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most useful tools for distinguishing between chaotic and\nstochastic time series is the so-called complexity-entropy causality plane.\nThis diagram involves two complexity measures: the Shannon entropy and the\nstatistical complexity. Recently, this idea has been generalized by considering\nthe Tsallis monoparametric generalization of the Shannon entropy, yielding\ncomplexity-entropy curves. These curves have proven to enhance the\ndiscrimination among different time series related to stochastic and chaotic\nprocesses of numerical and experimental nature. Here we further explore these\ncomplexity-entropy curves in the context of the R\\'enyi entropy, which is\nanother monoparametric generalization of the Shannon entropy. By combining the\nR\\'enyi entropy with the proper generalization of the statistical complexity,\nwe associate a parametric curve (the R\\'enyi complexity-entropy curve) with a\ngiven time series. We explore this approach in a series of numerical and\nexperimental applications, demonstrating the usefulness of this new technique\nfor time series analysis. We show that the R\\'enyi complexity-entropy curves\nenable the differentiation among time series of chaotic, stochastic, and\nperiodic nature. In particular, time series of stochastic nature are associated\nwith curves displaying positive curvature in a neighborhood of their initial\npoints, whereas curves related to chaotic phenomena have a negative curvature;\nfinally, periodic time series are represented by vertical straight lines.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:29:10 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jauregui", "Max", ""], ["Zunino", "Luciano", ""], ["Lenzi", "Ervin K.", ""], ["Mendes", "Renio S.", ""], ["Ribeiro", "Haroldo V.", ""]]}, {"id": "1801.05761", "submitter": "Jiaqi Cai", "authors": "Jiaqi Cai", "title": "Rasch Analysis of the Mathematics Self Concept Questionnaire", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TIMSS & PIRLS International Study Center is a research center at Boston\nCollege that conducts a series of assessments in a number of countries to\nmeasure trends in mathematics and science achievement at the fourth and eighth\ngrades. In general, TIMSS assessments include achievement tests as well as\nquestionnaires for student, parent, teacher, school, and curricular. There are\n63 participating countries and 14 benchmarking participants in TIMSS 2011,\nincluding 608,641 students, 49,429 teachers, 19,612 school principals, and the\nNational Research Coordinators of each country. TIMSS data are valuable for\nresearchers and analysts from all over the world, especially for those from\nparticipating countries, to conduct related studies to improve education.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 01:09:46 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Cai", "Jiaqi", ""]]}, {"id": "1801.06128", "submitter": "Zhongxiang Wang", "authors": "Zhongxiang Wang, Masoud Hamedi, Stanley Young", "title": "A methodology for calculating the latency of GPS-probe data", "comments": null, "journal-ref": "Transportation Research Record: Journal of the Transportation\n  Research Board, (2645), pp.76-85", "doi": "10.3141/2645-09", "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced GPS probe data has been gaining popularity in recent years as a\nsource for real-time traffic information. Efforts have been made to evaluate\nthe quality of such data from different perspectives. A quality indicator of\nany traffic data source is latency that describes the punctuality of data,\nwhich is critical for real-time operations, emergency response, and traveler\ninformation systems. This paper offers a methodology for measuring the probe\ndata latency, with respect to a selected reference source. Although Bluetooth\nre-identification data is used as the reference source, the methodology can be\napplied to any other ground-truth data source of choice (i.e. Automatic License\nPlate Readers, Electronic Toll Tag). The core of the methodology is a maximum\npattern matching algorithm that works with three different fitness objectives.\nTo test the methodology, sample field reference data were collected on multiple\nfreeways segments for a two-week period using portable Bluetooth sensors as\nground-truth. Equivalent GPS probe data was obtained from a private vendor, and\nits latency was evaluated. Latency at different times of the day, the impact of\nroad segmentation scheme on latency, and sensitivity of the latency to both\nspeed slowdown, and recovery from slowdown episodes are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 17:05:00 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Wang", "Zhongxiang", ""], ["Hamedi", "Masoud", ""], ["Young", "Stanley", ""]]}, {"id": "1801.06296", "submitter": "Rico Krueger", "authors": "Rico Krueger, Akshay Vij, Taha H. Rashidi", "title": "A Dirichlet Process Mixture Model of Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixed multinomial logit (MNL) model, which leverages the\ntruncated stick-breaking process representation of the Dirichlet process as a\nflexible nonparametric mixing distribution. The proposed model is a Dirichlet\nprocess mixture model and accommodates discrete representations of\nheterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL\nmodel, the proposed discrete choice model does not require the analyst to fix\nthe number of mixture components prior to estimation, as the complexity of the\ndiscrete mixing distribution is inferred from the evidence. For posterior\ninference in the proposed Dirichlet process mixture model of discrete choice,\nwe derive an expectation maximisation algorithm. In a simulation study, we\ndemonstrate that the proposed model framework can flexibly capture\ndifferently-shaped taste parameter distributions. Furthermore, we empirically\nvalidate the model framework in a case study on motorists' route choice\npreferences and find that the proposed Dirichlet process mixture model of\ndiscrete choice outperforms a latent class MNL model and mixed MNL models with\ncommon parametric mixing distributions in terms of both in-sample fit and\nout-of-sample predictive ability. Compared to extant modelling approaches, the\nproposed discrete choice model substantially abbreviates specification\nsearches, as it relies on less restrictive parametric assumptions and does not\nrequire the analyst to specify the complexity of the discrete mixing\ndistribution prior to estimation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:12:16 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Krueger", "Rico", ""], ["Vij", "Akshay", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1801.06437", "submitter": "Stephan Huckemann", "authors": "Karla Markert, Karolin Krehl, Carsten Gottschlich, Stephan F.\n  Huckemann", "title": "Detecting Anisotropy in Fingerprint Growth", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From infancy to adulthood, human growth is anisotropic, much more along the\nproximal-distal axis (height) than along the medial-lateral axis (width),\nparticularly at extremities. Detecting and modeling the rate of anisotropy in\nfingerprint growth, and possibly other growth patterns as well, facilitates the\nuse of children's fingerprints for long-term biometric identification. Using\nstandard fingerprint scanners, anisotropic growth is highly overshadowed by the\nvarying distortions created by each imprint, and it seems that this difficulty\nhas hampered to date the development of suitable methods, detecting anisotropy,\nlet alone, designing models. We provide a tool chain to statistically detect,\nwith a given confidence, anisotropic growth in fingerprints and its preferred\naxis, where we only require a standard fingerprint scanner and a minutiae\nmatcher. We build on a perturbation model, a new Procrustes-type algorithm, use\nand develop several parametric and non-parametric tests for different\nhypotheses, in particular for neighborhood hypotheses to detect the axis of\nanisotropy, where the latter tests are tunable to measurement accuracy. Taking\ninto account realistic distortions caused by pressing fingers on scanners, our\nsimulations based on real data indicate that, for example, already in rather\nsmall samples (56 matches) we can significantly detect proximal-distal growth\nif it exceeds medial-lateral growth by only around 5 percent. Our method is\nwell applicable to future datasets of children fingerprint time series and we\nprovide an implementation of our algorithms and tests with matched minutiae\npattern data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 15:01:13 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Markert", "Karla", ""], ["Krehl", "Karolin", ""], ["Gottschlich", "Carsten", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "1801.06533", "submitter": "Khalifa Es-Sebaiy", "authors": "Azzouz Dermoune, Khalifa Es-Sebaiy, Mohammed Es.Sebaiy and Jabrane\n  Moustaaid", "title": "Parametrizations, weights, and optimal prediction: Part 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the annual mean temperature prediction. The years\ntaken into account and the corresponding annual mean temperatures are denoted\nby $0,\\ldots, n$ and $t_0$, $\\ldots$, $t_n$, respectively. We propose to\npredict the temperature $t_{n+1}$ using the data $t_0$, $\\ldots$, $t_n$. For\neach $0\\leq l\\leq n$ and each parametrization $\\Theta^{(l)}$ of the Euclidean\nspace $\\mathbb{R}^{l+1}$ we construct a list of weights for the data\n$\\{t_0,\\ldots, t_l\\}$ based on the rows of $\\Theta^{(l)}$ which are correlated\nwith the constant trend. Using these weights we define a list of predictors of\n$t_{l+1}$ from the data $t_0$, $\\ldots$, $t_l$. We analyse how the\nparametrization affects the prediction, and provide three optimality criteria\nfor the selection of weights and parametrization. We illustrate our results for\nthe annual mean temperature of France and Morocco.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 23:25:55 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Dermoune", "Azzouz", ""], ["Es-Sebaiy", "Khalifa", ""], ["Sebaiy", "Mohammed Es.", ""], ["Moustaaid", "Jabrane", ""]]}, {"id": "1801.06596", "submitter": "Kairui Feng", "authors": "Siyuan Xian, Kairui Feng, Ning Lin, Reza Marsooli, Dan Chavas, Jie\n  Chen, Adam Hatzikyriakou", "title": "Rapid Assessment of Damaged Homes in the Florida Keys after Hurricane\n  Irma", "comments": "8 pages,3 figures", "journal-ref": null, "doi": "10.5194/nhess-18-2041-2018", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On September 10, 2017, Hurricane Irma made landfall in the Florida Keys and\ncaused significant damage. Informed by hydrodynamic storm surge and wave\nmodeling and post-storm satellite imagery, a rapid damage survey was soon\nconducted for 1600+ residential buildings in Big Pine Key and Marathon. Damage\ncategorizations and statistical analysis reveal distinct factors governing\ndamage at these two locations. The distance from the coast is significant for\nthe damage in Big Pine Key, as severely damaged buildings were located near\nnarrow waterways connected to the ocean. Building type and size are critical in\nMarathon, highlighted by the near-complete destruction of trailer communities\nthere. These observations raise issues of affordability and equity that need\nconsideration in damage recovery and rebuilding for resilience.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 23:09:58 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 05:19:50 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Xian", "Siyuan", ""], ["Feng", "Kairui", ""], ["Lin", "Ning", ""], ["Marsooli", "Reza", ""], ["Chavas", "Dan", ""], ["Chen", "Jie", ""], ["Hatzikyriakou", "Adam", ""]]}, {"id": "1801.06670", "submitter": "Alastair Rushworth Dr", "authors": "Alastair Rushworth", "title": "Bayesian Distributed Lag Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed lag models (DLMs) express the cumulative and delayed dependence\nbetween pairs of time-indexed response and explanatory variables. In practical\napplication, users of DLMs examine the estimated influence of a series of\nlagged covariates to assess patterns of dependence. Much recent methodological\nwork has sought to de- velop flexible parameterisations for smoothing the\nassociated lag parameters that avoid overfitting. However, this paper finds\nthat some widely-used DLMs introduce bias in the estimated lag influence, and\nare sensitive to the maximum lag which is typically chosen in advance of model\nfitting. Simulations show that bias and misspecification are dramatically\nreduced by generalising the smoothing model to allow varying penalisation of\nthe lag influence estimates. The resulting model is shown to have substantially\nfewer effective parameters and lower bias, providing the user with confidence\nthat the estimates are robust to prior model choice.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 13:00:24 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Rushworth", "Alastair", ""]]}, {"id": "1801.06757", "submitter": "Arturo Erdely", "authors": "Arturo Erdely", "title": "La falacia del empate t\\'ecnico electoral", "comments": "22 pages, 8 figures, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is argued that the concept of \"technical tie\" in electoral polls and quick\ncounts has no probabilistic basis, and that instead the uncertainty associated\nwith these statistical exercises should be expressed in terms of a probability\nof victory of the leading candidate.\n  -----\n  Se argumenta que el concepto de \"empate t\\'ecnico\" en encuestas y conteos\nr\\'apidos electorales no tiene fundamento probabil\\'istico, y que en su lugar\nla incertidumbre asociada a dichos ejercicios estad\\'isticos debiera expresarse\nen t\\'erminos de una probabilidad de triunfo del candidato puntero.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 03:18:43 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 00:10:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Erdely", "Arturo", ""]]}, {"id": "1801.06809", "submitter": "Soumita Modak", "authors": "Soumita Modak and Uttam Bandyopadhyay", "title": "A new nonparametric test for two sample multivariate location problem\n  with application to astronomy", "comments": "18 pages, 1 Fig", "journal-ref": "Journal of Statistical Theory and Applications, 2019, 18, 136-146", "doi": "10.2991/jsta.d.190515.002", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a nonparametric test for the identity of two multivariate\ncontinuous distribution functions (d.f.'s) when they differ in locations. The\ntest uses Wilcoxon rank-sum statistics on distances between observations for\neach of the components and is unaffected by outliers. It is numerically\ncompared with two existing procedures in terms of power. The simulation study\nshows that its power is strictly increasing in the sample sizes and/or in the\nnumber of components. The applicability of this test is demonstrated by use of\ntwo astronomical data sets on early-type galaxies.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 12:11:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Modak", "Soumita", ""], ["Bandyopadhyay", "Uttam", ""]]}, {"id": "1801.06936", "submitter": "Ning Ning", "authors": "Jinwen Qiu, Wenjian Liu and Ning Ning", "title": "Evolution of Regional Innovation with Spatial Knowledge Spillovers:\n  Convergence or Divergence?", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends endogenous economic growth models to incorporate knowledge\nexternality. We explores whether spatial knowledge spillovers among regions\nexist, whether spatial knowledge spillovers promote regional innovative\nactivities, and whether external knowledge spillovers affect the evolution of\nregional innovations in the long run. We empirically verify the theoretical\nresults through applying spatial statistics and econometric model in the\nanalysis of panel data of 31 regions in China. An accurate estimate of the\nrange of knowledge spillovers is achieved and the convergence of regional\nknowledge growth rate is found, with clear evidences that developing regions\nbenefit more from external knowledge spillovers than developed regions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 02:14:53 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 04:34:19 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 07:49:31 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Qiu", "Jinwen", ""], ["Liu", "Wenjian", ""], ["Ning", "Ning", ""]]}, {"id": "1801.07012", "submitter": "Jocelyn Chauvet", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG, UM3), Xavier Bry\n  (IMAG), Fr\\'ed\\'eric Mortier", "title": "Extension de la r\\'egression lin\\'eaire g\\'en\\'eralis\\'ee sur\n  composantes supervis\\'ees (SCGLR) aux donn\\'ees group\\'ees", "comments": "in French", "journal-ref": "48\\`emes Journ\\'ees de Statistique de la SFdS, May 2016,\n  Montpellier, France", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address component-based regularisation of a multivariate Generalized\nLinear Mixed Model. A set of random responses Y is modelled by a GLMM, using a\nset X of explanatory variables and a set T of additional covariates. Variables\nin X are assumed many and redundant: generalized linear mixed regression\ndemands regularisation with respect to X. By contrast, variables in T are\nassumed few and selected so as to demand no regularisation. Regularisation is\nperformed building an appropriate number of orthogonal components that both\ncontribute to model Y and capture relevant structural information in X. We\npropose to optimize a SCGLR-specific criterion within a Schall's algorithm in\norder to estimate the model. This extension of SCGLR is tested on simulated and\nreal data, and compared to Ridge-and Lasso-based regularisations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:53:13 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG, UM3"], ["Bry", "Xavier", "", "IMAG"], ["Mortier", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1801.07047", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Julius Gordon", "title": "News-based forecasts of macroeconomic indicators: A semantic path model\n  for interpretable predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The macroeconomic climate influences operations with regard to, e.g., raw\nmaterial prices, financing, supply chain utilization and demand quotas. In\norder to adapt to the economic environment, decision-makers across the public\nand private sectors require accurate forecasts of the economic outlook.\nExisting predictive frameworks base their forecasts primarily on time series\nanalysis, as well as the judgments of experts. As a consequence, current\napproaches are often biased and prone to error. In order to reduce forecast\nerrors, this paper presents an innovative methodology that extends lag\nvariables with unstructured data in the form of financial news: (1) we apply a\nvariety of models from machine learning to word counts as a high-dimensional\ninput. However, this approach suffers from low interpretability and\noverfitting, motivating the following remedies. (2) We follow the intuition\nthat the economic climate is driven by general sentiments and suggest a\nprojection of words onto latent semantic structures as a means of feature\nengineering. (3) We propose a semantic path model, together with estimation\ntechnique based on regularization, in order to yield full interpretability of\nthe forecasts. We demonstrate the predictive performance of our approach by\nutilizing 80,813 ad hoc announcements in order to make long-term forecasts of\nup to 24 months ahead regarding key macroeconomic indicators. Back-testing\nreveals a considerable reduction in forecast errors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 11:26:30 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 20:24:03 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Gordon", "Julius", ""]]}, {"id": "1801.07104", "submitter": "Paul Pudaite", "authors": "Paul R. Pudaite", "title": "Heating Up in NBA Free Throw Shooting", "comments": "26 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I demonstrate that repetition heats players up, while interruption cools\nplayers down in NBA free throw shooting. My analysis also suggests that fatigue\nand stress come into play. If, as seems likely, all four of these effects have\ncomparable impact on field goal shooting, they would justify strategic choices\nthroughout a basketball game that take into account the hot hand. More\ngenerally my analysis motivates approaching causal investigation of the\nvariation in the quality of all types of human performance by seeking to\noperationalize and measure these effects. Viewing the hot hand as a dynamic,\ncausal process motivates an alternative application of the concept of the hot\nhand: instead of trying to detect which player happens to be hot at the moment,\npromote that which heats up you and your allies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 00:28:22 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Pudaite", "Paul R.", ""]]}, {"id": "1801.07165", "submitter": "Bochao Jia", "authors": "Bochao Jia", "title": "The application of Monte Carlo methods for learning generalized linear\n  model", "comments": null, "journal-ref": "Biom Biostat Int J. 2018; 7(5): 422-427", "doi": "10.15406/bbij.2018.07.00241", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo method is a broad class of computational algorithms that rely on\nrepeated random sampling to obtain numerical results. They are often used in\nphysical and mathematical problems and are most useful when it is difficult or\nimpossible to use other mathematical methods. Basically, many statisticians\nhave been increasingly drawn to Monte Carlo method in three distinct problem\nclasses: optimization, numerical integration, and generating draws from a\nprobability distribution. In this paper, we will introduce the Monte Carlo\nmethod for calculating coefficients in Generalized Linear Model(GLM),\nespecially for Logistic Regression. Our main methods are Metropolis\nHastings(MH) Algorithms and Stochastic Approximation in Monte Carlo\nComputation(SAMC). For comparison, we also get results automatically using MLE\nmethod in R software.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 22:51:12 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 19:55:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Jia", "Bochao", ""]]}, {"id": "1801.07318", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Seth R. Flaxman, Daniel E. Runcie, Mike West", "title": "Variable Prioritization in Nonlinear Black Box Methods: A Genetic\n  Association Case Study", "comments": "28 pages, 5 figures, 1 tables; Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The central aim in this paper is to address variable selection questions in\nnonlinear and nonparametric regression. Motivated by statistical genetics,\nwhere nonlinear interactions are of particular interest, we introduce a novel\nand interpretable way to summarize the relative importance of predictor\nvariables. Methodologically, we develop the \"RelATive cEntrality\" (RATE)\nmeasure to prioritize candidate genetic variants that are not just marginally\nimportant, but whose associations also stem from significant covarying\nrelationships with other variants in the data. We illustrate RATE through\nBayesian Gaussian process regression, but the methodological innovations apply\nto other \"black box\" methods. It is known that nonlinear models often exhibit\ngreater predictive accuracy than linear models, particularly for phenotypes\ngenerated by complex genetic architectures. With detailed simulations and two\nreal data association mapping studies, we show that applying RATE enables an\nexplanation for this improved performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:57:39 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 23:32:04 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 00:36:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Crawford", "Lorin", ""], ["Flaxman", "Seth R.", ""], ["Runcie", "Daniel E.", ""], ["West", "Mike", ""]]}, {"id": "1801.07351", "submitter": "Claire Donnat", "authors": "Claire Donnat, Susan Holmes", "title": "Tracking network dynamics: a survey of distances and similarity metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From longitudinal biomedical studies to social networks, graphs have emerged\nas a powerful framework for describing evolving interactions between agents in\ncomplex systems. In such studies, after pre-processing, the data can be\nrepresented by a set of graphs, each representing a system's state at different\npoints in time. The analysis of the system's dynamics depends on the selection\nof the appropriate analytical tools. After characterizing similarities between\nstates, a critical step lies in the choice of a distance between graphs capable\nof reflecting such similarities. While the literature offers a number of\ndistances that one could a priori choose from, their properties have been\nlittle investigated and no guidelines regarding the choice of such a distance\nhave yet been provided. In particular, most graph distances consider that the\nnodes are exchangeable and do not take into account node identities. Accounting\nfor the alignment of the graphs enables us to enhance these distances'\nsensitivity to perturbations in the network and detect important changes in\ngraph dynamics. Thus the selection of an adequate metric is a decisive --yet\ndelicate--practical matter.\n  In the spirit of Goldenberg, Zheng and Fienberg's seminal 2009 review, the\npurpose of this article is to provide an overview of commonly-used graph\ndistances and an explicit characterization of the structural changes that they\nare best able to capture. We use as a guiding thread to our discussion the\napplication of these distances to the analysis of both a longitudinal\nmicrobiome dataset and a brain fMRI study. We show examples of using\npermutation tests to detect the effect of covariates on the graphs'\nvariability. Synthetic examples provide intuition as to the qualities and\ndrawbacks of the different distances. Above all, we provide some guidance for\nchoosing one distance over another in certain types of applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 23:53:55 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 15:45:58 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Donnat", "Claire", ""], ["Holmes", "Susan", ""]]}, {"id": "1801.07742", "submitter": "Luiza Mihaela Paun", "authors": "L. Mihaela Paun, M. Umar Qureshi, Mitchel Colebank, Nicholas A. Hill,\n  Mette S. Olufsen, Mansoor A. Haider, Dirk Husmeier", "title": "MCMC methods for inference in a mathematical model of pulmonary\n  circulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study performs parameter inference in a partial differential equations\nsystem of pulmonary circulation. We use a fluid dynamics network model that\ntakes selected parameter values and mimics the behaviour of the pulmonary\nhaemodynamics under normal physiological and pathological conditions. This is\nof medical interest as it enables tracking the progression of pulmonary\nhypertension. We show how we make the fluids model tractable by reducing the\nparameter dimension from a 55D to a 5D problem. The Delayed Rejection Adaptive\nMetropolis (DRAM) algorithm, coupled with constraint nonlinear optimization is\nsuccessfully used to learn the parameter values and quantify the uncertainty in\nthe parameter estimates. To accommodate for different magnitudes of the\nparameter values, we introduce an improved parameter scaling technique in the\nDRAM algorithm. Formal convergence diagnostics are employed to check for\nconvergence of the Markov chains. Additionally, we perform model selection\nusing different information criteria, including Watanabe Akaike Information\nCriteria.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:33:39 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Paun", "L. Mihaela", ""], ["Qureshi", "M. Umar", ""], ["Colebank", "Mitchel", ""], ["Hill", "Nicholas A.", ""], ["Olufsen", "Mette S.", ""], ["Haider", "Mansoor A.", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1801.07765", "submitter": "Adrian Dobra", "authors": "Adrian Dobra, Camilo Valdes, Dragana Ajdic, Bertrand Clarke and\n  Jennifer Clarke", "title": "Modeling association in microbial communities with clique loglinear\n  models", "comments": "30 pages, 17 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing awareness of the important roles that microbial\ncommunities play in complex biological processes. Modern investigation of these\noften uses next generation sequencing of metagenomic samples to determine\ncommunity composition. We propose a statistical technique based on clique\nloglinear models and Bayes model averaging to identify microbial components in\na metagenomic sample at various taxonomic levels that have significant\nassociations. We describe the model class, a stochastic search technique for\nmodel selection, and the calculation of estimates of posterior probabilities of\ninterest. We demonstrate our approach using data from the Human Microbiome\nProject and from a study of the skin microbiome in chronic wound healing. Our\ntechnique also identifies significant dependencies among microbial components\nas evidence of possible microbial syntrophy.\n  KEYWORDS: contingency tables, graphical models, model selection, microbiome,\nnext generation sequencing\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 20:48:25 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Dobra", "Adrian", ""], ["Valdes", "Camilo", ""], ["Ajdic", "Dragana", ""], ["Clarke", "Bertrand", ""], ["Clarke", "Jennifer", ""]]}, {"id": "1801.07767", "submitter": "Takoua Jendoubi", "authors": "Takoua Jendoubi, Timothy M.D. Ebbels", "title": "Integrative analysis of time course metabolic data and biomarker\n  discovery", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metabonomics time-course experiments provide the opportunity to understand\nthe changes to an organism by observing the evolution of metabolic profiles in\nresponse to internal or external stimuli. Along with other omic longitudinal\nprofiling technologies, these techniques have great potential to complement the\nanalysis of complex relations between variations across diverse omic variables\nand provide unique insights into the underlying biology of the system. However,\nmany statistical methods currently used to analyse short time-series omic data\nare i) prone to overfitting or ii) do not take into account the experimental\ndesign or iii) do not make full use of the multivariate information intrinsic\nto the data or iv) unable to uncover multiple associations between different\nomic data. The model we propose is an attempt to i) overcome overfitting by\nusing a weakly informative Bayesian model, ii) capture experimental design\nconditions through a mixed-effects model, iii) model interdependencies between\nvariables by augmenting the mixed-effects model with a conditional\nauto-regressive (CAR) component and iv) identify potential associations between\nheterogeneous omic variables .\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 20:50:03 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:44:12 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Jendoubi", "Takoua", ""], ["Ebbels", "Timothy M. D.", ""]]}, {"id": "1801.07793", "submitter": "Yeawon Yoo", "authors": "Yeawon Yoo, Adolfo R. Escobedo, and J. Kyle Skolfield", "title": "A New Correlation Coefficient for Aggregating Non-strict and Incomplete\n  Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.CO math.OC stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a correlation coefficient that is designed to deal with a\nvariety of ranking formats including those containing non-strict (i.e.,\nwith-ties) and incomplete (i.e., unknown) preferences. The correlation\ncoefficient is designed to enforce a neutral treatment of incompleteness\nwhereby no assumptions are made about individual preferences involving unranked\nobjects. The new measure, which can be regarded as a generalization of the\nseminal Kendall tau correlation coefficient, is proven to satisfy a set of\nmetric-like axioms and to be equivalent to a recently developed ranking\ndistance function associated with Kemeny aggregation. In an effort to further\nunify and enhance both robust ranking methodologies, this work proves the\nequivalence of an additional distance and correlation-coefficient pairing in\nthe space of non-strict incomplete rankings. These connections induce new exact\noptimization methodologies: a specialized branch and bound algorithm and an\nexact integer programming formulation. Moreover, the bridging of these\ncomplementary theories reinforces the singular suitability of the featured\ncorrelation coefficient to solve the general consensus ranking problem. The\nlatter premise is bolstered by an accompanying set of experiments on random\ninstances, which are generated via a herein developed sampling technique\nconnected with the classic Mallows distribution of ranking data. Associated\nexperiments with the branch and bound algorithm demonstrate that, as data\nbecomes noisier, the featured correlation coefficient yields relatively fewer\nalternative optimal solutions and that the aggregate rankings tend to be closer\nto an underlying ground truth shared by a majority.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 22:14:45 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 09:45:09 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 00:55:49 GMT"}, {"version": "v4", "created": "Sat, 16 Feb 2019 20:38:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Yoo", "Yeawon", ""], ["Escobedo", "Adolfo R.", ""], ["Skolfield", "J. Kyle", ""]]}, {"id": "1801.07822", "submitter": "Wenqian Wang", "authors": "Wenqian Wang and Beth Andrews", "title": "Partially Specified Spatial Autoregressive Model with Artificial Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial autoregressive model, introduced by Clif and Ord in 1970s has been\nwidely applied in many areas of science and econometrics such as regional\neconomics, public finance, political sciences, agricultural economics,\nenvironmental studies and transportation analyses. As information technology\ngrows rapidly, observations are seldom independent from others so a space\nautoregressive models can take this dependence into account and draw more\nreliable conclusions between covariates and the target variable itself. Based\non the classical spatial model, Su and Jin proposed a semi-parametric model\nnamed as partially specified spatial autoregressive model (PSAR) to allow for\nmore flexibility in modeling. And to estimate this nonparametric component, we\nuse the neural network model which adds more flexibility to the classical model\nand allows for variations in the choice of activation functions according to\ndifferent types of data. This paper extends an artificial neural network model\nto a partially specified space autoregressive model and proposes maximum\nlikelihood estimators instead of quasi-maximum likelihood estimates. We\nestablish the consistency and asymptotic normality of the estimators in this\nmodel. These results are obtained under some standard conditions in spatial\nmodels as well as neural network models. To illustrate, we investigate the\nquality of the normal approximation for finite samples by means of numerical\nsimulation studies with three common choices of error distributions (standard\nnormal, student-t distribution and the Laplace distribution). We apply our\nproposed model to a soil-water tension problem and a criminal study in Chicago.\nThe results showed that our model can capture the spatial dependence between\nunits as well as the unknown correlation structure between the target variable\nand covariates.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 00:49:25 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 15:04:57 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Wenqian", ""], ["Andrews", "Beth", ""]]}, {"id": "1801.07826", "submitter": "Susan Athey", "authors": "Susan Athey, David Blei, Robert Donnelly, Francisco Ruiz, and Tobias\n  Schmidt", "title": "Estimating Heterogeneous Consumer Preferences for Restaurants and Travel\n  Time Using Mobile Location Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes consumer choices over lunchtime restaurants using data\nfrom a sample of several thousand anonymous mobile phone users in the San\nFrancisco Bay Area. The data is used to identify users' approximate typical\nmorning location, as well as their choices of lunchtime restaurants. We build a\nmodel where restaurants have latent characteristics (whose distribution may\ndepend on restaurant observables, such as star ratings, food category, and\nprice range), each user has preferences for these latent characteristics, and\nthese preferences are heterogeneous across users. Similarly, each item has\nlatent characteristics that describe users' willingness to travel to the\nrestaurant, and each user has individual-specific preferences for those latent\ncharacteristics. Thus, both users' willingness to travel and their base utility\nfor each restaurant vary across user-restaurant pairs. We use a Bayesian\napproach to estimation. To make the estimation computationally feasible, we\nrely on variational inference to approximate the posterior distribution, as\nwell as stochastic gradient descent as a computational approach. Our model\nperforms better than more standard competing models such as multinomial logit\nand nested logit models, in part due to the personalization of the estimates.\nWe analyze how consumers re-allocate their demand after a restaurant closes to\nnearby restaurants versus more distant restaurants with similar\ncharacteristics, and we compare our predictions to actual outcomes. Finally, we\nshow how the model can be used to analyze counterfactual questions such as what\ntype of restaurant would attract the most consumers in a given location.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 23:55:42 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Athey", "Susan", ""], ["Blei", "David", ""], ["Donnelly", "Robert", ""], ["Ruiz", "Francisco", ""], ["Schmidt", "Tobias", ""]]}, {"id": "1801.08007", "submitter": "Ricardo Cris\\'ostomo", "authors": "Ricardo Crisostomo, Lorena Couso", "title": "Financial density forecasts: A comprehensive comparison of risk-neutral\n  and historical schemes", "comments": "Journal of Forecasting, 2018", "journal-ref": null, "doi": "10.1002/for.2521", "report-no": null, "categories": "q-fin.RM math.PR q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the forecasting ability of the most commonly used benchmarks\nin financial economics. We approach the usual caveats of probabilistic\nforecasts studies -small samples, limited models and non-holistic validations-\nby performing a comprehensive comparison of 15 predictive schemes during a time\nperiod of over 21 years. All densities are evaluated in terms of their\nstatistical consistency, local accuracy and forecasting errors. Using a new\ncomposite indicator, the Integrated Forecast Score (IFS), we show that\nrisk-neutral densities outperform historical-based predictions in terms of\ninformation content. We find that the Variance Gamma model generates the\nhighest out-of-sample likelihood of observed prices and the lowest predictive\nerrors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts\nacross the entire density range. In contrast, lognormal densities, the Heston\nmodel or the Breeden-Litzenberger formula yield biased predictions and are\nrejected in statistical tests.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 14:50:49 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 14:28:47 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Crisostomo", "Ricardo", ""], ["Couso", "Lorena", ""]]}, {"id": "1801.08061", "submitter": "Dana Goin", "authors": "Dana E. Goin, Jennifer Ahern", "title": "Identification of Spikes in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of unexpectedly high values in a time series is useful for\nepidemiologists, economists, and other social scientists interested in the\neffect of an exposure spike on an outcome variable. However, the best method to\nidentify spikes in time series is not known. This paper aims to fill this gap\nby testing the performance of several spike detection methods in a simulation\nsetting. We created simulations parameterized by monthly violence rates in nine\nCalifornia cities that represented different series features, and randomly\ninserted spikes into the series. We then compared the ability to detect spikes\nof the following methods: ARIMA modeling, Kalman filtering and smoothing,\nwavelet modeling with soft thresholding, and an iterative outlier detection\nmethod. We varied the magnitude of spikes from 10-50% of the mean rate over the\nstudy period and varied the number of spikes inserted from 1 to 10. We assessed\nperformance of each method using sensitivity and specificity. The Kalman\nfiltering and smoothing procedure had the best overall performance. We applied\nKalman filtering and smoothing to the monthly violence rates in nine California\ncities and identified spikes in the rate over the 2005-2012 period.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 16:32:40 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Goin", "Dana E.", ""], ["Ahern", "Jennifer", ""]]}, {"id": "1801.08153", "submitter": "Yikai Wang", "authors": "Ying Guo, Yikai Wang, Terri Marin, Easley Kirk, Ravi M. Patel,\n  Cassandra D. Josephson", "title": "Statistical methods for characterizing transfusion-related changes in\n  regional oxygenation using Near-infrared spectroscopy (NIRS) in preterm\n  infants", "comments": "25 pages, Statistical Methods in Medical Research. (2018)", "journal-ref": null, "doi": "10.1177/0962280218786302", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near infrared spectroscopy (NIRS) is an imaging-based diagnostic tool that\nprovides non-invasive and continuous evaluation of regional tissue oxygenation\nin real-time. In recent years, NIRS has show promise as a useful monitoring\ntechnology to help detect relative tissue ischemia that could lead to\nsignificant morbidity and mortality in preterm infants. However, some issues\ninherent in NIRS technology use on neonates, such as wide fluctuation in\nsignals, signal dropout and low limit of detection of the device, pose\nchallenges that may obscure reliable interpretation of the NIRS measurements\nusing current methods of analysis. In this paper, we propose new statistical\nmethods to analyse mesenteric rSO2 (regional oxygenation) produced by NIRS to\nevaluate oxygenation in intestinal tissues and investigate oxygenation response\nto red blood cell transfusion (RBC) in preterm infants. We present a mean area\nunder the curve (MAUC) measure and a slope measure to capture the mean rSO2\nlevel and temporal trajectory of rSO2, respectively. Estimation methods are\ndeveloped for these measures and nonparametric testing procedures are proposed\nto detect RBC-related changes in mesenteric oxygenation in preterm infants.\nThrough simulation studies, we show that the proposed methods demonstrate\nimproved accuracy in characterizing the mean level and changing pattern of\nmesenteric rSO2 and also increased statistical power in detecting RBC-related\nchanges, as compared with standard approaches. We apply our methods to a NIRS\nstudy in preterm infants receiving RBC transfusion from Emory Univerity to\nevaluate the pre- and post-transfusion mesenteric oxygenation in preterm\ninfants.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 19:01:40 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 18:29:02 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Guo", "Ying", ""], ["Wang", "Yikai", ""], ["Marin", "Terri", ""], ["Kirk", "Easley", ""], ["Patel", "Ravi M.", ""], ["Josephson", "Cassandra D.", ""]]}, {"id": "1801.08300", "submitter": "Soumita Modak", "authors": "Uttam Bandyopadhyay and Soumita Modak", "title": "Bivariate density estimation using normal-gamma kernel with application\n  to astronomy", "comments": "29 pages, 8 figs", "journal-ref": "Journal of Applied Probability and Statistics, 2018, 13, 23-39", "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimation of a bivariate density function with\nsupport $\\Re\\times[0,\\infty)$, where a classical bivariate kernel estimator\ncauses boundary bias due to the non-negative variable. To overcome this\nproblem, we propose four kernel density estimators and compare their\nperformances in terms of the mean integrated squared error. Simulation study\nshows that the estimator based on the proposed normal-gamma kernel performs\nbest. Two astronomical data sets are used to demonstrate the applicability of\nthis estimator.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 07:58:52 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 15:39:18 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Bandyopadhyay", "Uttam", ""], ["Modak", "Soumita", ""]]}, {"id": "1801.08474", "submitter": "Patrick Raanes PhD", "authors": "Patrick N. Raanes, Marc Bocquet, and Alberto Carrassi", "title": "Adaptive covariance inflation in the ensemble Kalman filter by Gaussian\n  scale mixtures", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3386", "report-no": null, "categories": "physics.data-an nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies multiplicative inflation: the complementary scaling of the\nstate covariance in the ensemble Kalman filter (EnKF). Firstly, error sources\nin the EnKF are catalogued and discussed in relation to inflation; nonlinearity\nis given particular attention as a source of sampling error. In response, the\n\"finite-size\" refinement known as the EnKF-N is re-derived via a Gaussian scale\nmixture, again demonstrating how it yields adaptive inflation. Existing methods\nfor adaptive inflation estimation are reviewed, and several insights are gained\nfrom a comparative analysis. One such adaptive inflation method is selected to\ncomplement the EnKF-N to make a hybrid that is suitable for contexts where\nmodel error is present and imperfectly parameterized. Benchmarks are obtained\nfrom experiments with the two-scale Lorenz model and its slow-scale truncation.\nThe proposed hybrid EnKF-N method of adaptive inflation is found to yield\nsystematic accuracy improvements in comparison with the existing methods,\nalbeit to a moderate degree.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 16:33:01 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 21:55:22 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 13:23:07 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 14:02:16 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Raanes", "Patrick N.", ""], ["Bocquet", "Marc", ""], ["Carrassi", "Alberto", ""]]}, {"id": "1801.08494", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks", "title": "Evaluating Predictive Models of Student Success: Closing the\n  Methodological Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model evaluation -- the process of making inferences about the performance of\npredictive models -- is a critical component of predictive modeling research in\nlearning analytics. We survey the state of the practice with respect to model\nevaluation in learning analytics, which overwhelmingly uses only naive methods\nfor model evaluation or statistical tests which are not appropriate for\npredictive model evaluation. We conduct a critical comparison of both null\nhypothesis significance testing (NHST) and a preferred Bayesian method for\nmodel evaluation. Finally, we apply three methods -- the na{\\\"i}ve average\ncommonly used in learning analytics, NHST, and Bayesian -- to a predictive\nmodeling experiment on a large set of MOOC data. We compare 96 different\npredictive models, including different feature sets, statistical modeling\nalgorithms, and tuning hyperparameters for each, using this case study to\ndemonstrate the different experimental conclusions these evaluation techniques\nprovide.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 14:02:08 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 18:33:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1801.08532", "submitter": "Weitao Duan", "authors": "Ya Xu, Weitao Duan, Shaochen Huang", "title": "SQR: Balancing Speed, Quality and Risk in Online Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled experimentation, also called A/B testing, is widely adopted to\naccelerate product innovations in the online world. However, how fast we\ninnovate can be limited by how we run experiments. Most experiments go through\na \"ramp up\" process where we gradually increase the traffic to the new\ntreatment to 100%. We have seen huge inefficiency and risk in how experiments\nare ramped, and it is getting in the way of innovation. This can go both ways:\nwe ramp too slowly and much time and resource is wasted; or we ramp too fast\nand suboptimal decisions are made. In this paper, we build up a ramping\nframework that can effectively balance among Speed, Quality and Risk (SQR). We\nstart out by identifying the top common mistakes experimenters make, and then\nintroduce the four SQR principles corresponding to the four ramp phases of an\nexperiment. To truly scale SQR to all experiments, we develop a statistical\nalgorithm that is embedded into the process of running every experiment to\nautomatically recommend ramp decisions. Finally, to complete the whole picture,\nwe briefly cover the auto-ramp engineering infrastructure that can collect\ninputs and execute on the recommendations timely and reliably.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 18:54:26 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Xu", "Ya", ""], ["Duan", "Weitao", ""], ["Huang", "Shaochen", ""]]}, {"id": "1801.08602", "submitter": "Xianan Huang", "authors": "Xianan Huang, Huei Peng", "title": "Eco-Routing based on a Data Driven Fuel Consumption Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric fuel consumption model is developed and used for eco-routing\nalgorithm development in this paper. Six months of driving information from the\ncity of Ann Arbor is collected from 2,000 vehicles. The road grade information\nfrom more than 1,100 km of road network is modeled and the software Autonomie\nis used to calculate fuel consumption for all trips on the road network. Four\ndifferent routing strategies including shortest distance, shortest time,\neco-routing, and travel-time-constrained eco-routing are compared. The results\nshow that eco-routing can reduce fuel consumption, but may increase travel\ntime. A travel-time-constrained eco-routing algorithm is developed to keep most\nthe fuel saving benefit while incurring very little increase in travel time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 00:24:04 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Huang", "Xianan", ""], ["Peng", "Huei", ""]]}, {"id": "1801.08626", "submitter": "Yunda Huang", "authors": "Lily Zhang, Peter B. Gilbert, Edmund Capparelli, Yunda Huang", "title": "Pharmacokinetics Simulations for Studying Correlates of Prevention\n  Efficacy of Passive HIV-1 Antibody Prophylaxis in the Antibody Mediated\n  Prevention (AMP) Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate\nwhether drug concentration over time, as estimated by non-linear mixed effects\npharmacokinetics (PK) models, is associated with HIV infection rate. We\nconducted a simulation study of marker sampling designs, and evaluated the\neffect of study adherence and sub-cohort sample size on PK model estimates in\nmultiple-dose studies. With m=120, even under low adherence (about half of\nstudy visits missing per participant), reasonably unbiased and consistent\nestimates of most fixed and random effect terms were obtained. Coarsened marker\nsampling schedules were also studied.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:43:57 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zhang", "Lily", ""], ["Gilbert", "Peter B.", ""], ["Capparelli", "Edmund", ""], ["Huang", "Yunda", ""]]}, {"id": "1801.08628", "submitter": "Luis Leon-Novelo", "authors": "Violeta G. Hennessey, Luis G. Leon-Novelo, Juan Li, Li Zhu, Eric Chi\n  and Joseph G. Ibrahim", "title": "A Bayesian Joint model for Longitudinal DAS28 Scores and Competing Risk\n  Informative Drop Out in a Rheumatoid Arthritis Clinical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rheumatoid arthritis clinical trials are strategically designed to collect\nthe disease activity score of each patient over multiple clinical visits,\nmeanwhile a patient may drop out before their intended completion due to\nvarious reasons. The dropout terminates the longitudinal data collection on the\npatients activity score. In the presence of informative dropout, that is, the\ndropout depends on latent variables from the longitudinal process, simply\napplying a model to analyze the longitudinal outcomes may lead to biased\nresults because the assumption of random dropout is violated. In this paper we\ndevelop a data driven Bayesian joint model for modeling DAS28 scores and\ncompeting risk informative drop out. The motivating example is a clinical trial\nof Etanercept and Methotrexate with radiographic Patient Outcomes (TEMPO,\nKeystone et.al).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 23:07:30 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Hennessey", "Violeta G.", ""], ["Leon-Novelo", "Luis G.", ""], ["Li", "Juan", ""], ["Zhu", "Li", ""], ["Chi", "Eric", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "1801.08668", "submitter": "Vibhu Agarwal", "authors": "Vibhu Agarwal, Matthew Smuck, Nigam H Shah", "title": "Monitoring physical function in patients with knee osteoarthritis using\n  data from wearable activity monitors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently used clinical assessments for physical function do not objectively\nquantify daily activities in routine living. Wearable activity monitors enable\nobjective measurement of routine daily activities, but do not map to clinically\nmeasured physical performance measures. We represent physical function as a\ndaily activity profile derived from minute-level activity data obtained via a\nwearable activity monitor. We construct daily activity profiles representing\naverage time spent in a set of activity classes over consecutive days using the\nOsteoarthritis Initiative (OAI) data. Using the daily activity profile as\ninput, we trained statistical models that classify subjects into quartiles of\nobjective measurements of physical function as measured via the 400m walk test,\nthe 20m walk test and 5 times sit stand test. We evaluated model performance on\nheld out data from the same calendar year as that used to train the models as\nwell as on activity data two years into the future. The daily activity profile\npredicts physical performance as measured via clinical assessments. Using held\nout data, the AUC obtained in classifying performance values in the 1st\nquartile was 0.79, 0.78 and 0.72, for the 400m walk, the 20m walk and 5 times\nsit stand tests. For classifying performance values in the 4th quartile, the\nAUC obtained was 0.77, 0.66 and 0.73 respectively. Evaluated on data from two\nyears into the future, for the 20m pace test and the 5 times sit stand tests,\nthe highest AUC obtained was 0.77 and 0.68 for the 1st quartile and 0.75 and\n0.70 for the 4th quartile respectively. We can construct activity profiles\nrepresenting actual physical function as demonstrated by the relationship\nbetween the activity profiles and the clinically measured physical performance\nmeasures. Measurement of physical performance via the activity profile as\ndescribed can enable remote functional monitoring of patients.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 04:08:21 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Agarwal", "Vibhu", ""], ["Smuck", "Matthew", ""], ["Shah", "Nigam H", ""]]}, {"id": "1801.08686", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Junjie Zhu, Chiara Sabatti", "title": "Selection-adjusted inference: an application to confidence intervals for\n  cis-eQTL effect sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of eQTL studies is to identify the genetic variants that influence\nthe expression levels of the genes in an organism. High throughput technology\nhas made such studies possible: in a given tissue sample, it enables us to\nquantify the expression levels of approximately 20,000 genes and to record the\nalleles present at millions of genetic polymorphisms. While obtaining this data\nis relatively cheap once a specimen is at hand, obtaining human tissue remains\na costly endeavor. Thus, eQTL studies continue to be based on relatively small\nsample sizes, with this limitation particularly serious for tissues of most\nimmediate medical relevance. Given the high dimensional nature of this datasets\nand the large number of hypotheses tested, the scientific community has adopted\nearly on multiplicity adjustment procedures, which primarily control the false\ndiscoveries rate for the identification of genetic variants with influence on\nthe expression levels. In contrast, a problem that has not received much\nattention to date is that of providing estimates of the effect sizes associated\nto these variants, in a way that accounts for the considerable amount of\nselection. We illustrate how the recently developed conditional inference\napproach can be deployed to obtain confidence intervals for the eQTL effect\nsizes with reliable coverage. The procedure we propose is based on a randomized\nhierarchical strategy that both reflects the steps typically adopted in state\nof the art investigations and introduces the use of randomness instead of data\nsplitting to maximize the use of available data. Analysis of the GTEx Liver\ndataset (v6) suggests that naively obtained confidence intervals would likely\nnot cover the true values of effect sizes and that the number of local genetic\npolymorphisms influencing the expression level of genes might be\nunderestimated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 06:17:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 03:59:22 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Zhu", "Junjie", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1801.08929", "submitter": "Matthew Levine", "authors": "Matthew E. Levine, David J. Albers, George Hripcsak", "title": "Methodological variations in lagged regression for detecting physiologic\n  drug effects in EHR data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We studied how lagged linear regression can be used to detect the physiologic\neffects of drugs from data in the electronic health record (EHR). We\nsystematically examined the effect of methodological variations ((i) time\nseries construction, (ii) temporal parameterization, (iii) intra-subject\nnormalization, (iv) differencing (lagged rates of change achieved by taking\ndifferences between consecutive measurements), (v) explanatory variables, and\n(vi) regression models) on performance of lagged linear methods in this\ncontext. We generated two gold standards (one knowledge-base derived, one\nexpert-curated) for expected pairwise relationships between 7 drugs and 4 labs,\nand evaluated how the 64 unique combinations of methodological perturbations\nreproduce gold standards. Our 28 cohorts included patients in Columbia\nUniversity Medical Center/NewYork-Presbyterian Hospital clinical database. The\nmost accurate methods achieved AUROC of 0.794 for knowledge-base derived gold\nstandard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%\nCI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],\nexpert-curated gold standard) across all methods that re-parameterize time\naccording to sequence and use either a joint autoregressive model with\ndifferencing or an independent lag model without differencing. The complement\nof this set of methods achieved a mean AUROC close to 0.5, indicating the\nimportance of these choices. We conclude that time- series analysis of EHR data\nwill likely rely on some of the beneficial pre-processing and modeling\nmethodologies identified, and will certainly benefit from continued careful\nanalysis of methodological perturbations. This study found that methodological\nvariations, such as pre-processing and representations, significantly affect\nresults, exposing the importance of evaluating these components when comparing\nmachine-learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 18:43:32 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Levine", "Matthew E.", ""], ["Albers", "David J.", ""], ["Hripcsak", "George", ""]]}, {"id": "1801.09002", "submitter": "Sophia Kyriakou", "authors": "Sophia Kyriakou, Ioannis Kosmidis, Nicola Sartori", "title": "Median bias reduction in random-effects meta-analysis and\n  meta-regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects models are frequently used to synthesise information from\ndifferent studies in meta-analysis. While likelihood-based inference is\nattractive both in terms of limiting properties and of implementation, its\napplication in random-effects meta-analysis may result in misleading\nconclusions, especially when the number of studies is small to moderate. The\ncurrent paper shows how methodology that reduces the asymptotic bias of the\nmaximum likelihood estimator of the variance component can also substantially\nimprove inference about the mean effect size. The results are derived for the\nmore general framework of random-effects meta-regression, which allows the mean\neffect size to vary with study-specific covariates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 23:17:08 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 08:54:28 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 13:31:44 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 14:32:33 GMT"}, {"version": "v5", "created": "Wed, 23 May 2018 18:29:35 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Kyriakou", "Sophia", ""], ["Kosmidis", "Ioannis", ""], ["Sartori", "Nicola", ""]]}, {"id": "1801.09126", "submitter": "Tania Roy", "authors": "Fushing Hsieh, Kevin Fujii, Tania Roy, Cho-Jui Hsieh, Brenda McCowan", "title": "Graphic displays of MLB pitching mechanics and its evolutions in\n  PITCHf/x data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systemic and idiosyncratic patterns in pitching mechanics of 24 top starting\npitchers in Major League Baseball (MLB) are extracted and discovered from\nPITCHf/x database. These evolving patterns across different pitchers or seasons\nare represented through three exclusively developed graphic displays.\nUnderstanding on such patterned evolutions will be beneficial for pitchers'\nwellbeing in signaling potential injury, and will be critical for expert\nknowledge in comparing pitchers. Based on data-driven computing, a universal\ncomposition of patterns is identified on all pitchers' mutual conditional\nentropy matrices. The first graphic display reveals that this universality\naccommodates physical laws as well as systemic characteristics of pitching\nmechanics. Such visible characters point to large scale factors for\ndifferentiating between distinct clusters of pitchers, and simultaneously lead\nto detailed factors for comparing individual pitchers. The second graphic\ndisplay shows choices of features that are able to express a pitcher's\nseason-by-season pitching contents via a series of 3(+2)D point-cloud\ngeometries. The third graphic display exhibits exquisitely a pitcher's\nidiosyncratic pattern-information of pitching across seasons by demonstrating\nall his pitch-subtype evolutions. These heatmap-based graphic displays are\nplatforms for visualizing and understanding pitching mechanics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 19:28:45 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hsieh", "Fushing", ""], ["Fujii", "Kevin", ""], ["Roy", "Tania", ""], ["Hsieh", "Cho-Jui", ""], ["McCowan", "Brenda", ""]]}, {"id": "1801.09231", "submitter": "Saptarshi Das", "authors": "Suparna Dutta Sinha, Saptarshi Das, Sujata Tarafdar, and Tapati Dutta", "title": "Monitoring of Wild Pseudomonas Biofilm Strain Conditions Using\n  Statistical Characterisation of Scanning Electron Microscopy Images", "comments": "34 pages, 14 figures", "journal-ref": "Industrial & Engineering Chemistry Research, volume 56, no. 34,\n  pages 9496-9512, 2017", "doi": "10.1021/acs.iecr.7b01106", "report-no": null, "categories": "physics.bio-ph cond-mat.soft q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper proposes a novel method of quantification of the variation\nin biofilm architecture, in correlation with the alteration of growth\nconditions that include, variations of substrate and conditioning layer. The\npolymeric biomaterial serving as substrates are widely used in implants and\nindwelling medical devices, while the plasma proteins serve as the conditioning\nlayer. The present method uses descriptive statistics of FESEM images of\nbiofilms obtained during a variety of growth conditions. We aim to explore here\nthe texture and fractal analysis techniques, to identify the most\ndiscriminatory features which are capable of predicting the difference in\nbiofilm growth conditions. We initially extract some statistical features of\nbiofilm images on bare polymer surfaces, followed by those on the same\nsubstrates adsorbed with two different types of plasma proteins, viz. Bovine\nserum albumin (BSA) and Fibronectin (FN), for two different adsorption times.\nThe present analysis has the potential to act as a futuristic technology for\ndeveloping a computerized monitoring system in hospitals with automated image\nanalysis and feature extraction, which may be used to predict the growth\nprofile of an emerging biofilm on surgical implants or similar medical\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 13:55:15 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sinha", "Suparna Dutta", ""], ["Das", "Saptarshi", ""], ["Tarafdar", "Sujata", ""], ["Dutta", "Tapati", ""]]}, {"id": "1801.09232", "submitter": "Saptarshi Das", "authors": "Bogdan Alexandru Cociu, Saptarshi Das, Lucia Billeci, Wasifa Jamal,\n  Koushik Maharatna, Sara Calderoni, Antonio Narzisi, and Filippo Muratori", "title": "Multimodal Functional and Structural Brain Connectivity Analysis in\n  Autism: A Preliminary Integrated Approach with EEG, fMRI and DTI", "comments": "14 pages, 14 figures, IEEE Transactions on Cognitive and\n  Developmental Systems, 2017", "journal-ref": null, "doi": "10.1109/TCDS.2017.2680408", "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach of integrating different neuroimaging\ntechniques to characterize an autistic brain. Different techniques like EEG,\nfMRI and DTI have traditionally been used to find biomarkers for autism, but\nthere have been very few attempts for a combined or multimodal approach of EEG,\nfMRI and DTI to understand the neurobiological basis of autism spectrum\ndisorder (ASD). Here, we explore how the structural brain network correlate\nwith the functional brain network, such that the information encompassed by\nthese two could be uncovered only by using the latter. In this paper, source\nlocalization from EEG using independent component analysis (ICA) and dipole\nfitting has been applied first, followed by selecting those dipoles that are\nclosest to the active regions identified with fMRI. This allows translating the\nhigh temporal resolution of EEG to estimate time varying connectivity at the\nspatial source level. Our analysis shows that the estimated functional\nconnectivity between two active regions can be correlated with the physical\nproperties of the structure obtained from DTI analysis. This constitutes a\nfirst step towards opening the possibility of using pervasive EEG to monitor\nthe long-term impact of ASD treatment without the need for frequent expensive\nfMRI or DTI investigations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 14:04:50 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cociu", "Bogdan Alexandru", ""], ["Das", "Saptarshi", ""], ["Billeci", "Lucia", ""], ["Jamal", "Wasifa", ""], ["Maharatna", "Koushik", ""], ["Calderoni", "Sara", ""], ["Narzisi", "Antonio", ""], ["Muratori", "Filippo", ""]]}, {"id": "1801.09238", "submitter": "Saptarshi Das", "authors": "Saptarshi Das, Kaushik Halder, and Amitava Gupta", "title": "Performance Analysis of Robust Stable PID Controllers Using Dominant\n  Pole Placement for SOPTD Process Models", "comments": "50 pages, 42 figures, Knowledge-Based Systems, 2018", "journal-ref": null, "doi": "10.1016/j.knosys.2018.01.030", "report-no": null, "categories": "cs.SY cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives new formulations for designing dominant pole placement\nbased proportional-integral-derivative (PID) controllers to handle second order\nprocesses with time delays (SOPTD). Previously, similar attempts have been made\nfor pole placement in delay-free systems. The presence of the time delay term\nmanifests itself as a higher order system with variable number of interlaced\npoles and zeros upon Pade approximation, which makes it difficult to achieve\nprecise pole placement control. We here report the analytical expressions to\nconstrain the closed loop dominant and non-dominant poles at the desired\nlocations in the complex s-plane, using a third order Pade approximation for\nthe delay term. However, invariance of the closed loop performance with\ndifferent time delay approximation has also been verified using increasing\norder of Pade, representing a closed to reality higher order delay dynamics.\nThe choice of the nature of non-dominant poles e.g. all being complex, real or\na combination of them modifies the characteristic equation and influences the\nachievable stability regions. The effect of different types of non-dominant\npoles and the corresponding stability regions are obtained for nine test-bench\nprocesses indicating different levels of open-loop damping and lag to delay\nratio. Next, we investigate which expression yields a wider stability region in\nthe design parameter space by using Monte Carlo simulations while uniformly\nsampling a chosen design parameter space. Various time and frequency domain\ncontrol performance parameters are investigated next, as well as their\ndeviations with uncertain process parameters, using thousands of Monte Carlo\nsimulations, around the robust stable solution for each of the nine test-bench\nprocesses.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 14:32:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Das", "Saptarshi", ""], ["Halder", "Kaushik", ""], ["Gupta", "Amitava", ""]]}, {"id": "1801.09261", "submitter": "Xu Wu", "authors": "Xu Wu, Tomasz Kozlowski, Hadi Meidani, Koroush Shirvan", "title": "Inverse Uncertainty Quantification using the Modular Bayesian Approach\n  based on Gaussian Process, Part 2: Application to TRACE", "comments": null, "journal-ref": null, "doi": "10.1016/j.nucengdes.2018.06.003", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Uncertainty Quantification (UQ) is a process to quantify the\nuncertainties in random input parameters while achieving consistency between\ncode simulations and physical observations. In this paper, we performed inverse\nUQ using an improved modular Bayesian approach based on Gaussian Process (GP)\nfor TRACE physical model parameters using the BWR Full-size Fine-Mesh Bundle\nTests (BFBT) benchmark steady-state void fraction data. The model discrepancy\nis described with a GP emulator. Numerical tests have demonstrated that such\ntreatment of model discrepancy can avoid over-fitting. Furthermore, we\nconstructed a fast-running and accurate GP emulator to replace TRACE full model\nduring Markov Chain Monte Carlo (MCMC) sampling. The computational cost was\ndemonstrated to be reduced by several orders of magnitude.\n  A sequential approach was also developed for efficient test source allocation\n(TSA) for inverse UQ and validation. This sequential TSA methodology first\nselects experimental tests for validation that has a full coverage of the test\ndomain to avoid extrapolation of model discrepancy term when evaluated at input\nsetting of tests for inverse UQ. Then it selects tests that tend to reside in\nthe unfilled zones of the test domain for inverse UQ, so that one can extract\nthe most information for posterior probability distributions of calibration\nparameters using only a relatively small number of tests. This research\naddresses the \"lack of input uncertainty information\" issue for TRACE physical\ninput parameters, which was usually ignored or described using expert opinion\nor user self-assessment in previous work. The resulting posterior probability\ndistributions of TRACE parameters can be used in future uncertainty,\nsensitivity and validation studies of TRACE code for nuclear reactor system\ndesign and safety analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 18:27:28 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wu", "Xu", ""], ["Kozlowski", "Tomasz", ""], ["Meidani", "Hadi", ""], ["Shirvan", "Koroush", ""]]}, {"id": "1801.09293", "submitter": "Qian Xiao", "authors": "Qian Xiao, Lin Wang, Hongquan Xu", "title": "Application of Kriging Models for a Drug Combination Experiment on Lung\n  Cancer", "comments": "Submitted to \"Statistics in Medicine\"", "journal-ref": "Statistics in Medicine, Volume 38, Issue 2, 2019, Pages 236-246", "doi": "10.1002/sim.7971", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial drugs have been widely applied in disease treatment, especially\nchemotherapy for cancer, due to its improved efficacy and reduced toxicity\ncompared with individual drugs. The study of combinatorial drugs requires\nefficient experimental designs and proper follow-up statistical modelling\ntechniques. Linear and non-linear models are often used in the response surface\nmodelling for such experiments. We propose the use of Kriging models to better\ndepict the response surfaces of combinatorial drugs and take into account the\nmeasurement error. We further study how proper experimental designs can reduce\nthe required number of runs. We illustrate our method via a combinatorial drug\nexperiment on lung cancer. We demonstrate that only 27 runs are needed to\npredict all 512 runs in the original experiment and achieve better precision\nthan existing analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 21:40:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Xiao", "Qian", ""], ["Wang", "Lin", ""], ["Xu", "Hongquan", ""]]}, {"id": "1801.09541", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio and Alexina J. Mason and Gianluca Baio", "title": "A Full Bayesian Model to Handle Structural Ones and Missingness in\n  Economic Evaluations from Individual-Level Data", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic evaluations from individual-level data are an important component of\nthe process of technology appraisal, with a view to informing resource\nallocation decisions. A critical problem in these analyses is that both\neffectiveness and cost data typically present some complexity (e.g. non\nnormality, spikes and missingness) that should be addressed using appropriate\nmethods. However, in routine analyses, simple standardised approaches are\ntypically used, possibly leading to biased inferences. We present a general\nBayesian framework that can handle the complexity. We show the benefits of\nusing our approach with a motivating example, the MenSS trial, for which there\nare spikes at one in the effectiveness and missingness in both outcomes. We\ncontrast a set of increasingly complex models and perform sensitivity analysis\nto assess the robustness of the conclusions to a range of plausible missingness\nassumptions. This paper highlights the importance of adopting a comprehensive\nmodelling approach to economic evaluations and the strategic advantages of\nbuilding these complex models within a Bayesian framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 14:55:13 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 19:36:56 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Gabrio", "Andrea", ""], ["Mason", "Alexina J.", ""], ["Baio", "Gianluca", ""]]}, {"id": "1801.09546", "submitter": "Michael Lash", "authors": "Michael T. Lash and Jason Slater and Philip M. Polgreen and Alberto M.\n  Segre", "title": "21 Million Opportunities: A 19 Facility Investigation of Factors\n  Affecting Hand Hygiene Compliance via Linear Predictive Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.03540", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This large-scale study, consisting of 21.3 million hand hygiene opportunities\nfrom 19 distinct facilities in 10 different states, uses linear predictive\nmodels to expose factors that may affect hand hygiene compliance. We examine\nthe use of features such as temperature, relative humidity, influenza severity,\nday/night shift, federal holidays and the presence of new medical residents in\npredicting daily hand hygiene compliance; the investigation is undertaken using\nboth a \"global\" model to glean general trends, and facility-specific models to\nelicit facility-specific insights. The results suggest that colder temperatures\nand federal holidays have an adverse effect on hand hygiene compliance rates,\nand that individual cultures and attitudes regarding hand hygiene exist among\nfacilities.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 04:40:59 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lash", "Michael T.", ""], ["Slater", "Jason", ""], ["Polgreen", "Philip M.", ""], ["Segre", "Alberto M.", ""]]}, {"id": "1801.09594", "submitter": "Tom Britton", "authors": "Tom Britton", "title": "Basic stochastic transmission models and their inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current survey paper concerns stochastic mathematical models for the\nspread of infectious diseases. It starts with the simplest setting of a\nhomogeneous population in which a transmittable disease spreads during a short\noutbreak. Assuming a large population some important features are presented:\nbranching process approximation, basic reproduction number $R_0$, and final\nsize of an outbreak. Some extensions towards realism are then discussed: models\nfor endemicity, various heterogeneities, and prior immmunity. The focus is then\nshifted to statistical inference. What can be estimated for these models for\nvarious levels of detailed data and with what precision? The paper ends by\ndescribing how the inference results may be used for determining successful\nvaccination strategies. This paper will appear as a chapter of a forthcoming\nbook entitled \\emph{Handbook of Infectious Disease Epidemiology}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:01:04 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Britton", "Tom", ""]]}, {"id": "1801.09617", "submitter": "Christopher Grob", "authors": "Christopher Grob, Andreas Bley", "title": "Comparison of wait time approximations in distribution networks using\n  (R,Q)-order policies", "comments": "27 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different approximations for the wait time in distribution\nnetworks, in which all warehouses use an (R,Q)-order policy. Reporting on the\nresults of extensive computational experiments, we evaluate the quality of\nseveral approximations presented in the literature. In these experiments, we\nused a simulation framework that was set-up to replicate the behavior of the\nmaterial flow in a real distribution network rather than to comply with the\nassumptions made in the literature for the different approximation. First, we\nused random demand data to analyze which approximation works best under which\nconditions. In a second step, we then checked if the results obtained for\nrandom data can be confirmed also for real-world demand data from our\nindustrial partner. Eventually, we derive some guidelines which shall help\npractitioners to select approximations which are suited well for their\napplication. Still, our results recommend further testing with the actual\napplication's data to verify if a chosen wait time approximation is indeed\nappropriate in a specific application setting.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:47:22 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Grob", "Christopher", ""], ["Bley", "Andreas", ""]]}, {"id": "1801.09652", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Jingshu Wang, Gibran Hemani, Jack Bowden, Dylan S.\n  Small", "title": "Statistical inference in two-sample summary-data Mendelian randomization\n  using robust adjusted profile score", "comments": "59 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a method of exploiting genetic variation to\nunbiasedly estimate a causal effect in presence of unmeasured confounding. MR\nis being widely used in epidemiology and other related areas of population\nscience. In this paper, we study statistical inference in the increasingly\npopular two-sample summary-data MR design. We show a linear model for the\nobserved associations approximately holds in a wide variety of settings when\nall the genetic variants satisfy the exclusion restriction assumption, or in\ngenetic terms, when there is no pleiotropy. In this scenario, we derive a\nmaximum profile likelihood estimator with provable consistency and asymptotic\nnormality. However, through analyzing real datasets, we find strong evidence of\nboth systematic and idiosyncratic pleiotropy in MR, echoing the omnigenic model\nof complex traits that is recently proposed in genetics. We model the\nsystematic pleiotropy by a random effects model, where no genetic variant\nsatisfies the exclusion restriction condition exactly. In this case we propose\na consistent and asymptotically normal estimator by adjusting the profile\nscore. We then tackle the idiosyncratic pleiotropy by robustifying the adjusted\nprofile score. We demonstrate the robustness and efficiency of the proposed\nmethods using several simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 18:13:44 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 19:09:50 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 02:31:18 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Wang", "Jingshu", ""], ["Hemani", "Gibran", ""], ["Bowden", "Jack", ""], ["Small", "Dylan S.", ""]]}, {"id": "1801.09795", "submitter": "Eduardo Elias Ribeiro Junior", "authors": "Eduardo E. Ribeiro Jr, Walmes M. Zeviani, Wagner H. Bonat, Clarice G.\n  B. Dem\\'etrio, John Hinde", "title": "Reparametrization of COM-Poisson Regression Models with Applications in\n  the Analysis of Experimental Data", "comments": "23 pages, 12 figures and 7 tables. Submitted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the analysis of count data often the equidispersion assumption is not\nsuitable, hence the Poisson regression model is inappropriate. As a\ngeneralization of the Poisson distribution, the COM-Poisson distribution can\ndeal with under-, equi- and overdispersed count data. It is a member of the\nexponential family of distributions and has well known special cases. In spite\nof the nice properties of the COM-Poisson distribution, its location parameter\ndoes not correspond to the expectation, which complicates the interpretation of\nregression models. In this paper, we propose a straightforward\nreparametrization of the COM-Poisson distribution based on an approximation to\nthe expectation of this distribution. The main advantage of our new\nparametrization is the straightforward interpretation of the regression\ncoefficients in terms of the expectation, as usual in the context of\ngeneralized linear models. Furthermore, the estimation and inference for the\nnew COM-Poisson regression model can be done based on the likelihood paradigm.\nWe carried out simulation studies to verify the finite sample properties of the\nmaximum likelihood estimators. The results from our simulation study show that\nthe maximum likelihood estimators are unbiased and consistent for both\nregression and dispersion parameters. We observed that the empirical\ncorrelation between the regression and dispersion parameter estimators is close\nto zero, which suggests that these parameters are orthogonal. We illustrate the\napplication of the proposed model through the analysis of three data sets with\nover-, under- and equidispersed count data. The study of distribution\nproperties through a consideration of dispersion, zero-inflated and heavy tail\nindexes, together with the results of data analysis show the flexibility over\nstandard approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 23:26:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ribeiro", "Eduardo E.", "Jr"], ["Zeviani", "Walmes M.", ""], ["Bonat", "Wagner H.", ""], ["Dem\u00e9trio", "Clarice G. B.", ""], ["Hinde", "John", ""]]}, {"id": "1801.10047", "submitter": "Hugo Raguet", "authors": "Hugo Raguet and Amandine Marrel", "title": "Target and Conditional Sensitivity Analysis with Emphasis on Dependence\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of sensitivity analysis of complex phenomena in presence of\nuncertainty, we motivate and precise the idea of orienting the analysis towards\na critical domain of the studied phenomenon. We make a brief history of related\napproaches in the literature, and propose a more general and systematic\napproach. Nonparametric measures of dependence being well-suited to this\napproach, we also make a review of available methods and of their use for\nsensitivity analysis, and clarify some of their properties. As a byproduct, we\nnotably describe a new way of computing correlation ratios for Sobol' indices,\nwhich does not require specific experience plans nor rely on independence of\nthe input factors. Finally, we show on synthetic numerical experiments both the\ninterest of target and conditional sensitivity analysis, and the relevance of\nthe dependence measures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:10:25 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:01:55 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Raguet", "Hugo", ""], ["Marrel", "Amandine", ""]]}, {"id": "1801.10309", "submitter": "Xu Wu", "authors": "Xu Wu, Koroush Shirvan, Tomasz Kozlowski", "title": "Demonstration of the Relationship between Sensitivity and\n  Identifiability for Inverse Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.06.032", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Uncertainty Quantification (UQ), or Bayesian calibration, is the\nprocess to quantify the uncertainties of random input parameters based on\nexperimental data. The introduction of model discrepancy term is significant\nbecause \"over-fitting\" can theoretically be avoided. But it also poses\nchallenges in the practical applications. One of the mostly concerned and\nunresolved problem is the \"lack of identifiability\" issue. With the presence of\nmodel discrepancy, inverse UQ becomes \"non-identifiable\" in the sense that it\nis difficult to precisely distinguish between the parameter uncertainties and\nmodel discrepancy when estimating the calibration parameters. Previous research\nto alleviate the non-identifiability issue focused on using informative priors\nfor the calibration parameters and the model discrepancy, which is usually not\na viable solution because one rarely has such accurate and informative prior\nknowledge. In this work, we show that identifiability is largely related to the\nsensitivity of the calibration parameters with regards to the chosen responses.\nWe adopted an improved modular Bayesian approach for inverse UQ that does not\nrequire priors for the model discrepancy term. The relationship between\nsensitivity and identifiability was demonstrated with a practical example in\nnuclear engineering. It was shown that, in order for a certain calibration\nparameter to be statistically identifiable, it should be significant to at\nleast one of the responses whose data are used for inverse UQ. Good\nidentifiability cannot be achieved for a certain calibration parameter if it is\nnot significant to any of the responses. It is also demonstrated that \"fake\nidentifiability\" is possible if model responses are not appropriately chosen,\nor inaccurate but informative priors are specified.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 05:56:05 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wu", "Xu", ""], ["Shirvan", "Koroush", ""], ["Kozlowski", "Tomasz", ""]]}, {"id": "1801.10497", "submitter": "Saeed Zamanzad Gavidel", "authors": "Saeed Z.Gavidel, Jeremy L.Rickli", "title": "Remanufacturing cost analysis under uncertain core quality and return\n  conditions: extreme and non-extreme scenarios", "comments": "26 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainties in core quality condition, return quantity and timing can\npropagate and accumulate in process cost and complicate cost assessments.\nHowever, regardless of cost assessment complexities, accurate cost models are\nrequired for successful remanufacturing operation management. In this paper,\njoint effects of core quality condition, return quantity, and timing on\nremanufacturing cost under normal and extreme return conditions is analyzed. To\nconduct this analysis, a novel multivariate stochastic model called Stochastic\nCost of Remanufacturing Model (SCoRM) is developed. In building SCoRM, a Hybrid\nPareto Distribution (HPD), Bernoulli process, and a polynomial cost function\nare employed. It is discussed that core return process can be characterized as\na Discrete Time Markov Chain (DTMC). In a case study, SCoRM is applied to\nassess remanufacturing costs of steam traps of a chemical complex. Its accuracy\nanalyzed and variations of SCoRM in predictive tasks assessed by bootstrapping\ntechnique. Through this variation analysis the best and worst cost scenarios\ndetermined. Finally, to generate comparative insights regarding predictive\nperformance of SCoRM, the model is compared to artificial neural network,\nsupport vector machine, generalized additive model, and random forest\nalgorithms. Results indicate that SCoRM can be efficiently utilized to analyze\nremanufacturing cost. Keywords: Remanufacturing, extreme value theory, hybrid\nPareto distribution, stochastic model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:38:00 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Gavidel", "Saeed Z.", ""], ["Rickli", "Jeremy L.", ""]]}, {"id": "1801.10516", "submitter": "Caterina De Bacco", "authors": "Christa Brelsford and Caterina De Bacco", "title": "Are `Water Smart Landscapes' Contagious? An epidemic approach on\n  networks to study peer effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test the existence of a neighborhood based peer effect around\nparticipation in an incentive based conservation program called `Water Smart\nLandscapes' (WSL) in the city of Las Vegas, Nevada. We use 15 years of\ngeo-coded daily records of WSL program applications and approvals compiled by\nthe Southern Nevada Water Authority and Clark County Tax Assessors rolls for\nhome characteristics. We use this data to test whether a spatially mediated\npeer effect can be observed in WSL participation likelihood at the household\nlevel. We show that epidemic spreading models provide more flexibility in\nmodeling assumptions, and also provide one mechanism for addressing problems\nassociated with correlated unobservables than hazards models which can also be\napplied to address the same questions. We build networks of neighborhood based\npeers for 16 randomly selected neighborhoods in Las Vegas and test for the\nexistence of a peer based influence on WSL participation by using a\nSusceptible-Exposed-Infected-Recovered epidemic spreading model (SEIR), in\nwhich a home can become infected via autoinfection or through contagion from\nits infected neighbors. We show that this type of epidemic model can be\ndirectly recast to an additive-multiplicative hazard model, but not to purely\nmultiplicative one. Using both inference and prediction approaches we find\nevidence of peer effects in several Las Vegas neighborhoods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 14:49:16 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Brelsford", "Christa", ""], ["De Bacco", "Caterina", ""]]}, {"id": "1801.10544", "submitter": "Luis Enrique Correa Rocha Dr", "authors": "Luis Enrique Correa Rocha and Luana de Freitas Nascimento", "title": "Assessing student's achievement gap between ethnic groups in Brazil", "comments": null, "journal-ref": "Journal of Intelligence 2019, 7(1):7", "doi": "10.3390/jintelligence7010007", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achievement gaps refer to the difference in the performance on examinations\nof students belonging to different social groups. Achievement gaps between\nethnic groups have been observed in several countries with heterogeneous\npopulations. In this paper, we analyze achievement gaps between ethnic\npopulations in Brazil by studying the performance of a large cohort of senior\nhigh-school students in a standardized national exam. We separate ethnic groups\ninto the Brazilian states to remove potential biases associated to\ninfrastructure and financial resources, cultural background and ethnic\nclustering. We focus on the disciplines of mathematics and writing that involve\ndifferent cognitive functions. We estimate the gaps and their statistical\nsignificance through the Welch's t-test and study key socio-economic variables\nthat may explain the existence or absence of gaps. We identify that gaps\nbetween ethnic groups are either statistically insignificant (p<.01) or small\n(2%-6%) if statistically significant, for students living in households with\nlow income. Increasing gaps however may be observed for higher income. On the\nother hand, while higher parental education is associated to higher\nperformance, it may either increase, decrease or maintain the gaps between\nWhite and Black, and between White and Pardo students. Our results support that\nsocio-economic variables have major impact on student's performance in both\nmathematics and writing examinations irrespectively of ethnic backgrounds,\ngiving evidence that genetic factors have little or no effect on ethnic group\nperformance when students are exposed to similar cultural and financial\ncontexts.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 16:50:38 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rocha", "Luis Enrique Correa", ""], ["Nascimento", "Luana de Freitas", ""]]}]