[{"id": "2001.00068", "submitter": "Shanshan Cao", "authors": "Kai Ni, Shanshan Cao, Xiaoming Huo", "title": "Asymptotic convergence rate of the longest run in an inflating Bernoulli\n  net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image detection, one problem is to test whether the set, though mostly\nconsisting of uniformly scattered points, also contains a small fraction of\npoints sampled from some (a priori unknown) curve, for example, a curve with\n$C^{\\alpha}$-norm bounded by $\\beta$. One approach is to analyze the data by\ncounting membership in multiscale multianisotropic strips, which involves an\nalgorithm that delves into the length of the path connecting many consecutive\n\"significant\" nodes. In this paper, we develop the mathematical formalism of\nthis algorithm and analyze the statistical property of the length of the\nlongest significant run. The rate of convergence is derived. Using percolation\ntheory and random graph theory, we present a novel probabilistic model named\npseudo-tree model. Based on the asymptotic results for pseudo-tree model, we\nfurther study the length of the longest significant run in an \"inflating\"\nBernoulli net. We find that the probability parameter $p$ of significant node\nplays an important role: there is a threshold $p_c$, such that in the cases of\n$p<p_c$ and $p>p_c$, very different asymptotic behaviors of the length of the\nsignificant are observed. We apply our results to the detection of an\nunderlying curvilinear feature and argue that we achieve the lowest possible\ndetectable strength in theory.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:28:17 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ni", "Kai", ""], ["Cao", "Shanshan", ""], ["Huo", "Xiaoming", ""]]}, {"id": "2001.00074", "submitter": "Huang Huang", "authors": "Huang Huang, Dorit Hammerling, Bo Li, Richard Smith", "title": "Combining interdependent climate model outputs in CMIP5: A spatial\n  Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projections of future climate change rely heavily on climate models, and\ncombining climate models through a multi-model ensemble is both more accurate\nthan a single climate model and valuable for uncertainty quantification.\nHowever, Bayesian approaches to multi-model ensembles have been criticized for\nmaking oversimplified assumptions about bias and variability, as well as\ntreating different models as statistically independent. This paper extends the\nBayesian hierarchical approach of Sansom et al. (2017) by explicitly accounting\nfor spatial variability and inter-model dependence. We propose a Bayesian\nhierarchical model that accounts for bias between climate models and\nobservations, spatial and inter-model dependence, the emergent relationship\nbetween historical and future periods, and natural variability. Extensive\nsimulations show that our model provides better estimates and uncertainty\nquantification than the commonly used simple model mean. These results are\nillustrated using data from the CMIP5 model archive. As examples, for Central\nNorth America our projected mean temperature for 2070--2100 is about 0.8 K\nlower than the simple model mean, while for East Asia it is about 0.5 K higher;\nhowever, in both cases, the widths of the 90% credible intervals are of the\norder 3--6 K, so the uncertainties overwhelm the relatively small differences\nin projected mean temperatures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:50:31 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 08:10:03 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Huang", "Huang", ""], ["Hammerling", "Dorit", ""], ["Li", "Bo", ""], ["Smith", "Richard", ""]]}, {"id": "2001.00299", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Mohammad Ali Mansournia, Paul Gustafson", "title": "Concentration of Benefit index: A threshold-free summary metric for\n  quantifying the capacity of covariates to yield efficient treatment rules", "comments": "This submission was intended to be an update of the previous work\n  (arXiv:1901.05124) and not a new record. As such, the authors decided to\n  withdraw this record and will update arXiv:1901.05124 with an identical copy", "journal-ref": null, "doi": "10.1002/sim.8481", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data on treatment assignment, outcomes, and covariates from a randomized\ntrial are available, a question of interest is to what extent covariates can be\nused to optimize treatment decisions. Statistical hypothesis testing of\ncovariate-by-treatment interaction is ill-suited for this purpose. The\napplication of decision theory results in treatment rules that compare the\nexpected benefit of treatment given the patient's covariates against a\ntreatment threshold. However, determining treatment threshold is often\ncontext-specific, and any given threshold might seem arbitrary when the overall\ncapacity towards predicting treatment benefit is of concern. We propose the\nConcentration of Benefit index (Cb), a threshold-free metric that quantifies\nthe combined performance of covariates towards finding individuals who will\nbenefit the most from treatment. The construct of the proposed index is\ncomparing expected treatment outcomes with and without knowledge of covariates\nwhen one of a two randomly selected patients are to be treated. We show that\nthe resulting index can also be expressed in terms of the integrated efficiency\nof individualized treatment decision over the entire range of treatment\nthresholds. We propose parametric and semi-parametric estimators, the latter\nbeing suitable for out-of-sample validation and correction for optimism. We\nused data from a clinical trial to demonstrate the calculations in a\nstep-by-step fashion, and have provided the R code for implementation\n(https://github.com/msadatsafavi/txBenefit). The proposed index has intuitive\nand theoretically sound interpretation and can be estimated with relative ease\nfor a wide class of regression models. Beyond the conceptual developments,\nvarious aspects of estimation and inference for such a metric need to be\npursued in future research.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:38:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 18:43:46 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Mansournia", "Mohammad Ali", ""], ["Gustafson", "Paul", ""]]}, {"id": "2001.00405", "submitter": "Gianluca Mastrantonio", "authors": "Giovanna Jona Lasinio, Mario Santoro, Gianluca Mastrantonio", "title": "CircSpaceTime: an R package for spatial and spatio-temporal modeling of\n  Circular data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CircSpaceTime is the only R package currently available that implements\nBayesian models for spatial and spatio-temporal interpolation of circular data.\nSuch data are often found in applications where, among the many, wind\ndirections, animal movement directions, and wave directions are involved. To\nanalyze such data we need models for observations at locations s and times t,\nas the so-called geostatistical models, providing structured dependence assumed\nto decay in distance and time. The approach we take begins with Gaussian\nprocesses defined for linear variables over space and time. Then, we use either\nwrapping or projection to obtain processes for circular data. The models are\ncast as hierarchical, with fitting and inference within a Bayesian framework.\nAltogether, this package implements work developed by a series of papers; the\nmost relevant being Jona Lasinio, Gelfand, and Jona Lasinio (2012); Wang and\nGelfand (2014); Mastrantonio, Jona Lasinio, and Gelfand (2016). All procedures\nare written using Rcpp. Estimates are obtained by MCMC allowing parallelized\nmultiple chains run. The implementation of the proposed models is considerably\nimproved on the simple routines adopted in the research papers. As original\nrunning examples, for the spatial and spatio-temporal settings, we use wind\ndirections datasets over central Italy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 11:53:01 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Lasinio", "Giovanna Jona", ""], ["Santoro", "Mario", ""], ["Mastrantonio", "Gianluca", ""]]}, {"id": "2001.00529", "submitter": "Marcel Br\\\"autigam", "authors": "Marcel Br\\\"autigam and Marie Kratz", "title": "The Impact of the Choice of Risk and Dispersion Measure on\n  Procyclicality", "comments": "41 pages, 4 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procyclicality of historical risk measure estimation means that one tends to\nover-estimate future risk when present realized volatility is high and vice\nversa under-estimate future risk when the realized volatility is low. Out of it\ndifferent questions arise, relevant for applications and theory: What are the\nfactors which affect the degree of procyclicality? More specifically, how does\nthe choice of risk measure affect this? How does this behaviour vary with the\nchoice of realized volatility estimator? How do different underlying model\nassumptions influence the pro-cyclical effect? In this paper we consider three\ndifferent well-known risk measures (Value-at-Risk, Expected Shortfall,\nExpectile), the r-th absolute centred sample moment, for any integer $r>0$, as\nrealized volatility estimator (this includes the sample variance and the sample\nmean absolute deviation around the sample mean) and two models (either an iid\nmodel or an augmented GARCH($p$,$q$) model). We show that the strength of\nprocyclicality depends on these three factors, the choice of risk measure, the\nrealized volatility estimator and the model considered. But, no matter the\nchoices, the procyclicality will always be present.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 17:28:31 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Br\u00e4utigam", "Marcel", ""], ["Kratz", "Marie", ""]]}, {"id": "2001.00604", "submitter": "Carlos Sarraute PhD", "authors": "Antonio Vazquez Brust, Tomas Olego, German Rosati, Carolina Lang,\n  Guillermo Bozzoli, Diego Weinberg, Roberto Chuit, Martin A. Minnoni, Carlos\n  Sarraute", "title": "Detecting Areas of Potential High Prevalence of Chagas in Argentina", "comments": "Proceedings of the 2019 World Wide Web Conference. May 13-17, 2019.\n  San Francisco, CA, USA", "journal-ref": null, "doi": "10.1145/3308560.3316485", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A map of potential prevalence of Chagas disease (ChD) with high spatial\ndisaggregation is presented. It aims to detect areas outside the Gran Chaco\necoregion (hyperendemic for the ChD), characterized by high affinity with ChD\nand high health vulnerability.\n  To quantify potential prevalence, we developed several indicators: an\nAffinity Index which quantifies the degree of linkage between endemic areas of\nChD and the rest of the country. We also studied favorable habitability\nconditions for Triatoma infestans, looking for areas where the predominant\nmaterials of floors, roofs and internal ceilings favor the presence of the\ndisease vector.\n  We studied determinants of a more general nature that can be encompassed\nunder the concept of Health Vulnerability Index. These determinants are\nassociated with access to health providers and the socio-economic level of\ndifferent segments of the population.\n  Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which\ncombines the affinity index, the health vulnerability index, and the population\ndensity. We show and discuss the maps obtained. These maps are intended to\nassist public health specialists, decision makers of public health policies and\npublic officials in the development of cost-effective strategies to improve\naccess to diagnosis and treatment of ChD.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 19:34:58 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Brust", "Antonio Vazquez", ""], ["Olego", "Tomas", ""], ["Rosati", "German", ""], ["Lang", "Carolina", ""], ["Bozzoli", "Guillermo", ""], ["Weinberg", "Diego", ""], ["Chuit", "Roberto", ""], ["Minnoni", "Martin A.", ""], ["Sarraute", "Carlos", ""]]}, {"id": "2001.00626", "submitter": "Arun Verma Mr.", "authors": "Arun Verma, Manjesh K. Hanawal, and Nandyala Hemachandra", "title": "Unsupervised Online Feature Selection for Cost-Sensitive Medical\n  Diagnosis", "comments": "Accepted to NetHealth Workshop at COMSNETS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In medical diagnosis, physicians predict the state of a patient by checking\nmeasurements (features) obtained from a sequence of tests, e.g., blood test,\nurine test, followed by invasive tests. As tests are often costly, one would\nlike to obtain only those features (tests) that can establish the presence or\nabsence of the state conclusively. Another aspect of medical diagnosis is that\nwe are often faced with unsupervised prediction tasks as the true state of the\npatients may not be known. Motivated by such medical diagnosis problems, we\nconsider a {\\it Cost-Sensitive Medical Diagnosis} (CSMD) problem, where the\ntrue state of patients is unknown. We formulate the CSMD problem as a feature\nselection problem where each test gives a feature that can be used in a\nprediction model. Our objective is to learn strategies for selecting the\nfeatures that give the best trade-off between accuracy and costs. We exploit\nthe `Weak Dominance' property of problem to develop online algorithms that\nidentify a set of features which provides an `optimal' trade-off between cost\nand accuracy of prediction without requiring to know the true state of the\nmedical condition. Our empirical results validate the performance of our\nalgorithms on problem instances generated from real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 14:15:29 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Verma", "Arun", ""], ["Hanawal", "Manjesh K.", ""], ["Hemachandra", "Nandyala", ""]]}, {"id": "2001.00670", "submitter": "Joshua Paik", "authors": "Joshua Paik and Igor Rivin", "title": "Data Analysis of the Responses to Professor Abigail Thompson's Statement\n  on Mandatory Diversity Statements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An opinion piece by Abigail Thompson in the Notices of the American\nMathematical Society has engendered a lot of discussion, including three open\nletters with over 1400 signatures. We analyze the professional profiles of\nsignatories of these three letters, and, in particular, their citation records.\nWe find that when restricting to R1 math professors, the means of their\ncitations and citations per year are ordered $\\mu(A) < \\mu(B) < \\mu(C)$. The\nsignificance of these findings are validated using a one-sided permutation\ntest.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 23:51:20 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 22:17:15 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Paik", "Joshua", ""], ["Rivin", "Igor", ""]]}, {"id": "2001.00811", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis", "title": "Hydrological time series forecasting using simple combinations: Big data\n  testing and investigations on one-year ahead river flow predictability", "comments": null, "journal-ref": "Journal of Hydrology 590 (2020) 125205", "doi": "10.1016/j.jhydrol.2020.125205", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Delivering useful hydrological forecasts is critical for urban and\nagricultural water management, hydropower generation, flood protection and\nmanagement, drought mitigation and alleviation, and river basin planning and\nmanagement, among others. In this work, we present and appraise a new simple\nand flexible methodology for hydrological time series forecasting. This\nmethodology relies on (a) at least two individual forecasting methods and (b)\nthe median combiner of forecasts. The appraisal is made by using a big dataset\nconsisted of 90-year-long mean annual river flow time series from approximately\n600 stations. Covering large parts of North America and Europe, these stations\nrepresent various climate and catchment characteristics, and thus can\ncollectively support benchmarking. Five individual forecasting methods and 26\nvariants of the introduced methodology are applied to each time series. The\napplication is made in one-step ahead forecasting mode. The individual methods\nare the last-observation benchmark, simple exponential smoothing, complex\nexponential smoothing, automatic autoregressive fractionally integrated moving\naverage (ARFIMA) and Facebook's Prophet, while the 26 variants are defined by\nall the possible combinations (per two, three, four or five) of the five\nafore-mentioned methods. The new methodology is identified as well-performing\nin the long run, especially when more than two individual forecasting methods\nare combined within its framework. Moreover, the possibility of case-informed\nintegrations of diverse hydrological forecasting methods within systematic\nframeworks is algorithmically investigated and discussed. The related\ninvestigations encompass linear regression analyses, which aim at finding\ninterpretable relationships between the values of a representative forecasting\nperformance metric and the values of selected river flow statistics...\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:45:43 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 16:58:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""]]}, {"id": "2001.00878", "submitter": "Christopher Franck", "authors": "Christopher T. Franck Christopher E. Wilson", "title": "Predicting competitions by pairing conditional logistic regression and\n  subjective Bayes: An Academy Awards case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the outcome of elections, sporting events, entertainment awards,\nand other competitions has long captured the human imagination. Such prediction\nis growing in sophistication in these areas, especially in the rapidly growing\nfield of data-driven journalism intended for a general audience as the\navailability of historical information rapidly balloons. Providing statistical\nmethodology to probabilistically predict competition outcomes faces two main\nchallenges. First, a suitably general modeling approach is necessary to assign\nprobabilities to competitors. Second, the modeling framework must be able to\naccommodate expert opinion, which is usually available but difficult to fully\nencapsulate in typical data sets. We overcome these challenges with a combined\nconditional logistic regression/subjective Bayes approach. To illustrate the\nmethod, we re-analyze data from a recent Time.com piece in which the authors\nattempted to predict the 2019 Best Picture Academy Award winner using standard\nlogistic regression. Towards engaging and educating a broad readership, we\ndiscuss strategies to deploy the proposed method via an online application.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 16:23:56 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Wilson", "Christopher T. Franck Christopher E.", ""]]}, {"id": "2001.00889", "submitter": "Andre Assumpcao", "authors": "Andre Assumpcao and Julio Trecenti", "title": "Judicial Favoritism of Politicians: Evidence from Small Claims Court", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple studies have documented racial, gender, political ideology, or\nethnical biases in comparative judicial systems. Supplementing this literature,\nwe investigate whether judges rule cases differently when one of the litigants\nis a politician. We suggest a theory of power collusion, according to which\njudges might use rulings to buy cooperation or threaten members of the other\nbranches of government. We test this theory using a sample of small claims\ncases in the state of S\\~ao Paulo, Brazil, where no collusion should exist. The\nresults show a negative bias of 3.7 percentage points against litigant\npoliticians, indicating that judges punish, rather than favor, politicians in\ncourt. This punishment in low-salience cases serves as a warning sign for\npoliticians not to cross the judiciary when exercising checks and balances,\nsuggesting yet another barrier to judicial independence in development\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 16:57:27 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 14:04:43 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Assumpcao", "Andre", ""], ["Trecenti", "Julio", ""]]}, {"id": "2001.00996", "submitter": "Athanasios Rakitzis", "authors": "P. H. Tran and A. C. Rakitzis and H. D. Nguyen and Q. T. Nguyen and K.\n  P. Tran and C. Heuchenne", "title": "Monitoring the Multivariate Coefficient of Variation using Run Rules\n  Type Control Charts", "comments": "27 pages, 4 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In practice, there are processes where the in-control mean and standard\ndeviation of a quality characteristic is not stable. In such cases, the\ncoefficient of variation (CV) is a more appropriate measure for assessing\nprocess stability. In this paper, we consider the statistical design of Run\nRules based control charts for monitoring the CV of multivariate data. A Markov\nchain approach is used to evaluate the statistical performance of the proposed\ncharts. The computational results show that the Run Rules based charts\noutperform significantly the standard Shewhart control chart. Moreover, by\nchoosing an appropriate scheme, the Run Rules based charts perform better than\nthe Rum Sum control chart for monitoring the multivariate CV. An example in a\nspring manufacturing process is given to illustrate the implementation of the\nproposed charts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 21:51:43 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 13:25:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "P. H.", ""], ["Rakitzis", "A. C.", ""], ["Nguyen", "H. D.", ""], ["Nguyen", "Q. T.", ""], ["Tran", "K. P.", ""], ["Heuchenne", "C.", ""]]}, {"id": "2001.01040", "submitter": "Sourish Das", "authors": "Bharathi Manjula .K and Sourish Das and Jehadeesan .R", "title": "Causal Impact of Web Browsing and Other Factors on Research Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the causal impact of the web-search activity on the\nresearch publication. We considered observational prospective study design,\nwhere research activity of 267 scientists is being studied. We considered the\nPoisson and negative binomial regression model for our analysis. Based on the\nAkaike's Model selection criterion, we found the negative binomial regression\nperforms better than the Poisson regression. Detailed analysis indicates that\nthe higher web-search activity of 2016 related to the sci-indexed website has a\npositive significant impact on the research publication of 2017. We observed\nthat unique collaborations of 2016 and web-search activity of 2016 have a\nnon-linear but significant positive impact on the research publication of 2017.\nWhat-if analysis indicates the high web browsing activity leads to more number\nof the publication. However, interestingly we see a scientist with low web\nactivity can be as productive as others if her/his maximum hits are the\nsci-indexed journal. That is if the scientist uses web browsing only for\nresearch-related activity, then she/he can be equally productive even if\nher/his web activity is lower than fellow scientists.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 05:43:44 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["K", "Bharathi Manjula .", ""], ["Das", "Sourish", ""], ["R", "Jehadeesan .", ""]]}, {"id": "2001.01056", "submitter": "Sayan Chakraborty", "authors": "Sayan Chakraborty, Smit Shah, Kiumars Soltani, Anna Swigart", "title": "Root Cause Detection Among Anomalous Time Series Using Temporal State\n  Alignment", "comments": "6 pages, 7 figures, 2019 18th IEEE International Conference on\n  Machine Learning and Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent increase in the scale and complexity of software systems has\nintroduced new challenges to the time series monitoring and anomaly detection\nprocess. A major drawback of existing anomaly detection methods is that they\nlack contextual information to help stakeholders identify the cause of\nanomalies. This problem, known as root cause detection, is particularly\nchallenging to undertake in today's complex distributed software systems since\nthe metrics under consideration generally have multiple internal and external\ndependencies. Significant manual analysis and strong domain expertise is\nrequired to isolate the correct cause of the problem. In this paper, we propose\na method that isolates the root cause of an anomaly by analyzing the patterns\nin time series fluctuations. Our method considers the time series as\nobservations from an underlying process passing through a sequence of\ndiscretized hidden states. The idea is to track the propagation of the effect\nwhen a given problem causes unaligned but homogeneous shifts of the underlying\nstates. We evaluate our approach by finding the root cause of anomalies in\nZillows clickstream data by identifying causal patterns among a set of observed\nfluctuations.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 08:31:34 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chakraborty", "Sayan", ""], ["Shah", "Smit", ""], ["Soltani", "Kiumars", ""], ["Swigart", "Anna", ""]]}, {"id": "2001.01116", "submitter": "Zijian Zeng", "authors": "Zijian Zeng, Meng Li", "title": "Bayesian Median Autoregression for Robust Time Series Forecasting", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2020.11.002", "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian median autoregressive (BayesMAR) model for time series\nforecasting. The proposed method utilizes time-varying quantile regression at\nthe median, favorably inheriting the robustness of median regression in\ncontrast to the widely used mean-based methods. Motivated by a working Laplace\nlikelihood approach in Bayesian quantile regression, BayesMAR adopts a\nparametric model bearing the same structure as autoregressive models by\naltering the Gaussian error to Laplace, leading to a simple, robust, and\ninterpretable modeling strategy for time series forecasting. We estimate model\nparameters by Markov chain Monte Carlo. Bayesian model averaging is used to\naccount for model uncertainty, including the uncertainty in the autoregressive\norder, in addition to a Bayesian model selection approach. The proposed methods\nare illustrated using simulations and real data applications. An application to\nU.S. macroeconomic data forecasting shows that BayesMAR leads to favorable and\noften superior predictive performance compared to the selected mean-based\nalternatives under various loss functions that encompass both point and\nprobabilistic forecasts. The proposed methods are generic and can be used to\ncomplement a rich class of methods that build on autoregressive models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 19:44:33 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 15:07:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zeng", "Zijian", ""], ["Li", "Meng", ""]]}, {"id": "2001.01245", "submitter": "Stijn van Weezel", "authors": "Stijn van Weezel", "title": "Decline of war or end of positive check? Analysis of change in war size\n  distribution between 1816-2007", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines whether there has been a decline in the risk of death by\nbattle during wars, testing the 'long peace' hypothesis. The analysis relies on\nthe Expanded War Dataset (Gleditsch, 2004) covering intra- and inter-state wars\nbetween 1816-2007. Using untransformed data on war sizes, the estimates do not\nprovide empirical evidence for a decline in war over time. However, normalising\nthe data for global human population does illustrate a likely decline in war\nfrom 1947 onward. The results indicate that despite strong population growth\nwars have not become more severe.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 14:26:02 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 14:41:16 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["van Weezel", "Stijn", ""]]}, {"id": "2001.01372", "submitter": "J. Bryce Kalmbach", "authors": "J. Bryce Kalmbach, Jacob T. VanderPlas, Andrew J. Connolly", "title": "Applying Information Theory to Design Optimal Filters for Photometric\n  Redshifts", "comments": "29 pages, 17 figures, accepted to ApJ", "journal-ref": null, "doi": "10.3847/1538-4357/ab684f", "report-no": null, "categories": "astro-ph.IM cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply ideas from information theory to create a method for\nthe design of optimal filters for photometric redshift estimation. We show the\nmethod applied to a series of simple example filters in order to motivate an\nintuition for how photometric redshift estimators respond to the properties of\nphotometric passbands. We then design a realistic set of six filters covering\noptical wavelengths that optimize photometric redshifts for $z <= 2.3$ and $i <\n25.3$. We create a simulated catalog for these optimal filters and use our\nfilters with a photometric redshift estimation code to show that we can improve\nthe standard deviation of the photometric redshift error by 7.1% overall and\nimprove outliers 9.9% over the standard filters proposed for the Large Synoptic\nSurvey Telescope (LSST). We compare features of our optimal filters to LSST and\nfind that the LSST filters incorporate key features for optimal photometric\nredshift estimation. Finally, we describe how information theory can be applied\nto a range of optimization problems in astronomy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 02:56:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kalmbach", "J. Bryce", ""], ["VanderPlas", "Jacob T.", ""], ["Connolly", "Andrew J.", ""]]}, {"id": "2001.01595", "submitter": "Jean-Baptiste Camps", "authors": "Florian Cafiero and Jean-Baptiste Camps", "title": "Why Moli\\`ere most likely did write his plays", "comments": null, "journal-ref": "Science Advances, 27 Nov 2019: Vol. 5, no. 11, eaax5489", "doi": "10.1126/sciadv.aax5489", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As for Shakespeare, a hard-fought debate has emerged about Moli\\`ere, a\nsupposedly uneducated actor who, according to some, could not have written the\nmasterpieces attributed to him. In the past decades, the century-old thesis\naccording to which Pierre Corneille would be their actual author has become\npopular, mostly because of new works in computational linguistics. These\nresults are reassessed here through state-of-the-art attribution methods. We\nstudy a corpus of comedies in verse by major authors of Moli\\`ere and\nCorneille's time. Analysis of lexicon, rhymes, word forms, affixes,\nmorphosyntactic sequences, and function words do not give any clue that another\nauthor among the major playwrights of the time would have written the plays\nsigned under the name Moli\\`ere.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 15:23:11 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Cafiero", "Florian", ""], ["Camps", "Jean-Baptiste", ""]]}, {"id": "2001.01676", "submitter": "Beniamino Hadj-Amar", "authors": "Beniamino Hadj-Amar, B\\\"arbel Finkenst\\\"adt, Mark Fiecas, and Robert\n  Huckstepp", "title": "Identifying the Recurrence of Sleep Apnea Using a Harmonic Hidden Markov\n  Model", "comments": null, "journal-ref": "Annals of Applied Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model time-varying periodic and oscillatory processes by means\nof a hidden Markov model where the states are defined through the spectral\nproperties of a periodic regime. The number of states is unknown along with the\nrelevant periodicities, the role and number of which may vary across states. We\naddress this inference problem by a Bayesian nonparametric hidden Markov model\nassuming a sticky hierarchical Dirichlet process for the switching dynamics\nbetween different states while the periodicities characterizing each state are\nexplored by means of a trans-dimensional Markov chain Monte Carlo sampling\nstep. We develop the full Bayesian inference algorithm and illustrate the use\nof our proposed methodology for different simulation studies as well as an\napplication related to respiratory research which focuses on the detection of\napnea instances in human breathing traces.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 17:34:23 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:37:43 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 08:17:07 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 13:01:58 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hadj-Amar", "Beniamino", ""], ["Finkenst\u00e4dt", "B\u00e4rbel", ""], ["Fiecas", "Mark", ""], ["Huckstepp", "Robert", ""]]}, {"id": "2001.01702", "submitter": "Cyrille Mascart", "authors": "Cyrille Mascart, Alexandre Muzy, Patricia Reynaud-bouret", "title": "Efficient Simulation of Sparse Graphs of Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We derive new discrete event simulation algorithms for marked time point\nprocesses. The main idea is to couple a special structure, namely the\nassociated local independence graph, as defined by Didelez arXiv:0710.5874,\nwith the activity tracking algorithm [muzy, 2019] for achieving high\nperformance asynchronous simulations. With respect to classical algorithm, this\nallows reducing drastically the computational complexity, especially when the\ngraph is sparse.\n  [muzy, 2019] A. Muzy. 2019. Exploiting activity for the modeling and\nsimulation of dynamics and learning processes in hierarchical (neurocognitive)\nsystems. (Submitted to) Magazine of Computing in Science & Engineering (2019)\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:31:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 11:20:20 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 11:51:33 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 07:52:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mascart", "Cyrille", ""], ["Muzy", "Alexandre", ""], ["Reynaud-bouret", "Patricia", ""]]}, {"id": "2001.01797", "submitter": "Soheil Sadeghi Eshkevari", "authors": "Soheil Sadeghi Eshkevari, Thomas J. Matarazzo, Shamim N. Pakzad", "title": "Bridge Modal Identification using Acceleration Measurements within\n  Moving Vehicles", "comments": "submitted to Mechanical Systems and Signal Processing", "journal-ref": "Mechanical Systems and Signal Processing 141 (2020) 106733", "doi": "10.1016/j.ymssp.2020.106733", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles crossing bridge structures respond dynamically to the bridge's\nvibrations. An acceleration signal collected within a moving vehicle contains a\ntrace of the bridge's structural response, but also includes other sources such\nas the vehicle suspension system and surface roughness-induced vibrations. This\npaper introduces two methods for the bridge system identification using data\ncollected by a network of moving vehicles. The contributions of the vehicle\nsuspension system are removed by deconvolving the vehicle response in frequency\ndomain. The first approach utilizes the vehicle transfer function, and the\nsecond uses EEMD method. Next, roughness-induced vibrations are extracted using\nsecond-order blind identification (SOBI) method. After these two processes the\nresulting signal is equivalent to the readings of mobile sensors that scan the\nbridge's dynamic response. Structural modal identification using mobile sensor\ndata has been recently introduced as STRIDEX algorithm. The processed mobile\nsensor data is analyzed using STRIDEX to identify the modal properties of the\nbridge. The performance of the methods is validated on numerical case studies\nof a long single-span bridge monitored via a network of moving vehicles. The\nanalyses consider three road surface roughness patterns. Results show that the\nproposed algorithms are successful in extracting pure bridge vibrations, and\nproduce accurate and comprehensive modal properties of the bridge. The study\nshows that the proposed transfer function method can efficiently deconvolve the\nlinear dynamics of a moving vehicle. EEMD method is able to extract vehicle\ndynamic response without a-priori information about the vehicle. This study is\nthe first proposed methodology for complete bridge modal identification,\nincluding operational natural frequencies, mode shapes and damping ratios using\n\\textit{moving vehicles sensor data}.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 22:21:11 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Eshkevari", "Soheil Sadeghi", ""], ["Matarazzo", "Thomas J.", ""], ["Pakzad", "Shamim N.", ""]]}, {"id": "2001.01895", "submitter": "Philip Andrew Collender", "authors": "Philip A. Collender, Zhiyue Tom Hu, Charles Li, Qu Cheng, Xintong Li,\n  Yue You, Song Liang, Changhong Yang, Justin V. Remais", "title": "Machine-learning classifiers for logographic name matching in public\n  health applications: approaches for incorporating phonetic, visual, and\n  keystroke similarity in large-scale probabilistic record linkage", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate string-matching methods to account for complex variation in\nhighly discriminatory text fields, such as personal names, can enhance\nprobabilistic record linkage. However, discriminating between matching and\nnon-matching strings is challenging for logographic scripts, where similarities\nin pronunciation, appearance, or keystroke sequence are not directly encoded in\nthe string data. We leverage a large Chinese administrative dataset with known\nmatch status to develop logistic regression and Xgboost classifiers integrating\nmeasures of visual, phonetic, and keystroke similarity to enhance\nidentification of potentially-matching name pairs. We evaluate three methods of\nleveraging name similarity scores in large-scale probabilistic record linkage,\nwhich can adapt to varying match prevalence and information in supporting\nfields: (1) setting a threshold score based on predicted quality of\nname-matching across all record pairs; (2) setting a threshold score based on\npredicted discriminatory power of the linkage model; and (3) using empirical\nscore distributions among matches and nonmatches to perform Bayesian adjustment\nof matching probabilities estimated from exact-agreement linkage. In\nexperiments on holdout data, as well as data simulated with varying name error\nrates and supporting fields, a logistic regression classifier incorporated via\nthe Bayesian method demonstrated marked improvements over exact-agreement\nlinkage with respect to discriminatory power, match probability estimation, and\naccuracy, reducing the total number of misclassified record pairs by 21% in\ntest data and up to an average of 93% in simulated datasets. Our results\ndemonstrate the value of incorporating visual, phonetic, and keystroke\nsimilarity for logographic name matching, as well as the promise of our\nBayesian approach to leverage name-matching within large-scale record linkage.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 05:21:21 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Collender", "Philip A.", ""], ["Hu", "Zhiyue Tom", ""], ["Li", "Charles", ""], ["Cheng", "Qu", ""], ["Li", "Xintong", ""], ["You", "Yue", ""], ["Liang", "Song", ""], ["Yang", "Changhong", ""], ["Remais", "Justin V.", ""]]}, {"id": "2001.01924", "submitter": "James Watson", "authors": "Oliver P Watson, Isidro Cortes-Ciriano, James A Watson", "title": "A semi-supervised learning framework for quantitative structure-activity\n  regression modelling", "comments": "17 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning models, also known as quantitative structure-activity\nregression (QSAR) models, are increasingly used in assisting the process of\npreclinical, small molecule drug discovery. The models are trained on data\nconsisting of a finite dimensional representation of molecular structures and\ntheir corresponding target specific activities. These models can then be used\nto predict the activity of previously unmeasured novel compounds. In this work\nwe address two problems related to this approach. The first is to estimate the\nextent to which the quality of the model predictions degrades for compounds\nvery different from the compounds in the training data. The second is to adjust\nfor the screening dependent selection bias inherent in many training data sets.\nIn the most extreme cases, only compounds which pass an activity-dependent\nscreening are reported. By using a semi-supervised learning framework, we show\nthat it is possible to make predictions which take into account the similarity\nof the testing compounds to those in the training data and adjust for the\nreporting selection bias. We illustrate this approach using publicly available\nstructure-activity data on a large set of compounds reported by GlaxoSmithKline\n(the Tres Cantos AntiMalarial Set) to inhibit in vitro P. falciparum growth.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 07:56:49 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Watson", "Oliver P", ""], ["Cortes-Ciriano", "Isidro", ""], ["Watson", "James A", ""]]}, {"id": "2001.01992", "submitter": "Evan Baker", "authors": "Evan Baker, Peter Challenor, Matt Eames", "title": "Future Proofing a Building Design Using History Matching Inspired\n  Level-Set Techniques", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": "10.1111/rssc.12461", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  History Matching is a technique used to calibrate complex computer models,\nthat is, finding the input settings which lead to the simulated output matching\nup with real world observations. Key to this technique is the construction of\nemulators, which provide fast probabilistic predictions of future simulations.\nIn this work, we adapt the History Matching framework to tackle the problem of\nlevel set estimation, that is, finding input settings where the output is below\n(or above) some threshold. The developed methodology is heavily motivated by a\nspecific case study: how can one design a building that will be sufficiently\nprotected against overheating and sufficiently energy efficient, whilst\nconsidering the expected increases in temperature due to climate change? We\nsuccessfully manage to address this - greatly reducing a large initial set of\ncandidate building designs down to a small set of acceptable potential\nbuildings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:06:23 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 08:58:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Baker", "Evan", ""], ["Challenor", "Peter", ""], ["Eames", "Matt", ""]]}, {"id": "2001.01996", "submitter": "Lucy Prior", "authors": "Lucy Prior, Harvey Goldstein, George Leckie", "title": "School value-added models for multivariate academic and non-academic\n  outcomes: A more rounded approach to using student data to inform school\n  accountability", "comments": "Main Manuscript 38 pages, 4 figures, Supplementary Material 17 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Education systems around the world increasingly rely on school value-added\nmodels to hold schools to account. These models typically focus on a limited\nnumber of academic outcomes, failing to recognise the broader range of\nnon-academic student outcomes, attitudes and behaviours to which schools\ncontribute. We explore how the traditional multilevel modelling approach to\nschool value-added models can be extended to simultaneously analyse multiple\nacademic and non-academic outcomes and thereby can potentially provide a more\nrounded approach to using student data to inform school accountability. We\njointly model student attainment, absence and exclusion data for schools in\nEngland. We find different results across the three outcomes, in terms of the\nsize and consistency of school effects, and the importance of adjusting for\nstudent and school characteristics. The results suggest the three outcomes are\ncapturing fundamentally distinct aspects of school performance, recommending\nthe consideration of non-academic outcomes in systems of school accountability.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:16:33 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Prior", "Lucy", ""], ["Goldstein", "Harvey", ""], ["Leckie", "George", ""]]}, {"id": "2001.02036", "submitter": "Junshui Ma", "authors": "Junshui Ma, Daniel J. Holder", "title": "Selection Induced Contrast Estimate (SICE) Effect: An Attempt to\n  Quantify the Impact of Some Patient Selection Criteria in Randomized Clinical\n  Trials", "comments": "Many tricks have been hidden in the complex Inclusion/Exclusion (I/E)\n  criteria of clinical studies. People need to have an in-depth understanding\n  of those I/E criteria to better understand when and how a treatment\n  can/should be used", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Defining the Inclusion/Exclusion (I/E) criteria of a trial is one of the most\nimportant steps during a trial design. Increasingly complex I/E criteria\npotentially create information imbalance and transparency issues between the\npeople who design and run the trials and those who consume the information\nproduced by the trials. In order to better understand and quantify the impact\nof a category of I/E criteria on observed treatment effects, a concept, named\nthe Selection Induced Contrast Estimate (SICE) effect, is introduced and\nformulated in this paper. The SICE effect can exist in controlled clinical\ntrials when treatment affects the correlation between a marker used for\nselection and the response of interest. This effect is demonstrated with both\nsimulations and real clinical trial data. Although the statistical elements\nbehind the SICE effect have been well studied, explicitly formulating and\nstudying this effect can benefit several areas, including better transparency\nin I/E criteria, meta-analysis of multiple clinical trials, treatment effect\ninterpretation in real-world medical practice, etc.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 13:57:27 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ma", "Junshui", ""], ["Holder", "Daniel J.", ""]]}, {"id": "2001.02171", "submitter": "Guadalupe Reyes  Victoria", "authors": "Laura Elizalde Ram\\'irez, Edson Missael Flores Garc\\'ia, Patricia\n  Ram\\'irez Romero, J. Guadalupe Reyes Victoria", "title": "On a smooth scalar field characterizing the risk of exposure to\n  methyl-mercury due to non intentional consumption of shark meet in males of\n  M\\'exico city's metropolitan area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we obtain, through statistical and numerical methods, a\nsmooth function in the variables of the life stage and the concentration, which\nestimates the risk of exposure of methylmercury due to the unintentional\nconsumption of shark in men from Mexico City. With methods of the Theory of\nSingularities and Dynamical Systems, the stability of this risk function was\nshown by analyzing the associated vector field. The region of risk was obtained\nin the variables as mentioned above, and the average risk in the whole region\nwas calculated, which turns out to be a high index. The associated risk surface\nis a Hadamard surface embedded in the three-dimensional space $\\mathbb{R}^3$,\nand the points where the curvature is zero determine critical ages important\nfor the risk in men.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 20:57:06 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ram\u00edrez", "Laura Elizalde", ""], ["Garc\u00eda", "Edson Missael Flores", ""], ["Romero", "Patricia Ram\u00edrez", ""], ["Victoria", "J. Guadalupe Reyes", ""]]}, {"id": "2001.02198", "submitter": "Alexandra Koulouri", "authors": "Alexandra Koulouri, Ville Rimpil\\\"ainen and Nathan D. Smith", "title": "Position Dilution of Precision: a Bayesian point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected position error in many cases is far from feasible to be\nestimated experimentally using real satellite measurements which makes the\nmodel-based position dilution of precision (PDOP) crucial in positioning and\nnavigation applications. In the following text we derive the relationship\nbetween PDOP and position error and we explain that this relationship holds as\nlong as the model for the observation errors represents the true sources of\nerrors.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 20:41:43 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 10:46:12 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 22:20:25 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Koulouri", "Alexandra", ""], ["Rimpil\u00e4inen", "Ville", ""], ["Smith", "Nathan D.", ""]]}, {"id": "2001.02250", "submitter": "Yuan Yan", "authors": "Yuan Yan, Hsin-Cheng Huang, Marc G. Genton", "title": "Vector Autoregressive Models with Spatially Structured Coefficients for\n  Time Series on a Spatial Grid", "comments": null, "journal-ref": "Journal of Agricultural, Biological and Environmental Statistics,\n  2021", "doi": "10.1007/s13253-021-00444-4", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious spatiotemporal model for time series data on a\nspatial grid. Our model is capable of dealing with high-dimensional time series\ndata that may be collected at hundreds of locations and capturing the spatial\nnon-stationarity. In essence, our model is a vector autoregressive model that\nutilizes the spatial structure to achieve parsimony of autoregressive matrices\nat two levels. The first level ensures the sparsity of the autoregressive\nmatrices using a lagged-neighborhood scheme. The second level performs a\nspatial clustering of the non-zero autoregressive coefficients such that nearby\nlocations share similar coefficients. This model is interpretable and can be\nused to identify geographical subregions, within each of which, the time series\nshare similar dynamical behavior with homogeneous autoregressive coefficients.\nThe model parameters are obtained using the penalized maximum likelihood with\nan adaptive fused Lasso penalty. The estimation procedure is easy to implement\nand can be tailored to the need of a modeler. We illustrate the performance of\nthe proposed estimation algorithm in a simulation study. We apply our model to\na wind speed time series dataset generated from a climate model over Saudi\nArabia to illustrate its usefulness. Limitations and possible extensions of our\nmethod are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 19:11:14 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:54:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yan", "Yuan", ""], ["Huang", "Hsin-Cheng", ""], ["Genton", "Marc G.", ""]]}, {"id": "2001.02342", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas, Han Lin Shang, Abdel-Salam G. Abdel-Salam", "title": "Functional linear models for interval-valued data", "comments": "26 pages, 8 figures, to be published at Communications in Statistics\n  - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation of large databases in a specific format is a frequently used\nprocess to make the data easily manageable. Interval-valued data is one of the\ndata types that is generated by such an aggregation process. Using traditional\nmethods to analyze interval-valued data results in loss of information, and\nthus, several interval-valued data models have been proposed to gather reliable\ninformation from such data types. On the other hand, recent technological\ndevelopments have led to high dimensional and complex data in many application\nareas, which may not be analyzed by traditional techniques. Functional data\nanalysis is one of the most commonly used techniques to analyze such complex\ndatasets. While the functional extensions of much traditional statistical\ntechniques are available, the functional form of the interval-valued data has\nnot been studied well. This paper introduces the functional forms of some\nwell-known regression models that take interval-valued data. The proposed\nmethods are based on the function-on-function regression model, where both the\nresponse and predictor/s are functional. Through several Monte Carlo\nsimulations and empirical data analysis, the finite sample performance of the\nproposed methods is evaluated and compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 02:44:12 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""], ["Abdel-Salam", "Abdel-Salam G.", ""]]}, {"id": "2001.02466", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Toni Karvonen, Roland Hostettler, Simo S\\\"arkk\\\"a", "title": "Taylor Moment Expansion for Continuous-Discrete Gaussian Filtering and\n  Smoothing", "comments": "Submitted to IEEE Transactions on Automatic Control. Code is\n  available at (once accepted for publication)\n  https://github.com/zgbkdlm/TME-filter-smoother", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with non-linear Gaussian filtering and smoothing in\ncontinuous-discrete state-space models, where the dynamic model is formulated\nas an It\\^{o} stochastic differential equation (SDE), and the measurements are\nobtained at discrete time instants. We propose novel Taylor moment expansion\n(TME) Gaussian filter and smoother which approximate the moments of the SDE\nwith a temporal Taylor expansion. Differently from classical linearisation or\nIt\\^{o}--Taylor approaches, the Taylor expansion is formed for the moment\nfunctions directly and in time variable, not by using a Taylor expansion on the\nnon-linear functions in the model. We analyse the theoretical properties,\nincluding the positive definiteness of the covariance estimate and stability of\nthe TME Gaussian filter and smoother. By numerical experiments, we demonstrate\nthat the proposed TME Gaussian filter and smoother significantly outperform the\nstate-of-the-art methods in terms of estimation accuracy and numerical\nstability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 11:59:59 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhao", "Zheng", ""], ["Karvonen", "Toni", ""], ["Hostettler", "Roland", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2001.02574", "submitter": "Bashar Ahmad", "authors": "Bashar I Ahmad", "title": "A Survey of Wideband Spectrum Sensing Algorithms for Cognitive Radio\n  Networks and Sub-Nyquist Approaches", "comments": "First draft; this article builds on a chapter by the author in the\n  book titled \"Multimedia over Cognitive Radio Networks\" by CRC Press, FL, USA,\n  2015 (ISBN: 978-1-4822-1485-7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Radio (CR) networks presents a paradigm shift aiming to alleviate\nthe spectrum scarcity problem exasperated by the increasing demand on this\nlimited resource. It promotes dynamic spectrum access, cooperation among\nheterogeneous devices, and spectrum sharing. Spectrum sensing is a key\ncognitive radio functionality, which entails scanning the RF spectrum to unveil\nunderutilised spectral bands for opportunistic use. To achieve higher data\nrates while maintaining high quality of service QoS, effective wideband\nspectrum sensing routines are crucial due to their capability of achieving\nspectral awareness over wide frequency range(s)\\ and efficiently harnessing the\navailable opportunities. However, implementing wideband sensing under stringent\nsize, weight, power and cost requirements (e.g., for portable devices) brings\nformidable design challenges such as addressing potential prohibitively high\ncomplexity and data acquisition rates. This article gives a survey of various\nwideband spectrum sensing approaches outlining their advantages and\nlimitations; special attention is paid to approaches that utilise sub-Nyquist\nsampling techniques. Other aspects of CR such as cooperative sensing and\nperformance requirements are briefly addressed. Comparison between sub-Nyquist\nsensing approaches is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 09:01:41 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ahmad", "Bashar I", ""]]}, {"id": "2001.02734", "submitter": "Nima Safaei", "authors": "Nima Safaei and Chao Zhou", "title": "Gasoline Pricing Policies for Transportation Safety", "comments": "19 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic factors can have substantial effects on transportation crash trends.\nThis study makes a comprehensive examination of the relationship between the\nretail gasoline price (including state and federal fuel taxes) and\ntransportation fatal crashes from 2007 to 2016 in the US. Data on motor\nvehicle, bicycle and pedestrian fatal crashes come from Fatality Analysis\nReporting System (FARS) provided by the National Highway Safety Administration\n(NHTSA) and the gasoline price data is from U.S. Energy Information\nAdministration (EIA). Random effect negative binomial regression models are\nused to estimate the impact of inflation-adjusted gasoline prices on trends of\ntransportation fatal crashes. Initial results combined with results of previous\nstudies showed that gender and transportation mean type (motorcycle,\nnon-motorcycle, bicycle and pedestrian) play prominent roles in interpreting\nthe final model, so by using random effect negative binomial regression, seven\nmodels are developed to evaluate the effects of gasoline price changes on total\npopulation, male, female, motorcyclists, non-motorcyclists, bicyclists and\npedestrians separately. Our findings suggest that increasing the gasoline\nprices will not significantly alter the number of total fatal crashes. However,\nby looking at different vehicle types, it is estimated that one dollar increase\nin adjusted gasoline price is associated with 24.2% increase in the number of\nmotorcycle fatal crashes, 1.9% decrease in the number of non-motorcycle fatal\ncrashes, and 0.7% decrease in the number of pedestrian fatal crashes. Also,\nthere is no noticeable difference between male and female in response to the\ngasoline price changes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:48:45 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Safaei", "Nima", ""], ["Zhou", "Chao", ""]]}, {"id": "2001.03196", "submitter": "Baichuan Mo", "authors": "Baichuan Mo, Zhenliang Ma, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Assignment-based Path Choice Estimation for Metro Systems Using Smart\n  Card Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban rail services are the principal means of public transportation in many\ncities. To understand the crowding patterns and develop efficient operation\nstrategies in the system, obtaining path choices is important. This paper\nproposed an assignment-based path choice estimation framework using automated\nfare collection (AFC) data. The framework captures the inherent correlation of\ncrowding among stations, as well as the interaction between path choice and\nleft behind. The path choice estimation is formulated as an optimization\nproblem. The original problem is intractable because of a non-analytical\nconstraint and a non-linear equation constraint. A solution procedure is\nproposed to decompose the original problem into three tractable sub-problems,\nwhich can be solved efficiently. The model is validated using both synthetic\ndata and real-world AFC data in Hong Kong Mass Transit Railway (MTR) system.\nThe synthetic data test validates the model's effectiveness in estimating path\nchoice parameters, which can outperform the purely simulation-based\noptimization methods in both accuracy and efficiency. The test results using\nactual data show that the estimated path shares are more reasonable than\nsurvey-derived path shares and uniform path shares. Model robustness in terms\nof different initial values and different case study dates are also verified.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:32:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 17:54:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mo", "Baichuan", ""], ["Ma", "Zhenliang", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2001.03231", "submitter": "Mariah Schrum", "authors": "Mariah L. Schrum, Michael Johnson, Muyleng Ghuy, Matthew C. Gombolay", "title": "Four Years in Review: Statistical Practices of Likert Scales in\n  Human-Robot Interaction Studies", "comments": null, "journal-ref": null, "doi": "10.1145/3319502.3378178", "report-no": null, "categories": "cs.HC cs.RO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become more prevalent, the importance of the field of human-robot\ninteraction (HRI) grows accordingly. As such, we should endeavor to employ the\nbest statistical practices. Likert scales are commonly used metrics in HRI to\nmeasure perceptions and attitudes. Due to misinformation or honest mistakes,\nmost HRI researchers do not adopt best practices when analyzing Likert data. We\nconduct a review of psychometric literature to determine the current standard\nfor Likert scale design and analysis. Next, we conduct a survey of four years\nof the International Conference on Human-Robot Interaction (2016 through 2019)\nand report on incorrect statistical practices and design of Likert scales.\nDuring these years, only 3 of the 110 papers applied proper statistical testing\nto correctly-designed Likert scales. Our analysis suggests there are areas for\nmeaningful improvement in the design and testing of Likert scales. Lastly, we\nprovide recommendations to improve the accuracy of conclusions drawn from\nLikert data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 21:45:25 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 01:50:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Schrum", "Mariah L.", ""], ["Johnson", "Michael", ""], ["Ghuy", "Muyleng", ""], ["Gombolay", "Matthew C.", ""]]}, {"id": "2001.03367", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli, Iain Cruickshank, and Kathleen M. Carley", "title": "A Complex Networks Approach to Find Latent Clusters of Terrorist Groups", "comments": "24 pages, 8 figures", "journal-ref": "Appl Netw Sci 4, 59 (2019)", "doi": "10.1007/s41109-019-0184-6", "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given the extreme heterogeneity of actors and groups participating in\nterrorist actions, investigating and assessing their characteristics can be\nimportant to extract relevant information and enhance the knowledge on their\nbehaviors. The present work will seek to achieve this goal via a complex\nnetworks approach. This approach will allow finding latent clusters of similar\nterror groups using information on their operational characteristics.\nSpecifically, using open access data of terrorist attacks occurred worldwide\nfrom 1997 to 2016, we build a multi-partite network that includes terrorist\ngroups and related information on tactics, weapons, targets, active regions. We\npropose a novel algorithm for cluster formation that expands our earlier work\nthat solely used Gower's coefficient of similarity via the application of Von\nNeumann entropy for mode-weighting. This novel approach is compared with our\nprevious Gower-based method and a heuristic clustering technique that only\nfocuses on groups' ideologies. The comparative analysis demonstrates that the\nentropy-based approach tends to reliably reflect the structure of the data that\nnaturally emerges from the baseline Gower-based method. Additionally, it\nprovides interesting results in terms of behavioral and ideological\ncharacteristics of terrorist groups. We furthermore show that the\nideology-based procedure tends to distort or hide existing patterns. Among the\nmain statistical results, our work reveals that groups belonging to opposite\nideologies can share very common behaviors and that Islamist/jihadist groups\nhold peculiar behavioral characteristics with respect to the others.\nLimitations and potential work directions are also discussed, introducing the\nidea of a dynamic entropy-based framework.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 10:08:30 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Campedelli", "Gian Maria", ""], ["Cruickshank", "Iain", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2001.03396", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig, Jordi Cort\\'es Mart\\'inez and Guadalupe G\\'omez\n  Melis", "title": "Decision tool and Sample Size Calculator for Composite Endpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary points:\n  - This article considers the combination of two binary or two time-to-event\nendpoints to form the primary composite endpoint for leading a trial.\n  - It discusses the relative efficiency of choosing a composite endpoint over\none of its components in terms of: the frequencies of observing each component;\nthe relative treatment effect of the tested therapy; and the association\nbetween both components.\n  - We highlight the very important role of the association between components\nin choosing the most efficient endpoint to use as primary.\n  - For better grounded future trials, we recommend trialists to always\nreporting the association between components of the composite endpoint.\n  - Common fallacies to note when using composite endpoints: i) composite\nendpoints always imply higher power; ii) treatment effect on the composite\nendpoint is similar to the average effects of its components; and iii) the\nprobability of observing the primary endpoint increases significantly.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:40:06 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Mart\u00ednez", "Jordi Cort\u00e9s", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2001.03552", "submitter": "Collin A. Politsch", "authors": "Collin A. Politsch, Jessi Cisewski-Kehe, Rupert A. C. Croft, Larry\n  Wasserman", "title": "Trend Filtering -- II. Denoising Astronomical Signals with Varying\n  Degrees of Smoothness", "comments": "Part 2 of 2, Link to Part 1: arXiv:1908.07151; 15 pages, 7 figures", "journal-ref": null, "doi": "10.1093/mnras/staa110", "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.EP astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trend filtering---first introduced into the astronomical literature in Paper\nI of this series---is a state-of-the-art statistical tool for denoising\none-dimensional signals that possess varying degrees of smoothness. In this\nwork, we demonstrate the broad utility of trend filtering to observational\nastronomy by discussing how it can contribute to a variety of spectroscopic and\ntime-domain studies. The observations we discuss are (1) the Lyman-$\\alpha$\nforest of quasar spectra; (2) more general spectroscopy of quasars, galaxies,\nand stars; (3) stellar light curves with planetary transits; (4) eclipsing\nbinary light curves; and (5) supernova light curves. We study the\nLyman-$\\alpha$ forest in the greatest detail---using trend filtering to map the\nlarge-scale structure of the intergalactic medium along quasar-observer lines\nof sight. The remaining studies share broad themes of: (1) estimating\nobservable parameters of light curves and spectra; and (2) constructing\nobservational spectral/light-curve templates. We also briefly discuss the\nutility of trend filtering as a tool for one-dimensional data reduction and\ncompression.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:30:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Politsch", "Collin A.", ""], ["Cisewski-Kehe", "Jessi", ""], ["Croft", "Rupert A. C.", ""], ["Wasserman", "Larry", ""]]}, {"id": "2001.03798", "submitter": "Rui Zhu", "authors": "Rui Zhu, Subhashis Ghosal", "title": "Bayesian Semi-supervised learning under nonparanormality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is a classification method which makes use of both\nlabeled data and unlabeled data for training. In this paper, we propose a\nsemi-supervised learning algorithm using a Bayesian semi-supervised model. We\nmake a general assumption that the observations will follow two multivariate\nnormal distributions depending on their true labels after the same unknown\ntransformation. We use B-splines to put a prior on the transformation function\nfor each component. To use unlabeled data in a semi-supervised setting, we\nassume the labels are missing at random. The posterior distributions can then\nbe described using our assumptions, which we compute by the Gibbs sampling\ntechnique. The proposed method is then compared with several other available\nmethods through an extensive simulation study. Finally we apply the proposed\nmethod in real data contexts for diagnosing breast cancer and classify radar\nreturns. We conclude that the proposed method has better prediction accuracy in\na wide variety of cases.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 21:31:25 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhu", "Rui", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2001.03965", "submitter": "Adam Mahdi", "authors": "George Qian and Adam Mahdi", "title": "Sensitivity analysis methods in the biomedical sciences", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis is an important part of a mathematical modeller's\ntoolbox for model analysis. In this review paper, we describe the most\nfrequently used sensitivity techniques, discussing their advantages and\nlimitations, before applying each method to a simple model. Also included is a\nsummary of current software packages, as well as a modeller's guide for\ncarrying out sensitivity analyses. Finally, we apply the popular Morris and\nSobol methods to two models with biomedical applications, with the intention of\nproviding a deeper understanding behind both the principles of these methods\nand the presentation of their results.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 17:44:16 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Qian", "George", ""], ["Mahdi", "Adam", ""]]}, {"id": "2001.03984", "submitter": "Kjartan Kloster Osmundsen", "authors": "Kjartan Kloster Osmundsen, Tore Selland Kleppe, Roman Liesenfeld, Atle\n  Oglend", "title": "Estimating the Competitive Storage Model with Stochastic Trends in\n  Commodity Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a state-space model (SSM) for commodity prices that combines the\ncompetitive storage model with a stochastic trend. This approach fits into the\neconomic rationality of storage decisions, and adds to previous deterministic\ntrend specifications of the storage model. Parameters are estimated using a\nparticle Markov chain Monte Carlo procedure. Empirical application to four\ncommodity markets shows that the stochastic trend SSM is favored over\ndeterministic trend specifications. The stochastic trend SSM identifies\nstructural parameters that differ from those for deterministic trend\nspecifications. In particular, the estimated price elasticities of demand are\nsignificantly larger under the stochastic trend SSM.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 19:33:49 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Osmundsen", "Kjartan Kloster", ""], ["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""], ["Oglend", "Atle", ""]]}, {"id": "2001.03998", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto", "title": "Towards causality-aware predictions in static anticausal machine\n  learning tasks: the linear structural causal model case", "comments": "Causal Discovery & Causality-Inspired Machine Learning Workshop at\n  Neural Information Processing Systems 2020. (Contains some common material\n  with arXiv:2011.04128.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a counterfactual approach to train ``causality-aware\" predictive\nmodels that are able to leverage causal information in static anticausal\nmachine learning tasks (i.e., prediction tasks where the outcome influences the\nfeatures). In applications plagued by confounding, the approach can be used to\ngenerate predictions that are free from the influence of observed confounders.\nIn applications involving observed mediators, the approach can be used to\ngenerate predictions that only capture the direct or the indirect causal\ninfluences. Mechanistically, we train supervised learners on (counterfactually)\nsimulated features which retain only the associations generated by the causal\nrelations of interest. We focus on linear models, where analytical results\nconnecting covariances, causal effects, and prediction mean squared errors are\nreadily available. Quite importantly, we show that our approach does not\nrequire knowledge of the full causal graph. It suffices to know which variables\nrepresent potential confounders and/or mediators. We discuss the stability of\nthe method with respect to dataset shifts generated by selection biases and\nvalidate the approach using synthetic data experiments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 20:49:07 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 15:33:33 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Neto", "Elias Chaibub", ""]]}, {"id": "2001.04040", "submitter": "Wei Li", "authors": "Wei Li, Chunchen Liu, Zhi Geng and John Murray", "title": "Causal Mediation Analysis with Multiple Treatments and Latent\n  Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is used to evaluate direct and indirect causal\neffects of a treatment on an outcome of interest through an intermediate\nvariable or a mediator.It is difficult to identify the direct and indirect\ncausal effects because the mediator cannot be randomly assigned in many real\napplications. In this article, we consider a causal model including latent\nconfounders between the mediator and the outcome. We present sufficient\nconditions for identifying the direct and indirect effects and propose an\napproach for estimating them. The performance of the proposed approach is\nevaluated by simulation studies. Finally, we apply the approach to a data set\nof the customer loyalty survey by a telecom company.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 02:53:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Li", "Wei", ""], ["Liu", "Chunchen", ""], ["Geng", "Zhi", ""], ["Murray", "John", ""]]}, {"id": "2001.04045", "submitter": "Aerin Kim", "authors": "Rohit Pandey, Yingnong Dang, Gil Lapid Shafriri, Murali Chintalapati,\n  Aerin Kim", "title": "Breaking hypothesis testing for failure rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the utility of point processes and failure rates and the most\ncommon point process for modeling failure rates, the Poisson point process.\nNext, we describe the uniformly most powerful test for comparing the rates of\ntwo Poisson point processes for a one-sided test (henceforth referred to as the\n\"rate test\"). A common argument against using this test is that real world data\nrarely follows the Poisson point process. We thus investigate what happens when\nthe distributional assumptions of tests like these are violated and the test\nstill applied. We find a non-pathological example (using the rate test on a\nCompound Poisson distribution with Binomial compounding) where violating the\ndistributional assumptions of the rate test make it perform better (lower error\nrates). We also find that if we replace the distribution of the test statistic\nunder the null hypothesis with any other arbitrary distribution, the\nperformance of the test (described in terms of the false negative rate to false\npositive rate trade-off) remains exactly the same. Next, we compare the\nperformance of the rate test to a version of the Wald test customized to the\nNegative Binomial point process and find it to perform very similarly while\nbeing much more general and versatile. Finally, we discuss the applications to\nMicrosoft Azure. The code for all experiments performed is open source and\nlinked in the introduction.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 03:17:30 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Pandey", "Rohit", ""], ["Dang", "Yingnong", ""], ["Shafriri", "Gil Lapid", ""], ["Chintalapati", "Murali", ""], ["Kim", "Aerin", ""]]}, {"id": "2001.04226", "submitter": "John T. Whelan", "authors": "John T. Whelan and Adam Wodon", "title": "Prediction and Evaluation in College Hockey using the\n  Bradley-Terry-Zermelo Model", "comments": "21 pages, 8 figures, proceeding for the 2019 UP-STAT conference;\n  accepted for publication in Mathematics for Applications; scripts available\n  at https://gitlab.com/jtwsma/bradley-terry", "journal-ref": "Mathematics for Applications 8, 131 (2019)", "doi": "10.13164/ma.2019.09", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the application of the Bradley-Terry model to NCAA Division I\nMen's Ice Hockey. A Bayesian construction gives a joint posterior probability\ndistribution for the log-strength parameters, given a set of game results and a\nchoice of prior distribution. For several suitable choices of prior, it is\nstraightforward to find the maximum a posteriori point (MAP) and a Hessian\nmatrix, allowing a Gaussian approximation to be constructed. Posterior\npredictive probabilities can be estimated by 1) setting the log-strengths to\ntheir MAP values, 2) using the Gaussian approximation for analytical or Monte\nCarlo integration, or 3) applying importance sampling to re-weight the results\nof a Monte Carlo simulation. We define a method to evaluate any models which\ngenerate predicted probabilities for future outcomes, using the Bayes factor\ngiven the actual outcomes, and apply it to NCAA tournament results. Finally, we\ndescribe an on-line tool which currently estimates probabilities of future\nresults using MAP evaluation and describe how it can be refined using the\nGaussian approximation or importance sampling.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:21:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Whelan", "John T.", ""], ["Wodon", "Adam", ""]]}, {"id": "2001.04230", "submitter": "Chon Lok Lei", "authors": "Chon Lok Lei, Sanmitra Ghosh, Dominic G. Whittaker, Yasser\n  Aboelkassem, Kylie A. Beattie, Chris D. Cantwell, Tammo Delhaas, Charles\n  Houston, Gustavo Montes Novaes, Alexander V. Panfilov, Pras Pathmanathan,\n  Marina Riabiz, Rodrigo Weber dos Santos, John Walmsley, Keith Worden, Gary R.\n  Mirams and Richard D. Wilkinson", "title": "Considering discrepancy when calibrating a mechanistic electrophysiology\n  model", "comments": "This version is published in Philosophical Transactions of the Royal\n  Society A; Updated in response to reviewer comments, including: added details\n  to the introduction, fixed mathematical notations for clarity, and moved the\n  original Table 3 to the supplement to avoid confusion", "journal-ref": "Phil. Trans. R. Soc. A. 378 (2020): 20190349", "doi": "10.1098/rsta.2019.0349", "report-no": null, "categories": "stat.CO q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification (UQ) is a vital step in using mathematical models\nand simulations to take decisions. The field of cardiac simulation has begun to\nexplore and adopt UQ methods to characterise uncertainty in model inputs and\nhow that propagates through to outputs or predictions. In this perspective\npiece we draw attention to an important and under-addressed source of\nuncertainty in our predictions -- that of uncertainty in the model structure or\nthe equations themselves. The difference between imperfect models and reality\nis termed model discrepancy, and we are often uncertain as to the size and\nconsequences of this discrepancy. Here we provide two examples of the\nconsequences of discrepancy when calibrating models at the ion channel and\naction potential scales. Furthermore, we attempt to account for this\ndiscrepancy when calibrating and validating an ion channel model using\ndifferent methods, based on modelling the discrepancy using Gaussian processes\n(GPs) and autoregressive-moving-average (ARMA) models, then highlight the\nadvantages and shortcomings of each approach. Finally, suggestions and lines of\nenquiry for future work are provided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:26:13 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 13:50:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lei", "Chon Lok", ""], ["Ghosh", "Sanmitra", ""], ["Whittaker", "Dominic G.", ""], ["Aboelkassem", "Yasser", ""], ["Beattie", "Kylie A.", ""], ["Cantwell", "Chris D.", ""], ["Delhaas", "Tammo", ""], ["Houston", "Charles", ""], ["Novaes", "Gustavo Montes", ""], ["Panfilov", "Alexander V.", ""], ["Pathmanathan", "Pras", ""], ["Riabiz", "Marina", ""], ["Santos", "Rodrigo Weber dos", ""], ["Walmsley", "John", ""], ["Worden", "Keith", ""], ["Mirams", "Gary R.", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "2001.04507", "submitter": "L\\'eo Belzile", "authors": "L\\'eo R. Belzile, Anthony C. Davison, Holger Rootz\\'en, Dmitrii Zholud", "title": "Human mortality at extreme age", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a combination of extreme value theory, survival analysis and\ncomputer-intensive methods to analyze the mortality of Italian and French\nsemi-supercentenarians for whom there are validated records. After accounting\nfor the effects of the sampling frame, there appears to be a constant rate of\nmortality beyond age 108 years and no difference between countries and cohorts.\nThese findings are consistent with previous work based on the International\nDatabase on Longevity and suggest that any physical upper bound for humans is\nso large that it is unlikely to be approached. There is no evidence of\ndifferences in survival between women and men after age 108 in the Italian data\nand the International Database on Longevity; however survival is lower for men\nin the French data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 19:25:51 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:33:04 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Belzile", "L\u00e9o R.", ""], ["Davison", "Anthony C.", ""], ["Rootz\u00e9n", "Holger", ""], ["Zholud", "Dmitrii", ""]]}, {"id": "2001.04562", "submitter": "Francisco Reyne-Pugh", "authors": "Francisco Reyne-Pugh, Jos\\'e Pulgar, Alex Godoy-Fa\\'undez, Mario\n  Alvarado-Rybak and Crist\\'obal Galb\\'an-Malag\\'on", "title": "Assessing the Impact of the Physical Environment on Comfort and Job\n  Satisfaction in Offices", "comments": "22 pages, 6 Figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a model that allows to analyze the physical parameters\nthat determine the degree of environmental comfort of employees in offices.\nParameters such as air quality, noise, thermal environment, and lighting are\nconsidered. This model was developed through the use of partial least squares\nstructural equation models (PLS-SEM). Formative indicators (which cause the\nconstruct) and reflective indicators (caused or affected by the construct) were\nused, following the methodology proposed by Hair et al. (2014). The model was\nestimated using data obtained in surveys conducted in aeronautical control\noffices in Chile (DASADGAC). The model allows to evaluate the influence that\nenvironmental comfort has on people's job satisfaction. The results indicate\nthat the environmental parameters used significantly influence environmental\ncomfort, explaining 70.2% of its variance. In addition, it was obtained that\nthe influence of noise on environmental comfort proved to be greater than that\nof the rest of the environmental parameters studied, followed by air quality.\nOn the other hand, it was empirically proven that environmental comfort has a\nsignificant influence on job satisfaction, where the environmental parameters\nused explain the variance of job satisfaction by 28.9%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 23:35:31 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Reyne-Pugh", "Francisco", ""], ["Pulgar", "Jos\u00e9", ""], ["Godoy-Fa\u00fandez", "Alex", ""], ["Alvarado-Rybak", "Mario", ""], ["Galb\u00e1n-Malag\u00f3n", "Crist\u00f3bal", ""]]}, {"id": "2001.04734", "submitter": "Isuru Hewapathirana", "authors": "Isuru Udayangani Hewapathirana", "title": "Change Detection in Dynamic Attributed Networks", "comments": "39 pages, 10 figures", "journal-ref": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge\n  Discovery 9.3 (2019): e1286", "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A network provides powerful means of representing complex relationships\nbetween entities by abstracting entities as vertices, and relationships as\nedges connecting vertices in a graph. Beyond the presence or absence of\nrelationships, a network may contain additional information that can be\nattributed to the entities and their relationships. Attaching these additional\nattribute data to the corresponding vertices and edges yields an attributed\ngraph. Moreover, in the majority of real-world applications, such as online\nsocial networks, financial networks and transactional networks, relationships\nbetween entities evolve over time.\n  Change detection in dynamic attributed networks is an important problem in\nmany areas, such as fraud detection, cyber intrusion detection and health care\nmonitoring. It is a challenging problem because it involves a time sequence of\nattributed graphs, each of which is usually very large and can contain many\nattributes attached to the vertices and edges, resulting in a complex, high\ndimensional mathematical object.\n  In this survey we provide an overview of some of the existing change\ndetection methods that utilize attribute information. We categorize these\nmethods based on the levels of structure in the graph that are exploited to\ndetect changes. These levels are vertices, edges, subgraphs, communities and\nthe overall graph. We focus our attention on the strengths and weaknesses of\nthese methods, including performance and scalability. Finally we discuss some\npublicly available dynamic network datasets and give a brief overview of\nsimulation models to generate synthetic dynamic attributed networks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:07:37 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Hewapathirana", "Isuru Udayangani", ""]]}, {"id": "2001.04752", "submitter": "Subhrakanti Dey", "authors": "Subhrakanti Dey", "title": "Asymptotic Performance Analysis of Non-Bayesian Quickest Change\n  Detection with an Energy Harvesting Sensor", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a non-Bayesian sequential change detection based\non the Cumulative Sum (CUSUM) algorithm employed by an energy harvesting sensor\nwhere the distributions before and after the change are assumed to be known. In\na slotted discrete-time model, the sensor, exclusively powered by randomly\navailable harvested energy, obtains a sample and computes the log-likelihood\nratio of the two distributions if it has enough energy to sense and process a\nsample. If it does not have enough energy in a given slot, it waits until it\nharvests enough energy to perform the task in a future time slot. We derive\nasymptotic expressions for the expected detection delay (when a change actually\noccurs), and the asymptotic tail distribution of the run-length to a false\nalarm (when a change never happens). We show that when the average harvested\nenergy ($\\bar H$) is greater than or equal to the energy required to sense and\nprocess a sample ($E_s$), standard existing asymptotic results for the CUSUM\ntest apply since the energy storage level at the sensor is greater than $E_s$\nafter a sufficiently long time. However, when the $\\bar H < E_s$, the energy\nstorage level can be modelled by a positive Harris recurrent Markov chain with\na unique stationary distribution. Using asymptotic results from Markov random\nwalk theory and associated nonlinear Markov renewal theory, we establish\nasymptotic expressions for the expected detection delay and asymptotic\nexponentiality of the tail distribution of the run-length to a false alarm in\nthis non-trivial case. Numerical results are provided to support the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:54:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Dey", "Subhrakanti", ""]]}, {"id": "2001.04763", "submitter": "Kaushik Jana Dr.", "authors": "Axel Gandy, Kaushik Jana and Almut E. D. Veraart", "title": "Scoring Predictions at Extreme Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of quantiles at extreme tails is of interest in numerous\napplications. Extreme value modelling provides various competing predictors for\nthis point prediction problem. A common method of assessment of a set of\ncompeting predictors is to evaluate their predictive performance in a given\nsituation. However, due to the extreme nature of this inference problem, it can\nbe possible that the predicted quantiles are not seen in the historical\nrecords, particularly when the sample size is small. This situation poses a\nproblem to the validation of the prediction with its realisation. In this\narticle, we propose two non-parametric scoring approaches to assess extreme\nquantile prediction mechanisms. The proposed assessment methods are based on\npredicting a sequence of equally extreme quantiles on different parts of the\ndata. We then use the quantile scoring function to evaluate the competing\npredictors. The performance of the scoring methods is compared with the\nconventional scoring method and the superiority of the former methods are\ndemonstrated in a simulation study. The methods are then applied to reanalyse\ncyber Netflow data from Los Alamos National Laboratory and daily precipitation\ndata at a station in California available from Global Historical Climatology\nNetwork.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:30:53 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 07:15:23 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:27:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gandy", "Axel", ""], ["Jana", "Kaushik", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2001.04791", "submitter": "Henrique Hilleshein", "authors": "Henrique Hilleshein, Carlos H. M. de Lima, Hirley Alves, Matti\n  Latva-aho", "title": "Iterative Bayesian-based Localization Mechanism for Industry Verticals", "comments": "Paper is to appear in proceedings IEEE VTC'Spring 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate an iterative localization mechanism employing\nBayesian inference to estimate the position of a target using received signal\nstrength measurements. The probability density functions of the target's\ncoordinates are estimated through a Bayesian network. Herein, we consider an\niterative procedure whereby our predictor (posterior distribution) is updated\nin a sequential order whenever new measurements are made available. The\nperformance of the mechanism is assessed in terms of the respective root mean\nsquare error and kernel density estimation of the target coordinates. Our\nnumerical results showed the proposed iterative mechanism achieves increasingly\nbetter estimation of the target node position each updating round of the\nBayesian network with new input measurements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 14:18:37 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 08:35:58 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Hilleshein", "Henrique", ""], ["de Lima", "Carlos H. M.", ""], ["Alves", "Hirley", ""], ["Latva-aho", "Matti", ""]]}, {"id": "2001.05204", "submitter": "Ansgar Steland", "authors": "Nils Mause and Ansgar Steland", "title": "Detecting Changes in the Second Moment Structure of High-Dimensional\n  Sensor-Type Data in a $K$-Sample Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$ sample problem for high-dimensional vector time series is studied,\nespecially focusing on sensor data streams, in order to analyze the second\nmoment structure and detect changes across samples and/or across variables\ncumulated sum (CUSUM) statistics of bilinear forms of the sample covariance\nmatrix. In this model $K$ independent vector time series\n$\\mathbf{Y}_{T,1},\\dots,\\mathbf{Y}_{T,K}$ are observed over a time span $ [0,T]\n$, which may correspond to $K$ sensors (locations) yielding $d$-dimensional\ndata as well as $K$ locations where $d$ sensors emit univariate data. Unequal\nsample sizes are considered as arising when the sampling rate of the sensors\ndiffers. We provide large sample approximations and two related change-point\nstatistics, a sums of squares and a pooled variance statistic. The resulting\nprocedures are investigated by simulations and illustrated by analyzing a real\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 10:00:32 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Mause", "Nils", ""], ["Steland", "Ansgar", ""]]}, {"id": "2001.05388", "submitter": "Dean Scarff", "authors": "Dean Scarff", "title": "Estimation of Climbing Route Difficulty using Whole-History Rating", "comments": "8 pages, 4 figures; v2: fixed precision/recall order; v3: updated\n  results after FP precision change", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing grading systems for rock climbing routes assign a difficulty grade\nto a route based on the opinions of a few people. An objective approach to\nestimating route difficulty on an interval scale was obtained by adapting the\nWhole-History Rating (WHR) system to rock climbing. WHR's model was fitted to a\ndatabase of 236,095 ascents recorded by users on an established climbing\nwebsite. 73% of the ascents used in the dataset were classified as successful.\nPredictions were on average 85% accurate with 10-fold cross-validation. The\nresults suggest that an empirical rating system is accurate at assessing route\ndifficulty and is viable for revising conventional route grades.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 15:46:59 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 02:21:55 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 07:56:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Scarff", "Dean", ""]]}, {"id": "2001.05444", "submitter": "Stephanie Zonszein", "authors": "Peter M. Aronow (1), Dean Eckles (2), Cyrus Samii (3), Stephanie\n  Zonszein (3) ((1) Yale, (2) MIT, (3) NYU)", "title": "Spillover Effects in Experimental Data", "comments": "Forthcoming in Advances in Experimental Political Science, J. N.\n  Druckman and D. P. Green, eds. Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present current methods for estimating treatment effects and spillover\neffects under \"interference\", a term which covers a broad class of situations\nin which a unit's outcome depends not only on treatments received by that unit,\nbut also on treatments received by other units. To the extent that units react\nto each other, interact, or otherwise transmit effects of treatments, valid\ninference requires that we account for such interference, which is a departure\nfrom the traditional assumption that units' outcomes are affected only by their\nown treatment assignment. Interference and associated spillovers may be a\nnuisance or they may be of substantive interest to the researcher. In this\nchapter, we focus on interference in the context of randomized experiments. We\nreview methods for when interference happens in a general network setting. We\nthen consider the special case where interference is contained within a\nhierarchical structure. Finally, we discuss the relationship between\ninterference and contagion. We use the interference R package and simulated\ndata to illustrate key points. We consider efficient designs that allow for\nestimation of the treatment and spillover effects and discuss recent empirical\nstudies that try to capture such effects.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:37:58 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Aronow", "Peter M.", "", "Yale"], ["Eckles", "Dean", "", "MIT"], ["Samii", "Cyrus", "", "NYU"], ["Zonszein", "Stephanie", "", "NYU"]]}, {"id": "2001.05520", "submitter": "Philip White", "authors": "Philip A. White, Durban G. Keeler, Summer Rupper", "title": "Hierarchical Integrated Spatial Process Modeling of Monotone West\n  Antarctic Snow Density Curves", "comments": null, "journal-ref": "the Annals of Applied Statistics 2021, Vol. 15, No. 2, 556-571", "doi": "10.1214/21-AOAS1443", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snow density estimates below the surface, used with airplane-acquired\nice-penetrating radar measurements, give a site-specific history of snow water\naccumulation. Because it is infeasible to drill snow cores across all of\nAntarctica to measure snow density and because it is critical to understand how\nclimatic changes are affecting the world's largest freshwater reservoir, we\ndevelop methods that enable snow density estimation with uncertainty in regions\nwhere snow cores have not been drilled.\n  In inland West Antarctica, snow density increases monotonically as a function\nof depth, except for possible micro-scale variability or measurement error, and\nit cannot exceed the density of ice. We present a novel class of integrated\nspatial process models that allow interpolation of monotone snow density\ncurves. For computational feasibility, we construct the space-depth process\nthrough kernel convolutions of log-Gaussian spatial processes. We discuss model\ncomparison, model fitting, and prediction. Using this model, we extend\nestimates of snow density beyond the depth of the original core and estimate\nsnow density curves where snow cores have not been drilled. Along flight lines\nwith ice-penetrating radar, we use interpolated snow density curves to estimate\nrecent water accumulation and find predominantly decreasing water accumulation\nover recent decades.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 19:21:59 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 15:32:22 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 21:15:38 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 16:32:16 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["White", "Philip A.", ""], ["Keeler", "Durban G.", ""], ["Rupper", "Summer", ""]]}, {"id": "2001.05534", "submitter": "C\\'edric Beaulac", "authors": "C\\'edric Beaulac, Jeffrey S. Rosenthal, Qinglin Pei, Debra Friedman,\n  Suzanne Wolden and David Hodgson", "title": "An evaluation of machine learning techniques to predict the outcome of\n  children treated for Hodgkin-Lymphoma on the AHOD0031 trial: A report from\n  the Children's Oncology Group", "comments": null, "journal-ref": "Applied Artificial Intelligence 2020", "doi": "10.1080/08839514.2020.1815151", "report-no": null, "categories": "q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we analyze a data set containing information on children\nwith Hodgkin Lymphoma (HL) enrolled on a clinical trial. Treatments received\nand survival status were collected together with other covariates such as\ndemographics and clinical measurements. Our main task is to explore the\npotential of machine learning (ML) algorithms in a survival analysis context in\norder to improve over the Cox Proportional Hazard (CoxPH) model. We discuss the\nweaknesses of the CoxPH model we would like to improve upon and then we\nintroduce multiple algorithms, from well-established ones to state-of-the-art\nmodels, that solve these issues. We then compare every model according to the\nconcordance index and the brier score. Finally, we produce a series of\nrecommendations, based on our experience, for practitioners that would like to\nbenefit from the recent advances in artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 20:03:26 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 17:43:56 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Beaulac", "C\u00e9dric", ""], ["Rosenthal", "Jeffrey S.", ""], ["Pei", "Qinglin", ""], ["Friedman", "Debra", ""], ["Wolden", "Suzanne", ""], ["Hodgson", "David", ""]]}, {"id": "2001.05709", "submitter": "Regina Stegherr", "authors": "Regina Stegherr, Claudia Schmoor, Michael L\\\"ubbert, Tim Friede and\n  Jan Beyersmann", "title": "Estimating and comparing adverse event probabilities in the presence of\n  varying follow-up times and competing events", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": "10.1002/pst.2130", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety analyses in terms of adverse events (AEs) are an important aspect of\nbenefit-risk assessments of therapies. Compared to efficacy analyses AE\nanalyses are often rather simplistic. The probability of an AE of a specific\ntype is typically estimated by the incidence proportion, sometimes the\nincidence density or the Kaplan-Meier estimator are proposed. But these\nanalyses either do not account for censoring, rely on a too restrictive\nparametric model, or ignore competing events. With the non-parametric\nAalen-Johansen estimator as the gold-standard, these potential sources of bias\nare investigated in a data example from oncology and in simulations, both in\nthe one-sample and in the two-sample case. As the estimators may have large\nvariances at the end of follow-up, the estimators are not only compared at the\nmaximal event time but also at two quantiles of the observed times. To date,\nconsequences for safety comparisons have hardly been investigated in the\nliterature. The impact of using different estimators for group comparisons is\nunclear, as, for example, the ratio of two both underestimating or\noverestimating estimators may or may not be comparable to the ratio of the\ngold-standard estimator. Therefore, the ratio of the AE probabilities is also\ncalculated based on different approaches. By simulations investigating constant\nand non-constant hazards, different censoring mechanisms and event frequencies,\nwe show that ignoring competing events is more of a problem than falsely\nassuming constant hazards by use of the incidence density and that the choice\nof the AE probability estimator is crucial for group comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 09:26:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Stegherr", "Regina", ""], ["Schmoor", "Claudia", ""], ["L\u00fcbbert", "Michael", ""], ["Friede", "Tim", ""], ["Beyersmann", "Jan", ""]]}, {"id": "2001.05764", "submitter": "Rodney Vasconcelos Fonseca", "authors": "Rodney Fonseca, Guilherme Ludwig, Michel Montoril, Alu\\'isio Pinheiro", "title": "Nonparametric methods for detecting change in Multitemporal SAR/PolSAR\n  Satellite Data", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ nonparametric statistical procedures to analyse multitemporal\nSAR/PolSAR satellite images. The aim is two-fold. We seek parsimony in data\nrepresentation as well as efficient change detection. For these, wavelets and\ngeostatistical analyses are applied to the images (Morettin et al., 2017;\nKrainski et al., 2018). Following this representation, the dimension of the\nunderlying generating process is estimated (Fonseca and Pinheiro, 2019), and a\nset of multivariate characteristics is extracted. Change-points are then\ndetected via wavelets (Montoril et al., 2019).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 12:36:26 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Fonseca", "Rodney", ""], ["Ludwig", "Guilherme", ""], ["Montoril", "Michel", ""], ["Pinheiro", "Alu\u00edsio", ""]]}, {"id": "2001.05885", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Stacey Franklin Jones", "title": "Analysis of Queue Length Prediction from Probe Vehicles Problem with\n  Bunch Arrival Headways", "comments": "6 pages, 6 figures, IEEE Southeastcon 2009, Atlanta, GA, USA", "journal-ref": null, "doi": "10.1109/SECON.2009.5174112", "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the real-time prediction of queue lengths from probe\nvehicles for the Bunch arrival headways at an isolated intersection for\nundersaturated conditions. The paper incorporates the bunching effect of the\ntraffic into the evaluation of the accuracy of the predictions as a function of\nproportion of probe vehicles to entire vehicle population. Formulations are\npresented for predicting the expected queue length and its variance based on\nNegative Exponential and Bunched Exponential vehicle headways. Numerical\nresults for both vehicle headway types are documented to show how prediction\nerrors behave by the volume to capacity ratio and probe proportions. It is\nfound that the Poisson arrivals generate conservative confidence intervals and\ndemand higher probe proportions compared to Bunched Exponential headways at the\nsame arrival rate and probe proportion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:19:48 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Comert", "Gurcan", ""], ["Jones", "Stacey Franklin", ""]]}, {"id": "2001.05904", "submitter": "Gurcan Comert", "authors": "Mecit Cetin, Gurcan Comert", "title": "Flow Rate Estimation From Probe Vehicle Data And Sample Size\n  Requirements", "comments": "13 pages, 2 figures, 15th World Congress on Intelligent Transport\n  Systems and ITS America's 2008 Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest to use probe vehicles for traffic monitoring is growing. This\npaper is focused on the estimation of flow rate from probe vehicle data and the\nevaluation of sample size requirements. Three cases are considered depending on\nthe available information on the percentage of probes p and the flow rate\n\\lambda: i) p is known but \\lambda is unknown ii) \\lambda is known but p is\nunknown, and iii) both parameters are not known. Estimation methods for all\nthree cases are presented along with the reliability of these estimates. For\nthe first two cases, count data provide sufficient information to estimate the\nunknown parameters p and \\lambda individually when the other one is known. For\nthese two cases, simple analytical expressions are derived to analyze the\naccuracy and sample size requirements. However, when both parameters are\nunknown then additional information beyond count data is required. The position\nof probe vehicles in a queue at signalized intersections is used as additional\ninformation in that case. The results show for how long data need to be\ncollected to estimate the parameters at acceptable confidence levels.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:48:35 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Cetin", "Mecit", ""], ["Comert", "Gurcan", ""]]}, {"id": "2001.05948", "submitter": "S\\'andor Baran", "authors": "\\'Agnes Baran, Sebastian Lerch, Mehrez El Ayari and S\\'andor Baran", "title": "Machine learning for total cloud cover prediction", "comments": "24 pages, 7 figures", "journal-ref": "Neural Computing and Applications 33 (2021), 2605-2620", "doi": "10.1007/s00521-020-05139-4", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable forecasting of total cloud cover (TCC) is vital for\nmany areas such as astronomy, energy demand and production, or agriculture.\nMost meteorological centres issue ensemble forecasts of TCC, however, these\nforecasts are often uncalibrated and exhibit worse forecast skill than ensemble\nforecasts of other weather variables. Hence, some form of post-processing is\nstrongly required to improve predictive performance. As TCC observations are\nusually reported on a discrete scale taking just nine different values called\noktas, statistical calibration of TCC ensemble forecasts can be considered a\nclassification problem with outputs given by the probabilities of the oktas.\nThis is a classical area where machine learning methods are applied. We\ninvestigate the performance of post-processing using multilayer perceptron\n(MLP) neural networks, gradient boosting machines (GBM) and random forest (RF)\nmethods. Based on the European Centre for Medium-Range Weather Forecasts global\nTCC ensemble forecasts for 2002-2014 we compare these approaches with the\nproportional odds logistic regression (POLR) and multiclass logistic regression\n(MLR) models, as well as the raw TCC ensemble forecasts. We further assess\nwhether improvements in forecast skill can be obtained by incorporating\nensemble forecasts of precipitation as additional predictor. Compared to the\nraw ensemble, all calibration methods result in a significant improvement in\nforecast skill. RF models provide the smallest increase in predictive\nperformance, while MLP, POLR and GBM approaches perform best. The use of\nprecipitation forecast data leads to further improvements in forecast skill and\nexcept for very short lead times the extended MLP model shows the best overall\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:13:37 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Baran", "\u00c1gnes", ""], ["Lerch", "Sebastian", ""], ["Ayari", "Mehrez El", ""], ["Baran", "S\u00e1ndor", ""]]}, {"id": "2001.05965", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Hanna M. Sykulska-Lawrence", "title": "The Elliptical Ornstein-Uhlenbeck Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.EP eess.SP physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the elliptical Ornstein-Uhlenbeck (OU) process, which is a\ngeneralisation of the well-known univariate OU process to bivariate time\nseries. This process maps out elliptical stochastic oscillations over time in\nthe complex plane, which are observed in many applications of coupled bivariate\ntime series. The appeal of the model is that elliptical oscillations are\ngenerated using one simple first order SDE, whereas alternative models require\nmore complicated vectorised or higher order SDE representations. The second\nuseful feature is that parameter estimation can be performed robustly in the\nfrequency domain using only the modelled and observed power spectral density,\nwithout having to model and compute cross spectra of individual time series\ncomponents. We determine properties of the model including the conditions for\nstationarity, and the geometrical structure of the elliptical oscillations. We\ndemonstrate the utility of the model by measuring periodic and elliptical\nproperties of Earth's polar motion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:53:47 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 20:49:30 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 23:56:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Sykulska-Lawrence", "Hanna M.", ""]]}, {"id": "2001.06033", "submitter": "Vidhi Lalchand Miss", "authors": "Vidhi Lalchand", "title": "Extracting more from boosted decision trees: A high energy physics case\n  study", "comments": "Second Workshop on Machine Learning and the Physical Sciences\n  (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle identification is one of the core tasks in the data analysis\npipeline at the Large Hadron Collider (LHC). Statistically, this entails the\nidentification of rare signal events buried in immense backgrounds that mimic\nthe properties of the former. In machine learning parlance, particle\nidentification represents a classification problem characterized by overlapping\nand imbalanced classes. Boosted decision trees (BDTs) have had tremendous\nsuccess in the particle identification domain but more recently have been\novershadowed by deep learning (DNNs) approaches. This work proposes an\nalgorithm to extract more out of standard boosted decision trees by targeting\ntheir main weakness, susceptibility to overfitting. This novel construction\nharnesses the meta-learning techniques of boosting and bagging simultaneously\nand performs remarkably well on the ATLAS Higgs (H) to tau-tau data set (ATLAS\net al., 2014) which was the subject of the 2014 Higgs ML Challenge\n(Adam-Bourdarios et al., 2015). While the decay of Higgs to a pair of tau\nleptons was established in 2018 (CMS collaboration et al., 2017) at the\n4.9$\\sigma$ significance based on the 2016 data taking period, the 2014 public\ndata set continues to serve as a benchmark data set to test the performance of\nsupervised classification schemes. We show that the score achieved by the\nproposed algorithm is very close to the published winning score which leverages\nan ensemble of deep neural networks (DNNs). Although this paper focuses on a\nsingle application, it is expected that this simple and robust technique will\nfind wider applications in high energy physics.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:13:28 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Lalchand", "Vidhi", ""]]}, {"id": "2001.06052", "submitter": "Eric Auerbach", "authors": "Hossein Alidaee, Eric Auerbach, Michael P. Leung", "title": "Recovering Network Structure from Aggregated Relational Data using\n  Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network data can be expensive to collect. Breza et al. (2017) propose\naggregated relational data (ARD) as a low-cost substitute that can be used to\nrecover the structure of a latent social network when it is generated by a\nspecific parametric random effects model. Our main observation is that many\neconomic network formation models produce networks that are effectively\nlow-rank. As a consequence, network recovery from ARD is generally possible\nwithout parametric assumptions using a nuclear-norm penalized regression. We\ndemonstrate how to implement this method and provide finite-sample bounds on\nthe mean squared error of the resulting estimator for the distribution of\nnetwork links. Computation takes seconds for samples with hundreds of\nobservations. Easy-to-use code in R and Python can be found at\nhttps://github.com/mpleung/ARD.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:43:52 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Alidaee", "Hossein", ""], ["Auerbach", "Eric", ""], ["Leung", "Michael P.", ""]]}, {"id": "2001.06384", "submitter": "Seongyong Park Mr.", "authors": "Seongyong Park and Shujaat Khan", "title": "GSSMD: New metric for robust and interpretable assay quality assessment\n  and hit selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-throughput screening (HTS) campaigns, the Z'-factor and strictly\nstandardized mean difference (SSMD) are commonly used to assess the quality of\nassays and to select hits. However, these measures are vulnerable to outliers\nand their performances are highly sensitive to background distributions. Here,\nwe propose an alternative measure for assay quality assessment and hit\nselection. The proposed method is a non-parametric generalized variant of SSMD\n(GSSMD). In this paper, we have shown that the proposed method provides more\nrobust and intuitive way of assay quality assessment and hit selection.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:45:23 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:40:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Park", "Seongyong", ""], ["Khan", "Shujaat", ""]]}, {"id": "2001.06451", "submitter": "Shai Gorsky", "authors": "Shai Gorsky, Cliburn Chan, Li Ma", "title": "Coarsened mixtures of hierarchical skew normal kernels for flow\n  cytometry analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry (FCM) is the standard multi-parameter assay for measuring\nsingle cell phenotype and functionality. It is commonly used for quantifying\nthe relative frequencies of cell subsets in blood and disaggregated tissues. A\ntypical analysis of FCM data involves cell classification---that is, the\nidentification of cell subgroups in the sample---and comparisons of the cell\nsubgroups across samples or conditions. While modern experiments often\nnecessitate the collection and processing of samples in multiple batches,\nanalysis of FCM data across batches is challenging because differences across\nsamples may occur due to either true biological variation or technical reasons\nsuch as antibody lot effects or instrument optics across batches. Thus a\ncritical step in comparative analyses of multi-sample FCM data---yet missing in\nexisting automated methods for analyzing such data---is cross-sample\ncalibration, whose goal is to align corresponding cell subsets across multiple\nsamples in the presence of technical variations, so that biological variations\ncan be meaningfully compared. We introduce a Bayesian nonparametric\nhierarchical modeling approach for accomplishing both calibration and cell\nclassification simultaneously in a unified probabilistic manner. Three\nimportant features of our method make it particularly effective for analyzing\nmulti-sample FCM data: a nonparametric mixture avoids prespecifying the number\nof cell clusters; a hierarchical skew normal kernel that allows flexibility in\nthe shapes of the cell subsets and cross-sample variation in their locations;\nand finally the \"coarsening\" strategy makes inference robust to departures from\nthe model such as heavy-tailness not captured by the skew normal kernels. We\ndemonstrate the merits of our approach in simulated examples and carry out a\ncase study in the analysis of two multi-sample FCM data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 17:52:55 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 04:18:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gorsky", "Shai", ""], ["Chan", "Cliburn", ""], ["Ma", "Li", ""]]}, {"id": "2001.06457", "submitter": "Mahkameh Zarekarizi", "authors": "Mahkameh Zarekarizi, Vivek Srikrishnan, and Klaus Keller", "title": "Neglecting Uncertainties Biases House-Elevation Decisions to Manage\n  Riverine Flood Risks", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-19188-9", "report-no": null, "categories": "stat.AP physics.data-an q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homeowners around the world elevate houses to manage flood risks. Deciding\nhow high to elevate a house poses a nontrivial decision problem. The U.S.\nFederal Emergency Management Agency (FEMA) recommends elevating existing houses\nto the Base Flood Elevation (the elevation of the 100-yr flood) plus a\nfreeboard. This recommendation neglects many uncertainties. Here we analyze a\ncase-study of riverine flood risk management using a multi-objective robust\ndecision-making framework in the face of deep uncertainties. While the\nquantitative results are location-specific, the approach and overall insights\nare generalizable. We find strong interactions between the economic,\nengineering, and Earth science uncertainties, illustrating the need for\nexpanding on previous integrated analyses to further understand the nature and\nstrength of these connections. Considering deep uncertainties surrounding flood\nhazards, the discount rate, the house lifetime, and the fragility can increase\nthe economically optimal house elevation to values well above FEMA\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:00:11 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 23:55:35 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 17:59:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zarekarizi", "Mahkameh", ""], ["Srikrishnan", "Vivek", ""], ["Keller", "Klaus", ""]]}, {"id": "2001.06483", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Chenyang Gu, Michael Lopez, Jiayi Ji, and Juan\n  Wisnivesky", "title": "Estimation of Causal Effects of Multiple Treatments in Observational\n  Studies with a Binary Outcome", "comments": "3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1901.04312", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a dearth of robust methods to estimate the causal effects of\nmultiple treatments when the outcome is binary. This paper uses two unique sets\nof simulations to propose and evaluate the use of Bayesian Additive Regression\nTrees (BART) in such settings. First, we compare BART to several approaches\nthat have been proposed for continuous outcomes, including inverse probability\nof treatment weighting (IPTW), targeted maximum likelihood estimator (TMLE),\nvector matching and regression adjustment. Results suggest that under\nconditions of non-linearity and non-additivity of both the treatment assignment\nand outcome generating mechanisms, BART, TMLE and IPTW using generalized\nboosted models (GBM) provide better bias reduction and smaller root mean\nsquared error. BART and TMLE provide more consistent 95 per cent CI coverage\nand better large-sample convergence property. Second, we supply BART with a\nstrategy to identify a common support region for retaining inferential units\nand for avoiding extrapolating over areas of the covariate space where common\nsupport does not exist. BART retains more inferential units than the\ngeneralized propensity score based strategy, and shows lower bias, compared to\nTMLE or GBM, in a variety of scenarios differing by the degree of covariate\noverlap. A case study examining the effects of three surgical approaches for\nnon-small cell lung cancer demonstrates the methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 03:49:31 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hu", "Liangyuan", ""], ["Gu", "Chenyang", ""], ["Lopez", "Michael", ""], ["Ji", "Jiayi", ""], ["Wisnivesky", "Juan", ""]]}, {"id": "2001.06520", "submitter": "Alexander Gorban", "authors": "Elaine Fehrman, Vincent Egan, Alexander N. Gorban, Jeremy Levesley,\n  Evgeny M. Mirkes, Awaz K. Muhammad", "title": "Personality Traits and Drug Consumption. A Story Told by Data", "comments": "A preprint version prepared by the authors before the Springer\n  editorial work. 124 pages, 27 figures, 63 tables, bibl. 244", "journal-ref": "Springer, Cham, Research Monograph, 2019, ISBN 978-3-030-10441-2", "doi": "10.1007/978-3-030-10442-9", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a preprint version of the first book from the series: \"Stories told\nby data\". In this book a story is told about the psychological traits\nassociated with drug consumption. The book includes:\n  - A review of published works on the psychological profiles of drug users.\n  - Analysis of a new original database with information on 1885 respondents\nand usage of 18 drugs. (Database is available online.)\n  - An introductory description of the data mining and machine learning methods\nused for the analysis of this dataset.\n  - The demonstration that the personality traits (five factor model,\nimpulsivity, and sensation seeking), together with simple demographic data,\ngive the possibility of predicting the risk of consumption of individual drugs\nwith sensitivity and specificity above 70% for most drugs.\n  - The analysis of correlations of use of different substances and the\ndescription of the groups of drugs with correlated use (correlation pleiades).\n  - Proof of significant differences of personality profiles for users of\ndifferent drugs. This is explicitly proved for benzodiazepines, ecstasy, and\nheroin.\n  - Tables of personality profiles for users and non-users of 18 substances.\n  The book is aimed at advanced undergraduates or first-year PhD students, as\nwell as researchers and practitioners. No previous knowledge of machine\nlearning, advanced data mining concepts or modern psychology of personality is\nassumed. For more detailed introduction into statistical methods we recommend\nseveral undergraduate textbooks. Familiarity with basic statistics and some\nexperience in the use of probabilities would be helpful as well as some basic\ntechnical understanding of psychology.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 20:27:15 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Fehrman", "Elaine", ""], ["Egan", "Vincent", ""], ["Gorban", "Alexander N.", ""], ["Levesley", "Jeremy", ""], ["Mirkes", "Evgeny M.", ""], ["Muhammad", "Awaz K.", ""]]}, {"id": "2001.06548", "submitter": "Stephen Clark Dr", "authors": "Stephen Clark", "title": "Who voted for a No Deal Brexit? A Composition Model of Great Britains\n  2019 European Parliamentary Elections", "comments": "This article is complied with the main manuscript followed by the\n  supplementary materials showing cartographic maps, scatter plots and marginal\n  fit plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to use the votes cast at the 2019 European\nelections held in United Kingdom to re-visit the analysis conducted subsequent\nto its 2016 European Union referendum vote. This exercise provides a staging\npost on public opinion as the United Kingdom moves to leave the European Union\nduring 2020. A composition data analysis in a seemingly unrelated regression\nframework is adopted that respects the compositional nature of the vote\noutcome; each outcome is a share that adds up to 100% and each outcome is\nrelated to the alternatives. Contemporary explanatory data for each counting\narea is sourced from the themes of socio-demographics, employment, life\nsatisfaction and place. The study find that there are still strong and stark\ndivisions in the United Kingdom, defined by age, qualifications, employment and\nplace. The use of a compositional analysis approach produces challenges in\nregards to the interpretation of these models, but marginal plots are seen to\naid the interpretation somewhat.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 11:38:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Clark", "Stephen", ""]]}, {"id": "2001.06880", "submitter": "Vidhi Lalchand Miss", "authors": "Vidhi Lalchand", "title": "A meta-algorithm for classification using random recursive tree\n  ensembles: A high energy physics application", "comments": "MPhil Thesis (Scientific Computing, Physics, Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to propose a meta-algorithm for automatic\nclassification in the presence of discrete binary classes. Classifier learning\nin the presence of overlapping class distributions is a challenging problem in\nmachine learning. Overlapping classes are described by the presence of\nambiguous areas in the feature space with a high density of points belonging to\nboth classes. This often occurs in real-world datasets, one such example is\nnumeric data denoting properties of particle decays derived from high-energy\naccelerators like the Large Hadron Collider (LHC). A significant body of\nresearch targeting the class overlap problem use ensemble classifiers to boost\nthe performance of algorithms by using them iteratively in multiple stages or\nusing multiple copies of the same model on different subsets of the input\ntraining data. The former is called boosting and the latter is called bagging.\nThe algorithm proposed in this thesis targets a challenging classification\nproblem in high energy physics - that of improving the statistical significance\nof the Higgs discovery. The underlying dataset used to train the algorithm is\nexperimental data built from the official ATLAS full-detector simulation with\nHiggs events (signal) mixed with different background events (background) that\nclosely mimic the statistical properties of the signal generating class\noverlap. The algorithm proposed is a variant of the classical boosted decision\ntree which is known to be one of the most successful analysis techniques in\nexperimental physics. The algorithm utilizes a unified framework that combines\ntwo meta-learning techniques - bagging and boosting. The results show that this\ncombination only works in the presence of a randomization trick in the base\nlearners.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 18:22:18 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lalchand", "Vidhi", ""]]}, {"id": "2001.06923", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao and Jiliang Tang", "title": "Exploring Spatio-Temporal and Cross-Type Correlations for Crime\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crime prediction plays an impactful role in enhancing public security and\nsustainable development of urban. With recent advances in data collection and\nintegration technologies, a large amount of urban data with rich crime-related\ninformation and fine-grained spatio-temporal logs has been recorded. Such\nhelpful information can boost our understandings about the temporal evolution\nand spatial factors of urban crimes and can enhance accurate crime prediction.\nIn this paper, we perform crime prediction exploiting the cross-type and\nspatio-temporal correlations of urban crimes. In particular, we verify the\nexistence of correlations among different types of crime from temporal and\nspatial perspectives, and propose a coherent framework to mathematically model\nthese correlations for crime prediction. The extensive experimental results on\nreal-world data validate the effectiveness of the proposed framework. Further\nexperiments have been conducted to understand the importance of different\ncorrelations in crime prediction.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 00:34:53 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 02:20:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Tang", "Jiliang", ""]]}, {"id": "2001.06974", "submitter": "Ravi Goyal", "authors": "Ravi Goyal and Victor De Gruttola", "title": "Investigation of Patient-sharing Networks Using a Bayesian Network Model\n  Selection Approach for Congruence Class Models", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach to conduct network model selection is presented for a\ngeneral class of network models referred to as the congruence class models\n(CCMs). CCMs form a broad class that includes as special cases several common\nnetwork models, such as the Erd\\H{o}s-R\\'{e}nyi-Gilbert model, stochastic block\nmodel and many exponential random graph models. Due to the range of models able\nto be specified as a CCM, investigators are better able to select a model\nconsistent with generative mechanisms associated with the observed network\ncompared to current approaches. In addition, the approach allows for\nincorporation of prior information. We utilize the proposed Bayesian network\nmodel selection approach for CCMs to investigate several mechanisms that may be\nresponsible for the structure of patient-sharing networks, which are associated\nwith the cost and quality of medical care. We found evidence in support of\nheterogeneity in sociality but not selective mixing by provider type nor\ndegree.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 05:09:52 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Goyal", "Ravi", ""], ["De Gruttola", "Victor", ""]]}, {"id": "2001.07316", "submitter": "Jin Jin", "authors": "Jin Jin (1), Lin Zhang (2), Ethan Leng (3), Gregory J. Metzger (4),\n  Joseph S. Koopmeiners (2) ((1) Department of Biostatistics, Johns Hopkins\n  Bloomberg School of Public Health, Baltimore, MD, USA, (2) Division of\n  Biostatistics, School of Public Health, University of Minnesota, Minneapolis,\n  MN, USA, (3) Department of Biomedical Engineering, University of Minnesota,\n  Minneapolis, MN, USA, (4) Department of Radiology, Center for Magnetic\n  Resonance Research, University of Minnesota, Minneapolis, MN, USA)", "title": "Bayesian Spatial Models for Voxel-wise Prostate Cancer Classification\n  Using Multi-parametric MRI Data", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-parametric magnetic resonance imaging (mpMRI) plays an increasingly\nimportant role in the diagnosis of prostate cancer. Various computer-aided\ndetection algorithms have been proposed for automated prostate cancer detection\nby combining information from various mpMRI data components. However, there\nexist other features of mpMRI, including the spatial correlation between voxels\nand between-patient heterogeneity in the mpMRI parameters, that have not been\nfully explored in the literature but could potentially improve cancer detection\nif leveraged appropriately. This paper proposes novel voxel-wise Bayesian\nclassifiers for prostate cancer that account for the spatial correlation and\nbetween-patient heterogeneity in mpMRI. Modeling the spatial correlation is\nchallenging due to the extreme high dimensionality of the data, and we consider\nthree computationally efficient approaches using Nearest Neighbor Gaussian\nProcess (NNGP), knot-based reduced-rank approximation, and a conditional\nautoregressive (CAR) model, respectively. The between-patient heterogeneity is\naccounted for by adding a subject-specific random intercept on the mpMRI\nparameter model. Simulation results show that properly modeling the spatial\ncorrelation and between-patient heterogeneity improves classification accuracy.\nApplication to in vivo data illustrates that classification is improved by\nspatial modeling using NNGP and reduced-rank approximation but not the CAR\nmodel, while modeling the between-patient heterogeneity does not further\nimprove our classifier. Among our proposed models, the NNGP-based model is\nrecommended considering its robust classification accuracy and high\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 02:59:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Jin", "Jin", ""], ["Zhang", "Lin", ""], ["Leng", "Ethan", ""], ["Metzger", "Gregory J.", ""], ["Koopmeiners", "Joseph S.", ""]]}, {"id": "2001.07469", "submitter": "Ayman Hijazy", "authors": "Ayman Hijazy and Andr\\'as Zempl\\'eni", "title": "how well can sensitivity and sojourn time be estimated", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic disease progression models are governed by three main parameters:\nsensitivity, preclinical intensity, and sojourn time. The estimation of these\nparameters helps in optimizing screening programs and examine the improvement\nin survival. Multiple approaches exist to estimate those parameters. However,\nthese models are based on strong underlying assumptions. The main aim of this\narticle is to investigate the effect of these assumptions. For this purpose, we\ndeveloped a simulator to mimic a breast cancer screening program directly\nobserving the exact onset and the sojourn time of the disease. We investigate\nthe effects of assuming the sensitivity to be constant, inter-screening\ninterval and misspecifying the sojourn time. Our results indicate a strong\ncorrelation between the estimated parameters, and that the chosen sojourn\ntime-distribution has a strong effect on the accuracy of the estimation. These\nfindings shed a light on the seemingly discrepant results got by different\nauthors using the same data sets but different assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:11:36 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hijazy", "Ayman", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "2001.07646", "submitter": "Zhe Zheng", "authors": "Ke Liu, Zhe Zheng, Bo Zou, Mark Hansen", "title": "How Fast You Can Actually Fly: A Comparative Investigation of Flight\n  Airborne Time in China and the U.S", "comments": "44 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actual airborne time (AAT) is the time between wheels-off and wheels-on of a\nflight. Understanding the behavior of AAT is increasingly important given the\never growing demand for air travel and flight delays becoming more rampant. As\nno research on AAT exists, this paper performs the first empirical analysis of\nAAT behavior, comparatively for the U.S. and China. The focus is on how AAT is\naffected by scheduled block time (SBT), origin-destination (OD) distance, and\nthe possible pressure to reduce AAT from other parts of flight operations.\nMultiple econometric models are developed. The estimation results show that in\nboth countries AAT is highly correlated with SBT and OD distance. Flights in\nthe U.S. are faster than in China. On the other hand, facing ground delay prior\nto takeoff, a flight has limited capability to speed up. The pressure from\nshort turnaround time after landing to reduce AAT is immaterial. Sensitivity\nanalysis of AAT to flight length and aircraft utilization is further conducted.\nGiven the more abundant airspace, flexible routing networks, and efficient ATFM\nprocedures, a counterfactual that the AAT behavior in the U.S. were adopted in\nChina is examined. We find that by doing so significant efficiency gains could\nbe achieved in the Chinese air traffic system. On average, 11.8 minutes of AAT\nper flight would be saved, coming from both reduction in SBT and reduction in\nAAT relative to the new SBT. Systemwide fuel saving would amount to over 300\nmillion gallons with direct airline operating cost saving of nearly $1.3\nbillion nationwide in 2016.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:50:40 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Liu", "Ke", ""], ["Zheng", "Zhe", ""], ["Zou", "Bo", ""], ["Hansen", "Mark", ""]]}, {"id": "2001.07692", "submitter": "Jacob Mortensen", "authors": "Jacob Mortensen and Luke Bornn", "title": "Estimating locomotor demands during team play from broadcast-derived\n  tracking data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of optical tracking data across sports has given rise to the\nability to dissect athletic performance at a level unfathomable a decade ago.\nOne specific area that has seen substantial benefit is sports science, as high\nresolution coordinate data permits sports scientists to have to-the-second\nestimates of external load metrics, such as acceleration load and high speed\nrunning distance, traditionally used to understand the physical toll a game\ntakes on an athlete. Unfortunately, collecting this data requires installation\nof expensive hardware and paying costly licensing fees to data providers,\nrestricting its availability. Algorithms have been developed that allow a\ntraditional broadcast feed to be converted to x-y coordinate data, making\ntracking data easier to acquire, but coordinates are available for an athlete\nonly when that player is within the camera frame. Obviously, this leads to\ninaccuracies in player load estimates, limiting the usefulness of this data for\nsports scientists. In this research, we develop models that predict offscreen\nload metrics and demonstrate the viability of broadcast-derived tracking data\nfor understanding external load in soccer.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 18:50:22 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mortensen", "Jacob", ""], ["Bornn", "Luke", ""]]}, {"id": "2001.07745", "submitter": "Rohan Arambepola", "authors": "Rohan Arambepola, Peter Gething, Ewan Cameron", "title": "Nonparametric Causal Feature Selection for Spatiotemporal Risk Mapping\n  of Malaria Incidence in Madagascar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern disease mapping draws upon a wealth of high resolution spatial data\nproducts reflecting environmental and/or socioeconomic factors as covariates,\nor `features', within a geostatistical framework to improve predictions of\ndisease risk. Feature selection is an important step in building these models,\nhelping to reduce overfitting and computational complexity, and to improve\nmodel interpretability. Selecting only features that have a causal relationship\nwith the response variable could potentially improve predictions and\ngeneralisability, but identifying these causal features from\nnon-interventional, spatiotemporal data is a challenging problem. Here we\nexamine the performance of a causal feature selection procedure with regard to\nestimating malaria incidence in Madagascar. The studied procedure designed for\nthis task combines the PC algorithm with spatiotemporal prewhitening and\nkernel-based independence tests extended to accommodate aggregated data. This\ncase study reveals a clear advantage for causal feature selection in terms of\nthe out-of-sample predictive accuracy in a forward temporal estimation task,\nbut not in a spatiotemporal interpolation task, in comparison with thresholded\nspike-and-slab, for both linear and non-linear regression models. Compared to\nno feature selection, causal feature selection was most beneficial in settings\nwherein the volume of available data was low relative to the model complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:19:02 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:33:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Arambepola", "Rohan", ""], ["Gething", "Peter", ""], ["Cameron", "Ewan", ""]]}, {"id": "2001.07773", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "Missed opportunities in large scale comparison of QSAR and conformal\n  prediction methods and their applications in drug discovery", "comments": null, "journal-ref": "J Cheminform 11, 65 (2019)", "doi": "10.1186/s13321-019-0387-y", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Bosc et al. (J Cheminform 11(1): 4, 2019), published an article\ndescribing a case study that directly compares conformal predictions with\ntraditional QSAR methods for large-scale predictions of target-ligand binding.\nWe consider this study to be very important. Unfortunately, we have found\nseveral issues in the authors' approach as well as in the presentation of their\nfindings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:03:35 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "2001.07824", "submitter": "Fernando Alarid-Escudero", "authors": "Fernando Alarid-Escudero, Eline M. Krijkamp, Eva A. Enns, Alan Yang,\n  M.G. Myriam Hunink, Petros Pechlivanoglou, Hawre Jalal", "title": "Cohort State-Transition Models in R: A Tutorial", "comments": "Tutorial with 48 pages and 12 figures. For R code, see\n  https://github.com/DARTH-git/Cohort-modeling-tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision models can synthesize evidence from different sources to simulate\nlong-term consequences of different strategies in the presence of uncertainty.\nCohort state-transition models (cSTM) are decision models commonly used in\nmedical decision making to simulate hypothetical cohorts' transitions across\nvarious health states over time. This tutorial shows how to implement cSTMs in\nR, an open-source mathematical and statistical programming language. As an\nexample, we use a previously published cSTM-based cost-effectiveness analysis.\nWith this example, we illustrate both time-independent cSTMs, where transition\nprobabilities are constant over time, and time-dependent cSTMs, where\ntransition probabilities vary by age and are dependent on time spent in a\nhealth state (state residence). We also illustrate how to compute various\nepidemiological outcomes of interest, such as survival and prevalence. We\ndemonstrate how to calculate economic outcomes and conducting a\ncost-effectiveness analysis of multiple strategies using the example model, and\nprovide additional resources to conduct probabilistic sensitivity analyses. We\nprovide a link to a public repository with all the R code described in this\ntutorial that can be used to replicate the example or be adapted for various\ndecision modeling applications.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 00:14:28 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:40:27 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Alarid-Escudero", "Fernando", ""], ["Krijkamp", "Eline M.", ""], ["Enns", "Eva A.", ""], ["Yang", "Alan", ""], ["Hunink", "M. G. Myriam", ""], ["Pechlivanoglou", "Petros", ""], ["Jalal", "Hawre", ""]]}, {"id": "2001.07835", "submitter": "Zhimei Ren", "authors": "Zhimei Ren, Emmanuel Cand\\`es", "title": "Knockoffs with Side Information", "comments": "29 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assessing the importance of multiple variables or\nfactors from a dataset when side information is available. In principle, using\nside information can allow the statistician to pay attention to variables with\na greater potential, which in turn, may lead to more discoveries. We introduce\nan adaptive knockoff filter, which generalizes the knockoff procedure (Barber\nand Cand\\`es, 2015; Cand\\`es et al., 2018) in that it uses both the data at\nhand and side information to adaptively order the variables under study and\nfocus on those that are most promising. Adaptive knockoffs controls the\nfinite-sample false discovery rate (FDR) and we demonstrate its power by\ncomparing it with other structured multiple testing methods. We also apply our\nmethodology to real genetic data in order to find associations between genetic\nvariants and various phenotypes such as Crohn's disease and lipid levels. Here,\nadaptive knockoffs makes more discoveries than reported in previous studies on\nthe same datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 01:09:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Ren", "Zhimei", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2001.08038", "submitter": "Andrew Manderson", "authors": "Andrew A. Manderson and Robert J. B. Goudie", "title": "A numerically stable algorithm for integrating Bayesian models using\n  Markov melding", "comments": "18 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When statistical analyses consider multiple data sources, Markov melding\nprovides a method for combining the source-specific Bayesian models. Models\noften contain different quantities of information due to variation in the\nrichness of model-specific data, or availability of model-specific prior\ninformation. We show that this can make the multi-stage Markov chain Monte\nCarlo sampler employed by Markov melding unstable and unreliable. We propose a\nrobust multi-stage algorithm that estimates the required prior marginal\nself-density ratios using weighted samples, dramatically improving accuracy in\nthe tails of the distribution, thus stabilising the algorithm and providing\nreliable inference. We demonstrate our approach using an evidence synthesis for\ninferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:45:22 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Manderson", "Andrew A.", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2001.08089", "submitter": "Jakob Dambon", "authors": "Jakob A. Dambon (1 and 2), Fabio Sigrist (2), Reinhard Furrer (1) ((1)\n  University of Zurich, (2) Lucerne University of Applied Sciences and Arts)", "title": "Maximum Likelihood Estimation of Spatially Varying Coefficient Models\n  for Large Data with an Application to Real Estate Price Prediction", "comments": "revision: 35 pages, 14 figures, typo in likelihood corrected, DOI\n  added", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100470", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression models for spatial data, it is often assumed that the marginal\neffects of covariates on the response are constant over space. In practice,\nthis assumption might often be questionable. In this article, we show how a\nGaussian process-based spatially varying coefficient (SVC) model can be\nestimated using maximum likelihood estimation (MLE). In addition, we present an\napproach that scales to large data by applying covariance tapering. We compare\nour methodology to existing methods such as a Bayesian approach using the\nstochastic partial differential equation (SPDE) link, geographically weighted\nregression (GWR), and eigenvector spatial filtering (ESF) in both a simulation\nstudy and an application where the goal is to predict prices of real estate\napartments in Switzerland. The results from both the simulation study and\napplication show that the MLE approach results in increased predictive accuracy\nand more precise estimates. Since we use a model-based approach, we can also\nprovide predictive variances. In contrast to existing model-based approaches,\nour method scales better to data where both the number of spatial points is\nlarge and the number of spatially varying covariates is moderately-sized, e.g.,\nabove ten.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:46:29 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 11:06:00 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 07:31:48 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 14:20:51 GMT"}, {"version": "v5", "created": "Thu, 12 Nov 2020 16:33:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dambon", "Jakob A.", "", "1 and 2"], ["Sigrist", "Fabio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2001.08109", "submitter": "Xiaoming Li", "authors": "Xiaoming Li, Chun Wang, Xiao Huang", "title": "DDKSP: A Data-Driven Stochastic Programming Framework for Car-Sharing\n  Relocation Problem", "comments": "arXiv admin note: text overlap with arXiv:1909.09293", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Car-sharing issue is a popular research field in sharing economy. In this\npaper, we investigate the car-sharing relocation problem (CSRP) under uncertain\ndemands. Normally, the real customer demands follow complicating probability\ndistribution which cannot be described by parametric approaches. In order to\novercome the problem, an innovative framework called Data-Driven Kernel\nStochastic Programming (DDKSP) that integrates a non-parametric approach -\nkernel density estimation (KDE) and a two-stage stochastic programming (SP)\nmodel is proposed. Specifically, the probability distributions are derived from\nhistorical data by KDE, which are used as the input uncertain parameters for\nSP. Additionally, the CSRP is formulated as a two-stage SP model. Meanwhile, a\nMonte Carlo method called sample average approximation (SAA) and Benders\ndecomposition algorithm are introduced to solve the large-scale optimization\nmodel. Finally, the numerical experimental validations which are based on New\nYork taxi trip data sets show that the proposed framework outperforms the pure\nparametric approaches including Gaussian, Laplace and Poisson distributions\nwith 3.72% , 4.58% and 11% respectively in terms of overall profits.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 19:04:29 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Li", "Xiaoming", ""], ["Wang", "Chun", ""], ["Huang", "Xiao", ""]]}, {"id": "2001.08146", "submitter": "Marc Schneble", "authors": "Marc Schneble and G\\\"oran Kauermann", "title": "Estimation of Latent Network Flows in Bike-Sharing Systems", "comments": "27 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of latent network flows is a common problem in statistical network\nanalysis. The typical setting is that we know the margins of the network, i.e.\nin- and outdegrees, but the flows are unobserved. In this paper, we develop a\nmixed regression model to estimate network flows in a bike-sharing network if\nonly the hourly differences of in- and outdegrees at bike stations are known.\nWe also include exogenous covariates such as weather conditions. Two different\nparameterizations of the model are considered to estimate 1) the whole network\nflow and 2) the network margins only. The estimation of the model parameters is\nproposed via an iterative penalized maximum likelihood approach. This is\nexemplified by modeling network flows in the Vienna Bike-Sharing Network.\nFurthermore, a simulation study is conducted to show the performance of the\nmodel. For practical purposes it is crucial to predict when and at which\nstation there is a lack or an excess of bikes. For this application, our model\nshows to be well suited by providing quite accurate predictions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:02:42 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Schneble", "Marc", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2001.08170", "submitter": "Luke Keele", "authors": "Luke Keele, Stephen O'Neill, Richard Grieve", "title": "Comparing the Performance of Statistical Adjustment Methods By\n  Recovering the Experimental Benchmark from the REFLUX Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much evidence in comparative effectiveness research is based on observational\nstudies. Researchers who conduct observational studies typically assume that\nthere are no unobservable differences between the treated and control groups.\nTreatment effects are estimated after adjusting for observed differences\nbetween treated and controls. However, treatment effect estimates may be biased\ndue to model misspecification. That is, if the method of treatment effect\nestimation imposes unduly strong functional form assumptions, treatment effect\nestimates may be significantly biased. In this study, we compare the\nperformance of a wide variety of treatment effect estimation methods. We do so\nwithin the context of the REFLUX study from the UK. In REFLUX, after study\nqualification, participants were enrolled in either a randomized trial arm or\npatient preference arm. In the randomized trial, patients were randomly\nassigned to either surgery or medical management. In the patient preference\narm, participants selected to either have surgery or medical management. We\nattempt to recover the treatment effect estimate from the randomized trial arm\nusing the data from the patient preference arm of the study. We vary the method\nof treatment effect estimation and record which methods are successful and\nwhich are not. We apply over 20 different methods including standard regression\nmodels as well as advanced machine learning methods. We find that simple\npropensity score matching methods perform the worst. We also find significant\nvariation in performance across methods. The wide variation in performance\nsuggests analysts should use multiple methods of estimation as a robustness\ncheck.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:37:48 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Keele", "Luke", ""], ["O'Neill", "Stephen", ""], ["Grieve", "Richard", ""]]}, {"id": "2001.08175", "submitter": "Adam Ciarleglio", "authors": "Adam Ciarleglio, Eva Petkova, Ofer Harel", "title": "Multiple imputation in functional regression with applications to EEG\n  data in a depression study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current source density (CSD) power asymmetry, a measure derived from\nelectroencephalography (EEG), is a potential biomarker for major depressive\ndisorder (MDD). Though this measure is functional in nature (defined on the\nfrequency domain), it is typically reduced to a scalar value prior to analysis,\npossibly obscuring the relationship between brain function and MDD. To overcome\nthis issue, we sought to fit a functional regression model to estimate the\nassociation between CSD power asymmetry and MDD diagnostic status, adjusting\nfor age, sex, cognitive ability, and handedness using data from a large\nclinical study. Unfortunately, nearly 40\\% of the observations were missing\neither their functional EEG data, their cognitive ability score, or both. In\norder to take advantage of all of the available data, we propose an extension\nto multiple imputation by chained equations that handles both scalar and\nfunctional data. We also propose an extension to Rubin's Rules for pooling\nestimates from the multiply imputed data sets in order to conduct valid\ninference. We investigate the performance of the proposed extensions in a\nsimulation study and apply them to our clinical study data. Our analysis\nreveals that the association between CSD power asymmetry and diagnostic status\ndepends on both age and sex.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:41:08 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 15:21:29 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ciarleglio", "Adam", ""], ["Petkova", "Eva", ""], ["Harel", "Ofer", ""]]}, {"id": "2001.08363", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad, Wei Sun, Li Hsu", "title": "A covariance-enhanced approach to multi-tissue joint eQTL mapping with\n  application to transcriptome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptome-wide association studies based on genetically predicted gene\nexpression have the potential to identify novel regions associated with various\ncomplex traits. It has been shown that incorporating expression quantitative\ntrait loci (eQTLs) corresponding to multiple tissue types can improve power for\nassociation studies involving complex etiology. In this article, we propose a\nnew multivariate response linear regression model and method for predicting\ngene expression in multiple tissues simultaneously. Unlike existing methods for\nmulti-tissue joint eQTL mapping, our approach incorporates tissue-tissue\nexpression correlation, which allows us to more efficiently handle missing\nexpression measurements and more accurately predict gene expression using a\nweighted summation of eQTL genotypes. We show through simulation studies that\nour approach performs better than the existing methods in many scenarios. We\nuse our method to estimate eQTL weights for 29 tissues collected by GTEx, and\nshow that our approach significantly improves expression prediction accuracy\ncompared to competitors. Using our eQTL weights, we perform a\nmulti-tissue-based S-MultiXcan transcriptome-wide association study and show\nthat our method leads to more discoveries in novel regions and more discoveries\noverall than the existing methods. Estimated eQTL weights are available for\ndownload online at github.com/ajmolstad/MTeQTLResults.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 04:09:43 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Sun", "Wei", ""], ["Hsu", "Li", ""]]}, {"id": "2001.08570", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, Mohammed El Adoui, Manasa Vulchi, Paulette Turk,\n  Maryam Etesami, Pingfu Fu, Kaustav Bera, Stylianos Drisis, Vinay Varadan,\n  Donna Plecha, Mohammed Benjelloun, Jame Abraham, Anant Madabhushi", "title": "Deep learning-based prediction of response to HER2-targeted neoadjuvant\n  chemotherapy from pre-treatment dynamic breast MRI: A multi-institutional\n  validation study", "comments": "Braman and El Adoui contributed equally to this work. 33 pages, 3\n  figures in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting response to neoadjuvant therapy is a vexing challenge in breast\ncancer. In this study, we evaluate the ability of deep learning to predict\nresponse to HER2-targeted neo-adjuvant chemotherapy (NAC) from pre-treatment\ndynamic contrast-enhanced (DCE) MRI acquired prior to treatment. In a\nretrospective study encompassing DCE-MRI data from a total of 157 HER2+ breast\ncancer patients from 5 institutions, we developed and validated a deep learning\napproach for predicting pathological complete response (pCR) to HER2-targeted\nNAC prior to treatment. 100 patients who received HER2-targeted neoadjuvant\nchemotherapy at a single institution were used to train (n=85) and tune (n=15)\na convolutional neural network (CNN) to predict pCR. A multi-input CNN\nleveraging both pre-contrast and late post-contrast DCE-MRI acquisitions was\nidentified to achieve optimal response prediction within the validation set\n(AUC=0.93). This model was then tested on two independent testing cohorts with\npre-treatment DCE-MRI data. It achieved strong performance in a 28 patient\ntesting set from a second institution (AUC=0.85, 95% CI 0.67-1.0, p=.0008) and\na 29 patient multicenter trial including data from 3 additional institutions\n(AUC=0.77, 95% CI 0.58-0.97, p=0.006). Deep learning-based response prediction\nmodel was found to exceed a multivariable model incorporating predictive\nclinical variables (AUC < .65 in testing cohorts) and a model of\nsemi-quantitative DCE-MRI pharmacokinetic measurements (AUC < .60 in testing\ncohorts). The results presented in this work across multiple sites suggest that\nwith further validation deep learning could provide an effective and reliable\ntool to guide targeted therapy in breast cancer, thus reducing overtreatment\namong HER2+ patients.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:54:24 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Braman", "Nathaniel", ""], ["Adoui", "Mohammed El", ""], ["Vulchi", "Manasa", ""], ["Turk", "Paulette", ""], ["Etesami", "Maryam", ""], ["Fu", "Pingfu", ""], ["Bera", "Kaustav", ""], ["Drisis", "Stylianos", ""], ["Varadan", "Vinay", ""], ["Plecha", "Donna", ""], ["Benjelloun", "Mohammed", ""], ["Abraham", "Jame", ""], ["Madabhushi", "Anant", ""]]}, {"id": "2001.08681", "submitter": "Ian Dobson", "authors": "Kai Zhou, James R. Cruise, Chris J. Dent, Ian Dobson, Louis Wehenkel,\n  Zhaoyu Wang, Amy L. Wilson", "title": "Bayesian estimates of transmission line outage rates that consider line\n  dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transmission line outage rates are fundamental to power system reliability\nanalysis. Line outages are infrequent, occurring only about once a year, so\noutage data are limited. We propose a Bayesian hierarchical model that\nleverages line dependencies to better estimate outage rates of individual\ntransmission lines from limited outage data. The Bayesian estimates have a\nlower standard deviation than estimating the outage rates simply by dividing\nthe number of outages by the number of years of data, especially when the\nnumber of outages is small. The Bayesian model produces more accurate\nindividual line outage rates, as well as estimates of the uncertainty of these\nrates. Better estimates of line outage rates can improve system risk\nassessment, outage prediction, and maintenance scheduling.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:23:38 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhou", "Kai", ""], ["Cruise", "James R.", ""], ["Dent", "Chris J.", ""], ["Dobson", "Ian", ""], ["Wehenkel", "Louis", ""], ["Wang", "Zhaoyu", ""], ["Wilson", "Amy L.", ""]]}, {"id": "2001.08712", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, \\'Agnes Baran, Florian Pappenberger and Zied Ben\n  Bouall\\`egue", "title": "Statistical post-processing of heat index ensemble forecasts: is there a\n  royal road?", "comments": "29 pages, 12 figures", "journal-ref": "Quarterly Journal of the Royal Meteorological Society 146 (2020),\n  no. 732, 3416-3434", "doi": "10.1002/qj.3853", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effect of statistical post-processing on the probabilistic\nskill of discomfort index (DI) and indoor wet-bulb globe temperature (WBGTid)\nensemble forecasts, both calculated from the corresponding forecasts of\ntemperature and dew point temperature. Two different methodological approaches\nto calibration are compared. In the first case, we start with joint\npost-processing of the temperature and dew point forecasts and then create\ncalibrated samples of DI and WBGTid using samples from the obtained bivariate\npredictive distributions. This approach is compared with direct post-processing\nof the heat index ensemble forecasts. For this purpose, a novel ensemble model\noutput statistics model based on a generalized extreme value distribution is\nproposed. The predictive performance of both methods is tested on the\noperational temperature and dew point ensemble forecasts of the European Centre\nfor Medium-Range Weather Forecasts and the corresponding forecasts of DI and\nWBGTid. For short lead times (up to day 6), both approaches significantly\nimprove the forecast skill. Among the competing post-processing methods, direct\ncalibration of heat indices exhibits the best predictive performance, very\nclosely followed by the more general approach based on joint calibration of\ntemperature and dew point temperature. Additionally, a machine learning\napproach is tested and shows comparable performance for the case when one is\ninterested only in forecasting heat index warning level categories.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:03:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 20:27:01 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Baran", "\u00c1gnes", ""], ["Pappenberger", "Florian", ""], ["Bouall\u00e8gue", "Zied Ben", ""]]}, {"id": "2001.08793", "submitter": "Kristian Lum", "authors": "Kristian Lum, Chesa Boudin, and Megan Price", "title": "The impact of overbooking on a pre-trial risk assessment tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pre-trial risk assessment tools are used to make recommendations to judges\nabout appropriate conditions of pre-trial supervision for people who have been\narrested. Increasingly, there is concern about whether these models are\noperating fairly, including concerns about whether the models' input factors\nare fair measures of one's criminal activity. In this paper, we assess the\nimpact of booking charges that do not result in a conviction on a popular risk\nassessment tool, the Arnold Public Safety Assessment. Using data from a pilot\nrun of the tool in San Francisco, CA, we find that booking charges that do not\nresult in a conviction (i.e. charges that are dropped or end in an acquittal)\nincreased the recommended level of pre-trial supervision in around 27% of cases\nevaluated by the tool\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 20:33:47 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Lum", "Kristian", ""], ["Boudin", "Chesa", ""], ["Price", "Megan", ""]]}, {"id": "2001.08979", "submitter": "Amit Tewari", "authors": "Amit Tewari", "title": "Forecasting NIFTY 50 benchmark Index using Seasonal ARIMA time series\n  models", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.10332.95364", "report-no": null, "categories": "q-fin.ST cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper analyses how Time Series Analysis techniques can be applied to\ncapture movement of an exchange traded index in a stock market. Specifically,\nSeasonal Auto Regressive Integrated Moving Average (SARIMA) class of models is\napplied to capture the movement of Nifty 50 index which is one of the most\nactively exchange traded contracts globally [1]. A total of 729 model parameter\ncombinations were evaluated and the most appropriate selected for making the\nfinal forecast based on AIC criteria [8]. NIFTY 50 can be used for a variety of\npurposes such as benchmarking fund portfolios, launching of index funds,\nexchange traded funds (ETFs) and structured products. The index tracks the\nbehaviour of a portfolio of blue chip companies, the largest and most liquid\nIndian securities and can be regarded as a true reflection of the Indian stock\nmarket [2].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 12:58:48 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Tewari", "Amit", ""]]}, {"id": "2001.09016", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Neil P. Oxtoby, Alexandra L. Young, Esther E.\n  Bron, Arthur W. Toga, Michael W. Weiner, Frederik Barkhof, Nick C. Fox,\n  Polina Golland, Stefan Klein, Daniel C. Alexander", "title": "TADPOLE Challenge: Accurate Alzheimer's disease prediction through\n  crowdsourced forecasting of future data", "comments": "10 pages, 1 figure, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1805.03909", "journal-ref": "MICCAI Multimodal Brain Image Analysis Workshop, 2019", "doi": "10.1007/978-3-030-32281-6_1", "report-no": null, "categories": "q-bio.PE eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The TADPOLE Challenge compares the performance of algorithms at predicting\nthe future evolution of individuals at risk of Alzheimer's disease. TADPOLE\nChallenge participants train their models and algorithms on historical data\nfrom the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. Participants\nare then required to make forecasts of three key outcomes for ADNI-3 rollover\nparticipants: clinical diagnosis, ADAS-Cog 13, and total volume of the\nventricles -- which are then compared with future measurements. Strong points\nof the challenge are that the test data did not exist at the time of\nforecasting (it was acquired afterwards), and that it focuses on the\nchallenging problem of cohort selection for clinical trials by identifying fast\nprogressors. The submission phase of TADPOLE was open until 15 November 2017;\nsince then data has been acquired until April 2019 from 219 subjects with 223\nclinical visits and 150 Magnetic Resonance Imaging (MRI) scans, which was used\nfor the evaluation of the participants' predictions. Thirty-three teams\nparticipated with a total of 92 submissions. No single submission was best at\npredicting all three outcomes. For diagnosis prediction, the best forecast\n(team Frog), which was based on gradient boosting, obtained a multiclass area\nunder the receiver-operating curve (MAUC) of 0.931, while for ventricle\nprediction the best forecast (team EMC1), which was based on disease\nprogression modelling and spline regression, obtained mean absolute error of\n0.41% of total intracranial volume (ICV). For ADAS-Cog 13, no forecast was\nconsiderably better than the benchmark mixed effects model (BenchmarkME),\nprovided to participants before the submission deadline. Further analysis can\nhelp understand which input features and algorithms are most suitable for\nAlzheimer's disease prediction and for aiding patient stratification in\nclinical trials.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:06:12 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Marinescu", "Razvan V.", ""], ["Oxtoby", "Neil P.", ""], ["Young", "Alexandra L.", ""], ["Bron", "Esther E.", ""], ["Toga", "Arthur W.", ""], ["Weiner", "Michael W.", ""], ["Barkhof", "Frederik", ""], ["Fox", "Nick C.", ""], ["Golland", "Polina", ""], ["Klein", "Stefan", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "2001.09055", "submitter": "Mohsen Shahhosseini", "authors": "Mohsen Shahhosseini, Guiping Hu, Sotirios V. Archontoulis", "title": "Forecasting Corn Yield with Machine Learning Ensembles", "comments": null, "journal-ref": "Frontiers in Plant Science 11 (2020) 1120", "doi": "10.3389/fpls.2020.01120", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerge of new technologies to synthesize and analyze big data with\nhigh-performance computing, has increased our capacity to more accurately\npredict crop yields. Recent research has shown that Machine learning (ML) can\nprovide reasonable predictions, faster, and with higher flexibility compared to\nsimulation crop modeling. The earlier the prediction during the growing season\nthe better, but this has not been thoroughly investigated as previous studies\nconsidered all data available to predict yields. This paper provides a machine\nlearning based framework to forecast corn yields in three US Corn Belt states\n(Illinois, Indiana, and Iowa) considering complete and partial in-season\nweather knowledge. Several ensemble models are designed using blocked\nsequential procedure to generate out-of-bag predictions. The forecasts are made\nin county-level scale and aggregated for agricultural district, and state level\nscales. Results show that ensemble models based on weighted average of the base\nlearners outperform individual models. Specifically, the proposed ensemble\nmodel could achieve best prediction accuracy (RRMSE of 7.8%) and least mean\nbias error (-6.06 bu/acre) compared to other developed models. Comparing our\nproposed model forecasts with the literature demonstrates the superiority of\nforecasts made by our proposed ensemble model. Results from the scenario of\nhaving partial in-season weather knowledge reveal that decent yield forecasts\ncan be made as early as June 1st. To find the marginal effect of each input\nfeature on the forecasts made by the proposed ensemble model, a methodology is\nsuggested that is the basis for finding feature importance for the ensemble\nmodel. The findings suggest that weather features corresponding to weather in\nweeks 18-24 (May 1st to June 1st) are the most important input features.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 03:55:20 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 18:20:59 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Shahhosseini", "Mohsen", ""], ["Hu", "Guiping", ""], ["Archontoulis", "Sotirios V.", ""]]}, {"id": "2001.09097", "submitter": "Edward Wheatcroft", "authors": "Edward Wheatcroft", "title": "Forecasting football matches by predicting match statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the use of observed and predicted match statistics as\ninputs to forecasts of the outcomes of football matches. It is shown that, were\nit possible to know the match statistics in advance, highly informative\nforecasts of the match outcome could be made. Whilst, in practice, match\nstatistics are never available prior to the match, this leads to a simple\nphilosophy. If match statistics can be predicted pre-match, and if those\npredictions are accurate enough, it follows that informative match forecasts\ncan be made. It is shown that Generalised Attacking Performance (GAP) ratings,\nintroduced in a recent paper, provide a suitable methodology for predicting\nmatch statistics in advance and that they are informative enough to provide\ninformation beyond that reflected in the odds. A long term and robust gambling\nprofit is demonstrated when the forecasts are combined with two betting\nstrategies.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 17:05:18 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wheatcroft", "Edward", ""]]}, {"id": "2001.09191", "submitter": "Atma Bharathi Mani", "authors": "Atma Bharathi Mani, Ramanathan Sugumaran", "title": "Estimation of Building Rooftop Temperature from High Spatial Resolution\n  Aerial Thermal Images", "comments": "6 pages, 4 figures, AAG 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This letter presents a novel technique to calculate temperatures of building\nrooftops and other impervious surfaces from high spatial resolution aerial\nthermal images. In this study, we collected aerial radiance images of 30cm\nspatial resolution using a FLIR Phoenix imager in long-wave and mid-wave\ninfrared wavelengths for the city of Cedar Falls, USA to estimate building roof\ntemperature loss. Simultaneous ground temperature measurements were made at\npre-selected ground targets and roofs using 9 Fluke 561r infrared thermometers.\nAtmospheric correction of aerial images was performed by Empirical Line\nCalibration (ELC) method. The resulting ground-leaving radiances were corrected\nfor the emissivity of different roof types and the true kinetic temperature of\nthe building roofs was calculated. The ELC model was observed to perform better\nwhen only impervious surface targets were used for the regression. With an\nR2=0.71 for ELC, the method produced a root mean squared error of 0.74{\\deg}C\nfor asphalt roofs. Further, we observed that the microclimate plays a\nsignificant role while synchronizing aerial and ground measurements.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 05:56:43 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Mani", "Atma Bharathi", ""], ["Sugumaran", "Ramanathan", ""]]}, {"id": "2001.09359", "submitter": "Jing Wu", "authors": "Jing Wu, Anna L. Smith, Tian Zheng", "title": "Diagnostics and Visualization of Point Process Models for Event Times on\n  a Social Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point process models have been used to analyze interaction event times on a\nsocial network, in the hope to provides valuable insights for social science\nresearch. However, the diagnostics and visualization of the modeling results\nfrom such an analysis have received limited discussion in the literature. In\nthis paper, we develop a systematic set of diagnostic tools and visualizations\nfor point process models fitted to data from a network setting. We analyze the\nresidual process and Pearson residual on the network by inspecting their\nstructure and clustering structure. Equipped with these tools, we can validate\nwhether a model adequately captures the temporal and/or network structures in\nthe observed data. The utility of our approach is demonstrated using simulation\nstudies and point process models applied to a study of animal social\ninteractions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 20:23:54 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wu", "Jing", ""], ["Smith", "Anna L.", ""], ["Zheng", "Tian", ""]]}, {"id": "2001.09456", "submitter": "Francesco Sanna Passino", "authors": "Francesco Sanna Passino, Melissa J. M. Turcotte, Nicholas A. Heard", "title": "Graph link prediction in computer networks using Poisson matrix\n  factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph link prediction is an important task in cyber-security: relationships\nbetween entities within a computer network, such as users interacting with\ncomputers, or system libraries and the corresponding processes that use them,\ncan provide key insights into adversary behaviour. Poisson matrix factorisation\n(PMF) is a popular model for link prediction in large networks, particularly\nuseful for its scalability. In this article, PMF is extended to include\nscenarios that are commonly encountered in cyber-security applications.\nSpecifically, an extension is proposed to explicitly handle binary adjacency\nmatrices and include known categorical covariates associated with the graph\nnodes. A seasonal PMF model is also presented to handle seasonal networks. To\nallow the methods to scale to large graphs, variational methods are discussed\nfor performing fast inference. The results show an improved performance over\nthe standard PMF model and other statistical network models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 14:06:37 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 21:08:12 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 16:53:06 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Passino", "Francesco Sanna", ""], ["Turcotte", "Melissa J. M.", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "2001.09619", "submitter": "Irandokht Parviziomran", "authors": "Irandokht Parviziomran, Shun Cao, Krishnaswami Srihari, and Daehan Won", "title": "Data-Driven Prediction Model of Components Shift during Reflow Process\n  in Surface Mount Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surface mount technology (SMT), mounted components on soldered pads are\nsubject to move during reflow process. This capability is known as\nself-alignment and is the result of fluid dynamic behaviour of molten solder\npaste. This capability is critical in SMT because inaccurate self-alignment\ncauses defects such as overhanging, tombstoning, etc. while on the other side,\nit can enable components to be perfectly self-assembled on or near the desire\nposition. The aim of this study is to develop a machine learning model that\npredicts the components movement during reflow in x and y-directions as well as\nrotation. Our study is composed of two steps: (1) experimental data are studied\nto reveal the relationships between self-alignment and various factors\nincluding component geometry, pad geometry, etc. (2) advanced machine learning\nprediction models are applied to predict the distance and the direction of\ncomponents shift using support vector regression (SVR), neural network (NN),\nand random forest regression (RFR). As a result, RFR can predict components\nshift with the average fitness of 99%, 99%, and 96% and with average prediction\nerror of 13.47 (um), 12.02 (um), and 1.52 (deg.) for component shift in x, y,\nand rotational directions, respectively. This enhancement provides the future\ncapability of the parameters' optimization in the pick and placement machine to\ncontrol the best placement location and minimize the intrinsic defects caused\nby the self-alignment.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 08:00:23 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Parviziomran", "Irandokht", ""], ["Cao", "Shun", ""], ["Srihari", "Krishnaswami", ""], ["Won", "Daehan", ""]]}, {"id": "2001.09698", "submitter": "Ehtesham Iqbal", "authors": "Ehtesham Iqbal, Risha Govind, Alvin Romero, Olubanke Dzahini, Matthew\n  Broadbent, Robert Stewart, Tanya Smith, Chi-Hun Kim, Nomi Werbeloff, Richard\n  Dobson and Zina Ibrahim", "title": "The side effect profile of Clozapine in real world data of three large\n  mental hospitals", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0243437", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Mining the data contained within Electronic Health Records (EHRs)\ncan potentially generate a greater understanding of medication effects in the\nreal world, complementing what we know from Randomised control trials (RCTs).\nWe Propose a text mining approach to detect adverse events and medication\nepisodes from the clinical text to enhance our understanding of adverse effects\nrelated to Clozapine, the most effective antipsychotic drug for the management\nof treatment-resistant schizophrenia, but underutilised due to concerns over\nits side effects. Material and Methods: We used data from de-identified EHRs of\nthree mental health trusts in the UK (>50 million documents, over 500,000\npatients, 2835 of which were prescribed Clozapine). We explored the prevalence\nof 33 adverse effects by age, gender, ethnicity, smoking status and admission\ntype three months before and after the patients started Clozapine treatment. We\ncompared the prevalence of adverse effects with those reported in the Side\nEffects Resource (SIDER) where possible. Results: Sedation, fatigue, agitation,\ndizziness, hypersalivation, weight gain, tachycardia, headache, constipation\nand confusion were amongst the highest recorded Clozapine adverse effect in the\nthree months following the start of treatment. Higher percentages of all\nadverse effects were found in the first month of Clozapine therapy. Using a\nsignificance level of (p< 0.05) out chi-square tests show a significant\nassociation between most of the ADRs in smoking status and hospital admissions\nand some in gender and age groups. Further, the data was combined from three\ntrusts, and chi-square tests were applied to estimate the average effect of\nADRs in each monthly interval. Conclusion: A better understanding of how the\ndrug works in the real world can complement clinical trials and precision\nmedicine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:21:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Iqbal", "Ehtesham", ""], ["Govind", "Risha", ""], ["Romero", "Alvin", ""], ["Dzahini", "Olubanke", ""], ["Broadbent", "Matthew", ""], ["Stewart", "Robert", ""], ["Smith", "Tanya", ""], ["Kim", "Chi-Hun", ""], ["Werbeloff", "Nomi", ""], ["Dobson", "Richard", ""], ["Ibrahim", "Zina", ""]]}, {"id": "2001.09735", "submitter": "Homayoun Valafar", "authors": "Nicholas Boltin, Daniel Vu, Bethany Janos, Alyssa Shofner, Joan\n  Culley, Homayoun Valafar", "title": "An AI model for Rapid and Accurate Identification of Chemical Agents in\n  Mass Casualty Incidents", "comments": "7 pages, published in HIMS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we examine the effectiveness of WISER in identification of a\nchemical culprit during a chemical based Mass Casualty Incident (MCI). We also\nevaluate and compare Binary Decision Tree (BDT) and Artificial Neural Networks\n(ANN) using the same experimental conditions as WISER. The reverse engineered\nset of Signs/Symptoms from the WISER application was used as the training set\nand 31,100 simulated patient records were used as the testing set. Three sets\nof simulated patient records were generated by 5%, 10% and 15% perturbation of\nthe Signs/Symptoms of each chemical record. While all three methods achieved a\n100% training accuracy, WISER, BDT and ANN produced performances in the range\nof: 1.8%-0%, 65%-26%, 67%-21% respectively. A preliminary investigation of\ndimensional reduction using ANN illustrated a dimensional collapse from 79\nvariables to 40 with little loss of classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:08:41 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Boltin", "Nicholas", ""], ["Vu", "Daniel", ""], ["Janos", "Bethany", ""], ["Shofner", "Alyssa", ""], ["Culley", "Joan", ""], ["Valafar", "Homayoun", ""]]}, {"id": "2001.09764", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan and Ioanna Panagiotou and Willow Livengood and Robert\n  Kane and Andrew Cohen", "title": "Perfecting the Crime Machine", "comments": "11 pages, 55 figures, fixed typos, added references in Introduction\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study explores using different machine learning techniques and workflows\nto predict crime related statistics, specifically crime type in Philadelphia.\nWe use crime location and time as main features, extract different features\nfrom the two features that our raw data has, and build models that would work\nwith large number of class labels. We use different techniques to extract\nvarious features including combining unsupervised learning techniques and try\nto predict the crime type. Some of the models that we use are Support Vector\nMachines, Decision Trees, Random Forest, K-Nearest Neighbors. We report that\nthe Random Forest as the best performing model to predict crime type with an\nerror log loss of 2.3120.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 23:25:40 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 21:13:15 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alparslan", "Yigit", ""], ["Panagiotou", "Ioanna", ""], ["Livengood", "Willow", ""], ["Kane", "Robert", ""], ["Cohen", "Andrew", ""]]}, {"id": "2001.09902", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Zahra Khalilzadeh and Lizhi Wang", "title": "Predicting Yield Performance of Parents in Plant Breeding: A Neural\n  Collaborative Filtering Approach", "comments": "13 pages, 4 figures", "journal-ref": "PLoS ONE 15(5): e0233382 (2020)", "doi": "10.1371/journal.pone.0233382", "report-no": null, "categories": "cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental corn hybrids are created in plant breeding programs by crossing\ntwo parents, so-called inbred and tester, together. Identification of best\nparent combinations for crossing is challenging since the total number of\npossible cross combinations of parents is large and it is impractical to test\nall possible cross combinations due to limited resources of time and budget. In\nthe 2020 Syngenta Crop Challenge, Syngenta released several large datasets that\nrecorded the historical yield performances of around 4% of total cross\ncombinations of 593 inbreds with 496 testers which were planted in 280\nlocations between 2016 and 2018 and asked participants to predict the yield\nperformance of cross combinations of inbreds and testers that have not been\nplanted based on the historical yield data collected from crossing other\ninbreds and testers. In this paper, we present a collaborative filtering method\nwhich is an ensemble of matrix factorization method and neural networks to\nsolve this problem. Our computational results suggested that the proposed model\nsignificantly outperformed other models such as LASSO, random forest (RF), and\nneural networks. Presented method and results were produced within the 2020\nSyngenta Crop Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 16:39:45 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 00:10:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Khaki", "Saeed", ""], ["Khalilzadeh", "Zahra", ""], ["Wang", "Lizhi", ""]]}, {"id": "2001.09930", "submitter": "Xiaotong Jiang", "authors": "Xiaotong Jiang, Amanda E. Nelson, Rebecca J. Cleveland, Daniel P.\n  Beavers, Todd A. Schwartz, Liubov Arbeeva, Carolina Alvarez, Leigh F.\n  Callahan, Stephen Messier, Richard Loeser, Michael R. Kosorok", "title": "Technical Background for \"A Precision Medicine Approach to Develop and\n  Internally Validate Optimal Exercise and Weight Loss Treatments for\n  Overweight and Obese Adults with Knee Osteoarthritis\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide additional statistical background for the methodology developed in\nthe clinical analysis of knee osteoarthritis in \"A Precision Medicine Approach\nto Develop and Internally Validate Optimal Exercise and Weight Loss Treatments\nfor Overweight and Obese Adults with Knee Osteoarthritis\" (Jiang et al. 2020).\nJiang et al. 2020 proposed a pipeline to learn optimal treatment rules with\nprecision medicine models and compared them with zero-order models with a\nZ-test. The model performance was based on value functions, a scalar that\npredicts the future reward of each decision rule. The jackknife (i.e.,\nleave-one-out cross validation) method was applied to estimate the value\nfunction and its variance of several outcomes in IDEA. IDEA is a randomized\nclinical trial studying three interventions (exercise (E), dietary weight loss\n(D), and D+E) on overweight and obese participants with knee osteoarthritis. In\nthis report, we expand the discussion and justification with additional\nstatistical background. We elaborate more on the background of precision\nmedicine, the derivation of the jackknife estimator of value function and its\nestimated variance, the consistency property of jackknife estimator, as well as\nadditional simulation results that reflect more of the performance of jackknife\nestimators. We recommend reading Jiang et al. 2020 for clinical application and\ninterpretation of the optimal ITR of knee osteoarthritis as well as the overall\nunderstanding of the pipeline and recommend using this article to understand\nthe underlying statistical derivation and methodology.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:50:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 21:48:55 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 20:20:44 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Jiang", "Xiaotong", ""], ["Nelson", "Amanda E.", ""], ["Cleveland", "Rebecca J.", ""], ["Beavers", "Daniel P.", ""], ["Schwartz", "Todd A.", ""], ["Arbeeva", "Liubov", ""], ["Alvarez", "Carolina", ""], ["Callahan", "Leigh F.", ""], ["Messier", "Stephen", ""], ["Loeser", "Richard", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "2001.09945", "submitter": "Ayca Altay", "authors": "Ayca Altay, Melike Baykal-G\\\"ursoy, Pernille Hemmer", "title": "Behavior Associations in Lone-Actor Terrorists", "comments": "44 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrorist attacks carried out by individuals or single cells have\nsignificantly accelerated over the last 20 years. This type of terrorism,\ndefined as lone-actor (LA) terrorism, stands as one of the greatest security\nthreats of our time. Research on LA behavior and characteristics has emerged\nand accelerated over the last decade. While these studies have produced\nvaluable information on demographics, behavior, classifications, and warning\nsigns, the relationship among these characters are yet to be addressed.\nMoreover, the means of radicalization and attacking have changed over decades.\nThis study first identifies 25 binary behavioral characteristics of LAs and\nanalyzes 192 LAs recorded on three different databases. Next, the\nclassification is carried out according to first ideology, then to incident\nscene behavior via a virtual attacker-defender game, and, finally, according to\nthe clusters obtained from the data. In addition, within each class,\nstatistically significant associations and temporal relations are extracted\nusing the A-priori algorithm. These associations would be instrumental in\nidentifying the attacker type and intervene at the right time. The results\nindicate that while pre-9/11 LAs were mostly radicalized by the people in their\nenvironment, post-9/11 LAs are more diverse. Furthermore, the association\nchains for different LA types present unique characteristic pathways to\nviolence and after-attack behavior.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 18:05:52 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 20:07:28 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Altay", "Ayca", ""], ["Baykal-G\u00fcrsoy", "Melike", ""], ["Hemmer", "Pernille", ""]]}, {"id": "2001.10156", "submitter": "Mehrdad Yousefzadeh", "authors": "Rayan Kanfar, Obai Shaikh, Mehrdad Yousefzadeh, Tapan Mukerji", "title": "Real-Time Well Log Prediction From Drilling Data Using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.2523/IPTC-19693-MS", "report-no": null, "categories": "physics.geo-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to study the feasibility of predicting subsurface rock\nproperties in wells from real-time drilling data. Geophysical logs, namely,\ndensity, porosity and sonic logs are of paramount importance for subsurface\nresource estimation and exploitation. These wireline petro-physical\nmeasurements are selectively deployed as they are expensive to acquire;\nmeanwhile, drilling information is recorded in every drilled well. Hence a\npredictive tool for wireline log prediction from drilling data can help\nmanagement make decisions about data acquisition, especially for delineation\nand production wells. This problem is non-linear with strong ineractions\nbetween drilling parameters; hence the potential for deep learning to address\nthis problem is explored. We present a workflow for data augmentation and\nfeature engineering using Distance-based Global Sensitivity Analysis. We\npropose an Inception-based Convolutional Neural Network combined with a\nTemporal Convolutional Network as the deep learning model. The model is\ndesigned to learn both low and high frequency content of the data. 12 wells\nfrom the Equinor dataset for the Volve field in the North Sea are used for\nlearning. The model predictions not only capture trends but are also physically\nconsistent across density, porosity, and sonic logs. On the test data, the mean\nsquare error reaches a low value of 0.04 but the correlation coefficient\nplateaus around 0.6. The model is able however to differentiate between\ndifferent types of rocks such as cemented sandstone, unconsolidated sands, and\nshale.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 03:57:31 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Kanfar", "Rayan", ""], ["Shaikh", "Obai", ""], ["Yousefzadeh", "Mehrdad", ""], ["Mukerji", "Tapan", ""]]}, {"id": "2001.10171", "submitter": "Sourish Das", "authors": "Alka Yadav and Sourish Das and Anirban Chakraborti", "title": "Relationship between the melting Arctic Sea Ice Extent and North\n  Atlantic Oscillation Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic Sea Ice Extent (SIE) maintains the ocean circulation at the\nequilibrium and provides strong feedback in the earth's climate system. When\nthe Arctic Sea Ice melts in the summer, it results in the oceans absorbing and\nheating up the Arctic. As Arctic SIE is melting increasing rate, the oceans\nabsorb and heat up further. This contributes to rising sea surface temperature,\nwhich has a larger impact on global atmospheric pressure. Thus, the climate\nscientists are alarmed that global warming will cause the polar ice caps to\nmelt and that may lead to a \"critical instability\".\n  In our study, we construct a phase-space using the velocity and acceleration\nof the Arctic Sea Ice Extent (SIE) as two variables. From the data analysis, we\nshow that in recent decades the melting arctic SIE resulted in increasing\nphase-space volume, i.e., the phase-line distribution function has not been\nconstant along the trajectories. Our aim is to investigate the effect of the\nmelting Arctic SIE on the climate - particularly on the North Atlantic\nOscillation (NAO) index, a measure of variability in the atmospheric pressure\nat sea level. Based on a Granger causal model and a conservative Bootstrap\nstatistical test, we find that the changing phase-plane SIE does have a\nsignificant (at 0.01 percent) effect on the NAO index. It indicates melting SIE\nhas a significant effect on the changing weather pattern of the North Atlantic\nregion, especially in Europe and North America. In addition, we see that the\nyearly median of NAO is greater than the yearly average NAO, which indicates\nthat the distribution of NAO is negatively skewed, even though NAO follows\nnearly a mean zero stationary process. Our statistical study hints that we will\nsoon see a warmer climate in Eastern USA and Northern Europe. Naturally, a\nwarmer northern Europe would lead to a shrinking SIE, which can be a cause of\nalarm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 05:06:49 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Yadav", "Alka", ""], ["Das", "Sourish", ""], ["Chakraborti", "Anirban", ""]]}, {"id": "2001.10330", "submitter": "Nikolai Bode", "authors": "Nikolai Bode", "title": "Parameter Calibration in Crowd Simulation Models using Approximate\n  Bayesian Computation", "comments": "8 pages, 4 figures; preprint submitted to Proceedings of Pedestrian\n  and Evacuation Dynamics conference, August 21-24 2018, Lund, Sweden", "journal-ref": "Collective Dynamics, 5, 1-8 (2020)", "doi": "10.17815/CD.2020.68", "report-no": null, "categories": "cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation models for pedestrian crowds are a ubiquitous tool in research and\nindustry. It is crucial that the parameters of these models are calibrated\ncarefully and ultimately it will be of interest to compare competing models to\ndecide which model is best suited for a particular purpose. In this\ncontribution, I demonstrate how Approximate Bayesian Computation (ABC), which\nis already a popular tool in other areas of science, can be used for model\nfitting and model selection in a pedestrian dynamics context. I fit two\ndifferent models for pedestrian dynamics to data on a crowd passing in one\ndirection through a bottleneck. One model describes movement in\ncontinuous-space, the other model is a cellular automaton and thus describes\nmovement in discrete-space. In addition, I compare models to data using two\nmetrics. The first is based on egress times and the second on the velocity of\npedestrians in front of the bottleneck. My results show that while model\nfitting is successful, a substantial degree of uncertainty about the value of\nsome model parameters remains after model fitting. Importantly, the choice of\nmetric in model fitting can influence parameter estimates. Model selection is\ninconclusive for the egress time metric but supports the continuous-space model\nfor the velocity-based metric. These findings show that ABC is a flexible\napproach and highlight the difficulties associated with model fitting and model\nselection for pedestrian dynamics. ABC requires many simulation runs and\nchoosing appropriate metrics for comparing data to simulations requires careful\nattention. Despite this, I suggest ABC is a promising tool, because it is\nversatile and easily implemented for the growing number of openly available\ncrowd simulators and data sets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:08:53 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Bode", "Nikolai", ""]]}, {"id": "2001.10353", "submitter": "Eric Wolsztynski", "authors": "Eric Wolsztynski", "title": "Statistical Exploration of Relationships Between Routine and Agnostic\n  Features Towards Interpretable Risk Characterization", "comments": "This work was presented at the 2019 IEEE Medical Imaging Conference,\n  2 November, Manchester, UK. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As is typical in other fields of application of high throughput systems,\nradiology is faced with the challenge of interpreting increasingly\nsophisticated predictive models such as those derived from radiomics analyses.\nInterpretation may be guided by the learning output from machine learning\nmodels, which may however vary greatly with each technique. Whatever this\noutput model, it will raise some essential questions. How do we interpret the\nprognostic model for clinical implementation? How can we identify potential\ninformation structures within sets of radiomic features, in order to create\nclinically interpretable models? And how can we recombine or exploit potential\nrelationships between features towards improved interpretability? A number of\nstatistical techniques are explored to assess (possibly nonlinear)\nrelationships between radiological features from different angles.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:27:09 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wolsztynski", "Eric", ""]]}, {"id": "2001.10440", "submitter": "Ali J. Ghandour", "authors": "Ali J. Ghandour, Huda Hammoud and Samar Al-Hajj", "title": "Analyzing Factors Associated with Fatal Road Crashes: A Machine Learning\n  Approach", "comments": null, "journal-ref": null, "doi": "10.3390/ijerph17114111", "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road traffic injury accounts for a substantial human and economic burden\nglobally. Understanding risk factors contributing to fatal injuries is of\nparamount importance. In this study, we proposed a model that adopts a hybrid\nensemble machine learning classifier structured from sequential minimal\noptimization and decision trees to identify risk factors contributing to fatal\nroad injuries. The model was constructed, trained, tested, and validated using\nthe Lebanese Road Accidents Platform (LRAP) database of 8482 road crash\nincidents, with fatality occurrence as the outcome variable. A sensitivity\nanalysis was conducted to examine the influence of multiple factors on fatality\noccurrence. Seven out of the nine selected independent variables were\nsignificantly associated with fatality occurrence, namely, crash type, injury\nseverity, spatial cluster-ID, and crash time (hour). Evidence gained from the\nmodel data analysis will be adopted by policymakers and key stakeholders to\ngain insights into major contributing factors associated with fatal road\ncrashes and to translate knowledge into safety programs and enhanced road\npolicies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 07:00:49 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:28:00 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ghandour", "Ali J.", ""], ["Hammoud", "Huda", ""], ["Al-Hajj", "Samar", ""]]}, {"id": "2001.10488", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "Statistical Consequences of Fat Tails: Real World Preasymptotics,\n  Epistemology, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT q-fin.RM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monograph investigates the misapplication of conventional statistical\ntechniques to fat tailed distributions and looks for remedies, when possible.\n  Switching from thin tailed to fat tailed distributions requires more than\n\"changing the color of the dress\". Traditional asymptotics deal mainly with\neither n=1 or $n=\\infty$, and the real world is in between, under of the \"laws\nof the medium numbers\" --which vary widely across specific distributions. Both\nthe law of large numbers and the generalized central limit mechanisms operate\nin highly idiosyncratic ways outside the standard Gaussian or Levy-Stable\nbasins of convergence.\n  A few examples:\n  + The sample mean is rarely in line with the population mean, with effect on\n\"naive empiricism\", but can be sometimes be estimated via parametric methods.\n  + The \"empirical distribution\" is rarely empirical.\n  + Parameter uncertainty has compounding effects on statistical metrics.\n  + Dimension reduction (principal components) fails.\n  + Inequality estimators (GINI or quantile contributions) are not additive and\nproduce wrong results.\n  + Many \"biases\" found in psychology become entirely rational under more\nsophisticated probability distributions\n  + Most of the failures of financial economics, econometrics, and behavioral\neconomics can be attributed to using the wrong distributions.\n  This book, the first volume of the Technical Incerto, weaves a narrative\naround published journal articles.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:45:55 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 18:12:31 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "2001.10519", "submitter": "Felipe Maia Polo", "authors": "Felipe Maia Polo", "title": "Skills to not fall behind in school", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent studies emphasize how important the role of cognitive and\nsocial-emotional skills can be in determining people's quality of life.\nAlthough skills are of great importance in many aspects, in this paper we will\nfocus our efforts to better understand the relationship between several types\nof skills with academic progress delay. Our dataset contains the same students\nin 2012 and 2017, and we consider that there was a academic progress delay for\na specific student if he or she progressed less than expected in school grades.\nOur methodology primarily includes the use of a Bayesian logistic regression\nmodel and our results suggest that both cognitive and social-emotional skills\nmay impact the conditional probability of falling behind in school, and the\nmagnitude of the impact between the two types of skills can be comparable.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:45:26 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Polo", "Felipe Maia", ""]]}, {"id": "2001.10661", "submitter": "Christopher Ren", "authors": "Alice M.S Durieux, Christopher X. Ren, Matthew T. Calef, Rick\n  Chartrand, Michael S. Warren", "title": "BUDD: Multi-modal Bayesian Updating Deforestation Detections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global phenomenon of forest degradation is a pressing issue with severe\nimplications for climate stability and biodiversity protection. In this work we\ngenerate Bayesian updating deforestation detection (BUDD) algorithms by\nincorporating Sentinel-1 backscatter and interferometric coherence with\nSentinel-2 normalized vegetation index data. We show that the algorithm\nprovides good performance in validation AOIs. We compare the effectiveness of\ndifferent combinations of the three data modalities as inputs into the BUDD\nalgorithm and compare against existing benchmarks based on optical imagery.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 01:57:04 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Durieux", "Alice M. S", ""], ["Ren", "Christopher X.", ""], ["Calef", "Matthew T.", ""], ["Chartrand", "Rick", ""], ["Warren", "Michael S.", ""]]}, {"id": "2001.10664", "submitter": "Yang Chen", "authors": "David E Jones, Robert N Trangucci, Yang Chen", "title": "Quantifying Observed Prior Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We distinguish two questions (i) how much information does the prior contain?\nand (ii) what is the effect of the prior? Several measures have been proposed\nfor quantifying effective prior sample size, for example Clarke [1996] and\nMorita et al. [2008]. However, these measures typically ignore the likelihood\nfor the inference currently at hand, and therefore address (i) rather than\n(ii). Since in practice (ii) is of great concern, Reimherr et al. [2014]\nintroduced a new class of effective prior sample size measures based on\nprior-likelihood discordance. We take this idea further towards its natural\nBayesian conclusion by proposing measures of effective prior sample size that\nnot only incorporate the general mathematical form of the likelihood but also\nthe specific data at hand. Thus, our measures do not average across datasets\nfrom the working model, but condition on the current observed data.\nConsequently, our measures can be highly variable, but we demonstrate that this\nis because the impact of a prior can be highly variable. Our measures are Bayes\nestimates of meaningful quantities and well communicate the extent to which\ninference is determined by the prior, or framed differently, the amount of\neffort saved due to having prior information. We illustrate our ideas through a\nnumber of examples including a Gaussian conjugate model (continuous\nobservations), a Beta-Binomial model (discrete observations), and a linear\nregression model (two unknown parameters). Future work on further developments\nof the methodology and an application to astronomy are discussed at the end.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:15:09 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Jones", "David E", ""], ["Trangucci", "Robert N", ""], ["Chen", "Yang", ""]]}, {"id": "2001.10954", "submitter": "Charles Houston", "authors": "C. Houston, B. Marchand, L. Engelbert, C. D. Cantwell", "title": "Reducing complexity and unidentifiability when modelling human atrial\n  cells", "comments": "15 pages, 6 figures, submitted to Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2019.0339", "report-no": null, "categories": "q-bio.QM stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of a cellular action potential in cardiac modelling have\nbecome increasingly complex, particularly in gating kinetics which control the\nopening and closing of individual ion channel currents. As cardiac models\nadvance towards use in personalised medicine to inform clinical\ndecision-making, it is critical to understand the uncertainty hidden in\nparameter estimates from their calibration to experimental data. This study\napplies approximate Bayesian computation to re-calibrate the gating kinetics of\nfour ion channels in two existing human atrial cell models to their original\ndatasets, providing a measure of uncertainty and indication of potential issues\nwith selecting a single unique value given the available experimental data. Two\napproaches are investigated to reduce the uncertainty present: re-calibrating\nthe models to a more complete dataset and using a less complex formulation with\nfewer parameters to constrain. The re-calibrated models are inserted back into\nthe full cell model to study the overall effect on the action potential. The\nuse of more complete datasets does not eliminate uncertainty present in\nparameter estimates. The less complex model, particularly for the fast sodium\ncurrent, gave a better fit to experimental data alongside lower parameter\nuncertainty and improved computational speed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 16:57:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Houston", "C.", ""], ["Marchand", "B.", ""], ["Engelbert", "L.", ""], ["Cantwell", "C. D.", ""]]}, {"id": "2001.10977", "submitter": "Xiaoli Liu", "authors": "Xiaoli Liu, Pan Hu, Zhi Mao, Po-Chih Kuo, Peiyao Li, Chao Liu, Jie Hu,\n  Deyu Li, Desen Cao, Roger G. Mark, Leo Anthony Celi, Zhengbo Zhang, Feihu\n  Zhou", "title": "Interpretable Machine Learning Model for Early Prediction of Mortality\n  in Elderly Patients with Multiple Organ Dysfunction Syndrome (MODS): a\n  Multicenter Retrospective Study and Cross Validation", "comments": "33 pages, 14 figures, 14 tables, article, Co-author: Xiaoli Liu and\n  Pan Hu, Co-correspondence: Feihu Zhou and Zhengbo Zhang", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Elderly patients with MODS have high risk of death and poor\nprognosis. The performance of current scoring systems assessing the severity of\nMODS and its mortality remains unsatisfactory. This study aims to develop an\ninterpretable and generalizable model for early mortality prediction in elderly\npatients with MODS. Methods: The MIMIC-III, eICU-CRD and PLAGH-S databases were\nemployed for model generation and evaluation. We used the eXtreme Gradient\nBoosting model with the SHapley Additive exPlanations method to conduct early\nand interpretable predictions of patients' hospital outcome. Three types of\ndata source combinations and five typical evaluation indexes were adopted to\ndevelop a generalizable model. Findings: The interpretable model, with optimal\nperformance developed by using MIMIC-III and eICU-CRD datasets, was separately\nvalidated in MIMIC-III, eICU-CRD and PLAGH-S datasets (no overlapping with\ntraining set). The performances of the model in predicting hospital mortality\nas validated by the three datasets were: AUC of 0.858, sensitivity of 0.834 and\nspecificity of 0.705; AUC of 0.849, sensitivity of 0.763 and specificity of\n0.784; and AUC of 0.838, sensitivity of 0.882 and specificity of 0.691,\nrespectively. Comparisons of AUC between this model and baseline models with\nMIMIC-III dataset validation showed superior performances of this model; In\naddition, comparisons in AUC between this model and commonly used clinical\nscores showed significantly better performance of this model. Interpretation:\nThe interpretable machine learning model developed in this study using fused\ndatasets with large sample sizes was robust and generalizable. This model\noutperformed the baseline models and several clinical scores for early\nprediction of mortality in elderly ICU patients. The interpretative nature of\nthis model provided clinicians with the ranking of mortality risk features.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:15:34 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Liu", "Xiaoli", ""], ["Hu", "Pan", ""], ["Mao", "Zhi", ""], ["Kuo", "Po-Chih", ""], ["Li", "Peiyao", ""], ["Liu", "Chao", ""], ["Hu", "Jie", ""], ["Li", "Deyu", ""], ["Cao", "Desen", ""], ["Mark", "Roger G.", ""], ["Celi", "Leo Anthony", ""], ["Zhang", "Zhengbo", ""], ["Zhou", "Feihu", ""]]}, {"id": "2001.11087", "submitter": "Hananeh Alambeigi", "authors": "Hananeh Alambeigi, Anthony D. McDonald, Srinivas R. Tankasala", "title": "Crash Themes in Automated Vehicles: A Topic Modeling Analysis of the\n  California Department of Motor Vehicles Automated Vehicle Crash Database", "comments": "Transportation Research Board 99th Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vehicle technology promises to reduce the societal impact of\ntraffic crashes. Early investigations of this technology suggest that\nsignificant safety issues remain during control transfers between the\nautomation and human drivers and automation interactions with the\ntransportation system. In order to address these issues, it is critical to\nunderstand both the behavior of human drivers during these events and the\nenvironments where they occur. This article analyzes automated vehicle crash\nnarratives from the California Department of Motor Vehicles automated vehicle\ncrash database to identify safety concerns and gaps between crash types and\ncurrent areas of focus in the current research. The database was analyzed using\nprobabilistic topic modeling of open-ended crash narratives. Topic modeling\nanalysis identified five themes in the database: driver-initiated transition\ncrashes, sideswipe crashes during left-side overtakes, and rear-end collisions\nwhile the vehicle was stopped at an intersection, in a turn lane, and when the\ncrash involved oncoming traffic. Many crashes represented by the\ndriver-initiated transitions topic were also associated with the side-swipe\ncollisions. A substantial portion of the side-swipe collisions also involved\nmotorcycles. These findings highlight previously raised safety concerns with\ntransitions of control and interactions between vehicles in automated mode and\nthe transportation social network. In response to these findings, future\nempirical work should focus on driver-initiated transitions, overtakes, silent\nfailures, complex traffic situations, and adverse driving environments. Beyond\nthis future work, the topic modeling analysis method may be used as a tool to\nmonitor emergent safety issues.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 20:53:01 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Alambeigi", "Hananeh", ""], ["McDonald", "Anthony D.", ""], ["Tankasala", "Srinivas R.", ""]]}, {"id": "2001.11214", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno, Damien Challet", "title": "Nonparametric sign prediction of high-dimensional correlation matrix\n  coefficients", "comments": null, "journal-ref": null, "doi": "10.1209/0295-5075/133/48001", "report-no": null, "categories": "q-fin.GN q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to predict which correlation matrix coefficients are\nlikely to change their signs in the future in the high-dimensional regime, i.e.\nwhen the number of features is larger than the number of samples per feature.\nThe stability of correlation signs, two-by-two relationships, is found to\ndepend on three-by-three relationships inspired by Heider social cohesion\ntheory in this regime. We apply our method to US and Hong Kong equities\nhistorical data to illustrate how the structure of correlation matrices\ninfluences the stability of the sign of its coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 08:45:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bongiorno", "Christian", ""], ["Challet", "Damien", ""]]}, {"id": "2001.11275", "submitter": "Marjan Qazvini", "authors": "Amir T. Payandeh Najafabadi, Marjan Qazvini, Reza Ofoghi", "title": "The Impact of Oil and Gold Prices Shock on Tehran Stock Exchange: A\n  Copula Approach", "comments": null, "journal-ref": "Iranian Journal of Economic Studies Vol. 1, No. 2, Fall 2012,\n  23-47", "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several researches that deal with the behavior of SEs and their\nrelationships with different economical factors. These range from papers\ndealing with this subject through econometrical procedures to statistical\nmethods known as copula. This article considers the impact of oil and gold\nprice on Tehran Stock Exchange market (TSE). Oil and gold are two factors that\nare essential for the economy of Iran and their price are determined in the\nglobal market. The model used in this study is ARIMA-Copula. We used data from\nJanuary 1998 to January 2011 as training data to find the appropriate model.\nThe cross validation of model is measured by data from January 2011 to June\n2011. We conclude that: (i) there is no significant direct relationship between\ngold price and the TSE index, but the TSE is indirectly influenced by gold\nprice through other factors such as oil; and (ii) the TSE is not independent of\nthe volatility in oil price and Clayton copula can describe such dependence\nstructure between TSE and the oil price. Based on the property of Clayton\ncopula, which has lower tail dependency, as the oil price drops, stock index\nfalls. This means that decrease in oil price has an adverse effect on Iranian\neconomy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 11:59:50 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Najafabadi", "Amir T. Payandeh", ""], ["Qazvini", "Marjan", ""], ["Ofoghi", "Reza", ""]]}, {"id": "2001.11365", "submitter": "Cameron Williams", "authors": "Cameron J. Williams, Kevin J. Wilson and Nina Wilson", "title": "A Comparison of Prior Elicitation Aggregation using the Classical Method\n  and SHELF", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12691", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective Bayesian prior distributions elicited from experts can be\naggregated together to form group priors. This paper compares aggregated priors\nformed by Equal Weight Aggregation, the Classical Method, and the Sheffield\nElicitation Framework to each other and individual expert priors, using an\nexpert elicitation carried out for a clinical trial. Aggregation methods and\nindividual expert prior distributions are compared using proper scoring rules\nto compare the informativeness and calibration of the distributions. The three\naggregation methods outperform the individual experts, and the Sheffield\nElicitation Framework performs best amongst them.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 14:37:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 15:40:01 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 12:47:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Williams", "Cameron J.", ""], ["Wilson", "Kevin J.", ""], ["Wilson", "Nina", ""]]}, {"id": "2001.11399", "submitter": "Bojan Kostic", "authors": "Bojan Kostic, Romain Crastes dit Sourd, Stephane Hess, Joachim\n  Scheiner, Christian Holz-Rau, Francisco C. Pereira", "title": "Uncovering life-course patterns with causal discovery and survival\n  analysis", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel approach and an exploratory study for modelling life event\nchoices and occurrence from a probabilistic perspective through causal\ndiscovery and survival analysis. Our approach is formulated as a bi-level\nproblem. In the upper level, we build the life events graph, using causal\ndiscovery tools. In the lower level, for the pairs of life events,\ntime-to-event modelling through survival analysis is applied to model\ntime-dependent transition probabilities. Several life events were analysed,\nsuch as getting married, buying a new car, child birth, home relocation and\ndivorce, together with the socio-demographic attributes for survival modelling,\nsome of which are age, nationality, number of children, number of cars and home\nownership. The data originates from a survey conducted in Dortmund, Germany,\nwith the questionnaire containing a series of retrospective questions about\nresidential and employment biography, travel behaviour and holiday trips, as\nwell as socio-economic characteristic. Although survival analysis has been used\nin the past to analyse life-course data, this is the first time that a bi-level\nmodel has been formulated. The inclusion of a causal discovery algorithm in the\nupper-level allows us to first identify causal relationships between\nlife-course events and then understand the factors that might influence\ntransition rates between events. This is very different from more classic\nchoice models where causal relationships are subject to expert interpretations\nbased on model results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:30:16 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Kostic", "Bojan", ""], ["Sourd", "Romain Crastes dit", ""], ["Hess", "Stephane", ""], ["Scheiner", "Joachim", ""], ["Holz-Rau", "Christian", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "2001.11425", "submitter": "Fei Ding", "authors": "Fei Ding, Shiyuan He, David E. Jones, Jianhua Z. Huang", "title": "Supervised Functional PCA with Covariate Dependent Mean and Covariance\n  Structure", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating covariate information into functional data analysis methods can\nsubstantially improve modeling and prediction performance. However, many\nfunctional data analysis methods do not make use of covariate or supervision\ninformation, and those that do often have high computational cost or assume\nthat only the scores are related to covariates, an assumption that is usually\nviolated in practice. In this article, we propose a functional data analysis\nframework that relates both the mean and covariance function to covariate\ninformation. To facilitate modeling and ensure the covariance function is\npositive semi-definite, we represent it using splines and design a map from\nEuclidean space to the symmetric positive semi-definite matrix manifold. Our\nmodel is combined with a roughness penalty to encourage smoothness of the\nestimated functions in both the temporal and covariate domains. We also develop\nan efficient method for fast evaluation of the objective and gradient\nfunctions. Cross-validation is used to choose the tuning parameters. We\ndemonstrate the advantages of our approach through a simulation study and an\nastronomical data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:08:35 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ding", "Fei", ""], ["He", "Shiyuan", ""], ["Jones", "David E.", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2001.11461", "submitter": "Jonathan Bright", "authors": "Jonathan Bright, Nahema Marchal, Bharath Ganesh, Stevan Rudinac", "title": "Echo Chambers Exist! (But They're Full of Opposing Views)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of echo chambers, which suggests that online political discussions\ntake place in conditions of ideological homogeneity, has recently gained\npopularity as an explanation for patterns of political polarization and\nradicalization observed in many democratic countries. However, while\nmicro-level experimental work has shown evidence that individuals may gravitate\ntowards information that supports their beliefs, recent macro-level studies\nhave cast doubt on whether this tendency generates echo chambers in practice,\ninstead suggesting that cross-cutting exposures are a common feature of digital\nlife. In this article, we offer an explanation for these diverging results.\nBuilding on cognitive dissonance theory, and making use of observational trace\ndata taken from an online white nationalist website, we explore how individuals\nin an ideological 'echo chamber' engage with opposing viewpoints. We show that\nthis type of exposure, far from being detrimental to radical online\ndiscussions, is actually a core feature of such spaces that encourages people\nto stay engaged. The most common 'echoes' in this echo chamber are in fact the\nsound of opposing viewpoints being undermined and marginalized. Hence echo\nchambers exist not only in spite of but thanks to the unifying presence of\noppositional viewpoints. We conclude with reflections on policy implications of\nour study for those seeking to promote a more moderate political internet.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 17:14:57 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bright", "Jonathan", ""], ["Marchal", "Nahema", ""], ["Ganesh", "Bharath", ""], ["Rudinac", "Stevan", ""]]}, {"id": "2001.11473", "submitter": "Gonzalo Rios", "authors": "Gonzalo Rios", "title": "Transport Gaussian Processes for Regression", "comments": "19 pages, 2 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) priors are non-parametric generative models with\nappealing modelling properties for Bayesian inference: they can model\nnon-linear relationships through noisy observations, have closed-form\nexpressions for training and inference, and are governed by interpretable\nhyperparameters. However, GP models rely on Gaussianity, an assumption that\ndoes not hold in several real-world scenarios, e.g., when observations are\nbounded or have extreme-value dependencies, a natural phenomenon in physics,\nfinance and social sciences. Although beyond-Gaussian stochastic processes have\ncaught the attention of the GP community, a principled definition and rigorous\ntreatment is still lacking. In this regard, we propose a methodology to\nconstruct stochastic processes, which include GPs, warped GPs, Student-t\nprocesses and several others under a single unified approach. We also provide\nformulas and algorithms for training and inference of the proposed models in\nthe regression problem. Our approach is inspired by layers-based models, where\neach proposed layer changes a specific property over the generated stochastic\nprocess. That, in turn, allows us to push-forward a standard Gaussian white\nnoise prior towards other more expressive stochastic processes, for which\nmarginals and copulas need not be Gaussian, while retaining the appealing\nproperties of GPs. We validate the proposed model through experiments with\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 17:44:21 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Rios", "Gonzalo", ""]]}, {"id": "2001.11502", "submitter": "Noslen Hernandez", "authors": "Noslen Hern\\'andez, Aline Duarte, Guilherme Ost, Ricardo Fraiman,\n  Antonio Galves and Claudia D. Vargas", "title": "Retrieving the structure of probabilistic sequences of auditory stimuli\n  from EEG data", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a new probabilistic approach we model the relationship between\nsequences of auditory stimuli generated by stochastic chains and the\nelectroencephalographic (EEG) data acquired while 19 participants were exposed\nto those stimuli. The structure of the chains generating the stimuli are\ncharacterized by rooted and labeled trees whose leaves, henceforth called\ncontexts, represent the sequences of past stimuli governing the choice of the\nnext stimulus. A classical conjecture claims that the brain assigns\nprobabilistic models to samples of stimuli. If this is true, then the context\ntree generating the sequence of stimuli should be encoded in the brain\nactivity. Using an innovative statistical procedure we show that this context\ntree can effectively be extracted from the EEG data, thus giving support to the\nclassical conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 18:52:35 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 00:15:57 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hern\u00e1ndez", "Noslen", ""], ["Duarte", "Aline", ""], ["Ost", "Guilherme", ""], ["Fraiman", "Ricardo", ""], ["Galves", "Antonio", ""], ["Vargas", "Claudia D.", ""]]}, {"id": "2001.11552", "submitter": "Amir Karami", "authors": "Amir Karami, Cynthia Nicole White, Kayla Ford, Suzanne Swan, Melek\n  Yildiz Spinel", "title": "Unwanted Advances in Higher Education: Uncovering Sexual Harassment\n  Experiences in Academia with Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sexual harassment in academia is often a hidden problem because victims are\nusually reluctant to report their experiences. Recently, a web survey was\ndeveloped to provide an opportunity to share thousands of sexual harassment\nexperiences in academia. Using an efficient approach, this study collected and\ninvestigated more than 2,000 sexual harassment experiences to better understand\nthese unwanted advances in higher education. This paper utilized text mining to\ndisclose hidden topics and explore their weight across three variables:\nharasser gender, institution type, and victim's field of study. We mapped the\ntopics on five themes drawn from the sexual harassment literature and found\nthat more than 50% of the topics were assigned to the unwanted sexual attention\ntheme. Fourteen percent of the topics were in the gender harassment theme, in\nwhich insulting, sexist, or degrading comments or behavior was directed towards\nwomen. Five percent of the topics involved sexual coercion (a benefit is\noffered in exchange for sexual favors), 5% involved sex discrimination, and 7%\nof the topics discussed retaliation against the victim for reporting the\nharassment, or for simply not complying with the harasser. Findings highlight\nthe power differential between faculty and students, and the toll on students\nwhen professors abuse their power. While some topics did differ based on type\nof institution, there were no differences between the topics based on gender of\nharasser or field of study. This research can be beneficial to researchers in\nfurther investigation of this paper's dataset, and to policymakers in improving\nexisting policies to create a safe and supportive environment in academia.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:37:45 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Karami", "Amir", ""], ["White", "Cynthia Nicole", ""], ["Ford", "Kayla", ""], ["Swan", "Suzanne", ""], ["Spinel", "Melek Yildiz", ""]]}, {"id": "2001.11585", "submitter": "Geoff Boeing", "authors": "Geoff Boeing, Max Besbris, Ariela Schachter, John Kuk", "title": "Housing Search in the Age of Big Data: Smarter Cities or the Same Old\n  Blind Spots?", "comments": null, "journal-ref": null, "doi": "10.1080/10511482.2019.1684336", "report-no": null, "categories": "stat.AP cs.CY econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Housing scholars stress the importance of the information environment in\nshaping housing search behavior and outcomes. Rental listings have increasingly\nmoved online over the past two decades and, in turn, online platforms like\nCraigslist are now central to the search process. Do these technology platforms\nserve as information equalizers or do they reflect traditional information\ninequalities that correlate with neighborhood sociodemographics? We synthesize\nand extend analyses of millions of US Craigslist rental listings and find they\nsupply significantly different volumes, quality, and types of information in\ndifferent communities. Technology platforms have the potential to broaden,\ndiversify, and equalize housing search information, but they rely on landlord\nbehavior and, in turn, likely will not reach this potential without a\nsignificant redesign or policy intervention. Smart cities advocates hoping to\nbuild better cities through technology must critically interrogate technology\nplatforms and big data for systematic biases.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 22:10:29 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Boeing", "Geoff", ""], ["Besbris", "Max", ""], ["Schachter", "Ariela", ""], ["Kuk", "John", ""]]}, {"id": "2001.11685", "submitter": "Hao Yan", "authors": "Yujie Zhao, Hao Yan, Sarah E. Holte, Roxanne P. Kerani, Yajun Mei", "title": "Rapid Detection of Hot-spot by Tensor Decomposition with Application to\n  Weekly Gonorrhea Data", "comments": null, "journal-ref": "The XIIIth International Workshop on Intelligent Statistical\n  Quality Control, pp 289- 310, Hong Kong, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many bio-surveillance and healthcare applications, data sources are\nmeasured from many spatial locations repeatedly over time, say,\ndaily/weekly/monthly. In these applications, we are typically interested in\ndetecting hot-spots, which are defined as some structured outliers that are\nsparse over the spatial domain but persistent over time. In this paper, we\npropose a tensor decomposition method to detect when and where the hot-spots\noccur. Our proposed methods represent the observed raw data as a\nthree-dimensional tensor including a circular time dimension for\ndaily/weekly/monthly patterns, and then decompose the tensor into three\ncomponents: smooth global trend, local hot-spots, and residuals. A combination\nof LASSO and fused LASSO is used to estimate the model parameters, and a CUSUM\nprocedure is applied to detect when and where the hot-spots might occur. The\nusefulness of our proposed methodology is validated through numerical\nsimulation and a real-world dataset in the weekly number of gonorrhea cases\nfrom $2006$ to $2018$ for $50$ states in the United States.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 07:41:16 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 05:57:33 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhao", "Yujie", ""], ["Yan", "Hao", ""], ["Holte", "Sarah E.", ""], ["Kerani", "Roxanne P.", ""], ["Mei", "Yajun", ""]]}]