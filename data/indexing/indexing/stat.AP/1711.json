[{"id": "1711.00101", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and Kehui Chen", "title": "Nonparametric covariance estimation for mixed longitudinal studies, with\n  applications in midlife women's health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In mixed longitudinal studies, a group of subjects enter the study at\ndifferent ages (cross-sectional) and are followed for successive years\n(longitudinal). In the context of such studies, we consider nonparametric\ncovariance estimation with samples of noisy and partially observed functional\ntrajectories. The proposed algorithm is based on a noniterative\nsequential-aggregation scheme with only basic matrix operations and closed-form\nsolutions in each step. The good performance of the proposed method is\nsupported by both theory and numerical experiments. We also apply the proposed\nprocedure to a study on the working memory of midlife women, based on data from\nthe Study of Women's Health Across the Nation (SWAN).\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:42:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:30:28 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 22:17:09 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:07:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Chen", "Kehui", ""]]}, {"id": "1711.00157", "submitter": "Kyu Ha Lee", "authors": "Kyu Ha Lee, Brent A. Coull, Anna-Barbara Moscicki, Bruce J. Paster,\n  Jacqueline R. Starr", "title": "Bayesian Variable Selection for Multivariate Zero-Inflated Models:\n  Application to Microbiome Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microorganisms play critical roles in human health and disease. It is well\nknown that microbes live in diverse communities in which they interact\nsynergistically or antagonistically. Thus for estimating microbial associations\nwith clinical covariates, multivariate statistical models are preferred.\nMultivariate models allow one to estimate and exploit complex interdependencies\namong multiple taxa, yielding more powerful tests of exposure or treatment\neffects than application of taxon-specific univariate analyses. In addition,\nthe analysis of microbial count data requires special attention because data\ncommonly exhibit zero inflation. To meet these needs, we developed a Bayesian\nvariable selection model for multivariate count data with excess zeros that\nincorporates information on the covariance structure of the outcomes (counts\nfor multiple taxa), while estimating associations with the mean levels of these\noutcomes. Although there has been a great deal of effort in zero-inflated\nmodels for longitudinal data, little attention has been given to\nhigh-dimensional multivariate zero-inflated data modeled via a general\ncorrelation structure. Through simulation, we compared performance of the\nproposed method to that of existing univariate approaches, for both the binary\nand count parts of the model. When outcomes were correlated the proposed\nvariable selection method maintained type I error while boosting the ability to\nidentify true associations in the binary component of the model. For the count\npart of the model, in some scenarios the the univariate method had higher power\nthan the multivariate approach. This higher power was at a cost of a highly\ninflated false discovery rate not observed with the proposed multivariate\nmethod. We applied the approach to oral microbiome data from the Pediatric\nHIV/AIDS Cohort Oral Health Study and identified five species (of 44)\nassociated with HIV infection.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 01:34:02 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 16:01:08 GMT"}, {"version": "v3", "created": "Sun, 20 May 2018 16:20:10 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Lee", "Kyu Ha", ""], ["Coull", "Brent A.", ""], ["Moscicki", "Anna-Barbara", ""], ["Paster", "Bruce J.", ""], ["Starr", "Jacqueline R.", ""]]}, {"id": "1711.00197", "submitter": "Juan Pablo P\\'erez Monsalve", "authors": "Juan Pablo P\\'erez Monsalve and Freddy H. Mar\\'in Sanchez", "title": "Stochastic Modeling and Forecast of Hydrological Contributions in the\n  Colombian Electric System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper as show that hydrological contributions in the colombian\nelectrical system during the period between 2004 and 2016 have a periodic\ndynamic, with fundamental periods that are repeated every three years and that\ntend to oscillate around a long term average, in this context, such\ncontributions can be characterized by mean reversion stochastic processes with\nperiodic functional tendency. The objective of this paper is modeling and\nforecast the dynamic behavioral of hydrological contributions in the colombian\nelectric system. A description of climate and hydrology in Colombia is carried\nout, as well as an analysis of periodicity and distributional properties of the\ndata. A Gaussian estimation is performed, which allows to find all the constant\nand functional parameters from the historical data with daily frequency. The\nforecasts of the hydrological contributions are graphically illustrated for a\nperiod of three years and two alternatives to construct forecast fringes are\nproposed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 04:02:12 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Monsalve", "Juan Pablo P\u00e9rez", ""], ["Sanchez", "Freddy H. Mar\u00edn", ""]]}, {"id": "1711.00285", "submitter": "Anirudh Tomer", "authors": "Anirudh Tomer, Daan Nieboer, Monique J. Roobol, Ewout W. Steyerberg,\n  Dimitris Rizopoulos", "title": "Personalized Schedules for Surveillance of Low Risk Prostate Cancer\n  Patients", "comments": "16 pages, 5 figures, 1 table. Supplementary materials available at\n  https://goo.gl/jpPtL8", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low risk prostate cancer patients enrolled in active surveillance (AS)\nprograms commonly undergo biopsies on a frequent basis for examination of\ncancer progression. AS programs employ a fixed schedule of biopsies for all\npatients. Such fixed and frequent schedules, may schedule unnecessary biopsies\nfor the patients. Since biopsies have an associated risk of complications,\npatients do not always comply with the schedule, which increases the risk of\ndelayed detection of cancer progression. Motivated by the world's largest AS\nprogram, Prostate Cancer Research International Active Surveillance (PRIAS), in\nthis paper we present personalized schedules for biopsies to counter these\nproblems. Using joint models for time to event and longitudinal data, our\nmethods combine information from historical prostate-specific antigen (PSA)\nlevels and repeat biopsy results of a patient, to schedule the next biopsy. We\nalso present methods to compare personalized schedules with existing biopsy\nschedules.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:58:41 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Tomer", "Anirudh", ""], ["Nieboer", "Daan", ""], ["Roobol", "Monique J.", ""], ["Steyerberg", "Ewout W.", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1711.00303", "submitter": "Farkhondeh Sajadi Dr.", "authors": "Farkhondeh A. Sajadi", "title": "Assessing the reliability polynomial based on percolation theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST physics.soc-ph stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the robustness of network topologies. We use the\nconcept of percolation as measuring tool to assess the reliability polynomial\nof those systems which can be modeled as a general inhomogeneous random graph\nas well as scale-free random graph.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:05:46 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Sajadi", "Farkhondeh A.", ""]]}, {"id": "1711.00395", "submitter": "Adway Mitra", "authors": "Adway Mitra", "title": "Identifying Coherent Anomalies in Multi-Scale Spatio-Temporal Data using\n  Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many physical processes involve spatio-temporal observations, which can be\nstudied at different spatial and temporal scales. For example, rainfall data\nmeasured daily by rain gauges can be considered at daily, monthly or annual\ntemporal scales, and local, grid-wise, region-wise or country-wise spatial\nscales. In this work, we focus on detection of anomalies in such multi-scale\nspatio-temporal data. We consider an anomaly as an event where the measured\nvalues over a spatio-temporally extended region are significantly different\nfrom their long-term means. However we aim to avoid setting any thresholds on\nthe measured values and spatio-temporal sizes, because not only are thresholds\nsubjective but also the long-term mean values often vary spatially and\ntemporally. For this purpose we use spatio-Temporal Markov Random Field, where\nlatent states indicate anomaly type (positive anomaly, negative anomaly, no\nanomaly/normal). Spatio-temporal coherence is maintained through suitable edge\npotentials. The model is extended to multiple spatio-temporal scales to achieve\nour second goal: anomalies at any scale should be defined both on the data at\nthat scale, and also on anomalies at other scales. This allows us to trace an\nanomaly at a coarse scale to finer scales. For example, whether rainfall in a\nparticular year is anomalous over a region should depend not only on the total\nvolume of rainfall over the entire region, but also on whether there were such\nanomalies at the grid-scale, and the monthly scale. We use this approach to\nstudy rainfall anomalies over India -extremely diverse with respect to\nrainfall- for the period 1901-2011, and show its benefits over existing\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 10:30:00 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Mitra", "Adway", ""]]}, {"id": "1711.00421", "submitter": "Etienne Bonnassieux", "authors": "Etienne Bonnassieux, Cyril Tasse, Oleg Smirnov, Philippe Zarka", "title": "On the variance of radio interferometric calibration solutions:\n  Quality-based Weighting Schemes", "comments": "14 pages, 8 figures. Accepted", "journal-ref": "A&A 615, A66 (2018)", "doi": "10.1051/0004-6361/201732190", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the possibility of improving radio interferometric\nimages using an algorithm inspired by an optical method known as \"lucky\nimaging\", which would give more weight to the best-calibrated visibilities used\nto make a given image. A fundamental relationship between the statistics of\ninterferometric calibration solution residuals and those of the image-plane\npixels is derived in this paper. This relationship allows us to understand and\ndescribe the statistical properties of the residual image. In this framework,\nthe noise-map can be described as the Fourier transform of the covariance\nbetween residual visibilities in a new differential Fourier plane. Image-plane\nartefacts can be seen as one realisation of the pixel covariance distribution,\nwhich can be estimated from the antenna gain statistics. Based on this\nrelationship, we propose a means of improving images made with calibrated\nvisibilities using weighting schemes. This improvement would occur after\ncalibration, but before imaging - it is thus ideally used between major\niterations of self-calibration loops. Applying the weighting scheme to\nsimulated data improves the noise level in the final image at negligible\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:25:52 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 14:17:21 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bonnassieux", "Etienne", ""], ["Tasse", "Cyril", ""], ["Smirnov", "Oleg", ""], ["Zarka", "Philippe", ""]]}, {"id": "1711.00437", "submitter": "Claudio Fronterr\\`e", "authors": "Claudio Fronterr\\`e, Emanuele Giorgi, Peter J. Diggle", "title": "Geostatistical inference in the presence of geomasking: a\n  composite-likelihood approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In almost any geostatistical analysis, one of the underlying, often implicit,\nmodelling assump- tions is that the spatial locations, where measurements are\ntaken, are recorded without error. In this study we develop geostatistical\ninference when this assumption is not valid. This is often the case when, for\nexample, individual address information is randomly altered to provide pri-\nvacy protection or imprecisions are induced by geocoding processes and\nmeasurement devices. Our objective is to develop a method of inference based on\nthe composite likelihood that over- comes the inherent computational limits of\nthe full likelihood method as set out in Fanshawe and Diggle (2011). Through a\nsimulation study, we then compare the performance of our proposed approach with\nan N-weighted least squares estimation procedure, based on a corrected version\nof the empirical variogram. Our results indicate that the composite-likelihood\napproach outper- forms the latter, leading to smaller root-mean-square-errors\nin the parameter estimates. Finally, we illustrate an application of our method\nto analyse data on malnutrition from a Demographic and Health Survey conducted\nin Senegal in 2011, where locations were randomly perturbed to protect the\nprivacy of respondents.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:47:15 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fronterr\u00e8", "Claudio", ""], ["Giorgi", "Emanuele", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1711.00460", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Michael L. Stein", "title": "Locally stationary spatio-temporal interpolation of Argo profiling float\n  data", "comments": "28 pages, 8 figures, changes to the presentation throughout, some\n  material moved to the supplement, author version of the published paper", "journal-ref": "Proceedings of the Royal Society A 474: 20180400 (2018)", "doi": "10.1098/rspa.2018.0400", "report-no": null, "categories": "stat.AP physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argo floats measure seawater temperature and salinity in the upper 2,000 m of\nthe global ocean. Statistical analysis of the resulting spatio-temporal dataset\nis challenging due to its nonstationary structure and large size. We propose\nmapping these data using locally stationary Gaussian process regression where\ncovariance parameter estimation and spatio-temporal prediction are carried out\nin a moving-window fashion. This yields computationally tractable nonstationary\nanomaly fields without the need to explicitly model the nonstationary\ncovariance structure. We also investigate Student-$t$ distributed fine-scale\nvariation as a means to account for non-Gaussian heavy tails in ocean\ntemperature data. Cross-validation studies comparing the proposed approach with\nthe existing state-of-the-art demonstrate clear improvements in point\npredictions and show that accounting for the nonstationarity and\nnon-Gaussianity is crucial for obtaining well-calibrated uncertainties. This\napproach also provides data-driven local estimates of the spatial and temporal\ndependence scales for the global ocean which are of scientific interest in\ntheir own right.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:49:33 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 19:32:27 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 18:54:41 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kuusela", "Mikael", ""], ["Stein", "Michael L.", ""]]}, {"id": "1711.00484", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang, Amy Braverman, and Hai Nguyen", "title": "Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments", "comments": "Accepted version in Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:01:26 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 18:43:29 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""], ["Braverman", "Amy", ""], ["Nguyen", "Hai", ""]]}, {"id": "1711.00555", "submitter": "Qi Dong", "authors": "Jon Wakefield, Tracy Qi Dong, Vladimir N. Minin", "title": "Spatio-Temporal Analysis of Surveillance Data", "comments": "31 pages, 7 figures. This is an author-created preprint of a book\n  chapter to appear in the Handbook of Infectious Disease Data Analysis edited\n  by Leonhard Held, Niel Hens, Philip D O'Neill and Jacco Wallinga, Chapman and\n  Hall/CRC", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we consider space-time analysis of surveillance count data.\nSuch data are ubiquitous and a number of approaches have been proposed for\ntheir analysis. We first describe the aims of a surveillance endeavor, before\nreviewing and critiquing a number of common models. We focus on models in which\ntime is discretized to the time scale of the latent and infectious periods of\nthe disease under study. In particular, we focus on the time series SIR (TSIR)\nmodels originally described by Finkenstadt and Grenfell in their 2000 paper and\nthe epidemic/endemic models first proposed by Held, Hohle, and Hofmann in their\n2005 paper. We implement both of these models in the Stan software and\nillustrate their performance via analyses of measles data collected over a\n2-year period in 17 regions in the Weser-Ems region of Lower Saxony, Germany.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 22:43:35 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Wakefield", "Jon", ""], ["Dong", "Tracy Qi", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1711.00564", "submitter": "Gregor Kastner", "authors": "Martin Feldkircher, Florian Huber, Gregor Kastner", "title": "Sophisticated and small versus simple and sizeable: When does it pay off\n  to introduce drifting coefficients in Bayesian VARs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the relationship between model size and complexity in the\ntime-varying parameter VAR framework via thorough predictive exercises for the\nEuro Area, the United Kingdom and the United States. It turns out that\nsophisticated dynamics through drifting coefficients are important in small\ndata sets while simpler models tend to perform better in sizeable data sets. To\ncombine best of both worlds, novel shrinkage priors help to mitigate the curse\nof dimensionality, resulting in competitive forecasts for all scenarios\nconsidered. Furthermore, we discuss dynamic model selection to improve upon the\nbest performing individual model for each point in time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 23:34:11 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:45:40 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Kastner", "Gregor", ""]]}, {"id": "1711.00708", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 12:44:35 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "1711.00770", "submitter": "Marjan Cugmas", "authors": "Marjan Cugmas, Anu\\v{s}ka Ferligoj, Luka Kronegger", "title": "Scientific co-authorship networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the stability of the co-authorship networks in time. The\nanalysis is done on the networks of Slovenian researchers in two time periods\n(1991-2000 and 2001-2010). Two researchers are linked if they published at\nleast one scientific bibliographic unit in a given time period. As proposed by\nKronegger et al. (2011), the global network structures are examined by\ngeneralized blockmodeling with the assumed\nmulti-core--semi-periphery--periphery blockmodel type. The term core denotes a\ngroup of researchers who published together in a systematic way with each\nother.\n  The obtained blockmodels are comprehensively analyzed by visualizations and\nthrough considering several statistics regarding the global network structure.\nTo measure the stability of the obtained blockmodels, different adjusted\nmodified Rand and Wallace indices are applied. Those enable to distinguish\nbetween the splitting and merging of cores when operationalizing the stability\nof cores. Also, the adjusted modified indices can be used when new researchers\noccur in the second time period (newcomers) and when some researchers are no\nlonger present in the second time period (departures). The research disciplines\nare described and clustered according to the values of these indices.\nConsidering the obtained clusters, the sources of instability of the research\ndisciplines are studied (e.g., merging or splitting of cores, newcomers or\ndepartures). Furthermore, the differences in the stability of the obtained\ncores on the level of scientific disciplines are studied by linear regression\nanalysis where some personal characteristics of the researchers (e.g., age,\ngender), are also considered.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:54:20 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Cugmas", "Marjan", ""], ["Ferligoj", "Anu\u0161ka", ""], ["Kronegger", "Luka", ""]]}, {"id": "1711.00877", "submitter": "Zehang Li", "authors": "Zehang Richard Li, Tyler H. McCormick, Samuel J. Clark", "title": "Using Bayesian latent Gaussian graphical models to infer symptom\n  associations in verbal autopsies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning dependence relationships among variables of mixed types provides\ninsights in a variety of scientific settings and is a well-studied problem in\nstatistics. Existing methods, however, typically rely on copious, high quality\ndata to accurately learn associations. In this paper, we develop a method for\nscientific settings where learning dependence structure is essential, but data\nare sparse and have a high fraction of missing values. Specifically, our work\nis motivated by survey-based cause of death assessments known as verbal\nautopsies (VAs). We propose a Bayesian approach to characterize dependence\nrelationships using a latent Gaussian graphical model that incorporates\ninformative priors on the marginal distributions of the variables. We\ndemonstrate such information can improve estimation of the dependence\nstructure, especially in settings with little training data. We show that our\nmethod can be integrated into existing probabilistic cause-of-death assignment\nalgorithms and improves model performance while recovering dependence patterns\nbetween symptoms that can inform efficient questionnaire design in future data\ncollection.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:29:13 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 06:04:48 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 17:19:13 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2019 04:11:05 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Li", "Zehang Richard", ""], ["McCormick", "Tyler H.", ""], ["Clark", "Samuel J.", ""]]}, {"id": "1711.01092", "submitter": "Lars Siemer", "authors": "Lars Siemer and Wided Medjroubi", "title": "Cost-Optimal Operation of Energy Storage Units: Impact of Uncertainties\n  and Robust Estimator", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.DS physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid expansion of wind and solar energy leads to an increasing\nvolatility in the electricity generation. Previous studies have shown that\nstorage devices provide an opportunity to balance fluctuations in the power\ngrid. An economical operation of these devices is linked to solutions of\nprobabilistic optimization problems, due to the fact that future generation is\nnot deterministic in general. For this reason, reliable forecast methods as\nwell as appropriate robust optimization algorithms take an increasingly\nimportant role in future power operation systems. Taking an uncertain\navailability of electricity into account, we present a method to calculate\ncost-optimal charging strategies for energy storage units. The proposed method\nsolves subproblems which result from a sliding window approach applied on a\nlinear program by utilizing statistical measures. The prerequisite of this\nmethod is a recently developed fast algorithm for storage-related optimization\nproblems. To present the potential of the proposed method, a Power-To-Heat\nstorage system is investigated as an example using today's available forecast\ndata and a robust statistical measure. Second, the novel approach proposed here\nis compared with a common robust optimization method for stochastic scenario\nproblems. In comparison, the proposed method provides lower acquisition costs,\nespecially for today's available forecasts, and is more robust under\nperturbations in terms of deteriorating predictions, both based on empirical\nanalyses. Furthermore, the introduced approach is applicable to general\ncost-optimal operation problems and also real-time optimization concerning\nuncertain acquisition costs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 10:19:17 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Siemer", "Lars", ""], ["Medjroubi", "Wided", ""]]}, {"id": "1711.01241", "submitter": "Boyu Ren", "authors": "Boyu Ren, Sergio Bacallado, Stefano Favaro, Tommi Vatanen, Curtis\n  Huttenhower and Lorenzo Trippa", "title": "Bayesian Mixed Effects Models for Zero-inflated Compositions in\n  Microbiome Data Analysis", "comments": null, "journal-ref": null, "doi": "10.1214/19-AOAS1295", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting associations between microbial compositions and sample\ncharacteristics is one of the most important tasks in microbiome studies. Most\nof the existing methods apply univariate models to single microbial species\nseparately, with adjustments for multiple hypothesis testing. We propose a\nBayesian analysis for a generalized mixed effects linear model tailored to this\napplication. The marginal prior on each microbial composition is a Dirichlet\nProcess, and dependence across compositions is induced through a linear\ncombination of individual covariates, such as disease biomarkers or the\nsubject's age, and latent factors. The latent factors capture residual\nvariability and their dimensionality is learned from the data in a fully\nBayesian procedure. The proposed model is tested in data analyses and\nsimulation studies with zero-inflated compositions. In these settings, within\neach sample, a large proportion of counts per microbial species are equal to\nzero. In our Bayesian model a priori the probability of compositions with\nabsent microbial species is strictly positive. We propose an efficient\nalgorithm to sample from the posterior and visualizations of model parameters\nwhich reveal associations between covariates and microbial compositions. We\nevaluate the proposed method in simulation studies, and then analyze a\nmicrobiome dataset for infants with type 1 diabetes which contains a large\nproportion of zeros in the sample-specific microbial compositions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:08:39 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 23:16:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ren", "Boyu", ""], ["Bacallado", "Sergio", ""], ["Favaro", "Stefano", ""], ["Vatanen", "Tommi", ""], ["Huttenhower", "Curtis", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1711.01312", "submitter": "Martin Zhang", "authors": "Fei Xia, Martin J. Zhang, James Zou, David Tse", "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets grow richer, an important challenge is to leverage the full\nfeatures in the data to maximize the number of useful discoveries while\ncontrolling for false positives. We address this problem in the context of\nmultiple hypotheses testing, where for each hypothesis, we observe a p-value\nalong with a set of features specific to that hypothesis. For example, in\ngenetic association studies, each hypothesis tests the correlation between a\nvariant and the trait. We have a rich set of features for each variant (e.g.\nits location, conservation, epigenetics etc.) which could inform how likely the\nvariant is to have a true association. However popular testing approaches, such\nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting\n(IHW), either ignore these features or assume that the features are categorical\nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically\nlearns a discovery threshold as a function of all the hypothesis features. We\nparametrize the discovery threshold as a neural network, which enables flexible\nhandling of multi-dimensional discrete and continuous features as well as\nefficient end-to-end optimization. We prove that NeuralFDR has strong false\ndiscovery rate (FDR) guarantees, and show that it makes substantially more\ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the\nlearned discovery threshold is directly interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 19:27:11 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 08:22:26 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 06:01:26 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 20:44:38 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Xia", "Fei", ""], ["Zhang", "Martin J.", ""], ["Zou", "James", ""], ["Tse", "David", ""]]}, {"id": "1711.01318", "submitter": "David Jones", "authors": "David E. Jones, David C. Stenning, Eric B. Ford, Robert L. Wolpert,\n  Thomas J. Loredo, Christian Gilbertson, Xavier Dumusque", "title": "Improving Exoplanet Detection Power: Multivariate Gaussian Process\n  Models for Stellar Activity", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radial velocity method is one of the most successful techniques for\ndetecting exoplanets. It works by detecting the velocity of a host star induced\nby the gravitational effect of an orbiting planet, specifically the velocity\nalong our line of sight, which is called the radial velocity of the star.\nLow-mass planets typically cause their host star to move with radial velocities\nof 1 m/s or less. By analyzing a time series of stellar spectra from a host\nstar, modern astronomical instruments can in theory detect such planets.\nHowever, in practice, intrinsic stellar variability (e.g., star spots,\nconvective motion, pulsations) affects the spectra and often mimics a radial\nvelocity signal. This signal contamination makes it difficult to reliably\ndetect low-mass planets. A principled approach to recovering planet radial\nvelocity signals in the presence of stellar activity was proposed by Rajpaul et\nal. (2015). It uses a multivariate Gaussian process model to jointly capture\ntime series of the apparent radial velocity and multiple indicators of stellar\nactivity. We build on this work in two ways: (i) we propose using dimension\nreduction techniques to construct new high-information stellar activity\nindicators; and (ii) we extend the Rajpaul et al. (2015) model to a larger\nclass of models and use a power-based model comparison procedure to select the\nbest model. Despite significant interest in exoplanets, previous efforts have\nnot performed large-scale stellar activity model selection or attempted to\nevaluate models based on planet detection power. In the case of main sequence\nG2V stars, we find that our method substantially improves planet detection\npower compared to previous state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 20:00:59 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 20:11:54 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 01:17:35 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 04:43:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Jones", "David E.", ""], ["Stenning", "David C.", ""], ["Ford", "Eric B.", ""], ["Wolpert", "Robert L.", ""], ["Loredo", "Thomas J.", ""], ["Gilbertson", "Christian", ""], ["Dumusque", "Xavier", ""]]}, {"id": "1711.01335", "submitter": "Anna Ritz", "authors": "Zachary Campbell, Andrew Bray, Anna Ritz, Adam Groce", "title": "Differentially Private ANOVA Testing", "comments": "Accepted, camera-ready version presented at the 1st International\n  Conference on Data Intelligence and Security (ICDIS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern society generates an incredible amount of data about individuals, and\nreleasing summary statistics about this data in a manner that provably protects\nindividual privacy would offer a valuable resource for researchers in many\nfields. We present the first algorithm for analysis of variance (ANOVA) that\npreserves differential privacy, allowing this important statistical test to be\nconducted (and the results released) on databases of sensitive information. In\naddition to our private algorithm for the F test statistic, we show a rigorous\nway to compute p-values that accounts for the added noise needed to preserve\nprivacy. Finally, we present experimental results quantifying the statistical\npower of this differentially private version of the test, finding that a sample\nof several thousand observations is frequently enough to detect variation\nbetween groups. The differentially private ANOVA algorithm is a promising\napproach for releasing a common test statistic that is valuable in fields in\nthe sciences and social sciences.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:10:05 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 18:00:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Campbell", "Zachary", ""], ["Bray", "Andrew", ""], ["Ritz", "Anna", ""], ["Groce", "Adam", ""]]}, {"id": "1711.01378", "submitter": "Tomilayo Komolafe", "authors": "Tomilayo Komolafe and A. Valeria Quevedo, Srijan Sengupta, and William\n  H. Woodall", "title": "Statistical Evaluation of Spectral Methods for Anomaly Detection in\n  Networks", "comments": "39 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of networks for anomaly detection has attracted a lot of attention\nin recent years especially with the rise of connected devices and social\nnetworks. This is of importance as anomaly detection could span a wide range of\napplication, from detecting terrorist cells in counter-terrorism efforts to\nphishing attacks in social network circles. For this reason, numerous\ntechniques for anomaly detection have been introduced. However, application of\nthese techniques to more complex network models is hindered by various\nchallenges such as the size of the network being investigated, how much apriori\ninformation is needed, the size of the anomalous graph, among others. A recent\ntechnique introduced by Miller et al, which relies on a spectral framework for\nanomaly detection, has the potential to address many of these challenges. In\ntheir discussion of the spectral framework, three algorithms were proposed that\nrelied on the eigenvalues and eigenvectors of the residual matrix of a binary\nnetwork. The authors demonstrated the ability to detect anomalous subgraphs\nthat were less than 1% of the network size. However, to date, there is little\nwork that has been done to evaluate the statistical performance of these\nalgorithms. This study investigates the statistical properties of the spectral\nmethods, specifically the Chi-square and L1 norm algorithm proposed by Miller.\nWe will analyze the performance of the algorithm using simulated networks and\nalso extend the method's application to count networks. Finally we will make\nsome methodological improvements and recommendations to both algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 01:28:34 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 08:45:31 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 19:58:27 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 21:19:33 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Komolafe", "Tomilayo", ""], ["Quevedo", "A. Valeria", ""], ["Sengupta", "Srijan", ""], ["Woodall", "William H.", ""]]}, {"id": "1711.01512", "submitter": "Claudio Fuentes", "authors": "Jeremy T. Gaskins, Claudio Fuentes, and Rolando De la Cruz", "title": "A Bayesian Nonparametric Model for Predicting Pregnancy Outcomes Using\n  Longitudinal Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across several medical fields, developing an approach for disease\nclassification is an important challenge. The usual procedure is to fit a model\nfor the longitudinal response in the healthy population, a different model for\nthe longitudinal response in disease population, and then apply the Bayes'\ntheorem to obtain disease probabilities given the responses. Unfortunately,\nwhen substantial heterogeneity exists within each population, this type of\nBayes classification may perform poorly. In this paper, we develop a new\napproach by fitting a Bayesian nonparametric model for the joint outcome of\ndisease status and longitudinal response, and then use the clustering induced\nby the Dirichlet process in our model to increase the flexibility of the\nmethod, allowing for multiple subpopulations of healthy, diseased, and possibly\nmixed membership. In addition, we introduce an MCMC sampling scheme that\nfacilitates the assessment of the inference and prediction capabilities of our\nmodel. Finally, we demonstrate the method by predicting pregnancy outcomes\nusing longitudinal profiles on the $\\beta$--HCG hormone levels in a sample of\nChilean women being treated with assisted reproductive therapy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 01:18:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Gaskins", "Jeremy T.", ""], ["Fuentes", "Claudio", ""], ["De la Cruz", "Rolando", ""]]}, {"id": "1711.01570", "submitter": "Sarit Agami", "authors": "Sarit Agami and Robert J. Adler", "title": "Modeling of Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is a novel statistical technique,\nparticularly powerful for the analysis of large and high dimensional data sets.\nMuch of TDA is based on the tool of persistent homology, represented visually\nvia persistence diagrams. In an earlier paper we proposed a parametric\nrepresentation for the probability distributions of persistence diagrams, and\nbased on it provided a method for their replication. Since the typical\nsituation for big data is that only one persistence diagram is available, these\nreplications allow for conventional statistical inference, which, by its very\nnature, requires some form of replication. In the current paper we continue\nthis analysis, and further develop its practical statistical methodology, by\ninvestigating a wider class of examples than treated previously.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:08:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Agami", "Sarit", ""], ["Adler", "Robert J.", ""]]}, {"id": "1711.01583", "submitter": "Hossein Lolaee", "authors": "Hossein Lolaee and Mohammad Ali Akhaee", "title": "Robust Expectation-Maximization Algorithm for DOA Estimation of Acoustic\n  Sources in the Spherical Harmonic Domain", "comments": "10 pages, 9 figures, IEEE journal manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The direction of arrival (DOA) estimation of sound sources has been a popular\nsignal processing research topic due to its widespread applications. Using\nspherical microphone arrays (SMA), DOA estimation can be applied in the\nspherical harmonic (SH) domain without any spatial ambiguity. However, the\nenvironment reverberation and noise can degrade the estimation performance. In\nthis paper, we propose a new expectation maximization (EM) algorithm for\ndeterministic maximum likelihood (ML) DOA estimation of L sound sources in the\npresence of spatially nonuniform noise in the SH domain. Furthermore a new\nclosed-form Cramer-Rao bound (CRB) for the deterministic ML DOA estimation is\nderived for the signal model in the SH domain. The main idea of the proposed\nalgorithm is considering the general model of the received signal in the SH\ndomain, we reduce the complexity of the ML estimation by breaking it down into\ntwo steps: expectation and maximization steps. The proposed algorithm reduces\nthe complexity from 2L-dimensional space to L 2-dimensional space. Simulation\nresults indicate that the proposed algorithm shows at least an improvement of\n6dB in robustness in terms of root mean square error (RMSE). Moreover, the RMSE\nof the proposed algorithm is very close to the CRB compared to the recent\nmethods in reverberant and noisy environments in the large range of signal to\nnoise ratio.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 13:25:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lolaee", "Hossein", ""], ["Akhaee", "Mohammad Ali", ""]]}, {"id": "1711.01598", "submitter": "Xuan Bi", "authors": "Xuan Bi, Annie Qu and Xiaotong Shen", "title": "Multilayer tensor factorization with applications to recommender systems", "comments": "Accepted by the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been widely adopted by electronic commerce and\nentertainment industries for individualized prediction and recommendation,\nwhich benefit consumers and improve business intelligence. In this article, we\npropose an innovative method, namely the recommendation engine of multilayers\n(REM), for tensor recommender systems. The proposed method utilizes the\nstructure of a tensor response to integrate information from multiple modes,\nand creates an additional layer of nested latent factors to accommodate\nbetween-subjects dependency. One major advantage is that the proposed method is\nable to address the \"cold-start\" issue in the absence of information from new\ncustomers, new products or new contexts. Specifically, it provides more\neffective recommendations through sub-group information. To achieve scalable\ncomputation, we develop a new algorithm for the proposed method, which\nincorporates a maximum block improvement strategy into the cyclic\nblockwise-coordinate-descent algorithm. In theory, we investigate both\nalgorithmic properties for global and local convergence, along with the\nasymptotic consistency of estimated parameters. Finally, the proposed method is\napplied in simulations and IRI marketing data with 116 million observations of\nproduct sales. Numerical studies demonstrate that the proposed method\noutperforms existing competitors in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 14:40:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Bi", "Xuan", ""], ["Qu", "Annie", ""], ["Shen", "Xiaotong", ""]]}, {"id": "1711.01631", "submitter": "Di Claudio Elio D.", "authors": "Elio D. Di Claudio and Raffaele Parisi and Giovanni Jacovitti", "title": "Space Time MUSIC: Consistent Signal Subspace Estimation for Wide-band\n  Sensor Arrays", "comments": "15 pages, 10 figures. Accepted in a revised form by the IEEE Trans.\n  on Signal Processing on 12 February 1918. @IEEE2018", "journal-ref": null, "doi": "10.1109/TSP.2018.2811746", "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-band Direction of Arrival (DOA) estimation with sensor arrays is an\nessential task in sonar, radar, acoustics, biomedical and multimedia\napplications. Many state of the art wide-band DOA estimators coherently process\nfrequency binned array outputs by approximate Maximum Likelihood, Weighted\nSubspace Fitting or focusing techniques. This paper shows that bin signals\nobtained by filter-bank approaches do not obey the finite rank narrow-band\narray model, because spectral leakage and the change of the array response with\nfrequency within the bin create \\emph{ghost sources} dependent on the\nparticular realization of the source process. Therefore, existing DOA\nestimators based on binning cannot claim consistency even with the perfect\nknowledge of the array response. In this work, a more realistic array model\nwith a finite length of the sensor impulse responses is assumed, which still\nhas finite rank under a space-time formulation. It is shown that signal\nsubspaces at arbitrary frequencies can be consistently recovered under mild\nconditions by applying MUSIC-type (ST-MUSIC) estimators to the dominant\neigenvectors of the wide-band space-time sensor cross-correlation matrix. A\nnovel Maximum Likelihood based ST-MUSIC subspace estimate is developed in order\nto recover consistency. The number of sources active at each frequency are\nestimated by Information Theoretic Criteria. The sample ST-MUSIC subspaces can\nbe fed to any subspace fitting DOA estimator at single or multiple frequencies.\nSimulations confirm that the new technique clearly outperforms binning\napproaches at sufficiently high signal to noise ratio, when model mismatches\nexceed the noise floor.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 18:18:16 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 12:03:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Di Claudio", "Elio D.", ""], ["Parisi", "Raffaele", ""], ["Jacovitti", "Giovanni", ""]]}, {"id": "1711.02688", "submitter": "Zhongxiang Wang", "authors": "Zhongxiang Wang, Masoud Hamedi, Elham Sharifi, Stanley Young", "title": "A cross-vendor and cross-state analysis of the GPS-probe data latency", "comments": "This paper was submitted to TRB annual meeting 2018", "journal-ref": null, "doi": "10.1177/0361198118792768", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced GPS probe data has become a major source of real-time traffic\ninformation applications. In addition to traditional traveler advisory systems\nsuch as dynamic message signs (DMS) and 511 systems, probe data is being used\nfor automatic incident detection, Integrated Corridor Management (ICM), end of\nqueue warning systems, and mobility-related smartphone applications. Several\nprivate sector vendors offer minute by minute network-wide travel time and\nspeed probe data. The quality of such data in terms of deviation of the\nreported travel time and speeds from ground-truth has been extensively studied\nin recent years, and as a result concerns over the accuracy of probe data has\nmostly faded away. However, the latency of probe data, defined as the lag\nbetween the time that disturbance in traffic speed is reported in the\noutsourced data feed, and the time that the traffic is perturbed, has become a\nsubject of interest. The extent of latency of probe data for real-time\napplications is critical, so it is important to have a good understanding of\nthe amount of latency and its influencing factors. This paper uses high-quality\nindependent Bluetooth/Wi-Fi re-identification data collected on multiple\nfreeway segments in three different states, to measure the latency of the\nvehicle probe data provided by three major vendors. The statistical\ndistribution of the latency and its sensitivity to speed slowdown and recovery\nperiods are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 19:07:54 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 16:40:36 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wang", "Zhongxiang", ""], ["Hamedi", "Masoud", ""], ["Sharifi", "Elham", ""], ["Young", "Stanley", ""]]}, {"id": "1711.03202", "submitter": "Huan Lin", "authors": "Huan Lin, M. J. Caley and Scott A. Sisson", "title": "Estimating global species richness using symbolic data meta-analysis", "comments": "5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global species richness is a key biodiversity metric. Despite recent efforts\nto estimate global species richness, the resulting estimates have been highly\nuncertain and often logically inconsistent. Estimates lower down either the\ntaxonomic or geographic hierarchies are often larger than those above. Further,\nthese estimates have been represented in a wide variety of forms, including\nintervals (a, b), point estimates with no uncertainty, and point estimates with\neither symmetrical or asymmetrical bounds, making it difficult to combine\ninformation across different estimates. Here, we develop a Bayesian\nhierarchical approach to estimate the global species richness from published\nstudies. It allows us to recover interval estimates at each level of the\nhierarchy, even when data are partially or wholly unobserved, while respecting\nlogical constraints, and to determine the effects of estimation on the whole\nhierarchy of obtaining future estimates anywhere within it\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 23:04:29 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Lin", "Huan", ""], ["Caley", "M. J.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1711.03346", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Elizabeth P. Chou, Tzu-Wei Ko", "title": "Dimension Reduction of High-Dimensional Datasets Based on Stepwise SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study proposes a dimension reduction method, stepwise support\nvector machine (SVM), to reduce the dimensions of large p small n datasets. The\nproposed method is compared with other dimension reduction methods, namely, the\nPearson product difference correlation coefficient (PCCs), recursive feature\nelimination based on random forest (RF-RFE), and principal component analysis\n(PCA), by using five gene expression datasets. Additionally, the prediction\nperformance of the variables selected by our method is evaluated. The study\nfound that stepwise SVM can effectively select the important variables and\nachieve good prediction performance. Moreover, the predictions of stepwise SVM\nfor reduced datasets was better than those for the unreduced datasets. The\nperformance of stepwise SVM was more stable than that of PCA and RF-RFE, but\nthe performance difference with respect to PCCs was minimal. It is necessary to\nreduce the dimensions of large p small n datasets. We believe that stepwise SVM\ncan effectively eliminate noise in data and improve the prediction accuracy in\nany large p small n dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:16:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Chou", "Elizabeth P.", ""], ["Ko", "Tzu-Wei", ""]]}, {"id": "1711.03367", "submitter": "Dominik Liebl", "authors": "Dominik Liebl", "title": "Nonparametric Testing for Differences in Electricity Prices: The Case of\n  the Fukushima Nuclear Accident", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the problem of testing for differences in the mean\nelectricity prices before and after Germany's abrupt nuclear phaseout after the\nnuclear disaster in Fukushima Daiichi, Japan, in mid-March 2011. Taking into\naccount the nature of the data and the auction design of the electricity\nmarket, we approach this problem using a Local Linear Kernel (LLK) estimator\nfor the nonparametric mean function of sparse covariate-adjusted functional\ndata. We build upon recent theoretical work on the LLK estimator and propose a\ntwo-sample test statistics using a finite sample correction to avoid size\ndistortions. Our nonparametric test results on the price differences point to a\nSimpson's paradox explaining an unexpected result recently reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 13:28:49 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 12:18:43 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 10:38:01 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 13:03:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Liebl", "Dominik", ""]]}, {"id": "1711.03587", "submitter": "Walter Dempsey", "authors": "Walter Dempsey, Peng Liao, Santosh Kumar, Susan A. Murphy", "title": "The stratified micro-randomized trial design: sample size considerations\n  for testing nested causal effects of time-varying treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advancements in the field of mobile devices and wearable\nsensors have helped overcome obstacles in the delivery of care, making it\npossible to deliver behavioral treatments anytime and anywhere. Increasingly\nthe delivery of these treatments is triggered by predictions of risk or\nengagement which may have been impacted by prior treatments. Furthermore the\ntreatments are often designed to have an impact on individuals over a span of\ntime during which subsequent treatments may be provided.\n  Here we discuss our work on the design of a mobile health smoking cessation\nexperimental study in which two challenges arose. First the randomizations to\ntreatment should occur at times of stress and second the outcome of interest\naccrues over a period that may include subsequent treatment. To address these\nchallenges we develop the \"stratified micro-randomized trial,\" in which each\nindividual is randomized among treatments at times determined by predictions\nconstructed from outcomes to prior treatment and with randomization\nprobabilities depending on these outcomes. We define both conditional and\nmarginal proximal treatment effects. Depending on the scientific goal these\neffects may be defined over a period of time during which subsequent treatments\nmay be provided. We develop a primary analysis method and associated sample\nsize formulae for testing these effects.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 20:27:35 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Dempsey", "Walter", ""], ["Liao", "Peng", ""], ["Kumar", "Santosh", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1711.03623", "submitter": "Ines Wilms", "authors": "Ines Wilms, Sumanta Basu, Jacob Bien, David S. Matteson", "title": "Interpretable Vector AutoRegressions with Exogenous Time Series", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vector AutoRegressive (VAR) model is fundamental to the study of\nmultivariate time series. Although VAR models are intensively investigated by\nmany researchers, practitioners often show more interest in analyzing VARX\nmodels that incorporate the impact of unmodeled exogenous variables (X) into\nthe VAR. However, since the parameter space grows quadratically with the number\nof time series, estimation quickly becomes challenging. While several proposals\nhave been made to sparsely estimate large VAR models, the estimation of large\nVARX models is under-explored. Moreover, typically these sparse proposals\ninvolve a lasso-type penalty and do not incorporate lag selection into the\nestimation procedure. As a consequence, the resulting models may be difficult\nto interpret. In this paper, we propose a lag-based hierarchically sparse\nestimator, called \"HVARX\", for large VARX models. We illustrate the usefulness\nof HVARX on a cross-category management marketing application. Our results show\nhow it provides a highly interpretable model, and improves out-of-sample\nforecast accuracy compared to a lasso-type approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 22:15:33 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wilms", "Ines", ""], ["Basu", "Sumanta", ""], ["Bien", "Jacob", ""], ["Matteson", "David S.", ""]]}, {"id": "1711.03662", "submitter": "Juan Sosa", "authors": "Juan Sosa and Abel Rodriguez", "title": "A Latent Space Model for Cognitive Social Structures Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for modeling a set of directed, binary\nnetworks in the context of cognitive social structures (CSSs) data. We adopt a\nrelativist approach in which no assumption is made about the existence of an\nunderlying true network. More specifically, we rely on a generalized linear\nmodel that incorporates a bilinear structure to model transitivity effects\nwithin networks, and a hierarchical specification on the bilinear effects to\nborrow information across networks. This is a spatial model, in which the\nperception of each individual about the strength of the relationships can be\nexplained by the perceived position of the actors (themselves and others) on a\nlatent social space. A key goal of the model is to provide a mechanism to\nformally assess the agreement between each actors' perception of their own\nsocial roles with that of the rest of the group. Our experiments with both real\nand simulated data show that the capabilities of our model are comparable with\nor, even superior to, other models for CSS data reported in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 01:26:36 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:31:07 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 22:07:40 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Sosa", "Juan", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1711.03666", "submitter": "K.Shuvo Bakar", "authors": "K. Shuvo Bakar", "title": "Bayesian Gaussian models for interpolating large-dimensional data at\n  misaligned areal units", "comments": "9 pages, 2 figures and one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Areal level spatial data are often large, sparse and may appear with\ngeographical shapes that are regular or irregular (e.g., postcode). Moreover,\nsometimes it is important to obtain predictive inference in regular or\nirregular areal shapes that is misaligned with the observed spatial areal\ngeographical boundary. For example, in a survey the respondents were asked\nabout their postcode, however for policy making purposes, researchers are often\ninterested to obtain information at the SA2. The statistical challenge is to\nobtain spatial prediction at the SA2s, where the SA2s may have overlapped\ngeographical boundaries with postcodes. The study is motivated by a practical\nsurvey data obtained from the Australian National University (ANU) Poll. Here\nthe main research question is to understand respondents' satisfaction level\nwith the way Australia is heading. The data are observed at 1,944 postcodes\namong the 2,516 available postcodes across Australia, and prediction is\nobtained at the 2,196 SA2s. The proposed method also explored through a\ngrid-based simulation study, where data have been observed in a regular grid\nand spatial prediction has been done in a regular grid that has a misaligned\ngeographical boundary with the first regular grid-set. The real-life example\nwith ANU Poll data addresses the situation of irregular geographical boundaries\nthat are misaligned, i.e., model fitted with postcode data and hence obtained\nprediction at the SA2. A comparison study is also performed to validate the\nproposed method. In this paper, a Gaussian model is constructed under Bayesian\nhierarchy. The novelty lies in the development of the basis function that can\naddress spatial sparsity and localised spatial structure. It can also address\nthe large-dimensional spatial data modelling problem by constructing knot based\nreduced-dimensional basis functions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 01:46:17 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bakar", "K. Shuvo", ""]]}, {"id": "1711.03758", "submitter": "Sourabh Bhattacharya", "authors": "Noirrit Kiran Chandra, Richa Singh and Sourabh Bhattacharya", "title": "A Novel Bayesian Multiple Testing Approach to Deregulated miRNA\n  Discovery Harnessing Positional Clustering", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are small non-coding RNAs that function as regulators of\ngene expression. In recent years, there has been a tremendous and growing\ninterest among researchers to investigate the role of miRNAs in normal cellular\nas well as in disease processes. Thus to investigate the role of miRNAs in oral\ncancer, we analyse the expression levels of miRNAs to identify miRNAs with\nstatistically significant differential expression in cancer tissues.\n  In this article, we propose a novel Bayesian hierarchical model of miRNA\nexpression data. Compelling evidences have demonstrated that the transcription\nprocess of miRNAs in human genome is a latent process instrumental for the\nobserved expression levels. We take into account positional clustering of the\nmiRNAs in the analysis and model the latent transcription phenomenon\nnonparametrically by an appropriate Gaussian process.\n  For the testing purpose we employ a novel Bayesian multiple testing method\nwhere we mainly focus on utilizing the dependence structure between the\nhypotheses for better results, while also ensuring optimality in many respects.\nIndeed, our non-marginal method yielded results in accordance with the\nunderlying scientific knowledge which are found to be missed by the very\npopular Benjamini-Hochberg method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 10:17:09 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 18:00:47 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Singh", "Richa", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1711.03930", "submitter": "Jaehong Jeong", "authors": "Jaehong Jeong, Yuan Yan, Stefano Castruccio, Marc G. Genton", "title": "A Stochastic Generator of Global Monthly Wind Energy with Tukey\n  $g$-and-$h$ Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the uncertainty of wind energy potential from climate models is a\nvery time-consuming task and requires a considerable amount of computational\nresources. A statistical model trained on a small set of runs can act as a\nstochastic approximation of the original climate model, and be used to assess\nthe uncertainty considerably faster than by resorting to the original climate\nmodel for additional runs. While Gaussian models have been widely employed as\nmeans to approximate climate simulations, the Gaussianity assumption is not\nsuitable for winds at policy-relevant time scales, i.e., sub-annual. We propose\na trans-Gaussian model for monthly wind speed that relies on an autoregressive\nstructure with Tukey $g$-and-$h$ transformation, a flexible new class that can\nseparately model skewness and tail behavior. This temporal structure is\nintegrated into a multi-step spectral framework that is able to account for\nglobal nonstationarities across land/ocean boundaries, as well as across\nmountain ranges. Inference can be achieved by balancing memory storage and\ndistributed computation for a data set of 220 million points. Once fitted with\nas few as five runs, the statistical model can generate surrogates fast and\nefficiently on a simple laptop, and provide uncertainty assessments very close\nto those obtained from all the available climate simulations (forty) on a\nmonthly scale.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 17:40:44 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Jeong", "Jaehong", ""], ["Yan", "Yuan", ""], ["Castruccio", "Stefano", ""], ["Genton", "Marc G.", ""]]}, {"id": "1711.03962", "submitter": "Brian Vegetabile", "authors": "Brian Vegetabile, Jenny Molet, Tallie Z. Baram, and Hal Stern", "title": "Estimating the Entropy Rate of Finite Markov Chains with Application to\n  Behavior Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictability of behavior has emerged an an important characteristic in many\nfields including biology, medicine, and marketing. Behavior can be recorded as\na sequence of actions performed by an individual over a given time period. This\nsequence of actions can often be modeled as a stationary time-homogeneous\nMarkov chain and the predictability of the individual's behavior can be\nquantified by the entropy rate of the process. This paper provides a\ncomprehensive investigation of three estimators of the entropy rate of finite\nMarkov processes and a bootstrap procedure for providing standard errors. The\nfirst two methods directly estimate the entropy rate through estimates of the\ntransition matrix and stationary distribution of the process; the methods\ndiffer in the technique used to estimate the stationary distribution. The third\nmethod is related to the sliding-window Lempel-Ziv (SWLZ) compression\nalgorithm. The first two methods achieve consistent estimates of the true\nentropy rate for reasonably short observed sequences, but are limited by\nrequiring a priori specification of the order of the process. The method based\non the SWLZ algorithm does not require specifying the order of the process and\nis optimal in the limit of an infinite sequence, but is biased for short\nsequences. When used together, the methods can provide a clear picture of the\nentropy rate of an individual's behavior.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:51:17 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Vegetabile", "Brian", ""], ["Molet", "Jenny", ""], ["Baram", "Tallie Z.", ""], ["Stern", "Hal", ""]]}, {"id": "1711.04057", "submitter": "Frederick Matsen IV", "authors": "Jean Feng, David A. Shaw, Vladimir N. Minin, Noah Simon, Frederick A.\n  Matsen IV", "title": "Survival analysis of DNA mutation motifs with penalized proportional\n  hazards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antibodies, an essential part of our immune system, develop through an\nintricate process to bind a wide array of pathogens. This process involves\nrandomly mutating DNA sequences encoding these antibodies to find variants with\nimproved binding, though mutations are not distributed uniformly across\nsequence sites. Immunologists observe this nonuniformity to be consistent with\n\"mutation motifs\", which are short DNA subsequences that affect how likely a\ngiven site is to experience a mutation. Quantifying the effect of motifs on\nmutation rates is challenging: a large number of possible motifs makes this\nstatistical problem high dimensional, while the unobserved history of the\nmutation process leads to a nontrivial missing data problem. We introduce an\n$\\ell_1$-penalized proportional hazards model to infer mutation motifs and\ntheir effects. In order to estimate model parameters, our method uses a Monte\nCarlo EM algorithm to marginalize over the unknown ordering of mutations. We\nshow that our method performs better on simulated data compared to current\nmethods and leads to more parsimonious models. The application of proportional\nhazards to mutation processes is, to our knowledge, novel and formalizes the\ncurrent methods in a statistical framework that can be easily extended to\nanalyze the effect of other biological features on mutation rates.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 01:52:26 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 11:56:34 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Feng", "Jean", ""], ["Shaw", "David A.", ""], ["Minin", "Vladimir N.", ""], ["Simon", "Noah", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1711.04135", "submitter": "Philip Sansom", "authors": "Philip G. Sansom, and Daniel B. Williamson, and David B. Stephenson", "title": "State space models for non-stationary intermittently coupled systems: an\n  application to the North Atlantic Oscillation", "comments": "27 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Bayesian state space methods for modelling changes to the mean\nlevel or temporal correlation structure of an observed time series due to\nintermittent coupling with an unobserved process. Novel intervention methods\nare proposed to model the effect of repeated coupling as a single dynamic\nprocess. Latent time-varying autoregressive components are developed to model\nchanges in the temporal correlation structure. Efficient filtering and\nsmoothing methods are derived for the resulting class of models. We propose\nmethods for quantifying the component of variance attributable to an unobserved\nprocess, the effect during individual coupling events, and the potential for\nskilful forecasts.\n  The proposed methodology is applied to the study of winter-time variability\nin the dominant pattern of climate variation in the northern hemisphere, the\nNorth Atlantic Oscillation. Around 70% of the inter-annual variance in the\nwinter (Dec-Jan-Feb) mean level is attributable to an unobserved process.\nSkilful forecasts for winter (Dec-Jan-Feb) mean are possible from the beginning\nof December.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 13:39:09 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 14:00:27 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 20:21:04 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Sansom", "Philip G.", ""], ["Williamson", "Daniel B.", ""], ["Stephenson", "David B.", ""]]}, {"id": "1711.04139", "submitter": "Philip Sansom", "authors": "Philip G. Sansom, David B. Stephenson, and Thomas J. Bracegirdle", "title": "On constraining projections of future climate using observations and\n  simulations from multiple climate models", "comments": "Revised manuscript 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical climate models are used to project future climate change due to\nboth anthropogenic and natural causes. Differences between projections from\ndifferent climate models are a major source of uncertainty about future\nclimate. Emergent relationships shared by multiple climate models have the\npotential to constrain our uncertainty when combined with historical\nobservations. We combine projections from 13 climate models with observational\ndata to quantify the impact of emergent relationships on projections of future\nwarming in the Arctic at the end of the 21st century. We propose a hierarchical\nBayesian framework based on a coexchangeable representation of the relationship\nbetween climate models and the Earth system. We show how emergent constraints\nfit into the coexchangeable representation, and extend it to account for\ninternal variability simulated by the models and natural variability in the\nEarth system. Our analysis shows that projected warming in some regions of the\nArctic may be more than 2C lower and our uncertainty reduced by up to 30% when\nconstrained by historical observations. A detailed theoretical comparison with\nexisting multi-model projection frameworks is also provided. In particular, we\nshow that projections may be biased if we do not account for internal\nvariability in climate model predictions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 14:05:33 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 11:40:30 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 15:31:30 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 13:46:35 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Sansom", "Philip G.", ""], ["Stephenson", "David B.", ""], ["Bracegirdle", "Thomas J.", ""]]}, {"id": "1711.04702", "submitter": "Deisy Morselli Gysi", "authors": "Deisy Morselli Gysi, Andre Voigt, Tiago de Miranda Fragoso, Eivind\n  Almaas and Katja Nowick", "title": "wTO: an R package for computing weighted topological overlap and\n  consensus networks with an integrated visualization tool", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-018-2351-7", "report-no": null, "categories": "q-bio.MN stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analyses, such as of gene co-expression networks, metabolic networks\nand ecological networks have become a central approach for the systems-level\nstudy of biological data. Several software packages exist for generating and\nanalyzing such networks, either from correlation scores or the absolute value\nof a transformed score called weighted topological overlap (wTO). However,\nsince gene regulatory processes can up- or down-regulate genes, it is of great\ninterest to explicitly consider both positive and negative correlations when\nconstructing a gene co-expression network. Here, we present an R package for\ncalculating the wTO, that, in contrast to existing packages, explicitly\naddresses the sign of the wTO values, and is thus especially valuable for the\nanalysis of gene regulatory networks. The package includes the calculation of\np-values (raw and adjusted) for each pairwise gene score. Our package also\nallows the calculation of networks from time series (without replicates). Since\nnetworks from independent datasets (biological repeats or related studies) are\nnot the same due to technical and biological noise in the data, we\nadditionally, incorporated a novel method for calculating a consensus network\n(CN) from two or more networks into our R package. We compare our new wTO\npackage to state of art packages and demonstrate the application of the wTO and\nCN functions using 3 independently derived datasets from healthy human\npre-frontal cortex samples. To showcase an example for the time series\napplication we utilized a metagenomics data set. In this work, we developed a\nsoftware package that allows the computation of wTO networks, CNs and a\nvisualization tool in the R statistical environment. It is publicly available\non CRAN repositories under the GPL-2 Open Source License\n(https://cran.r-project.org/web/packages/wTO/).\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:54:57 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:44:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gysi", "Deisy Morselli", ""], ["Voigt", "Andre", ""], ["Fragoso", "Tiago de Miranda", ""], ["Almaas", "Eivind", ""], ["Nowick", "Katja", ""]]}, {"id": "1711.04738", "submitter": "Beatriz Etchegaray Garcia", "authors": "Julie Novak, Scott McGarvie, Beatriz Etchegaray Garcia", "title": "A Bayesian Model for Forecasting Hierarchically Structured Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task for any large-scale organization is to prepare forecasts of\nkey performance metrics. Often these organizations are structured in a\nhierarchical manner and for operational reasons, projections of these metrics\nmay have been obtained independently from one another at each level of the\nhierarchy by specialists focusing on certain areas within the business. There\nis no guarantee that when combined, these aggregates will be consistent with\nprojections produced directly at other levels of the hierarchy. We propose a\nBayesian hierarchical method that treats the initial forecasts as observed data\nwhich are then combined with prior information and historical predictive\naccuracy to infer a probability distribution of revised forecasts. When used to\ncreate point estimates, this method can reflect preferences for increased\naccuracy at specific levels in the hierarchy. We present simulated and real\ndata studies to demonstrate when our approach results in improved inferences\nover alternative methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:11:26 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Novak", "Julie", ""], ["McGarvie", "Scott", ""], ["Garcia", "Beatriz Etchegaray", ""]]}, {"id": "1711.04826", "submitter": "Timothy Brathwaite", "authors": "Timothy Brathwaite, Akshay Vij, Joan L. Walker", "title": "Machine Learning Meets Microeconomics: The Case of Decision Trees and\n  Discrete Choice", "comments": "39 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a microeconomic framework for decision trees: a popular machine\nlearning method. Specifically, we show how decision trees represent a\nnon-compensatory decision protocol known as disjunctions-of-conjunctions and\nhow this protocol generalizes many of the non-compensatory rules used in the\ndiscrete choice literature so far. Additionally, we show how existing decision\ntree variants address many economic concerns that choice modelers might have.\nBeyond theoretical interpretations, we contribute to the existing literature of\ntwo-stage, semi-compensatory modeling and to the existing decision tree\nliterature. In particular, we formulate the first bayesian model tree, thereby\nallowing for uncertainty in the estimated non-compensatory rules as well as for\ncontext-dependent preference heterogeneity in one's second-stage choice model.\nUsing an application of bicycle mode choice in the San Francisco Bay Area, we\nestimate our bayesian model tree, and we find that it is over 1,000 times more\nlikely to be closer to the true data-generating process than a multinomial\nlogit model (MNL). Qualitatively, our bayesian model tree automatically finds\nthe effect of bicycle infrastructure investment to be moderated by travel\ndistance, socio-demographics and topography, and our model identifies\ndiminishing returns from bike lane investments. These qualitative differences\nlead to bayesian model tree forecasts that directly align with the observed\nbicycle mode shares in regions with abundant bicycle infrastructure such as\nDavis, CA and the Netherlands. In comparison, MNL's forecasts are overly\noptimistic.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:04:04 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Brathwaite", "Timothy", ""], ["Vij", "Akshay", ""], ["Walker", "Joan L.", ""]]}, {"id": "1711.05204", "submitter": "Jonas Haslbeck", "authors": "Jonas M B Haslbeck, Laura F Bringmann, Lourens J Waldorp", "title": "A Tutorial on Estimating Time-Varying Vector Autoregressive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series of individual subjects have become a common data type in\npsychological research. These data allow one to estimate models of\nwithin-subject dynamics, and thereby avoid the notorious problem of making\nwithin-subjects inferences from between-subjects data, and naturally address\nheterogeneity between subjects. A popular model for these data is the Vector\nAutoregressive (VAR) model, in which each variable is predicted as a linear\nfunction of all variables at previous time points. A key assumption of this\nmodel is that its parameters are constant (or stationary) across time. However,\nin many areas of psychological research time-varying parameters are plausible\nor even the subject of study. In this tutorial paper, we introduce methods to\nestimate time-varying VAR models based on splines and kernel-smoothing\nwith/without regularization. We use simulations to evaluate the relative\nperformance of all methods in scenarios typical in applied research, and\ndiscuss their strengths and weaknesses. Finally, we provide a step-by-step\ntutorial showing how to apply the discussed methods to an openly available time\nseries of mood-related measurements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:14:30 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 21:42:20 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 21:04:34 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 14:54:16 GMT"}, {"version": "v5", "created": "Fri, 13 Mar 2020 09:45:25 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Haslbeck", "Jonas M B", ""], ["Bringmann", "Laura F", ""], ["Waldorp", "Lourens J", ""]]}, {"id": "1711.05243", "submitter": "Jacob Spertus", "authors": "Jacob Spertus, Samrachana Adhikari, and Sharon-Lise Normand", "title": "Regularization and Hierarchical Prior Distributions for Adjustment with\n  Health Care Claims Data: Rethinking Comorbidity Scores", "comments": "13 pages (w/o references and appendix), 2 figures, methodological\n  ties to arXiv:1710.03138", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health care claims data refer to information generated from interactions\nwithin health systems. They have been used in health services research for\ndecades to assess effectiveness of interventions, determine the quality of\nmedical care, predict disease prognosis, and monitor population health. While\nclaims data are relatively cheap and ubiquitous, they are high-dimensional,\nsparse, and noisy, typically requiring dimension reduction. In health services\nresearch, the most common data reduction strategy involves use of a comorbidity\nindex -- a single number summary reflecting overall patient health. We discuss\nBayesian regularization strategies and a novel hierarchical prior distribution\nas better options for dimension reduction in claims data. The specifications\nare designed to work with a large number of codes while controlling variance by\nshrinking coefficients towards zero or towards a group-level mean. A comparison\nof drug-eluting to bare-metal coronary stents illustrates approaches. In our\napplication, regularization and a hierarchical prior improved over comorbidity\nscores in terms of prediction and causal inference, as evidenced by\nout-of-sample fit and the ability to meet falsifiability endpoints.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:38:12 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Spertus", "Jacob", ""], ["Adhikari", "Samrachana", ""], ["Normand", "Sharon-Lise", ""]]}, {"id": "1711.05360", "submitter": "Alexander Shkolnik", "authors": "Lisa Goldberg, Alex Papanicolaou and Alex Shkolnik", "title": "The Dispersion Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation error has plagued quantitative finance since Harry Markowitz\nlaunched modern portfolio theory in 1952. Using random matrix theory, we\ncharacterize a source of bias in the sample eigenvectors of financial\ncovariance matrices. Unchecked, the bias distorts weights of minimum variance\nportfolios and leads to risk forecasts that are severely biased downward. To\naddress these issues, we develop an eigenvector bias correction. Our approach\nis distinct from the regularization and eigenvalue shrinkage methods found in\nthe literature. We provide theoretical guarantees on the improvement our\ncorrection provides as well as estimation methods for computing the optimal\ncorrection from data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:03:58 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 00:59:40 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 06:03:18 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 17:16:12 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Goldberg", "Lisa", ""], ["Papanicolaou", "Alex", ""], ["Shkolnik", "Alex", ""]]}, {"id": "1711.05656", "submitter": "Anastasia Ushakova", "authors": "Anastasia Ushakova and Slava J. Mikhaylov", "title": "Learning to Predict with Highly Granular Temporal Data: Estimating\n  individual behavioral profiles with smart meter data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big spatio-temporal datasets, available through both open and administrative\ndata sources, offer significant potential for social science research. The\nmagnitude of the data allows for increased resolution and analysis at\nindividual level. While there are recent advances in forecasting techniques for\nhighly granular temporal data, little attention is given to segmenting the time\nseries and finding homogeneous patterns. In this paper, it is proposed to\nestimate behavioral profiles of individuals' activities over time using\nGaussian Process-based models. In particular, the aim is to investigate how\nindividuals or groups may be clustered according to the model parameters. Such\na Bayesian non-parametric method is then tested by looking at the\npredictability of the segments using a combination of models to fit different\nparts of the temporal profiles. Model validity is then tested on a set of\nholdout data. The dataset consists of half hourly energy consumption records\nfrom smart meters from more than 100,000 households in the UK and covers the\nperiod from 2015 to 2016. The methodological approach developed in the paper\nmay be easily applied to datasets of similar structure and granularity, for\nexample social media data, and may lead to improved accuracy in the prediction\nof social dynamics and behavior.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 16:36:14 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 19:09:15 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ushakova", "Anastasia", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1711.05686", "submitter": "Hormuzd Katki", "authors": "Hormuzd A. Katki", "title": "Novel decision-theoretic and risk-stratification metrics of predictive\n  performance: Application to deciding who should undergo genetic testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, women are referred for BRCA1/2 mutation-testing only if their\nfamily-history of breast/ovarian cancer implies that their risk of carrying a\nmutation exceeds 10\\%. However, as mutation-testing costs fall, prominent\nvoices have called for testing all women, which would strain clinical resources\nby testing millions of women, almost all of whom will test negative. To better\nevaluate risk-thresholds for BRCA1/2 testing, we introduce two broadly\napplicable, linked metrics: Mean Risk Stratification (MRS) and a\ndecision-theoretic metric, Net Benefit of Information (NBI). MRS and NBI\nprovide a range of risk thresholds at which a marker/model is \"optimally\ninformative\", in the sense of maximizing both MRS and NBI. NBI is a function of\nonly MRS and the risk-threshold for action, connecting decision-theory to\nrisk-stratification and providing a decision-theoretic rationale for MRS. AUC\nand Youden's index reflect on both the fraction of maximum MRS, and of maximum\nNBI, attained by the marker/model, providing AUC and Youden's index with\nlong-sought decision-theoretic and risk-stratification rationale. To evaluate\nrisk-thresholds for BRCA1/2 testing, we propose an eclectic approach\nconsidering AUC, Net Benefit, and MRS/NBI. MRS/NBI interpret AUC in the context\nof mutation-prevalence and provide a range of risk thresholds for which the\nrisk model is optimally informative.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:34:11 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Katki", "Hormuzd A.", ""]]}, {"id": "1711.05779", "submitter": "Xin Chen", "authors": "Xin Chen, Yu Wang, Xiaxia Yu, Elinor Schoenfeld, Mary Saltz, Joel\n  Saltz, Fusheng Wang", "title": "Large-scale Analysis of Opioid Poisoning Related Hospital Visits in New\n  York State", "comments": null, "journal-ref": "AMIA Annu Symp Proc. 2018;2017:545-554", "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opioid related deaths are increasing dramatically in recent years, and opioid\nepidemic is worsening in the United States. Combating opioid epidemic becomes a\nhigh priority for both the U.S. government and local governments such as New\nYork State. Analyzing patient level opioid related hospital visits provides a\ndata driven approach to discover both spatial and temporal patterns and\nidentity potential causes of opioid related deaths, which provides essential\nknowledge for governments on decision making. In this paper, we analyzed opioid\npoisoning related hospital visits using New York State SPARCS data, which\nprovides diagnoses of patients in hospital visits. We identified all patients\nwith primary diagnosis as opioid poisoning from 2010-2014 for our main studies,\nand from 2003-2014 for temporal trend studies. We performed demographical based\nstudies, and summarized the historical trends of opioid poisoning. We used\nfrequent item mining to find co-occurrences of diagnoses for possible causes of\npoisoning or effects from poisoning. We provided zip code level spatial\nanalysis to detect local spatial clusters, and studied potential correlations\nbetween opioid poisoning and demographic and social-economic factors.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:55:05 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 19:40:23 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Chen", "Xin", ""], ["Wang", "Yu", ""], ["Yu", "Xiaxia", ""], ["Schoenfeld", "Elinor", ""], ["Saltz", "Mary", ""], ["Saltz", "Joel", ""], ["Wang", "Fusheng", ""]]}, {"id": "1711.05887", "submitter": "Richard Oentaryo", "authors": "Richard J. Oentaryo, Xavier Jayaraj Siddarth Ashok, Ee-Peng Lim,\n  Philips Kokoh Prasetyo", "title": "On Analyzing Job Hop Behavior and Talent Flow Networks", "comments": null, "journal-ref": "ICDM Data Science for Human Capital Management 2017", "doi": "10.1109/ICDMW.2017.172", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing job hopping behavior is important for the understanding of job\npreference and career progression of working individuals. When analyzed at the\nworkforce population level, job hop analysis helps to gain insights of talent\nflow and organization competition. Traditionally, surveys are conducted on job\nseekers and employers to study job behavior. While surveys are good at getting\ndirect user input to specially designed questions, they are often not scalable\nand timely enough to cope with fast-changing job landscape. In this paper, we\npresent a data science approach to analyze job hops performed by about 490,000\nworking professionals located in a city using their publicly shared profiles.\nWe develop several metrics to measure how much work experience is needed to\ntake up a job and how recent/established the job is, and then examine how these\nmetrics correlate with the propensity of hopping. We also study how job hop\nbehavior is related to job promotion/demotion. Finally, we perform network\nanalyses at the job and organization levels in order to derive insights on\ntalent flow as well as job and organizational competitiveness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 01:58:20 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Oentaryo", "Richard J.", ""], ["Ashok", "Xavier Jayaraj Siddarth", ""], ["Lim", "Ee-Peng", ""], ["Prasetyo", "Philips Kokoh", ""]]}, {"id": "1711.05923", "submitter": "Payal Gupta", "authors": "Payal Gupta and Monika Agrawal", "title": "Enhanced Array Aperture using Higher Order Statistics for DoA Estimation", "comments": "I want to withdraw the paper because of I have noticed many drawbacks\n  of the paper. I got the review about this \"it is not correct technically\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, the higher order statistics (HOS) and sparsity based array are most\ntalked about techniques to estimate the Direction of Arrival (DoA). They not\nonly provide enhanced Degree of Freedom (DoF) to handle underdetermined cases\nbut also improve the estimation accuracy of the system. To achieve high\naccuracy and more number of DoF with limited number of sensors, here we have\nproposed a method based on the fourth order statistics. The aperture of virtual\narray becomes O(16N^4) using N physical sensors. Proposed method can be\nextended to the HOS which increases the DoF by many folds. Numeric simulation\nvalidates these claims that the proposed method increases the resolution\ncapacity as well as maximize the DoF among all the earlier proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 04:51:42 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 17:40:46 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Gupta", "Payal", ""], ["Agrawal", "Monika", ""]]}, {"id": "1711.06002", "submitter": "Jens Sj\\\"olund", "authors": "Jens Sj\\\"olund, Anders Eklund, Evren \\\"Ozarslan, Magnus Herberthson,\n  Maria B{\\aa}nkestad and Hans Knutsson", "title": "Bayesian uncertainty quantification in linear models for diffusion MRI", "comments": "Added results from a group analysis and a comparison with residual\n  bootstrap", "journal-ref": "NeuroImage, 2018; 175:272-285", "doi": "10.1016/j.neuroimage.2018.03.059", "report-no": null, "categories": "stat.AP physics.data-an physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue\nmicrostructure. By fitting a model to the dMRI signal it is possible to derive\nvarious quantitative features. Several of the most popular dMRI signal models\nare expansions in an appropriately chosen basis, where the coefficients are\ndetermined using some variation of least-squares. However, such approaches lack\nany notion of uncertainty, which could be valuable in e.g. group analyses. In\nthis work, we use a probabilistic interpretation of linear least-squares\nmethods to recast popular dMRI models as Bayesian ones. This makes it possible\nto quantify the uncertainty of any derived quantity. In particular, for\nquantities that are affine functions of the coefficients, the posterior\ndistribution can be expressed in closed-form. We simulated measurements from\nsingle- and double-tensor models where the correct values of several quantities\nare known, to validate that the theoretically derived quantiles agree with\nthose observed empirically. We included results from residual bootstrap for\ncomparison and found good agreement. The validation employed several different\nmodels: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI)\nand Constrained Spherical Deconvolution (CSD). We also used in vivo data to\nvisualize maps of quantitative features and corresponding uncertainties, and to\nshow how our approach can be used in a group analysis to downweight subjects\nwith high uncertainty. In summary, we convert successful linear models for dMRI\nsignal estimation to probabilistic models, capable of accurate uncertainty\nquantification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:41:42 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 12:57:10 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sj\u00f6lund", "Jens", ""], ["Eklund", "Anders", ""], ["\u00d6zarslan", "Evren", ""], ["Herberthson", "Magnus", ""], ["B\u00e5nkestad", "Maria", ""], ["Knutsson", "Hans", ""]]}, {"id": "1711.06070", "submitter": "Juho Kopra", "authors": "Juho Kopra, Tommi H\\\"ark\\\"anen, Hanna Tolonen, Pekka Jousilahti, Kari\n  Kuulasmaa, Jaakko Reinikainen and Juha Karvanen", "title": "Adjusting for selective non-participation with re-contact data in the\n  FINRISK 2012 survey", "comments": "16 pages, 4 tables, 0 figures", "journal-ref": "Scandinavian Journal of Public Health, 2017", "doi": "10.1177/1403494817734774", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aims: A common objective of epidemiological surveys is to provide\npopulation-level estimates of health indicators. Survey results tend to be\nbiased under selective non-participation. One approach to bias reduction is to\ncollect information about non-participants by contacting them again and asking\nthem to fill in a questionnaire. This information is called re-contact data,\nand it allows to adjust the estimates for non-participation.\n  Methods: We analyse data from the FINRISK 2012 survey, where re-contact data\nwere collected. We assume that the respondents of the re-contact survey are\nsimilar to the remaining non-participants with respect to the health given\ntheir available background information. Validity of this assumption is\nevaluated based on the hospitalization data obtained through record linkage of\nsurvey data to the administrative registers. Using this assumption and multiple\nimputation, we estimate the prevalences of daily smoking and heavy alcohol\nconsumption and compare them to estimates obtained with a commonly used\nassumption that the participants represent the entire target group.\n  Results: This approach produces higher prevalence estimates than what is\nestimated from participants only. Among men, smoking prevalence estimate was\n28.5% (23.2% for participants), heavy alcohol consumption prevalence was 9.4%\n(6.8% for participants). Among women, smoking prevalence was 19.0% (16.5% for\nparticipants) and heavy alcohol consumption 4.8% (3.0% for participants).\nConclusion: Utilization of re-contact data is a useful method to adjust for\nnon-participation bias on population estimates in epidemiological surveys.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 13:05:56 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kopra", "Juho", ""], ["H\u00e4rk\u00e4nen", "Tommi", ""], ["Tolonen", "Hanna", ""], ["Jousilahti", "Pekka", ""], ["Kuulasmaa", "Kari", ""], ["Reinikainen", "Jaakko", ""], ["Karvanen", "Juha", ""]]}, {"id": "1711.06110", "submitter": "Marc Bocquet", "authors": "Pavel Sakov, Jean-Matthieu Haussaire, Marc Bocquet", "title": "An iterative ensemble Kalman filter in presence of additive model error", "comments": "Accepted for publication in the Quarterly Journal of the Royal\n  Meteorological Society", "journal-ref": null, "doi": "10.1002/qj.3213", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iterative ensemble Kalman filter (IEnKF) in a deterministic framework was\nintroduced in Sakov et al. (2012) to extend the ensemble Kalman filter (EnKF)\nand improve its performance in mildly up to strongly nonlinear cases.\n  However, the IEnKF assumes that the model is perfect. This assumption\nsimplified the update of the system at a time different from the observation\ntime, which made it natural to apply the IEnKF for smoothing. In this study, we\ngeneralise the IEnKF to the case of imperfect model with additive model error.\n  The new method called IEnKF-Q conducts a Gauss-Newton minimisation in\nensemble space. It combines the propagated analysed ensemble anomalies from the\nprevious cycle and model noise ensemble anomalies into a single ensemble of\nanomalies, and by doing so takes an algebraic form similar to that of the\nIEnKF. The performance of the IEnKF-Q is tested in a number of experiments with\nthe Lorenz-96 model, which show that the method consistently outperforms both\nthe EnKF and the IEnKF naively modified to accommodate additive model noise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:43:26 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 09:28:46 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sakov", "Pavel", ""], ["Haussaire", "Jean-Matthieu", ""], ["Bocquet", "Marc", ""]]}, {"id": "1711.06241", "submitter": "Reid Priedhorsky", "authors": "Reid Priedhorsky, Dave Osthus, Ashlynn R. Daughton, Kelly R. Moran,\n  Aron Culotta", "title": "Deceptiveness of internet data for disease surveillance", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR 17-24564", "categories": "cs.IT cs.SI math.IT q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying how many people are or will be sick, and where, is a critical\ningredient in reducing the burden of disease because it helps the public health\nsystem plan and implement effective outbreak response. This process of disease\nsurveillance is currently based on data gathering using clinical and laboratory\nmethods; this distributed human contact and resulting bureaucratic data\naggregation yield expensive procedures that lag real time by weeks or months.\nThe promise of new surveillance approaches using internet data, such as web\nevent logs or social media messages, is to achieve the same goal but faster and\ncheaper. However, prior work in this area lacks a rigorous model of information\nflow, making it difficult to assess the reliability of both specific approaches\nand the body of work as a whole.\n  We model disease surveillance as a Shannon communication. This new framework\nlets any two disease surveillance approaches be compared using a unified\nvocabulary and conceptual model. Using it, we describe and compare the\ndeficiencies suffered by traditional and internet-based surveillance, introduce\na new risk metric called deceptiveness, and offer mitigations for some of these\ndeficiencies. This framework also makes the rich tools of information theory\napplicable to disease surveillance. This better understanding will improve the\ndecision-making of public health practitioners by helping to leverage\ninternet-based surveillance in a way complementary to the strengths of\ntraditional surveillance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:37:31 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 20:29:10 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Priedhorsky", "Reid", ""], ["Osthus", "Dave", ""], ["Daughton", "Ashlynn R.", ""], ["Moran", "Kelly R.", ""], ["Culotta", "Aron", ""]]}, {"id": "1711.06249", "submitter": "Cheikh Tidiane Seck", "authors": "Cheikh Tidiane Seck, Gane Samb Lo", "title": "Uniform weak convergence of poverty measures with relative poverty lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general continuous form of poverty index that\nencompasses most of the existing formulas in the literature. We then propose a\nconsistent estimator for this index in case the poverty line is a functional of\nthe distribution. We also establish a uniform functional Central Limit Theorem\nfor the proposed estimator over a suitable product class of real-valued\nfunctions. As a consequence, testing procedures based either on single or\nsimultaneously several poverty indices can be developed. A simulation study\nshowing the asymptotic normality of the estimator is given as well as an\napplication to real data for estimating the effect of relative poverty lines on\nthe variance of the poverty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:49:10 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Seck", "Cheikh Tidiane", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1711.06261", "submitter": "Kean Dequeant", "authors": "Kean Dequeant (1,2), Pierre Lemaire (2), Marie-Laure Espinouse (G2),\n  Philippe Vialletelle (1) ((1) ST-CROLLES, (2) G-SCOP\\_ROSP)", "title": "A study of variability induced by events dependency in microelectronic\n  production", "comments": "International Conference on Industrial Engineering and Systems\n  Management, Oct 2017, Saarebr{\\\"u}cke, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -Complex manufacturing systems are subject to high levels of variability that\ndecrease productivity, increase cycle times and severely impact the systems\ntractability. As accurate modelling of the sources of variability is a\ncornerstone to intelligent decision making, we investigate the consequences of\nthe assumption of independent and identically distributed variables that is\noften made when modelling sources of variability such as down-times, arrivals,\nor process-times. We first explain the experiment setting that allows, through\nsimulations and statistical tests, to measure the variability potential stored\nin a specific sequence of data. We show from industrial data that dependent\nbehaviors might actually be the rule with potentially considerable consequences\nin terms of cycle time. As complex industries require strong levers to allow\ntheir tractability, this work underlines the need for a richer and more\naccurate modelling of real systems. Keywords-variability; cycle time; dependent\nevents; simulation; complex manufacturing; industry 4.0 I. Accurate modelling\nof variability and the independence assumption Industry 4.0 is said to be the\nnext industrial revolution. The proper use of real-time information in complex\nmanufacturing systems is expected to allow more customization of products in\nhighly flexible production factories. Semiconductor High Mix Low Volume (HMLV)\nmanufacturing facilities (called fabs) are one example of candidates for this\ntransition towards \"smart industries\". However, because of the high levels of\nvariability, the environment of a HMLV fab is highly stochastic and difficult\nto manage. The uncontrolled variability limits the predictability of the system\nand thus the ability to meet delivery requirements in terms of volumes, cycle\ntimes and due dates. Typically, the HMLV STMicroelectronics Crolles 300 fab\nregularly experiences significant mix changes that result in unanticipated\nbottlenecks, leading to firefighting to meet commitment to customers. The\noverarching goal of our strategy is to improve the forecasting of future\noccurrences of bottlenecks and cycle time issues in order to anticipate them\nthrough allocation of the correct attention and resources. Our current finite\ncapacity projection engine can effectively forecast bottlenecks, but it does\nnot include reliable cycle time estimates. In order to enhance our projections,\nbetter forecast cycle time losses (queuing times), improve the tractability of\nour system and reduce our cycle times, we now need accurate dynamic cycle time\npredictions. As increased cycle-time is the main reason workflow variability is\nstudied (both by the scientific community and practitioners, see e.g. [1] and\n[2]), what follows concentrates on cycle times. Moreover, the \"variability\" we\naccount for should be understood as the potential to create higher cycle times,\neven though \"variability\" may be understood in a broader meaning. This choice\nis made for the sake of clarity, but the methodology we propose and the\ndiscussion we lead can be applied to any other measurable indicator. Sources of\nvariability have been intensely investigated in both the literature and the\nindustry, and tool down-times, arrivals variability as well as process-time\nvariability are recognized as the major sources of variability in that sense\nthat they create higher cycle times (see [3] for a review and discussion). As a\nconsequence, these factors are widely integrated into queuing formulas and\nsimulation models with the objective to better model the complex reality of\nmanufacturing facilities. One commonly accepted assumption in the development\nof these models is that the variables (MTBF, MTTR, processing times, time\nbetween arrivals, etc.) are independent and identically distributed (i.i.d.)\nrandom variables. However, these assumptions might be the reason for models\ninaccuracies as [4] points out in a literature review on queuing theory.\nSeveral authors have studied the potential effects of dependencies, such as [5]\nwho studied the potential effects of dependencies between arrivals and\nprocess-times or [6] who investigated dependent process times, [4] also gives\nfurther references for studies on dependencies effects. In a previous work [3],\nwe pinpointed a few elements from industrial data that questioned the viability\nof this assumption in complex manufacturing systems. Figure 1: Number of\narrivals per week from real data (A) and generated by removing dependencies (B)\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:07:17 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Dequeant", "Kean", "", "ST-CROLLES", "G-SCOP\\_ROSP"], ["Lemaire", "Pierre", "", "G-SCOP\\_ROSP"], ["Espinouse", "Marie-Laure", "", "G2"], ["Vialletelle", "Philippe", "", "ST-CROLLES"]]}, {"id": "1711.06323", "submitter": "Jonathan Hersh", "authors": "Boris Babenko (1), Jonathan Hersh (2), David Newhouse (3), Anusha\n  Ramakrishnan (3), and Tom Swartz (1) ((1) Orbital Insight, (2) Chapman\n  University, (3) World Bank)", "title": "Poverty Mapping Using Convolutional Neural Networks Trained on High and\n  Medium Resolution Satellite Images, With an Application in Mexico", "comments": "4 pages, 2 figures, Presented at NIPS 2017 Workshop on Machine\n  Learning for the Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the spatial distribution of poverty in developing countries remains\nan important and costly challenge. These \"poverty maps\" are key inputs for\npoverty targeting, public goods provision, political accountability, and impact\nevaluation, that are all the more important given the geographic dispersion of\nthe remaining bottom billion severely poor individuals. In this paper we train\nConvolutional Neural Networks (CNNs) to estimate poverty directly from high and\nmedium resolution satellite images. We use both Planet and Digital Globe\nimagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively,\ncovering all 2 million sq. km. of Mexico. Benchmark poverty estimates come from\nthe 2014 MCS-ENIGH combined with the 2015 Intercensus and are used to estimate\npoverty rates for 2,456 Mexican municipalities. CNNs are trained using the 896\nmunicipalities in the 2014 MCS-ENIGH. We experiment with several architectures\n(GoogleNet, VGG) and use GoogleNet as a final architecture where weights are\nfine-tuned from ImageNet. We find that 1) the best models, which incorporate\nsatellite-estimated land use as a predictor, explain approximately 57% of the\nvariation in poverty in a validation sample of 10 percent of MCS-ENIGH\nmunicipalities; 2) Across all MCS-ENIGH municipalities explanatory power\nreduces to 44% in a CNN prediction and landcover model; 3) Predicted poverty\nfrom the CNN predictions alone explains 47% of the variation in poverty in the\nvalidation sample, and 37% over all MCS-ENIGH municipalities; 4) In urban areas\nwe see slight improvements from using Digital Globe versus Planet imagery,\nwhich explain 61% and 54% of poverty variation respectively. We conclude that\nCNNs can be trained end-to-end on satellite imagery to estimate poverty,\nalthough there is much work to be done to understand how the training process\ninfluences out of sample validation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 21:27:33 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Babenko", "Boris", ""], ["Hersh", "Jonathan", ""], ["Newhouse", "David", ""], ["Ramakrishnan", "Anusha", ""], ["Swartz", "Tom", ""]]}, {"id": "1711.06325", "submitter": "Anthony Greenberg", "authors": "Anthony J. Greenberg", "title": "Fast ordered sampling of DNA sequence variants", "comments": "six figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in the amount of genomic data is matched by increasing power\nof consumer-grade computers. Even applications that require powerful servers\ncan be quickly tested on desktop or laptop machines if we can generate\nrepresentative samples from large data sets. I describe a fast and\nmemory-efficient implementation of an on-line sampling method developed for\ntape drives 30 years ago. Focusing on genotype files, I test the performance of\nthis technique on modern solid-state and spinning hard drives, and show that it\nperforms well compared to a simple sampling scheme. I illustrate its utility by\ndeveloping a method to quickly estimate genome-wide patterns of linkage\ndisequilibrium (LD) decay with distance. I provide open-source software that\nsamples loci from several variant format files, a separate program that\nperforms LD decay estimates, and a C++ library that lets developers incorporate\nthese methods into their own projects.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 21:35:12 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Greenberg", "Anthony J.", ""]]}, {"id": "1711.06349", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks", "title": "Student Success Prediction in MOOCs", "comments": null, "journal-ref": null, "doi": "10.1007/s11257-018-9203-z", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models of student success in Massive Open Online Courses (MOOCs)\nare a critical component of effective content personalization and adaptive\ninterventions. In this article we review the state of the art in predictive\nmodels of student success in MOOCs and present a categorization of MOOC\nresearch according to the predictors (features), prediction (outcomes), and\nunderlying theoretical model. We critically survey work across each category,\nproviding data on the raw data source, feature engineering, statistical model,\nevaluation method, prediction architecture, and other aspects of these\nexperiments. Such a review is particularly useful given the rapid expansion of\npredictive modeling research in MOOCs since the emergence of major MOOC\nplatforms in 2012. This survey reveals several key methodological gaps, which\ninclude extensive filtering of experimental subpopulations, ineffective student\nmodel evaluation, and the use of experimental data which would be unavailable\nfor real-world student success prediction and intervention, which is the\nultimate goal of such models. Finally, we highlight opportunities for future\nresearch, which include temporal modeling, research bridging predictive and\nexplanatory student models, work which contributes to learning theory, and\nevaluating long-term learner success in MOOCs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:12:47 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 17:09:39 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 01:00:09 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1711.06502", "submitter": "Antonio Forcina", "authors": "Antonio Forcina and Paolo Carbone", "title": "Modelling dark current and hot pixels in imaging sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Gaussian mixture model with a complex covariance structure was used to\nanalyse experimental data from images recorded by a digital sensor under\ndarkness, to model the effects of temperature and duration of exposure on\nartificial signals (dark current), on ordinary and possibly defective (hot)\npixels. The model accounts for two components of variance within each latent\ntype: random noise in each image and lack of uniformity within the sensor; both\ncomponents are allowed to depend on experimental conditions. The results seem\nto indicate that the way dark current grows with the duration of exposure and\ntemperature cannot be represented by a simple parametric model. The latent\nclass model detects the presence of at least two types of hot pixels, where the\nless frequent ones have also a more extreme behaviour. Though the lack of\nuniformity of the sensor is amplified by duration of exposure and temperature,\npixels characteristics seem to deviate in the same direction and with the same\nrelative size.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:40:47 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 10:32:26 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Forcina", "Antonio", ""], ["Carbone", "Paolo", ""]]}, {"id": "1711.06538", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Artur Dubrawski", "title": "Discovery of Complex Anomalous Patterns of Sexual Violence in El\n  Salvador", "comments": "Conference paper at Data for Policy 2016 - Frontiers of Data Science\n  for Government: Ideas, Practices and Projections (Data for Policy)", "journal-ref": null, "doi": "10.5281/zenodo.571551", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sexual violence is a product of organized crime or social imaginary, the\nlinks between sexual violence episodes can be understood as a latent structure.\nWith this assumption in place, we can use data science to uncover complex\npatterns. In this paper we focus on the use of data mining techniques to unveil\ncomplex anomalous spatiotemporal patterns of sexual violence. We illustrate\ntheir use by analyzing all reported rapes in El Salvador over a period of nine\nyears. Through our analysis, we are able to provide evidence of phenomena that,\nto the best of our knowledge, have not been previously reported in literature.\nWe devote special attention to a pattern we discover in the East, where\nunderage victims report their boyfriends as perpetrators at anomalously high\nrates. Finally, we explain how such analyzes could be conducted in real-time,\nenabling early detection of emerging patterns to allow law enforcement agencies\nand policy makers to react accordingly.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:58:44 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1711.06758", "submitter": "Gregor Robinson", "authors": "Gregor Robinson, Ian Grooms, William Kleiber", "title": "Improving particle filter performance by smoothing observations", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1175/MWR-D-17-0349.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows that increasing the observation variance at small scales\ncan reduce the ensemble size required to avoid collapse in particle filtering\nof spatially-extended dynamics and improve the resulting uncertainty\nquantification at large scales. Particle filter weights depend on how well\nensemble members agree with observations, and collapse occurs when a few\nensemble members receive most of the weight. Collapse causes catastrophic\nvariance underestimation. Increasing small-scale variance in the observation\nerror model reduces the incidence of collapse by de-emphasizing small-scale\ndifferences between the ensemble members and the observations. Doing so smooths\nthe posterior mean, though it does not smooth the individual ensemble members.\nTwo options for implementing the proposed observation error model are\ndescribed. Taking discretized elliptic differential operators as an observation\nerror covariance matrix provides the desired property of a spectrum that grows\nin the approach to small scales. This choice also introduces structure\nexploitable by scalable computation techniques, including multigrid solvers and\nmultiresolution approximations to the corresponding integral operator.\nAlternatively the observations can be smoothed and then assimilated under the\nassumption of independent errors, which is equivalent to assuming large errors\nat small scales. The method is demonstrated on a linear stochastic partial\ndifferential equation, where it significantly reduces the occurrence of\nparticle filter collapse while maintaining accuracy. It also improves\ncontinuous ranked probability scores by as much as 25%, indicating that the\nweighted ensemble more accurately represents the true distribution. The method\nis compatible with other techniques for improving the performance of particle\nfilters.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:52:44 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 17:49:22 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 22:14:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Robinson", "Gregor", ""], ["Grooms", "Ian", ""], ["Kleiber", "William", ""]]}, {"id": "1711.06786", "submitter": "Hong Xu", "authors": "Therese Anders and Hong Xu and Cheng Cheng and T. K. Satish Kumar", "title": "Measuring Territorial Control in Civil Wars Using Hidden Markov Models:\n  A Data Informatics-Based Approach", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Territorial control is a key aspect shaping the dynamics of civil war.\nDespite its importance, we lack data on territorial control that are\nfine-grained enough to account for subnational spatio-temporal variation and\nthat cover a large set of conflicts. To resolve this issue, we propose a\ntheoretical model of the relationship between territorial control and tactical\nchoice in civil war and outline how Hidden Markov Models (HMMs) are suitable to\ncapture theoretical intuitions and estimate levels of territorial control. We\ndiscuss challenges of using HMMs in this application and mitigation strategies\nfor future work.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 01:35:59 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 08:18:55 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Anders", "Therese", ""], ["Xu", "Hong", ""], ["Cheng", "Cheng", ""], ["Kumar", "T. K. Satish", ""]]}, {"id": "1711.06813", "submitter": "Jerzy Wieczorek", "authors": "Varun Kshirsagar, Jerzy Wieczorek, Sharada Ramanathan, Rachel Wells", "title": "Household poverty classification in data-scarce environments: a machine\n  learning approach", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World, 7 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to identify poor households in data-scarce countries by\nleveraging information contained in nationally representative household\nsurveys. It employs standard statistical learning techniques---cross-validation\nand parameter regularization---which together reduce the extent to which the\nmodel is over-fitted to match the idiosyncracies of observed survey data. The\nautomated framework satisfies three important constraints of this development\nsetting: i) The prediction model uses at most ten questions, which limits the\ncosts of data collection; ii) No computation beyond simple arithmetic is needed\nto calculate the probability that a given household is poor, immediately after\ndata on the ten indicators is collected; and iii) One specification of the\nmodel (i.e. one scorecard) is used to predict poverty throughout a country that\nmay be characterized by significant sub-national differences. Using survey data\nfrom Zambia, the model's out-of-sample predictions distinguish poor households\nfrom non-poor households using information contained in ten questions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 04:57:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kshirsagar", "Varun", ""], ["Wieczorek", "Jerzy", ""], ["Ramanathan", "Sharada", ""], ["Wells", "Rachel", ""]]}, {"id": "1711.06845", "submitter": "Muaz Niazi", "authors": "Mohsin Adalat, Muaz A. Niazi", "title": "Evaluating Roles of Central Users in Online Communication Networks: A\n  Case Study of #PanamaLeaks", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.NI nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has changed the ways of communication, where everyone is\nequipped with the power to express their opinions to others in online\ndiscussion platforms. Previously, a number of stud- ies have been presented to\nidentify opinion leaders in online discussion networks. Feng (\"Are you\nconnected? Evaluating information cascade in online discussion about the\n#RaceTogether campaign\", Computers in Human Behavior, 2016) identified five\ntypes of central users and their communication patterns in an online\ncommunication network of a limited time span. However, to trace the change in\ncommunication pattern, a long-term analysis is required. In this study, we\ncritically analyzed framework presented by Feng based on five types of central\nusers in online communication network and their communication pattern in a\nlong-term manner. We take another case study presented by Udnor et al.\n(\"Determining social media impact on the politics of developing countries using\nsocial network analytics\", Program, 2016) to further understand the dynamics as\nwell as to perform validation . Results indicate that there may not exist all\nof these central users in an online communication network in a long-term\nmanner. Furthermore, we discuss the changing positions of opinion leaders and\ntheir power to keep isolates interested in an online discussion network.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:57:21 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Adalat", "Mohsin", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1711.06940", "submitter": "Dennis Shen", "authors": "Muhammad Jehangir Amjad, Devavrat Shah, and Dennis Shen", "title": "Robust Synthetic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust generalization of the synthetic control method for\ncomparative case studies. Like the classical method, we present an algorithm to\nestimate the unobservable counterfactual of a treatment unit. A distinguishing\nfeature of our algorithm is that of de-noising the data matrix via singular\nvalue thresholding, which renders our approach robust in multiple facets: it\nautomatically identifies a good subset of donors, overcomes the challenges of\nmissing data, and continues to work well in settings where covariate\ninformation may not be provided. To begin, we establish the condition under\nwhich the fundamental assumption in synthetic control-like approaches holds,\ni.e. when the linear relationship between the treatment unit and the donor pool\nprevails in both the pre- and post-intervention periods. We provide the first\nfinite sample analysis for a broader class of models, the Latent Variable\nModel, in contrast to Factor Models previously considered in the literature.\nFurther, we show that our de-noising procedure accurately imputes missing\nentries, producing a consistent estimator of the underlying signal matrix\nprovided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the\nfraction of observed data and $T$ is the time interval of interest. Under the\nsame setting, we prove that the mean-squared-error (MSE) in our prediction\nestimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the\nnoise variance. Using a data aggregation method, we show that the MSE can be\nmade as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to\na consistent estimator. We also introduce a Bayesian framework to quantify the\nmodel uncertainty through posterior probabilities. Our experiments, using both\nreal-world and synthetic datasets, demonstrate that our robust generalization\nyields an improvement over the classical synthetic control method.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 23:22:34 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Amjad", "Muhammad Jehangir", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""]]}, {"id": "1711.07007", "submitter": "Carolina Eu\\'an Campos", "authors": "Carolina Euan, Ying Sun and Hernando Ombao", "title": "Coherence-based Time Series Clustering for Brain Connectivity\n  Visualization", "comments": "27 pages, 21 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the hierarchical cluster coherence (HCC) method for brain signals,\na procedure for characterizing connectivity in a network by clustering nodes or\ngroups of channels that display high level of coordination as measured by\n\"cluster-coherence\". While the most common approach to measuring dependence\nbetween clusters is through pairs of single time series, our method proposes\ncluster coherence which measures dependence between whole clusters rather than\nbetween single elements. Thus it takes into account both the dependence between\nclusters and within channels in a cluster. Using our method, the identified\nclusters contain time series that exhibit high cross-dependence in the spectral\ndomain. That is, these clusters correspond to connected brain regions with\nsynchronized oscillatory activity. In the simulation studies, we show that the\nproposed HCC outperforms commonly used clustering algorithms, such as average\ncoherence and minimum coherence based methods. To study clustering in a network\nof multichannel electroencephalograms (EEG) during an epileptic seizure, we\napplied the HCC method and identified connectivity in alpha (8-12) Hertz and\nbeta (16-30) Hertz bands at different phases of the recording: before an\nepileptic seizure, during the early and middle phases of the seizure episode.\nTo increase the potential impact of this work in neuroscience, we also\ndeveloped the HCC-Vis, an R-Shiny app (RStudio), which can be downloaded from\nthis https://carolinaeuan.shinyapps.io/hcc-vis/.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 11:56:32 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Euan", "Carolina", ""], ["Sun", "Ying", ""], ["Ombao", "Hernando", ""]]}, {"id": "1711.07230", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis", "title": "Optimism-Based Adaptive Regulation of Linear-Quadratic Systems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge for adaptive regulation of linear-quadratic systems is the\ntrade-off between identification and control. An adaptive policy needs to\naddress both the estimation of unknown dynamics parameters (exploration), as\nwell as the regulation of the underlying system (exploitation). To this end,\noptimism-based methods which bias the identification in favor of optimistic\napproximations of the true parameter are employed in the literature. A number\nof asymptotic results have been established, but their finite time counterparts\nare few, with important restrictions.\n  This study establishes results for the worst-case regret of optimism-based\nadaptive policies. The presented high probability upper bounds are optimal up\nto logarithmic factors. The non-asymptotic analysis of this work requires very\nmild assumptions; (i) stabilizability of the system's dynamics, and (ii)\nlimiting the degree of heaviness of the noise distribution. To establish such\nbounds, certain novel techniques are developed to comprehensively address the\nprobabilistic behavior of dependent random matrices with heavy-tailed\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:52:25 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 15:54:41 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 01:56:55 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1711.07629", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Noel Cressie and Clint Shumack", "title": "On statistical approaches to generate Level 3 products from satellite\n  remote sensing retrievals", "comments": "28 pages, 10 figures, 4 tables", "journal-ref": "Zammit-Mangion, A.; Cressie, N.; Shumack, C. On Statistical\n  Approaches to Generate Level 3 Products from Satellite Remote Sensing\n  Retrievals. Remote Sens. 2018, 10, 155", "doi": "10.3390/rs10010155", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite remote sensing of trace gases such as carbon dioxide (CO$_2$) has\nincreased our ability to observe and understand Earth's climate. However, these\nremote sensing data, specifically~Level 2 retrievals, tend to be irregular in\nspace and time, and hence, spatio-temporal prediction is required to infer\nvalues at any location and time point. Such inferences are not only required to\nanswer important questions about our climate, but they are also needed for\nvalidating the satellite instrument, since Level 2 retrievals are generally not\nco-located with ground-based remote sensing instruments. Here, we discuss\nstatistical approaches to construct Level 3 products from Level 2 retrievals,\nplacing particular emphasis on the strengths and potential pitfalls when using\nstatistical prediction in this context. Following this discussion, we use a\nspatio-temporal statistical modelling framework known as fixed rank kriging\n(FRK) to obtain global predictions and prediction standard errors of\ncolumn-averaged carbon dioxide based on Version 7r and Version 8r retrievals\nfrom the Orbiting Carbon Observatory-2 (OCO-2) satellite. The FRK predictions\nallow us to validate statistically the Level 2 retrievals globally even though\nthe data are at locations and at time points that do not coincide with\nvalidation data. Importantly, the validation takes into account the prediction\nuncertainty, which is dependent both on the temporally-varying density of\nobservations around the ground-based measurement sites and on the\nspatio-temporal high-frequency components of the trace gas field that are not\nexplicitly modelled. Here, for validation of remotely-sensed CO$_2$ data, we\nuse observations from the Total Carbon Column Observing Network. We demonstrate\nthat the resulting FRK product based on Version 8r compares better with TCCON\ndata than that based on Version 7r.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 04:25:46 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 22:26:28 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Cressie", "Noel", ""], ["Shumack", "Clint", ""]]}, {"id": "1711.07763", "submitter": "Jacob Skauvold", "authors": "Jacob Skauvold and Jo Eidsvik", "title": "Data Assimilation for a Geological Process Model Using the Ensemble\n  Kalman Filter", "comments": "34 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conditioning a geological process-based computer\nsimulation, which produces basin models by simulating transport and deposition\nof sediments, to data. Emphasising uncertainty quantification, we frame this as\na Bayesian inverse problem, and propose to characterize the posterior\nprobability distribution of the geological quantities of interest by using a\nvariant of the ensemble Kalman filter, an estimation method which linearly and\nsequentially conditions realisations of the system state to data.\n  A test case involving synthetic data is used to assess the performance of the\nproposed estimation method, and to compare it with similar approaches. We\nfurther apply the method to a more realistic test case, involving real well\ndata from the Colville foreland basin, North Slope, Alaska.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:11:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Skauvold", "Jacob", ""], ["Eidsvik", "Jo", ""]]}, {"id": "1711.07801", "submitter": "Harry Crane", "authors": "Harry Crane", "title": "Why \"Redefining Statistical Significance\" Will Not Improve\n  Reproducibility and Could Make the Replication Crisis Worse", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent proposal to \"redefine statistical significance\" (Benjamin, et al.\nNature Human Behaviour, 2017) claims that false positive rates \"would\nimmediately improve\" by factors greater than two and replication rates would\ndouble simply by changing the conventional cutoff for 'statistical\nsignificance' from P<0.05 to P<0.005. I analyze the veracity of these claims,\nfocusing especially on how Benjamin, et al neglect the effects of P-hacking in\nassessing the impact of their proposal. My analysis shows that once P-hacking\nis accounted for the perceived benefits of the lower threshold all but\ndisappear, prompting two main conclusions: (i) The claimed improvements to\nfalse positive rate and replication rate in Benjamin, et al (2017) are\nexaggerated and misleading. (ii) There are plausible scenarios under which the\nlower cutoff will make the replication crisis worse.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:26:42 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Crane", "Harry", ""]]}, {"id": "1711.07812", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Andreas Wieser", "title": "Jaccard analysis and LASSO-based feature selection for location\n  fingerprinting with limited computational complexity", "comments": "16 pages, 4 figures, and 2 tables. Accepted to publish on LBS 2018,\n  Zurich", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to reduce both computational complexity and data\nstorage requirements for the online positioning stage of a fingerprinting-based\nindoor positioning system (FIPS) by introducing segmentation of the region of\ninterest (RoI) into sub-regions, sub-region selection using a modified Jaccard\nindex, and feature selection based on randomized least absolute shrinkage and\nselection operator (LASSO). We implement these steps into a Bayesian framework\nof position estimation using the maximum a posteriori (MAP) principle. An\nadditional benefit of these steps is that the time for estimating the position,\nand the required data storage are virtually independent of the size of the RoI\nand of the total number of available features within the RoI. Thus the proposed\nsteps facilitate application of FIPS to large areas. Results of an experimental\nanalysis using real data collected in an office building using a Nexus 6P smart\nphone as user device and a total station for providing position ground truth\ncorroborate the expected performance of the proposed approach. The positioning\naccuracy obtained by only processing 10 automatically identified features\ninstead of all available ones and limiting position estimation to 10\nautomatically identified sub-regions instead of the entire RoI is equivalent to\nprocessing all available data. In the chosen example, 50% of the errors are\nless than 1.8 m and 90% are less than 5 m. However, the computation time using\nthe automatically identified subset of data is only about 1% of that required\nfor processing the entire data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:42:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1711.07949", "submitter": "Eric Potash", "authors": "Eric Potash", "title": "Randomization Bias in Field Trials to Evaluate Targeting Methods", "comments": null, "journal-ref": "Economics Letters 167 (2018) 131-135", "doi": "10.1016/j.econlet.2018.03.012", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the evaluation of methods for targeting the allocation of\nlimited resources to a high-risk subpopulation. We consider a randomized\ncontrolled trial to measure the difference in efficiency between two targeting\nmethods and show that it is biased. An alternative, survey-based design is\nshown to be unbiased. Both designs are simulated for the evaluation of a policy\nto target lead hazard investigations using a predictive model. Based on our\nfindings, we advised the Chicago Department of Public Health to use the survey\ndesign for their field trial. Our work anticipates further developments in\neconomics that will be important as predictive modeling becomes an increasingly\ncommon policy tool.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:20:28 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 14:39:56 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 21:27:49 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Potash", "Eric", ""]]}, {"id": "1711.08208", "submitter": "Sebasti\\'an Casta\\~no-Candamil", "authors": "Sebastian Casta\\~no-Candamil and Andreas Meinel and Michael Tangermann", "title": "Post-hoc labeling of arbitrary EEG recordings for data-efficient\n  evaluation of neural decoding methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cognitive, sensory and motor processes have correlates in oscillatory\nneural sources, which are embedded as a subspace into the recorded brain\nsignals. Decoding such processes from noisy\nmagnetoencephalogram/electroencephalogram (M/EEG) signals usually requires the\nuse of data-driven analysis methods. The objective evaluation of such decoding\nalgorithms on experimental raw signals, however, is a challenge: the amount of\navailable M/EEG data typically is limited, labels can be unreliable, and raw\nsignals often are contaminated with artifacts. The latter is specifically\nproblematic, if the artifacts stem from behavioral confounds of the oscillatory\nneural processes of interest.\n  To overcome some of these problems, simulation frameworks have been\nintroduced for benchmarking decoding methods. Generating artificial brain\nsignals, however, most simulation frameworks make strong and partially\nunrealistic assumptions about brain activity, which limits the generalization\nof obtained results to real-world conditions.\n  In the present contribution, we thrive to remove many shortcomings of current\nsimulation frameworks and propose a versatile alternative, that allows for\nobjective evaluation and benchmarking of novel data-driven decoding methods for\nneural signals. Its central idea is to utilize post-hoc labelings of arbitrary\nM/EEG recordings. This strategy makes it paradigm-agnostic and allows to\ngenerate comparatively large datasets with noiseless labels. Source code and\ndata of the novel simulation approach are made available for facilitating its\nadoption.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 10:14:51 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Casta\u00f1o-Candamil", "Sebastian", ""], ["Meinel", "Andreas", ""], ["Tangermann", "Michael", ""]]}, {"id": "1711.08240", "submitter": "Ahmad Wasfi Bitar", "authors": "Ahmad W. Bitar, Jean-Philippe Ovarlez, Loong-Fah Cheong", "title": "Sparsity-based Cholesky Factorization and its Application to\n  Hyperspectral Anomaly Detection", "comments": "To be published on IEEE CAMSAP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating large covariance matrices has been a longstanding important\nproblem in many applications and has attracted increased attention over several\ndecades. This paper deals with two methods based on pre-existing works to\nimpose sparsity on the covariance matrix via its unit lower triangular matrix\n(aka Cholesky factor) $\\mathbf{T}$. The first method serves to estimate the\nentries of $\\mathbf{T}$ using the Ordinary Least Squares (OLS), then imposes\nsparsity by exploiting some generalized thresholding techniques such as Soft\nand Smoothly Clipped Absolute Deviation (SCAD). The second method directly\nestimates a sparse version of $\\mathbf{T}$ by penalizing the negative normal\nlog-likelihood with $L_1$ and SCAD penalty functions. The resulting covariance\nestimators are always guaranteed to be positive definite. Some Monte-Carlo\nsimulations as well as experimental data demonstrate the effectiveness of our\nestimators for hyperspectral anomaly detection using the Kelly anomaly\ndetector.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:46:41 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 17:02:59 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Bitar", "Ahmad W.", ""], ["Ovarlez", "Jean-Philippe", ""], ["Cheong", "Loong-Fah", ""]]}, {"id": "1711.08413", "submitter": "Subhadip Dey", "authors": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective utilization of photovoltaic (PV) plants requires weather\nvariability robust global solar radiation (GSR) forecasting models. Random\nweather turbulence phenomena coupled with assumptions of clear sky model as\nsuggested by Hottel pose significant challenges to parametric & non-parametric\nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires\ncostly high-tech radiometer and expert dependent instrument handling and\nmeasurements, which are subjective. As such, a computer aided monitoring (CAM)\nsystem to evaluate PV plant operation feasibility by employing smart grid past\ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a\n6-layer deep neural network trained on data collected at two weather stations\nlocated near Kalyani metrological site, West Bengal, India. The daily GSR\nprediction performance using SolarisNet outperforms the existing state of art\nand its efficacy in inferring past GSR data insights to comprehend daily and\nseasonal GSR variability along with its competence for short term forecasting\nis discussed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:41:40 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 18:56:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Dey", "Subhadip", ""], ["Pratiher", "Sawon", ""], ["Banerjee", "Saon", ""], ["Mukherjee", "Chanchal Kumar", ""]]}, {"id": "1711.08960", "submitter": "Benjamin All\\'evius", "authors": "Benjamin All\\'evius and Michael H\\\"ohle", "title": "Prospective Detection of Outbreaks", "comments": "This manuscript is a preprint of a chapter to appear in the Handbook\n  of Infectious Disease Data Analysis, Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman \\& Hall/CRC, 2018. Please use the book for\n  possible citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter surveys univariate and multivariate methods for infectious\ndisease outbreak detection. The setting considered is a prospective one: data\narrives sequentially as part of the surveillance systems maintained by public\nhealth authorities, and the task is to determine whether to 'sound the alarm'\nor not, given the recent history of data. The chapter begins by describing two\npopular detection methods for univariate time series data: the EARS algorithm\nof the CDC, and the Farrington algorithm more popular at European public health\ninstitutions. This is followed by a discussion of methods that extend some of\nthe univariate methods to a multivariate setting. This may enable the detection\nof outbreaks whose signal is only weakly present in any single data stream\nconsidered on its own. The chapter ends with a longer discussion of methods for\noutbreak detection in spatio-temporal data. These methods are not only tasked\nwith determining if and when an outbreak started to emerge, but also where. In\nparticular, the scan statistics methodology for outbreak cluster detection in\ndiscrete-time area-referenced data is discussed, as well as similar methods for\ncontinuous-time, continuous-space data. As a running example to illustrate the\nmethods covered in the chapter, a dataset on invasive meningococcal disease in\nGermany in the years 2002-2008 is used. This data and the methods covered are\navailable through the R packages surveillance and scanstatistics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 13:48:04 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["All\u00e9vius", "Benjamin", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1711.08970", "submitter": "Ahmad Wasfi Bitar", "authors": "Ahmad W. Bitar, Loong-Fah Cheong, Jean-Philippe Ovarlez", "title": "Sparse and Low-Rank Matrix Decomposition for Automatic Target Detection\n  in Hyperspectral Imagery", "comments": "Some wrong information present in the published IEEE version are\n  corrected here (marked in red)", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing (IEEE TGRS)\n  2019", "doi": "10.1109/TGRS.2019.2897635", "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a target prior information, our goal is to propose a method for\nautomatically separating targets of interests from the background in\nhyperspectral imagery. More precisely, we regard the given hyperspectral image\n(HSI) as being made up of the sum of low-rank background HSI and a sparse\ntarget HSI that contains the targets based on a pre-learned target dictionary\nconstructed from some online spectral libraries. Based on the proposed method,\ntwo strategies are briefly outlined and evaluated to realize the target\ndetection on both synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:16:07 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 10:35:06 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 22:13:25 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 17:32:53 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Bitar", "Ahmad W.", ""], ["Cheong", "Loong-Fah", ""], ["Ovarlez", "Jean-Philippe", ""]]}, {"id": "1711.09002", "submitter": "Miguel Moreles Dr.", "authors": "Miguel Angel Moreles, Joaquin Pe\\~na, Paola Vargas, Adriana Monroy,\n  Silvestre Alavez", "title": "Estimation and svm classification of glucose-insulin model parameters\n  from OGTT data. An aid for diabetes diagnostics", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Oral Glucose Tolerance Test (OGTT), a patient, after an overnight fast\ningests a load of glucose. Then measurements of glucose concentration are taken\nevery 30 minutes during two hours. The test is used to aid diagnosis of\ndiabetes, namely, type 2 diabetes mellitus and glucose intolerance. Several\nmathematical models have been introduced to describe the glucose-insulin system\nduring an OGTT. Models consist on systems of differential equations where most\nparameters are unknown. Estimation of these parameters is an aim of this work.\nIn a minimal model, two of such parameters are proposed for classification by\nmeans of a SVM technique. Consequently, a case is made for this classification\nas an aid for diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 20:16:10 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Moreles", "Miguel Angel", ""], ["Pe\u00f1a", "Joaquin", ""], ["Vargas", "Paola", ""], ["Monroy", "Adriana", ""], ["Alavez", "Silvestre", ""]]}, {"id": "1711.09013", "submitter": "Arnold Kalmbach", "authors": "Arnold Kalmbach, Heidi M. Sosik, Gregory Dudek, and Yogesh Girdhar", "title": "Learning Seasonal Phytoplankton Communities with Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop and demonstrate a probabilistic generative model for\nphytoplankton communities. The proposed model takes counts of a set of\nphytoplankton taxa in a timeseries as its training data, and models communities\nby learning sparse co-occurrence structure between the taxa. Our model is\nprobabilistic, where communities are represented by probability distributions\nover the species, and each time-step is represented by a probability\ndistribution over the communities. The proposed approach uses a non-parametric,\nspatiotemporal topic model to encourage the communities to form an\ninterpretable representation of the data, without making strong assumptions\nabout the communities. We demonstrate the quality and interpretability of our\nmethod by its ability to improve performance of a simplistic regression model.\nWe show that simple linear regression is sufficient to predict the community\ndistribution learned by our method, and therefore the taxon distributions, from\na set of naively chosen environment variables. In contrast, a similar\nregression model is insufficient to predict the taxon distributions directly or\nthrough PCA with the same level of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 21:20:26 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 15:35:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Kalmbach", "Arnold", ""], ["Sosik", "Heidi M.", ""], ["Dudek", "Gregory", ""], ["Girdhar", "Yogesh", ""]]}, {"id": "1711.09161", "submitter": "Marco Broccardo", "authors": "Marco Broccardo, Arnaud Mignan, Stefan Wiemer, Bozidar Stojadinovic,\n  and Domenico Giardini", "title": "Hierarchical Bayesian modeling of fluid-induced seismicity", "comments": "20 pages, 4 figures, Geophysical Research Letters 2017", "journal-ref": null, "doi": "10.1002/2017GL075251", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a Bayesian hierarchical framework to model\nfluid-induced seismicity. The framework is based on a non-homogeneous Poisson\nprocess (NHPP) with a fluid-induced seismicity rate proportional to the rate of\ninjected fluid. The fluid-induced seismicity rate model depends upon a set of\nphysically meaningful parameters, and has been validated for six fluid-induced\ncase studies. In line with the vision of hierarchical Bayesian modeling, the\nrate parameters are considered as random variables. We develop both the\nBayesian inference and updating rules, which are used to develop a\nprobabilistic forecasting model. We tested the Basel 2006 fluid-induced seismic\ncase study to prove that the hierarchical Bayesian model offers a suitable\nframework to coherently encode both epistemic uncertainty and aleatory\nvariability. Moreover, it provides a robust and consistent short-term seismic\nforecasting model suitable for online risk quantification and mitigation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:38:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Broccardo", "Marco", ""], ["Mignan", "Arnaud", ""], ["Wiemer", "Stefan", ""], ["Stojadinovic", "Bozidar", ""], ["Giardini", "Domenico", ""]]}, {"id": "1711.09196", "submitter": "Richard Diehl Martinez", "authors": "Richard Diehl Martinez, Anthony Carrington, Tiffany Kuo, Lena Tarhuni,\n  Nour Adel Zaki Abdel-Motaal", "title": "The Impact of an AirBnb Host's Listing Description 'Sentiment' and\n  Length On Occupancy Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant literature regarding the way product review\nsentiment affects brand loyalty. Intrigued by how natural language influences\nconsumer choice, we were motivated to examine whether an AirBnb host's\noccupancy rate (how often their listing is booked out of the days they\nindicated their listing was available) can be determined by the perceived\nsentiment and length of their description summary. Our main goal, more\ngenerally, was to determine which features, including (but not limited to)\nsentiment and description length, most influence a host's occupancy rate. We\ndefine sentiment score through a natural language algorithm process, based on\nthe AFINN dictionary. Using AirBnB data on New York City, our hypothesis is\nthat higher sentiment scores (more positive descriptions) and longer summary\nlength lead to higher occupancy rates. Our results show that while longer\nsummary length may positively influence occupancy rates, more positive summary\ndescriptions have no effect. Instead, we find that other factors such as number\nof reviews and number of amenities, in addition to summary length, are better\nindicators of occupancy rate.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 06:19:27 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Martinez", "Richard Diehl", ""], ["Carrington", "Anthony", ""], ["Kuo", "Tiffany", ""], ["Tarhuni", "Lena", ""], ["Abdel-Motaal", "Nour Adel Zaki", ""]]}, {"id": "1711.09365", "submitter": "Zaid Sawlan", "authors": "Marco Iglesias, Zaid Sawlan, Marco Scavino, Raul Tempone, Christopher\n  Wood", "title": "Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aac224", "report-no": null, "categories": "stat.CO math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 10:34:58 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 12:16:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Iglesias", "Marco", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Tempone", "Raul", ""], ["Wood", "Christopher", ""]]}, {"id": "1711.09429", "submitter": "Yang Chen", "authors": "Yang Chen, Xiao-Li Meng, Xufei Wang, David A. van Dyk, Herman L.\n  Marshall, Vinay L. Kashyap", "title": "Calibration Concordance for Astronomical Instruments via Multiplicative\n  Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration data are often obtained by observing several well-understood\nobjects simultaneously with multiple instruments, such as satellites for\nmeasuring astronomical sources. Analyzing such data and obtaining proper\nconcordance among the instruments is challenging when the physical source\nmodels are not well understood, when there are uncertainties in \"known\"\nphysical quantities, or when data quality varies in ways that cannot be fully\nquantified. Furthermore, the number of model parameters increases with both the\nnumber of instruments and the number of sources. Thus, concordance of the\ninstruments requires careful modeling of the mean signals, the intrinsic source\ndifferences, and measurement errors. In this paper, we propose a log-Normal\nhierarchical model and a more general log-t model that respect the\nmultiplicative nature of the mean signals via a half-variance adjustment, yet\npermit imperfections in the mean modeling to be absorbed by residual variances.\nWe present analytical solutions in the form of power shrinkage in special cases\nand develop reliable MCMC algorithms for general cases. We apply our method to\nseveral data sets obtained with a variety of X-ray telescopes such as Chandra.\nWe demonstrate that our method provides helpful and practical guidance for\nastrophysicists when adjusting for disagreements among instruments.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 17:28:08 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 14:19:23 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 21:24:33 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Chen", "Yang", ""], ["Meng", "Xiao-Li", ""], ["Wang", "Xufei", ""], ["van Dyk", "David A.", ""], ["Marshall", "Herman L.", ""], ["Kashyap", "Vinay L.", ""]]}, {"id": "1711.09490", "submitter": "Hyemin Han", "authors": "Hyemin Han, Kangwook Lee, Firat Soylu", "title": "Simulating outcomes of interventions using a multipurpose simulation\n  program based on the Evolutionary Causal Matrices and Markov Chain", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-017-1151-0", "report-no": null, "categories": "stat.AP cs.CE cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Predicting long-term outcomes of interventions is necessary for educational\nand social policy-making processes that might widely influence our society for\nthe long-term. However, performing such predictions based on data from\nlarge-scale experiments might be challenging due to the lack of time and\nresources. In order to address this issue, computer simulations based on\nEvolutionary Causal Matrices and Markov Chain can be used to predict long-term\noutcomes with relatively small-scale lab data. In this paper, we introduce\nPython classes implementing a computer simulation model and presented some\npilots implementations demonstrating how the model can be utilized for\npredicting outcomes of diverse interventions. We also introduce the\nclass-structured simulation module both with real experimental data and with\nhypothetical data formulated based on social psychological theories. Classes\ndeveloped and tested in the present study provide researchers and practitioners\nwith a feasible and practical method to simulate intervention outcomes\nprospectively.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:24:58 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Han", "Hyemin", ""], ["Lee", "Kangwook", ""], ["Soylu", "Firat", ""]]}, {"id": "1711.09609", "submitter": "Alex Gibberd Dr", "authors": "Alex Gibberd, Jordan Noble, Edward Cohen", "title": "Characterising Dependency in Computer Networks using Spectral Coherence", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transmission or reception of packets passing between computers can be\nrepresented in terms of time-stamped events and the resulting activity\nunderstood in terms of point-processes. Interestingly, in the disparate domain\nof neuroscience, models for describing dependent point-processes are well\ndeveloped. In particular, spectral methods which decompose second-order\ndependency across different frequencies allow for a rich characterisation of\npoint-processes. In this paper, we investigate using the spectral coherence\nstatistic to characterise computer network activity, and determine if, and how,\ndevice messaging may be dependent. We demonstrate on real data, that for many\ndevices there appears to be very little dependency between device messaging\nchannels. However, when significant coherence is detected it appears highly\nstructured, a result which suggests coherence may prove useful for\ndiscriminating between types of activity at the network level.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 10:17:38 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gibberd", "Alex", ""], ["Noble", "Jordan", ""], ["Cohen", "Edward", ""]]}, {"id": "1711.09677", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic, Ljubomir Buturovic, Simon Thomas, David E Leahy", "title": "Binary classification models with \"Uncertain\" predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary classification models which can assign probabilities to categories\nsuch as \"the tissue is 75% likely to be tumorous\" or \"the chemical is 25%\nlikely to be toxic\" are well understood statistically, but their utility as an\ninput to decision making is less well explored. We argue that users need to\nknow which is the most probable outcome, how likely that is to be true and, in\naddition, whether the model is capable enough to provide an answer. It is the\nlast case, where the potential outcomes of the model explicitly include \"don't\nknow\" that is addressed in this paper. Including this outcome would better\nseparate those predictions that can lead directly to a decision from those\nwhere more data is needed. Where models produce an \"Uncertain\" answer similar\nto a human reply of \"don't know\" or \"50:50\" in the examples we refer to\nearlier, this would translate to actions such as \"operate on tumour\" or \"remove\ncompound from use\" where the models give a \"more true than not\" answer. Where\nthe models judge the result \"Uncertain\" the practical decision might be \"carry\nout more detailed laboratory testing of compound\" or \"commission new tissue\nanalyses\". The paper presents several examples where we first analyse the\neffect of its introduction, then present a methodology for separating\n\"Uncertain\" from binary predictions and finally, we provide arguments for its\nuse in practice.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:29:42 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 10:49:25 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 15:10:52 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Krstajic", "Damjan", ""], ["Buturovic", "Ljubomir", ""], ["Thomas", "Simon", ""], ["Leahy", "David E", ""]]}, {"id": "1711.09715", "submitter": "ANtoine Marot", "authors": "Antoine Marot, Sami Tazi, Benjamin Donnot (LRI, TAU), Patrick\n  Panciatici", "title": "Guided Machine Learning for power grid segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of large scale power grids into zones is crucial for control\nroom operators when managing the grid complexity near real time. In this paper\nwe propose a new method in two steps which is able to automatically do this\nsegmentation, while taking into account the real time context, in order to help\nthem handle shifting dynamics. Our method relies on a \"guided\" machine learning\napproach. As a first step, we define and compute a task specific \"Influence\nGraph\" in a guided manner. We indeed simulate on a grid state chosen\ninterventions, representative of our task of interest (managing active power\nflows in our case). For visualization and interpretation, we then build a\nhigher representation of the grid relevant to this task by applying the graph\ncommunity detection algorithm \\textit{Infomap} on this Influence Graph. To\nillustrate our method and demonstrate its practical interest, we apply it on\ncommonly used systems, the IEEE-14 and IEEE-118. We show promising and original\ninterpretable results, especially on the previously well studied RTS-96 system\nfor grid segmentation. We eventually share initial investigation and results on\na large-scale system, the French power grid, whose segmentation had a\nsurprising resemblance with RTE's historical partitioning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:44:01 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 13:34:11 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 10:44:58 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Marot", "Antoine", "", "LRI, TAU"], ["Tazi", "Sami", "", "LRI, TAU"], ["Donnot", "Benjamin", "", "LRI, TAU"], ["Panciatici", "Patrick", ""]]}, {"id": "1711.09761", "submitter": "Jinpeng Guo", "authors": "Jinpeng Guo, Feng Liu, Xuemin Zhang, Yunhe Hou, Shengwei Mei", "title": "Mitigating Blackout Risk via Maintenance : Inference from Simulation\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas maintenance has been recognized as an important and effective means\nfor risk management in power systems, it turns out to be intractable if\ncascading blackout risk is considered due to the extremely high computational\ncomplexity. In this paper, based on the inference from the blackout simulation\ndata, we propose a methodology to efficiently identify the most influential\ncomponent(s) for mitigating cascading blackout risk in a large power system. To\nthis end, we first establish an analytic relationship between maintenance\nstrategies and blackout risk estimation by inferring from the data of cascading\noutage simulations. Then we formulate the component maintenance decision-making\nproblem as a nonlinear 0-1 programming. Afterwards, we quantify the credibility\nof blackout risk estimation, leading to an adaptive method to determine the\nleast required number of simulations, which servers as a crucial parameter of\nthe optimization model. Finally, we devise two heuristic algorithms to find\napproximate optimal solutions to the model with very high efficiency. Numerical\nexperiments well manifest the efficacy and high efficiency of our methodology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:06:09 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Guo", "Jinpeng", ""], ["Liu", "Feng", ""], ["Zhang", "Xuemin", ""], ["Hou", "Yunhe", ""], ["Mei", "Shengwei", ""]]}, {"id": "1711.10090", "submitter": "Abolfazl Safikhani", "authors": "Abolfazl Safikhani, Camille Kamga, Sandeep Mudigonda, Sabiheh Sadat\n  Faghih, Bahman Moghimi", "title": "Spatio-temporal Modeling of Yellow Taxi Demands in New York City Using\n  Generalized STAR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly dynamic urban space in a metropolis such as New York City, the\nspatio-temporal variation in demand for transportation, particularly taxis, is\nimpacted by various factors such as commuting, weather, road work and closures,\ndisruption in transit services, etc. To understand the user demand for taxis\nthrough space and time, a generalized spatio-temporal autoregressive (STAR)\nmodel is proposed in this study. In order to deal with the high dimensionality\nof the model, LASSO-type penalized methods are proposed to tackle the parameter\nestimation. The forecasting performance of the proposed models is measured\nusing the out-of-sample mean squared prediction error (MSPE), and it is found\nthat the proposed models outperform other alternative models such as vector\nautoregressive (VAR) models. The proposed modeling framework has an easily\ninterpretable parameter structure and practical to be applied by taxi\noperators. Efficiency of the proposed model also helps in model estimation in\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 02:44:57 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Safikhani", "Abolfazl", ""], ["Kamga", "Camille", ""], ["Mudigonda", "Sandeep", ""], ["Faghih", "Sabiheh Sadat", ""], ["Moghimi", "Bahman", ""]]}, {"id": "1711.10131", "submitter": "Shan Suthaharan", "authors": "Shan Suthaharan", "title": "A fatal point concept and a low-sensitivity quantitative measure for\n  traffic safety analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variability of the clusters generated by clustering techniques in the\ndomain of latitude and longitude variables of fatal crash data are\nsignificantly unpredictable. This unpredictability, caused by the randomness of\nfatal crash incidents, reduces the accuracy of crash frequency (i.e., counts of\nfatal crashes per cluster) which is used to measure traffic safety in practice.\nIn this paper, a quantitative measure of traffic safety that is not\nsignificantly affected by the aforementioned variability is proposed. It\nintroduces a fatal point -- a segment with the highest frequency of fatality --\nconcept based on cluster characteristics and detects them by imposing rounding\nerrors to the hundredth decimal place of the longitude. The frequencies of the\ncluster and the cluster's fatal point are combined to construct a low-sensitive\nquantitative measure of traffic safety for the cluster. The performance of the\nproposed measure of traffic safety is then studied by varying the parameter k\nof k-means clustering with the expectation that other clustering techniques can\nbe adopted in a similar fashion. The 2015 North Carolina fatal crash dataset of\nFatality Analysis Reporting System (FARS) is used to evaluate the proposed\nfatal point concept and perform experimental analysis to determine the\neffectiveness of the proposed measure. The empirical study shows that the\naverage traffic safety, measured by the proposed quantitative measure over\nseveral clusters, is not significantly affected by the variability, compared to\nthat of the standard crash frequency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 05:37:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Suthaharan", "Shan", ""]]}, {"id": "1711.10396", "submitter": "Pouyan Ahmadi", "authors": "Khondkar Islam, Pouyan Ahmadi, Salman Yousaf", "title": "Assessment Formats and Student Learning Performance: What is the\n  Relation?", "comments": "Proceedings of The 7th Research in Engineering Education Symposium\n  (REES 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although compelling assessments have been examined in recent years, more\nstudies are required to yield a better understanding of the several methods\nwhere assessment techniques significantly affect student learning process. Most\nof the educational research in this area does not consider demographics data,\ndiffering methodologies, and notable sample size. To address these drawbacks,\nthe objective of our study is to analyse student learning outcomes of multiple\nassessment formats for a web-facilitated in-class section with an asynchronous\nonline class of a core data communications course in the Undergraduate IT\nprogram of the Information Sciences and Technology (IST) Department at George\nMason University (GMU). In this study, students were evaluated based on course\nassessments such as home and lab assignments, skill-based assessments, and\ntraditional midterm and final exams across all four sections of the course. All\nsections have equivalent content, assessments, and teaching methodologies.\nStudent demographics such as exam type and location preferences are considered\nin our study to determine whether they have any impact on their learning\napproach. Large amount of data from the learning management system (LMS),\nBlackboard (BB) Learn, had to be examined to compare the results of several\nassessment outcomes for all students within their respective section and\namongst students of other sections. To investigate the effect of dissimilar\nassessment formats on student performance, we had to correlate individual\nquestion formats with the overall course grade. The results show that\ncollective assessment formats allow students to be effective in demonstrating\ntheir knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:35:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Islam", "Khondkar", ""], ["Ahmadi", "Pouyan", ""], ["Yousaf", "Salman", ""]]}, {"id": "1711.10411", "submitter": "Yang Feng", "authors": "Yang Feng, Yichao Wu, Leonard Stefanski", "title": "Nonparametric Independence Screening via Favored Smoothing Bandwidth", "comments": "22 pages", "journal-ref": "Journal of Statistical Planning and Inference Volume 197, December\n  2018, Pages 1-14", "doi": "10.1016/j.jspi.2017.11.006", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible nonparametric regression method for\nultrahigh-dimensional data. As a first step, we propose a fast screening method\nbased on the favored smoothing bandwidth of the marginal local constant\nregression. Then, an iterative procedure is developed to recover both the\nimportant covariates and the regression function. Theoretically, we prove that\nthe favored smoothing bandwidth based screening possesses the model selection\nconsistency property. Simulation studies as well as real data analysis show the\ncompetitive performance of the new procedure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:11:20 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Feng", "Yang", ""], ["Wu", "Yichao", ""], ["Stefanski", "Leonard", ""]]}, {"id": "1711.10416", "submitter": "Matthew Price-Williams", "authors": "Matthew Price-Williams, Nick Heard", "title": "Statistical Modelling of Computer Network Traffic Event Times", "comments": "22 pages 5 figure 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a statistical model for the arrival times of connection\nevents in a computer network. Edges between nodes in a network can be\ninterpreted and modelled as point processes where events in the process\nindicate information being sent along that edge. A model of normal behaviour\ncan be constructed for each edge in the network by identifying key network user\nfeatures such as seasonality and self-exciting behaviour, where events\ntypically arise in bursts at particular times of day. When monitoring the\nnetwork in real time, unusual patterns of activity could indicate the presence\nof a malicious actor. Four different models for self-exciting behaviour are\nintroduced and compared using data collected from the Imperial College and Los\nAlamos National Laboratory computer networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:21:53 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Price-Williams", "Matthew", ""], ["Heard", "Nick", ""]]}, {"id": "1711.10552", "submitter": "Georgios Papaioannou Mr", "authors": "George P Papaioannou, Christos Dikaiakos, Anargyros Dramountanis,\n  Dionysios S Georgiadis and Panagiotis G Papaioannou", "title": "Using nonlinear stochastic and deterministic (chaotic tools) to test the\n  EMH of two Electricity Markets the case of Italy and Greece", "comments": "arXiv admin note: text overlap with arXiv:cond-mat/0103621 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilization of non-linear tools to characterize the state of development of\nthe electricity markets in Italy and Greece. This is equivalent to testing the\nEfficient Market Hypothesis on these markets. The tools include a variety of\ncomplexity measures like Maximal Lyapunov and Hurst exponents and HHI index for\nmarket concentration and Entropy, a measure of uncertainty and complexity in a\ndynamical system, applied on the electricity wholesale marginal prices PUN and\nSMP of Italy and Greece.Our aim is to measure the complexity and dimensionality\nof the manifold on which the underlying stochastic dynamical system, govenring\nthe prices, evolve. We also use the conditional volatility of prices, which is\na measure of the market risk, and its connection with stability, and Hurst\nexponent to investigate the properties of the fluctuations of the prices which\nare the footprints of the idiosyncrracies of each market.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 11:14:07 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Papaioannou", "George P", ""], ["Dikaiakos", "Christos", ""], ["Dramountanis", "Anargyros", ""], ["Georgiadis", "Dionysios S", ""], ["Papaioannou", "Panagiotis G", ""]]}, {"id": "1711.10755", "submitter": "Rui Feng", "authors": "Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang", "title": "Representation Learning for Scale-free Networks", "comments": "8 figures; accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims to learn the low-dimensional representations of\nvertexes in a network, while structure and inherent properties of the network\nis preserved. Existing network embedding works primarily focus on preserving\nthe microscopic structure, such as the first- and second-order proximity of\nvertexes, while the macroscopic scale-free property is largely ignored.\nScale-free property depicts the fact that vertex degrees follow a heavy-tailed\ndistribution (i.e., only a few vertexes have high degrees) and is a critical\nproperty of real-world networks, such as social networks. In this paper, we\nstudy the problem of learning representations for scale-free networks. We first\ntheoretically analyze the difficulty of embedding and reconstructing a\nscale-free network in the Euclidean space, by converting our problem to the\nsphere packing problem. Then, we propose the \"degree penalty\" principle for\ndesigning scale-free property preserving network embedding algorithm: punishing\nthe proximity between high-degree vertexes. We introduce two implementations of\nour principle by utilizing the spectral techniques and a skip-gram model\nrespectively. Extensive experiments on six datasets show that our algorithms\nare able to not only reconstruct heavy-tailed distributed degree distribution,\nbut also outperform state-of-the-art embedding models in various network mining\ntasks, such as vertex classification and link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:15:17 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feng", "Rui", ""], ["Yang", "Yang", ""], ["Hu", "Wenjie", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1711.10786", "submitter": "Alessio Pollice", "authors": "Alessio Pollice, Giovanna Jona Lasinio, Roberta Rossi, Mariana Amato,\n  Thomas Kneib, Stefan Lang", "title": "Bayesian Measurement Error Correction in Structured Additive\n  Distributional Regression with an Application to the Analysis of Sensor Data\n  on Soil-Plant Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility of the Bayesian approach to account for covariates with\nmeasurement error is combined with semiparametric regression models for a class\nof continuous, discrete and mixed univariate response distributions with\npotentially all parameters depending on a structured additive predictor. Markov\nchain Monte Carlo enables a modular and numerically efficient implementation of\nBayesian measurement error correction based on the imputation of unobserved\nerror-free covariate values. We allow for very general measurement errors,\nincluding correlated replicates with heterogeneous variances. The proposal is\nfirst assessed by a simulation trial, then it is applied to the assessment of a\nsoil-plant relationship crucial for implementing efficient agricultural\nmanagement practices. Observations on multi-depth soil information forage\nground-cover for a seven hectares Alfalfa stand in South Italy were obtained\nusing sensors with very refined spatial resolution. Estimating a functional\nrelation between ground-cover and soil with these data involves addressing\nissues linked to the spatial and temporal misalignment and the large data size.\nWe propose a preliminary spatial interpolation on a lattice covering the field\nand subsequent analysis by a structured additive distributional regression\nmodel accounting for measurement error in the soil covariate. Results are\ninterpreted and commented in connection to possible Alfalfa management\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:37:32 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Pollice", "Alessio", ""], ["Lasinio", "Giovanna Jona", ""], ["Rossi", "Roberta", ""], ["Amato", "Mariana", ""], ["Kneib", "Thomas", ""], ["Lang", "Stefan", ""]]}, {"id": "1711.10814", "submitter": "Hiroshi Tsukimoto", "authors": "Hiroshi Tsukimoto and Takefumi Matsubara", "title": "A new fMRI data analysis method using cross validation: Negative BOLD\n  responses may be the deactivations of interneurons", "comments": "23 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although functional magnetic resonance imaging (fMRI) is widely used for the\nstudy of brain functions, the blood oxygenation level dependent (BOLD) effect\nis incompletely understood. Particularly, negative BOLD responses(NBRs) is\ncontroversial. This paper presents a new fMRI data analysis method, which is\nmore accurate than the typical conventional method. The authors conducted the\nexperiments of simple repetition, and analyzed the data by the new method. The\nresults strongly suggest that the deactivations(NBRs) detected by the new\nmethod are the deactivations of interneurons, because the deactivation ratios\nobtained by the new method approximately equals the deactivation ratios of\ninterneurons obtained by the study of interneurons. The (de)activations\ndetected by the new method are largely different from those detected by the\nconventional method. The new method is more accurate than the conventional\nmethod, and therefore the (de)activations detected by the new method may be\ncorrect and the (de)activations detected by the conventional method may be\nincorrect. A large portion of the deactivations of inhibitory interneurons is\nalso considered to be activations. Therefore, the right-tailed t-test, which is\nusually performed in the conventional method, does not detect the whole\nactivation, because the right-tailed t-test only detects the activations of\nexcitatory neurons, and neglect the deactivations of inhibitory interneurons. A\nlot of fMRI studies so far by the conventional method should be re-examined by\nthe new method, and many results obtained so far will be modified.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:32:51 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 08:43:26 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Tsukimoto", "Hiroshi", ""], ["Matsubara", "Takefumi", ""]]}, {"id": "1711.10910", "submitter": "Ilan Price", "authors": "Ilan Price, Jaroslav Fowkes, Daniel Hopman", "title": "Gaussian Processes for Demand Unconstraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in revenue management is unconstraining demand\ndata. Existing state of the art single-class unconstraining methods make\nrestrictive assumptions about the form of the underlying demand and can perform\npoorly when applied to data which breaks these assumptions. In this paper, we\npropose a novel unconstraining method that uses Gaussian process (GP)\nregression. We develop a novel GP model by constructing and implementing a new\nnon-stationary covariance function for the GP which enables it to learn and\nextrapolate the underlying demand trend. We show that this method can cope with\nimportant features of realistic demand data, including nonlinear demand trends,\nvariations in total demand, lengthy periods of constraining, non-exponential\ninter-arrival times, and discontinuities/changepoints in demand data. In all\nsuch circumstances, our results indicate that GPs outperform existing\nsingle-class unconstraining methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:21:24 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Price", "Ilan", ""], ["Fowkes", "Jaroslav", ""], ["Hopman", "Daniel", ""]]}, {"id": "1711.10937", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (1,2,3), Anne-Laure Foug\\`eres (3), Philippe Naveau\n  (2), Olivier Mestre (1) ((1) CNRM, (2) LSCE, (3) ICJ)", "title": "Forest-based methods and ensemble model output statistics for rainfall\n  ensemble forecasting", "comments": null, "journal-ref": null, "doi": "10.1175/WAF-D-18-0149.1", "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall ensemble forecasts have to be skillful for both low precipitation\nand extreme events. We present statistical post-processing methods based on\nQuantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric\nextension for heavy-tailed distributions. Our goal is to improve ensemble\nquality for all types of precipitation events, heavy-tailed included, subject\nto a good overall performance. Our hybrid proposed methods are applied to daily\n51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France\nusing the M{\\'e}t{\\'e}o-France ensemble prediction system called PEARP. They\nprovide calibrated pre-dictive distributions and compete favourably with\nstate-of-the-art methods like Analogs method or Ensemble Model Output\nStatistics. In particular, hybrid forest-based procedures appear to bring an\nadded value to the forecast of heavy rainfall.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:17:17 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM", "LSCE", "ICJ"], ["Foug\u00e8res", "Anne-Laure", "", "ICJ"], ["Naveau", "Philippe", "", "LSCE"], ["Mestre", "Olivier", "", "CNRM"]]}, {"id": "1711.10992", "submitter": "Aurya Javeed", "authors": "Aurya Javeed", "title": "An Uncertainty Principle for Estimates of Floquet Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a Cram\\'er-Rao lower bound for the variance of Floquet multiplier\nestimates that have been constructed from stable limit cycles perturbed by\nnoise. To do so, we consider perturbed periodic orbits in the plane. We use a\nperiodic autoregressive process to model the intersections of these orbits with\ncross sections, then passing to the limit of a continuum of sections to obtain\na bound that depends on the continuous flow restricted to the (nontrivial)\nFloquet mode. We compare our bound against the empirical variance of estimates\nconstructed using several cross sections. The section-based estimates are close\nto being optimal. We posit that the utility of our bound persists in higher\ndimensions when computed along Floquet modes for real and distinct multipliers.\nOur bound elucidates some of the empirical observations noted in the\nliterature; e.g., (a) it is the number of cycles (as opposed to the frequency\nof observations) that drives the variance of estimates to zero, and (b) the\nestimator variance has a positive lower bound as the noise amplitude tends to\nzero.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:16:10 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Javeed", "Aurya", ""]]}, {"id": "1711.11022", "submitter": "Prithwish Chakraborty", "authors": "Prithwish Chakraborty and Vishrawas Gopalakrishnan and Sharon M.H.\n  Alford and Faisal Farooq", "title": "A Novel Data-Driven Framework for Risk Characterization and Prediction\n  from Electronic Medical Records: A Case Study of Renal Failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic medical records (EMR) contain longitudinal information about\npatients that can be used to analyze outcomes. Typically, studies on EMR data\nhave worked with established variables that have already been acknowledged to\nbe associated with certain outcomes. However, EMR data may also contain\nhitherto unrecognized factors for risk association and prediction of outcomes\nfor a disease. In this paper, we present a scalable data-driven framework to\nanalyze EMR data corpus in a disease agnostic way that systematically uncovers\nimportant factors influencing outcomes in patients, as supported by data and\nwithout expert guidance. We validate the importance of such factors by using\nthe framework to predict for the relevant outcomes. Specifically, we analyze\nEMR data covering approximately 47 million unique patients to characterize\nrenal failure (RF) among type 2 diabetic (T2DM) patients. We propose a\nspecialized L1 regularized Cox Proportional Hazards (CoxPH) survival model to\nidentify the important factors from those available from patient encounter\nhistory. To validate the identified factors, we use a specialized generalized\nlinear model (GLM) to predict the probability of renal failure for individual\npatients within a specified time window. Our experiments indicate that the\nfactors identified via our data-driven method overlap with the patient\ncharacteristics recognized by experts. Our approach allows for scalable,\nrepeatable and efficient utilization of data available in EMRs, confirms prior\nmedical knowledge and can generate new hypothesis without expert supervision.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:50:33 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chakraborty", "Prithwish", ""], ["Gopalakrishnan", "Vishrawas", ""], ["Alford", "Sharon M. H.", ""], ["Farooq", "Faisal", ""]]}, {"id": "1711.11057", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "title": "On the use of bootstrap with variational inference: Theory,\n  interpretation, and a two-sample test example", "comments": "Accepted to the Annals of Applied Statistics; 34 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a general approach for approximating complex density\nfunctions, such as those arising in latent variable models, popular in machine\nlearning. It has been applied to approximate the maximum likelihood estimator\nand to carry out Bayesian inference, however, quantification of uncertainty\nwith variational inference remains challenging from both theoretical and\npractical perspectives. This paper is concerned with developing uncertainty\nmeasures for variational inference by using bootstrap procedures. We first\ndevelop two general bootstrap approaches for assessing the uncertainty of a\nvariational estimate and the study the underlying bootstrap theory in both\nfixed- and increasing-dimension settings. We then use the bootstrap approach\nand our theoretical results in the context of mixed membership modeling with\nmultivariate binary data on functional disability from the National Long Term\nCare Survey. We carry out a two-sample approach to test for changes in the\nrepeated measures of functional disability for the subset of individuals\npresent in 1989 and 1994 waves.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:08:07 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 03:35:00 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Wang", "Y. Samuel", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1711.11101", "submitter": "Michael Kuhn", "authors": "Michael A. Kuhn (1,2) and Eric D. Feigelson (3,1) ((1) Millennium\n  Institute of Astrophysics, (2) Universidad de Valpara\\'iso, (3) Pennsylvania\n  State University)", "title": "Mixture Models in Astronomy", "comments": "This is a revised preprint of a chapter in the Handbook of Mixture\n  Analysis, edited by S. Fr\\\"uwirth-Schnatter, G. Celeux, and C. P. Robert,\n  published in the \"Handbooks of Modern Statistical Methods\" series by Chapman\n  & Hall/CRC, 2018; revisions include several additional references and several\n  changes to mathematical notation", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models combine multiple components into a single probability density\nfunction. They are a natural statistical model for many situations in\nastronomy, such as surveys containing multiple types of objects, cluster\nanalysis in various data spaces, and complicated distribution functions. This\nchapter in the CRC Handbook of Mixture Analysis is concerned with astronomical\napplications of mixture models for cluster analysis, classification, and\nsemi-parametric density estimation. We present several classification examples\nfrom the literature, including identification of a new class, analysis of\ncontaminants, and overlapping populations. In most cases, mixtures of normal\n(Gaussian) distributions are used, but it is sometimes necessary to use\ndifferent distribution functions derived from astrophysical experience. We also\naddress the use of mixture models for the analysis of spatial distributions of\nobjects, like galaxies in redshift surveys or young stars in star-forming\nregions. In the case of galaxy clustering, mixture models may not be the\noptimal choice for understanding the homogeneous and isotropic structure of\nvoids and filaments. However, we show that mixture models, using astrophysical\nmodels for star clusters, may provide a natural solution to the problem of\nsubdividing a young stellar population into subclusters. Finally, we explore\nhow mixture models can be used for mathematically advanced modeling of data\nwith heteroscedastic uncertainties or missing values, providing two example\nalgorithms, the measurement error regression model of Kelly (2007) and the\nExtreme Deconvolution model of Bovy et al. (2011). The challenges presented by\nastronomical science, aided by the public availability of catalogs from major\nsurveys and missions, are a rich area for collaboration between statisticians\nand astronomers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:00:18 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 19:09:06 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kuhn", "Michael A.", ""], ["Feigelson", "Eric D.", ""]]}, {"id": "1711.11359", "submitter": "Andrea Saltelli", "authors": "Andrea Saltelli, Ksenia Aleksankina, William Becker, Pamela Fennell,\n  Federico Ferretti, Niels Holst, Sushan Li, Qiongli Wu", "title": "Why So Many Published Sensitivity Analyses Are False. A Systematic\n  Review of Sensitivity Analysis Practices", "comments": "23 pages using double space", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis (SA) has much to offer for a very large class of\napplications, such as model selection, calibration, optimization, quality\nassurance and many others. Sensitivity analysis offers crucial contextual\ninformation regarding a prediction by answering the question \"Which uncertain\ninput factors are responsible for the uncertainty in the prediction?\" SA is\ndistinct from uncertainty analysis (UA), which instead addresses the question\n\"How uncertain is the prediction?\" As we discuss in the present paper much\nconfusion exists in the use of these terms. A proper uncertainty analysis of\nthe output of a mathematical model needs to map what the model does when the\ninput factors are left free to vary over their range of existence. A fortiori,\nthis is true of a sensitivity analysis. Despite this, most UA and SA still\nexplore the input space; moving along mono-dimensional corridors which leave\nthe space of variation of the input factors mostly unscathed. We use results\nfrom a bibliometric analysis to show that many published SA fail the elementary\nrequirement to properly explore the space of the input factors. The results,\nwhile discipline-dependent, point to a worrying lack of standards and of\nrecognized good practices. The misuse of sensitivity analysis in mathematical\nmodelling is at least as serious as the misuse of the p-test in statistical\nmodelling. Mature methods have existed for about two decades to produce a\ndefensible sensitivity analysis. We end by offering a rough guide for proper\nuse of the methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 12:50:38 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 17:41:51 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Saltelli", "Andrea", ""], ["Aleksankina", "Ksenia", ""], ["Becker", "William", ""], ["Fennell", "Pamela", ""], ["Ferretti", "Federico", ""], ["Holst", "Niels", ""], ["Li", "Sushan", ""], ["Wu", "Qiongli", ""]]}, {"id": "1711.11399", "submitter": "Ali Saeb Dr.", "authors": "Ali Saeb", "title": "A note on power generalized extreme value distribution and its\n  properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to the generalized extreme value (GEV) family, the generalized\nextreme value distributions under power normalization are introduced by\nRoudsari (1999) and Barakat et al. (2013). In this article, we study the\nasymptotic behavior of GEV laws under power normalization and derive\nexpressions for the kth moments, entropy, ordering in dispersion, rare event\nestimation and application of real data set. We also show that, under some\nconditions, the Shannon entropy and variance of GEV families are ordered.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:00:13 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Saeb", "Ali", ""]]}, {"id": "1711.11411", "submitter": "Pouyan Ahmadi", "authors": "Pouyan Ahmadi, Khondkar Islam. Salman Yousaf", "title": "An Empirical Study of Teaching Methodologies and Learning Outcomes for\n  Online and in-class Networking Course Sections", "comments": null, "journal-ref": "Proceedings of 2017 ASEE Zone 2 Spring Conference", "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance student learning, we demonstrate an experimental study to analyze\nstudent learning outcomes in online and in-class sections of a core data\ncommunications course of the Undergraduate IT program in the Information\nSciences and Technology (IST) Department at George Mason University (GMU). In\nthis study, student performance is evaluated based on course assessments. This\nincludes home and lab assignments, skill-based assessment, and traditional\nmidterm exam across all 4 sections of the course. All sections have analogous\ncontent, assessment plan and teaching methodologies. Student demographics such\nas exam type and location preferences that may play an important role in their\nlearning process are considered in our study. We had to collect vast amount of\ndata from the learning management system (LMS), Blackboard (BB) Learn, in order\nto compare and study the results of several assessment outcomes for all\nstudents within their respective section and amongst students of other\nsections. We then tried to understand whether demographics have any influence\non student performance by correlating individual student survey response to\nhis/her performance in the class. The numerical results up to mid-semester\nreveal remarkable insights on student success in the online and face-to-face\n(F2F) sections.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:39:02 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ahmadi", "Pouyan", ""], ["Yousaf", "Khondkar Islam. Salman", ""]]}]