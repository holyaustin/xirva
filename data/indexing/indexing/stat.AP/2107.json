[{"id": "2107.00280", "submitter": "Roi Naveiro", "authors": "David R\\'ios Insua, William N. Caballero, Roi Naveiro", "title": "Managing driving modes in automated driving systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current technologies are unable to produce massively deployable, fully\nautonomous vehicles that do not require human intervention. Such technological\nlimitations are projected to persist for decades. Therefore, roadway scenarios\nrequiring a driver to regain control of a vehicle, and vice versa, will remain\ncritical to the safe operation of semi-autonomous vehicles for the foreseeable\nfuture. Herein, we adopt a comprehensive perspective on this problem taking\ninto account the operational design domain, driver and environment monitoring,\ntrajectory planning, and driver intervention performance assessment. Leveraging\ndecision analysis and Bayesian forecasting, both the support of driving mode\nmanagement decisions and the issuing of early warnings to the driver are\naddressed. A statistical modeling framework is created and a suite of\nalgorithms are developed to manage driving modes and issue relevant warnings in\naccordance with the management by exception principle. The efficacy of these\ndeveloped methods are then illustrated and examined via a simulated case study.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:10:38 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Insua", "David R\u00edos", ""], ["Caballero", "William N.", ""], ["Naveiro", "Roi", ""]]}, {"id": "2107.00502", "submitter": "Naomi Elizabeth Hannaford", "authors": "Naomi E. Hannaford, Sarah E. Heaps, Tom M. W. Nye, Thomas P. Curtis,\n  Ben Allen, Andrew Golightly, Darren J. Wilkinson", "title": "A sparse Bayesian hierarchical vector autoregressive model for microbial\n  dynamics in a wastewater treatment plant", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper function of a wastewater treatment plant (WWTP) relies on maintaining\na delicate balance between a multitude of competing microorganisms. Gaining a\ndetailed understanding of the complex network of interactions therein is\nessential to maximising not only current operational efficiencies, but also for\nthe effective design of new treatment technologies. Metagenomics offers an\ninsight into these dynamic systems through the analysis of the microbial DNA\nsequences present. Unique taxa are inferred through sequence clustering to form\noperational taxonomic units (OTUs), with per-taxa abundance estimates obtained\nfrom corresponding sequence counts. The data in this study comprise weekly OTU\ncounts from an activated sludge (AS) tank of a WWTP. To model the OTU dynamics,\nwe develop a Bayesian hierarchical vector autoregressive model, which is a\nlinear approximation to the commonly used generalised Lotka-Volterra (gLV)\nmodel. To tackle the high dimensionality and sparsity of the data, they are\nfirst clustered into 12 \"bins\" using a seasonal phase-based approach. The\nautoregressive coefficient matrix is assumed to be sparse, so we explore\ndifferent shrinkage priors by analysing simulated data sets before selecting\nthe regularised horseshoe prior for the biological application. We find that\nammonia and chemical oxygen demand have a positive relationship with several\nbins and pH has a positive relationship with one bin. These results are\nsupported by findings in the biological literature. We identify several\nnegative interactions, which suggests OTUs in different bins may be competing\nfor resources and that these relationships are complex. We also identify two\npositive interactions. Although simpler than a gLV model, our vector\nautoregression offers valuable insight into the microbial dynamics of the WWTP.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:49:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hannaford", "Naomi E.", ""], ["Heaps", "Sarah E.", ""], ["Nye", "Tom M. W.", ""], ["Curtis", "Thomas P.", ""], ["Allen", "Ben", ""], ["Golightly", "Andrew", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "2107.00527", "submitter": "Jacopo Diquigiovanni", "authors": "Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini", "title": "Distribution-Free Prediction Bands for Multivariate Functional Time\n  Series: an Application to the Italian Gas Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification in forecasting represents a topic of great\nimportance in statistics, especially when dealing with complex data\ncharacterized by non-trivial dependence structure. Pushed by novel works\nconcerning distribution-free prediction, we propose a scalable procedure that\noutputs closed-form simultaneous prediction bands for multivariate functional\nresponse variables in a time series setting, which is able to guarantee\nperformance bounds in terms of unconditional coverage and asymptotic exactness,\nboth under some conditions. After evaluating its performance on synthetic data,\nthe method is used to build multivariate prediction bands for daily demand and\noffer curves in the Italian gas market. The prediction framework thus obtained\nallows traders to directly evaluate the impact of their own offers/bids on the\nmarket, providing an intriguing tool for the business practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:13:16 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Diquigiovanni", "Jacopo", ""], ["Fontana", "Matteo", ""], ["Vantini", "Simone", ""]]}, {"id": "2107.00548", "submitter": "Aseel Mohamed", "authors": "Aseel Sameer Mohamed, Nooriya A. Mohammed", "title": "Comparison of forecasting of the risk of coronavirus (COVID 19) in high\n  quality and low quality healthcare systems, using ANN models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID 19 is a disease that has abnormal over 170 nations worldwide. The\nnumber of infected people (either sick or dead) has been growing at a worrying\nratio in virtually all the affected countries. Forecasting procedures can be\ninstructed so helping in scheming well plans and in captivating creative\nconclusions. These procedures measure the conditions of the previous thus\nallowing well forecasts around the state to arise in the future. These\npredictions strength helps to make contradiction of likely pressures and\nsignificances. Forecasting procedures production a very main character in\nelastic precise predictions. In this case study used two models in order to\ndiagnose optimal approach by compared the outputs. This study was introduced\nforecasting procedures into Artificial Neural Network models compared with\nregression model. Data collected from Al Kindy Teaching Hospital from the\nperiod of 28/5/2019 to 28/7/2019 show an energetic part in forecasting.\nForecasting of a disease can be done founded on several parameters such as the\nage, gender, number of daily infections, number of patient with other disease\nand number of death. Though, forecasting procedures arise with their private\ndata of tests. This study chats these tests and also offers a set of\ncommendations for the persons who are presently hostile the global COVID 19\ndisease.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:40:35 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 21:47:02 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mohamed", "Aseel Sameer", ""], ["Mohammed", "Nooriya A.", ""]]}, {"id": "2107.00593", "submitter": "Lucius Bynum", "authors": "Lucius E.J. Bynum, Joshua R. Loftus, Julia Stoyanovich", "title": "Impact Remediation: Optimal Interventions to Reduce Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A significant body of research in the data sciences considers unfair\ndiscrimination against social categories such as race or gender that could\noccur or be amplified as a result of algorithmic decisions. Simultaneously,\nreal-world disparities continue to exist, even before algorithmic decisions are\nmade. In this work, we draw on insights from the social sciences and humanistic\nstudies brought into the realm of causal modeling and constrained optimization,\nand develop a novel algorithmic framework for tackling pre-existing real-world\ndisparities. The purpose of our framework, which we call the \"impact\nremediation framework,\" is to measure real-world disparities and discover the\noptimal intervention policies that could help improve equity or access to\nopportunity for those who are underserved with respect to an outcome of\ninterest. We develop a disaggregated approach to tackling pre-existing\ndisparities that relaxes the typical set of assumptions required for the use of\nsocial categories in structural causal models. Our approach flexibly\nincorporates counterfactuals and is compatible with various ontological\nassumptions about the nature of social categories. We demonstrate impact\nremediation with a real-world case study and compare our disaggregated approach\nto an existing state-of-the-art approach, comparing its structure and resulting\npolicy recommendations. In contrast to most work on optimal policy learning, we\nexplore disparity reduction itself as an objective, explicitly focusing the\npower of algorithms on reducing inequality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:35:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bynum", "Lucius E. J.", ""], ["Loftus", "Joshua R.", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2107.00856", "submitter": "Ye Wang", "authors": "Licheng Liu, Ye Wang, and Yiqing Xu", "title": "A Practical Guide to Counterfactual Estimators for Causal Inference with\n  Time-Series Cross-Sectional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified framework of counterfactual estimation for\ntime-series cross-sectional data, which estimates the average treatment effect\non the treated by directly imputing treated counterfactuals. Examples include\nthe fixed effects counterfactual estimator, interactive fixed effects\ncounterfactual estimator, and matrix completion estimator. These estimators\nprovide more reliable causal estimates than conventional twoway fixed effects\nmodels when treatment effects are heterogeneous or unobserved time-varying\nconfounders exist. Under this framework, we propose a new dynamic treatment\neffects plot, as well as several diagnostic tests, to help researchers gauge\nthe validity of the identifying assumptions. We illustrate these methods with\ntwo political economy examples and develop an open-source package, fect, in\nboth R and Stata to facilitate implementation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 06:08:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Licheng", ""], ["Wang", "Ye", ""], ["Xu", "Yiqing", ""]]}, {"id": "2107.00960", "submitter": "Alexander McNeil", "authors": "Martin Bladt and Alexander J. McNeil", "title": "Time series models with infinite-order partial copula dependence", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stationary and ergodic time series can be constructed using an s-vine\ndecomposition based on sets of bivariate copula functions. The extension of\nsuch processes to infinite copula sequences is considered and shown to yield a\nrich class of models that generalizes Gaussian ARMA and ARFIMA processes to\nallow both non-Gaussian marginal behaviour and a non-Gaussian description of\nthe serial partial dependence structure. Extensions of classical causal and\ninvertible representations of linear processes to general s-vine processes are\nproposed and investigated. A practical and parsimonious method for\nparameterizing s-vine processes using the Kendall partial autocorrelation\nfunction is developed. The potential of the resulting models to give improved\nstatistical fits in many applications is indicated with an example using\nmacroeconomic data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:46:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bladt", "Martin", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "2107.00975", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno, Fatemah Alqallaf, Claudio Agostinelli", "title": "A Robust Seemingly Unrelated Regressions For Row-Wise And Cell-Wise\n  Contamination", "comments": "18 pages, 6 figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Seemingly Unrelated Regressions (SUR) model is a wide used estimation\nprocedure in econometrics, insurance and finance, where very often, the\nregression model contains more than one equation. Unknown parameters,\nregression coefficients and covariances among the errors terms, are estimated\nusing algorithms based on Generalized Least Squares or Maximum Likelihood, and\nthe method, as a whole, is very sensitive to outliers. To overcome this problem\nM-estimators and S-estimators are proposed in the literature together with fast\nalgorithms. However, these procedures are only able to cope with row-wise\noutliers in the error terms, while their performance becomes very poor in the\npresence of cell-wise outliers and as the number of equations increases. A new\nrobust approach is proposed which is able to perform well under both\ncontamination types as well as it is fast to compute. Illustrations based on\nMonte Carlo simulations and a real data example are provided.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:13:16 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Alqallaf", "Fatemah", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "2107.01095", "submitter": "Eric Jacobson", "authors": "Eric Jacobson", "title": "Who Votes for Library Bonds? A Principal Component Exploration", "comments": "26 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research has shown a relationship between voter characteristics and\nvoter support for tax bonds. These findings, however, are difficult to\ninterpret because of the high degree of collinearity across the measures. From\n13 demographic measures of voters in a library bond election, seven independent\nprincipal components were extracted which accounted for 95 percent of the\nvariance. Whereas the direct demographic measures showed inconsistent\nrelationships with voting, the principal components of low SES, college\nexperience, female and service job were related to affirmative voting, while\nhigh home value was related to negative voting.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:01:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jacobson", "Eric", ""]]}, {"id": "2107.01144", "submitter": "Antonio Elias", "authors": "A. El\\'ias, J. M. Morales and S. Pineda", "title": "Depth-based Outlier Detection for Grouped Smart Meters: a Functional\n  Data Analysis Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart metering infrastructures collect data almost continuously in the form\nof fine-grained long time series. These massive time series often have common\ndaily patterns that are repeated between similar days or seasons and shared\nbetween grouped meters. Within this context, we propose a method to highlight\nindividuals with abnormal daily dependency patterns, which we term evolution\noutliers. To this end, we approach the problem from the standpoint of\nFunctional Data Analysis (FDA), by treating each daily record as a function or\ncurve. We then focus on the morphological aspects of the observed curves, such\nas daily magnitude, daily shape, derivatives, and inter-day evolution. The\nproposed method for evolution outliers relies on the concept of functional\ndepth, which has been a cornerstone in the literature of FDA to build shape and\nmagnitude outlier detection methods. In conjunction with our evolution outlier\nproposal, these methods provide an outlier detection toolbox for smart meter\ndata that covers a wide palette of functional outliers classes. We illustrate\nthe outlier identification ability of this toolbox using actual smart metering\ndata corresponding to photovoltaic energy generation and circuit voltage\nrecords.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:42:29 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["El\u00edas", "A.", ""], ["Morales", "J. M.", ""], ["Pineda", "S.", ""]]}, {"id": "2107.01246", "submitter": "Gabriel Hassler", "authors": "Gabriel W. Hassler, Brigida Gallone, Leandro Aristide, William L.\n  Allen, Max R. Tolkoff, Andrew J. Holbrook, Guy Baele, Philippe Lemey and Marc\n  A. Suchard", "title": "Principled, practical, flexible, fast: a new approach to phylogenetic\n  factor analysis", "comments": "27 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological phenotypes are products of complex evolutionary processes in which\nselective forces influence multiple biological trait measurements in unknown\nways. Phylogenetic factor analysis disentangles these relationships across the\nevolutionary history of a group of organisms. Scientists seeking to employ this\nmodeling framework confront numerous modeling and implementation decisions, the\ndetails of which pose computational and replicability challenges. General and\nimpactful community employment requires a data scientific analysis plan that\nbalances flexibility, speed and ease of use, while minimizing model and\nalgorithm tuning. Even in the presence of non-trivial phylogenetic model\nconstraints, we show that one may analytically address latent factor\nuncertainty in a way that (a) aids model flexibility, (b) accelerates\ncomputation (by as much as 500-fold) and (c) decreases required tuning. We\nfurther present practical guidance on inference and modeling decisions as well\nas diagnosing and solving common problems in these analyses. We codify this\nanalysis plan in an automated pipeline that distills the potentially\noverwhelming array of modeling decisions into a small handful of (typically\nbinary) choices. We demonstrate the utility of these methods and analysis plan\nin four real-world problems of varying scales.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:40:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hassler", "Gabriel W.", ""], ["Gallone", "Brigida", ""], ["Aristide", "Leandro", ""], ["Allen", "William L.", ""], ["Tolkoff", "Max R.", ""], ["Holbrook", "Andrew J.", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2107.01251", "submitter": "Savannah Bergquist", "authors": "Savannah Bergquist, Gabriel Brooks, Mary Beth Landrum, Nancy Keating,\n  Sherri Rose", "title": "Uncertainty in Lung Cancer Stage for Outcome Estimation via Set-Valued\n  Classification", "comments": "Code available at:\n  https://github.com/sl-bergquist/cancer_classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Difficulty in identifying cancer stage in health care claims data has limited\noncology quality of care and health outcomes research. We fit prediction\nalgorithms for classifying lung cancer stage into three classes (stages I/II,\nstage III, and stage IV) using claims data, and then demonstrate a method for\nincorporating the classification uncertainty in outcomes estimation. Leveraging\nset-valued classification and split conformal inference, we show how a fixed\nalgorithm developed in one cohort of data may be deployed in another, while\nrigorously accounting for uncertainty from the initial classification step. We\ndemonstrate this process using SEER cancer registry data linked with Medicare\nclaims data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:52:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bergquist", "Savannah", ""], ["Brooks", "Gabriel", ""], ["Landrum", "Mary Beth", ""], ["Keating", "Nancy", ""], ["Rose", "Sherri", ""]]}, {"id": "2107.01590", "submitter": "Deyu Ming", "authors": "Deyu Ming and Daniel Williamson and Serge Guillas", "title": "Deep Gaussian Process Emulation using Stochastic Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep Gaussian process (DGP) inference method for computer\nmodel emulation using stochastic imputation. By stochastically imputing the\nlatent layers, the approach transforms the DGP into the linked GP, a\nstate-of-the-art surrogate model formed by linking a system of feed-forward\ncoupled GPs. This transformation renders a simple while efficient DGP training\nprocedure that only involves optimizations of conventional stationary GPs. In\naddition, the analytically tractable mean and variance of the linked GP allows\none to implement predictions from DGP emulators in a fast and accurate manner.\nWe demonstrate the method in a series of synthetic examples and real-world\napplications, and show that it is a competitive candidate for efficient DGP\nsurrogate modeling in comparison to the variational inference and the\nfully-Bayesian approach. A $\\texttt{Python}$ package $\\texttt{dgpsi}$\nimplementing the method is also produced and available at\nhttps://github.com/mingdeyu/DGP.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 10:46:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ming", "Deyu", ""], ["Williamson", "Daniel", ""], ["Guillas", "Serge", ""]]}, {"id": "2107.01629", "submitter": "Ziwei Cong", "authors": "Ziwei Cong, Jia Liu, Puneet Manchanda", "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal\n  Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The common belief about the growing medium of livestreaming is that its value\nlies in its \"live\" component. In this paper, we leverage data from a large\nlivestreaming platform to examine this belief. We are able to do this as this\nplatform also allows viewers to purchase the recorded version of the\nlivestream. We summarize the value of livestreaming content by estimating how\ndemand responds to price before, on the day of, and after the livestream. We do\nthis by proposing a generalized Orthogonal Random Forest framework. This\nframework allows us to estimate heterogeneous treatment effects in the presence\nof high-dimensional confounders whose relationships with the treatment policy\n(i.e., price) are complex but partially known. We find significant dynamics in\nthe price elasticity of demand over the temporal distance to the scheduled\nlivestreaming day and after. Specifically, demand gradually becomes less price\nsensitive over time to the livestreaming day and is inelastic on the\nlivestreaming day. Over the post-livestream period, demand is still sensitive\nto price, but much less than the pre-livestream period. This indicates that the\nvlaue of livestreaming persists beyond the live component. Finally, we provide\nsuggestive evidence for the likely mechanisms driving our results. These are\nquality uncertainty reduction for the patterns pre- and post-livestream and the\npotential of real-time interaction with the creator on the day of the\nlivestream.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 13:50:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cong", "Ziwei", ""], ["Liu", "Jia", ""], ["Manchanda", "Puneet", ""]]}, {"id": "2107.01942", "submitter": "Callum Murphy-Barltrop", "authors": "C. J. R. Murphy-Barltrop and J. L. Wadsworth and E. F. Eastoe", "title": "On the Estimation of Bivariate Return Curves for Extreme Values", "comments": "41 pages (without supplementary), 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the multivariate setting, defining extremal risk measures is important in\nmany contexts, such as finance, environmental planning and structural\nengineering. In this paper, we review the literature on extremal bivariate\nreturn curves, a risk measure that is the natural bivariate extension to a\nreturn level, and propose new estimation methods based on multivariate extreme\nvalue models that can account for both asymptotic dependence and asymptotic\nindependence. We identify gaps in the existing literature and propose novel\ntools for testing and validating return curves and comparing estimates from a\nrange of multivariate models. These tools are then used to compare a selection\nof models through simulation and case studies. We conclude with a discussion\nand list some of the challenges.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 11:17:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Murphy-Barltrop", "C. J. R.", ""], ["Wadsworth", "J. L.", ""], ["Eastoe", "E. F.", ""]]}, {"id": "2107.01979", "submitter": "Niek Tax", "authors": "Niek Tax, Kees Jan de Vries, Mathijs de Jong, Nikoleta Dosoula, Bram\n  van den Akker, Jon Smith, Olivier Thuong, Lucas Bernardi", "title": "Machine Learning for Fraud Detection in E-Commerce: A Research Agenda", "comments": "Accepted and to appear in the proceedings of the KDD 2021 co-located\n  workshop: the 2nd International Workshop on Deployable Machine Learning for\n  Security Defense (MLHat)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud detection and prevention play an important part in ensuring the\nsustained operation of any e-commerce business. Machine learning (ML) often\nplays an important role in these anti-fraud operations, but the organizational\ncontext in which these ML models operate cannot be ignored. In this paper, we\ntake an organization-centric view on the topic of fraud detection by\nformulating an operational model of the anti-fraud departments in e-commerce\norganizations. We derive 6 research topics and 12 practical challenges for\nfraud detection from this operational model. We summarize the state of the\nliterature for each research topic, discuss potential solutions to the\npractical challenges, and identify 22 open research challenges.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:37:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tax", "Niek", ""], ["de Vries", "Kees Jan", ""], ["de Jong", "Mathijs", ""], ["Dosoula", "Nikoleta", ""], ["Akker", "Bram van den", ""], ["Smith", "Jon", ""], ["Thuong", "Olivier", ""], ["Bernardi", "Lucas", ""]]}, {"id": "2107.02043", "submitter": "Hongping Zhang", "authors": "Hongping Zhang, Zhenfeng Shao, Jinqi Zhao, Xiao Huang, Jie Yang, Bin\n  Hu and Wenfu Wu", "title": "An extended watershed-based zonal statistical AHP model for flood risk\n  estimation: Constraining runoff converging related indicators by\n  sub-watersheds", "comments": "This paper is a research paper, it contains 40 pages and 8 figures.\n  This paper is a modest contribution to the ongoing discussions the accuracy\n  of flood risk estimation via AHP model improved by adopting pixels replaced\n  with sub-watersheds as basic unit", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Floods are highly uncertain events, occurring in different regions, with\nvarying prerequisites and intensities. A highly reliable flood disaster risk\nmap can help reduce the impact of floods for flood management, disaster\ndecreasing, and urbanization resilience. In flood risk estimation, the widely\nused analytic hierarchy process (AHP) usually adopts pixel as a basic unit, it\ncannot capture the similar threaten caused by neighborhood source flooding\ncells at sub-watershed scale. Thus, an extended watershed-based zonal\nstatistical AHP model constraining runoff converging related indicators by\nsub-watersheds (WZSAHP-Slope & Stream) is proposed to fill this gap. Taking the\nChaohu basin as test case, we validated the proposed method with a real-flood\narea extracted in July 2020. The results indicate that the WZSAHP-Slope &\nStream model using multiple flow direction division watersheds to calculate\nstatistics of distance from stream and slope by maximum statistic method\noutperformed other tested methods. Compering with pixel-based AHP method, the\nproposed method can improve the correct ratio by 16% (from 67% to 83%) and fit\nratio by 1% (from 13% to 14%) as in validation 1, and improve the correct ratio\nby 37% (from 23% to 60%) and fit ratio by 6% (from 12% to 18%) as in validation\n2.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:04:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Hongping", ""], ["Shao", "Zhenfeng", ""], ["Zhao", "Jinqi", ""], ["Huang", "Xiao", ""], ["Yang", "Jie", ""], ["Hu", "Bin", ""], ["Wu", "Wenfu", ""]]}, {"id": "2107.02146", "submitter": "Ali Mahzarnia", "authors": "Ali Mahzarnia and Jun Song", "title": "Multivariate functional group sparse regression: functional predictor\n  selection", "comments": "The R package that is developed for this paper is available at\n  GitHub. See https://github.com/Ali-Mahzarnia/MFSGrp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose methods for functional predictor selection and the\nestimation of smooth functional coefficients simultaneously in a\nscalar-on-function regression problem under high-dimensional multivariate\nfunctional data setting. In particular, we develop two methods for functional\ngroup-sparse regression under a generic Hilbert space of infinite dimension. We\nshow the convergence of algorithms and the consistency of the estimation and\nthe selection (oracle property) under infinite-dimensional Hilbert spaces.\nSimulation studies show the effectiveness of the methods in both the selection\nand the estimation of functional coefficients. The applications to the\nfunctional magnetic resonance imaging (fMRI) reveal the regions of the human\nbrain related to ADHD and IQ.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:11:28 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 15:03:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Mahzarnia", "Ali", ""], ["Song", "Jun", ""]]}, {"id": "2107.02394", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Roselinde Kessels, Rico Krueger, Daniel J Graham", "title": "Face masks, vaccination rates and low crowding drive the demand for the\n  London Underground during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has drastically impacted people's travel behaviour and\nout-of-home activity participation. While countermeasures are being eased with\nincreasing vaccination rates, the demand for public transport remains\nuncertain. To investigate user preferences to travel by London Underground\nduring the pandemic, we conducted a stated choice experiment among its\npre-pandemic users (N=961). We analysed the collected data using multinomial\nand mixed logit models. Our analysis provides insights into the sensitivity of\nthe demand for the London Underground with respect to travel attributes\n(crowding density and travel time), the epidemic situation (confirmed new\nCOVID-19 cases), and interventions (vaccination rates and mandatory face\nmasks). Mandatory face masks and higher vaccination rates are the top two\ndrivers of travel demand for the London Underground during COVID-19. The\npositive impact of vaccination rates on the Underground demand increases with\ncrowding density, and the positive effect of mandatory face masks decreases\nwith travel time. Mixed logit reveals substantial preference heterogeneity. For\ninstance, while the average effect of mandatory face masks is positive,\npreferences of around 20% of the pre-pandemic users to travel by the\nUnderground are negatively affected. The estimated demand sensitivities are\nrelevant for supply-demand management in transit systems and the calibration of\nadvanced epidemiological models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:21:59 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bansal", "Prateek", ""], ["Kessels", "Roselinde", ""], ["Krueger", "Rico", ""], ["Graham", "Daniel J", ""]]}, {"id": "2107.02463", "submitter": "Florian Haselbeck", "authors": "Florian Haselbeck and Dominik G. Grimm", "title": "EVARS-GPR: EVent-triggered Augmented Refitting of Gaussian Process\n  Regression for Seasonal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series forecasting is a growing domain with diverse applications.\nHowever, changes of the system behavior over time due to internal or external\ninfluences are challenging. Therefore, predictions of a previously learned\nfore-casting model might not be useful anymore. In this paper, we present\nEVent-triggered Augmented Refitting of Gaussian Process Regression for Seasonal\nData (EVARS-GPR), a novel online algorithm that is able to handle sudden shifts\nin the target variable scale of seasonal data. For this purpose, EVARS-GPR\ncom-bines online change point detection with a refitting of the prediction\nmodel using data augmentation for samples prior to a change point. Our\nexperiments on sim-ulated data show that EVARS-GPR is applicable for a wide\nrange of output scale changes. EVARS-GPR has on average a 20.8 % lower RMSE on\ndifferent real-world datasets compared to methods with a similar computational\nresource con-sumption. Furthermore, we show that our algorithm leads to a\nsix-fold reduction of the averaged runtime in relation to all comparison\npartners with a periodical refitting strategy. In summary, we present a\ncomputationally efficient online fore-casting algorithm for seasonal time\nseries with changes of the target variable scale and demonstrate its\nfunctionality on simulated as well as real-world data. All code is publicly\navailable on GitHub: https://github.com/grimmlab/evars-gpr.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:20:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Haselbeck", "Florian", ""], ["Grimm", "Dominik G.", ""]]}, {"id": "2107.02537", "submitter": "Alfredo Egidio dos Reis", "authors": "Yacine Koucha and Alfredo D. Egidio dos Reis", "title": "Approximations to ultimate ruin probabilities with a Wienner process\n  perturbation", "comments": "Master dissertation work, 18 pages, 4 figures, 8 numerical tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.PR q-fin.ST stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adapt the classic Cram\\'er-Lundberg collective risk theory\nmodel to a perturbed model by adding a Wiener process to the compound Poisson\nprocess, which can be used to incorporate premium income uncertainty, interest\nrate fluctuations and changes in the number of policyholders. Our study is part\nof a Master dissertation, our aim is to make a short overview and present\nadditionally some new approximation methods for the infinite time ruin\nprobabilities for the perturbed risk model. We present four different\napproximation methods for the perturbed risk model. The first method is based\non iterative upper and lower approximations to the maximal aggregate loss\ndistribution. The second method relies on a four-moment exponential De Vylder\napproximation. The third method is based on the first-order Pad\\'e\napproximation of the Renyi and De Vylder approximations. The last method is the\nsecond order Pad\\'e-Ramsay approximation. These are generated by fitting one,\ntwo, three or four moments of the claim amount distribution, which greatly\ngeneralizes the approximations. We test the precision of approximations using a\ncombination of light and heavy tailed distributions for the individual claim\namount. We assess the ultimate ruin probability and present numerical results\nfor the exponential, gamma, and mixed exponential claim distributions,\ndemonstrating the high accuracy of these four methods. Analytical and numerical\nmethods are used to highlight the practical implications of our findings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:06:05 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koucha", "Yacine", ""], ["Reis", "Alfredo D. Egidio dos", ""]]}, {"id": "2107.02546", "submitter": "Cheng Chang", "authors": "Chang Cheng, Yadong Yan, Mingjun Guan, Jianan Zhang, Yu Wang", "title": "Tactile Sensing with a Tendon-Driven Soft Robotic Finger", "comments": "6 pages, 10 figures, submitted to ICCMA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel tactile sensing mechanism for soft robotic fingers is\nproposed. Inspired by the proprioception mechanism found in mammals, the\nproposed approach infers tactile information from a strain sensor attached on\nthe finger's tendon. We perform experiments to test the tactile sensing\ncapabilities of the proposed structures, and our results indicate this method\nis capable of palpating texture and stiffness in both abduction and flexion\ncontact. Under systematic cross validation, the proposed system achieved 100%\nand 99.7% accuracy in texture and stiffness discrimination respectively, which\nvalidate the viability of this approach. Furthermore, we use statistics tools\nto determine the significance of various features extracted for classification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:22:49 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Cheng", "Chang", ""], ["Yan", "Yadong", ""], ["Guan", "Mingjun", ""], ["Zhang", "Jianan", ""], ["Wang", "Yu", ""]]}, {"id": "2107.02633", "submitter": "Luis Enrique Correa Rocha Prof", "authors": "Luis E C Rocha and Petter Holme and Claudio D G Linhares", "title": "The global migration network of sex-workers", "comments": "Comments and feedback welcomed. Two tables and 6 figures including SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN physics.pop-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differences in the social and economic environment across countries encourage\nhumans to migrate in search of better living conditions, including job\nopportunities, higher salaries, security and welfare. Quantifying global\nmigration is, however, challenging because of poor recording, privacy issues\nand residence status. This is particularly critical for some classes of\nmigrants involved in stigmatised, unregulated or illegal activities. Escorting\nservices or high-end prostitution are well-paid activities that attract workers\nall around the world. In this paper, we study international migration patterns\nof sex-workers by using network methods. Using an extensive international\nonline advertisement directory of escorting services and information about\nindividual escorts, we reconstruct a migrant flow network where nodes represent\neither origin or destination countries. The links represent the direct routes\nbetween two countries. The migration network of sex-workers shows different\nstructural patterns than the migration of the general population. The network\ncontains a strong core where mutual migration is often observed between a group\nof high-income European countries, yet Europe is split into different network\ncommunities with specific ties to non-European countries. We find\nnon-reciprocal relations between countries, with some of them mostly offering\nwhile others attract workers. The GDP per capita is a good indicator of country\nattractiveness for incoming workers and service rates but is unrelated to the\nprobability of emigration. The median financial gain of migrating, in\ncomparison to working at the home country, is 15.9%. Only sex-workers coming\nfrom 77% of the countries have financial gains with migration and average gains\ndecrease with the GDPc of the country of origin. Our results shows that\nhigh-end sex-worker migration is regulated by economic, geographic and cultural\naspects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:18:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Rocha", "Luis E C", ""], ["Holme", "Petter", ""], ["Linhares", "Claudio D G", ""]]}, {"id": "2107.02677", "submitter": "Andrey Skripnikov", "authors": "A. Skripnikov (1), N. Wagner (1), J. Shafer (2), M. Beck (3), E.\n  Sherwood (3), M. Burke (3) ((1) New College of Florida, (2) Science and\n  Environment Council of Southwest Florida, (3) Tampa Bay Estuary Program)", "title": "Using Localized Twitter Activity for Red Tide Impact Assessment", "comments": "40 pages,11 figures (27 image files though), 5 tables, submitted to\n  \"Harmful Algae\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Red tide blooms of the dinoflagellate Karenia brevis (K. brevis) produce\ntoxic coastal conditions that can impact marine organisms and human health,\nwhile also affecting local economies. During the extreme Florida red tide event\nof 2017-2019, residents and visitors turned to social media platforms to both\nreceive disaster-related information and communicate their own sentiments and\nexperiences. This was the first major red tide event since the ubiquitous use\nof social media, thus providing unique crowd-sourced reporting of red tide\nimpacts. We evaluated the spatial and temporal accuracy of red tide topic\nactivity on Twitter, taking tweet sentiments and user types (e.g. media,\ncitizens) into consideration, and compared tweet activity with reported red\ntide conditions, such as K. brevis cell counts, levels of dead fish and\nrespiratory irritation on local beaches. The analysis was done on multiple\nlevels with respect to both locality (e.g., entire Gulf coast, county-level,\ncity-level, zip code tabulation areas) and temporal frequencies (e.g. daily,\nevery three days, weekly), resulting in strong correlations between local\nper-capita Twitter activity and the actual red tide conditions observed in the\narea. Moreover, an association was observed between proximity to the affected\ncoastal areas and per-capita counts for relevant tweets. Results show that\nTwitter is a reliable proxy of the red tide's local impacts and development\nover time, which can potentially be used as one of the tools for more efficient\nassessment and a more coordinated response to the disaster in real time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:35:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Skripnikov", "A.", ""], ["Wagner", "N.", ""], ["Shafer", "J.", ""], ["Beck", "M.", ""], ["Sherwood", "E.", ""], ["Burke", "M.", ""]]}, {"id": "2107.02886", "submitter": "Annabel Davies", "authors": "Annabel L. Davies, Theodoros Papakonstantinou, Adriani Nikolakopoulou,\n  Gerta R\\\"ucker and Tobias Galla", "title": "Network meta-analysis and random walks", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis (NMA) is a central tool for evidence synthesis in\nclinical research. The results of an NMA depend critically on the quality of\nevidence being pooled. In assessing the validity of an NMA, it is therefore\nimportant to know the proportion contributions of each direct treatment\ncomparison to each network treatment effect. The construction of proportion\ncontributions is based on the observation that each row of the hat matrix\nrepresents a so-called 'evidence flow network' for each treatment comparison.\nHowever, the existing algorithm used to calculate these values is associated\nwith ambiguity according to the selection of paths. In this work we present a\nnovel analogy between NMA and random walks. We use this analogy to derive\nclosed-form expressions for the proportion contributions. A random walk on a\ngraph is a stochastic process that describes a succession of random 'hops'\nbetween vertices which are connected by an edge. The weight of an edge relates\nto the probability that the walker moves along that edge. We use the graph\nrepresentation of NMA to construct the transition matrix for a random walk on\nthe network of evidence. We show that the net number of times a walker crosses\neach edge of the network is related to the evidence flow network. By then\ndefining a random walk on the directed evidence flow network, we derive\nanalytically the matrix of proportion contributions. The random-walk approach,\nin addition to being computationally more efficient, has none of the associated\nambiguity of the existing algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:43:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Davies", "Annabel L.", ""], ["Papakonstantinou", "Theodoros", ""], ["Nikolakopoulou", "Adriani", ""], ["R\u00fccker", "Gerta", ""], ["Galla", "Tobias", ""]]}, {"id": "2107.02906", "submitter": "Alessandro Casa", "authors": "Maria Frizzarin, Antonio Bevilacqua, Bhaskar Dhariyal, Katarina\n  Domijan, Federico Ferraccioli, Elena Hayes, Georgiana Ifrim, Agnieszka\n  Konkolewska, Thach Le Nguyen, Uche Mbaka, Giovanna Ranzato, Ashish Singh,\n  Marco Stefanucci, Alessandro Casa", "title": "Mid infrared spectroscopy and milk quality traits: a data analysis\n  competition at the \"International Workshop on Spectroscopy and Chemometrics\n  2021\"", "comments": "17 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A chemometric data analysis challenge has been arranged during the first\nedition of the \"International Workshop on Spectroscopy and Chemometrics\",\norganized by the Vistamilk SFI Research Centre and held online in April 2021.\nThe aim of the competition was to build a calibration model in order to predict\nmilk quality traits exploiting the information contained in mid-infrared\nspectra only. Three different traits have been provided, presenting\nheterogeneous degrees of prediction complexity thus possibly requiring\ntrait-specific modelling choices. In this paper the different approaches\nadopted by the participants are outlined and the insights obtained from the\nanalyses are critically discussed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:40:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Frizzarin", "Maria", ""], ["Bevilacqua", "Antonio", ""], ["Dhariyal", "Bhaskar", ""], ["Domijan", "Katarina", ""], ["Ferraccioli", "Federico", ""], ["Hayes", "Elena", ""], ["Ifrim", "Georgiana", ""], ["Konkolewska", "Agnieszka", ""], ["Nguyen", "Thach Le", ""], ["Mbaka", "Uche", ""], ["Ranzato", "Giovanna", ""], ["Singh", "Ashish", ""], ["Stefanucci", "Marco", ""], ["Casa", "Alessandro", ""]]}, {"id": "2107.03119", "submitter": "Sheng Dai", "authors": "Sheng Dai", "title": "Variable selection in convex quantile regression: L1-norm or L0-norm\n  regularization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The curse of dimensionality is a recognized challenge in nonparametric\nestimation. This paper develops a new L0-norm regularization approach to the\nconvex quantile and expectile regressions for subset variable selection. We\nshow how to use mixed integer programming to solve the proposed L0-norm\nregularization approach in practice and build a link to the commonly used\nL1-norm regularization approach. A Monte Carlo study is performed to compare\nthe finite sample performances of the proposed L0-penalized convex quantile and\nexpectile regression approaches with the L1-norm regularization approaches. The\nproposed approach is further applied to benchmark the sustainable development\nperformance of the OECD countries and empirically analyze the accuracy in the\ndimensionality reduction of variables. The results from the simulation and\napplication illustrate that the proposed L0-norm regularization approach can\nmore effectively address the curse of dimensionality than the L1-norm\nregularization approach in multidimensional spaces.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:58:05 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Dai", "Sheng", ""]]}, {"id": "2107.03230", "submitter": "Luka Grb\\v{c}i\\'c", "authors": "Luka Grb\\v{c}i\\'c, Sini\\v{s}a Dru\\v{z}eta, Goran Mau\\v{s}a, Tomislav\n  Lipi\\'c, Darija Vuki\\'c Lu\\v{s}i\\'c, Marta Alvir, Ivana Lu\\v{c}in, Ante\n  Sikirica, Davor Davidovi\\'c, Vanja Trava\\v{s}, Daniela Kalafatovi\\'c,\n  Kristina Pikelj, Hana Fajkovi\\'c, Toni Holjevi\\'c and Lado Kranj\\v{c}evi\\'c", "title": "Coastal water quality prediction based on machine learning with feature\n  interpretation and spatio-temporal analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Coastal water quality management is a public health concern, as poor coastal\nwater quality can harbor pathogens that are dangerous to human health.\nTourism-oriented countries need to actively monitor the condition of coastal\nwater at tourist popular sites during the summer season. In this study, routine\nmonitoring data of $Escherichia\\ Coli$ and enterococci across 15 public beaches\nin the city of Rijeka, Croatia, were used to build machine learning models for\npredicting their levels based on environmental parameters as well as to\ninvestigate their relationships with environmental stressors. Gradient Boosting\n(Catboost, Xgboost), Random Forests, Support Vector Regression and Artificial\nNeural Networks were trained with measurements from all sampling sites and used\nto predict $E.\\ Coli$ and enterococci values based on environmental features.\nThe evaluation of stability and generalizability with 10-fold cross validation\nanalysis of the machine learning models, showed that the Catboost algorithm\nperformed best with R$^2$ values of 0.71 and 0.68 for predicting $E.\\ Coli$ and\nenterococci, respectively, compared to other evaluated ML algorithms including\nXgboost, Random Forests, Support Vector Regression and Artificial Neural\nNetworks. We also use the SHapley Additive exPlanations technique to identify\nand interpret which features have the most predictive power. The results show\nthat site salinity measured is the most important feature for forecasting both\n$E.\\ Coli$ and enterococci levels. Finally, the spatial and temporal accuracy\nof both ML models were examined at sites with the lowest coastal water quality.\nThe spatial $E. Coli$ and enterococci models achieved strong R$^2$ values of\n0.85 and 0.83, while the temporal models achieved R$^2$ values of 0.74 and\n0.67. The temporal model also achieved moderate R$^2$ values of 0.44 and 0.46\nat a site with high coastal water quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:00:14 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:09:03 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Grb\u010di\u0107", "Luka", ""], ["Dru\u017eeta", "Sini\u0161a", ""], ["Mau\u0161a", "Goran", ""], ["Lipi\u0107", "Tomislav", ""], ["Lu\u0161i\u0107", "Darija Vuki\u0107", ""], ["Alvir", "Marta", ""], ["Lu\u010din", "Ivana", ""], ["Sikirica", "Ante", ""], ["Davidovi\u0107", "Davor", ""], ["Trava\u0161", "Vanja", ""], ["Kalafatovi\u0107", "Daniela", ""], ["Pikelj", "Kristina", ""], ["Fajkovi\u0107", "Hana", ""], ["Holjevi\u0107", "Toni", ""], ["Kranj\u010devi\u0107", "Lado", ""]]}, {"id": "2107.03249", "submitter": "Christoph Gerlinger", "authors": "Sarah B\\\"ohme, Christoph Gerlinger, Susanne Huschens, Annett Kucka,\n  Niclas K\\\"urschner, Friedhelm Leverkus, Michael Schlichting, Waldemar\n  Siemens, Kati Sternberg, Liping Hofmann-Xu", "title": "Patient-reported outcomes in the context of the benefit assessment in\n  Germany", "comments": "46 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Since the 2011 Act on the Reform of the Market for Medicinal Products,\nbenefit dossiers are submitted by pharmaceutical companies to facilitate the\nHealth Technology Assessment (HTA) appraisals in Germany. The Institute for\nQuality and Efficiency in Health Care conducts the added benefit assessment\nfollowing their General Methods Paper, which was updated November 5, 2020. This\nWhite Paper is dedicated to patient-reported outcomes (PRO) to highlight their\nimportance for the added benefit assessment. We focus on methodological aspects\nbut consider also other relevant requirements and challenges, which are\ndemanded by G-BA and IQWiG. The following topics will be presented and\ndiscussed: 1. Role of PRO in HTA decision making exemplary to benefit\nassessment in Germany 2. Guidances of PRO evaluations 3. PRO Estimand framework\n4. Perception and requirements for PRO within the German benefit assessment 5.\nValidity of instrument 6. Response thresholds for assessing clinical relevance\nof PRO 7. PRO endpoints / outcome measures / operationalization 8. Missing PRO\ndata 9. PRO after treatment discontinuation This White Paper aims to provide\ndeeper insights about new requirements concerning PRO evaluations for HTA\ndecision making in Germany, highlight points to consider that should inform\nglobal development in terms of study planning and frame the requirements also\nin the context of global recommendations and guidelines. We also aim to enhance\nthe understanding of the complexity when preparing the benefit dossier and\npromote further scientific discussions where appropriate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:26:49 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 08:03:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["B\u00f6hme", "Sarah", ""], ["Gerlinger", "Christoph", ""], ["Huschens", "Susanne", ""], ["Kucka", "Annett", ""], ["K\u00fcrschner", "Niclas", ""], ["Leverkus", "Friedhelm", ""], ["Schlichting", "Michael", ""], ["Siemens", "Waldemar", ""], ["Sternberg", "Kati", ""], ["Hofmann-Xu", "Liping", ""]]}, {"id": "2107.03289", "submitter": "Mikkel Meyer Andersen", "authors": "Mikkel M Andersen and David J Balding", "title": "Assessing the forensic value of DNA evidence from Y chromosomes and\n  mitogenomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Y-chromosomal and mitochondrial DNA profiles have been used as evidence in\ncourts for decades, yet the problem of evaluating the weight of evidence has\nnot been adequately resolved. Both are lineage markers (inherited from just one\nparent), which presents different interpretation challenges compared with\nstandard autosomal DNA profiles (inherited from both parents), for which\nrecombination increases profile diversity and weakens the effects of\nrelatedness. We review approaches to the evaluation of lineage marker profiles\nfor forensic identification, focussing on the key roles of profile mutation\nrate and relatedness. Higher mutation rates imply fewer individuals matching\nthe profile of an alleged contributor, but they will be more closely related.\nThis makes it challenging to evaluate the possibility that one of these\nmatching individuals could be the true source, because relatedness may make\nthem more plausible alternative contributors than less-related individuals, and\nthey may not be well mixed in the population. These issues reduce the\nusefulness of profile databases drawn from a broad population: the larger the\npopulation, the lower the profile relative frequency because of lower\nrelatedness with the alleged contributor. Many evaluation methods do not\nadequately take account of relatedness, but its effects have become more\npronounced with the latest generation of high-mutation-rate Y profiles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:25:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Andersen", "Mikkel M", ""], ["Balding", "David J", ""]]}, {"id": "2107.03366", "submitter": "Alexander Mayer", "authors": "Alexander Mayer and Dominik Wied", "title": "Estimation and Inference in Factor Copula Models with Exogenous\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A factor copula model is proposed in which factors are either simulable or\nestimable from exogenous information. Point estimation and inference are based\non a simulated methods of moments (SMM) approach with non-overlapping\nsimulation draws. Consistency and limiting normality of the estimator is\nestablished and the validity of bootstrap standard errors is shown. Doing so,\nprevious results from the literature are verified under low-level conditions\nimposed on the individual components of the factor structure. Monte Carlo\nevidence confirms the accuracy of the asymptotic theory in finite samples and\nan empirical application illustrates the usefulness of the model to explain the\ncross-sectional dependence between stock returns.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:25:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Mayer", "Alexander", ""], ["Wied", "Dominik", ""]]}, {"id": "2107.03431", "submitter": "Anastasia Mantziou", "authors": "Anastasia Mantziou, Simon Lunagomez, Robin Mitra", "title": "Bayesian model-based clustering for multiple network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is increasing appetite for analysing multiple network data. This is\ndifferent to analysing traditional data sets, where now each observation in the\ndata comprises a network. Recent technological advancements have allowed the\ncollection of this type of data in a range of different applications. This has\ninspired researchers to develop statistical models that most accurately\ndescribe the probabilistic mechanism that generates a network population and\nuse this to make inferences about the underlying structure of the network data.\nOnly a few studies developed to date consider the heterogeneity that can exist\nin a network population. We propose a Mixture of Measurement Error Models for\nidentifying clusters of networks in a network population, with respect to\nsimilarities detected in the connectivity patterns among the networks' nodes.\nExtensive simulation studies show our model performs well in both clustering\nmultiple network data and inferring the model parameters. We further apply our\nmodel on two real world multiple network data sets resulting from the fields of\nComputing (Human Tracking Systems) and Neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 18:29:53 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Mantziou", "Anastasia", ""], ["Lunagomez", "Simon", ""], ["Mitra", "Robin", ""]]}, {"id": "2107.03544", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Ashley E. Walton, Linda M. Collins, Predrag Klasnja,\n  Stephanie T. Lanza, Inbal Nahum-Shani, Mashifiqui Rabbi, Michael A. Russell,\n  Maureen A. Walton, Hyesun Yoo, Susan A. Murphy", "title": "The Micro-Randomized Trial for Developing Digital Interventions:\n  Experimental Design and Data Analysis Considerations", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.05880,\n  arXiv:2004.10241", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Just-in-time adaptive interventions (JITAIs) are time-varying adaptive\ninterventions that use frequent opportunities for the intervention to be\nadapted--weekly, daily, or even many times a day. The micro-randomized trial\n(MRT) has emerged for use in informing the construction of JITAIs. MRTs can be\nused to address research questions about whether and under what circumstances\nJITAI components are effective, with the ultimate objective of developing\neffective and efficient JITAI. The purpose of this article is to clarify why,\nwhen, and how to use MRTs; to highlight elements that must be considered when\ndesigning and implementing an MRT; and to review primary and secondary analyses\nmethods for MRTs. We briefly review key elements of JITAIs and discuss a\nvariety of considerations that go into planning and designing an MRT. We\nprovide a definition of causal excursion effects suitable for use in primary\nand secondary analyses of MRT data to inform JITAI development. We review the\nweighted and centered least-squares (WCLS) estimator which provides consistent\ncausal excursion effect estimators from MRT data. We describe how the WCLS\nestimator along with associated test statistics can be obtained using standard\nstatistical software such as R (R Core Team, 2019). Throughout we illustrate\nthe MRT design and analyses using the HeartSteps MRT, for developing a JITAI to\nincrease physical activity among sedentary individuals. We supplement the\nHeartSteps MRT with two other MRTs, SARA and BariFit, each of which highlights\ndifferent research questions that can be addressed using the MRT and\nexperimental design considerations that might arise.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 00:25:59 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 05:40:51 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Qian", "Tianchen", ""], ["Walton", "Ashley E.", ""], ["Collins", "Linda M.", ""], ["Klasnja", "Predrag", ""], ["Lanza", "Stephanie T.", ""], ["Nahum-Shani", "Inbal", ""], ["Rabbi", "Mashifiqui", ""], ["Russell", "Michael A.", ""], ["Walton", "Maureen A.", ""], ["Yoo", "Hyesun", ""], ["Murphy", "Susan A.", ""]]}, {"id": "2107.03617", "submitter": "Dale Townsend", "authors": "D. Townsend, C. Nel", "title": "Traffic prediction at signalised intersections using Integrated Nested\n  Laplace Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Bayesian approach to predicting traffic flows at signalised intersections\nis considered using the the INLA framework. INLA is a deterministic,\ncomputationally efficient alternative to MCMC for estimating a posterior\ndistribution. It is designed for latent Gaussian models where the parameters\nfollow a joint Gaussian distribution. An assumption which naturally evolves\nfrom an LGM is that of a Gaussian Markov Random Field (GMRF). It can be shown\nthat a traffic prediction model based in both space and time satisfies this\nassumption, and as such the INLA algorithm provides accurate prediction when\nspace, time, and other relevant covariants are included in the model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 05:45:15 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Townsend", "D.", ""], ["Nel", "C.", ""]]}, {"id": "2107.03619", "submitter": "Dale Townsend", "authors": "D. Townsend", "title": "Validation and Inference of Agent Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Agent Based Modelling (ABM) is a computational framework for simulating the\nbehaviours and interactions of autonomous agents. As Agent Based Models are\nusually representative of complex systems, obtaining a likelihood function of\nthe model parameters is nearly always intractable. There is a necessity to\nconduct inference in a likelihood free context in order to understand the model\noutput. Approximate Bayesian Computation is a suitable approach for this\ninference. It can be applied to an Agent Based Model to both validate the\nsimulation and infer a set of parameters to describe the model. Recent research\nin ABC has yielded increasingly efficient algorithms for calculating the\napproximate likelihood. These are investigated and compared using a pedestrian\nmodel in the Hamilton CBD.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 05:53:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Townsend", "D.", ""]]}, {"id": "2107.03686", "submitter": "Tommaso Costa", "authors": "Tommaso Costa, Franco Cauda", "title": "A bayesian reanalysis of the phase III aducanumab (ADU) trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we have conducted a reanalysis of the phase III aducanumab\n(ADU) summary statistics announced by Biogen, in particular the result of the\nClinical Dementia Rating-Sum of Boxes (CDR-SB). The results showed that the\nevidence on the efficacy of the drug is very low and a more clearer view of the\nresults of clinical trials are presented in the Bayesian framework that can be\nuseful for future development and research in the field.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 09:00:57 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 17:46:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Costa", "Tommaso", ""], ["Cauda", "Franco", ""]]}, {"id": "2107.03825", "submitter": "Argyrios Vartholomaios", "authors": "Argyrios Vartholomaios, Stamatis Karlos, Eleftherios Kouloumpris,\n  Grigorios Tsoumakas", "title": "Short-term Renewable Energy Forecasting in Greece using Prophet\n  Decomposition and Tree-based Ensembles", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy production using renewable sources exhibits inherent uncertainties due\nto their intermittent nature. Nevertheless, the unified European energy market\npromotes the increasing penetration of renewable energy sources (RES) by the\nregional energy system operators. Consequently, RES forecasting can assist in\nthe integration of these volatile energy sources, since it leads to higher\nreliability and reduced ancillary operational costs for power systems. This\npaper presents a new dataset for solar and wind energy generation forecast in\nGreece and introduces a feature engineering pipeline that enriches the\ndimensional space of the dataset. In addition, we propose a novel method that\nutilizes the innovative Prophet model, an end-to-end forecasting tool that\nconsiders several kinds of nonlinear trends in decomposing the energy time\nseries before a tree-based ensemble provides short-term predictions. The\nperformance of the system is measured through representative evaluation\nmetrics, and by estimating the model's generalization under an industryprovided\nscheme of absolute error thresholds. The proposed hybrid model competes with\nbaseline persistence models, tree-based regression ensembles, and the Prophet\nmodel, managing to outperform them, presenting both lower error rates and more\nfavorable error distribution.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:12:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Vartholomaios", "Argyrios", ""], ["Karlos", "Stamatis", ""], ["Kouloumpris", "Eleftherios", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2107.03863", "submitter": "Felix Leopoldo Rios", "authors": "Felix L. Rios, Giusi Moffa, Jack Kuipers", "title": "Benchpress: a scalable and platform-independent workflow for\n  benchmarking structure learning algorithms for graphical models", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the relationship between the variables in a study domain and\nmodelling the data generating mechanism is a fundamental problem in many\nempirical sciences. Probabilistic graphical models are one common approach to\ntackle the problem. Learning the graphical structure is computationally\nchallenging and a fervent area of current research with a plethora of\nalgorithms being developed. To facilitate the benchmarking of different\nmethods, we present a novel automated workflow, called benchpress for producing\nscalable, reproducible, and platform-independent benchmarks of structure\nlearning algorithms for probabilistic graphical models. Benchpress is\ninterfaced via a simple JSON-file, which makes it accessible for all users,\nwhile the code is designed in a fully modular fashion to enable researchers to\ncontribute additional methodologies. Benchpress currently provides an interface\nto a large number of state-of-the-art algorithms from libraries such as BiDAG,\nbnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as\na variety of methods for data generating models and performance evaluation.\nAlongside user-defined models and randomly generated datasets, the software\ntool also includes a number of standard datasets and graphical models from the\nliterature, which may be included in a benchmarking workflow. We demonstrate\nthe applicability of this workflow for learning Bayesian networks in four\ntypical data scenarios. The source code and documentation is publicly available\nfrom http://github.com/felixleopoldo/benchpress.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:19:28 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Rios", "Felix L.", ""], ["Moffa", "Giusi", ""], ["Kuipers", "Jack", ""]]}, {"id": "2107.03975", "submitter": "Jorge Silva", "authors": "Jorge F. Silva", "title": "Compressibility Analysis of Asymptotically Mean Stationary Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work provides new results for the analysis of random sequences in terms\nof $\\ell_p$-compressibility. The results characterize the degree in which a\nrandom sequence can be approximated by its best $k$-sparse version under\ndifferent rates of significant coefficients (compressibility analysis). In\nparticular, the notion of strong $\\ell_p$-characterization is introduced to\ndenote a random sequence that has a well-defined asymptotic limit (sample-wise)\nof its best $k$-term approximation error when a fixed rate of significant\ncoefficients is considered (fixed-rate analysis). The main theorem of this work\nshows that the rich family of asymptotically mean stationary (AMS) processes\nhas a strong $\\ell_p$-characterization. Furthermore, we present results that\ncharacterize and analyze the $\\ell_p$-approximation error function for this\nfamily of processes. Adding ergodicity in the analysis of AMS processes, we\nintroduce a theorem demonstrating that the approximation error function is\nconstant and determined in closed-form by the stationary mean of the process.\nOur results and analyses contribute to the theory and understanding of\ndiscrete-time sparse processes and, on the technical side, confirm how\ninstrumental the point-wise ergodic theorem is to determine the compressibility\nexpression of discrete-time processes even when stationarity and ergodicity\nassumptions are relaxed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:06:21 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Silva", "Jorge F.", ""]]}, {"id": "2107.04010", "submitter": "Alise Danielle Midtfjord", "authors": "Alise Danielle Midtfjord, Riccardo De Bin and Arne Bang Huseby", "title": "A Machine Learning Approach to Safer Airplane Landings: Predicting\n  Runway Conditions using Weather and Flight Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of snow and ice on runway surfaces reduces the available\ntire-pavement friction needed for retardation and directional control and\ncauses potential economic and safety threats for the aviation industry during\nthe winter seasons. To activate appropriate safety procedures, pilots need\naccurate and timely information on the actual runway surface conditions. In\nthis study, XGBoost is used to create a combined runway assessment system,\nwhich includes a classifcation model to predict slippery conditions and a\nregression model to predict the level of slipperiness. The models are trained\non weather data and data from runway reports. The runway surface conditions are\nrepresented by the tire-pavement friction coefficient, which is estimated from\nflight sensor data from landing aircrafts. To evaluate the performance of the\nmodels, they are compared to several state-of-the-art runway assessment\nmethods. The XGBoost models identify slippery runway conditions with a ROC AUC\nof 0.95, predict the friction coefficient with a MAE of 0.0254, and outperforms\nall the previous methods. The results show the strong abilities of machine\nlearning methods to model complex, physical phenomena with a good accuracy when\ndomain knowledge is used in the variable extraction. The XGBoost models are\ncombined with SHAP (SHapley Additive exPlanations) approximations to provide a\ncomprehensible decision support system for airport operators and pilots, which\ncan contribute to safer and more economic operations of airport runways.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:01:13 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Midtfjord", "Alise Danielle", ""], ["De Bin", "Riccardo", ""], ["Huseby", "Arne Bang", ""]]}, {"id": "2107.04029", "submitter": "Florian Wirthm\\\"uller", "authors": "Florian Wirthm\\\"uller, Jochen Hipp, Christian Reichenb\\\"acher and\n  Manfred Reichert", "title": "The Atlas of Lane Changes: Investigating Location-dependent Lane Change\n  Behaviors Using Measurement Data from a Customer Fleet", "comments": "the article has been accepted for publication during the 24th IEEE\n  Intelligent Transportation Systems Conference (ITSC), 8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of surrounding traffic participants behavior is a crucial and\nchallenging task for driver assistance and autonomous driving systems. Today's\napproaches mainly focus on modeling dynamic aspects of the traffic situation\nand try to predict traffic participants behavior based on this. In this article\nwe take a first step towards extending this common practice by calculating\nlocation-specific a-priori lane change probabilities. The idea behind this is\nstraight forward: The driving behavior of humans may vary in exactly the same\ntraffic situation depending on the respective location. E.g. drivers may ask\nthemselves: Should I pass the truck in front of me immediately or should I wait\nuntil reaching the less curvy part of my route lying only a few kilometers\nahead? Although, such information is far away from allowing behavior prediction\non its own, it is obvious that today's approaches will greatly benefit when\nincorporating such location-specific a-priori probabilities into their\npredictions. For example, our investigations show that highway interchanges\ntend to enhance driver's motivation to perform lane changes, whereas curves\nseem to have lane change-dampening effects. Nevertheless, the investigation of\nall considered local conditions shows that superposition of various effects can\nlead to unexpected probabilities at some locations. We thus suggest dynamically\nconstructing and maintaining a lane change probability map based on customer\nfleet data in order to support onboard prediction systems with additional\ninformation. For deriving reliable lane change probabilities a broad customer\nfleet is the key to success.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:29:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 06:45:16 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wirthm\u00fcller", "Florian", ""], ["Hipp", "Jochen", ""], ["Reichenb\u00e4cher", "Christian", ""], ["Reichert", "Manfred", ""]]}, {"id": "2107.04208", "submitter": "Michael Bertolacci", "authors": "Noel Cressie and Michael Bertolacci and Andrew Zammit-Mangion", "title": "From Many to One: Consensus Inference in a MIP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Model Intercomparison Project (MIP) consists of teams who each estimate the\nsame underlying quantity (e.g., temperature projections to the year 2070), and\nthe spread of the estimates indicates their uncertainty. It recognizes that a\ncommunity of scientists will not agree completely but that there is value in\nlooking for a consensus and information in the range of disagreement. A simple\naverage of the teams' outputs gives a consensus estimate, but it does not\nrecognize that some outputs are more variable than others. Statistical analysis\nof variance (ANOVA) models offer a way to obtain a weighted consensus estimate\nof outputs with a variance that is the smallest possible and hence the tightest\npossible 'one-sigma' and 'two-sigma' intervals. Modulo dependence between MIP\noutputs, the ANOVA approach weights a team's output inversely proportional to\nits variation. When external verification data are available for evaluating the\nfidelity of each MIP output, ANOVA weights can also provide a prior\ndistribution for Bayesian Model Averaging to yield a consensus estimate. We use\na MIP of carbon dioxide flux inversions to illustrate the ANOVA-based weighting\nand subsequent consensus inferences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 04:56:18 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cressie", "Noel", ""], ["Bertolacci", "Michael", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "2107.04316", "submitter": "Janne R\\\"aty", "authors": "Janne R\\\"aty, Johannes Breidenbach, Marius Hauglin, Rasmus Astrup", "title": "Prediction of butt rot volume in Norway spruce forest stands using\n  harvester, remotely sensed and environmental data", "comments": "22 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Butt rot (BR) damages associated with Norway spruce (Picea abies [L.] Karst.)\naccount for considerable economic losses in timber production across the\nnorthern hemisphere. While information on BR damages is critical for optimal\ndecision-making in forest management, the maps of BR damages are typically\nlacking in forest information systems. We predicted timber volume damaged by BR\nat the stand-level in Norway using harvester information of 186,026 stems\n(clear-cuts), remotely sensed, and environmental data (e.g. climate and terrain\ncharacteristics). We utilized random forest (RF) models with two sets of\npredictor variables: (1) predictor variables available after harvest\n(theoretical case) and (2) predictor variables available prior to harvest\n(mapping case). We found that forest attributes characterizing the maturity of\nforest, such as remote sensing-based height, harvested timber volume and\nquadratic mean diameter at breast height, were among the most important\npredictor variables. Remotely sensed predictor variables obtained from airborne\nlaser scanning data and Sentinel-2 imagery were more important than the\nenvironmental variables. The theoretical case with a leave-stand-out\ncross-validation achieved an RMSE of 11.4 $m^3ha^{-1}$ (pseudo $R^2$: 0.66)\nwhereas the mapping case resulted in a pseudo $R^2$ of 0.60. When the spatially\ndistinct k-means clusters of harvested forest stands were used as units in the\ncross-validation, the RMSE value and pseudo $R^2$ associated with the mapping\ncase were 15.6 $m^3ha^{-1}$ and 0.37, respectively. This indicates that the\nknowledge about the BR status of spatially close stands is of high importance\nfor obtaining satisfactory error rates in the mapping of BR damages.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:09:32 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["R\u00e4ty", "Janne", ""], ["Breidenbach", "Johannes", ""], ["Hauglin", "Marius", ""], ["Astrup", "Rasmus", ""]]}, {"id": "2107.04412", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Xiang Yan, Xilei Zhao and Zhuoxuan Cao", "title": "Identifying latent shared mobility preference segments in low-income\n  communities: ride-hailing, fixed-route bus, and mobility-on-demand transit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts of Mobility-on-Demand (MOD) and Mobility as a Service (MaaS), which\nfeature the integration of various shared-use mobility options, have gained\nwidespread popularity in recent years. While these concepts promise great\nbenefits to travelers, their heavy reliance on technology raises equity\nconcerns as socially disadvantaged population groups can be left out in an era\nof on-demand mobility. This paper investigates the potential uptake of MOD\ntransit services (integrated fixed-route and on-demand services) among\ntravelers living in low-income communities. Specially, we analyze people's\nlatent attitude towards three shared-use mobility services, including\nride-hailing services, fixed-route transit, and MOD transit. We conduct a\nlatent class cluster analysis of 825 survey respondents sampled from low-income\nneighborhoods in Detroit and Ypsilanti, Michigan. We identified three latent\nsegments: shared-mode enthusiast, shared-mode opponent, and fixed-route transit\nloyalist. People from the shared-mode enthusiast segment often use ride-hailing\nservices and live in areas with poor transit access, and they are likely to be\nthe early adopters of MOD transit services. The shared-mode opponent segment\nmainly includes vehicle owners who lack interests in shared mobility options.\nThe fixed-route transit loyalist segment includes a considerable share of\nlow-income individuals who face technological barriers to use the MOD transit.\nWe also find that males, college graduates, car owners, people with a mobile\ndata plan, and people living in poor-transit-access areas have a higher level\nof preferences for MOD transit services. We conclude with policy\nrecommendations for developing more accessible and equitable MOD transit\nservices.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:21:08 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Xinyi", ""], ["Yan", "Xiang", ""], ["Zhao", "Xilei", ""], ["Cao", "Zhuoxuan", ""]]}, {"id": "2107.04496", "submitter": "Jizi Shangguan", "authors": "Jizi Shangguan", "title": "Joint Modeling of Longitudinal and Survival Data with Censored\n  Single-index Varying Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical and biological research, longitudinal data and survival data types\nare commonly seen. Traditional statistical models mostly consider to deal with\neither of the data types, such as linear mixed models for longitudinal data,\nand the Cox models for survival data, while they do not adjust the association\nbetween these two different data types. It is desirable to have a joint\nmodeling approach which accomadates both data types and the dependency between\nthem. In this paper, we extend traditional single-index models to a new joint\nmodeling approach, by replacing the single-index component to a varying\ncoefficient component to deal with longitudinal outcomes, and accomadate the\nrandom censoring problem in survival analysis by nonparametric synthetic data\nregression for the link function. Numerical experiments are conducted to\nevaluate the finite sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:37:43 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Shangguan", "Jizi", ""]]}, {"id": "2107.04647", "submitter": "Americo Cunha Jr", "authors": "Jo\\~ao Pedro Norenberg, Americo Cunha Jr, Samuel da Silva, Paulo\n  S\\'ergio Varoto", "title": "Global sensitivity analysis of (a)symmetric energy harvesters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CE cs.SY math.DS physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric variability is inevitable in actual energy harvesters and can\ndefine crucial aspects of the system performance, especially in susceptible\nsystems to small perturbations. In this way, this work aims to identify the\nmost critical parameters in the dynamics of (a)symmetric bistable energy\nharvesters with nonlinear piezoelectric coupling, considering the variability\nof their physical and excitation parameters. For this purpose, a global\nsensitivity analysis based on the Sobol' indices is performed by an orthogonal\ndecomposition in terms of conditional variances to access the dependence of the\nrecovered power concerning the harvester parameters. This technique quantifies\nthe variance concerning each parameter individually and jointly regarding the\ntotal variation of the model. The results indicate that the frequency and\namplitude of excitation, asymmetric bias angle, and piezoelectric coupling at\nthe electrical domain are the most influential parameters that affect the mean\npower harvested. It has also been shown that the order of importance of the\nparameters can change from stable conditions. In possession of this, a better\nunderstanding of the system under analysis is obtained, identifying vital\nparameters that rule the change of dynamic behavior and constituting a powerful\ntool in the robust design and prediction of nonlinear harvesters.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:45:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Norenberg", "Jo\u00e3o Pedro", ""], ["Cunha", "Americo", "Jr"], ["da Silva", "Samuel", ""], ["Varoto", "Paulo S\u00e9rgio", ""]]}, {"id": "2107.04839", "submitter": "Xin Chen", "authors": "Xin Chen, Rui Song, Jiajia Zhang, Swann Arp Adams, Liuquan Sun, Wenbin\n  Lu", "title": "On Estimating Optimal Regime for Treatment Initiation Time Based on\n  Restricted Mean Residual Lifetime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When to initiate treatment on patients is an important problem in many\nmedical studies such as AIDS and cancer. In this article, we formulate the\ntreatment initiation time problem for time-to-event data and propose an optimal\nindividualized regime that determines the best treatment initiation time for\nindividual patients based on their characteristics. Different from existing\noptimal treatment regimes where treatments are undertaken at a pre-specified\ntime, here new challenges arise from the complicated missing mechanisms in\ntreatment initiation time data and the continuous treatment rule in terms of\ninitiation time. To tackle these challenges, we propose to use restricted mean\nresidual lifetime as a value function to evaluate the performance of different\ntreatment initiation regimes, and develop a nonparametric estimator for the\nvalue function, which is consistent even when treatment initiation times are\nnot completely observable and their distribution is unknown. We also establish\nthe asymptotic properties of the resulting estimator in the decision rule and\nits associated value function estimator. In particular, the asymptotic\ndistribution of the estimated value function is nonstandard, which follows a\nweighted chi-squared distribution. The finite-sample performance of the\nproposed method is evaluated by simulation studies and is further illustrated\nwith an application to a breast cancer data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 13:52:49 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:27:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chen", "Xin", ""], ["Song", "Rui", ""], ["Zhang", "Jiajia", ""], ["Adams", "Swann Arp", ""], ["Sun", "Liuquan", ""], ["Lu", "Wenbin", ""]]}, {"id": "2107.05578", "submitter": "Markus Foote", "authors": "Markus D. Foote (1,2), Philip E. Dennison (3), Patrick R. Sullivan\n  (3), Kelly B. O'Neill (3), Andrew K. Thorpe (4), David R. Thompson (4),\n  Daniel H. Cusworth (4), Riley Duren (4,5), Sarang C. Joshi (1,2) ((1)\n  Scientific Computing and Imaging Institute, University of Utah (2) Department\n  of Biomedical Engineering, University of Utah (3) Department of Geography,\n  University of Utah (4) Jet Propulsion Laboratory, California Institute of\n  Technology (5) Office of Research, Innovation, and Impact, University of\n  Arizona)", "title": "Impact of Scene-Specific Enhancement Spectra on Matched Filter\n  Greenhouse Gas Retrievals from Imaging Spectroscopy", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph astro-ph.EP astro-ph.IM eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Matched filter (MF) techniques have been widely used for retrieval of\ngreenhouse gas enhancements (enh.) from imaging spectroscopy datasets. While\nmultiple algorithmic techniques and refinements have been proposed, the\ngreenhouse gas target spectrum used for concentration enh. estimation has\nremained largely unaltered since the introduction of quantitative MF\nretrievals. The magnitude of retrieved methane and carbon dioxide enh., and\nthereby integrated mass enh. (IME) and estimated flux of point-source emitters,\nis heavily dependent on this target spectrum. Current standard use of molecular\nabsorption coefficients to create unit enh. target spectra does not account for\nabsorption by background concentrations of greenhouse gases, solar and sensor\ngeometry, or atmospheric water vapor absorption. We introduce geometric and\natmospheric parameters into the generation of scene-specific (SS) unit enh.\nspectra to provide target spectra that are compatible with all greenhouse gas\nretrieval MF techniques. For methane plumes, IME resulting from use of\nstandard, generic enh. spectra varied from -22 to +28.7% compared to SS enh.\nspectra. Due to differences in spectral shape between the generic and SS enh.\nspectra, differences in methane plume IME were linked to surface spectral\ncharacteristics in addition to geometric and atmospheric parameters. IME\ndifferences for carbon dioxide plumes, with generic enh. spectra producing\nintegrated mass enh. -76.1 to -48.1% compared to SS enh. spectra. Fluxes\ncalculated from these integrated enh. would vary by the same %s, assuming\nequivalent wind conditions. Methane and carbon dioxide IME were most sensitive\nto changes in solar zenith angle and ground elevation. SS target spectra can\nimprove confidence in greenhouse gas retrievals and flux estimates across\ncollections of scenes with diverse geometric and atmospheric conditions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:44:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Foote", "Markus D.", ""], ["Dennison", "Philip E.", ""], ["Sullivan", "Patrick R.", ""], ["O'Neill", "Kelly B.", ""], ["Thorpe", "Andrew K.", ""], ["Thompson", "David R.", ""], ["Cusworth", "Daniel H.", ""], ["Duren", "Riley", ""], ["Joshi", "Sarang C.", ""]]}, {"id": "2107.05579", "submitter": "Thomas Mellan", "authors": "Tresnia Berah, Thomas A. Mellan, Xenia Miscouridou, Swapnil Mishra,\n  Kris V. Parag, Mikko S. Pakkanen and Samir Bhatt", "title": "Unifying the effective reproduction number, incidence, and prevalence\n  under a stochastic age-dependent branching process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Renewal equations are a popular approach used in modelling the number of new\ninfections (incidence) in an outbreak. A unified set of renewal equations where\nincidence, prevalence and cumulative incidence can all be recovered from the\nsame stochastic process has not been attempted. Here, we derive a set of\nrenewal equations from an age-dependent branching process with a time-varying\nreproduction number. Our new derivation utilises a measure-theoretic approach\nand yields a fully self-contained mathematical exposition. We find that the\nrenewal equations commonly used in epidemiology for modelling incidence are an\nequivalent special case of our equations. We show that these our equations are\ninternally consistent in the sense that they can be separately linked under the\ncommon back calculation approach between prevalence and incidence. We introduce\na computationally efficient discretisation scheme to solve these renewal\nequations, and this algorithm is highly parallelisable as it relies on row sums\nand elementwise multiplication. Finally we present a simple simulation example\nin the probabilistic programming language Stan where we jointly fit incidence\nand prevalence under a single time-varying reproduction number and generation\ninterval.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:58:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Berah", "Tresnia", ""], ["Mellan", "Thomas A.", ""], ["Miscouridou", "Xenia", ""], ["Mishra", "Swapnil", ""], ["Parag", "Kris V.", ""], ["Pakkanen", "Mikko S.", ""], ["Bhatt", "Samir", ""]]}, {"id": "2107.05592", "submitter": "Dhagash Mehta", "authors": "Cynthia Pagliaro, Dhagash Mehta, Han-Tai Shiao, Shaofei Wang, Luwei\n  Xiong", "title": "Investor Behavior Modeling by Analyzing Financial Advisor Notes: A\n  Machine Learning Perspective", "comments": "8 pages, 2 column format, 7 figures+5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling investor behavior is crucial to identifying behavioral coaching\nopportunities for financial advisors. With the help of natural language\nprocessing (NLP) we analyze an unstructured (textual) dataset of financial\nadvisors' summary notes, taken after every investor conversation, to gain first\never insights into advisor-investor interactions. These insights are used to\npredict investor needs during adverse market conditions; thus allowing advisors\nto coach investors and help avoid inappropriate financial decision-making.\nFirst, we perform topic modeling to gain insight into the emerging topics and\ntrends. Based on this insight, we construct a supervised classification model\nto predict the probability that an advised investor will require behavioral\ncoaching during volatile market periods. To the best of our knowledge, ours is\nthe first work on exploring the advisor-investor relationship using\nunstructured data. This work may have far-reaching implications for both\ntraditional and emerging financial advisory service models like robo-advising.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:12:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pagliaro", "Cynthia", ""], ["Mehta", "Dhagash", ""], ["Shiao", "Han-Tai", ""], ["Wang", "Shaofei", ""], ["Xiong", "Luwei", ""]]}, {"id": "2107.05602", "submitter": "Rohit Dube", "authors": "Rohit Dube", "title": "Understanding the Communist Party of China's Information Operations", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Communist Party of China is known to engage in Information Operations to\ninfluence public opinion. In this paper, we seek to understand the tactics used\nby the Communist Party in a recent Information Operation - the one conducted to\ninfluence the narrative around the pro-democracy movement in Hong Kong. We use\na Twitter dataset containing account information and tweets for the operation.\nOur research shows that the Hong Kong operation was (at least) partially\nconducted manually by humans rather than entirely by automated bots. We also\nshow that the Communist Party mixed in personal attacks on Chinese dissidents\nand messages on COVID-19 with the party's views on the protests during the\noperation. Finally, we conclude that the Information Operation network in the\nTwitter dataset was set up to amplify content generated elsewhere rather than\nto influence the narrative with original content.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:34:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dube", "Rohit", ""]]}, {"id": "2107.05619", "submitter": "Claire Donnat", "authors": "Saskia Comess, Hannah Wang, Susan Holmes, Claire Donnat", "title": "Statistical Modeling for Practical Pooled Testing During the COVID-19\n  Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pooled testing offers an efficient solution to the unprecedented testing\ndemands of the COVID-19 pandemic, although with potentially lower sensitivity\nand increased costs to implementation in some settings. Assessments of this\ntrade-off typically assume pooled specimens are independent and identically\ndistributed. Yet, in the context of COVID-19, these assumptions are often\nviolated: testing done on networks (housemates, spouses, co-workers) captures\ncorrelated individuals, while infection risk varies substantially across time,\nplace and individuals. Neglecting dependencies and heterogeneity may bias\nestablished optimality grids and induce a sub-optimal implementation of the\nprocedure. As a lesson learned from this pandemic, this paper highlights the\nnecessity of integrating field sampling information with statistical modeling\nto efficiently optimize pooled testing. Using real data, we show that (a)\ngreater gains can be achieved at low logistical cost by exploiting natural\ncorrelations (non-independence) between samples -- allowing improvements in\nsensitivity and efficiency of up to 30% and 90% respectively; and (b) these\ngains are robust despite substantial heterogeneity across pools\n(non-identical). Our modeling results complement and extend the observations of\nBarak et al (2021) who report an empirical sensitivity well beyond\nexpectations. Finally, we provide an interactive tool for selecting an optimal\npool size using contextual information\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:55:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Comess", "Saskia", ""], ["Wang", "Hannah", ""], ["Holmes", "Susan", ""], ["Donnat", "Claire", ""]]}, {"id": "2107.05767", "submitter": "Lucka Gianvechio", "authors": "Carmen Melo Toledo, Guilherme Mendes Bassedon, Jonathan Batista\n  Ferreira, Lucka de Godoy Gianvechio, Carlos Guatimosim, Felipe Maia Polo,\n  Renato Vicente", "title": "Effects of personality traits in predicting grade retention of Brazilian\n  students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Student's grade retention is a key issue faced by many education systems,\nespecially those in developing countries. In this paper, we seek to gauge the\nrelevance of students' personality traits in predicting grade retention in\nBrazil. For that, we used data collected in 2012 and 2017, in the city of\nSertaozinho, countryside of the state of Sao Paulo, Brazil. The surveys taken\nin Sertaozinho included several socioeconomic questions, standardized tests,\nand a personality test. Moreover, students were in grades 4, 5, and 6 in 2012.\nOur approach was based on training machine learning models on the surveys' data\nto predict grade retention between 2012 and 2017 using information from 2012 or\nbefore, and then using some strategies to quantify personality traits'\npredictive power. We concluded that, besides proving to be fairly better than a\nrandom classifier when isolated, personality traits contribute to prediction\neven when using socioeconomic variables and standardized tests results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:23:13 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Toledo", "Carmen Melo", ""], ["Bassedon", "Guilherme Mendes", ""], ["Ferreira", "Jonathan Batista", ""], ["Gianvechio", "Lucka de Godoy", ""], ["Guatimosim", "Carlos", ""], ["Polo", "Felipe Maia", ""], ["Vicente", "Renato", ""]]}, {"id": "2107.05805", "submitter": "Adam Peterson", "authors": "Adam Peterson and Emma Sanchez-Vaznaugh and Brisa Sanchez", "title": "Heterogeneous Effects in the Built Environment", "comments": "13 pages, 3 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to estimate distance-dependent heterogeneous\nassociations between point-referenced exposures to built environment\ncharacteristics and health outcomes. By estimating associations that depend\nnon-linearly on distance between subjects and point-referenced exposures, this\nmethod addresses the modifiable area-unit problem that is pervasive in the\nbuilt environment literature. Additionally, by estimating heterogeneous\neffects, the method also addresses the uncertain geographic context problem.\nThe key innovation of our method is to combine ideas from the non-parametric\nfunction estimation literature and the Bayesian Dirichlet process literature.\nThe former is used to estimate nonlinear associations between subject's\noutcomes and proximate built environment features, and the latter identifies\nclusters within the population that have different effects. We study this\nmethod in simulations and apply our model to study heterogeneity in the\nassociation between fast food restaurant availability and weight status of\nchildren attending schools in Los Angeles, California.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 01:46:16 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peterson", "Adam", ""], ["Sanchez-Vaznaugh", "Emma", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2107.05933", "submitter": "Lingsong Meng", "authors": "Lingsong Meng and Zhiguang Huo", "title": "Outcome-guided Bayesian Clustering for Disease Subtype Discovery Using\n  High-dimensional Transcriptomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of disease subtypes is an essential step for developing\nprecision medicine, and disease subtyping via omics data has become a popular\napproach. While promising, subtypes obtained from conventional approaches may\nnot be necessarily associated with clinical outcomes. The collection of rich\nclinical data along with omics data has provided an unprecedented opportunity\nto facilitate the disease subtyping process and to discovery clinically\nmeaningful disease subtypes. Thus, we developed an outcome-guided Bayesian\nclustering (GuidedBayesianClustering) method to fully integrate the clinical\ndata and the high-dimensional omics data. A Gaussian mixed model framework was\napplied to perform sample clustering; a spike-and-slab prior was utilized to\nperform gene selection; a mixture model prior was employed to incorporate the\nguidance from a clinical outcome variable; and a decision framework was adopted\nto infer the false discovery rate of the selected genes. We deployed conjugate\npriors to facilitate efficient Gibbs sampling. Our proposed full Bayesian\nmethod is capable of simultaneously (i) obtaining sample clustering (disease\nsubtype discovery); (ii) performing feature selection (select genes related to\nthe disease subtype); and (iii) utilizing clinical outcome variable to guide\nthe disease subtype discovery. The superior performance of the\nGuidedBayesianClustering was demonstrated through simulations and applications\nof breast cancer expression data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:10:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Meng", "Lingsong", ""], ["Huo", "Zhiguang", ""]]}, {"id": "2107.06072", "submitter": "Udit Bhatia", "authors": "Surender V Raj, Manish Kumar, Udit Bhatia", "title": "Fragility curves for power transmission towers in Odisha, India, based\n  on observed damage during 2019 Cyclone Fani", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lifeline infrastructure systems such as a power transmission network in\ncoastal regions are vulnerable to strong winds generated during tropical\ncyclones. Understanding the fragility of individual towers is helpful in\nimproving the resilience of such systems. Fragility curves have been developed\nin the past for some regions, but without considering relevant epistemic\nuncertainties. Further, risk and resilience studies are best performed using\nthe fragility curves specific to a region. Such studies become particularly\nimportant if the region is exposed to cyclones rather frequently. This paper\npresents the development of fragility curves for high-voltage power\ntransmission towers in the state of Odisha, India, based on macro-level damage\ndata from 2019 cyclone Fani, which was obtained through concerned government\noffices. Two types of damages were identified, namely, collapse and partial\ndamage. Accordingly, fragility curves for collapse and functionality disruption\ndamage states were developed considering relevant aleatory and epistemic\nuncertainties. The latter class of uncertainties included that associated with\nwind speed estimation at a location and the finite sample uncertainty. The most\nsignificant contribution in the epistemic uncertainty was due to the wind speed\nestimation at a location. The median and logarithmic standard deviation for the\n50th percentile fragility curve associated with collapse was close to that for\nthe functionality disruption damage state. These curves also compared\nreasonably well with those reported for similar structures in other parts of\nthe world.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:59:58 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Raj", "Surender V", ""], ["Kumar", "Manish", ""], ["Bhatia", "Udit", ""]]}, {"id": "2107.06096", "submitter": "Thayer Alshaabi", "authors": "Yi Li, Asieh Ahani, Haimao Zhan, Kevin Foley, Thayer Alshaabi, Kelsey\n  Linnell, Peter Sheridan Dodds, Christopher M. Danforth, Adam Fox", "title": "Blending search queries with social media data to improve forecasts of\n  economic indicators", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forecasting of political, economic, and public health indicators using\ninternet activity has demonstrated mixed results. For example, while some\nmeasures of explicitly surveyed public opinion correlate well with social media\nproxies, the opportunity for profitable investment strategies to be driven\nsolely by sentiment extracted from social media appears to have expired.\nNevertheless, the internet's space of potentially predictive input signals is\ncombinatorially vast and will continue to invite careful exploration. Here, we\ncombine unemployment related search data from Google Trends with economic\nlanguage on Twitter to attempt to nowcast and forecast: 1. State and national\nunemployment claims for the US, and 2. Consumer confidence in G7 countries.\nBuilding off of a recently developed search-query-based model, we show that\nincorporating Twitter data improves forecasting of unemployment claims, while\nthe original method remains marginally better at nowcasting. Enriching the\ninput signal with temporal statistical features (e.g., moving average and rate\nof change) further reduces errors, and improves the predictive utility of the\nproposed method when applied to other economic indices, such as consumer\nconfidence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:53:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Yi", ""], ["Ahani", "Asieh", ""], ["Zhan", "Haimao", ""], ["Foley", "Kevin", ""], ["Alshaabi", "Thayer", ""], ["Linnell", "Kelsey", ""], ["Dodds", "Peter Sheridan", ""], ["Danforth", "Christopher M.", ""], ["Fox", "Adam", ""]]}, {"id": "2107.06223", "submitter": "Sara Lopes De Moraes", "authors": "Sara Lopes de Moraes, Ricardo Almendra, Ligia Vizeu Barrozo", "title": "Impact of heat waves and cold spells on cause-specific mortality in the\n  city of Sao Paulo, Brazil", "comments": "28 pages, 2 tables, and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The impact of heat waves and cold spells on mortality has become a major\npublic health problem worldwide, especially among older adults living in low-\nto middle-income countries. This study aimed to investigate the effects of heat\nwaves and cold spells under different definitions on cause-specific mortality\namong people aged 65 years and over in Sao Paulo from 2006 to 2015. A\nquasi-Poisson generalized linear model with a distributed lag model was used to\ninvestigate the association between cause-specific mortality and extreme air\ntemperature events. To evaluate the effects of the intensity under different\ndurations, we considered 12 heat wave and nine cold spell definitions. Our\nresults showed an increase in cause-specific deaths related to heat waves and\ncold spells under several definitions. The highest risk of death related to\nheat waves was identified mostly at higher temperature thresholds with longer\nevents. We verified that men were more vulnerable to die from an ischemic\nstroke on heat waves and cold spells days than women, while women presented a\nhigher risk of dying from ischemic heart diseases during cold spells and tended\nto have a higher risk of chronic obstructive pulmonary disease than men.\nIdentification of heat wave- and cold spell-related mortality is important for\nthe development and promotion of public health measures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:23:05 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["de Moraes", "Sara Lopes", ""], ["Almendra", "Ricardo", ""], ["Barrozo", "Ligia Vizeu", ""]]}, {"id": "2107.06268", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a winning method of the IEEE DataPort Competition on Day-Ahead\nElectricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load\nforecasting approach is based on online forecast combination of multiple point\nprediction models. It contains four steps: i) data cleaning and preprocessing,\nii) a holiday adjustment procedure, iii) training of individual forecasting\nmodels, iv) forecast combination by smoothed Bernstein Online Aggregation\n(BOA). The approach is flexible and can quickly adopt to new energy system\nsituations as they occurred during and after COVID-19 shutdowns. The pool of\nindividual prediction models ranges from rather simple time series models to\nsophisticated models like generalized additive models (GAMs) and\nhigh-dimensional linear models estimated by lasso. They incorporate\nautoregressive, calendar and weather effects efficiently. All steps contain\nnovel concepts that contribute to the excellent forecasting performance of the\nproposed method. This holds particularly for the holiday adjustment procedure\nand the fully adaptive smoothed BOA approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:51:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "2107.06435", "submitter": "Lirong Xia", "authors": "Lirong Xia", "title": "A Smoothed Impossibility Theorem on Condorcet Criterion and\n  Participation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1988, Moulin proved an insightful and surprising impossibility theorem\nthat reveals a fundamental incompatibility between two commonly-studied axioms\nof voting: no resolute voting rule (which outputs a single winner) satisfies\nCondorcet Criterion and Participation simultaneously when the number of\nalternatives m is at least four. In this paper, we prove an extension of this\nimpossibility theorem using smoothed analysis: for any fixed $m\\ge 4$ and any\nvoting rule r, under mild conditions, the smoothed likelihood for both\nCondorcet Criterion and Participation to be satisfied is at most\n$1-\\Omega(n^{-3})$, where n is the number of voters that is sufficiently large.\nOur theorem immediately implies a quantitative version of the theorem for\ni.i.d. uniform distributions, known as the Impartial Culture in social choice\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 00:39:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xia", "Lirong", ""]]}, {"id": "2107.06539", "submitter": "Xuxue Sun", "authors": "Xuxue Sun, Mingyang Li", "title": "Bayesian Lifetime Regression with Multi-type Group-shared Latent\n  Heterogeneity", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Products manufactured from the same batch or utilized in the same region\noften exhibit correlated lifetime observations due to the latent heterogeneity\ncaused by the influence of shared but unobserved covariates. The unavailable\ngroup-shared covariates involve multiple different types (e.g., discrete,\ncontinuous, or mixed-type) and induce different structures of indispensable\ngroup-shared latent heterogeneity. Without carefully capturing such latent\nheterogeneity, the lifetime modeling accuracy will be significantly undermined.\nIn this work, we propose a generic Bayesian lifetime modeling approach by\ncomprehensively investigating the structures of group-shared latent\nheterogeneity caused by different types of group-shared unobserved covariates.\nThe proposed approach is flexible to characterize multi-type group-shared\nlatent heterogeneity in lifetime data. Besides, it can handle the case of lack\nof group membership information and address the issue of limited sample size.\nBayesian sampling algorithm with data augmentation technique is further\ndeveloped to jointly quantify the influence of observed covariates and\ngroup-shared latent heterogeneity. Further, we conduct comprehensive numerical\nstudy to demonstrate the improved performance of proposed modeling approach via\ncomparison with alternative models. We also present empirical study results to\ninvestigate the impacts of group number and sample size per group on estimating\nthe group-shared latent heterogeneity and to demonstrate model identifiability\nof proposed approach for different structures of unobserved group-shared\ncovariates. We also present a real case study to illustrate the effectiveness\nof proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:11:30 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sun", "Xuxue", ""], ["Li", "Mingyang", ""]]}, {"id": "2107.06545", "submitter": "Thomas House", "authors": "Thomas House, Lorenzo Pellis, Emma Pritchard, Angela R. McLean and A.\n  Sarah Walker", "title": "Total Effect Analysis of Vaccination on Household Transmission in the\n  Office for National Statistics COVID-19 Infection Survey", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the distribution of numbers of secondary cases in households\nin the Office for National Statistics COVID-19 Infection Survey (ONS CIS),\nstratified by timing of vaccination and infection in the households. This shows\na total effect of a statistically significant approximate halving of the\nsecondary attack rate in households following vaccination.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:28:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["House", "Thomas", ""], ["Pellis", "Lorenzo", ""], ["Pritchard", "Emma", ""], ["McLean", "Angela R.", ""], ["Walker", "A. Sarah", ""]]}, {"id": "2107.06675", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "M5 Competition Uncertainty: Overdispersion, distributional forecasting,\n  GAMLSS and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The M5 competition uncertainty track aims for probabilistic forecasting of\nsales of thousands of Walmart retail goods. We show that the M5 competition\ndata faces strong overdispersion and sporadic demand, especially zero demand.\nWe discuss resulting modeling issues concerning adequate probabilistic\nforecasting of such count data processes. Unfortunately, the majority of\npopular prediction methods used in the M5 competition (e.g. lightgbm and\nxgboost GBMs) fails to address the data characteristics due to the considered\nobjective functions. The distributional forecasting provides a suitable\nmodeling approach for to the overcome those problems. The GAMLSS framework\nallows flexible probabilistic forecasting using low dimensional distributions.\nWe illustrate, how the GAMLSS approach can be applied for the M5 competition\ndata by modeling the location and scale parameter of various distributions,\ne.g. the negative binomial distribution. Finally, we discuss software packages\nfor distributional modeling and their drawback, like the R package gamlss with\nits package extensions, and (deep) distributional forecasting libraries such as\nTensorFlow Probability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:05:55 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "2107.06754", "submitter": "Xiaozhou Yang", "authors": "Xiaozhou Yang, Nan Chen, Chao Zhai", "title": "A Particle Filter Approach to Power System Line Outage Detection Using\n  Load and Generator Bus Dynamics", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited phasor measurement unit (PMU) and varying signal strength levels make\nfast real-time transmission-line outage detection challenging. Existing\napproaches focus on monitoring nodal algebraic variables, i.e., voltage phase\nangle and magnitude. Their effectiveness is predicated on both strong outage\nsignals in voltage and PMUs in the outage location's vicinity. We propose a\nunified detection framework that utilizes both generator dynamic states and\nnodal voltage information. The inclusion of generator dynamics makes detection\nfaster and more robust to a priori unknown outage locations, which we\ndemonstrate using the IEEE 39-bus test system. In particular, the scheme\nachieves an over 80% detection rate for 80% of the lines, and most outages are\ndetected within 0.2 seconds. The new approach could be implemented to improve\nsystem operators' real-time situational awareness by detecting outages faster\nand providing a breakdown of outage signals for diagnostic purposes, making\npower systems more resilient.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:08:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Yang", "Xiaozhou", ""], ["Chen", "Nan", ""], ["Zhai", "Chao", ""]]}, {"id": "2107.06867", "submitter": "Anthony McIntosh PhD", "authors": "Anthony R McIntosh", "title": "Comparison of Canonical Correlation and Partial Least Squares analyses\n  of simulated and empirical data", "comments": "40 pages, 12 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we compared the general forms of CCA and PLS on three\nsimulated and two empirical datasets, all having large sample sizes. We took\nsuccessively smaller subsamples of these data to evaluate sensitivity,\nreliability, and reproducibility. In null data having no correlation within or\nbetween blocks, both methods showed equivalent false positive rates across\nsample sizes. Both methods also showed equivalent detection in data with weak\nbut reliable effects until sample sizes drop below n=50. In the case of strong\neffects, both methods showed similar performance unless the correlations of\nitems within one data block were high. For PLS, the results were reproducible\nacross sample sizes for strong effects, except at the smallest sample sizes. On\nthe contrary, the reproducibility for CCA declined when the within-block\ncorrelations were high. This was ameliorated if a principal components analysis\n(PCA) was performed and the component scores used to calculate the cross-block\nmatrix. The outcome of our examination gives three messages. First, for data\nwith reasonable within and between block structure, CCA and PLS give comparable\nresults. Second, if there are high correlations within either block, this can\ncompromise the reliability of CCA results. This known issue of CCA can be\nremedied with PCA before cross-block calculation. This, however, assumes that\nthe PCA structure is stable for a given sample. Third, null hypothesis testing\ndoes not guarantee that the results are reproducible, even with large sample\nsizes. This final outcome suggests that both statistical significance and\nreproducibility be assessed for any data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:36:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["McIntosh", "Anthony R", ""]]}, {"id": "2107.07206", "submitter": "Samuel St\\'ephan", "authors": "Matthieu Garcin, Samuel St\\'ephan", "title": "Credit scoring using neural networks and SURE posterior probability\n  calibration", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we compare the performances of a logistic regression and a\nfeed forward neural network for credit scoring purposes. Our results show that\nthe logistic regression gives quite good results on the dataset and the neural\nnetwork can improve a little the performance. We also consider different sets\nof features in order to assess their importance in terms of prediction\naccuracy. We found that temporal features (i.e. repeated measures over time)\ncan be an important source of information resulting in an increase in the\noverall model accuracy. Finally, we introduce a new technique for the\ncalibration of predicted probabilities based on Stein's unbiased risk estimate\n(SURE). This calibration technique can be applied to very general calibration\nfunctions. In particular, we detail this method for the sigmoid function as\nwell as for the Kumaraswamy function, which includes the identity as a\nparticular case. We show that stacking the SURE calibration technique with the\nclassical Platt method can improve the calibration of predicted probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:30:09 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Garcin", "Matthieu", ""], ["St\u00e9phan", "Samuel", ""]]}, {"id": "2107.07256", "submitter": "Marcela Niemczyk", "authors": "Marcela Niemczyk and D. Robert Iskander", "title": "Statistical modeling of corneal OCT speckle. A distributional model-free\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In biomedical optics, it is often of interest to statistically model the\namplitude of the speckle using some distributional models with their parameters\nacting as biomarkers. In this paper, a paradigm shift is being advocated in\nwhich a distributional model-free approach is used. Specifically, a range of\ndistances, evaluated in different domains, between an empirical nonparametric\ndistribution of the normalized speckle amplitude sample and the benchmark\nRayleigh distribution, is considered. Using OCT images from phantoms, two\nex-vivo experiments with porcine corneas and an in-vivo experiment with human\ncorneas, an evidence is provided that the distributional model-free approach,\ndespite its simplicity, could lead to better results than the best-fitted\n(among a range of considered models) distributional model. Concluding, in\npractice, the distributional model-free approach should be considered as the\nfirst choice to speckle modeling before a distributional-based approach is\nutilized.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:27:29 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Niemczyk", "Marcela", ""], ["Iskander", "D. Robert", ""]]}, {"id": "2107.07489", "submitter": "Jean-Gabriel Young", "authors": "Jean-Gabriel Young, Alec Kirkley, M. E. J. Newman", "title": "Clustering of heterogeneous populations of networks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods for reconstructing networks from repeated measurements\ntypically assume that all measurements are generated from the same underlying\nnetwork structure. This need not be the case, however. People's social networks\nmight be different on weekdays and weekends, for instance. Brain networks may\ndiffer between healthy patients and those with dementia or other conditions.\nHere we describe a Bayesian analysis framework for such data that allows for\nthe fact that network measurements may be reflective of multiple possible\nstructures. We define a finite mixture model of the measurement process and\nderive a fast Gibbs sampling procedure that samples exactly from the full\nposterior distribution of model parameters. The end result is a clustering of\nthe measured networks into groups with similar structure. We demonstrate the\nmethod on both real and synthetic network populations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:42:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Young", "Jean-Gabriel", ""], ["Kirkley", "Alec", ""], ["Newman", "M. E. J.", ""]]}, {"id": "2107.07561", "submitter": "Luiza Piancastelli", "authors": "Luiza S.C. Piancastelli, Nial Friel, Wagner Barreto-Souza and Hernando\n  Ombao", "title": "Multivariate Conway-Maxwell-Poisson Distribution: Sarmanov Method and\n  Doubly-Intractable Bayesian Inference", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multivariate count distribution with Conway-Maxwell\n(COM)-Poisson marginals is proposed. To do this, we develop a modification of\nthe Sarmanov method for constructing multivariate distributions. Our\nmultivariate COM-Poisson (MultCOMP) model has desirable features such as (i) it\nadmits a flexible covariance matrix allowing for both negative and positive\nnon-diagonal entries; (ii) it overcomes the limitation of the existing\nbivariate COM-Poisson distributions in the literature that do not have\nCOM-Poisson marginals; (iii) it allows for the analysis of multivariate counts\nand is not just limited to bivariate counts. Inferential challenges are\npresented by the likelihood specification as it depends on a number of\nintractable normalizing constants involving the model parameters. These\nobstacles motivate us to propose a Bayesian inferential approach where the\nresulting doubly-intractable posterior is dealt with via the exchange algorithm\nand the Grouped Independence Metropolis-Hastings algorithm. Numerical\nexperiments based on simulations are presented to illustrate the proposed\nBayesian approach. We analyze the potential of the MultCOMP model through a\nreal data application on the numbers of goals scored by the home and away teams\nin the Premier League from 2018 to 2021. Here, our interest is to assess the\neffect of a lack of crowds during the COVID-19 pandemic on the well-known home\nteam advantage. A MultCOMP model fit shows that there is evidence of a\ndecreased number of goals scored by the home team, not accompanied by a reduced\nscore from the opponent. Hence, our analysis suggests a smaller home team\nadvantage in the absence of crowds, which agrees with the opinion of several\nfootball experts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:48:42 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Piancastelli", "Luiza S. C.", ""], ["Friel", "Nial", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2107.07581", "submitter": "Duarte Dinis", "authors": "Duarte Caldeira Dinis, Jos\\'e Rui Figueira, \\^Angelo Palos Teixeira", "title": "A multiple criteria approach for ship risk classification: An\n  alternative to the Paris MoU Ship Risk Profile", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Paris Memorandum of Understanding on Port State Control (Paris MoU) is\nresponsible for controlling substandard shipping in European waters and,\nconsequently, increasing the standards of safety, pollution prevention, and\nonboard living and working conditions. Since 2011, the Memorandum adopted a\nsystem of points, named the \"Ship Risk Profile\" (SRP), under which each ship is\nassigned a risk profile according to its score on a set of criteria. Being a\nmultiple criteria decision aiding (MCDA) tool at its core, comprising criteria,\nweights, and risk categories, limited research has been performed on the SRP\nfrom an MCDA perspective. The purpose of this paper is to propose an MCDA\napproach for ship risk classification through the Deck of Cards Method (DCM).\nThe DCM is particularly suitable within this context as it allows, intuitively\nfor the decision-maker, to model preference among different criteria and among\ndifferent levels on criteria scales. First, a framework is built, based on the\ncriteria established in the SRP. Second, the DCM is used to build the MCDA\nmodel, including the definition of the criteria value functions and criteria\nweights. Finally, the proposed MCDA model is applied to a dataset of ships and\nthe results are discussed. Robust results have been obtained with the proposed\napproach, making it a potential alternative to the current SRP.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:10:45 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Dinis", "Duarte Caldeira", ""], ["Figueira", "Jos\u00e9 Rui", ""], ["Teixeira", "\u00c2ngelo Palos", ""]]}, {"id": "2107.07582", "submitter": "Venet Osmani", "authors": "Behrooz Mamandipoor, Wesley Yeung, Louis Agha-Mir-Salim, David J.\n  Stone, Venet Osmani, Leo Anthony Celi", "title": "Prediction of Blood Lactate Values in Critically Ill Patients: A\n  Retrospective Multi-center Cohort Study", "comments": "15 pages, 6 Appendices", "journal-ref": "J Clin Monit Comput. 2021 PMID: 34224051", "doi": "10.1007/s10877-021-00739-4", "report-no": null, "categories": "q-bio.QM cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Elevations in initially obtained serum lactate levels are strong\npredictors of mortality in critically ill patients. Identifying patients whose\nserum lactate levels are more likely to increase can alert physicians to\nintensify care and guide them in the frequency of tending the blood test. We\ninvestigate whether machine learning models can predict subsequent serum\nlactate changes.\n  Methods. We investigated serum lactate change prediction using the MIMIC-III\nand eICU-CRD datasets in internal as well as external validation of the eICU\ncohort on the MIMIC-III cohort. Three subgroups were defined based on the\ninitial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4\nmmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on\nincrease or decrease of serum lactate levels between the groups. We also\nperformed sensitivity analysis by defining the outcome as lactate change of\n>10% and furthermore investigated the influence of the time interval between\nsubsequent lactate measurements on predictive performance.\n  Results. The LSTM models were able to predict deterioration of serum lactate\nvalues of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the\nnormal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI\n0.840-0.851) for the severe group, with a slightly lower performance in the\nexternal validation.\n  Conclusion. The LSTM demonstrated good discrimination of patients who had\ndeterioration in serum lactate levels. Clinical studies are needed to evaluate\nwhether utilization of a clinical decision support tool based on these results\ncould positively impact decision-making and patient outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:46:47 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mamandipoor", "Behrooz", ""], ["Yeung", "Wesley", ""], ["Agha-Mir-Salim", "Louis", ""], ["Stone", "David J.", ""], ["Osmani", "Venet", ""], ["Celi", "Leo Anthony", ""]]}, {"id": "2107.07602", "submitter": "Ron Sarafian", "authors": "Ron Sarafian, Itai Kloog, Jonathan D. Rosenblatt", "title": "Optimal-Design Domain-Adaptation for Exposure Prediction in Two-Stage\n  Epidemiological Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the first stage of a two-stage study, the researcher uses a statistical\nmodel to impute the unobserved exposures. In the second stage, imputed\nexposures serve as covariates in epidemiological models. Imputation error in\nthe first stage operate as measurement errors in the second stage, and thus\nbias exposure effect estimates. This study aims to improve the estimation of\nexposure effects by sharing information between the first and second stage. At\nthe heart of our estimator is the observation that not all second-stage\nobservations are equally important to impute. We thus borrow ideas from the\noptimal-experimental-design theory, to identify individuals of higher\nimportance. We then improve the imputation of these individuals using ideas\nfrom the machine-learning literature of domain-adaptation. Our simulations\nconfirm that the exposure effect estimates are more accurate than the current\nbest practice. An empirical demonstration yields smaller estimates of PM effect\non hyperglycemia risk, with tighter confidence bands. Sharing information\nbetween environmental scientist and epidemiologist improves health effect\nestimates. Our estimator is a principled approach for harnessing this\ninformation exchange, and may be applied to any two stage study.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 20:50:43 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Sarafian", "Ron", ""], ["Kloog", "Itai", ""], ["Rosenblatt", "Jonathan D.", ""]]}, {"id": "2107.07605", "submitter": "Guy Nason Prof.", "authors": "Guy P Nason and James L Wei", "title": "Quantifying the economic response to COVID-19 mitigations and death\n  rates via forecasting Purchasing Managers' Indices using Generalised Network\n  Autoregressive models with exogenous variables", "comments": "To be read before the Royal Statistical Society at the Society's 2021\n  annual conference held in Manchester on Wednesday, September 8th 2021, the\n  President, Professor Sylvia Richardson, in the Chair. Accepted by the Journal\n  of the Royal Statistical Society, Series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge of the current state of economies, how they respond to COVID-19\nmitigations and indicators, and what the future might hold for them is\nimportant. We use recently-developed generalised network autoregressive (GNAR)\nmodels, using trade-determined networks, to model and forecast the Purchasing\nManagers' Indices for a number of countries. We use networks that link\ncountries where the links themselves, or their weights, are determined by the\ndegree of export trade between the countries. We extend these models to include\nnode-specific time series exogenous variables (GNARX models), using this to\nincorporate COVID-19 mitigation stringency indices and COVID-19 death rates\ninto our analysis. The highly parsimonious GNAR models considerably outperform\nvector autoregressive models in terms of mean-squared forecasting error and our\nGNARX models themselves outperform GNAR ones. Further mixed frequency modelling\npredicts the extent to which that the UK economy will be affected by harsher,\nweaker or no interventions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:45:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Nason", "Guy P", ""], ["Wei", "James L", ""]]}, {"id": "2107.07648", "submitter": "Yutong Wu", "authors": "Yutong Wu, Erich D. Jarvis and Abhra Sarkar", "title": "Bayesian Markov Renewal Mixed Models for Vocalization Syntax", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studying the neurological, genetic and evolutionary basis of human vocal\ncommunication mechanisms is an important field of neuroscience. In the absence\nof high quality data on humans, mouse vocalization experiments in laboratory\nsettings have been proven to be useful in providing valuable insights into\nmammalian vocal development and evolution, including especially the impact of\ncertain genetic mutations. Data sets from mouse vocalization experiments\nusually consist of categorical syllable sequences along with continuous\ninter-syllable interval times for mice of different genotypes vocalizing under\nvarious contexts. Few statistical models have considered the inference for both\ntransition probabilities and inter-state intervals. The latter is of particular\nimportance as increased inter-state intervals can be an indication of possible\nvocal impairment. In this paper, we propose a class of novel Markov renewal\nmixed models that capture the stochastic dynamics of both state transitions and\ninter-state interval times. Specifically, we model the transition dynamics and\nthe inter-state intervals using Dirichlet and gamma mixtures, respectively,\nallowing the mixture probabilities in both cases to vary flexibly with fixed\ncovariate effects as well as random individual-specific effects. We apply our\nmodel to analyze the impact of a mutation in the Foxp2 gene on mouse vocal\nbehavior. We find that genotypes and social contexts significantly affect the\ninter-state interval times but, compared to previous analyses, the influences\nof genotype and social context on the syllable transition dynamics are weaker.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 00:06:22 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 22:09:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wu", "Yutong", ""], ["Jarvis", "Erich D.", ""], ["Sarkar", "Abhra", ""]]}, {"id": "2107.07668", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier and Molly James and Hani Ali", "title": "Predicting Drought and Subsidence Risks in France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The economic consequences of drought episodes are increasingly important,\nalthough they are often difficult to apprehend in part because of the\ncomplexity of the underlying mechanisms. In this article, we will study one of\nthe consequences of drought, namely the risk of subsidence (or more\nspecifically clay shrinkage induced subsidence), for which insurance has been\nmandatory in France for several decades. Using data obtained from several\ninsurers, representing about a quarter of the household insurance market, over\nthe past twenty years, we propose some statistical models to predict the\nfrequency but also the intensity of these droughts, for insurers, showing that\nclimate change will have probably major economic consequences on this risk. But\neven if we use more advanced models than standard regression-type models (here\nrandom forests to capture non linearity and cross effects), it is still\ndifficult to predict the economic cost of subsidence claims, even if all\ngeophysical and climatic information is available.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 02:09:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Charpentier", "Arthur", ""], ["James", "Molly", ""], ["Ali", "Hani", ""]]}, {"id": "2107.07804", "submitter": "Florian Huber", "authors": "Florian Huber and Gary Koop", "title": "Subspace Shrinkage in Conjugate Bayesian Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Macroeconomists using large datasets often face the choice of working with\neither a large Vector Autoregression (VAR) or a factor model. In this paper, we\ndevelop methods for combining the two using a subspace shrinkage prior.\nSubspace priors shrink towards a class of functions rather than directly\nforcing the parameters of a model towards some pre-specified location. We\ndevelop a conjugate VAR prior which shrinks towards the subspace which is\ndefined by a factor model. Our approach allows for estimating the strength of\nthe shrinkage as well as the number of factors. After establishing the\ntheoretical properties of our proposed prior, we carry out simulations and\napply it to US macroeconomic data. Using simulations we show that our framework\nsuccessfully detects the number of factors. In a forecasting exercise involving\na large macroeconomic data set we find that combining VARs with factor models\nusing our prior can lead to forecast improvements.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:15:32 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Huber", "Florian", ""], ["Koop", "Gary", ""]]}, {"id": "2107.07865", "submitter": "Vittorio Cipolla Dr.", "authors": "Vittorio Cipolla, Vincenzo Binante, Karim Abu Salem, Giuseppe Palaia,\n  Davide Zanetti", "title": "A DoE-based approach for the implementation of structural surrogate\n  models in the early stage design of box-wing aircraft", "comments": "20 pages", "journal-ref": "Aerospace Science and Technology (2021)", "doi": "10.1016/j.ast.2021.106968", "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the possible ways to face the challenge of reducing the environmental\nimpact of aviation, without limiting the growth of air transport, is the\nintroduction of more efficient, radically different aircraft architectures.\nAmong these, the box-wing one represents a promising solution, at least in the\ncase of its application to short-to-medium haul aircraft, which, according to\nthe achievement of the H2020 project \"PARSIFAL\", would bring to a 20% reduction\nin terms of emitted CO2 per passenger-kilometre. The present paper faces the\nproblem of estimating the structural mass of such a disruptive configuration in\nthe early stages of the design, underlining the limitations in this capability\nof the approaches available by literature and proposing a DoE-based approach to\ndefine surrogate models suitable for such purpose. A test case from the project\n\"PARSIFAL\" is used for the first conception of the approach, starting from the\nFinite Element Model parametrization, then followed by the construction of a\ndatabase of FEM results, hence introducing the regression models and\nimplementing them in an optimization framework. Results achieved are\ninvestigated in order to validate both the wing sizing and the optimization\nprocedure. Finally, an additional test case resulting from the application of\nthe box-wing layout to the regional aircraft category within the Italian\nresearch project \"PROSIB\", is briefly presented to further assess the\ncapabilities of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:45:22 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 16:57:48 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Cipolla", "Vittorio", ""], ["Binante", "Vincenzo", ""], ["Salem", "Karim Abu", ""], ["Palaia", "Giuseppe", ""], ["Zanetti", "Davide", ""]]}, {"id": "2107.07881", "submitter": "Philipp Dechent", "authors": "Philipp Dechent, Samuel Greenbank, Felix Hildenbrand, Saad Jbabdi,\n  Dirk Uwe Sauer, David A. Howey", "title": "Estimation of Li-ion degradation test sample sizes required to\n  understand cell-to-cell variability", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ageing of lithium-ion batteries results in irreversible reduction in\nperformance. Intrinsic variability between cells, caused by manufacturing\ndifferences, occurs at all stages of life and increases with age. Researchers\nneed to know the minimum number of cells they should test to give an accurate\nrepresentation of population variability, since testing many cells is\nexpensive. In this paper, empirical capacity versus time ageing models were\nfitted to various degradation datasets assuming that the model parameters could\nbe drawn from a distribution describing a larger population. Using a\nhierarchical Bayesian approach, we estimated the number of cells required to be\ntested. Depending on the complexity of the ageing model, models with 1, 2 or 3\nparameters respectively required data from at least 9, 11 or 13 cells for a\nconsistent fit. This implies that researchers will need to test at least these\nnumbers of cells at each test point in their experiment to capture\nmanufacturing variability.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 17:05:42 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Dechent", "Philipp", ""], ["Greenbank", "Samuel", ""], ["Hildenbrand", "Felix", ""], ["Jbabdi", "Saad", ""], ["Sauer", "Dirk Uwe", ""], ["Howey", "David A.", ""]]}, {"id": "2107.07942", "submitter": "Christoph Rothe", "authors": "Claudia Noack and Tomasz Olma and Christoph Rothe", "title": "Flexible Covariate Adjustments in Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empirical regression discontinuity (RD) studies often use covariates to\nincrease the precision of their estimates. In this paper, we propose a novel\nclass of estimators that use such covariate information more efficiently than\nthe linear adjustment estimators that are currently used widely in practice.\nOur approach can accommodate a possibly large number of either discrete or\ncontinuous covariates. It involves running a standard RD analysis with an\nappropriately modified outcome variable, which takes the form of the difference\nbetween the original outcome and a function of the covariates. We characterize\nthe function that leads to the estimator with the smallest asymptotic variance,\nand show how it can be estimated via modern machine learning, nonparametric\nregression, or classical parametric methods. The resulting estimator is easy to\nimplement, as tuning parameters can be chosen as in a conventional RD analysis.\nAn extensive simulation study illustrates the performance of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:00:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Noack", "Claudia", ""], ["Olma", "Tomasz", ""], ["Rothe", "Christoph", ""]]}, {"id": "2107.07965", "submitter": "Xiequan Fan", "authors": "Xiequan Fan, Qi-Man Shao", "title": "Self-normalized Cramer moderate deviations for a supercritical\n  Galton-Watson process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(Z_n)_{n\\geq0}$ be a supercritical Galton-Watson process. Consider the\nLotka-Nagaev estimator for the offspring mean. In this paper, we establish\nself-normalized Cram\\'{e}r type moderate deviations and Berry-Esseen's bounds\nfor the Lotka-Nagaev estimator. The results are believed to be optimal or near\noptimal.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:30:34 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Fan", "Xiequan", ""], ["Shao", "Qi-Man", ""]]}, {"id": "2107.07967", "submitter": "Georgia Papadogeorgou", "authors": "Fan Li (Duke University), Zizhong Tian, Jennifer Bobb, Georgia\n  Papadogeorgou, Fan Li (Yale University School of Public Health)", "title": "Clarifying Selection Bias in Cluster Randomized Trials: Estimands and\n  Estimation", "comments": "Keywords: causal inference, cluster randomized trial,\n  intention-to-treat, heterogeneous treatment effect, post-randomization,\n  principal stratification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In cluster randomized trials (CRTs), patients are typically recruited after\nclusters are randomized, and the recruiters and patients are not blinded to the\nassignment. This leads to differential recruitment process and systematic\ndifferences in baseline characteristics of the recruited patients between\nintervention and control arms, inducing post-randomization selection bias. We\nrigorously define the causal estimands in the presence of post-randomization\nconfounding. We elucidate the conditions under which standard covariate\nadjustment methods can validly estimate these estimands. We discuss the\nadditional data and assumptions necessary for estimating the causal effects\nwhen such conditions are not met. Adopting the principal stratification\nframework in causal inference, we clarify there are two intention-to-treat\n(ITT) causal estimands in CRTs: one for the overall population and one for the\nrecruited population. We derive the analytical formula of the two estimands in\nterms of principal-stratum-specific causal effects. We assess the empirical\nperformance of two common covariate adjustment methods, multivariate regression\nand propensity score weighting, under different data generating processes. When\ntreatment effects are heterogeneous across principal strata, the ITT effect on\nthe overall population differs from the ITT effect on the recruited population.\nA naive ITT analysis of the recruited sample leads to biased estimate of both\nITT effects. In the presence of post-randomization selection and without\nadditional data on the non-recruited subjects, the ITT effect on the recruited\npopulation is estimable only when the treatment effects are homogenous between\nprincipal strata, and the ITT effect on the overall population is generally not\nestimable. The extent to which covariate adjustment can remove selection bias\ndepends on the degree of effect heterogeneity across principal strata.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:33:11 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Fan", "", "Duke University"], ["Tian", "Zizhong", "", "Duke University"], ["Bobb", "Jennifer", "", "Duke University"], ["Papadogeorgou", "Georgia", "", "Duke University"], ["Li", "Fan", "", "Duke University"]]}, {"id": "2107.08288", "submitter": "Jianhua Huang", "authors": "Rui Tuo, Shiyuan He, Arash Pourhabib, Yu Ding and Jianhua Z. Huang", "title": "A Reproducing Kernel Hilbert Space Approach to Functional Calibration of\n  Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a frequentist solution to the functional calibration\nproblem, where the value of a calibration parameter in a computer model is\nallowed to vary with the value of control variables in the physical system. The\nneed of functional calibration is motivated by engineering applications where\nusing a constant calibration parameter results in a significant mismatch\nbetween outputs from the computer model and the physical experiment.\nReproducing kernel Hilbert spaces (RKHS) are used to model the optimal\ncalibration function, defined as the functional relationship between the\ncalibration parameter and control variables that gives the best prediction.\nThis optimal calibration function is estimated through penalized least squares\nwith an RKHS-norm penalty and using physical data. An uncertainty\nquantification procedure is also developed for such estimates. Theoretical\nguarantees of the proposed method are provided in terms of prediction\nconsistency and consistency of estimating the optimal calibration function. The\nproposed method is tested using both real and synthetic data and exhibits more\nrobust performance in prediction and uncertainty quantification than the\nexisting parametric functional calibration method and a state-of-art Bayesian\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 17:12:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tuo", "Rui", ""], ["He", "Shiyuan", ""], ["Pourhabib", "Arash", ""], ["Ding", "Yu", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2107.08732", "submitter": "Nial Friel", "authors": "Francesca Basini, Vasiliki Tsouli, Ioannis Ntzoufras, Nial Friel", "title": "Assessing competitive balance in the English Premier League for over\n  forty seasons using a stochastic block model", "comments": "31 pages. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Competitive balance is a desirable feature in any professional sports league\nand encapsulates the notion that there is unpredictability in the outcome of\ngames as opposed to an imbalanced league in which the outcome of some games are\nmore predictable than others, for example, when an apparent strong team plays\nagainst a weak team. In this paper, we develop a model-based clustering\napproach to provide an assessment of the balance between teams in a league. We\npropose a novel Bayesian model to represent the results of a football season as\na dense network with nodes identified by teams and categorical edges\nrepresenting the outcome of each game. The resulting stochastic block model\nfacilitates the probabilistic clustering of teams to assess whether there are\ncompetitive imbalances in a league. A key question then is to assess the\nuncertainty around the number of clusters or blocks and consequently estimation\nof the partition or allocation of teams to blocks. To do this, we develop an\nMCMC algorithm that allows the joint estimation of the number of blocks and the\nallocation of teams to blocks. We apply our model to each season in the English\npremier league from $1978/79$ to $2019/20$. A key finding of this analysis is\nevidence which suggests a structural change from a reasonably balanced league\nto a two-tier league which occurred around the early 2000's.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 10:01:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Basini", "Francesca", ""], ["Tsouli", "Vasiliki", ""], ["Ntzoufras", "Ioannis", ""], ["Friel", "Nial", ""]]}, {"id": "2107.08787", "submitter": "Ting Qi", "authors": "Yichen Lu, Jane Fridlyand, Tiffany Tang, Ting Qi, Noah Simon and Ning\n  Leng", "title": "The Future will be Different than Today: Model Evaluation Considerations\n  when Developing Translational Clinical Biomarker", "comments": "Paper has 4 pages, 2 figures. Appendix are supplementary at the end", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Finding translational biomarkers stands center stage of the future of\npersonalized medicine in healthcare. We observed notable challenges in\nidentifying robust biomarkers as some with great performance in one scenario\noften fail to perform well in new trials (e.g. different population,\nindications). With rapid development in the clinical trial world (e.g. assay,\ndisease definition), new trials very likely differ from legacy ones in many\nperspectives and in development of biomarkers this heterogeneity should be\nconsidered. In response, we recommend considering building in the heterogeneity\nwhen evaluating biomarkers. In this paper, we present one evaluation strategy\nby using leave-one-study-out (LOSO) in place of conventional cross-validation\n(cv) methods to account for the potential heterogeneity across trials used for\nbuilding and testing the biomarkers. To demonstrate the performance of K-fold\nvs LOSO cv in estimating the effect size of biomarkers, we leveraged data from\nclinical trials and simulation studies. In our assessment, LOSO cv provided a\nmore objective estimate of the future performance. This conclusion remained\ntrue across different evaluation metrics and different statistical methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:36:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lu", "Yichen", ""], ["Fridlyand", "Jane", ""], ["Tang", "Tiffany", ""], ["Qi", "Ting", ""], ["Simon", "Noah", ""], ["Leng", "Ning", ""]]}, {"id": "2107.08917", "submitter": "Stanislav Nagy", "authors": "Sami Helander, Petra Laketa, Pauliina Ilmonen, Stanislav Nagy, Germain\n  Van Bever, and Lauri Viitasaari", "title": "Integrated shape-sensitive functional metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a new integrated ball (pseudo)metric which provides an\nintermediary between a chosen starting (pseudo)metric d and the L_p distance in\ngeneral function spaces. Selecting d as the Hausdorff or Fr\\'echet distances,\nwe introduce integrated shape-sensitive versions of these supremum-based\nmetrics. The new metrics allow for finer analyses in functional settings, not\nattainable applying the non-integrated versions directly. Moreover, convergent\ndiscrete approximations make computations feasible in practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:01:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Helander", "Sami", ""], ["Laketa", "Petra", ""], ["Ilmonen", "Pauliina", ""], ["Nagy", "Stanislav", ""], ["Van Bever", "Germain", ""], ["Viitasaari", "Lauri", ""]]}, {"id": "2107.08922", "submitter": "Zhi Liu", "authors": "Zhi Liu and Nikhil Garg", "title": "Test-optional Policies: Overcoming Strategic Behavior and Informational\n  Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the Covid-19 pandemic, more than 500 US-based colleges and\nuniversities went \"test-optional\" for admissions and promised that they would\nnot penalize applicants for not submitting test scores, part of a longer trend\nto rethink the role of testing in college admissions. However, it remains\nunclear how (and whether) a college can simultaneously use test scores for\nthose who submit them, while not penalizing those who do not--and what that\npromise even means. We formalize these questions, and study how a college can\novercome two challenges with optional testing: $\\textit{strategic applicants}$\n(when those with low test scores can pretend to not have taken the test), and\n$\\textit{informational gaps}$ (it has more information on those who submit a\ntest score than those who do not). We find that colleges can indeed do so, if\nand only if they are able to use information on who has test access and are\nwilling to randomize admissions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:36:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Zhi", ""], ["Garg", "Nikhil", ""]]}, {"id": "2107.08995", "submitter": "Smitha Milli", "authors": "Smitha Milli, Luca Belli, Moritz Hardt", "title": "Causal Inference Struggles with Agency on Online Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online platforms regularly conduct randomized experiments to understand how\nchanges to the platform causally affect various outcomes of interest. However,\nexperimentation on online platforms has been criticized for having, among other\nissues, a lack of meaningful oversight and user consent. As platforms give\nusers greater agency, it becomes possible to conduct observational studies in\nwhich users self-select into the treatment of interest as an alternative to\nexperiments in which the platform controls whether the user receives treatment\nor not. In this paper, we conduct four large-scale within-study comparisons on\nTwitter aimed at assessing the effectiveness of observational studies derived\nfrom user self-selection on online platforms. In a within-study comparison,\ntreatment effects from an observational study are assessed based on how\neffectively they replicate results from a randomized experiment with the same\ntarget population. We test the naive difference in group means estimator, exact\nmatching, regression adjustment, and inverse probability of treatment weighting\nwhile controlling for plausible confounding variables. In all cases, all\nobservational estimates perform poorly at recovering the ground-truth estimate\nfrom the analogous randomized experiments. In all cases except one, the\nobservational estimates have the opposite sign of the randomized estimate. Our\nresults suggest that observational studies derived from user self-selection are\na poor alternative to randomized experimentation on online platforms. In\ndiscussing our results, we postulate \"Catch-22\"s that suggest that the success\nof causal inference in these settings may be at odds with the original\nmotivations for providing users with greater agency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:14:00 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Milli", "Smitha", ""], ["Belli", "Luca", ""], ["Hardt", "Moritz", ""]]}, {"id": "2107.09009", "submitter": "Andreas Markoulidakis Mr", "authors": "Andreas Markoulidakis, Peter Holmans, Philip Pallmann, Monica Busse,\n  Beth Ann Griffin", "title": "How balance and sample size impact bias in the estimation of causal\n  treatment effects: A simulation study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observational studies are often used to understand relationships between\nexposures and outcomes. They do not, however, allow conclusions about causal\nrelationships to be drawn unless statistical techniques are used to account for\nthe imbalance of confounders across exposure groups. Propensity score and\nbalance weighting (PSBW) are useful techniques that aim to reduce the\nimbalances between exposure groups by weighting the groups to look alike on the\nobserved confounders. Despite the plethora of available methods to estimate\nPSBW, there is little guidance on what one defines as adequate balance, and\nunbiased and robust estimation of the causal treatment effect is not guaranteed\nunless several conditions hold. Accurate inference requires that 1. the\ntreatment allocation mechanism is known, 2. the relationship between the\nbaseline covariates and the outcome is known, 3. adequate balance of baseline\ncovariates is achieved post-weighting, 4. a proper set of covariates to control\nfor confounding bias is known, and 5. a large enough sample size is available.\nIn this article, we use simulated data of various sizes to investigate the\ninfluence of these five factors on statistical inference. Our findings provide\nevidence that the maximum Kolmogorov- Smirnov statistic is the proper\nstatistical measure to assess balance on the baseline covariates, in contrast\nto the mean standardised mean difference used in many applications, and 0.1 is\na suitable threshold to consider as acceptable balance. Finally, we recommend\nthat 60-80 observations, per confounder per treatment group, are required to\nobtain a reliable and unbiased estimation of the causal treatment effect.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:38:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Markoulidakis", "Andreas", ""], ["Holmans", "Peter", ""], ["Pallmann", "Philip", ""], ["Busse", "Monica", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2107.09125", "submitter": "Grigorios Lavrentiadis", "authors": "Grigorios Lavrentiadis and Norman A. Abrahamson", "title": "A Non-ergodic Spectral Acceleration Ground Motion Model for California\n  Developed with Random Vibration Theory", "comments": "32 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new approach for creating a non-ergodic $PSA$ ground-motion model (GMM) is\npresented which account for the magnitude dependence of the non-ergodic\neffects. In this approach, the average $PSA$ scaling is controlled by an\nergodic $PSA$ GMM, and the non-ergodic effects are captured with non-ergodic\n$PSA$ factors, which are the adjustment that needs to be applied to an ergodic\n$PSA$ GMM to incorporate the non-ergodic effects. The non-ergodic $PSA$ factors\nare based on $EAS$ non-ergodic effects and are converted to $PSA$ through\nRandom Vibration Theory (RVT). The advantage of this approach is that it better\ncaptures the non-ergodic source, path, and site effects through the small\nmagnitude earthquakes. Due to the linear properties of Fourier Transform, the\n$EAS$ non-ergodic effects of the small events can be applied directly to the\nlarge magnitude events. This is not the case for $PSA$, as response spectrum is\ncontrolled by a range of frequencies, making $PSA$ non-ergodic effects depended\non the spectral shape which is magnitude dependent. Two $PSA$ non-ergodic GMMs\nare derived using the ASK14 and CY14 GMMs as backbone models, respectively. The\nnon-ergodic $EAS$ effects are estimated with the LAK21 GMM. The RVT\ncalculations are performed with the V75 peak factor model, the $D_{a0.05-0.85}$\nestimate of AS96 for the ground-motion duration, and BT15 oscillator-duration\nmodel. The California subset of the NGAWest2 database is used for both models.\nThe total aleatory standard deviation of the two non-ergodic $PSA$ GMMs is\napproximately $30$ to $35\\%$ smaller than the total aleatory standard deviation\nof the corresponding ergodic $PSA$ GMMs. This reduction has a significant\nimpact on hazard calculations at large return periods. In remote areas, far\nfrom stations and past events, the reduction of aleatory variability is\naccompanied by an increase of epistemic uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:45:37 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lavrentiadis", "Grigorios", ""], ["Abrahamson", "Norman A.", ""]]}, {"id": "2107.09160", "submitter": "Meini Tang", "authors": "Meini Tang, Chee-Ming Ting, Hernando Ombao", "title": "BICNet: A Bayesian Approach for Estimating Task Effects on Intrinsic\n  Connectivity Networks in fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intrinsic connectivity networks (ICNs) are specific dynamic functional brain\nnetworks that are consistently found under various conditions including rest\nand task. Studies have shown that some stimuli actually activate intrinsic\nconnectivity through either suppression, excitation, moderation or\nmodification. Nevertheless, the structure of ICNs and task-related effects on\nICNs are not yet fully understood. In this paper, we propose a Bayesian\nIntrinsic Connectivity Network (BICNet) model to identify the ICNs and quantify\nthe task-related effects on the ICN dynamics. Using an extended Bayesian\ndynamic sparse latent factor model, the proposed BICNet has the following\nadvantages: (1) it simultaneously identifies the individual ICNs and\ngroup-level ICN spatial maps; (2) it robustly identifies ICNs by jointly\nmodeling resting-state functional magnetic resonance imaging (rfMRI) and\ntask-related functional magnetic resonance imaging (tfMRI); (3) compared to\nindependent component analysis (ICA)-based methods, it can quantify the\ndifference of ICNs amplitudes across different states; (4) it automatically\nperforms feature selection through the sparsity of the ICNs rather than ad-hoc\nthresholding. The proposed BICNet was applied to the rfMRI and language tfMRI\ndata from the Human Connectome Project (HCP) and the analysis identified\nseveral ICNs related to distinct language processing functions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 21:13:50 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tang", "Meini", ""], ["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""]]}, {"id": "2107.09316", "submitter": "Vijay Kumar", "authors": "M. Arshad, M. Khetan, V. Kumar, A.K. Pathak", "title": "Record-Based Transmuted Generalized Linear Exponential Distribution with\n  Increasing, Decreasing and Bathtub Shaped Failure Rates", "comments": "29 pages, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear exponential distribution is a generalization of the exponential\nand Rayleigh distributions. This distribution is one of the best models to fit\ndata with increasing failure rate (IFR). But it does not provide a reasonable\nfit for modeling data with decreasing failure rate (DFR) and bathtub shaped\nfailure rate (BTFR). To overcome this drawback, we propose a new record-based\ntransmuted generalized linear exponential (RTGLE) distribution by using the\ntechnique of Balakrishnan and He (2021). The family of RTGLE distributions is\nmore flexible to fit the data sets with IFR, DFR, and BTFR, and also\ngeneralizes several well-known models as well as some new record-based\ntransmuted models. This paper aims to study the statistical properties of RTGLE\ndistribution, like, the shape of the probability density function and hazard\nfunction, quantile function and its applications, moments and its generating\nfunction, order and record statistics, Renyi entropy. The maximum likelihood\nestimators, least squares and weighted least squares estimators,\nAnderson-Darling estimators, Cramer-von Mises estimators of the unknown\nparameters are constructed and their biases and mean squared errors are\nreported via Monte Carlo simulation study. Finally, the real data set based on\nfailure time illustrates the goodness of fit and applicability of the proposed\ndistribution; hence, suitable recommendations are forwarded.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:07:26 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Arshad", "M.", ""], ["Khetan", "M.", ""], ["Kumar", "V.", ""], ["Pathak", "A. K.", ""]]}, {"id": "2107.09357", "submitter": "Mario Beraha", "authors": "Mario Beraha, Daniele Falco and Alessandra Guglielmi", "title": "JAGS, NIMBLE, Stan: a detailed comparison among Bayesian MCMC software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is the comparison of the performance of the three\npopular software platforms JAGS, NIMBLE and Stan. These probabilistic\nprogramming languages are able to automatically generate samples from the\nposterior distribution of interest using MCMC algorithms, starting from the\nspecification of a Bayesian model, i.e. the likelihood and the prior. The final\ngoal is to present a detailed analysis of their strengths and weaknesses to\nstatisticians or applied scientists. In this way, we wish to contribute to make\nthem fully aware of the pros and cons of this software. We carry out a\nsystematic comparison of the three platforms on a wide class of models, prior\ndistributions, and data generating mechanisms. Our extensive simulation studies\nevaluate the quality of the MCMC chains produced, the efficiency of the\nsoftware and the goodness of fit of the output. We also consider the efficiency\nof the parallelization made by the three platforms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:24:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Beraha", "Mario", ""], ["Falco", "Daniele", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2107.09365", "submitter": "Flora Alarcon", "authors": "Flora Alarcon and Violaine Plant\\'e-Bordeneuve and Gregory Nuel", "title": "Study of the Parent-of-origin effect in monogenic diseases with variable\n  age of onset. Application on ATTRv", "comments": "14 pages, 4 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In genetic diseases with variable age of onset, an accurate estimation of the\nsurvival function for the mutation carriers and also modifying factors effects\nestimations are important for the management of asymptomatic gene carriers\nacross life. Among the modifying factors, the gender of the parent transmitting\nthe mutation (i.e. the parent-of-origin effect) has been shown to have a\nsignificant effect on survival curve estimation on transthyretin familial\namyloid polyneuropathy (ATTRv) families. However, as most genotypes are\nunknown, the parent-of-origin must be calculated through a probability\nestimated from the pedigree. We propose in this article to extend the method\nproviding mutation carrier survival estimates in order to estimate the\nparent-of-origin effect. The method is both validated on simulated data and\napplied to familly samples with ATTRv.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:36:19 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Alarcon", "Flora", ""], ["Plant\u00e9-Bordeneuve", "Violaine", ""], ["Nuel", "Gregory", ""]]}, {"id": "2107.09410", "submitter": "George Leckie", "authors": "George Leckie and Lucy Prior", "title": "A Comparison of Value-Added Models for School Accountability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  School accountability systems increasingly hold schools to account for their\nperformances using value-added models purporting to measure the effect of\nschools on student learning. The most common approach is to fit a linear\nregression of student current achievement on student prior achievement, where\nthe school effects are the school means of the predicted residuals. In the\nliterature further adjustments are made for student sociodemographics and\nsometimes school composition and 'non-malleable' characteristics. However,\naccountability systems typically make fewer adjustments: for transparency to\nend users, because data is unavailable or of insufficient quality, or for\nideological reasons. There is therefore considerable interest in understanding\nthe extent to which simpler models give similar school effects to more\ntheoretically justified but complex models. We explore these issues via a case\nstudy and empirical analysis of England's 'Progress 8' secondary school\naccountability system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:17:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Leckie", "George", ""], ["Prior", "Lucy", ""]]}, {"id": "2107.09538", "submitter": "Brian Bush", "authors": "Brian W. Bush, Joanne Wendelberger, Rebecca Hanes", "title": "Adaptively Sampling via Regional Variance-Based Sensitivities", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "NREL/JA-6A20-78775", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the well-established variance-based methods for global\nsensitivity analysis, we develop a local total sensitivity index that\ndecomposes the global total sensitivity conditions by independent variables'\nvalues. We employ this local sensitivity index in a new method of experimental\ndesign that sequentially and adaptively samples the domain of a multivariate\nfunction according to local contributions to the global variance. The method is\ndemonstrated on a nonlinear illustrative example that has a three-dimensional\ndomain and a three-dimensional codomain, but also on a complex,\nhigh-dimensional simulation for assessing the industrial viability of the\nproduction of bioproducts from biomass.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:51:36 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bush", "Brian W.", ""], ["Wendelberger", "Joanne", ""], ["Hanes", "Rebecca", ""]]}, {"id": "2107.09633", "submitter": "Matthew Aldridge", "authors": "Matthew Aldridge", "title": "Pooled testing to isolate infected individuals", "comments": "5 pages, 2 figures. Presented at CISS 2021; video of talk:\n  https://www.youtube.com/watch?v=m-3e6OdBYZg", "journal-ref": "2021 55th Annual Conference on Information Sciences and Systems\n  (CISS), 2021", "doi": "10.1109/CISS50987.2021.9400313", "report-no": null, "categories": "stat.AP cs.IT math.IT q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual problem for group testing is this: For a given number of\nindividuals and a given prevalence, how many tests T* are required to find\nevery infected individual? In real life, however, the problem is usually\ndifferent: For a given number of individuals, a given prevalence, and a limited\nnumber of tests T much smaller than T*, how can these tests best be used? In\nthis conference paper, we outline some recent results on this problem for two\nmodels. First, the \"practical\" model, which is relevant for screening for\nCOVID-19 and has tests that are highly specific but imperfectly sensitive,\nshows that simple algorithms can be outperformed at low prevalence and high\nsensitivity. Second, the \"theoretical\" model of very low prevalence with\nperfect tests gives interesting new mathematical results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:26:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Aldridge", "Matthew", ""]]}, {"id": "2107.09730", "submitter": "Liangyuan Hu", "authors": "Jung-Yi Joyce Lin, Liangyuan Hu, Chuyue Huang, Steven Lawrence, Usha\n  Govindarajulu", "title": "Strategies for variable selection in large-scale healthcare database\n  studies with missing covariate and outcome data", "comments": "18 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has shown that combining bootstrap imputation with tree-based\nmachine learning variable selection methods can recover the good performance\nachievable on fully observed data when covariate and outcome data are missing\nat random (MAR). This approach however is computationally expensive, especially\non large-scale datasets. We propose an inference-based method RR-BART, that\nleverages the likelihood-based Bayesian machine learning technique, Bayesian\nAdditive Regression Trees, and uses Rubin's rule to combine the estimates and\nvariances of the variable importance measures on multiply imputed datasets for\nvariable selection in the presence of missing data. A representative simulation\nstudy suggests that RR-BART performs at least as well as combining bootstrap\nwith BART, BI-BART, but offers substantial computational savings, even in\ncomplex conditions of nonlinearity and nonadditivity with a large percentage of\noverall missingness under the MAR mechanism. RR-BART is also less sensitive to\nthe end note prior via the hyperparameter $k$ than BI-BART, and does not depend\non the selection threshold value $\\pi$ as required by BI-BART. Our simulation\nstudies also suggest that encoding the missing values of a binary predictor as\na separate category significantly improves the power of selecting the binary\npredictor for BI-BART. We further demonstrate the methods via a case study of\nrisk factors for 3-year incidence of metabolic syndrome with data from the\nStudy of Women's Health Across the Nation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 19:01:24 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lin", "Jung-Yi Joyce", ""], ["Hu", "Liangyuan", ""], ["Huang", "Chuyue", ""], ["Lawrence", "Steven", ""], ["Govindarajulu", "Usha", ""]]}, {"id": "2107.09765", "submitter": "Jaime Sevilla", "authors": "Jaime Sevilla and Alexandra Mayn", "title": "A conditional independence test for causality in econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Y-test is a useful tool for detecting missing confounders in the context\nof a multivariate regression.However, it is rarely used in practice since it\nrequires identifying multiple conditionally independent instruments, which is\noften impossible. We propose a heuristic test which relaxes the independence\nrequirement. We then show how to apply this heuristic test on a price-demand\nand a firm loan-productivity problem. We conclude that the test is informative\nwhen the variables are linearly related with Gaussian additive noise, but it\ncan be misleading in other contexts. Still, we believe that the test can be a\nuseful concept for falsifying a proposed control set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:35:13 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Sevilla", "Jaime", ""], ["Mayn", "Alexandra", ""]]}, {"id": "2107.09948", "submitter": "Alex John Quijano", "authors": "Alex John Quijano, Rick Dale, and Suzanne Sindi", "title": "A Statistical Model of Word Rank Evolution", "comments": "This manuscript - with 53 pages and 28 figures - is a draft for a\n  journal research article submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. This work explores the word rank dynamics of eight\nlanguages by investigating the Google Books corpus unigram frequency data set.\nWe observed the rank changes of the unigrams from 1900 to 2008 and compared it\nto a Wright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing words. This work explains the mathematical framework of the model\n- written as a Markov Chain with multinomial transition probabilities - to show\nhow frequencies of words change in time. From our observations in the data and\nour model, word rank stability shows two types of characteristics: (1) the\nincrease/decrease in ranks are monotonic, or (2) the average rank stays the\nsame. Based on our model, high-ranked words tend to be more stable while\nlow-ranked words tend to be more volatile. Some words change in ranks in two\nways: (a) by an accumulation of small increasing/decreasing rank changes in\ntime and (b) by shocks of increase/decrease in ranks. Most of the stopwords and\nSwadesh words are observed to be stable in ranks across eight languages. These\nsignatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:57:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 06:41:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Quijano", "Alex John", ""], ["Dale", "Rick", ""], ["Sindi", "Suzanne", ""]]}, {"id": "2107.10041", "submitter": "Kohei Nishi", "authors": "Kohei Nishi", "title": "The impact of increasing COVID-19 cases/deaths on the number of uncivil\n  tweets directed at governments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political expression through social media such as Twitter has already taken\nroot as a form of political participation. However, it is less clear what kind\nof political messages people send out on social media and under what\ncircumstances they do so. This study theorizes that when government policy\nperformance worsens, people get angry or frustrated and send uncivil messages\nto the government. To test this theory, the current study classifies tweets\ndirected at U.S. state governors as uncivil or not, using a neural network\nmachine-learning model, and examines the impact of worsening state-level\nCOVID-19 indicators on the number of uncivil tweets directed at the state\ngovernors. The results show that increasing state-level COVID-19 cases and\ndeaths lead to higher numbers of uncivil tweets directed at state governors.\nThis suggests that people evaluate the government's performance through actions\nother than voting.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:19:14 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 05:03:36 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Nishi", "Kohei", ""]]}, {"id": "2107.10118", "submitter": "Joshua Keller", "authors": "Joshua P. Keller, Tianjian Zhou, Andee Kaplan, G. Brooke Anderson, Wen\n  Zhou", "title": "Tracking the Transmission Dynamics of COVID-19 with a Time-Varying\n  Coefficient State-Space Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of COVID-19 has been greatly impacted by regulatory policies and\nbehavior patterns that vary across counties, states, and countries.\nPopulation-level dynamics of COVID-19 can generally be described using a set of\nordinary differential equations, but these deterministic equations are\ninsufficient for modeling the observed case rates, which can vary due to local\ntesting and case reporting policies and non-homogeneous behavior among\nindividuals. To assess the impact of population mobility on the spread of\nCOVID-19, we have developed a novel Bayesian time-varying coefficient\nstate-space model for infectious disease transmission. The foundation of this\nmodel is a time-varying coefficient compartment model to recapitulate the\ndynamics among susceptible, exposed, undetected infectious, detected\ninfectious, undetected removed, detected non-infectious, detected recovered,\nand detected deceased individuals. The infectiousness and detection parameters\nare modeled to vary by time, and the infectiousness component in the model\nincorporates information on multiple sources of population mobility. Along with\nthis compartment model, a multiplicative process model is introduced to allow\nfor deviation from the deterministic dynamics. We apply this model to observed\nCOVID-19 cases and deaths in several US states and Colorado counties. We find\nthat population mobility measures are highly correlated with transmission rates\nand can explain complicated temporal variation in infectiousness in these\nregions. Additionally, the inferred connections between mobility and\nepidemiological parameters, varying across locations, have revealed the\nheterogeneous effects of different policies on the dynamics of COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 14:44:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Keller", "Joshua P.", ""], ["Zhou", "Tianjian", ""], ["Kaplan", "Andee", ""], ["Anderson", "G. Brooke", ""], ["Zhou", "Wen", ""]]}, {"id": "2107.10148", "submitter": "Jingyu Ji", "authors": "Jingyu Ji, Deyuan Li and Zhengjun Zhang", "title": "Decoupling Systemic Risk into Endopathic and Exopathic Competing Risks\n  Through Autoregressive Conditional Accelerated Fr\\'echet Model", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying systemic risk patterns in geopolitical, economic, financial,\nenvironmental, transportation, epidemiological systems, and their impacts is\nthe key to risk management. This paper introduces two new endopathic and\nexopathic competing risks. The paper integrates the new extreme value theory\nfor maxima of maxima and the autoregressive conditional Fr\\'echet model for\nsystemic risk into a new autoregressive conditional accelerated Fr\\'echet\n(AcAF) model, which enables decoupling systemic risk into endopathic and\nexopathic competing risks. The paper establishes the probabilistic properties\nof stationarity and ergodicity of the AcAF model. Statistical inference is\ndeveloped through conditional maximum likelihood estimation. The consistency\nand asymptotic normality of the estimators are derived. Simulation demonstrates\nthe efficiency of the proposed estimators and the AcAF model's flexibility in\nmodeling heterogeneous data. Empirical studies on the stock returns in S\\&P 500\nand the cryptocurrency trading show the superior performance of the proposed\nmodel in terms of the identified risk patterns, endopathic and exopathic\ncompeting risks, being informative with greater interpretability, enhancing the\nunderstanding of the systemic risks of a market and their causes, and making\nbetter risk management possible.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:25:23 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ji", "Jingyu", ""], ["Li", "Deyuan", ""], ["Zhang", "Zhengjun", ""]]}, {"id": "2107.10239", "submitter": "Aurelia Bustos", "authors": "Aurelia Bustos (1), Patricio Mas_Serrano (2 and 3), Mari L. Boquera\n  (2), Jose M. Salinas (4) ((1) MedBravo, (2) Hospital General Universitario de\n  Alicante Spain -HGUA, (3) Institute for Health and Biomedical Research of\n  Alicante -ISABIAL, (4) Department of Health Informatics, Hospital\n  Universitario San Juan de Alicante Spain)", "title": "Machine Learning for Real-World Evidence Analysis of COVID-19\n  Pharmacotherapy", "comments": "22 pages, 7 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduction: Real-world data generated from clinical practice can be used to\nanalyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate\nthe results of randomized clinical trials (RCTs). Machine learning (ML) methods\nare being used in RWE and are promising tools for precision-medicine. In this\nstudy, ML methods are applied to study the efficacy of therapies on COVID-19\nhospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312\nCOVID-19 hospital admissions - dated between January 2020 and January 2021 from\n10 health departments, were used respectively for training and validation of\nseparate treatment-effect models (TE-ML) for remdesivir, corticosteroids,\ntocilizumab, lopinavir-ritonavir, azithromycin and\nchloroquine/hydroxychloroquine. 2390 admissions from 2 additional health\ndepartments were reserved as an independent test to analyze retrospectively the\nsurvival benefits of therapies in the population selected by the TE-ML models\nusing cox-proportional hazard models. TE-ML models were adjusted using\ntreatment propensity scores to control for pre-treatment confounding variables\nassociated to outcome and further evaluated for futility. ML architecture was\nbased on boosted decision-trees. Results: In the populations identified by the\nTE-ML models, only Remdesivir and Tocilizumab were significantly associated\nwith an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and\n0.21 (P = 0.001), respectively. No survival benefits from chloroquine\nderivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to\nexplain the predictions of TE-ML models are explored at patient-level as\npotential tools for personalized decision making and precision medicine.\nConclusion: ML methods are suitable tools toward RWE analysis of COVID-19\npharmacotherapies. Results obtained reproduce published results on RWE and\nvalidate the results from RCTs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:28:54 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bustos", "Aurelia", "", "2 and 3"], ["Mas_Serrano", "Patricio", "", "2 and 3"], ["Boquera", "Mari L.", ""], ["Salinas", "Jose M.", ""]]}, {"id": "2107.10306", "submitter": "Dan Wang", "authors": "Dan Wang, Zhi Chen, Ionut Florescu", "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating", "comments": "16 pages, 11 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:47:35 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Dan", ""], ["Chen", "Zhi", ""], ["Florescu", "Ionut", ""]]}, {"id": "2107.10340", "submitter": "Andrea Arnold", "authors": "Andrea Arnold, Loris Fichera", "title": "Identification of Tissue Optical Properties During Thermal Laser-Tissue\n  Interactions: Approach and Preliminary Evaluation", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.data-an q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a computational framework to estimate the physical\ntissue properties that govern the thermal response of laser-irradiated tissue.\nWe focus in particular on two quantities, the absorption and scattering\ncoefficients, which describe the optical absorption of light in the tissue and\nwhose knowledge is vital to correctly plan medical laser treatments. To perform\nthe estimation, we utilize an implementation of the Ensemble Kalman Filter\n(EnKF), a type of Bayesian filtering algorithm for data assimilation. Unlike\nprior approaches, in this work we estimate the tissue optical properties based\non the observed surface thermal response to laser irradiation. This method has\nthe potential for straightforward implementation in a clinical setup, as it\nwould only require a simple thermal sensor, e.g., a miniaturized infrared\ncamera. Because the optical properties of tissue can undergo shifts during\nlaser exposure, we employ a variant of EnKF capable of tracking time-varying\nparameters. Through preliminary evaluation in simulation, we demonstrate the\nability of the proposed technique to identify the tissue optical properties and\ntrack their dynamic changes during laser exposure, while simultaneously\ntracking changes in the tissue temperature at locations beneath the surface.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 20:10:39 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Arnold", "Andrea", ""], ["Fichera", "Loris", ""]]}, {"id": "2107.10398", "submitter": "\\'Oscar Escudero Arnanz", "authors": "\\'Oscar Escudero-Arnanz, Joaqu\\'in Rodr\\'iguez-\\'Alvarez, Karl\n  {\\O}yvind Mikalsen, Robert Jenssen, Cristina Soguero-Ruiz", "title": "On the Use of Time Series Kernel and Dimensionality Reduction to\n  Identify the Acquisition of Antimicrobial Multidrug Resistance in the\n  Intensive Care Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.med-ph q-bio.PE stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The acquisition of Antimicrobial Multidrug Resistance (AMR) in patients\nadmitted to the Intensive Care Units (ICU) is a major global concern. This\nstudy analyses data in the form of multivariate time series (MTS) from 3476\npatients recorded at the ICU of University Hospital of Fuenlabrada (Madrid)\nfrom 2004 to 2020. 18\\% of the patients acquired AMR during their stay in the\nICU. The goal of this paper is an early prediction of the development of AMR.\nTowards that end, we leverage the time-series cluster kernel (TCK) to learn\nsimilarities between MTS. To evaluate the effectiveness of TCK as a kernel, we\napplied several dimensionality reduction techniques for visualization and\nclassification tasks. The experimental results show that TCK allows identifying\na group of patients that acquire the AMR during the first 48 hours of their ICU\nstay, and it also provides good classification capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:44:55 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Escudero-Arnanz", "\u00d3scar", ""], ["Rodr\u00edguez-\u00c1lvarez", "Joaqu\u00edn", ""], ["Mikalsen", "Karl \u00d8yvind", ""], ["Jenssen", "Robert", ""], ["Soguero-Ruiz", "Cristina", ""]]}, {"id": "2107.10400", "submitter": "Elijah Cole", "authors": "Sara Beery, Elijah Cole, Joseph Parker, Pietro Perona, Kevin Winner", "title": "Species Distribution Modeling for Machine Learning Practitioners: A\n  Review", "comments": "ACM COMPASS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conservation science depends on an accurate understanding of what's happening\nin a given ecosystem. How many species live there? What is the makeup of the\npopulation? How is that changing over time? Species Distribution Modeling (SDM)\nseeks to predict the spatial (and sometimes temporal) patterns of species\noccurrence, i.e. where a species is likely to be found. The last few years have\nseen a surge of interest in applying powerful machine learning tools to\nchallenging problems in ecology. Despite its considerable importance, SDM has\nreceived relatively little attention from the computer science community. Our\ngoal in this work is to provide computer scientists with the necessary\nbackground to read the SDM literature and develop ecologically useful ML-based\nSDM algorithms. In particular, we introduce key SDM concepts and terminology,\nreview standard models, discuss data availability, and highlight technical\nchallenges and pitfalls.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 17:50:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Beery", "Sara", ""], ["Cole", "Elijah", ""], ["Parker", "Joseph", ""], ["Perona", "Pietro", ""], ["Winner", "Kevin", ""]]}, {"id": "2107.10572", "submitter": "Ines Wilms", "authors": "Ines Wilms, Rebecca Killick and David S. Matteson", "title": "Graphical Influence Diagnostics for Changepoint Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Changepoint models enjoy a wide appeal in a variety of disciplines to model\nthe heterogeneity of ordered data. Graphical influence diagnostics to\ncharacterize the influence of single observations on changepoint models are,\nhowever, lacking. We address this gap by developing a framework for\ninvestigating instabilities in changepoint segmentations and assessing the\ninfluence of single observations on various outputs of a changepoint analysis.\nWe construct graphical diagnostic plots that allow practitioners to assess\nwhether instabilities occur; how and where they occur; and to detect\ninfluential individual observations triggering instability. We analyze well-log\ndata to illustrate how such influence diagnostic plots can be used in practice\nto reveal features of the data that may otherwise remain hidden.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:54:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wilms", "Ines", ""], ["Killick", "Rebecca", ""], ["Matteson", "David S.", ""]]}, {"id": "2107.10647", "submitter": "Joaquin Cordero", "authors": "Joaqu\\'in Cordero, Alfredo Bolt and Mauricio Valle", "title": "An\\'alisis de Canasta de mercado en supermercados mediante mapas\n  auto-organizados", "comments": "18 pages, in Spanish, 7 Figures, 5 tables, Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduction: An important chain of supermarkets in the western zone of the\ncapital of Chile, needs to obtain key information to make decisions, this\ninformation is available in the databases but needs to be processed due to the\ncomplexity and quantity of information which becomes difficult to visualiz,.\nMethod: For this purpose, an algorithm was developed using artificial neural\nnetworks applying Kohonen's SOM method. To carry it out, certain key procedures\nmust be followed to develop it, such as data mining that will be responsible\nfor filtering and then use only the relevant data for market basket analysis.\nAfter filtering the information, the data must be prepared. After data\npreparation, we prepared the Python programming environment to adapt it to the\nsample data, then proceed to train the SOM with its parameters set after test\nresults. Result: the result of the SOM obtains the relationship between the\nproducts that were most purchased by positioning them topologically close, to\nform promotions, packs and bundles for the retail manager to take into\nconsideration, because these relationships were obtained as a result of the SOM\ntraining with the real transactions of the clients. Conclusion: Based on this,\nrecommendations on frequent shopping baskets have been made to the supermarket\nchain that provided the data used in the research\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:52:14 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cordero", "Joaqu\u00edn", ""], ["Bolt", "Alfredo", ""], ["Valle", "Mauricio", ""]]}, {"id": "2107.10659", "submitter": "Ashwin Machanavajjhala", "authors": "Sam Haney and William Sexton and Ashwin Machanavajjhala and Michael\n  Hay and Gerome Miklau", "title": "Differentially Private Algorithms for 2020 Census Detailed DHC Race \\&\n  Ethnicity", "comments": "Presented at Theory and Practice of Differential Privacy Workshop\n  (TPDP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article describes a proposed differentially private (DP) algorithms that\nthe US Census Bureau is considering to release the Detailed Demographic and\nHousing Characteristics (DHC) Race & Ethnicity tabulations as part of the 2020\nCensus. The tabulations contain statistics (counts) of demographic and housing\ncharacteristics of the entire population of the US crossed with detailed races\nand tribes at varying levels of geography. We describe two differentially\nprivate algorithmic strategies, one based on adding noise drawn from a\ntwo-sided Geometric distribution that satisfies \"pure\"-DP, and another based on\nadding noise from a Discrete Gaussian distribution that satisfied a well\nstudied variant of differential privacy, called Zero Concentrated Differential\nPrivacy (zCDP). We analytically estimate the privacy loss parameters ensured by\nthe two algorithms for comparable levels of error introduced in the statistics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:35:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Haney", "Sam", ""], ["Sexton", "William", ""], ["Machanavajjhala", "Ashwin", ""], ["Hay", "Michael", ""], ["Miklau", "Gerome", ""]]}, {"id": "2107.10724", "submitter": "Rohan Alexander", "authors": "Annie Collins, Rohan Alexander", "title": "Reproducibility of COVID-19 pre-prints", "comments": "14 pages, 6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To examine the reproducibility of COVID-19 research, we create a dataset of\npre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January\n2020 and 30 June 2021 that are related to COVID-19. We extract the text from\nthese pre-prints and parse them looking for keyword markers signalling the\navailability of the data and code underpinning the pre-print. For the\npre-prints that are in our sample, we are unable to find markers of either open\ndata or open code for 75 per cent of those on arXiv, 67 per cent of those on\nbioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv.\nWe conclude that there may be value in having authors categorize the degree of\nopenness of their pre-print as part of the pre-print submissions process, and\nmore broadly, there is a need to better integrate open science training into a\nwide range of fields.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:02:06 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Collins", "Annie", ""], ["Alexander", "Rohan", ""]]}, {"id": "2107.10828", "submitter": "Mario Beykirch", "authors": "Mario Beykirch, Tim Janke, Imed Tayeche and Florian Steinke", "title": "Probabilistic Forecast Combination for Anomaly Detection in Building\n  Heat Load Time Series", "comments": "Accepted in the proceedings of ISGT-Europe 2021 to be published by\n  IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automated anomaly detection for building level\nheat load time series. An anomaly detection model must be applicable to a\ndiverse group of buildings and provide robust results on heat load time series\nwith low signal-to-noise ratios, several seasonalities, and significant\nexogenous effects. We propose to employ a probabilistic forecast combination\napproach based on an ensemble of deterministic forecasts in an anomaly\ndetection scheme that classifies observed values based on their probability\nunder a predictive distribution. We show empirically that forecast based\nanomaly detection provides improved accuracy when employing a forecast\ncombination approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:39:22 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Beykirch", "Mario", ""], ["Janke", "Tim", ""], ["Tayeche", "Imed", ""], ["Steinke", "Florian", ""]]}, {"id": "2107.10835", "submitter": "James Bagrow", "authors": "James P. Bagrow and Sune Lehmann", "title": "Recovering lost and absent information in temporal networks", "comments": "19 pages, 5 figures, 1 table, plus supporting information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The full range of activity in a temporal network is captured in its edge\nactivity data -- time series encoding the tie strengths or on-off dynamics of\neach edge in the network. However, in many practical applications, edge-level\ndata are unavailable, and the network analyses must rely instead on node\nactivity data which aggregates the edge-activity data and thus is less\ninformative. This raises the question: Is it possible to use the static network\nto recover the richer edge activities from the node activities? Here we show\nthat recovery is possible, often with a surprising degree of accuracy given how\nmuch information is lost, and that the recovered data are useful for subsequent\nnetwork analysis tasks. Recovery is more difficult when network density\nincreases, either topologically or dynamically, but exploiting dynamical and\ntopological sparsity enables effective solutions to the recovery problem. We\nformally characterize the difficulty of the recovery problem both theoretically\nand empirically, proving the conditions under which recovery errors can be\nbounded and showing that, even when these conditions are not met, good quality\nsolutions can still be derived. Effective recovery carries both promise and\nperil, as it enables deeper scientific study of complex systems but in the\ncontext of social systems also raises privacy concerns when social information\ncan be aggregated across multiple data sources.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:49:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bagrow", "James P.", ""], ["Lehmann", "Sune", ""]]}, {"id": "2107.10911", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi", "title": "Estimating survival parameters under conditionally independent left\n  truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EHR-derived databases are commonly subject to left truncation, a type of\nselection bias induced due to patients needing to survive long enough to\nsatisfy certain entry criteria. Standard methods to adjust for left truncation\nbias rely on an assumption of marginal independence between entry and survival\ntimes, which may not always be satisfied in practice. In this work, we examine\nhow a weaker assumption of conditional independence can result in unbiased\nestimation of common statistical parameters. In particular, we show the\nestimability of conditional parameters in a truncated dataset, and of marginal\nparameters that leverage reference data containing non-truncated data on\nconfounders. The latter is complementary to observational causal inference\nmethodology applied to real world external comparators, which is a common use\ncase for real world databases. We implement our proposed methods in simulation\nstudies, demonstrating unbiased estimation and valid statistical inference. We\nalso illustrate estimation of a survival distribution under conditionally\nindependent left truncation in a real world clinico-genomic database.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:14:57 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Sondhi", "Arjun", ""]]}, {"id": "2107.11014", "submitter": "Fan Yang", "authors": "Guanglei Hong, Fan Yang, and Xu Qin", "title": "Post-Treatment Confounding in Causal Mediation Studies: A Cutting-Edge\n  Problem and A Novel Solution via Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal mediation studies that decompose an average treatment effect into a\nnatural indirect effect (NIE) and a natural direct effect (NDE), examples of\npost-treatment confounding are abundant. Past research has generally considered\nit infeasible to adjust for a post-treatment confounder of the mediator-outcome\nrelationship due to incomplete information: it is observed under the actual\ntreatment condition while missing under the counterfactual treatment condition.\nThis study proposes a new sensitivity analysis strategy for handling\npost-treatment confounding and incorporates it into weighting-based causal\nmediation analysis without making extra identification assumptions. Under the\nsequential ignorability of the treatment assignment and of the mediator, we\nobtain the conditional distribution of the post-treatment confounder under the\ncounterfactual treatment as a function of not just pretreatment covariates but\nalso its counterpart under the actual treatment. The sensitivity analysis then\ngenerates a bound for the NIE and that for the NDE over a plausible range of\nthe conditional correlation between the post-treatment confounder under the\nactual and that under the counterfactual conditions. Implemented through either\nimputation or integration, the strategy is suitable for binary as well as\ncontinuous measures of post-treatment confounders. Simulation results\ndemonstrate major strengths and potential limitations of this new solution. A\nre-analysis of the National Evaluation of Welfare-to-Work Strategies (NEWWS)\nRiverside data reveals that the initial analytic results are sensitive to\nomitted post-treatment confounding.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:27:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hong", "Guanglei", ""], ["Yang", "Fan", ""], ["Qin", "Xu", ""]]}, {"id": "2107.11059", "submitter": "Mario W\\\"uthrich V.", "authors": "Ronald Richman and Mario V. W\\\"uthrich", "title": "LocalGLMnet: interpretable deep learning for tabular data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-fin.ST stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning models have gained great popularity in statistical modeling\nbecause they lead to very competitive regression models, often outperforming\nclassical statistical models such as generalized linear models. The\ndisadvantage of deep learning models is that their solutions are difficult to\ninterpret and explain, and variable selection is not easily possible because\ndeep learning models solve feature engineering and variable selection\ninternally in a nontransparent way. Inspired by the appealing structure of\ngeneralized linear models, we propose a new network architecture that shares\nsimilar features as generalized linear models, but provides superior predictive\npower benefiting from the art of representation learning. This new architecture\nallows for variable selection of tabular data and for interpretation of the\ncalibrated deep learning model, in fact, our approach provides an additive\ndecomposition in the spirit of Shapley values and integrated gradients.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:38:33 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Richman", "Ronald", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "2107.11124", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz, Dagmara Nikulin", "title": "COVID-19 and the gig economy in Poland", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We use a dataset covering nearly the entire target population based on\npassively collected data from smartphones to measure the impact of the first\nCOVID-19 wave on the gig economy in Poland. In particular, we focus on\ntransportation (Uber, Bolt) and delivery (Wolt, Takeaway, Glover, DeliGoo)\napps, which make it possible to distinguish between the demand and supply part\nof this market. Based on Bayesian structural time-series models, we estimate\nthe causal impact of the first COVID-19 wave on the number of active drivers\nand couriers. We show a significant relative increase for Wolt and Glover (15%\nand 24%) and a slight relative decrease for Uber and Bolt (-3% and -7%) in\ncomparison to a counterfactual control. The change for Uber and Bolt can be\npartially explained by the prospect of a new law (the so-called Uber Lex),\nwhich was already announced in 2019 and is intended to regulate the work of\nplatform drivers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 10:27:01 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Nikulin", "Dagmara", ""]]}, {"id": "2107.11133", "submitter": "Etienne Theising", "authors": "Etienne Theising, Dominik Wied, Daniel Ziggel", "title": "Reference Class Selection in Similarity-Based Forecasting of Sales\n  Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to find appropriate outside views for sales\nforecasts of analysts. The idea is to find reference classes, i.e. peer groups,\nfor each analyzed company separately. Hence, additional companies are\nconsidered that share similarities to the firm of interest with respect to a\nspecific predictor. The classes are regarded to be optimal if the forecasted\nsales distributions match the actual distributions as closely as possible. The\nforecast quality is measured by applying goodness-of-fit tests on the estimated\nprobability integral transformations and by comparing the predicted quantiles.\nThe method is applied on a data set consisting of 21,808 US firms over the time\nperiod 1950 - 2019, which is also descriptively analyzed. It appears that in\nparticular the past operating margins are good predictors for the distribution\nof future sales. A case study with a comparison of our forecasts with actual\nanalysts' estimates emphasizes the relevance of our approach in practice.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 10:54:24 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Theising", "Etienne", ""], ["Wied", "Dominik", ""], ["Ziggel", "Daniel", ""]]}, {"id": "2107.11233", "submitter": "Rodrigo Labouriau", "authors": "J.S. Pelck, H. Holthusen, M. Edelenbos, A. Luca and R. Labouriau", "title": "Multivariate Methods for Detection of Rubbery Rot in Storage Apples by\n  Monitoring Volatile Organic Compounds: An Example of Multivariate Generalised\n  Mixed Models", "comments": "11 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article is a case study illustrating the use of a multivariate\nstatistical method for screening potential chemical markers for early detection\nof post-harvest disease in storage fruit. We simultaneously measure a range of\nvolatile organic compounds (VOCs) and two measures of severity of disease\ninfection in apples under storage: the number of apples presenting visible\nsymptoms and the lesion area. We use multivariate generalised linear mixed\nmodels (MGLMM) for studying association patterns of those simultaneously\nobserved responses via the covariance structure of random components.\nRemarkably, those MGLMMs can be used to represent patterns of association\nbetween quantities of different statistical nature. In the particular example\nconsidered in this paper, there are positive responses (concentrations of VOC,\nGamma distribution based models), positive responses possibly containing\nobservations with zero values (lesion area, Compound Poisson distribution based\nmodels) and binomially distributed responses (proportion of apples presenting\ninfection symptoms). We represent patterns of association inferred with the\nMGLMMs using graphical models (a network represented by a graph), which allow\nus to eliminate spurious associations due to a cascade of indirect correlations\nbetween the responses.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 13:47:09 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Pelck", "J. S.", ""], ["Holthusen", "H.", ""], ["Edelenbos", "M.", ""], ["Luca", "A.", ""], ["Labouriau", "R.", ""]]}, {"id": "2107.11323", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith and Philip B. Stark and Aaditya Ramdas", "title": "RiLACS: Risk-Limiting Audits via Confidence Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately determining the outcome of an election is a complex task with many\npotential sources of error, ranging from software glitches in voting machines\nto procedural lapses to outright fraud. Risk-limiting audits (RLA) are\nstatistically principled \"incremental\" hand counts that provide statistical\nassurance that reported outcomes accurately reflect the validly cast votes. We\npresent a suite of tools for conducting RLAs using confidence sequences --\nsequences of confidence sets which uniformly capture an electoral parameter of\ninterest from the start of an audit to the point of an exhaustive recount with\nhigh probability. Adopting the SHANGRLA framework, we design nonnegative\nmartingales which yield computationally and statistically efficient confidence\nsequences and RLAs for a wide variety of election types.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:03:40 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Stark", "Philip B.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2107.11542", "submitter": "Marija Vucelja", "authors": "Hanqing Zhao and Marija Vucelja", "title": "Nonreversible Markov chain Monte Carlo algorithm for efficient\n  generation of Self-Avoiding Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient nonreversible Markov chain Monte Carlo algorithm to\ngenerate self-avoiding walks with a variable endpoint. In two dimensions, the\nnew algorithm slightly outperforms the two-move nonreversible Berretti-Sokal\nalgorithm introduced by H.~Hu, X.~Chen, and Y.~Deng in \\cite{old}, while for\nthree-dimensional walks, it is 3--5 times faster. The new algorithm introduces\nnonreversible Markov chains that obey global balance and allows for three types\nof elementary moves on the existing self-avoiding walk: shorten, extend or\nalter conformation without changing the walk's length.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 06:16:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhao", "Hanqing", ""], ["Vucelja", "Marija", ""]]}, {"id": "2107.11589", "submitter": "Shahram Heydari Dr", "authors": "Shahram Heydari, Garyfallos Konstantinoudis, Abdul Wahid Behsoodi", "title": "Effect of the COVID-19 pandemic on bike-sharing demand and hire time:\n  Evidence from Santander Cycles in London", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has been influencing travel behaviour in many urban\nareas around the world since the beginning of 2020. As a consequence,\nbike-sharing schemes have been affected partly due to the change in travel\ndemand and behaviour as well as a shift from public transit. This study\nestimates the varying effect of the COVID-19 pandemic on the London\nbike-sharing system (Santander Cycles) over the period March-December 2020. We\nemployed a Bayesian second-order random walk time-series model to account for\ntemporal correlation in the data. We compared the observed number of cycle\nhires and hire time with their respective counterfactuals (what would have been\nif the pandemic had not happened) to estimate the magnitude of the change\ncaused by the pandemic. The results indicated that following a reduction in\ncycle hires in March and April 2020, the demand rebounded from May 2020,\nremaining in the expected range of what would have been if the pandemic had not\noccurred. This could indicate the resiliency of Santander Cycles. With respect\nto hire time, an important increase occurred in April, May, and June 2020,\nindicating that bikes were hired for longer trips, perhaps partly due to a\nshift from public transit.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 11:43:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Heydari", "Shahram", ""], ["Konstantinoudis", "Garyfallos", ""], ["Behsoodi", "Abdul Wahid", ""]]}, {"id": "2107.11593", "submitter": "Haoqi Qian", "authors": "Zhengyu Shi, Libo Wu, Haoqi Qian and Yingjie Tian", "title": "Inferring Economic Condition Uncertainty from Electricity Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the uncertainties in economic conditions are of significant\nimportance for both decision makers as well as market players. In this paper,\nwe propose a novel method based on Hidden Markov Model (HMM) to construct the\nEconomic Condition Uncertainty (ECU) index that can be used to infer the\neconomic condition uncertainties. The ECU index is a dimensionless index ranges\nbetween zero and one, this makes it to be comparable among sectors, regions and\nperiods. We use the daily electricity consumption data of nearly 20 thousand\nfirms in Shanghai from 2018 to 2020 to construct the ECU indexes. Results show\nthat all ECU indexes, no matter at sectoral level or regional level,\nsuccessfully captured the negative impacts of COVID-19 on Shanghai's economic\nconditions. Besides, the ECU indexes also presented the heterogeneities in\ndifferent districts as well as in different sectors. This reflects the facts\nthat changes in uncertainties of economic conditions are mainly related to\nregional economic structures and targeted regulation policies faced by sectors.\nThe ECU index can also be easily extended to measure uncertainties of economic\nconditions in different fields which has great potentials in the future.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 12:28:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shi", "Zhengyu", ""], ["Wu", "Libo", ""], ["Qian", "Haoqi", ""], ["Tian", "Yingjie", ""]]}, {"id": "2107.11687", "submitter": "Jixian Wang", "authors": "Jixian Wang", "title": "On matching-adjusted indirect comparison and calibration estimation", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect comparisons have been increasingly used to compare data from\ndifferent sources such as clinical trials and observational data in, e.g., a\ndisease registry. To adjust for population differences between data sources,\nmatching-adjusted indirect comparison (MAIC) has been used in several\napplications including health technology assessment and drug regulatory\nsubmissions. In fact, MAIC can be considered as a special case of a range of\nmethods known as calibration estimation in survey sampling. However, to our\nbest knowledge, this connection has not been examined in detail. This paper\nmakes three contributions: 1. We examined this connection by comparing MAIC and\na few commonly used calibration estimation methods, including the entropy\nbalancing approach, which is equivalent to MAIC. 2. We considered the standard\nerror (SE) estimation of the MAIC estimators and propose a model-independent SE\nestimator and examine its performance by simulation. 3. We conducted a\nsimulation to compare these commonly used approaches to evaluate their\nperformance in indirect comparison scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 20:43:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Jixian", ""]]}, {"id": "2107.11856", "submitter": "Amine Amor", "authors": "Amine Amor (1), Pietro Lio' (1), Vikash Singh (1), Ramon Vi\\~nas\n  Torn\\'e (1), Helena Andres Terre (1)", "title": "Graph Representation Learning on Tissue-Specific Multi-Omics", "comments": "This paper was accepted at the 2021 ICML Workshop on Computational\n  Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combining different modalities of data from human tissues has been critical\nin advancing biomedical research and personalised medical care. In this study,\nwe leverage a graph embedding model (i.e VGAE) to perform link prediction on\ntissue-specific Gene-Gene Interaction (GGI) networks. Through ablation\nexperiments, we prove that the combination of multiple biological modalities\n(i.e multi-omics) leads to powerful embeddings and better link prediction\nperformances. Our evaluation shows that the integration of gene methylation\nprofiles and RNA-sequencing data significantly improves the link prediction\nperformance. Overall, the combination of RNA-sequencing and gene methylation\ndata leads to a link prediction accuracy of 71% on GGI networks. By harnessing\ngraph representation learning on multi-omics data, our work brings novel\ninsights to the current literature on multi-omics integration in\nbioinformatics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:38:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Amor", "Amine", ""], ["Lio'", "Pietro", ""], ["Singh", "Vikash", ""], ["Torn\u00e9", "Ramon Vi\u00f1as", ""], ["Terre", "Helena Andres", ""]]}, {"id": "2107.11971", "submitter": "Tahir Mahmood Dr", "authors": "Inez Maria Zwetsloot, Tahir Mahmood, Funmilola Mary Taiwo and Zezhong\n  Wang", "title": "A Real Time Monitoring Approach for Bivariate Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of changes in the frequency of events is an important task,\nin, for example, disease surveillance, monitoring of high-quality processes,\nreliability monitoring and public health. In this article, we focus on\ndetecting changes in multivariate event data, by monitoring the\ntime-between-events (TBE). Existing multivariate TBE charts are limited in the\nsense that, they only signal after an event occurred for each of the individual\nprocesses. This results in delays (i.e., long time to signal), especially if it\nis of interest to detect a change in one or a few of the processes. We propose\na bivariate TBE (BTBE) chart which is able to signal in real time. We derive\nanalytical expressions for the control limits and average time-to-signal\nperformance, conduct a performance evaluation and compare our chart to an\nexisting method. The findings showed that our method is a realistic approach to\nmonitor bivariate time-between-event data, and has better detection ability\nthan existing methods. A large benefit of our method is that it signals in\nreal-time and that due to the analytical expressions no simulation is needed.\nThe proposed method is implemented on a real-life dataset related to AIDS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:51:51 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zwetsloot", "Inez Maria", ""], ["Mahmood", "Tahir", ""], ["Taiwo", "Funmilola Mary", ""], ["Wang", "Zezhong", ""]]}, {"id": "2107.12222", "submitter": "Shir Aviv-Reuven", "authors": "Shir Aviv-Reuven and Ariel Rosenfeld (Department of Information\n  Sciences, Bar-Ilan University, Israel)", "title": "Journal subject classification: intra- and inter-system discrepancies in\n  Web Of Science and Scopus", "comments": "25 pages, 20 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Journal classification into subject categories is an important aspect in\nscholarly research evaluation as well as in bibliometric analysis. Journal\nclassification systems use a variety of (partially) overlapping and\nnon-exhaustive subject categories which results in many journals being\nclassified into more than a single subject category. As such, discrepancies are\nlikely to be encountered within any given system and between different systems.\nIn this study, we set to examine both types of discrepancies in the two most\nwidely used indexing systems - Web Of Science and Scopus. We use known distance\nmeasures, as well as logical set theory to examine and compare the category\nschemes defined by these systems. Our results demonstrate significant\ndiscrepancies within each system where a higher number of classified categories\ncorrelates with increased range and variance of rankings within them, and where\nredundant categories are found. Our results also show significant discrepancies\nbetween the two system. Specifically, very few categories in one system are\n\"similar\" to categories in the second system, where \"similarity\" is measured by\nsubset & interesting categories and minimally covering categories. Taken\njointly, our findings suggest that both types of discrepancies are systematic\nand cannot be easily disregarded when relying on these subject classification\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:02:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aviv-Reuven", "Shir", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"], ["Rosenfeld", "Ariel", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"]]}, {"id": "2107.12539", "submitter": "Takahiro Yoshida", "authors": "Takahiro Yoshida and Hajime Seya", "title": "Spatial prediction of apartment rent using regression-based and machine\n  learning-based approaches with a large dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing a large dataset (at most, the order of n = 10^6), this study\nattempts enhance the literature on the comparison between regression and\nmachine learning (ML)-based rent price prediction models by adding new\nempirical evidence and considering the spatial dependence of the observations.\nThe regression-based approach incorporates the nearest neighbor Gaussian\nprocesses (NNGP) model, enabling the application of kriging to large datasets.\nIn contrast, the ML-based approach utilizes typical models: extreme gradient\nboosting (XGBoost), random forest (RF), and deep neural network (DNN). The\nout-of-sample prediction accuracy of these models was compared using Japanese\napartment rent data, with a varying order of sample sizes (i.e., n = 10^4,\n10^5, 10^6). The results showed that, as the sample size increased, XGBoost and\nRF outperformed NNGP with higher out-of-sample prediction accuracy. XGBoost\nachieved the highest prediction accuracy for all sample sizes and error\nmeasures in both logarithmic and real scales and for all price bands (when n =\n10^5 and 10^6). A comparison of several methods to account for the spatial\ndependence in RF showed that simply adding spatial coordinates to the\nexplanatory variables may be sufficient.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:15:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Yoshida", "Takahiro", ""], ["Seya", "Hajime", ""]]}, {"id": "2107.12554", "submitter": "Aldo Taranto", "authors": "Aldo Taranto, Ron Addie, Shahjahan Khan", "title": "Bi-Directional Grid Constrained Stochastic Processes' Link to Multi-Skew\n  Brownian Motion", "comments": "Manuscript accepted for publication in the Journal of Applied\n  Probability & Statistics and will appear in issue 1 of volume 17 to be\n  published in April 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-Directional Grid Constrained (BGC) stochastic processes (BGCSPs) constrain\nthe random movement toward the origin steadily more and more, the further they\ndeviate from the origin, rather than all at once imposing reflective barriers,\nas does the well-established theory of It^o diffusions with such reflective\nbarriers. We identify that BGCSPs are a variant rather than a special case of\nthe multi-skew Brownian motion (M-SBM). This is because they have their own\ncomplexities, such as the barriers being hidden (not known in advance) and not\nnecessarily constant over time. We provide an M-SBM theoretical framework and\nalso a simulation framework to elaborate deeper properties of BGCSPs. The\nsimulation framework is then applied by generating numerous simulations of the\nconstrained paths and the results are analysed. BGCSPs have applications in\nfinance and indeed many other fields requiring graduated constraining, from\nboth above and below the initial position.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:12:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Taranto", "Aldo", ""], ["Addie", "Ron", ""], ["Khan", "Shahjahan", ""]]}, {"id": "2107.12771", "submitter": "Yiwen Chen", "authors": "Yiwen Chen", "title": "Theoretical Study and Comparison of SPSA and RDSA Algorithms", "comments": "33 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic approximation (SA) algorithms are widely used in system\noptimization problems when only noisy measurements of the system are available.\nThis paper studies two types of SA algorithms in a multivariate\nKiefer-Wolfowitz setting: random-direction SA (RDSA) and\nsimultaneous-perturbation SA (SPSA), and then describes the bias term,\nconvergence, and asymptotic normality of RDSA algorithms. The gradient\nestimations in RDSA and SPSA have different forms and, consequently, use\ndifferent types of random perturbations. This paper looks at various valid\ndistributions for perturbations in RDSA and SPSA and then compares the two\nalgorithms using mean-square errors computed from asymptotic distribution. From\nboth a theoretical and numerical point of view, we find that SPSA generally\noutperforms RDSA.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:50:17 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Chen", "Yiwen", ""]]}, {"id": "2107.12857", "submitter": "Sudeep Bapat", "authors": "Sudeep R. Bapat", "title": "Sequentially estimating the dynamic contact angle of sessile saliva\n  droplets in view of SARS-CoV-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Estimating the contact angle of a virus infected saliva droplet is seen to be\nan important area of research as it presents an idea about the drying time of\nthe respective droplet and in turn of the growth of the underlying pandemic. In\nthis paper we extend the data presented by Balusamy, Banerjee and Sahu\n[Lifetime of sessile saliva droplets in the context of SARS-CoV-2, Int. J. Heat\nMass Transf. 123, 105178 (2021)], where the contact angles are fitted using a\nnewly proposed half-circular wrapped-exponential model, and a sequential\nconfidence interval estimation approach is established which largely reduces\nboth time and cost with regards to data collection.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:45:20 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Bapat", "Sudeep R.", ""]]}, {"id": "2107.12863", "submitter": "Marta Spreafico", "authors": "Marta Spreafico, Francesca Ieva and Marta Fiocco", "title": "Longitudinal Latent Overall Toxicity (LOTox) profiles in osteosarcoma: a\n  new taxonomy based on latent Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer trials, the analysis of longitudinal toxicity data is a difficult\ntask due to the presence of multiple adverse events with different extents of\ntoxic burden. Models able to summarise and quantify the overall toxic risk and\nits evolution during cancer therapy, which deals with both the longitudinal and\nthe categorical aspects of toxicity levels progression, are necessary, but\nstill not well developed. In this work, a novel approach based on Latent Markov\n(LM) models for longitudinal toxicity data is proposed. The latent status of\ninterest is the Latent Overall Toxicity (LOTox) condition of each patient,\nwhich affects the distribution of the observed toxic levels over treatment. By\nassuming the existence of a LM chain for LOTox dynamics, the new approach aims\nat identifying different latent states of overall toxicity burden (LOTox\nstates). This approach investigates how patients move between latent states\nduring chemotherapy treatment allowing for the reconstruction of personalized\nlongitudinal LOTox profiles. This methodology has never been applied to\nosteosarcoma treatment and provides new insights for medical decisions in\nchildhood cancer therapy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:58:52 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 09:42:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Spreafico", "Marta", ""], ["Ieva", "Francesca", ""], ["Fiocco", "Marta", ""]]}, {"id": "2107.12952", "submitter": "Shahram Heydari Dr", "authors": "Niloofar Shoari, Shahram Heydari, Marta Blangiardo", "title": "School neighbourhood and compliance with WHO-recommended annual NO2\n  guideline: a case study of Greater London", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite several national and local policies towards cleaner air in England,\nmany schools in London breach the WHO-recommended concentrations of air\npollutants such as NO2 and PM2.5. This is while, previous studies highlight\nsignificant adverse health effects of air pollutants on children's health. In\nthis paper we adopted a Bayesian spatial hierarchical model to investigate\nfactors that affect the odds of schools exceeding the WHO-recommended\nconcentration of NO2 (i.e., 40 ug/m3 annual mean) in Greater London (UK). We\nconsidered a host of variables including schools' characteristics as well as\ntheir neighbourhoods' attributes from household, socioeconomic,\ntransport-related, land use, built and natural environment characteristics\nperspectives. The results indicated that transport-related factors including\nthe number of traffic lights and bus stops in the immediate vicinity of\nschools, and borough-level bus fuel consumption are determinant factors that\nincrease the likelihood of non-compliance with the WHO guideline. In contrast,\ndistance from roads, river transport, and underground stations, vehicle speed\n(an indicator of traffic congestion), the proportion of borough-level green\nspace, and the area of green space at schools reduce the likelihood of\nexceeding the WHO recommended concentration of NO2. As a sensitivity analysis,\nwe repeated our analysis under a hypothetical scenario in which the recommended\nconcentration of NO2 is 35 ug/m3, instead of 40 ug/m3. Our results underscore\nthe importance of adopting clean fuel technologies on buses, installing green\nbarriers, and reducing motorised traffic around schools in reducing exposure to\nNO2 concentrations in proximity to schools. This study would be useful for\nlocal authority decision making with the aim of improving air quality for\nschool-aged children in urban settings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:14:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Shoari", "Niloofar", ""], ["Heydari", "Shahram", ""], ["Blangiardo", "Marta", ""]]}, {"id": "2107.13146", "submitter": "Tomomi Matsui", "authors": "Sachika Kurokawa and Tomomi Matsui (Graduate School of Engineering,\n  Tokyo Institute of Technology)", "title": "Dynamic Programming and Linear Programming for Odds Problem", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the odds problem, proposed by Bruss in 2000, and its\nvariants. A recurrence relation called a dynamic programming (DP) equation is\nused to find an optimal stopping policy of the odds problem and its variants.\nIn 2013, Buchbinder, Jain, and Singh proposed a linear programming (LP)\nformulation for finding an optimal stopping policy of the classical secretary\nproblem, which is a special case of the odds problem. The proposed linear\nprogramming problem, which maximizes the probability of a win, differs from the\nDP equations known for long time periods. This paper shows that an ordinary DP\nequation is a modification of the dual problem of linear programming including\nthe LP formulation proposed by Buchbinder, Jain, and Singh.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 03:14:37 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kurokawa", "Sachika", "", "Graduate School of Engineering,\n  Tokyo Institute of Technology"], ["Matsui", "Tomomi", "", "Graduate School of Engineering,\n  Tokyo Institute of Technology"]]}, {"id": "2107.13256", "submitter": "Henrik Bette", "authors": "Henrik M. Bette, Edgar Jungblut, Thomas Guhr", "title": "Non-stationarity in correlation matrices for wind turbine SCADA-data and\n  implications for failure detection", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.app-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern utility-scale wind turbines are equipped with a Supervisory Control\nAnd Data Acquisition (SCADA) system gathering vast amounts of operational data\nthat can be used for failure analysis and prediction to improve operation and\nmaintenance of turbines. We analyse high freqeuency SCADA-data from an offshore\nwindpark and evaluate Pearson correlation matrices for a variety of observables\nwith a moving time window. This renders possible an asessment of\nnon-stationarity in mutual dependcies of different types of data. Drawing from\nour experience in other complex systems, such as financial markets and traffic,\nwe show this by employing a hierarchichal $k$-means clustering algorithm on the\ncorrelation matrices. The different clusters exhibit distinct typical\ncorrelation structures to which we refer as states. Looking first at only one\nand later at multiple turbines, the main dependence of these states is shown to\nbe on wind speed. In accordance, we identify them as operational states arising\nfrom different settings in the turbine control system based on the available\nwind speed. We model the boundary wind speeds seperating the states based on\nthe clustering solution. This allows the usage of our methodology for failure\nanalysis or prediction by sorting new data based on wind speed and comparing it\nto the respective operational state, thereby taking the non-stationarity into\naccount.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 10:32:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bette", "Henrik M.", ""], ["Jungblut", "Edgar", ""], ["Guhr", "Thomas", ""]]}, {"id": "2107.13394", "submitter": "Syed Hasib Akhter Faruqui", "authors": "Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Susan P\n  Fisher-Hoch, and Joseph B Mccormic", "title": "Nonlinear State Space Modeling and Control of the Impact of Patients'\n  Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic\n  Conditions", "comments": "Submitted to IEEE Access for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:01:46 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Faruqui", "Syed Hasib Akhter", ""], ["Alaeddini", "Adel", ""], ["Wang", "Jing", ""], ["Fisher-Hoch", "Susan P", ""], ["Mccormic", "Joseph B", ""]]}, {"id": "2107.13462", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Rob J Hyndman, Christoph Bergmeir", "title": "MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with\n  Multiple Seasonal Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The decomposition of time series into components is an important task that\nhelps to understand time series and can enable better forecasting. Nowadays,\nwith high sampling rates leading to high-frequency data (such as daily, hourly,\nor minutely data), many real-world datasets contain time series data that can\nexhibit multiple seasonal patterns. Although several methods have been proposed\nto decompose time series better under these circumstances, they are often\ncomputationally inefficient or inaccurate. In this study, we propose Multiple\nSeasonal-Trend decomposition using Loess (MSTL), an extension to the\ntraditional Seasonal-Trend decomposition using Loess (STL) procedure, allowing\nthe decomposition of time series with multiple seasonal patterns. In our\nevaluation on synthetic and a perturbed real-world time series dataset,\ncompared to other decomposition benchmarks, MSTL demonstrates competitive\nresults with lower computational cost. The implementation of MSTL is available\nin the R package forecast.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:14:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bandara", "Kasun", ""], ["Hyndman", "Rob J", ""], ["Bergmeir", "Christoph", ""]]}, {"id": "2107.13535", "submitter": "Americo Cunha Jr", "authors": "Mario Germ\\'an Sandoval, Americo Cunha Jr, Rubens Sampaio", "title": "Identification of parameters in the torsional dynamics of a drilling\n  process through Bayesian statistics", "comments": null, "journal-ref": "Mec\\'anica Computacional, vol. 32, pp. 763-773, 2013", "doi": null, "report-no": null, "categories": "stat.ME cs.CE physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the estimation of the parameters of an experimental setup,\nwhich is modeled as a system with three degrees of freedom, composed by a\nshaft, two rotors, and a DC motor, that emulates a drilling process. A Bayesian\ntechnique is used in the estimation process, to take into account the\nuncertainties and variabilities intrinsic to the measurement taken, which are\nmodeled as a noise of Gaussian nature. With this procedure it is expected to\ncheck the reliability of the nominal values of the physical parameters of the\ntest rig. An estimation process assuming that nine parameters of the\nexperimental apparatus are unknown is conducted, and the results show that for\nsome quantities the relative deviation with respect to the nominal values is\nvery high. This deviation evidentiates a strong deficiency in the mathematical\nmodel used to describe the dynamic behavior of the experimental apparatus.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:14:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sandoval", "Mario Germ\u00e1n", ""], ["Cunha", "Americo", "Jr"], ["Sampaio", "Rubens", ""]]}, {"id": "2107.13599", "submitter": "Sina Bagheri Nezhad", "authors": "Sina Bagheri Nezhad, Nasser Mozayani, Elham Abdi, Setareh Rostami", "title": "Smoking prevalence in Covid-19 patients", "comments": "8 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the prevalence rate of smoking in Covid-19 patients and\nexamine whether there is a difference in the distribution of smokers between\nthe two statistical populations of critically ill patients with Covid-19 and\nthe entire Iranian population or not. To do this, we first prepared a sample of\n1040 Covid-19 patients admitted to hospitals in Tehran, Rasht, and Bojnord.\nThen, through the non-parametric statistical runs test, we show that the sample\nwas randomly selected, and it is possible to generalize the result of tests on\nthe sample to the community of hospitalized Covid-19 patients. In continuation,\nwe examined the hypothesis that the smoking prevalence among Covid-19 patients\nadmitted to hospitals is equal to the prevalence rate of smoking in Iranian\nsociety. For this purpose, we used the non-parametric chi-square test, and it\nwas observed that this hypothesis is rejected. The data show a significant\ndifference in the prevalence of smoking between critically ill Covid-19\npatients and the whole of Iranian society. Additionally, we examined this\nhypothesis in some subpopulations, and the results were the same.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 19:05:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Nezhad", "Sina Bagheri", ""], ["Mozayani", "Nasser", ""], ["Abdi", "Elham", ""], ["Rostami", "Setareh", ""]]}, {"id": "2107.13683", "submitter": "Patrick Haas", "authors": "Patrick A. Haas", "title": "Mental Age Compatibility: Quantification through the Convolution of\n  Probability Distributions", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We build on the empirical finding that a human being's mental age is normally\ndistributed around the chronological age. This opposes the frequent societal\nassumption \"mental = chronological\" which is known to be false in general but\nentertained for simplicity due to lack of methodology; hence disregarding that,\nf.e., people of different chronological ages can be much closer in their mental\nages. As a quantitative approach on a scientific basis, we set up a general\nformula for the probability that two individuals of given ages are mentally\nwithin a certain range of years and investigate its implications i.a. by\ncritically analyzing popular assumptions on age and computing statistical\nexpectations within populations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 00:43:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Haas", "Patrick A.", ""]]}, {"id": "2107.13721", "submitter": "Bayan Saparbayeva", "authors": "Zhengwu Zhang and Bayan Saparbayeva", "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainfold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data comes from many aspects, including infinite dimensionality and\nnonlinearity, as well as time domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\S^2$, which is\ninvariant to random time warping or re-parameterization of the function.\nUtilizing the nice geometry of $\\S^2$, we develop a set of efficient and\naccurate tools for temporal alignment of functions, geodesic and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata, and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in mainfold-valued FDA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 03:11:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Saparbayeva", "Bayan", ""]]}, {"id": "2107.13763", "submitter": "Yunyi Shen", "authors": "Yunyi Shen and Claudia Solis-Lemus", "title": "CARlasso: An R package for the estimation of sparse microbial networks\n  with predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microbiome data analyses require statistical tools that can simultaneously\ndecode microbes' reactions to the environment and interactions among microbes.\nWe introduce CARlasso, the first user-friendly open-source and publicly\navailable R package to fit a chain graph model for the inference of sparse\nmicrobial networks that represent both interactions among nodes and effects of\na set of predictors. Unlike in standard regression approaches, the edges\nrepresent the correct conditional structure among responses and predictors that\nallows the incorporation of prior knowledge from controlled experiments. In\naddition, CARlasso 1) enforces sparsity in the network via LASSO; 2) allows for\nan adaptive extension to include different shrinkage to different edges; 3) is\ncomputationally inexpensive through an efficient Gibbs sampling algorithm so it\ncan equally handle small and big data; 4) allows for continuous, binary,\ncounting and compositional responses via proper hierarchical structure, and 5)\nhas a similar syntax to lm for ease of use. The package also supports Bayesian\ngraphical LASSO and several of its hierarchical models as well as lower level\none-step sampling functions of the CAR-LASSO model for users to extend.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:17:35 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shen", "Yunyi", ""], ["Solis-Lemus", "Claudia", ""]]}, {"id": "2107.13856", "submitter": "David Howey", "authors": "Antti Aitio and David A. Howey", "title": "Predicting battery end of life from solar off-grid system field data\n  using machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hundreds of millions of people lack access to electricity. Decentralised\nsolar-battery systems are key for addressing this whilst avoiding carbon\nemissions and air pollution, but are hindered by relatively high costs and\nrural locations that inhibit timely preventative maintenance. Accurate\ndiagnosis of battery health and prediction of end of life from operational data\nimproves user experience and reduces costs. But lack of controlled validation\ntests and variable data quality mean existing lab-based techniques fail to\nwork. We apply a scaleable probabilistic machine learning approach to diagnose\nhealth in 1027 solar-connected lead-acid batteries, each running for 400-760\ndays, totalling 620 million data rows. We demonstrate 73% accurate prediction\nof end of life, eight weeks in advance, rising to 82% at the point of failure.\nThis work highlights the opportunity to estimate health from existing\nmeasurements using `big data' techniques, without additional equipment,\nextending lifetime and improving performance in real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 09:37:48 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Aitio", "Antti", ""], ["Howey", "David A.", ""]]}, {"id": "2107.13956", "submitter": "Li Su", "authors": "Li Su, Yafeng Cheng, Dora I.A. Pereira, Jonathan J. Powell", "title": "Modelling disease progression with multi-level electronic health records\n  data and informative observation times: an application to treating iron\n  deficiency anaemia in primary care of the UK", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modelling disease progression of iron deficiency anaemia (IDA) following oral\niron supplement prescriptions is a prerequisite for evaluating the\ncost-effectiveness of oral iron supplements. Electronic health records (EHRs)\nfrom the Clinical Practice Research Datalink (CPRD) provide rich longitudinal\ndata on IDA disease progression in patients registered with 663 General\nPractitioner (GP) practices in the UK, but they also create challenges in\nstatistical analyses. First, the CPRD data are clustered at multi-levels (i.e.,\nGP practices and patients), but their large volume makes it computationally\ndifficult to implement estimation of standard random effects models for\nmulti-level data. Second, observation times in the CPRD data are irregular and\ncould be informative about the disease progression. For example, shorter/longer\ngap times between GP visits could be associated with deteriorating/improving\nIDA. Existing methods to address informative observation times are mostly based\non complex joint models, which adds more computational burden. To tackle these\nchallenges, we develop a computationally efficient approach to modelling\ndisease progression with EHRs data while accounting for variability at\nmulti-level clusters and informative observation times. We apply the proposed\nmethod to the CPRD data to investigate IDA improvement and treatment\nintolerance following oral iron prescriptions in primary care of the UK.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:34:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Su", "Li", ""], ["Cheng", "Yafeng", ""], ["Pereira", "Dora I. A.", ""], ["Powell", "Jonathan J.", ""]]}, {"id": "2107.14026", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Fearghal Kearney", "title": "Dynamic functional time-series forecasts of foreign exchange implied\n  volatility surfaces", "comments": "52 pages, 5 figures, to appear at the International Journal of\n  Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents static and dynamic versions of univariate, multivariate,\nand multilevel functional time-series methods to forecast implied volatility\nsurfaces in foreign exchange markets. We find that dynamic functional principal\ncomponent analysis generally improves out-of-sample forecast accuracy. More\nspecifically, the dynamic univariate functional time-series method shows the\ngreatest improvement. Our models lead to multiple instances of statistically\nsignificant improvements in forecast accuracy for daily EUR-USD, EUR-GBP, and\nEUR-JPY implied volatility surfaces across various maturities, when benchmarked\nagainst established methods. A stylised trading strategy is also employed to\ndemonstrate the potential economic benefits of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:56:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shang", "Han Lin", ""], ["Kearney", "Fearghal", ""]]}, {"id": "2107.14045", "submitter": "Americo Cunha Jr", "authors": "Vinicius Gon\\c{c}alves Lopes, Jo\\~ao Victor L. L. Peterson, Americo\n  Cunha Jr", "title": "The nonlinear dynamics of a bistable energy harvesting system with\n  colored noise disturbances", "comments": null, "journal-ref": "Journal of Computational Interdisciplinary Sciences, vol. 10, pp.\n  125, 2019", "doi": "10.6062/jcis.2019.10.03.0166", "report-no": null, "categories": "cond-mat.stat-mech cs.CE physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the nonlinear stochastic dynamics of a piezoelectric\nenergy harvesting system subjected to a harmonic external excitation disturbed\nby Gaussian colored noise. A parametric analysis is conducted, where the\neffects of the standard deviation and the correlation time of colored noise on\nthe system response are investigated. The numerical results suggest a strong\ninfluence of noise on the system response for higher values of correlation time\nand standard deviation, and a low (noise level independent) influence for low\nvalues of correlation time.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:00:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Lopes", "Vinicius Gon\u00e7alves", ""], ["Peterson", "Jo\u00e3o Victor L. L.", ""], ["Cunha", "Americo", "Jr"]]}, {"id": "2107.14166", "submitter": "Andreas Philippou N", "authors": "Andreas N. Philippou", "title": "Why Do Polls Fail? The Case of Four US Presidential Elections, Brexit,\n  and Two India General Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most widely known and important applications of probability and\nstatistics is scientific polling to forecast election results. In 1936, Gallup\npredicted correctly the victory of Roosevelt over Landon in the US presidential\nelection, using scientific sampling of 3,000 persons, whereas the Literary\nDigest failed using 2.4 million answers to 10 million mailed questionnaires to\nautomobile and telephone owners. Since then, polls have grown to be a\nflourishing and very influential and important industry, spreading around the\nworld. Polls have mostly been accurate in the US presidential elections, with a\nfew exceptions. Their two most notable failures were their wrong predictions of\nthe US 1948 and 2016 presidential elections. Most polls failed too in the 2016\nUK Referendum, in the 2014 and 2019 India Lok Sabha elections, and in the US\n2020 presidential election, even though in the latter three they did predict\nthe winner. We discuss these polls in the present paper. The failure in 1948\nwas due to non-random sampling. In 2016 and 2020 it was mainly due to the\nproblem of non-response and possible biases of the pollsters. In 2014 and 2019\nit was due to non-response and political biases of the polling agencies and\nnews outlets that produced the polls.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:43:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Philippou", "Andreas N.", ""]]}, {"id": "2107.14203", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen, Tracy Cai, Matei Zaharia, James Zou", "title": "Did the Model Change? Efficiently Assessing Machine Learning API Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) prediction APIs are increasingly widely used. An ML API\ncan change over time due to model updates or retraining. This presents a key\nchallenge in the usage of the API because it is often not clear to the user if\nand how the ML model has changed. Model shifts can affect downstream\napplication performance and also create oversight issues (e.g. if consistency\nis desired). In this paper, we initiate a systematic investigation of ML API\nshifts. We first quantify the performance shifts from 2020 to 2021 of popular\nML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We\nidentified significant model shifts in 12 out of 36 cases we investigated.\nInterestingly, we found several datasets where the API's predictions became\nsignificantly worse over time. This motivated us to formulate the API shift\nassessment problem at a more fine-grained level as estimating how the API\nmodel's confusion matrix changes over time when the data distribution is\nconstant. Monitoring confusion matrix shifts using standard random sampling can\nrequire a large number of samples, which is expensive as each API call costs a\nfee. We propose a principled adaptive sampling algorithm, MASA, to efficiently\nestimate confusion matrix shifts. MASA can accurately estimate the confusion\nmatrix shifts in commercial ML APIs using up to 90% fewer samples compared to\nrandom sampling. This work establishes ML API shifts as an important problem to\nstudy and provides a cost-effective approach to monitor such shifts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:41:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Lingjiao", ""], ["Cai", "Tracy", ""], ["Zaharia", "Matei", ""], ["Zou", "James", ""]]}]