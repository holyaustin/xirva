[{"id": "1004.0209", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Robert Tibshirani", "title": "Inference with Transposable Data: Modeling the Effects of Row and Column\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of large-scale inference on the row or column\nvariables of data in the form of a matrix. Often this data is transposable,\nmeaning that both the row variables and column variables are of potential\ninterest. An example of this scenario is detecting significant genes in\nmicroarrays when the samples or arrays may be dependent due to underlying\nrelationships. We study the effect of both row and column correlations on\ncommonly used test-statistics, null distributions, and multiple testing\nprocedures, by explicitly modeling the covariances with the matrix-variate\nnormal distribution. Using this model, we give both theoretical and simulation\nresults revealing the problems associated with using standard statistical\nmethodology on transposable data. We solve these problems by estimating the row\nand column covariances simultaneously, with transposable regularized covariance\nmodels, and de-correlating or sphering the data as a pre-processing step. Under\nreasonable assumptions, our method gives test statistics that follow the scaled\ntheoretical null distribution and are approximately independent. Simulations\nbased on various models with structured and observed covariances from real\nmicroarray data reveal that our method offers substantial improvements in two\nareas: 1) increased statistical power and 2) correct estimation of false\ndiscovery rates.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 19:19:31 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1004.0356", "submitter": "Sandra Hala Dandach", "authors": "Sandra H. Dandach, Ruggero Carli and Francesco Bullo", "title": "Accuracy and Decision Time for Sequential Decision Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies prototypical strategies to sequentially aggregate\nindependent decisions. We consider a collection of agents, each performing\nbinary hypothesis testing and each obtaining a decision over time. We assume\nthe agents are identical and receive independent information. Individual\ndecisions are sequentially aggregated via a threshold-based rule. In other\nwords, a collective decision is taken as soon as a specified number of agents\nreport a concordant decision (simultaneous discordant decisions and no-decision\noutcomes are also handled).\n  We obtain the following results. First, we characterize the probabilities of\ncorrect and wrong decisions as a function of time, group size and decision\nthreshold. The computational requirements of our approach are linear in the\ngroup size. Second, we consider the so-called fastest and majority rules,\ncorresponding to specific decision thresholds. For these rules, we provide a\ncomprehensive scalability analysis of both accuracy and decision time. In the\nlimit of large group sizes, we show that the decision time for the fastest rule\nconverges to the earliest possible individual time, and that the decision\naccuracy for the majority rule shows an exponential improvement over the\nindividual accuracy. Additionally, via a theoretical and numerical analysis, we\ncharacterize various speed/accuracy tradeoffs. Finally, we relate our results\nto some recent observations reported in the cognitive information processing\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2010 16:25:20 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2010 16:24:15 GMT"}, {"version": "v3", "created": "Fri, 27 Aug 2010 15:57:38 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Dandach", "Sandra H.", ""], ["Carli", "Ruggero", ""], ["Bullo", "Francesco", ""]]}, {"id": "1004.0432", "submitter": "Francois Bavaud", "authors": "Fran\\c{c}ois Bavaud", "title": "Stereotype bias: a simple formal model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the relative inertia of a statistical group with respect to the\ninertia of the overall sample defines an unique point, the in-focus, which\nconstitutes a context-dependent measure of typical group tendency, biased in\ncomparison to the group centroid. Maximizing the relative inertia yields an\nunique out-focal point, polarized in the reverse direction. This mechanism\nevokes the relative variability reduction of the outgroup reported in Social\nPsychology, and the stereotypic-like behavior of the in-focus, whose bias\nvanishes if the outgroup is constituted of a single individual. In this\npicture, the out-focus plays the role of an anti-stereotypical position,\nidentical to the in-focus of the complementary group.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2010 10:12:42 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Bavaud", "Fran\u00e7ois", ""]]}, {"id": "1004.0637", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano and Takayuki Mizuno", "title": "Sales Distribution of Consumer Electronics", "comments": null, "journal-ref": "Physica A 390, 309-318, 2011", "doi": "10.1016/j.physa.2010.09.033", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the uniform most powerful unbiased test, we observed the sales\ndistribution of consumer electronics in Japan on a daily basis and report that\nit follows both a lognormal distribution and a power-law distribution and\ndepends on the state of the market. We show that these switches occur quite\noften. The underlying sales dynamics found between both periods nicely matched\na multiplicative process. However, even though the multiplicative term in the\nprocess displays a size-dependent relationship when a steady lognormal\ndistribution holds, it shows a size-independent relationship when the power-law\ndistribution holds. This difference in the underlying dynamics is responsible\nfor the difference in the two observed distributions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 14:14:53 GMT"}, {"version": "v2", "created": "Sat, 4 Sep 2010 13:15:51 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Hisano", "Ryohei", ""], ["Mizuno", "Takayuki", ""]]}, {"id": "1004.0678", "submitter": "Christopher P. Saunders", "authors": "Christopher P. Saunders, Linda J. Davis, Andrea C. Lamas, John J.\n  Miller, Donald T. Gantz", "title": "Construction and evaluation of classifiers for forensic document\n  analysis", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS379 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 381-399", "doi": "10.1214/10-AOAS379", "report-no": "IMS-AOAS-AOAS379", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we illustrate a statistical approach to questioned document\nexamination. Specifically, we consider the construction of three classifiers\nthat predict the writer of a sample document based on categorical data. To\nevaluate these classifiers, we use a data set with a large number of writers\nand a small number of writing samples per writer. Since the resulting\nclassifiers were found to have near perfect accuracy using leave-one-out\ncross-validation, we propose a novel Bayesian-based cross-validation method for\nevaluating the classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 18:39:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 05:28:50 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Saunders", "Christopher P.", ""], ["Davis", "Linda J.", ""], ["Lamas", "Andrea C.", ""], ["Miller", "John J.", ""], ["Gantz", "Donald T.", ""]]}, {"id": "1004.0858", "submitter": "Benjamin Golub", "authors": "Benjamin Golub, Yair Livne", "title": "Strategic Random Networks: Why Social Networking Technology Matters", "comments": "34 pages, 3 figures; v2 and v3 correct typographical errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops strategic foundations for an important statistical model\nof random networks with heterogeneous expected degrees. Based on this, we show\nhow social networking services that subtly alter the costs and indirect\nbenefits of relationships can cause large changes in behavior and welfare. In\nthe model, agents who value friends and friends of friends choose how much to\nsocialize, which increases the probabilities of links but is costly. There is a\nsharp transition from fragmented, sparse equilibrium networks to connected,\ndense ones when the value of friends of friends crosses a cost-dependent\nthreshold. This transition mitigates an extreme inefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 14:31:55 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2010 00:24:03 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2010 00:10:15 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Golub", "Benjamin", ""], ["Livne", "Yair", ""]]}, {"id": "1004.1147", "submitter": "Veronica J Berrocal", "authors": "Veronica J. Berrocal, Alan E. Gelfand and David M. Holland", "title": "A bivariate space-time downscaler under space and time misalignment", "comments": "26 pages; 5 tables; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ozone and particulate matter PM2.5 are co-pollutants that have long been\nassociated with increased public health risks. Information on concentration\nlevels for both pollutants come from two sources: monitoring sites and output\nfrom complex numerical models that produce concentration surfaces over large\nspatial regions. In this paper, we offer a fully-model based approach for\nfusing these two sources of information for the pair of co-pollutants which is\ncomputationally feasible over large spatial regions and long periods of time.\nDue to the association between concentration levels of the two environmental\ncontaminants, it is expected that information regarding one will help to\nimprove prediction of the other. Misalignment is an obvious issue since the\nmonitoring networks for the two contaminants only partly intersect and because\nthe collection rate for PM2.5 is typically less frequent than that for ozone.\nExtending previous work in Berrocal et al. (2010), we introduce a bivariate\ndownscaler that provides a flexible class of bivariate space-time assimilation\nmodels. We discuss computational issues for model fitting and analyze a dataset\nfor ozone and PM2.5 for the ozone season during year 2002. We show a modest\nimprovement in predictive performance, not surprising in a setting where we can\nanticipate only a small gain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 18:22:33 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Berrocal", "Veronica J.", ""], ["Gelfand", "Alan E.", ""], ["Holland", "David M.", ""]]}, {"id": "1004.1234", "submitter": "Max Little", "authors": "Max A. Little, Bradley C. Steel, Fan Bai, Yoshiyuki Sowa, Thomas\n  Bilyard, David M. Mueller, Richard M. Berry, Nick S. Jones", "title": "Steps and bumps: precision extraction of discrete states of molecular\n  machines using physically-based, high-throughput time series analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.bpj.2011.05.070", "report-no": null, "categories": "q-bio.QM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report new statistical time-series analysis tools providing significant\nimprovements in the rapid, precision extraction of discrete state dynamics from\nlarge databases of experimental observations of molecular machines. By building\nphysical knowledge and statistical innovations into analysis tools, we\ndemonstrate new techniques for recovering discrete state transitions buried in\nhighly correlated molecular noise. We demonstrate the effectiveness of our\napproach on simulated and real examples of step-like rotation of the bacterial\nflagellar motor and the F1-ATPase enzyme. We show that our method can clearly\nidentify molecular steps, symmetries and cascaded processes that are too weak\nfor existing algorithms to detect, and can do so much faster than existing\nalgorithms. Our techniques represent a major advance in the drive towards\nautomated, precision, highthroughput studies of molecular machine dynamics.\nModular, open-source software that implements these techniques is provided at\nhttp://www.eng.ox.ac.uk/samp/members/max/software/\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 03:28:15 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Little", "Max A.", ""], ["Steel", "Bradley C.", ""], ["Bai", "Fan", ""], ["Sowa", "Yoshiyuki", ""], ["Bilyard", "Thomas", ""], ["Mueller", "David M.", ""], ["Berry", "Richard M.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1004.2006", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "Machine learning approach to inverse problem and unfolding procedure", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A procedure for unfolding the true distribution from experimental data is\npresented. Machine learning methods are applied for simultaneous identification\nof an apparatus function and solving of an inverse problem. A priori\ninformation about the true distribution from theory or previous experiments is\nused for Monte-Carlo simulation of the training sample. The training sample can\nbe used to calculate a transformation from the true distribution to the\nmeasured one. This transformation provides a robust solution for an unfolding\nproblem with minimal biases and statistical errors for the set of distributions\nused to create the training sample. The dimensionality of the solved problem\ncan be arbitrary. A numerical example is presented to illustrate and validate\nthe procedure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 16:34:37 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 18:01:58 GMT"}, {"version": "v3", "created": "Wed, 25 May 2011 13:32:49 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1004.2268", "submitter": "Luk Arnaut", "authors": "Luk R. Arnaut", "title": "Angular Spectral Plane-Wave Expansion of Nonstationary Random Fields in\n  Stochastic Mode-Stirred Reverberation Processes", "comments": "22 pages, 3 figures, accepted for publication in Phys. Rev. E", "journal-ref": null, "doi": "10.1103/PhysRevE.81.041133", "report-no": null, "categories": "physics.optics math-ph math.MP physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an integral expression for the plane-wave expansion of the\ntime-varying (nonstationary) random field inside a mode-stirred reverberation\nchamber. It is shown that this expansion is a so-called oscillatory process,\nwhose kernel can be expressed explicitly in closed form. The effect of\nnonstationarity is a modulation of the spectral field on a time scale that is a\nfunction of the cavity relaxation time. It is also shown how the contribution\nby a nonzero initial value of the field can be incorporated into the expansion.\nThe results are extended to a special class of second-order processes, relevant\nto the perception of a mode-stirred reverberation field by a device under test\nwith a first-order (relaxation-type) frequency response.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 21:01:47 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Arnaut", "Luk R.", ""]]}, {"id": "1004.2287", "submitter": "Vivian Viallon", "authors": "Vivian Viallon, Onureena Banerjee, Gregoire Rey, Eric Jougla, Joel\n  Coste", "title": "An empirical comparative study of approximate methods for binary\n  graphical models; application to the search of associations among causes of\n  death in French death certificates", "comments": "29 pages, 4 figures.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking for associations among multiple variables is a topical issue in\nstatistics due to the increasing amount of data encountered in biology,\nmedicine and many other domains involving statistical applications. Graphical\nmodels have recently gained popularity for this purpose in the statistical\nliterature. Following the ideas of the LASSO procedure designed for the linear\nregression framework, recent developments dealing with graphical model\nselection have been based on $\\ell_1$-penalization. In the binary case,\nhowever, exact inference is generally very slow or even intractable because of\nthe form of the so-called log-partition function. Various approximate methods\nhave recently been proposed in the literature and the main objective of this\npaper is to compare them. Through an extensive simulation study, we show that a\nsimple modification of a method relying on a Gaussian approximation achieves\ngood performance and is very fast. We present a real application in which we\nsearch for associations among causes of death recorded on French death\ncertificates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 23:21:54 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Viallon", "Vivian", ""], ["Banerjee", "Onureena", ""], ["Rey", "Gregoire", ""], ["Jougla", "Eric", ""], ["Coste", "Joel", ""]]}, {"id": "1004.2785", "submitter": "Stefano Andreon", "authors": "S. Andreon (INAF-OABrera)", "title": "The stellar mass fraction and baryon content of galaxy clusters and\n  groups", "comments": "MNRAS, in press", "journal-ref": null, "doi": "10.1111/j.1365-2966.2010.16856.x", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Abridged] The analysis of a sample of 52 clusters with precise and\nhypothesis-parsimonious measurements of mass shows that low mass clusters and\ngroups are not simple scaled-down versions of their massive cousins in terms of\nstellar content: lighter clusters have more stars per unit cluster mass. The\nsame analysis also shows that the stellar content of clusters and groups\ndisplays an intrinsic spread at a given cluster mass, i.e. clusters are not\nsimilar each other in the amount of stars they contain, not even at a fixed\ncluster mass. The stellar mass fraction depends on halo mass with (logarithmic)\nslope -0.55+/-0.08 and with 0.15+/-0.02 dex of intrinsic scatter at a fixed\ncluster mass. The intrinsic scatter at a fixed cluster mass we determine for\ngas mass fractions is smaller, 0.06+/-0.01 dex. The intrinsic scatter in both\nthe stellar and gas mass fractions is a distinctive signature that the regions\nfrom which clusters and groups collected matter, a few tens of Mpc, are yet not\nrepresentative, in terms of gas and baryon content, of the mean matter content\nof the Universe. The observed stellar mass fraction values are in marked\ndisagreement with gasdynamics simulations with cooling and star formation of\nclusters and groups. We found the the baryon (gas+stellar) fraction is fairly\nconstant for clusters and groups with 13.7<lg(mass)<15.0 solar masses and it is\noffset from the WMAP-derived value by about 6 sigmas. The offset could be\nrelated to the possible non universality of the baryon fraction pointed out by\nour measurements of the intrinsic scatter. Our analysis is the first that does\nnot assume that clusters are identically equal at a given halo mass and it is\nalso more accurate in many aspects. The data and code used for the stochastic\ncomputation are distributed with the paper.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 08:45:13 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Andreon", "S.", "", "INAF-OABrera"]]}, {"id": "1004.3726", "submitter": "Valerie Monbet", "authors": "Dominique Drouet Mari and Valerie Monbet", "title": "Using a priori knowledge to construct copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our purpose is to model the dependence between two random variables, taking\ninto account a priori knowledge on these variables. For example, in many\napplications (oceanography, finance...), there exists an order relation between\nthe two variables; when one takes high values, the other cannot take low\nvalues, but the contrary is possible. The dependence for the high values of the\ntwo variables is, therefore, not symmetric.\n  However a minimal dependence also exists: low values of one variable are\nassociated with low values of the other variable. The dependence can also be\nextreme for the maxima or the minima of the two variables. In this paper, we\nconstruct step by step asymmetric copulas with asymptotic minimal dependence,\nand with or without asymptotic maximal dependence, using mixture variables to\nget at first asymmetric dependence and then minimal dependence. We fit these\nmodels to a real dataset of sea states and compare them using Likelihood Ratio\nTests when they are nested, and BIC- criterion (Bayesian Information criterion)\notherwise.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 15:24:58 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Mari", "Dominique Drouet", ""], ["Monbet", "Valerie", ""]]}, {"id": "1004.3925", "submitter": "Nial Friel", "authors": "Nial Friel and Anthony N. Pettitt", "title": "Classification using distance nearest neighbours", "comments": "12 pages, 2 figures. To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-010-9179-y", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new probabilistic classification algorithm using a\nMarkov random field approach. The joint distribution of class labels is\nexplicitly modelled using the distances between feature vectors. Intuitively, a\nclass label should depend more on class labels which are closer in the feature\nspace, than those which are further away. Our approach builds on previous work\nby Holmes and Adams (2002, 2003) and Cucala et al. (2008). Our work shares many\nof the advantages of these approaches in providing a probabilistic basis for\nthe statistical inference. In comparison to previous work, we present a more\nefficient computational algorithm to overcome the intractability of the Markov\nrandom field model. The results of our algorithm are encouraging in comparison\nto the k-nearest neighbour algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 14:09:08 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2010 09:55:51 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Friel", "Nial", ""], ["Pettitt", "Anthony N.", ""]]}, {"id": "1004.4027", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Optimization Under Unknown Constraints", "comments": "19 pages, 8 figures, Valencia discussion paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of complex functions, such as the output of computer simulators,\nis a difficult task that has received much attention in the literature. A less\nstudied problem is that of optimization under unknown constraints, i.e., when\nthe simulator must be invoked both to determine the typical real-valued\nresponse and to determine if a constraint has been violated, either for\nphysical or policy reasons. We develop a statistical approach based on Gaussian\nprocesses and Bayesian learning to both approximate the unknown function and\nestimate the probability of meeting the constraints. A new integrated\nimprovement criterion is proposed to recognize that responses from inputs that\nviolate the constraint may still be informative about the function, and thus\ncould potentially be useful in the optimization. The new criterion is\nillustrated on synthetic data, and on a motivating optimization problem from\nhealth care policy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 23:36:58 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2010 16:39:50 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}, {"id": "1004.4308", "submitter": "Sergiy Vorobyov A.", "authors": "Omid Taheri and Sergiy A. Vorobyov", "title": "Segmented compressed sampling for analog-to-information conversion:\n  Method and performance analysis", "comments": "32 pages, 5 figures, submitted to the IEEE Transactions on Signal\n  Processing in April 2010", "journal-ref": "O. Taheri and S.A. Vorobyov, \"Segmented compressed sampling for\n  analog-to-information conversion: Method and performance analysis,\" IEEE\n  Trans. Signal Processing, vol. 59, no. 2, pp. 554-572, Feb. 2011", "doi": "10.1109/TSP.2010.2091411", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new segmented compressed sampling method for analog-to-information\nconversion (AIC) is proposed. An analog signal measured by a number of parallel\nbranches of mixers and integrators (BMIs), each characterized by a specific\nrandom sampling waveform, is first segmented in time into $M$ segments. Then\nthe sub-samples collected on different segments and different BMIs are reused\nso that a larger number of samples than the number of BMIs is collected. This\ntechnique is shown to be equivalent to extending the measurement matrix, which\nconsists of the BMI sampling waveforms, by adding new rows without actually\nincreasing the number of BMIs. We prove that the extended measurement matrix\nsatisfies the restricted isometry property with overwhelming probability if the\noriginal measurement matrix of BMI sampling waveforms satisfies it. We also\nshow that the signal recovery performance can be improved significantly if our\nsegmented AIC is used for sampling instead of the conventional AIC. Simulation\nresults verify the effectiveness of the proposed segmented compressed sampling\nmethod and the validity of our theoretical studies.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2010 20:51:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Taheri", "Omid", ""], ["Vorobyov", "Sergiy A.", ""]]}, {"id": "1004.4704", "submitter": "Cosma Rohilla Shalizi", "authors": "Cosma Rohilla Shalizi, Andrew C. Thomas", "title": "Homophily and Contagion Are Generically Confounded in Observational\n  Social Network Studies", "comments": "27 pages, 9 figures. V2: Revised in response to referees. V3: Ditto", "journal-ref": "Sociological Methods and Research, vol. 40 (2011), pp. 211--239", "doi": "10.1177/0049124111404820", "report-no": null, "categories": "stat.AP cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider processes on social networks that can potentially involve three\nfactors: homophily, or the formation of social ties due to matching individual\ntraits; social contagion, also known as social influence; and the causal effect\nof an individual's covariates on their behavior or other measurable responses.\nWe show that, generically, all of these are confounded with each other.\nDistinguishing them from one another requires strong assumptions on the\nparametrization of the social process or on the adequacy of the covariates used\n(or both). In particular we demonstrate, with simple examples, that asymmetries\nin regression coefficients cannot identify causal effects, and that very simple\nmodels of imitation (a form of social contagion) can produce substantial\ncorrelations between an individual's enduring traits and their choices, even\nwhen there is no intrinsic affinity between them. We also suggest some possible\nconstructive responses to these results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 04:06:53 GMT"}, {"version": "v2", "created": "Tue, 12 Oct 2010 15:00:01 GMT"}, {"version": "v3", "created": "Tue, 30 Nov 2010 00:24:15 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""], ["Thomas", "Andrew C.", ""]]}, {"id": "1004.4856", "submitter": "Jonathan Touboul", "authors": "Jonathan Touboul and Alberto Romagnoni and Robert Schwartz", "title": "On the Dynamic Interplay between Positive and Negative Affects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional disorders and psychological flourishing are the result of complex\ninteractions between positive and negative affects that depend on external\nevents and the subject's internal representations. Based on psychological data,\nwe mathematically model the dynamical balance between positive and negative\naffects as a function of the response to external positive and negative events.\nThis modeling allows the investigation of the relative impact of two leading\nforms of therapy on affect balance. The model uses a delay differential\nequation to analytically study the complete bifurcation diagram of the system.\nWe compare the results of the model to psychological data on a single,\nrecurrently depressed patient that was administered the two types of therapies\nconsidered (viz., coping-focused vs. affect-focused). The model leads to the\nprediction that stabilization at a normal state may rely on evaluating one's\nemotional state through an historical ongoing emotional state rather than in a\nnarrow present window. The simple mathematical model proposed here offers a\ntheoretically grounded quantitative framework for investigating the temporal\nprocess of change and parameters of resilience to relapse.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 16:29:05 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 17:42:52 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Touboul", "Jonathan", ""], ["Romagnoni", "Alberto", ""], ["Schwartz", "Robert", ""]]}, {"id": "1004.4956", "submitter": "Yingying Li", "authors": "Jianqing Fan, Yingying Li, Ke Yu", "title": "Vast Volatility Matrix Estimation using High Frequency Data for\n  Portfolio Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.ST q-fin.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio allocation with gross-exposure constraint is an effective method to\nincrease the efficiency and stability of selected portfolios among a vast pool\nof assets, as demonstrated in Fan et al (2008). The required high-dimensional\nvolatility matrix can be estimated by using high frequency financial data. This\nenables us to better adapt to the local volatilities and local correlations\namong vast number of assets and to increase significantly the sample size for\nestimating the volatility matrix. This paper studies the volatility matrix\nestimation using high-dimensional high-frequency data from the perspective of\nportfolio selection. Specifically, we propose the use of \"pairwise-refresh\ntime\" and \"all-refresh time\" methods proposed by Barndorff-Nielsen et al (2008)\nfor estimation of vast covariance matrix and compare their merits in the\nportfolio selection. We also establish the concentration inequalities of the\nestimates, which guarantee desirable properties of the estimated volatility\nmatrix in vast asset allocation with gross exposure constraints. Extensive\nnumerical studies are made via carefully designed simulations. Comparing with\nthe methods based on low frequency daily data, our methods can capture the most\nrecent trend of the time varying volatility and correlation, hence provide more\naccurate guidance for the portfolio allocation in the next time period. The\nadvantage of using high-frequency data is significant in our simulation and\nempirical studies, which consist of 50 simulated assets and 30 constituent\nstocks of Dow Jones Industrial Average index.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 06:11:20 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Li", "Yingying", ""], ["Yu", "Ke", ""]]}, {"id": "1004.5300", "submitter": "Etienne Roquain", "authors": "Kyung In Kim, Etienne Roquain (PMA), Mark Van De Wiel", "title": "Spatial clustering of array CGH features in combination with\n  hierarchical multiple testing", "comments": null, "journal-ref": "Statistical Applications in Genetics and Molecular Biology (2010)\n  Vol. 9 : Iss. 1, Article 40", "doi": "10.2202/1544-6115.1532", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for clustering DNA features using array CGH data\nfrom multiple tumor samples. We distinguish data-collapsing: joining contiguous\nDNA clones or probes with extremely similar data into regions, from clustering:\njoining contiguous, correlated regions based on a maximum likelihood principle.\nThe model-based clustering algorithm accounts for the apparent spatial patterns\nin the data. We evaluate the randomness of the clustering result by a cluster\nstability score in combination with cross-validation. Moreover, we argue that\nthe clustering really captures spatial genomic dependency by showing that\ncoincidental clustering of independent regions is very unlikely. Using the\nregion and cluster information, we combine testing of these for association\nwith a clinical variable in an hierarchical multiple testing approach. This\nallows for interpreting the significance of both regions and clusters while\ncontrolling the Family-Wise Error Rate simultaneously. We prove that in the\ncontext of permutation tests and permutation-invariant clusters it is allowed\nto perform clustering and testing on the same data set. Our procedures are\nillustrated on two cancer data sets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 14:24:13 GMT"}, {"version": "v2", "created": "Sun, 19 Dec 2010 16:44:06 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Kim", "Kyung In", "", "PMA"], ["Roquain", "Etienne", "", "PMA"], ["Van De Wiel", "Mark", ""]]}, {"id": "1004.5587", "submitter": "Valerie Hower", "authors": "Steven N. Evans, Valerie Hower and Lior Pachter", "title": "Coverage statistics for sequence census methods", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We study the statistical properties of fragment coverage in\ngenome sequencing experiments. In an extension of the classic Lander-Waterman\nmodel, we consider the effect of the length distribution of fragments. We also\nintroduce the notion of the shape of a coverage function, which can be used to\ndetect abberations in coverage. The probability theory underlying these\nproblems is essential for constructing models of current high-throughput\nsequencing experiments, where both sample preparation protocols and sequencing\ntechnology particulars can affect fragment length distributions.\n  Results: We show that regardless of fragment length distribution and under\nthe mild assumption that fragment start sites are Poisson distributed, the\nfragments produced in a sequencing experiment can be viewed as resulting from a\ntwo-dimensional spatial Poisson process. We then study the jump skeleton of the\nthe coverage function, and show that the induced trees are Galton-Watson trees\nwhose parameters can be computed.\n  Conclusions: Our results extend standard analyses of shotgun sequencing that\nfocus on coverage statistics at individual sites, and provide a null model for\ndetecting deviations from random coverage in high-throughput sequence census\nbased experiments. By focusing on fragments, we are also led to a new approach\nfor visualizing sequencing data that should be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 18:36:40 GMT"}], "update_date": "2010-05-03", "authors_parsed": [["Evans", "Steven N.", ""], ["Hower", "Valerie", ""], ["Pachter", "Lior", ""]]}]