[{"id": "0710.0178", "submitter": "Julia Brettschneider", "authors": "Julia Brettschneider, Francois Collin, Benjamin M. Bolstad, Terence P.\n  Speed", "title": "Quality assessment for short oligonucleotide microarray data", "comments": "32 pages plus 12 figure pages (17 figures total), correction of\n  typos, conversion of some figures into color", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  Quality of microarray gene expression data has emerged as a new research\ntopic. As in other areas, microarray quality is assessed by comparing suitable\nnumerical summaries across microarrays, so that outliers and trends can be\nvisualized, and poor quality arrays or variable quality sets of arrays can be\nidentified. Since each single array comprises tens or hundreds of thousands of\nmeasurements, the challenge is to find numerical summaries which can be used to\nmake accurate quality calls. To this end, several new quality measures are\nintroduced based on probe level and probeset level information, all obtained as\na by-product of the low-level analysis algorithms RMA/fitPLM for Affymetrix\nGeneChips. Quality landscapes spatially localize chip or hybridization\nproblems. Numerical chip quality measures are derived from the distributions of\nNormalized Unscaled Standard Errors and of Relative Log Expressions. Quality of\nchip batches is assessed by Residual Scale Factors. These quality assessment\nmeasures are demonstrated on a variety of datasets (spike-in experiments, small\nlab experiments, multi-site studies). They are compared with Affymetrix's\nindividual chip quality report.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2007 23:56:00 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2007 05:57:38 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Brettschneider", "Julia", ""], ["Collin", "Francois", ""], ["Bolstad", "Benjamin M.", ""], ["Speed", "Terence P.", ""]]}, {"id": "0710.0559", "submitter": "Patrice Gaubert", "authors": "Fran\\c{c}ois Gardes (CERMSEM), Greg Duncan, Patrice Gaubert (SAMOS),\n  Marc Gurgand (DELTA), Christophe Starzec (TEAM)", "title": "Panel and Pseudo-Panel Estimation of Cross-Sectional and Time Series\n  Elasticities of Food Consumption: The Case of American and Polish Data", "comments": "12 p", "journal-ref": "Journal of Business & Economic Statistics 23, 2 (2005) 242-253", "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  The problem addressed in this article is the bias to income and expenditure\nelasticities estimated on pseudo-panel data caused by measurement error and\nunobserved heterogeneity. We gauge empirically these biases by comparing\ncross-sectional, pseudo-panel and true panel data from both Polish and American\nexpenditure surveys. Our results suggest that unobserved heterogeneity imparts\na downward bias to cross-section estimates of income elasticities of at-home\nfood expenditures and an upward bias to estimates of income elasticities of\naway-from-home food expenditures. \"Within\" and first-difference estimators\nsuffer less bias, but only if the effects of measurement error are accounted\nfor with instrumental variables.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2007 15:22:32 GMT"}], "update_date": "2007-10-03", "authors_parsed": [["Gardes", "Fran\u00e7ois", "", "CERMSEM"], ["Duncan", "Greg", "", "SAMOS"], ["Gaubert", "Patrice", "", "SAMOS"], ["Gurgand", "Marc", "", "DELTA"], ["Starzec", "Christophe", "", "TEAM"]]}, {"id": "0710.0657", "submitter": "Brandon Whitcher", "authors": "Brandon Whitcher, Thomas C. M. Lee, Jeffrey B. Weiss, Timothy J. Hoar,\n  Douglas W. Nychka", "title": "A Multiresolution Census Algorithm for Calculating Vortex Statistics in\n  Turbulent Flows", "comments": null, "journal-ref": "Journal of the Royal Statistical Society. Series C (Applied\n  Statistics) Vol. 57, No. 3 (2008), pp. 293-312", "doi": "10.1111/j.1467-9876.2007.00614.x", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": null, "abstract": "  The fundamental equations that model turbulent flow do not provide much\ninsight into the size and shape of observed turbulent structures. We\ninvestigate the efficient and accurate representation of structures in\ntwo-dimensional turbulence by applying statistical models directly to the\nsimulated vorticity field. Rather than extract the coherent portion of the\nimage from the background variation, as in the classical signal-plus-noise\nmodel, we present a model for individual vortices using the non-decimated\ndiscrete wavelet transform. A template image, supplied by the user, provides\nthe features to be extracted from the vorticity field. By transforming the\nvortex template into the wavelet domain, specific characteristics present in\nthe template, such as size and symmetry, are broken down into components\nassociated with spatial frequencies. Multivariate multiple linear regression is\nused to fit the vortex template to the vorticity field in the wavelet domain.\nSince all levels of the template decomposition may be used to model each level\nin the field decomposition, the resulting model need not be identical to the\ntemplate. Application to a vortex census algorithm that records quantities of\ninterest (such as size, peak amplitude, circulation, etc.) as the vorticity\nfield evolves is given. The multiresolution census algorithm extracts coherent\nstructures of all shapes and sizes in simulated vorticity fields and is able to\nreproduce known physical scaling laws when processing a set of voriticity\nfields that evolve over time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2007 21:54:15 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Whitcher", "Brandon", ""], ["Lee", "Thomas C. M.", ""], ["Weiss", "Jeffrey B.", ""], ["Hoar", "Timothy J.", ""], ["Nychka", "Douglas W.", ""]]}, {"id": "0710.0745", "submitter": "Patrice Gaubert", "authors": "Marie-Th\\'er\\`ese Boyer-Xambeu (LED - EA3391), Ghislain Deleplace (LED\n  - EA3391), Patrice Gaubert (SAMOS), Lucien Gillard (LED - EA3391), Madalina\n  Olteanu (SAMOS)", "title": "Mixing Kohonen Algorithm, Markov Switching Model and Detection of\n  Multiple Change-Points: An Application to Monetary History", "comments": null, "journal-ref": "Computational and Ambient Intelligence, Springer (Ed.) (2007)\n  547-555", "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": null, "abstract": "  The present paper aims at locating the breakings of the integration process\nof an international system observed during about 50 years in the 19th century.\nA historical study could link them to special events, which operated as\nexogenous shocks on this process. The indicator of integration used is the\nspread between the highest and the lowest among the London, Hamburg and Paris\ngold-silver prices. Three algorithms are combined to study this integration: a\nperiodization obtained with the SOM algorithm is confronted to the estimation\nof a two-regime Markov switching model, in order to give an interpretation of\nthe changes of regime; in the same time change-points are identified over the\nwhole period providing a more precise interpretation of the various types of\nregulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 09:26:43 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Boyer-Xambeu", "Marie-Th\u00e9r\u00e8se", "", "LED - EA3391"], ["Deleplace", "Ghislain", "", "LED\n  - EA3391"], ["Gaubert", "Patrice", "", "SAMOS"], ["Gillard", "Lucien", "", "LED - EA3391"], ["Olteanu", "Madalina", "", "SAMOS"]]}, {"id": "0710.0849", "submitter": "Enrico Rogora", "authors": "Alessandro Figa' Talamanca, Angelo Guerriero, Alberto Leone, Gian\n  Piero Mignoli, Enrico Rogora", "title": "Decomposition of variance in terms of conditional means", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  We test against two different sets of data an apparently new approach to the\nanalysis of the variance of a numerical variable which depends on qualitative\ncharacters. We suggest that this approach be used to complement other existing\ntechniques to study the interdependence of the variables involved. According to\nour method the variance is expressed as a sum of orthogonal components,\nobtained as differences of conditional means, with respect to the qualitative\ncharacters. The resulting expression for the variance depends on the ordering\nin which the characters are considered. We suggest an algorithm which leads to\nan ordering which is deemed natural. The first set of data concerns the score\nachieved by a population of students, on an entrance examination, based on a\nmultiple choice test with 30 questions. In this case the qualitative characters\nare dyadic and correspond to correct or incorrect answer to each question. The\nsecond set of data concerns the delay in obtaining the degree for a population\nof graduates of Italian universities. The variance in this case is analyzed\nwith respect to a set of seven specific qualitative characters of the\npopulation studied (gender, previous education, working condition, parent's\neducational level, field of study, etc.)\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 17:38:00 GMT"}], "update_date": "2007-10-04", "authors_parsed": [["Talamanca", "Alessandro Figa'", ""], ["Guerriero", "Angelo", ""], ["Leone", "Alberto", ""], ["Mignoli", "Gian Piero", ""], ["Rogora", "Enrico", ""]]}, {"id": "0710.0850", "submitter": "Piergiacomo Sabino", "authors": "Piergiacomo Sabino (Dipartimento di Matematica, Universit\\`a degli\n  Studi di Bari)", "title": "Monte Carlo Methods and Path-Generation techniques for Pricing\n  Multi-asset Path-dependent Options", "comments": "34 pages, 4 figure, 15 tables", "journal-ref": null, "doi": null, "report-no": "Report 36/07", "categories": "math.PR stat.AP", "license": null, "abstract": "  We consider the problem of pricing path-dependent options on a basket of\nunderlying assets using simulations. As an example we develop our studies using\nAsian options. Asian options are derivative contracts in which the underlying\nvariable is the average price of given assets sampled over a period of time.\nDue to this structure, Asian options display a lower volatility and are\ntherefore cheaper than their standard European counterparts. This paper is a\nsurvey of some recent enhancements to improve efficiency when pricing Asian\noptions by Monte Carlo simulation in the Black-Scholes model. We analyze the\ndynamics with constant and time-dependent volatilities of the underlying asset\nreturns. We present a comparison between the precision of the standard Monte\nCarlo method (MC) and the stratified Latin Hypercube Sampling (LHS). In\nparticular, we discuss the use of low-discrepancy sequences, also known as\nQuasi-Monte Carlo method (QMC), and a randomized version of these sequences,\nknown as Randomized Quasi Monte Carlo (RQMC). The latter has proven to be a\nuseful variance reduction technique for both problems of up to 20 dimensions\nand for very high dimensions. Moreover, we present and test a new path\ngeneration approach based on a Kronecker product approximation (KPA) in the\ncase of time-dependent volatilities. KPA proves to be a fast generation\ntechnique and reduces the computational cost of the simulation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 17:38:53 GMT"}], "update_date": "2007-10-04", "authors_parsed": [["Sabino", "Piergiacomo", "", "Dipartimento di Matematica, Universit\u00e0 degli\n  Studi di Bari"]]}, {"id": "0710.1001", "submitter": "Vitaliy Kurlin", "authors": "V. Kurlin, L. Mihaylova", "title": "Connectivity of Random 1-Dimensional Networks", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in wireless sensor networks is to find the minimal\nnumber of randomly deployed sensors making a network connected with a given\nprobability. In practice sensors are often deployed one by one along a\ntrajectory of a vehicle, so it is natural to assume that arbitrary probability\ndensity functions of distances between successive sensors in a segment are\ngiven. The paper computes the probability of connectivity and coverage of\n1-dimensional networks and gives estimates for a minimal number of sensors for\nimportant distributions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2007 12:57:34 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2009 21:01:10 GMT"}], "update_date": "2009-10-10", "authors_parsed": [["Kurlin", "V.", ""], ["Mihaylova", "L.", ""]]}, {"id": "0710.2024", "submitter": "Volker Franz H", "authors": "Volker H. Franz", "title": "Ratios: A short guide to confidence limits and proper use", "comments": "60 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": null, "abstract": "  Researchers often calculate ratios of measured quantities. Specifying\nconfidence limits for ratios is difficult and the appropriate methods are often\nunknown. Appropriate methods are described (Fieller, Taylor, special bootstrap\nmethods). For the Fieller method a simple geometrical interpretation is given.\nMonte Carlo simulations show when these methods are appropriate and that the\nmost frequently used methods (index method and zero-variance method) can lead\nto large liberal deviations from the desired confidence level. It is discussed\nwhen we can use standard regression or measurement error models and when we\nhave to resort to specific models for heteroscedastic data. Finally, an old\nwarning is repeated that we should be aware of the problems of spurious\ncorrelations if we use ratios.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2007 14:46:48 GMT"}], "update_date": "2007-10-11", "authors_parsed": [["Franz", "Volker H.", ""]]}, {"id": "0710.2740", "submitter": "Angshuman Sarkar", "authors": "Rudrani Banerjee, Angshuman Sarkar", "title": "Reliability of Module Based Software System", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2007_129", "categories": "stat.AP", "license": null, "abstract": "  This paper consider the problem of determining the reliability of a software\nsystem which can be decomposed in a number of modules. We have derived the\nexpression of the reliability of a system using the Markovian model for the\ntransfer of control between modules in order. We have given the expression of\nreliability by considering both benign and catastrophic failure. The expression\nof reliability presented in this work is applicable for some control software\nwhich are designed to detect its own internal errors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2007 08:55:10 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Banerjee", "Rudrani", ""], ["Sarkar", "Angshuman", ""]]}, {"id": "0710.3447", "submitter": "Victor Kromer", "authors": "Victor Kromer", "title": "Problems of Testology", "comments": "5 pages, 1 Table, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Some problems of testology are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2007 07:43:34 GMT"}], "update_date": "2007-11-12", "authors_parsed": [["Kromer", "Victor", ""]]}, {"id": "0710.3473", "submitter": "Gavin Shaddick", "authors": "Duncan Lee and Gavin Shaddick", "title": "Modelling the effects of air pollution on health using Bayesian Dynamic\n  Generalised Linear Models", "comments": "Accepted for publication in Environmetrics, October 2nd 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": null, "abstract": "  The relationship between short-term exposure to air pollution and mortality\nor morbidity has been the subject of much recent research, in which the\nstandard method of analysis uses Poisson linear or additive models. In this\npaper we use a Bayesian dynamic generalised linear model (DGLM) to estimate\nthis relationship, which allows the standard linear or additive model to be\nextended in two ways: (i) the long-term trend and temporal correlation present\nin the health data can be modelled by an autoregressive process rather than a\nsmooth function of calendar time; (ii) the effects of air pollution are allowed\nto evolve over time. The efficacy of these two extensions are investigated by\napplying a series of dynamic and non-dynamic models to air pollution and\nmortality data from Greater London. A Bayesian approach is taken throughout,\nand a Markov chain monte carlo simulation algorithm is presented for inference.\nAn alternative likelihood based analysis is also presented, in order to allow a\ndirect comparison with the only previous analysis of air pollution and health\ndata using a DGLM.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2007 09:51:34 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2012 15:43:08 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Lee", "Duncan", ""], ["Shaddick", "Gavin", ""]]}, {"id": "0710.4117", "submitter": "Ioannis Kontoyiannis", "authors": "Yun Gao, Ioannis Kontoyiannis and Elie Bienenstock", "title": "From the entropy to the statistical structure of spike trains", "comments": null, "journal-ref": "In Proceedings of the 2006 International Symposium on Information\n  Theory, Seattle, WA, July 2006", "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT math.PR stat.AP", "license": null, "abstract": "  We use statistical estimates of the entropy rate of spike train data in order\nto make inferences about the underlying structure of the spike train itself. We\nfirst examine a number of different parametric and nonparametric estimators\n(some known and some new), including the ``plug-in'' method, several versions\nof Lempel-Ziv-based compression algorithms, a maximum likelihood estimator\ntailored to renewal processes, and the natural estimator derived from the\nContext-Tree Weighting method (CTW). The theoretical properties of these\nestimators are examined, several new theoretical results are developed, and all\nestimators are systematically applied to various types of synthetic data and\nunder different conditions.\n  Our main focus is on the performance of these entropy estimators on the\n(binary) spike trains of 28 neurons recorded simultaneously for a one-hour\nperiod from the primary motor and dorsal premotor cortices of a monkey. We show\nhow the entropy estimates can be used to test for the existence of long-term\nstructure in the data, and we construct a hypothesis test for whether the\nrenewal process model is appropriate for these spike trains. Further, by\napplying the CTW algorithm we derive the maximum a posterior (MAP) tree model\nof our empirical data, and comment on the underlying structure it reveals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2007 18:14:17 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2007 19:15:01 GMT"}, {"version": "v3", "created": "Thu, 27 Mar 2008 13:51:43 GMT"}], "update_date": "2008-03-27", "authors_parsed": [["Gao", "Yun", ""], ["Kontoyiannis", "Ioannis", ""], ["Bienenstock", "Elie", ""]]}, {"id": "0710.4171", "submitter": "Hamidreza Saligheh Rad", "authors": "Kamal Shahtalebi, Golam Reza Bakhshi, Hamidreza Saligheh Rad", "title": "Interference Cancelation in Coherent CDMA Systems Using Parallel\n  Iterative Algorithms", "comments": "5 pages, 3 figures, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Least mean square-partial parallel interference cancelation (LMS-PPIC) is a\npartial interference cancelation using adaptive multistage structure in which\nthe normalized least mean square (NLMS) adaptive algorithm is engaged to obtain\nthe cancelation weights. The performance of the NLMS algorithm is mostly\ndependent to its step-size. A fixed and non-optimized step-size causes the\npropagation of error from one stage to the next one. When all user channels are\nbalanced, the unit magnitude is the principal property of the cancelation\nweight elements. Based on this fact and using a set of NLMS algorithms with\ndifferent step-sizes, the parallel LMS-PPIC (PLMS-PPIC) method is proposed. In\neach iteration of the algorithm, the parameter estimate of the NLMS algorithm\nis chosen to match the elements' magnitudes of the cancelation weight estimate\nwith unity. Simulation results are given to compare the performance of our\nmethod with the LMS-PPIC algorithm in three cases: balanced channel, unbalanced\nchannel and time varying channel.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 00:31:31 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Shahtalebi", "Kamal", ""], ["Bakhshi", "Golam Reza", ""], ["Rad", "Hamidreza Saligheh", ""]]}, {"id": "0710.4172", "submitter": "Hamidreza Saligheh Rad", "authors": "Kamal Shahtalebi, Hamidreza Saligheh Rad, Golam Reza Bakhshi", "title": "Interference Cancelation in Non-coherent CDMA Systems Using Parallel\n  Iterative Algorithms", "comments": "5 pages, 3 figures, 1 table, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Parallel least mean square-partial parallel interference cancelation\n(PLMS-PPIC) is a partial interference cancelation which employs adaptive\nmultistage structure. In this algorithm the channel phases for all users are\nassumed to be known. Having only their quarters in (0,2\\pi), a modified version\nof PLMS-PPIC is proposed in this paper to simultaneously estimate the channel\nphases and the cancelation weights. Simulation examples are given in the cases\nof balanced, unbalanced and time varying channels to show the performance of\nthe modified PLMS-PPIC method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 00:36:00 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Shahtalebi", "Kamal", ""], ["Rad", "Hamidreza Saligheh", ""], ["Bakhshi", "Golam Reza", ""]]}, {"id": "0710.4173", "submitter": "Hamidreza Saligheh Rad", "authors": "Kamal Shahtalebi, Golam Reza Bakhshi, Hamidreza Saligheh Rad", "title": "Full MIMO Channel Estimation Using A Simple Adaptive Partial Feedback\n  Method", "comments": "4 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  Partial feedback in multiple-input multiple-output (MIMO) communication\nsystems provides tremendous capacity gain and enables the transmitter to\nexploit channel condition and to eliminate channel interference. In the case of\nseverely limited feedback, constructing a quantized partial feedback is an\nimportant issue. To reduce the computational complexity of the feedback system,\nin this paper we introduce an adaptive partial method in which at the\ntransmitter, an easy to implement least square adaptive algorithm is engaged to\ncompute the channel state information. In this scheme at the receiver, the time\nvarying step-size is replied to the transmitter via a reliable feedback\nchannel. The transmitter iteratively employs this feedback information to\nestimate the channel weights. This method is independent of the employed\nspace-time coding schemes and gives all channel components. Simulation examples\nare given to evaluate the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 00:41:28 GMT"}], "update_date": "2007-10-24", "authors_parsed": [["Shahtalebi", "Kamal", ""], ["Bakhshi", "Golam Reza", ""], ["Rad", "Hamidreza Saligheh", ""]]}, {"id": "0710.4404", "submitter": "Brahim Boudarbat", "authors": "Brahim Boudarbat, Lee Grenon", "title": "Attrition and Non-Response in Panel Data: The Case of the Canadian\n  Survey of Labor and Income Dynamics", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2007_136", "categories": "stat.AP", "license": null, "abstract": "  This paper provides an analysis of the effects of attrition and non-response\non employment and wages using the Canadian Survey of Labour and Income\nDynamics. We consider a structural model composed of three freely correlated\nequations for nonattrition/response, employment and wages. The model is\nestimated using microdata from 22,990 individuals who provided sufficient\ninformation in the first wave of the 1996-2001 panel. The main findings of the\npaper are that attrition is not random. Attritors and non-respondents likely\nare less attached to employment and come from low-income population. The\ncorrelation between non-attrition and employment is positive and statistically\nsignificant, though small. Also, wage estimates are biased upwards. Observed\nwages are on average higher than wages that would be observed if all the\nindividuals initially selected in the panel remained in the sample.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2007 08:27:37 GMT"}], "update_date": "2007-10-25", "authors_parsed": [["Boudarbat", "Brahim", ""], ["Grenon", "Lee", ""]]}, {"id": "0710.4536", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Bayesian treed Gaussian process models with an application to computer\n  modeling", "comments": "32 pages, 9 figures, to appear in the Journal of the American\n  Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a computer experiment for the design of a rocket booster, this\npaper explores nonstationary modeling methodologies that couple stationary\nGaussian processes with treed partitioning. Partitioning is a simple but\neffective method for dealing with nonstationarity. The methodological\ndevelopments and statistical computing details which make this approach\nefficient are described in detail. In addition to providing an analysis of the\nrocket booster simulator, our approach is demonstrated to be effective in other\narenas.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2007 19:10:12 GMT"}, {"version": "v10", "created": "Tue, 17 Mar 2009 15:46:13 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2008 14:58:55 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2008 17:46:37 GMT"}, {"version": "v4", "created": "Sat, 16 Feb 2008 13:14:40 GMT"}, {"version": "v5", "created": "Thu, 28 Feb 2008 18:06:34 GMT"}, {"version": "v6", "created": "Mon, 3 Mar 2008 18:31:35 GMT"}, {"version": "v7", "created": "Fri, 4 Apr 2008 08:28:48 GMT"}, {"version": "v8", "created": "Mon, 3 Nov 2008 18:35:42 GMT"}, {"version": "v9", "created": "Mon, 24 Nov 2008 12:55:18 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}, {"id": "0710.4788", "submitter": "Brandon Whitcher", "authors": "Volker J. Schmid, Brandon Whitcher, Anwar R. Padhani, N. Jane Taylor,\n  Guang-Zhong Yang", "title": "A Bayesian Hierarchical Model for the Analysis of a Longitudinal Dynamic\n  Contrast-Enhanced MRI Cancer Study", "comments": null, "journal-ref": "Magnetic Resonance in Medicine, 2009, 61(1), 163-174", "doi": "10.1002/mrm.21807", "report-no": null, "categories": "stat.AP physics.med-ph", "license": null, "abstract": "  Imaging in clinical oncology trials provides a wealth of information that\ncontributes to the drug development process, especially in early phase studies.\nThis paper focuses on kinetic modeling in DCE-MRI, inspired by mixed-effects\nmodels that are frequently used in the analysis of clinical trials. Instead of\nsummarizing each scanning session as a single kinetic parameter -- such as\nmedian $\\ktrans$ across all voxels in the tumor ROI -- we propose to analyze\nall voxel time courses from all scans and across all subjects simultaneously in\na single model. The kinetic parameters from the usual non-linear regression\nmodel are decomposed into unique components associated with factors from the\nlongitudinal study; e.g., treatment, patient and voxel effects. A Bayesian\nhierarchical model provides the framework in order to construct a data model, a\nparameter model, as well as prior distributions. The posterior distribution of\nthe kinetic parameters is estimated using Markov chain Monte Carlo (MCMC)\nmethods. Hypothesis testing at the study level for an overall treatment effect\nis straightforward and the patient- and voxel-level parameters capture random\neffects that provide additional information at various levels of resolution to\nallow a thorough evaluation of the clinical trial. The proposed method is\nvalidated with a breast cancer study, where the subjects were imaged before and\nafter two cycles of chemotherapy, demonstrating the clinical potential of this\nmethod to longitudinal oncology studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2007 11:19:59 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Schmid", "Volker J.", ""], ["Whitcher", "Brandon", ""], ["Padhani", "Anwar R.", ""], ["Taylor", "N. Jane", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "0710.5268", "submitter": "Vivian Viallon", "authors": "V. Viallon, J. Benichou, F. Clavel-Chapelon and S. Ragusa", "title": "How to evaluate the calibration of a disease risk prediction tool", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  To evaluate the calibration of a disease risk prediction tool, the quantity\n$E/O$, i.e., the ratio of the expected number of events to the observed number\nof events, is generally computed. However, because of censoring, or more\nprecisely because of individuals who drop out before the termination of the\nstudy, this quantity is generally unavailable for the complete population study\nand an alternative estimate has to be computed. In this paper, we present and\ncompare four methods to do this. We show that two of the most commonly used\nmethods generally lead to biased estimates. Our arguments are first based on\nsome theoretic considerations. Then, we perform a simulation study to highlight\nthe magnitude of the previously mentioned biases. As a concluding example, we\nevaluate the calibration of an existing predictive model for breast cancer on\nthe E3N-EPIC cohort.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2007 17:20:50 GMT"}], "update_date": "2007-10-30", "authors_parsed": [["Viallon", "V.", ""], ["Benichou", "J.", ""], ["Clavel-Chapelon", "F.", ""], ["Ragusa", "S.", ""]]}, {"id": "0710.5805", "submitter": "Gavin Shaddick", "authors": "Gavin Shaddick, Duncan Lee, James V. Zidek, Ruth Salway", "title": "Estimating exposure response functions using ambient pollution\n  concentrations", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  This paper presents an approach to estimating the health effects of an\nenvironmental hazard. The approach is general in nature, but is applied here to\nthe case of air pollution. It uses a computer model involving ambient pollution\nand temperature inputs, to simulate the exposures experienced by individuals in\nan urban area, whilst incorporating the mechanisms that determine exposures.\nThe output from the model comprises a set of daily exposures for a sample of\nindividuals from the population of interest. These daily exposures are\napproximated by parametric distributions, so that the predictive exposure\ndistribution of a randomly selected individual can be generated. These\ndistributions are then incorporated into a hierarchical Bayesian framework\n(with inference using Markov Chain Monte Carlo simulation) in order to examine\nthe relationship between short-term changes in exposures and health outcomes,\nwhilst making allowance for long-term trends, seasonality, the effect of\npotential confounders and the possibility of ecological bias.\n  The paper applies this approach to particulate pollution (PM$_{10}$) and\nrespiratory mortality counts for seniors in greater London ($\\geq$65 years)\nduring 1997. Within this substantive epidemiological study, the effects on\nhealth of ambient concentrations and (estimated) personal exposures are\ncompared.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2007 14:55:24 GMT"}], "update_date": "2007-11-01", "authors_parsed": [["Shaddick", "Gavin", ""], ["Lee", "Duncan", ""], ["Zidek", "James V.", ""], ["Salway", "Ruth", ""]]}, {"id": "0710.5837", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Joo Hee Lee, and Ricardo Silva", "title": "On estimating covariances between many assets with histories of highly\n  variable length", "comments": "39 pages, 5 figures, 2 tables, submitted to JCGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative portfolio allocation requires the accurate and tractable\nestimation of covariances between a large number of assets, whose histories can\ngreatly vary in length. Such data are said to follow a monotone missingness\npattern, under which the likelihood has a convenient factorization. Upon\nfurther assuming that asset returns are multivariate normally distributed, with\nhistories at least as long as the total asset count, maximum likelihood (ML)\nestimates are easily obtained by performing repeated ordinary least squares\n(OLS) regressions, one for each asset. Things get more interesting when there\nare more assets than historical returns. OLS becomes unstable due to\nrank--deficient design matrices, which is called a \"big p small n\" problem. We\nexplore remedies that involve making a change of basis, as in principal\ncomponents or partial least squares regression, or by applying shrinkage\nmethods like ridge regression or the lasso. This enables the estimation of\ncovariances between large sets of assets with histories of essentially\narbitrary length, and offers improvements in accuracy and interpretation. We\nfurther extend the method by showing how external factors can be incorporated.\nThis allows for the adaptive use of factors without the restrictive assumptions\ncommon in factor models. Our methods are demonstrated on randomly generated\ndata, and then benchmarked by the performance of balanced portfolios using real\nhistorical financial returns. An accompanying R package called monomvn,\ncontaining code implementing the estimators described herein, has been made\nfreely available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2007 15:36:20 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2007 10:44:14 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2007 16:53:16 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2008 20:49:15 GMT"}, {"version": "v5", "created": "Mon, 10 Mar 2008 09:19:16 GMT"}, {"version": "v6", "created": "Tue, 7 Oct 2008 02:24:19 GMT"}, {"version": "v7", "created": "Tue, 24 Feb 2009 17:28:34 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Joo Hee", ""], ["Silva", "Ricardo", ""]]}]