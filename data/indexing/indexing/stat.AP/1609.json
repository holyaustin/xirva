[{"id": "1609.00051", "submitter": "Sean Meyn", "authors": "Yue Chen, Ana Bu\\v{s}i\\'c, and Sean Meyn", "title": "Estimation and Control of Quality of Service in Demand Dispatch", "comments": "Submitted for publication, August 2016. arXiv admin note: text\n  overlap with arXiv:1409.6941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well known that flexibility of energy consumption can be harnessed\nfor the purposes of grid-level ancillary services. In particular, through\ndistributed control of a collection of loads, a balancing authority regulation\nsignal can be tracked accurately, while ensuring that the quality of service\n(QoS) for each load is acceptable {\\it on average}. In this paper it is argued\nthat a histogram of QoS is approximately Gaussian, and consequently each load\nwill eventually receive poor service. Statistical techniques are developed to\nestimate the mean and variance of QoS as a function of the power spectral\ndensity of the regulation signal. It is also shown that additional local\ncontrol can eliminate risk: The histogram of QoS is {\\it truncated} through\nthis local control, so that strict bounds on service quality are guaranteed.\nWhile there is a tradeoff between the grid-level tracking performance (capacity\nand accuracy) and the bounds imposed on QoS, it is found that the loss of\ncapacity is minor in typical cases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:52:48 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Chen", "Yue", ""], ["Bu\u0161i\u0107", "Ana", ""], ["Meyn", "Sean", ""]]}, {"id": "1609.00141", "submitter": "Gavin Shaddick", "authors": "Gavin Shaddick, Matthew L. Thomas, Amelia Jobling, Michael Brauer,\n  Aaron van Donkelaar, Rick Burnett, Howard Chang, Aaron Cohen, Rita Van\n  Dingenen, Carlos Dora, Sophie Gumy, Yang Liu, Randall Martin, Lance A.\n  Waller, Jason West, James V. Zidek, Annette Pr\\\"uss-Ust\\\"un", "title": "Data Integration Model for Air Quality: A Hierarchical Approach to the\n  Global Estimation of Exposures to Ambient Air Pollution", "comments": "23 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution is a major risk factor for global health, with both ambient and\nhousehold air pollution contributing substantial components of the overall\nglobal disease burden. One of the key drivers of adverse health effects is fine\nparticulate matter ambient pollution (PM$_{2.5}$) to which an estimated 3\nmillion deaths can be attributed annually. The primary source of information\nfor estimating exposures has been measurements from ground monitoring networks\nbut, although coverage is increasing, there remain regions in which monitoring\nis limited. Ground monitoring data therefore needs to be supplemented with\ninformation from other sources, such as satellite retrievals of aerosol optical\ndepth and chemical transport models. A hierarchical modelling approach for\nintegrating data from multiple sources is proposed allowing spatially-varying\nrelationships between ground measurements and other factors that estimate air\nquality. Set within a Bayesian framework, the resulting Data Integration Model\nfor Air Quality (DIMAQ) is used to estimate exposures, together with associated\nmeasures of uncertainty, on a high resolution grid covering the entire world.\nBayesian analysis on this scale can be computationally challenging and here\napproximate Bayesian inference is performed using Integrated Nested Laplace\nApproximations. Model selection and assessment is performed by cross-validation\nwith the final model offering substantial increases in predictive accuracy,\nparticularly in regions where there is sparse ground monitoring, when compared\nto current approaches: root mean square error (RMSE) reduced from 17.1 to 10.7,\nand population weighted RMSE from 23.1 to 12.1 $\\mu$gm$^{-3}$. Based on\nsummaries of the posterior distributions for each grid cell, it is estimated\nthat 92% of the world's population reside in areas exceeding the World Health\nOrganization's Air Quality Guidelines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 08:14:01 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 16:56:11 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Shaddick", "Gavin", ""], ["Thomas", "Matthew L.", ""], ["Jobling", "Amelia", ""], ["Brauer", "Michael", ""], ["van Donkelaar", "Aaron", ""], ["Burnett", "Rick", ""], ["Chang", "Howard", ""], ["Cohen", "Aaron", ""], ["Van Dingenen", "Rita", ""], ["Dora", "Carlos", ""], ["Gumy", "Sophie", ""], ["Liu", "Yang", ""], ["Martin", "Randall", ""], ["Waller", "Lance A.", ""], ["West", "Jason", ""], ["Zidek", "James V.", ""], ["Pr\u00fcss-Ust\u00fcn", "Annette", ""]]}, {"id": "1609.00243", "submitter": "Kentaro Katahira", "authors": "Kentaro Katahira and Yuichi Yamashita", "title": "A prototype model for evaluating psychiatric research strategies:\n  Diagnostic category-based approaches vs. the RDoC approach", "comments": "31 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a theoretical framework for evaluating psychiatric\nresearch strategies. The strategies to be evaluated include a conventional\ndiagnostic category-based approach and dimensional approach that have been\nencouraged by the National Institute for Mental Health (NIMH), outlined as\nResearch Domain Criteria (RDoC). The proposed framework is based on the\nstatistical modeling of the processes by which pathogenetic factors are\ntranslated to behavioral measures and how the research strategies can detect\npotential pathogenetic factors. The framework provides the statistical power\nfor quantifying how efficiently relevant pathogenetic factors are detected\nunder various conditions. We present several theoretical and numerical results\nhighlighting the merits and demerits of the strategies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 13:59:50 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Katahira", "Kentaro", ""], ["Yamashita", "Yuichi", ""]]}, {"id": "1609.00360", "submitter": "Shuo Chen", "authors": "Shuo Chen, Yishi Xing, Jian Kang, Dinesh Shukla, Peter Kochunov, and\n  L. Elliot Hong", "title": "A Network Object Method to Uncover Hidden Disorder-Related Brain\n  Connectome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuropsychiatric disorders impact functional connectivity of the brain at the\nnetwork level. The identification and statistical testing of disorder-related\nnetworks remains challenging. We propose novel methods to streamline the\ndetection and testing of the hidden, disorder-related connectivity patterns as\nnetwork-objects. We define an abnormal connectome subnetwork as a\nnetwork-object that includes three classes: nodes of brain areas, edges\nrepresenting brain connectomic features, and an organized graph topology formed\nby these nodes and edges. Comparing to the conventional statistical methods,\nthe proposed approach simultaneously reduces false positive and negative\ndiscovery rates by letting edges borrow strengths precisely with the guidance\nof graph topological information, which effectively improves the\nreproducibility of findings across brain connectome studies. The network-object\nanalyses may provide insights into how brain connectome is systematically\nimpaired by brain illnesses.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 19:38:14 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 21:45:30 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Chen", "Shuo", ""], ["Xing", "Yishi", ""], ["Kang", "Jian", ""], ["Shukla", "Dinesh", ""], ["Kochunov", "Peter", ""], ["Hong", "L. Elliot", ""]]}, {"id": "1609.00506", "submitter": "Walter Schachermayer", "authors": "Erich Neuwirth, Walter Schachermayer", "title": "Some Statistics concerning the Austrian Presidential Election 2016", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2016 Austrian presidential runoff election has been repealed by the\nAustrian constitutional court. The results of the counted votes had yielded a\nvictory of Alexander van der Bellen by a margin of 30.863 votes as compared to\nthe votes for Norbert Hofer. However, the constitutional court found that\n77.769 votes were \"contaminated\" as there have been -- at least on a formal\nlevel - violations of the legal procedure when counting those votes. For\nexample, the envelopes were opened prematurely, or not all the members of the\nelectoral board were present during the counting etc. Hence the court\nconsidered the scenario that the irregular counting of these votes might have\ncaused a reversal of the result as possible. The constitutional court sentenced\nthat this possibility presents a sufficient irregularity in order to order a\nrepetition of the entire election. While it is, of course, possible that the\nirregular counting of those 77.769 votes reversed the result, we shall show\nthat the probability, that this indeed has happened, is ridiculously low.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 08:58:48 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Neuwirth", "Erich", ""], ["Schachermayer", "Walter", ""]]}, {"id": "1609.00626", "submitter": "Shinichi Nakajima", "authors": "Shinichi Nakajima, Sebastian Krause, Dirk Weissenborn, Sven Schmeier,\n  Nico Goernitz, Feiyu Xu", "title": "SynsetRank: Degree-adjusted Random Walk for Relation Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In relation extraction, a key process is to obtain good detectors that find\nrelevant sentences describing the target relation. To minimize the necessity of\nlabeled data for refining detectors, previous work successfully made use of\nBabelNet, a semantic graph structure expressing relationships between synsets,\nas side information or prior knowledge. The goal of this paper is to enhance\nthe use of graph structure in the framework of random walk with a few\nadjustable parameters. Actually, a straightforward application of random walk\ndegrades the performance even after parameter optimization. With the insight\nfrom this unsuccessful trial, we propose SynsetRank, which adjusts the initial\nprobability so that high degree nodes influence the neighbors as strong as low\ndegree nodes. In our experiment on 13 relations in the FB15K-237 dataset,\nSynsetRank significantly outperforms baselines and the plain random walk\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:42:18 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 22:46:29 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Nakajima", "Shinichi", ""], ["Krause", "Sebastian", ""], ["Weissenborn", "Dirk", ""], ["Schmeier", "Sven", ""], ["Goernitz", "Nico", ""], ["Xu", "Feiyu", ""]]}, {"id": "1609.00639", "submitter": "Johannes Friedrich", "authors": "Johannes Friedrich, Pengcheng Zhou, Liam Paninski", "title": "Fast Online Deconvolution of Calcium Imaging Data", "comments": "Extended version that significantly elaborates on the conference\n  proceeding that appeared at NIPS 2016", "journal-ref": "PLOS Computational Biology 2017; 13(3): e1005423", "doi": "10.1371/journal.pcbi.1005423", "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescent calcium indicators are a popular means for observing the spiking\nactivity of large neuronal populations, but extracting the activity of each\nneuron from raw fluorescence calcium imaging data is a nontrivial problem. We\npresent a fast online active set method to solve this sparse non-negative\ndeconvolution problem. Importantly, the algorithm progresses through each time\nseries sequentially from beginning to end, thus enabling real-time online\nestimation of neural activity during the imaging session. Our algorithm is a\ngeneralization of the pool adjacent violators algorithm (PAVA) for isotonic\nregression and inherits its linear-time computational complexity. We gain\nremarkable increases in processing speed: more than one order of magnitude\ncompared to currently employed state of the art convex solvers relying on\ninterior point methods. Unlike these approaches, our method can exploit warm\nstarts; therefore optimizing model hyperparameters only requires a handful of\npasses through the data. A minor modification can further improve the quality\nof activity inference by imposing a constraint on the minimum spike size. The\nalgorithm enables real-time simultaneous deconvolution of $O(10^5)$ traces of\nwhole-brain larval zebrafish imaging data on a laptop.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 15:10:38 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 18:19:55 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 17:06:50 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Friedrich", "Johannes", ""], ["Zhou", "Pengcheng", ""], ["Paninski", "Liam", ""]]}, {"id": "1609.00677", "submitter": "Rakshith Jagannath", "authors": "Rakshith Jagannath", "title": "Detection and Estimation of Multiple DoA Targets with Single Snapshot\n  Measurements", "comments": "This version is contained in 1705.07561 and hence in redundant", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the problems of detecting the number of\nnarrow-band, far-field targets and estimating their corresponding directions of\narrivals (DoAs) from single snapshot measurements. We use the principles of\nsparse signal recovery (SSR) for detection and estimation of multiple targets.\nIn the SSR framework, the DoA estimation problem is grid based and can be posed\nas the lasso optimization problem. The corresponding DoA detection problem\nreduces to estimating the optimal regularization parameter ($\\tau$) of the\nlasso problem for achieving the required probability of correct detection\n($P_c$). We propose finite sample and asymptotic test statistics for detecting\nthe number of sources with the required $P_c$ at moderate to high signal to\nnoise ratios. Once the number of sources are detected, or equivalently the\noptimal $\\hat{\\tau}$ is estimated, the corresponding DoAs can be estimated by\nsolving the lasso with regularization parameter set to $\\hat{\\tau}$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:38:14 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 14:47:58 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 17:29:03 GMT"}, {"version": "v4", "created": "Mon, 11 Sep 2017 08:07:17 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Jagannath", "Rakshith", ""]]}, {"id": "1609.00689", "submitter": "Christina Lioma Assoc. Prof", "authors": "Niels Dalum Hansen and Christina Lioma and K{\\aa}re M{\\o}lbak", "title": "Ensemble Learned Vaccination Uptake Prediction using Web Search Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that uses ensemble learning to combine clinical and\nweb-mined time-series data in order to predict future vaccination uptake. The\nclinical data is official vaccination registries, and the web data is query\nfrequencies collected from Google Trends. Experiments with official vaccine\nrecords show that our method predicts vaccination uptake effectively (4.7 Root\nMean Squared Error). Whereas performance is best when combining clinical and\nweb data, using solely web data yields comparative performance. To our\nknowledge, this is the first study to predict vaccination uptake using web data\n(with and without clinical data).\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 18:23:22 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Hansen", "Niels Dalum", ""], ["Lioma", "Christina", ""], ["M\u00f8lbak", "K\u00e5re", ""]]}, {"id": "1609.00696", "submitter": "Scott Bruce", "authors": "Scott A. Bruce, Martica H. Hall, Daniel J. Buysse, and Robert T.\n  Krafty", "title": "Adaptive Bayesian Spectral Analysis of Nonstationary Biomedical Time\n  Series", "comments": "46 pages (not including title page), 6 figures at the end of the\n  article, 2 web supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies of biomedical time series signals aim to measure the association\nbetween frequency-domain properties of time series and clinical and behavioral\ncovariates. However, the time-varying dynamics of these associations are\nlargely ignored due to a lack of methods that can assess the changing nature of\nthe relationship through time. This article introduces a method for the\nsimultaneous and automatic analysis of the association between the time-varying\npower spectrum and covariates. The procedure adaptively partitions the grid of\ntime and covariate values into an unknown number of approximately stationary\nblocks and nonparametrically estimates local spectra within blocks through\npenalized splines. The approach is formulated in a fully Bayesian framework, in\nwhich the number and locations of partition points are random, and fit using\nreversible jump Markov chain Monte Carlo techniques. Estimation and inference\naveraged over the distribution of partitions allows for the accurate analysis\nof spectra with both smooth and abrupt changes. The proposed methodology is\nused to analyze the association between the time-varying spectrum of heart rate\nvariability and self-reported sleep quality in a study of older adults serving\nas the primary caregiver for their ill spouse.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 18:49:44 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 16:16:55 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bruce", "Scott A.", ""], ["Hall", "Martica H.", ""], ["Buysse", "Daniel J.", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1609.00711", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid and Robert Garthoff", "title": "Generalized Spatial and Spatiotemporal Autoregressive Conditional\n  Heteroscedasticity", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2018.07.005", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new spatial model that incorporates\nheteroscedastic variance depending on neighboring locations. The proposed\nprocess is regarded as the spatial equivalent to the temporal autoregressive\nconditional heteroscedasticity (ARCH) model. We show additionally how the\nintroduced spatial ARCH model can be used in spatiotemporal settings. In\ncontrast to the temporal ARCH model, in which the distribution is known given\nthe full information set of the prior periods, the distribution is not\nstraightforward in the spatial and spatiotemporal setting. However, it is\npossible to estimate the parameters of the model using the maximum-likelihood\napproach. Via Monte Carlo simulations, we demonstrate the performance of the\nestimator for a specific spatial weighting matrix. Moreover, we combine the\nknown spatial autoregressive model with the spatial ARCH model assuming\nheteroscedastic errors. Eventually, the proposed autoregressive process is\nillustrated using an empirical example. Specifically, we model lung cancer\nmortality in 3108 U.S. counties and compare the introduced model with two\nbenchmark approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 19:38:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""], ["Garthoff", "Robert", ""]]}, {"id": "1609.01176", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Victor Kristof, Antonio J. Gonz\\'alez Ferrer, Matthias\n  Grossglauser", "title": "The Player Kernel: Learning Team Strengths Based on Implicit Player\n  Contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we draw attention to a connection between skill-based models of\ngame outcomes and Gaussian process classification models. The Gaussian process\nperspective enables a) a principled way of dealing with uncertainty and b) rich\nmodels, specified through kernel functions. Using this connection, we tackle\nthe problem of predicting outcomes of football matches between national teams.\nWe develop a player kernel that relates any two football matches through the\nplayers lined up on the field. This makes it possible to share knowledge gained\nfrom observing matches between clubs (available in large quantities) and\nmatches between national teams (available only in limited quantities). We\nevaluate our approach on the Euro 2008, 2012 and 2016 final tournaments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 14:21:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Maystre", "Lucas", ""], ["Kristof", "Victor", ""], ["Ferrer", "Antonio J. Gonz\u00e1lez", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1609.01389", "submitter": "Rourab Paul", "authors": "Rourab Paul, Hemanta Dey, Amlan Chakrabarti, Ranjan Ghosh", "title": "Accelerating More Secure RC4 : Implementation of Seven FPGA Designs in\n  Stages upto 8 byte per clock", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RC4 can be made more secured if an additional RC4-like Post-KSA Random\nShuffing (PKRS) process is introduced between KSA and PRGA. It can also be made\nsignificantly faster if RC4 bytes are processed in a FPGA embedded system using\nmultiple coprocessors functioning in parallel. The PKRS process is tuned to\nform as many S-boxes as required by particular design architectures involving\nmultiple coprocessors, each one undertaking byte-by-byte processing. Following\na ecent idea [1] [2] the speed of execution of each processor is also enhanced\nby another fold if the byte-by-byte processing is replaced by a scheme of\nprocessing two consecutive bytes together. Adopting some new innovative\nconcepts, three hardware design architectures are proposed in a suitable FPGA\nembedded system involving 1, 2 and 4 coprocessors functioning in parallel and a\nstudy is made on accelerating RC4 by processing bytes in byte-by-byte mode\nachieving throughputs from 1-byte-in-1-clock to 4-bytes-in-1-clock. The\nhardware designs are appropriately upgraded to accelerate RC4 further by\nprocessing 2 onsecutive RC4 bytes together and it has been possible to achieve\na maximum throughput of 8-bytes per clock in Xilinx Virtex-5 LX110t FPGA [3]\narchitecture followed by secured data communication between two FPGA boards.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 04:37:04 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 18:08:24 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Paul", "Rourab", ""], ["Dey", "Hemanta", ""], ["Chakrabarti", "Amlan", ""], ["Ghosh", "Ranjan", ""]]}, {"id": "1609.01708", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco J. Rubio", "title": "Tractable Bayesian variable selection: beyond normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection often assumes normality, but the effects of model\nmisspecification are not sufficiently understood. There are sound reasons\nbehind this assumption, particularly for large $p$: ease of interpretation,\nanalytical and computational convenience. More flexible frameworks exist,\nincluding semi- or non-parametric models, often at the cost of some\ntractability. We propose a simple extension of the Normal model that allows for\nskewness and thicker-than-normal tails but preserves tractability. It leads to\neasy interpretation and a log-concave likelihood that facilitates optimization\nand integration. We characterize asymptotically parameter estimation and Bayes\nfactor rates, in particular studying the effects of model misspecification.\nUnder suitable conditions misspecified Bayes factors are consistent and induce\nsparsity at the same asymptotic rates than under the correct model. However,\nthe rates to detect signal are altered by an exponential factor, often\nresulting in a loss of sensitivity. These deficiencies can be ameliorated by\ninferring the error distribution from the data, a simple strategy that can\nimprove inference substantially. Our work focuses on the likelihood and can\nthus be combined with any likelihood penalty or prior, but here we focus on\nnon-local priors to induce extra sparsity and ameliorate finite-sample effects\ncaused by misspecification. Our results highlight the practical importance of\nfocusing on the likelihood rather than solely on the prior, when it comes to\nBayesian variable selection. The methodology is available in R package `mombf'.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 19:58:35 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 08:48:32 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 16:57:40 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco J.", ""]]}, {"id": "1609.02313", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, James Dziura, Floryt van Wesel", "title": "Pathophysiological Domains Underlying the Metabolic Syndrome: An\n  Alternative Factor Analytic Strategy", "comments": "Postprint, 41 pages, includes supplementary material", "journal-ref": "Annals of Epidemiology, 24 (2014): 762-770", "doi": "10.1016/j.annepidem.2014.07.012", "report-no": null, "categories": "stat.AP q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Factor analysis (FA) has become part and parcel in metabolic\nsyndrome (MBS) research. Both exploration- and confirmation-driven factor\nanalyzes are rampant. However, factor analytic results on MBS differ widely. A\nsituation that is at least in part attributable to misapplication of FA. Here,\nour purpose is (i) to review factor analytic efforts in the study of MBS with\nemphasis on misusage of the FA model and (ii) to propose an alternative factor\nanalytic strategy.\n  Methods: The proposed factor analytic strategy consists of four steps and\nconfronts weaknesses in application of the FA model. At its heart lies the\nexplicit separation of dimensionality and pattern selection as well as the\ndirect evaluation of competing inequality-constrained loading patterns. A\nhigh-profile MBS data set with anthropometric measurements on overweight\nchildren and adolescents is reanalyzed using this strategy.\n  Results: The reanalysis implied a more parsimonious constellation of\npathophysiological domains underlying phenotypic expressions of MBS than the\noriginal analysis (and many other analyzes). The results emphasize correlated\nfactors of impaired glucose metabolism and impaired lipid metabolism.\n  Conclusions: Pathophysiological domains underlying phenotypic expressions of\nMBS included in the analysis are driven by multiple interrelated metabolic\nimpairments. These findings indirectly point to the possible existence of a\nmultifactorial aetiology.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 08:06:22 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["Dziura", "James", ""], ["van Wesel", "Floryt", ""]]}, {"id": "1609.02354", "submitter": "Leopoldo Catania", "authors": "David Ardia, Kris Boudt, Leopoldo Catania", "title": "Generalized Autoregressive Score Models in R: The GAS Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package GAS for the analysis of time series under\nthe Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and\nHarvey (2013). The distinctive feature of the GAS approach is the use of the\nscore function as the driver of time-variation in the parameters of nonlinear\nmodels. The GAS package provides functions to simulate univariate and\nmultivariate GAS processes, estimate the GAS parameters and to make time series\nforecasts. We illustrate the use of the GAS package with a detailed case study\non estimating the time-varying conditional densities of a set of financial\nassets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 09:40:44 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Ardia", "David", ""], ["Boudt", "Kris", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1609.02369", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "Stochastic Tail Exponent For Asymmetric Power Laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine random variables in the power law/regularly varying class with\nstochastic tail exponent, the exponent $\\alpha$ having its own distribution. We\nshow the effect of stochasticity of $\\alpha$ on the expectation and higher\nmoments of the random variable. For instance, the moments of a right-tailed or\nright-asymmetric variable, when finite, increase with the variance of $\\alpha$;\nthose of a left-asymmetric one decreases. The same applies to conditional\nshortfall (CVar), or mean-excess functions. We prove the general case and\nexamine the specific situation of lognormally distributed $\\alpha \\in\n[b,\\infty), b>1$. The stochasticity of the exponent induces a significant bias\nin the estimation of the mean and higher moments in the presence of data\nuncertainty. This has consequences on sampling error as uncertainty about\n$\\alpha$ translates into a higher expected mean. The bias is conserved under\nsummation, even upon large enough a number of summands to warrant convergence\nto the stable distribution. We establish inequalities related to the asymmetry.\nWe also consider the situation of capped power laws (i.e. with compact\nsupport), and apply it to the study of violence by Cirillo and Taleb (2016). We\nshow that uncertainty concerning the historical data increases the true mean.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 10:36:02 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 18:52:23 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 15:22:38 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1609.02409", "submitter": "Kardi Teknomo", "authors": "John Boaz Lee and Kardi Teknomo", "title": "Comparison of several short-term traffic speed forecasting models", "comments": "6 pages, Lee, J. B. and Teknomo, K. (2014) A review of various\n  short-term traffic speed forecasting models, Proceeding of the 12th National\n  Conference in Information Technology Education (NCITE 2014), October 23 - 25,\n  2014, Boracay, Philippines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of smartphones in recent years has made it possible\nfor us to collect large amounts of traffic data. Special software installed on\nthe phones of drivers allow us to gather GPS trajectories of their vehicles on\nthe road network. In this paper, we simulate the trajectories of multiple\nagents on a road network and use various models to forecast the short-term\ntraffic speed of various links. Our results show that traditional techniques\nlike multiple regression and artificial neural networks work well but simpler\nadaptive models that do not require prior training also perform comparatively\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:30:37 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Lee", "John Boaz", ""], ["Teknomo", "Kardi", ""]]}, {"id": "1609.02629", "submitter": "Wesley Lee", "authors": "Wesley Lee, Bailey K. Fosdick, and Tyler H. McCormick", "title": "Inferring social structure from continuous-time interaction data", "comments": null, "journal-ref": "Applied Stochastic Models in Business and Industry 2018, Vol. 34,\n  No. 2, 87-104", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational event data, which consist of events involving pairs of actors over\ntime, are now commonly available at the finest of temporal resolutions.\nExisting continuous-time methods for modeling such data are based on point\nprocesses and directly model interaction \"contagion,\" whereby one interaction\nincreases the propensity of future interactions among actors, often as dictated\nby some latent variable structure. In this article, we present an alternative\napproach to using temporal-relational point process models for continuous-time\nevent data. We characterize interactions between a pair of actors as either\nspurious or that resulting from an underlying, persistent connection in a\nlatent social network. We argue that consistent deviations from expected\nbehavior, rather than solely high frequency counts, are crucial for identifying\nwell-established underlying social relationships. This study aims to explore\nthese latent network structures in two contexts: one comprising of college\nstudents and another involving barn swallows.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 01:08:49 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 23:45:00 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lee", "Wesley", ""], ["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1609.02688", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (ENSAI, IRMAR), Anne Ruiz-Gazen (TSE)", "title": "A comparison of pivotal sampling and unequal probability sampling with\n  replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any implementation of pivotal sampling is more efficient than\nmultinomial sampling. This property entails the weak consistency of the\nHorvitz-Thompson estimator and the existence of a conservative variance\nestimator. A small simulation study supports our findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:23:26 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 14:29:33 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Chauvet", "Guillaume", "", "ENSAI, IRMAR"], ["Ruiz-Gazen", "Anne", "", "TSE"]]}, {"id": "1609.02700", "submitter": "Clement Chevalier", "authors": "S\\'ebastien Marmin (IMSV, I2M), Cl\\'ement Chevalier, David Ginsbourger\n  (IMSV)", "title": "Efficient batch-sequential Bayesian optimization with moments of\n  truncated Gaussian vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the efficient parallelization of Bayesian global optimization\nalgorithms, and more specifically of those based on the expected improvement\ncriterion and its variants. A closed form formula relying on multivariate\nGaussian cumulative distribution functions is established for a generalized\nversion of the multipoint expected improvement criterion. In turn, the latter\nrelies on intermediate results that could be of independent interest concerning\nmoments of truncated Gaussian vectors. The obtained expansion of the criterion\nenables studying its differentiability with respect to point batches and\ncalculating the corresponding gradient in closed form. Furthermore , we derive\nfast numerical approximations of this gradient and propose efficient batch\noptimization strategies. Numerical experiments illustrate that the proposed\napproaches enable computational savings of between one and two order of\nmagnitudes, hence enabling derivative-based batch-sequential acquisition\nfunction maximization to become a practically implementable and efficient\nstandard.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:42:12 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Marmin", "S\u00e9bastien", "", "IMSV, I2M"], ["Chevalier", "Cl\u00e9ment", "", "IMSV"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1609.02793", "submitter": "Eszter Kiraly-Proag", "authors": "Eszter Kiraly-Proag, J. Douglas Zechar, Valentin Gischig, Stefan\n  Wiemer, Dimitrios Karvounis, Joseph Doetsch", "title": "Validating induced seismicity forecast models - Induced Seismicity Test\n  Bench", "comments": "http://onlinelibrary.wiley.com/doi/10.1002/2016JB013236/abstract", "journal-ref": null, "doi": "10.1002/2016JB013236", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induced earthquakes often accompany fluid injection, and the seismic hazard\nthey pose threatens various underground engineering projects. Models to monitor\nand control induced seismic hazard with traffic light systems should be\nprobabilistic, forward-looking, and updated as new data arrive. In this study,\nwe propose an Induced Seismicity Test Bench to test and rank such models; this\ntest bench can be used for model development, model selection, and ensemble\nmodel building. We apply the test bench to data from the Basel 2006 and\nSoultz-sous-For\\^ets 2004 geothermal stimulation projects, and we assess\nforecasts from two models: Shapiro and Smoothed Seismicity (SaSS) and\nHydraulics and Seismics (HySei). These models incorporate a different mix of\nphysics-based elements and stochastic representation of the induced sequences.\nOur results show that neither model is fully superior to the other. Generally,\nHySei forecasts the seismicity rate better after shut-in, but is only mediocre\nat forecasting the spatial distribution. On the other hand, SaSS forecasts the\nspatial distribution better and gives better seismicity rate estimates before\nshut-in. The shut-in phase is a difficult moment for both models in both\nreservoirs: the models tend to underpredict the seismicity rate around, and\nshortly after, shut-in.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 13:47:35 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Kiraly-Proag", "Eszter", ""], ["Zechar", "J. Douglas", ""], ["Gischig", "Valentin", ""], ["Wiemer", "Stefan", ""], ["Karvounis", "Dimitrios", ""], ["Doetsch", "Joseph", ""]]}, {"id": "1609.02938", "submitter": "Hau-tieng Wu", "authors": "Su Li, Hau-tieng Wu", "title": "Extract fetal ECG from single-lead abdominal ECG by de-shape short time\n  Fourier transform and nonlocal median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple fundamental frequency detection problem and the source\nseparation problem from a single-channel signal containing multiple oscillatory\ncomponents and a nonstationary noise are both challenging tasks. To extract the\nfetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we\nface both challenges. In this paper, we propose a novel method to extract the\nfetal ECG signal from the single channel maternal abdominal ECG signal, without\nany additional measurement. The algorithm is composed of three main\ningredients. First, the maternal and fetal heart rates are estimated by the\nde-shape short time Fourier transform, which is a recently proposed nonlinear\ntime-frequency analysis technique; second, the beat tracking technique is\napplied to accurately obtain the maternal and fetal R peaks; third, the\nmaternal and fetal ECG waveforms are established by the nonlocal median. The\nalgorithm is evaluated on a simulated fetal ECG signal database ({\\em fecgsyn}\ndatabase), and tested on two real databases with the annotation provided by\nexperts ({\\em adfecgdb} database and {\\em CinC2013} database). In general, the\nalgorithm could be applied to solve other detection and source separation\nproblems, and reconstruct the time-varying wave-shape function of each\noscillatory component.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:26:31 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Li", "Su", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1609.03229", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis", "title": "The Anatomy of the Three-Point Shot: Spatial Bias, Fractals and the\n  Three-Point Line in the NBA", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though it might have taken some time, the three-point line ultimately\nchanged the way the game is played as evidenced by the increase in the\nthree-point shot attempts over the years. However, during the last few years we\nhave experienced record-breaking seasons in terms of both three-point attempts\nand field goals made. This brings back to the surface questions such as \"What\nis the rationale behind the three-point line?\", \"Is the three-point shot\ndistance appropriate?\" and many more similar questions. In this work, even\nthough we do not take a stand against the three-point line, we provide evidence\nthat challenge its distance. In particular, we analyze shot charts and we\nidentify a statistically significant discontinuity in the shot attempts between\n1-feet zones just inside and outside the three-point line. In addition we\nintroduce a metric inspired by fractal theory to quantify this bias and our\nresults clearly indicate that the space dimensionality in these areas of the\ncourt is not fully exploited. By further examining the field goal percentages\nin the zones considered, we do not identify a similar discontinuity, i.e., the\nfield goal percentage just inside the three-point line and just outside the\nline are statistically identical. Therefore, even though the shooting behavior\nof the teams appears to be rational, it raises important questions about the\nrationality of the three-point line itself.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 23:19:01 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pelechrinis", "Konstantinos", ""]]}, {"id": "1609.03257", "submitter": "Nathan Baker", "authors": "Luke J. Gosink, Christopher C. Overall, Sarah M. Reehl, Paul D.\n  Whitney, David L. Mobley, Nathan A. Baker", "title": "Bayesian Model Averaging for Ensemble-Based Estimates of Solvation Free\n  Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies the Bayesian Model Averaging (BMA) statistical ensemble\ntechnique to estimate small molecule solvation free energies. There is a wide\nrange of methods available for predicting solvation free energies, ranging from\nempirical statistical models to ab initio quantum mechanical approaches. Each\nof these methods is based on a set of conceptual assumptions that can affect\npredictive accuracy and transferability. Using an iterative statistical\nprocess, we have selected and combined solvation energy estimates using an\nensemble of 17 diverse methods from the fourth Statistical Assessment of\nModeling of Proteins and Ligands (SAMPL) blind prediction study to form a\nsingle, aggregated solvation energy estimate. The ensemble design process\nevaluates the statistical information in each individual method as well as the\nperformance of the aggregate estimate obtained from the ensemble as a whole.\nMethods that possess minimal or redundant information are pruned from the\nensemble and the evaluation process repeats until aggregate predictive\nperformance can no longer be improved. We show that this process results in a\nfinal aggregate estimate that outperforms all individual methods by reducing\nestimate errors by as much as 91% to 1.2 kcal/mol accuracy. We also compare our\niterative refinement approach to other statistical ensemble approaches and\ndemonstrate that this iterative process reduces estimate errors by as much as\n61%. This work provides a new approach for accurate solvation free energy\nprediction and lays the foundation for future work on aggregate models that can\nbalance computational cost with prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 03:21:11 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 02:40:35 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Gosink", "Luke J.", ""], ["Overall", "Christopher C.", ""], ["Reehl", "Sarah M.", ""], ["Whitney", "Paul D.", ""], ["Mobley", "David L.", ""], ["Baker", "Nathan A.", ""]]}, {"id": "1609.03297", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner H. Bonat and C\\'elestin C. Kokonendji", "title": "Flexible Tweedie regression models for continuous data", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": "10.1080/00949655.2017.1318876", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie regression models provide a flexible family of distributions to deal\nwith non-negative highly right-skewed data as well as symmetric and heavy\ntailed data and can handle continuous data with probability mass at zero. The\nestimation and inference of Tweedie regression models based on the maximum\nlikelihood method are challenged by the presence of an infinity sum in the\nprobability function and non-trivial restrictions on the power parameter space.\nIn this paper, we propose two approaches for fitting Tweedie regression models,\nnamely, quasi- and pseudo-likelihood. We discuss the asymptotic properties of\nthe two approaches and perform simulation studies to compare our methods with\nthe maximum likelihood method. In particular, we show that the quasi-likelihood\nmethod provides asymptotically efficient estimation for regression parameters.\nThe computational implementation of the alternative methods is faster and\neasier than the orthodox maximum likelihood, relying on a simple Newton scoring\nalgorithm. Simulation studies showed that the quasi- and pseudo-likelihood\napproaches present estimates, standard errors and coverage rates similar to the\nmaximum likelihood method. Furthermore, the second-moment assumptions required\nby the quasi- and pseudo-likelihood methods enables us to extend the Tweedie\nregression models to the class of quasi-Tweedie regression models in the\nWedderburn's style. Moreover, it allows to eliminate the non-trivial\nrestriction on the power parameter space, and thus provides a flexible\nregression model to deal with continuous data. We provide \\texttt{R}\nimplementation and illustrate the application of Tweedie regression models\nusing three data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 08:04:38 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner H.", ""], ["Kokonendji", "C\u00e9lestin C.", ""]]}, {"id": "1609.03301", "submitter": "Lei Chu", "authors": "Lei Chu, Robert Qiu, Xing He, Zenan Ling, Yadong Liu", "title": "Massive Streaming PMU Data Modeling and Analytics in Smart Grid State\n  Evaluation Based on Multiple High-Dimensional Covariance Tests", "comments": "IEEE, transations on Big Data, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analogous deployment of phase measurement units (PMUs), the increase of\ndata quantum and the deregulation of energy market, all call for the robust\nstate evaluation in large scale power systems. Implementing model based\nestimators is impractical because of the complexity scale of solving the high\ndimension power flow equations. In this paper, we first represent massive\nstreaming PMU data as big random matrix flow. By exploiting the variations in\nthe covariance matrix of the massive streaming PMU data, a novel power state\nevaluation algorithm is then developed based on the multiple high dimensional\ncovariance matrix tests. The proposed test statistic is flexible and\nnonparametric, which assumes no specific parameter distribution or dimension\nstructure for the PMU data. Besides, it can jointly reveal the relative\nmagnitude, duration and location of a system event. For the sake of practical\napplication, we reduce the computation of the proposed test statistic from\n$O(\\varepsilon n_g^4)$ to $O(\\eta n_g^2)$ by principal component calculation\nand redundant computation elimination. The novel algorithm is numerically\nevaluated utilizing the IEEE 30-, 118-bus system, a Polish 2383-bus system, and\na real 34-PMU system. The case studies illustrate and verify the superiority of\nproposed state evaluation indicator.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 08:32:19 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 06:04:35 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 15:36:30 GMT"}, {"version": "v4", "created": "Tue, 20 Jun 2017 02:59:21 GMT"}, {"version": "v5", "created": "Thu, 22 Jun 2017 08:01:20 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Chu", "Lei", ""], ["Qiu", "Robert", ""], ["He", "Xing", ""], ["Ling", "Zenan", ""], ["Liu", "Yadong", ""]]}, {"id": "1609.03400", "submitter": "H\\'el\\`ene Ruffieux", "authors": "H\\'el\\`ene Ruffieux, Anthony C. Davison, J\\\"org Hager, Irina\n  Irincheeva", "title": "Efficient inference for genetic association studies with multiple\n  outcomes", "comments": null, "journal-ref": "Biostatistics, 2017", "doi": "10.1093/biostatistics/kxx007", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combined inference for heterogeneous high-dimensional data is critical in\nmodern biology, where clinical and various kinds of molecular data may be\navailable from a single study. Classical genetic association studies regress a\nsingle clinical outcome on many genetic variants one by one, but there is an\nincreasing demand for joint analysis of many molecular outcomes and genetic\nvariants in order to unravel functional interactions. Unfortunately, most\nexisting approaches to joint modelling are either too simplistic to be powerful\nor are impracticable for computational reasons. Inspired by Richardson et al.\n(2010, Bayesian Statistics 9), we consider a sparse multivariate regression\nmodel that allows simultaneous selection of predictors and associated\nresponses. As Markov chain Monte Carlo (MCMC) inference on such models can be\nprohibitively slow when the number of genetic variants exceeds a few thousand,\nwe propose a variational inference approach which produces posterior\ninformation very close to that of MCMC inference, at a much reduced\ncomputational cost. Extensive numerical experiments show that our approach\noutperforms popular variable selection methods and tailored Bayesian\nprocedures, dealing within hours with problems involving hundreds of thousands\nof genetic variants and tens to hundreds of clinical or molecular outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 13:43:37 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 15:47:09 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 20:15:05 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Ruffieux", "H\u00e9l\u00e8ne", ""], ["Davison", "Anthony C.", ""], ["Hager", "J\u00f6rg", ""], ["Irincheeva", "Irina", ""]]}, {"id": "1609.03439", "submitter": "Eleni-Rosalina Andrinopoulou", "authors": "Eleni-Rosalina Andrinopoulou, Paul H.C. Eilers, Johanna J.M.\n  Takkenberg and Dimitris Rizopoulos", "title": "Improved Dynamic Predictions from Joint Models of Longitudinal and\n  Survival Data with Time-Varying Effects using P-splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of cardio-thoracic surgery, valve function is monitored over\ntime after surgery. The motivation for our research comes from a study which\nincludes patients who received a human tissue valve in the aortic position.\nThese patients are followed prospectively over time by standardized\nechocardiographic assessment of valve function. Loss of follow-up could be\ncaused by valve intervention or the death of the patient. One of the main\ncharacteristics of the human valve is that its durability is limited.\nTherefore, it is of interest to obtain a prognostic model in order for the\nphysicians to scan trends in valve function over time and plan their next\nintervention, accounting for the characteristics of the data.\n  Several authors have focused on deriving predictions under the standard joint\nmodeling of longitudinal and survival data framework that assumes a constant\neffect for the coefficient that links the longitudinal and survival outcomes.\nHowever, in our case this may be a restrictive assumption. Since the valve\ndegenerates, the association between the biomarker with survival may change\nover time.\n  To improve dynamic predictions we propose a Bayesian joint model that allows\na time-varying coefficient to link the longitudinal and the survival processes,\nusing P-splines. We evaluate the performance of the model in terms of\ndiscrimination and calibration, while accounting for censoring.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 15:12:12 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 09:58:48 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Andrinopoulou", "Eleni-Rosalina", ""], ["Eilers", "Paul H. C.", ""], ["Takkenberg", "Johanna J. M.", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1609.03758", "submitter": "Anirudh Acharya", "authors": "Anirudh Acharya, Madalin Guta", "title": "Statistical analysis of low rank tomography with compressive random\n  measurements", "comments": "19 pages, 3 figures", "journal-ref": "J. Phys. A: Math. Theor. 50 195301 (2017)", "doi": "10.1088/1751-8121/aa682e", "report-no": null, "categories": "quant-ph math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical problem of `compressive' estimation of low rank\nstates with random basis measurements, where the estimation error is expressed\nterms of two metrics - the Frobenius norm and quantum infidelity. It is known\nthat unlike the case of general full state tomography, low rank states can be\nidentified from a reduced number of observables' expectations. Here we\ninvestigate whether for a fixed sample size $N$, the estimation error\nassociated to a `compressive' measurement setup is `close' to that of the\nsetting where a large number of bases are measured. In terms of the Frobenius\nnorm, we demonstrate that for all states the error attains the optimal rate\n$rd/N$ with only $O(r \\log{d})$ random basis measurements. We provide an\nillustrative example of a single qubit and demonstrate a concentration in the\nFrobenius error about its optimal for all qubit states. In terms of the quantum\ninfidelity, we show that such a concentration does not exist uniformly over all\nstates. Specifically, we show that for states that are nearly pure and close to\nthe surface of the Bloch sphere, the mean infidelity scales as $1/\\sqrt{N}$ but\nthe constant converges to zero as the number of settings is increased. This\ndemonstrates a lack of `compressive' recovery for nearly pure states in this\nmetric.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 10:36:39 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Acharya", "Anirudh", ""], ["Guta", "Madalin", ""]]}, {"id": "1609.03893", "submitter": "Yu Jin", "authors": "Yu Jin, Joseph F. JaJa, Rong Chen, Edward H. Herskovits", "title": "Scalable Algorithms for Generating and Analyzing Structural Brain\n  Networks with a Varying Number of Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Magnetic Resonance Imaging (MRI) exploits the anisotropic diffusion\nof water molecules in the brain to enable the estimation of the brain's\nanatomical fiber tracts at a relatively high resolution. In particular,\ntractographic methods can be used to generate whole-brain anatomical\nconnectivity matrix where each element provides an estimate of the connectivity\nstrength between the corresponding voxels. Structural brain networks are built\nusing the connectivity information and a predefined brain parcellation, where\nthe nodes of the network represent the brain regions and the edge weights\ncapture the connectivity strengths between the corresponding brain regions.\nThis paper introduces a number of novel scalable methods to generate and\nanalyze structural brain networks with a varying number of nodes. In\nparticular, we introduce a new parallel algorithm to quickly generate large\nscale connectivity-based parcellations for which voxels in a region possess\nhighly similar connectivity patterns to the rest of the regions. We show that\nthe corresponding regional structural consistency is always superior to\nrandomly generated parcellations over a wide range of parcellation sizes.\nCorresponding brain networks with a varying number of nodes are analyzed using\nstandard graph-theorectic measures, as well as, new measures derived from\nspectral graph theory. Our results indicate increasingly more statistical power\nof brain networks with larger numbers of nodes and the relatively unique shape\nof the spectral profile of large brain networks relative to other well-known\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 15:17:58 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Jin", "Yu", ""], ["JaJa", "Joseph F.", ""], ["Chen", "Rong", ""], ["Herskovits", "Edward H.", ""]]}, {"id": "1609.04078", "submitter": "Brendon Brewer", "authors": "Oliver G. Stevenson and Brendon J. Brewer", "title": "Bayesian survival analysis of batsmen in Test cricket", "comments": "Accepted for publication in the Journal of Quantitative Analysis in\n  Sports. 21 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cricketing knowledge tells us batting is more difficult early in a player's\ninnings but becomes easier as a player familiarizes themselves with the\nconditions. In this paper, we develop a Bayesian survival analysis method to\npredict the Test Match batting abilities for international cricketers. The\nmodel is applied in two stages, firstly to individual players, allowing us to\nquantify players' initial and equilibrium batting abilities, and the rate of\ntransition between the two. This is followed by implementing the model using a\nhierarchical structure, providing us with more general inference concerning a\nselected group of opening batsmen from New Zealand. The results indicate most\nplayers begin their innings playing with between only a quarter and half of\ntheir potential batting ability. Using the hierarchical structure we are able\nto make predictions for the batting abilities of the next opening batsman to\ndebut for New Zealand. Additionally, we compare and identify players who excel\nin the role of opening the batting, which has practical implications in terms\nof batting order and team selection policy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 00:39:26 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 09:14:18 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Stevenson", "Oliver G.", ""], ["Brewer", "Brendon J.", ""]]}, {"id": "1609.04103", "submitter": "Lei Han", "authors": "Lei Han, Juanzhen Sun, Wei Zhang, Yuanyuan Xiu, Hailei Feng, Yinjing\n  Lin", "title": "A Machine Learning Nowcasting Method based on Real-time Reanalysis Data", "comments": "22 pages, 11 figures, submitted to Journal of Geophysical Research:\n  Atmospheres", "journal-ref": "J. Geophys. Res. Atmos., 122, (2017)", "doi": "10.1002/2016JD025783", "report-no": null, "categories": "physics.ao-ph cs.CV physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite marked progress over the past several decades, convective storm\nnowcasting remains a challenge because most nowcasting systems are based on\nlinear extrapolation of radar reflectivity without much consideration for other\nmeteorological fields. The variational Doppler radar analysis system (VDRAS) is\nan advanced convective-scale analysis system capable of providing analysis of\n3-D wind, temperature, and humidity by assimilating Doppler radar observations.\nAlthough potentially useful, it is still an open question as to how to use\nthese fields to improve nowcasting. In this study, we present results from our\nfirst attempt at developing a Support Vector Machine (SVM) Box-based nOWcasting\n(SBOW) method under the machine learning framework using VDRAS analysis data.\nThe key design points of SBOW are as follows: 1) The study domain is divided\ninto many position-fixed small boxes and the nowcasting problem is transformed\ninto one question, i.e., will a radar echo > 35 dBZ appear in a box in 30\nminutes? 2) Box-based temporal and spatial features, which include time trends\nand surrounding environmental information, are elaborately constructed, and 3)\nThe box-based constructed features are used to first train the SVM classifier,\nand then the trained classifier is used to make predictions. Compared with\ncomplicated and expensive expert systems, the above design of SBOW allows the\nsystem to be small, compact, straightforward, and easy to maintain and expand\nat low cost. The experimental results show that, although no complicated\ntracking algorithm is used, SBOW can predict the storm movement trend and storm\ngrowth with reasonable skill.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:01:29 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 06:39:31 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Han", "Lei", ""], ["Sun", "Juanzhen", ""], ["Zhang", "Wei", ""], ["Xiu", "Yuanyuan", ""], ["Feng", "Hailei", ""], ["Lin", "Yinjing", ""]]}, {"id": "1609.04156", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Lourens J. Waldorp, Ren\\'e M\\~ottus, Denny Borsboom", "title": "The Gaussian Graphical Model in Cross-sectional and Time-series Data", "comments": "Accepted pending revision in Multivariate Behavioral Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the Gaussian graphical model (GGM; an undirected network of\npartial correlation coefficients) and detail its utility as an exploratory data\nanalysis tool. The GGM shows which variables predict one-another, allows for\nsparse modeling of covariance structures, and may highlight potential causal\nrelationships between observed variables. We describe the utility in 3 kinds of\npsychological datasets: datasets in which consecutive cases are assumed\nindependent (e.g., cross-sectional data), temporally ordered datasets (e.g., n\n= 1 time series), and a mixture of the 2 (e.g., n > 1 time series). In\ntime-series analysis, the GGM can be used to model the residual structure of a\nvector-autoregression analysis (VAR), also termed graphical VAR. Two network\nmodels can then be obtained: a temporal network and a contemporaneous network.\nWhen analyzing data from multiple subjects, a GGM can also be formed on the\ncovariance structure of stationary means---the between-subjects network. We\ndiscuss the interpretation of these models and propose estimation methods to\nobtain these networks, which we implement in the R packages graphicalVAR and\nmlVAR. The methods are showcased in two empirical examples, and simulation\nstudies on these methods are included in the supplementary materials.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 07:41:34 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 16:01:35 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 08:47:24 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 11:11:04 GMT"}, {"version": "v5", "created": "Fri, 15 Sep 2017 22:07:17 GMT"}, {"version": "v6", "created": "Thu, 8 Feb 2018 14:25:38 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Epskamp", "Sacha", ""], ["Waldorp", "Lourens J.", ""], ["M\u00f5ttus", "Ren\u00e9", ""], ["Borsboom", "Denny", ""]]}, {"id": "1609.04222", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Rob J Hyndman", "title": "Grouped functional time series forecasting: An application to\n  age-specific mortality rates", "comments": "31 pages, 5 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-specific mortality rates are often disaggregated by different attributes,\nsuch as sex, state and ethnicity. Forecasting age-specific mortality rates at\nthe national and sub-national levels plays an important role in developing\nsocial policy. However, independent forecasts at the sub-national levels may\nnot add up to the forecasts at the national level. To address this issue, we\nconsider reconciling forecasts of age-specific mortality rates, extending the\nmethods of Hyndman et al. (2011) to functional time series, where age is\nconsidered as a continuum. The grouped functional time series methods are used\nto produce point forecasts of mortality rates that are aggregated appropriately\nacross different disaggregation factors. For evaluating forecast uncertainty,\nwe propose a bootstrap method for reconciling interval forecasts. Using the\nregional age-specific mortality rates in Japan, obtained from the Japanese\nMortality Database, we investigate the one- to ten-step-ahead point and\ninterval forecast accuracies between the independent and grouped functional\ntime series forecasting methods. The proposed methods are shown to be useful\nfor reconciling forecasts of age-specific mortality rates at the national and\nsub-national levels. They also enjoy improved forecast accuracy averaged over\ndifferent disaggregation factors. Supplemental materials for the article are\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 11:32:08 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Shang", "Han Lin", ""], ["Hyndman", "Rob J", ""]]}, {"id": "1609.04231", "submitter": "Jia Guo", "authors": "Jia Guo and Jin-Ting Zhang", "title": "A Further Study of an $L^2$-norm Based Test for the Equality of Several\n  Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the multi-sample equal covariance function (ECF) testing problem, Zhang\n(2013) proposed an $L^{2}$-norm based test. However, its asymptotic power and\nfinite sample performance have not been studied. In this paper, its asymptotic\npower is investigated under some mild conditions. It is shown that the\n$L^2$-norm based test is root-$n$ consistent. In addition, intensive simulation\nstudies demonstrate that in terms of size-controlling and power, the\n$L^{2}$-norm based test outperforms the dimension-reduction based test proposed\nby Fremdt et al. (2013) when the functional data are less correlated or when\nthe effective signal information is located in high frequencies. Two real data\napplications are also presented to demonstrate the good performance of the\n$L^2$-norm based test.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:13:07 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guo", "Jia", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04232", "submitter": "Jia Guo", "authors": "Jia Guo, Bu Zhou and Jin-Ting Zhang", "title": "A Supremum-Norm Based Test for the Equality of Several Covariance\n  Functions", "comments": "arXiv admin note: text overlap with arXiv:1609.04231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new test for the equality of several covariance\nfunctions for functional data. Its test statistic is taken as the supremum\nvalue of the sum of the squared differences between the estimated individual\ncovariance functions and the pooled sample covariance function, hoping to\nobtain a more powerful test than some existing tests for the same testing\nproblem. The asymptotic random expression of this test statistic under the null\nhypothesis is obtained. To approximate the null distribution of the proposed\ntest statistic, we describe a parametric bootstrap method and a non-parametric\nbootstrap method. The asymptotic random expression of the proposed test is also\nstudied under a local alternative and it is shown that the proposed test is\nroot-$n$ consistent. Intensive simulation studies are conducted to demonstrate\nthe finite sample performance of the proposed test and it turns out that the\nproposed test is indeed more powerful than some existing tests when functional\ndata are highly correlated. The proposed test is illustrated with three real\ndata examples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:15:14 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Guo", "Jia", ""], ["Zhou", "Bu", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04234", "submitter": "Jia Guo", "authors": "Jia Guo and Jin-Ting Zhang", "title": "Two New Tests for Equality of Several Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two new tests for testing the equality of the\ncovariance functions of several functional populations, namely a quasi GPF test\nand a quasi $F_{\\max}$ test. The asymptotic random expressions of the two tests\nunder the null hypothesis are derived. We show that the asymptotic null\ndistribution of the quasi GPF test is a chi-squared-type mixture whose\ndistribution can be well approximated by a simple scaled chi-squared\ndistribution. We also adopt a random permutation method for approximating the\nnull distributions of the quasi GPF and $F_{\\max}$ tests. The random\npermutation method is applicable for both large and finite sample sizes. The\nasymptotic distributions of the two tests under a local alternative are\ninvestigated and they are shown to be root-n consistent. Simulation studies are\npresented to demonstrate the finite-sample performance of the new tests against\nthree existing tests. They show that our new tests are more powerful than the\nthree existing tests when the covariance functions at different time points\nhave different scales. An illustrative example is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:16:32 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guo", "Jia", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04282", "submitter": "Mark Levene", "authors": "Trevor Fenner, Mark Levene, and George Loizou", "title": "A multiplicative process for generating the rank-order distribution of\n  UK election results", "comments": "12 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human dynamics and sociophysics suggest statistical models that may explain\nand provide us with a better understanding of social phenomena. Here we propose\na generative multiplicative decrease model that gives rise to a rank-order\ndistribution and allows us to analyse the results of the last three UK\nparliamentary elections. We provide empirical evidence that the additive\nWeibull distribution, which can be generated from our model, is a close fit to\nthe electoral data, offering a novel interpretation of the recent election\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 14:15:58 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Fenner", "Trevor", ""], ["Levene", "Mark", ""], ["Loizou", "George", ""]]}, {"id": "1609.04383", "submitter": "David Sharrow", "authors": "David J. Sharrow, Jessica Godwin, Yanjun He, Samuel J. Clark, Adrian\n  E. Raftery", "title": "Probabilistic Population Projections for Countries with Generalized\n  HIV/AIDS Epidemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations (UN) issued official probabilistic population projections\nfor all countries to 2100 in July 2015. This was done by simulating future\nlevels of total fertility and life expectancy from Bayesian hierarchical\nmodels, and combining the results using a standard cohort-component projection\nmethod. The 40 countries with generalized HIV/AIDS epidemics were treated\ndifferently from others, in that the projections used the highly multistate\nSpectrum/EPP model, a complex 15-compartment model that was designed for\nshort-term projections of quantities relevant to policy for the epidemic. Here\nwe propose a simpler approach that is more compatible with the existing UN\nprobabilistic projection methodology for other countries. Changes in life\nexpectancy are projected probabilistically using a simple time series\nregression model on current life expectancy, HIV prevalence and ART coverage.\nThese are then converted to age- and sex-specific mortality rates using a new\nfamily of model life tables designed for countries with HIV/AIDS epidemics that\nreproduces the characteristic hump in middle adult mortality. These are then\ninput to the standard cohort-component method, as for other countries. The\nmethod performed well in an out-of-sample cross-validation experiment. It gives\nsimilar population projections to Spectrum/EPP in the short run, while being\nsimpler and avoiding multistate modeling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:11:03 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Sharrow", "David J.", ""], ["Godwin", "Jessica", ""], ["He", "Yanjun", ""], ["Clark", "Samuel J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1609.04466", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Joyce C. Ho, Joydeep Ghosh, Stephanie Kreml, Abel N\n  Kho, Joshua C Denny, Bradley A Malin, Jimeng Sun", "title": "Phenotyping using Structured Collective Matrix Factorization of\n  Multi--source EHR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of electronic health records (EHRs) have\nspearheaded the initiative for precision medicine using data driven approaches.\nEssential to this effort is the ability to identify patients with certain\nmedical conditions of interest from simple queries on EHRs, or EHR-based\nphenotypes. Existing rule--based phenotyping approaches are extremely labor\nintensive. Instead, dimensionality reduction and latent factor estimation\ntechniques from machine learning can be adapted for phenotype extraction with\nno (or minimal) human supervision.\n  We propose to identify an easily interpretable latent space shared across\nvarious sources of EHR data as potential candidates for phenotypes. By\nincorporating multiple EHR data sources (e.g., diagnosis, medications, and lab\nreports) available in heterogeneous datatypes in a generalized\n\\textit{Collective Matrix Factorization (CMF)}, our methods can generate rich\nphenotypes. Further, easy interpretability in phenotyping application requires\nsparse representations of the candidate phenotypes, for example each phenotype\nderived from patients' medication and diagnosis data should preferably be\nrepresented by handful of diagnosis and medications, ($5$--$10$ active\ncomponents). We propose a constrained formulation of CMF for estimating sparse\nphenotypes. We demonstrate the efficacy of our model through an extensive\nempirical study on EHR data from Vanderbilt University Medical Center.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:38:40 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Ho", "Joyce C.", ""], ["Ghosh", "Joydeep", ""], ["Kreml", "Stephanie", ""], ["Kho", "Abel N", ""], ["Denny", "Joshua C", ""], ["Malin", "Bradley A", ""], ["Sun", "Jimeng", ""]]}, {"id": "1609.04470", "submitter": "Kaisey Mandel", "authors": "Kaisey S. Mandel, Daniel Scolnic, Hikmatali Shariff, Ryan J. Foley and\n  Robert P. Kirshner", "title": "The Type Ia Supernova Color-Magnitude Relation and Host Galaxy Dust: A\n  Simple Hierarchical Bayesian Model", "comments": "23 pages, 16 figures, minor corrections; accepted for publication in\n  ApJ", "journal-ref": null, "doi": "10.3847/1538-4357/aa6038", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Type Ia supernova (SN Ia) cosmology analyses currently use a\nsimplistic linear regression of magnitude versus color and light curve shape,\nwhich does not model intrinsic SN Ia variations and host galaxy dust as\nphysically distinct effects, resulting in low color-magnitude slopes. We\nconstruct a probabilistic generative model for the dusty distribution of\nextinguished absolute magnitudes and apparent colors as the convolution of a\nintrinsic SN Ia color-magnitude distribution and a host galaxy dust\nreddening-extinction distribution. If the intrinsic color-magnitude ($M_B$ vs.\n$B-V$) slope $\\beta_{int}$ differs from the host galaxy dust law $R_B$, this\nconvolution results in a specific curve of mean extinguished absolute magnitude\nvs. apparent color. The derivative of this curve smoothly transitions from\n$\\beta_{int}$ in the blue tail to $R_B$ in the red tail of the apparent color\ndistribution. The conventional linear fit approximates this effective curve\nnear the average apparent color, resulting in an apparent slope $\\beta_{app}$\nbetween $\\beta_{int}$ and $R_B$. We incorporate these effects into a\nhierarchical Bayesian statistical model for SN Ia light curve measurements, and\nanalyze a dataset of SALT2 optical light curve fits of 248 nearby SN Ia at z <\n0.10. The conventional linear fit obtains $\\beta_{app} \\approx 3$. Our model\nfinds a $\\beta_{int} = 2.3 \\pm 0.3$ and a distinct dust law of $R_B = 3.8 \\pm\n0.3$, consistent with the average for Milky Way dust, while correcting a\nsystematic distance bias of $\\sim 0.10$ mag in the tails of the apparent color\ndistribution. Finally, we extend our model to examine the SN Ia luminosity-host\nmass dependence in terms of intrinsic and dust components.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:49:24 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 21:16:59 GMT"}, {"version": "v3", "created": "Sat, 4 Feb 2017 14:44:44 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Mandel", "Kaisey S.", ""], ["Scolnic", "Daniel", ""], ["Shariff", "Hikmatali", ""], ["Foley", "Ryan J.", ""], ["Kirshner", "Robert P.", ""]]}, {"id": "1609.04523", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Lexin Li", "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis", "comments": "42 pages. To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in neuroimaging analysis, we propose a new\nregression model, Sparse TensOr REsponse regression (STORE), with a tensor\nresponse and a vector predictor. STORE embeds two key sparse structures:\nelement-wise sparsity and low-rankness. It can handle both a non-symmetric and\na symmetric tensor response, and thus is applicable to both structural and\nfunctional neuroimaging data. We formulate the parameter estimation as a\nnon-convex optimization problem, and develop an efficient alternating updating\nalgorithm. We establish a non-asymptotic estimation error bound for the actual\nestimator obtained from the proposed algorithm. This error bound reveals an\ninteresting interaction between the computational efficiency and the\nstatistical rate of convergence. When the distribution of the error tensor is\nGaussian, we further obtain a fast estimation error rate which allows the\ntensor dimension to grow exponentially with the sample size. We illustrate the\nefficacy of our model through intensive simulations and an analysis of the\nAutism spectrum disorder neuroimaging data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 06:51:51 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:32:26 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 23:01:54 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1609.04958", "submitter": "Christian R\\\"over", "authors": "David Conen, Barbora Arendack\\'a, Christian R\\\"over, Leonard Bergau,\n  Pascal Mu\\~noz, Sofieke Wijers, Christian Sticherling, Markus Zabel, Tim\n  Friede", "title": "Gender differences in appropriate shocks and mortality among patients\n  with primary prophylactic implantable cardioverter-defibrillators: Systematic\n  review and meta-analysis", "comments": "15 pages, 4 figures, 4 tables", "journal-ref": "PLoS ONE 11(9):e0162756, 2016", "doi": "10.1371/journal.pone.0162756", "report-no": null, "categories": "physics.med-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND: Some but not all prior studies have shown that women receiving a\nprimary prophylactic implantable cardioverter defibrillator (ICD) have a lower\nrisk of death and appropriate shocks than men.\n  PURPOSE: To evaluate the effect of gender on the risk of appropriate shock,\nall-cause mortality and inappropriate shock in contemporary studies of patients\nreceiving a primary prophylactic ICD.\n  DATA SOURCE: PubMed, LIVIVO, Cochrane CENTRAL between 2010 and 2016.\n  STUDY SELECTION: Studies providing at least 1 gender-specific risk estimate\nfor the outcomes of interest.\n  DATA EXTRACTION: Abstracts were screened independently for potentially\neligible studies for inclusion. Thereby each abstract was reviewed by at least\ntwo authors.\n  DATA SYNTHESIS: Out of 680 abstracts retained by our search strategy, 20\nstudies including 46'657 patients had gender-specific information on at least\none of the relevant endpoints. Mean age across the individual studies varied\nbetween 58 and 69 years. The proportion of women enrolled ranged from 10% to\n30%. Across 6 available studies, women had a significantly lower risk of first\nappropriate shock compared with men (pooled multivariable adjusted hazard ratio\n0.62 (95% CI [0.44; 0.88]). Across 14 studies reporting multivariable adjusted\ngender-specific hazard ratio estimates for all-cause mortality, women had a\nlower risk of death than men (pooled hazard ratio 0.75 (95% CI [0.66; 0.86]).\nThere was no statistically significant difference for the incidence of first\ninappropriate shocks (3 studies, pooled hazard ratio 0.99 (95% CI [0.56;\n1.73]).\n  CONCLUSION: In this large contemporary meta-analysis, women had a\nsignificantly lower risk of appropriate shocks and death than men, but a\nsimilar risk of inappropriate shocks. These data may help to select patients\nwho benefit from primary prophylactic ICD implantation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 09:02:27 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Conen", "David", ""], ["Arendack\u00e1", "Barbora", ""], ["R\u00f6ver", "Christian", ""], ["Bergau", "Leonard", ""], ["Mu\u00f1oz", "Pascal", ""], ["Wijers", "Sofieke", ""], ["Sticherling", "Christian", ""], ["Zabel", "Markus", ""], ["Friede", "Tim", ""]]}, {"id": "1609.05430", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel, Anja Leue and Norbert Hilger", "title": "Treating reflective indicators as causal-formative indicators in order\n  to compute factor score estimates or unit-weighted scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual scores on common factors are required in some applied settings\n(e.g., business and marketing settings). Common factors are based on reflective\nindicators, but their scores cannot unambiguously be determined. Therefore,\nfactor score estimates and unit-weighted scales are used in order to provide\nindividual scores. It is shown that these scores are based on treating the\nreflective indicators as if they were causal-formative indicators. This\nmodification of the causal status of the indicators should be justified.\nTherefore, the fit of the models implied by factor score estimates and\nunit-weighted scales should be investigated in order to ascertain the validity\nof the scores.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 06:44:38 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 19:09:46 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Leue", "Anja", ""], ["Hilger", "Norbert", ""]]}, {"id": "1609.05475", "submitter": "Takashi Shinzato", "authors": "Takashi Shinzato", "title": "Replica Analysis for the Duality of the Portfolio Optimization Problem", "comments": "6 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.94.052307", "report-no": null, "categories": "q-fin.PM cond-mat.dis-nn math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, the primal-dual problem consisting of the investment\nrisk minimization problem and the expected return maximization problem in the\nmean-variance model is discussed using replica analysis. As a natural extension\nof the investment risk minimization problem under only a budget constraint that\nwe analyzed in a previous study, we herein consider a primal-dual problem in\nwhich the investment risk minimization problem with budget and expected return\nconstraints is regarded as the primal problem, and the expected return\nmaximization problem with budget and investment risk constraints is regarded as\nthe dual problem. With respect to these optimal problems, we analyze a quenched\ndisordered system involving both of these optimization problems using the\napproach developed in statistical mechanical informatics, and confirm that both\noptimal portfolios can possess the primal-dual structure. Finally, the results\nof numerical simulations are shown to validate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 12:09:28 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Shinzato", "Takashi", ""]]}, {"id": "1609.05564", "submitter": "Paul Ginzberg", "authors": "Paul Ginzberg, Federico Giorgi, Andrea Califano", "title": "Searching for Gene Sets with Mutually Exclusive Mutations", "comments": "Submitted to the proceedings of the American Statistical Association\n  Joint Statistical Meetings (JSM), held in Chicago in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer cells evolve through random somatic mutations. \"Beneficial\" mutations\nwhich disrupt key pathways (e.g. cell cycle regulation) are subject to natural\nselection. Multiple mutations may lead to the same \"beneficial\" effect, in\nwhich case there is no selective advantage to having more than one of these\nmutations. Hence we are interested in finding sets of genes whose mutations are\napproximately mutually exclusive (anti-co-occurring) within the TCGA Pancancer\ndataset. In principle, finding the best set is NP Hard. Nevertheless, we will\nshow how a new Mutation anti-co-OCcurrence Algorithm (MOCA) provides an\neffective greedy search and testing algorithm with guaranteed control of the\nfamilywise error rate or false discovery rate, by combining some\nunder-appreciated ideas from frequentist hypothesis testing. These ideas\ninclude: (a) A novel exact conditional test for the tendency of multiple sets\nto have a large/small union/intersection, which generalises Fisher's exact test\nof 2x2 tables. (b) Randomised hypothesis tests for discrete distributions. (c)\nStouffer's method for combining p-values. (d) Weighted multiple hypothesis\ntesting. A new approach to setting a-priori weights which generates additional\nimplicit hypothesis tests is suggested, and allows us to preserve almost all\nstatistical power when testing pairs despite introducing a combinatorially\nlarge number of additional hypotheses.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 23:06:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ginzberg", "Paul", ""], ["Giorgi", "Federico", ""], ["Califano", "Andrea", ""]]}, {"id": "1609.05684", "submitter": "Francisco Javier Rubio", "authors": "F. J. Rubio and M. F. J. Steel", "title": "Flexible linear mixed models with improper priors for longitudinal and\n  survival data", "comments": "To appear in the Electronic Journal of Statistics. Additional R codes\n  can be found at: http://rpubs.com/FJRubio", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach using improper priors for hierarchical linear\nmixed models with flexible random effects and residual error distributions. The\nerror distribution is modelled using scale mixtures of normals, which can\ncapture tails heavier than those of the normal distribution. This\ngeneralisation is useful to produce models that are robust to the presence of\noutliers. The case of asymmetric residual errors is also studied. We present\ngeneral results for the propriety of the posterior that also cover cases with\ncensored observations, allowing for the use of these models in the contexts of\npopular longitudinal and survival analyses. We consider the use of copulas with\nflexible marginals for modelling the dependence between the random effects, but\nour results cover the use of any random effects distribution. Thus, our paper\nprovides a formal justification for Bayesian inference in a very wide class of\nmodels (covering virtually all of the literature) under attractive prior\nstructures that limit the amount of required user elicitation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 12:17:35 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 12:08:51 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 15:09:26 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rubio", "F. J.", ""], ["Steel", "M. F. J.", ""]]}, {"id": "1609.05805", "submitter": "Debasis Kundu Professor", "authors": "Shuvashree Mondal and Debasis Kundu", "title": "A New Two Sample Type-II Progressive Censoring Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive censoring scheme has received considerable attention in recent\nyears. In this paper we introduce a new type-II progressive censoring scheme\nfor two samples. It is observed that the proposed censoring scheme is\nanalytically more tractable than the existing joint progressive type-II\ncensoring scheme proposed by Rasouli and Balakrishnan \\cite{RB:2010}. It has\nsome other advantages also. We study the statistical inference of the unknown\nparameters based on the assumptions that the lifetime distribution of the\nexperimental units for the two samples follow exponential distribution with\ndifferent scale parameters. The maximum likelihood estimators of the unknown\nparameters are obtained and their exact distributions are derived. Based on the\nexact distributions of the maximum likelihood estimators exact confidence\nintervals are also constructed. For comparison purposes we have used bootstrap\nconfidence intervals also. It is observed that the bootstrap confidence\nintervals work very well and they are very easy to implement in practice. Some\nsimulation experiments are performed to compare the performances of the\nproposed method with the existing one, and the performances of the proposed\nmethod are quite satisfactory. One data analysis has been performed for\nillustrative purposes. Finally we propose some open problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:00:03 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Mondal", "Shuvashree", ""], ["Kundu", "Debasis", ""]]}, {"id": "1609.05959", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Nazarov", "title": "Conformalized Kernel Ridge Regression", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General predictive models do not provide a measure of confidence in\npredictions without Bayesian assumptions. A way to circumvent potential\nrestrictions is to use conformal methods for constructing non-parametric\nconfidence regions, that offer guarantees regarding validity. In this paper we\nprovide a detailed description of a computationally efficient conformal\nprocedure for Kernel Ridge Regression (KRR), and conduct a comparative\nnumerical study to see how well conformal regions perform against the Bayesian\nconfidence sets. The results suggest that conformalized KRR can yield\npredictive confidence regions with specified coverage rate, which is essential\nin constructing anomaly detection systems based on predictive models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 22:30:36 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Nazarov", "Ivan", ""]]}, {"id": "1609.06052", "submitter": "Christoffer Moesgaard Albertsen", "authors": "Christoffer Moesgaard Albertsen, Anders Nielsen, Uffe H{\\o}gsbro\n  Thygesen", "title": "Choosing the observational likelihood in state-space stock assessment\n  models", "comments": "To be published in Canadian Journal of Fisheries and Aquatic Sciences", "journal-ref": null, "doi": "10.1139/cjfas-2015-0532", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data used in stock assessment models result from combinations of biological,\necological, fishery, and sampling processes. Since different types of errors\npropagate through these processes it can be difficult to identify a particular\nfamily of distributions for modelling errors on observations a priori. By\nimplementing several observational likelihoods, modelling both numbers- and\nproportions-at-age, in an age based state-space stock assessment model, we\ncompare the model fit for each choice of likelihood along with the implications\nfor spawning stock biomass and average fishing mortality. We propose using AIC\nintervals based on fitting the full observational model for comparing different\nobservational likelihoods. Using data from four stocks, we show that the model\nfit is improved by modelling the correlation of observations within years.\nHowever, the best choice of observational likelihood differs for different\nstocks, and the choice is important for the short-term conclusions drawn from\nthe assessment model; in particular, the choice can influence total allowable\ncatch advise based on reference points.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 08:20:04 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Albertsen", "Christoffer Moesgaard", ""], ["Nielsen", "Anders", ""], ["Thygesen", "Uffe H\u00f8gsbro", ""]]}, {"id": "1609.06070", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Sarah Brockhaus, Kornelia Gentsch, Klaus Scherer and\n  Sonja Greven", "title": "Boosting Factor-Specific Functional Historical Models for the Detection\n  of Synchronisation in Bioelectrical Signals", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The link between different psychophysiological measures during emotion\nepisodes is not well understood. To analyse the functional relationship between\nelectroencephalography (EEG) and facial electromyography (EMG), we apply\nhistorical function-on-function regression models to EEG and EMG data that were\nsimultaneously recorded from 24 participants while they were playing a\ncomputerised gambling task. Given the complexity of the data structure for this\napplication, we extend simple functional historical models to models including\nrandom historical effects, factor-specific historical effects, and\nfactor-specific random historical effects. Estimation is conducted by a\ncomponent-wise gradient boosting algorithm, which scales well to large data\nsets and complex models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 09:42:32 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 09:31:56 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Brockhaus", "Sarah", ""], ["Gentsch", "Kornelia", ""], ["Scherer", "Klaus", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.06424", "submitter": "{\\O}rnulf Borgan", "authors": "{\\O}rnulf Borgan (Department of Mathematics, University of Oslo)", "title": "Do Japanese and Italian women live longer than women in Scandinavia?", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Life expectancies at birth are routinely computed from period life tables.\nSuch period life expectancies may be distorted by selection when comparing\ncountries where the living conditions improved earlier (like Norway and Sweden)\nwith countries where they improved later (like Italy and Japan). One way to get\na fair comparison between the countries, is to use cohort data and consider the\nexpected number of years lost before a given age a. Contrary to the results\nbased on period data, one then finds that Italian women may expect to lose more\nyears than women in Norway and Sweden, while there are no indications that\nJapanese women will lose fewer years than Scandinavian women.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 06:13:03 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Borgan", "\u00d8rnulf", "", "Department of Mathematics, University of Oslo"]]}, {"id": "1609.06680", "submitter": "Lucas Macri", "authors": "Shiyuan He, Wenlong Yuan, Jianhua Z. Huang, James Long and Lucas M.\n  Macri", "title": "Period estimation for sparsely-sampled quasi-periodic light curves\n  applied to Miras", "comments": "Changes in v3: minor edits to match the published version. Software\n  package and test data set available at http://github.com/shiyuanhe/varStar", "journal-ref": "The Astronomical Journal, 152 (6), 164 (2016)", "doi": "10.3847/0004-6256/152/6/164", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a non-linear semi-parametric Gaussian process model to estimate\nperiods of Miras with sparsely-sampled light curves. The model uses a\nsinusoidal basis for the periodic variation and a Gaussian process for the\nstochastic changes. We use maximum likelihood to estimate the period and the\nparameters of the Gaussian process, while integrating out the effects of other\nnuisance parameters in the model with respect to a suitable prior distribution\nobtained from earlier studies. Since the likelihood is highly multimodal for\nperiod, we implement a hybrid method that applies the quasi-Newton algorithm\nfor Gaussian process parameters and search the period/frequency parameter over\na dense grid.\n  A large-scale, high-fidelity simulation is conducted to mimic the sampling\nquality of Mira light curves obtained by the M33 Synoptic Stellar Survey. The\nsimulated data set is publicly available and can serve as a testbed for future\nevaluation of different period estimation methods. The semi-parametric model\noutperforms an existing algorithm on this simulated test data set as measured\nby period recovery rate and quality of the resulting Period-Luminosity\nrelations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:51:54 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 18:17:50 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 17:37:07 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["He", "Shiyuan", ""], ["Yuan", "Wenlong", ""], ["Huang", "Jianhua Z.", ""], ["Long", "James", ""], ["Macri", "Lucas M.", ""]]}, {"id": "1609.06757", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Miao Liu, and Jonathan P. How", "title": "Quickest Change Detection Approach to Optimal Control in Markov Decision\n  Processes with Model Changes", "comments": "In Proceedings of American Control Conference 2017, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal control in non-stationary Markov decision processes (MDP) is a\nchallenging problem. The aim in such a control problem is to maximize the\nlong-term discounted reward when the transition dynamics or the reward function\ncan change over time. When a prior knowledge of change statistics is available,\nthe standard Bayesian approach to this problem is to reformulate it as a\npartially observable MDP (POMDP) and solve it using approximate POMDP solvers,\nwhich are typically computationally demanding. In this paper, the problem is\nanalyzed through the viewpoint of quickest change detection (QCD), a set of\ntools for detecting a change in the distribution of a sequence of random\nvariables. Current methods applying QCD to such problems only passively detect\nchanges by following prescribed policies, without optimizing the choice of\nactions for long term performance. We demonstrate that ignoring the\nreward-detection trade-off can cause a significant loss in long term rewards,\nand propose a two threshold switching strategy to solve the issue. A\nnon-Bayesian problem formulation is also proposed for scenarios where a\nBayesian formulation cannot be defined. The performance of the proposed two\nthreshold strategy is examined through numerical analysis on a non-stationary\nMDP task, and the strategy outperforms the state-of-the-art QCD methods in both\nBayesian and non-Bayesian settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 21:13:33 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 21:37:52 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Banerjee", "Taposh", ""], ["Liu", "Miao", ""], ["How", "Jonathan P.", ""]]}, {"id": "1609.06805", "submitter": "Lawrence Bardwell", "authors": "Lawrence Bardwell, Idris Eckley, Paul Fearnhead, Simon Smith and\n  Martin Spott", "title": "Most recent changepoint detection in Panel data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting recent changepoints in time-series can be important for short-term\nprediction, as we can then base predictions just on the data since the\nchangepoint. In many applications we have panel data, consisting of many\nrelated univariate time-series. We present a novel approach to detect sets of\nmost recent changepoints in such panel data which aims to pool information\nacross time-series, so that we preferentially infer a most recent change at the\nsame time-point in multiple series. Our approach is computationally efficient\nas it involves analysing each time-series independently to obtain a profile\nlikelihood like quantity that summarises the evidence for the series having\neither no change or a specific value for its most recent changepoint. We then\npost-process this output from each time-series to obtain a potentially small\nset of times for the most recent changepoints, and, for each time, the set of\nseries which has their most recent changepoint at that time. We demonstrate the\nusefulness of this method on two data sets: forecasting events in a\ntelecommunications network and inference about changes in the net asset ratio\nfor a panel of US firms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 03:15:52 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 08:50:53 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 20:22:54 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Bardwell", "Lawrence", ""], ["Eckley", "Idris", ""], ["Fearnhead", "Paul", ""], ["Smith", "Simon", ""], ["Spott", "Martin", ""]]}, {"id": "1609.06864", "submitter": "Alessandro Magrini", "authors": "Alessandro Magrini, Davide Luciani and Federico Mattia Stefanini", "title": "A probabilistic network for the diagnosis of acute cardiopulmonary\n  diseases", "comments": "The DOI of the article published after peer review was added. A\n  technical detail was added in Section 3.2, Formula 8 (as a consequence, the\n  ID of all the subsequent formulas result augmented by 1 with respect to the\n  previous version). The prior standard deviation of the Gamma distribution in\n  Table 4 was fixed (in the previous version, the prior variance was indicated,\n  instead)", "journal-ref": "Biometrical Journal, 60(1), 174-195, January 2018", "doi": "10.1002/bimj.201600206", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the development of a probabilistic network for the diagnosis\nof acute cardiopulmonary diseases is presented. This paper is a draft version\nof the article published after peer review in 2018\n(https://doi.org/10.1002/bimj.201600206). A panel of expert physicians\ncollaborated to specify the qualitative part, that is a directed acyclic graph\ndefining a factorization of the joint probability distribution of domain\nvariables. The quantitative part, that is the set of all conditional\nprobability distributions defined by each factor, was estimated in the Bayesian\nparadigm: we applied a special formal representation, characterized by a low\nnumber of parameters and a parameterization intelligible for physicians,\nelicited the joint prior distribution of parameters from medical experts, and\nupdated it by conditioning on a dataset of hospital patient records using\nMarkov Chain Monte Carlo simulation. Refinement was cyclically performed until\nthe probabilistic network provided satisfactory Concordance Index values for a\nselection of acute diseases and reasonable inference on six fictitious patient\ncases. The probabilistic network can be employed to perform medical diagnosis\non a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167\npatient findings.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 08:28:38 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 09:38:01 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Magrini", "Alessandro", ""], ["Luciani", "Davide", ""], ["Stefanini", "Federico Mattia", ""]]}, {"id": "1609.06874", "submitter": "Facundo Costa", "authors": "Facundo Costa, Hadj Batatia, Thomas Oberlin and Jean-Yves Tourneret", "title": "EEG reconstruction and skull conductivity estimation using a Bayesian\n  model promoting structured sparsity", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M/EEG source localization is an open research issue. To solve it, it is\nimportant to have good knowledge of several physical parameters to build a\nreliable head operator. Amongst them, the value of the conductivity of the\nhuman skull has remained controversial. This report introduces a novel\nhierarchical Bayesian framework to estimate the skull conductivity jointly with\nthe brain activity from the M/EEG measurements to improve the reconstruction\nquality. A partially collapsed Gibbs sampler is used to draw samples\nasymptotically distributed according to the associated posterior. The generated\nsamples are then used to estimate the brain activity and the model\nhyperparameters jointly in a completely unsupervised framework. We use\nsynthetic and real data to illustrate the improvement of the reconstruction.\nThe performance of our method is also compared with two optimization algorithms\nintroduced by Vallagh\\'e \\textit{et al.} and Gutierrez \\textit{et al.}\nrespectively, showing that our method is able to provide results of similar or\nbetter quality while remaining applicable in a wider array of situations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 09:02:01 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 20:56:38 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Costa", "Facundo", ""], ["Batatia", "Hadj", ""], ["Oberlin", "Thomas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1609.07007", "submitter": "Jona Cederbaum", "authors": "Jona Cederbaum, Fabian Scheipl and Sonja Greven", "title": "Fast symmetric additive covariance smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast bivariate smoothing approach for symmetric surfaces that\nhas a wide range of applications. We show how it can be applied to estimate the\ncovariance function in longitudinal data as well as multiple additive\ncovariances in functional data with complex correlation structures. Our\nsymmetric smoother can handle (possibly noisy) data sampled on a common, dense\ngrid as well as irregularly or sparsely sampled data. Estimation is based on\nbivariate penalized spline smoothing using a mixed model representation and the\nsymmetry is used to reduce computation time compared to the usual non-symmetric\nsmoothers. We outline the application of our approach in functional principal\ncomponent analysis and demonstrate its practical value in two applications. The\napproach is evaluated in extensive simulations. We provide documented open\nsource software implementing our fast symmetric bivariate smoother building on\nestablished algorithms for additive models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:00:25 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cederbaum", "Jona", ""], ["Scheipl", "Fabian", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.07196", "submitter": "M. Giselle Fern\\'andez-Godino", "authors": "M. Giselle Fern\\'andez-Godino, Chanyoung Park, Nam-Ho Kim and Raphael\n  T. Haftka", "title": "Review of multi-fidelity models", "comments": "46 pages, 13 figures. The paper is under review to be published in\n  the Structural and Multidisciplinary Optimization journal, Springer", "journal-ref": "AIAA Journal 57, no. 5 (2019): 2039-2054", "doi": "10.2514/1.J057750", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations are often computationally expensive and the need for multiple\nrealizations, as in uncertainty quantification or optimization, makes surrogate\nmodels an attractive option. For expensive high-fidelity models (HFMs),\nhowever, even performing the number of simulations needed for fitting a\nsurrogate may be too expensive. Inexpensive but less accurate low-fidelity\nmodels (LFMs) are often also available. Multi-fidelity models (MFMs) combine\nHFMs and LFMs in order to achieve accuracy at a reasonable cost. With the\nincreasing popularity of MFMs in mind, the aim of this paper is to summarize\nthe state-of-the-art of MFM trends. For this purpose, publications in this\nfield are classified based on application, surrogate selection if any, the\ndifference between fidelities, the method used to combine these fidelities, the\nfield of application and the year published.\n  Available methods of combining fidelities are also reviewed, focusing our\nattention especially on multi-fidelity surrogate models in which fidelities are\ncombined inside a surrogate model. Computation time savings are usually the\nreason for using MFMs, hence it is important to properly report the achieved\nsavings. Unfortunately, we find that many papers do not present sufficient\ninformation to determine these savings. Therefore, the paper also includes\nguidelines for authors to present their MFM savings in a way that is useful to\nfuture MFM users. Based on papers that provided enough information, we find\nthat time savings are highly problem dependent and that MFM methods we surveyed\nprovided time savings up to 90%.\n  Keywords: Multi-fidelity, Variable-complexity, Variable-fidelity, Surrogate\nmodels, Optimization, Uncertainty quantification, Review, Survey\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 00:35:53 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 03:37:43 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 23:05:37 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fern\u00e1ndez-Godino", "M. Giselle", ""], ["Park", "Chanyoung", ""], ["Kim", "Nam-Ho", ""], ["Haftka", "Raphael T.", ""]]}, {"id": "1609.07217", "submitter": "Xiao Liu", "authors": "Xiao Liu, Kyongmin Yeo, Jayant Kalagnanam", "title": "Statistical Modeling for Spatio-Temporal Degradation Data", "comments": "30 pages, 7 figures. Manuscript prepared for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the modeling of an important class of degradation\ndata, which are collected from a spatial domain over time; for example, the\nsurface quality degradation. Like many existing time-dependent stochastic\ndegradation models, a special random field is constructed for modeling the\nspatio-temporal degradation process. In particular, we express the degradation\nat any spatial location and time as an additive superposition of two stochastic\ncomponents: a dynamic spatial degradation generation process, and a\nspatio-temporal degradation propagation process. Some unique challenges are\naddressed, including the spatial heterogeneity of the degradation process, the\nspatial propagation of degradation to neighboring areas, the anisotropic and\nspace-time non-separable covariance structure often associated with a complex\nspatio-temporal degradation process, and the computational issue related to\nparameter estimation. When the spatial dependence is ignored, we show that the\nproposed spatio-temporal degradation model incorporates some existing pure\ntime-dependent degradation processes as its special cases. We also show the\nconnection, under special conditions, between the proposed model and general\nphysical degradation processes which are often defined by stochastic partial\ndifferential equations. A numerical example is presented to illustrate the\nmodeling approach and model validation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 03:16:10 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 21:27:33 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Liu", "Xiao", ""], ["Yeo", "Kyongmin", ""], ["Kalagnanam", "Jayant", ""]]}, {"id": "1609.07363", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Guillem Rigaill", "title": "Changepoint Detection in the Presence of Outliers", "comments": "Updated to include a proof of consistency and accuracy of estimating\n  change points using the biweight loss function", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 10:56:16 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Fearnhead", "Paul", ""], ["Rigaill", "Guillem", ""]]}, {"id": "1609.07378", "submitter": "Anton Bezuglov", "authors": "Anton Bezuglov, Brian Blanton, and Reinaldo Santiago", "title": "Multi-Output Artificial Neural Network for Storm Surge Prediction in\n  North Carolina", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During hurricane seasons, emergency managers and other decision makers need\naccurate and `on-time' information on potential storm surge impacts. Fully\ndynamical computer models, such as the ADCIRC tide, storm surge, and wind-wave\nmodel take several hours to complete a forecast when configured at high spatial\nresolution. Additionally, statically meaningful ensembles of high-resolution\nmodels (needed for uncertainty estimation) cannot easily be computed in near\nreal-time. This paper discusses an artificial neural network model for storm\nsurge prediction in North Carolina. The network model provides fast, real-time\nstorm surge estimates at coastal locations in North Carolina. The paper studies\nthe performance of the neural network model vs. other models on synthetic and\nreal hurricane data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:24:44 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Bezuglov", "Anton", ""], ["Blanton", "Brian", ""], ["Santiago", "Reinaldo", ""]]}, {"id": "1609.07407", "submitter": "Joshua Rapp", "authors": "Joshua Rapp and Vivek K Goyal", "title": "A Few Photons Among Many: Unmixing Signal and Noise for Photon-Efficient\n  Active Imaging", "comments": null, "journal-ref": "IEEE TCI vol. 3, no. 3 (2017) 445-459", "doi": "10.1109/TCI.2017.2706028", "report-no": null, "categories": "stat.AP physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional LIDAR systems require hundreds or thousands of photon detections\nto form accurate depth and reflectivity images. Recent photon-efficient\ncomputational imaging methods are remarkably effective with only 1.0 to 3.0\ndetected photons per pixel, but they are not demonstrated at\nsignal-to-background ratio (SBR) below 1.0 because their imaging accuracies\ndegrade significantly in the presence of high background noise. We introduce a\nnew approach to depth and reflectivity estimation that focuses on unmixing\ncontributions from signal and noise sources. At each pixel in an image,\nshort-duration range gates are adaptively determined and applied to remove\ndetections likely to be due to noise. For pixels with too few detections to\nperform this censoring accurately, we borrow data from neighboring pixels to\nimprove depth estimates, where the neighborhood formation is also adaptive to\nscene content. Algorithm performance is demonstrated on experimental data at\nvarying levels of noise. Results show improved performance of both reflectivity\nand depth estimates over state-of-the-art methods, especially at low\nsignal-to-background ratios. In particular, accurate imaging is demonstrated\nwith SBR as low as 0.04. This validation of a photon-efficient, noise-tolerant\nmethod demonstrates the viability of rapid, long-range, and low-power LIDAR\nimaging.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 15:49:24 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Rapp", "Joshua", ""], ["Goyal", "Vivek K", ""]]}, {"id": "1609.07464", "submitter": "Yaoyuan Vincent Tan", "authors": "Yaoyuan Vincent Tan, Carol A.C. Flannagan, and Michael R. Elliott", "title": "Predicting human-driving behavior to help driverless vehicles drive:\n  random intercept Bayesian Additive Regression Trees", "comments": "6 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of driverless vehicles has spurred the need to predict human\ndriving behavior to facilitate interaction between driverless and human-driven\nvehicles. Predicting human driving movements can be challenging, and poor\nprediction models can lead to accidents between the driverless and human-driven\nvehicles. We used the vehicle speed obtained from a naturalistic driving\ndataset to predict whether a human-driven vehicle would stop before executing a\nleft turn. In a preliminary analysis, we found that BART produced less variable\nand higher AUC values compared to a variety of other state-of-the-art binary\npredictor methods. However, BART assumes independent observations, but our\ndataset consists of multiple observations clustered by driver. Although methods\nextending BART to clustered or longitudinal data are available, they lack\nreadily available software and can only be applied to clustered continuous\noutcomes. We extend BART to handle correlated binary observations by adding a\nrandom intercept and used a simulation study to determine bias, root mean\nsquared error, 95% coverage, and average length of 95% credible interval in a\ncorrelated data setting. We then successfully implemented our random intercept\nBART model to our clustered dataset and found substantial improvements in\nprediction performance compared to BART and random intercept linear logistic\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 18:54:46 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 15:29:39 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Tan", "Yaoyuan Vincent", ""], ["Flannagan", "Carol A. C.", ""], ["Elliott", "Michael R.", ""]]}, {"id": "1609.07480", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis", "title": "Predictive modelling of football injuries", "comments": "PhD Thesis submitted and defended successfully at the Department of\n  Computer Science at University College London", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this thesis is to investigate the potential of predictive\nmodelling for football injuries. This work was conducted in close collaboration\nwith Tottenham Hotspurs FC (THFC), the PGA European tour and the participation\nof Wolverhampton Wanderers (WW).\n  Three investigations were conducted:\n  1. Predicting the recovery time of football injuries using the UEFA injury\nrecordings: The UEFA recordings is a common standard for recording injuries in\nprofessional football. For this investigation, three datasets of UEFA injury\nrecordings were available. Different machine learning algorithms were used in\norder to build a predictive model. The performance of the machine learning\nmodels is then improved by using feature selection conducted through\ncorrelation-based subset feature selection and random forests.\n  2. Predicting injuries in professional football using exposure records: The\nrelationship between exposure (in training hours and match hours) in\nprofessional football athletes and injury incidence was studied. A common\nproblem in football is understanding how the training schedule of an athlete\ncan affect the chance of him getting injured. The task was to predict the\nnumber of days a player can train before he gets injured.\n  3. Predicting intrinsic injury incidence using in-training GPS measurements:\nA significant percentage of football injuries can be attributed to overtraining\nand fatigue. GPS data collected during training sessions might provide\nindicators of fatigue, or might be used to detect very intense training\nsessions which can lead to overtraining. This research used GPS data gathered\nduring training sessions of the first team of THFC, in order to predict whether\nan injury would take place during a week.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:58:42 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kampakis", "Stylianos", ""]]}, {"id": "1609.07511", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Mortality and life expectancy forecasting for a group of populations in\n  developed countries: A robust multilevel functional data method", "comments": "18 pages, 2 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1606.05067", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust multilevel functional data method is proposed to forecast\nage-specific mortality rate and life expectancy for two or more populations in\ndeveloped countries with high-quality vital registration systems. It uses a\nrobust multilevel functional principal component analysis of aggregate and\npopulation-specific data to extract the common trend and population-specific\nresidual trend among populations. This method is applied to age- and\nsex-specific mortality rate and life expectancy for the United Kingdom from\n1922 to 2011, and its forecast accuracy is then further compared with standard\nmultilevel functional data method. For forecasting both age-specific mortality\nand life expectancy, the robust multilevel functional data method produces more\naccurate point and interval forecasts than the standard multilevel functional\ndata method in the presence of outliers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 20:51:35 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1609.07662", "submitter": "Evgeny Burnaev", "authors": "Alexey Artemov and Evgeny Burnaev", "title": "Detecting Performance Degradation of Software-Intensive Systems in the\n  Presence of Trends and Long-Range Dependence", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As contemporary software-intensive systems reach increasingly large scale, it\nis imperative that failure detection schemes be developed to help prevent\ncostly system downtimes. A promising direction towards the construction of such\nschemes is the exploitation of easily available measurements of system\nperformance characteristics such as average number of processed requests and\nqueue size per unit of time. In this work, we investigate a holistic\nmethodology for detection of abrupt changes in time series data in the presence\nof quasi-seasonal trends and long-range dependence with a focus on failure\ndetection in computer systems. We propose a trend estimation method enjoying\noptimality properties in the presence of long-range dependent noise to estimate\nwhat is considered \"normal\" system behaviour. To detect change-points and\nanomalies, we develop an approach based on the ensembles of \"weak\" detectors.\nWe demonstrate the performance of the proposed change-point detection scheme\nusing an artificial dataset, the publicly available Abilene dataset as well as\nthe proprietary geoinformation system dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 19:20:18 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1609.07731", "submitter": "Luca Martino", "authors": "Luca Martino, Jesse Read, Victor Elvira, Francisco Louzada", "title": "Cooperative Parallel Particle Filters for online model selection and\n  applications to Urban Mobility", "comments": "A preliminary Matlab code is provided at\n  http://www.mathworks.com/matlabcentral/fileexchange/58597-model-averanging-particle-filter,\n  Digital Signal Processing, 2016", "journal-ref": "Digital Signal Processing, Volume 60, Pages 172-185, 2017", "doi": "10.1016/j.dsp.2016.09.011", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a sequential Monte Carlo scheme for the dual purpose of Bayesian\ninference and model selection. We consider the application context of urban\nmobility, where several modalities of transport and different measurement\ndevices can be employed. Therefore, we address the joint problem of online\ntracking and detection of the current modality. For this purpose, we use\ninteracting parallel particle filters, each one addressing a different model.\nThey cooperate for providing a global estimator of the variable of interest\nand, at the same time, an approximation of the posterior density of each model\ngiven the data. The interaction occurs by a parsimonious distribution of the\ncomputational effort, with online adaptation for the number of particles of\neach filter according to the posterior probability of the corresponding model.\nThe resulting scheme is simple and flexible. We have tested the novel technique\nin different numerical experiments with artificial and real data, which confirm\nthe robustness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 11:10:48 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""], ["Elvira", "Victor", ""], ["Louzada", "Francisco", ""]]}, {"id": "1609.07806", "submitter": "Adeshina Bello oyedele", "authors": "A.O. Bello, F.A. Oguntolu, O.M. Adetutu, J.P. Ojedokun", "title": "Application of Bootstrap Re-sampling Method to a Categorical Data of\n  HIV/AIDS Spread across different Social-Economic Classes", "comments": null, "journal-ref": null, "doi": "10.5923/j.statistics.20150504.04", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research reports on the relationship and significance of social-economic\nfactors (age, sex, employment status) and modes of HIV/AIDS transmission to the\nHIV/AIDS spread. Logistic regression model, a form of probabilistic function\nfor binary response was used to relate social-economic factors (age, sex,\nemployment status) to HIV/AIDS spread. The statistical predictive model was\nused to project the likelihood response of HIV/AIDS spread with a larger\npopulation using 10,000 Bootstrap re-sampling observations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 21:33:14 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Bello", "A. O.", ""], ["Oguntolu", "F. A.", ""], ["Adetutu", "O. M.", ""], ["Ojedokun", "J. P.", ""]]}, {"id": "1609.07912", "submitter": "Antoine Tixier", "authors": "Antoine J.-P. Tixier, Matthew R. Hallowell, Balaji Rajagopalan", "title": "Construction Safety Risk Modeling and Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By building on a recently introduced genetic-inspired attribute-based\nconceptual framework for safety risk analysis, we propose a novel methodology\nto compute construction univariate and bivariate construction safety risk at a\nsituational level. Our fully data-driven approach provides construction\npractitioners and academicians with an easy and automated way of extracting\nvaluable empirical insights from databases of unstructured textual injury\nreports. By applying our methodology on an attribute and outcome dataset\ndirectly obtained from 814 injury reports, we show that the frequency-magnitude\ndistribution of construction safety risk is very similar to that of natural\nphenomena such as precipitation or earthquakes. Motivated by this observation,\nand drawing on state-of-the-art techniques in hydroclimatology and insurance,\nwe introduce univariate and bivariate nonparametric stochastic safety risk\ngenerators, based on Kernel Density Estimators and Copulas. These generators\nenable the user to produce large numbers of synthetic safety risk values\nfaithfully to the original data, allowing safetyrelated decision-making under\nuncertainty to be grounded on extensive empirical evidence. Just like the\naccurate modeling and simulation of natural phenomena such as wind or\nstreamflow is indispensable to successful structure dimensioning or water\nreservoir management, we posit that improving construction safety calls for the\naccurate modeling, simulation, and assessment of safety risk. The underlying\nassumption is that like natural phenomena, construction safety may benefit from\nbeing studied in an empirical and quantitative way rather than qualitatively\nwhich is the current industry standard. Finally, a side but interesting finding\nis that attributes related to high energy levels and to human error emerge as\nstrong risk shapers on the dataset we used to illustrate our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 10:19:02 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tixier", "Antoine J. -P.", ""], ["Hallowell", "Matthew R.", ""], ["Rajagopalan", "Balaji", ""]]}, {"id": "1609.08028", "submitter": "Lingxue Zhu", "authors": "Lingxue Zhu, Jing Lei, Bernie Devlin and Kathryn Roeder", "title": "A Unified Statistical Framework for Single Cell and Bulk RNA Sequencing\n  Data", "comments": null, "journal-ref": "Ann. Appl. Stat., Volume 12, Number 1 (2018), 609-632", "doi": "10.1214/17-AOAS1110", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in technology have enabled the measurement of RNA levels for\nindividual cells. Compared to traditional tissue-level bulk RNA-seq data,\nsingle cell sequencing yields valuable insights about gene expression profiles\nfor different cell types, which is potentially critical for understanding many\ncomplex human diseases. However, developing quantitative tools for such data\nremains challenging because of high levels of technical noise, especially the\n\"dropout\" events. A \"dropout\" happens when the RNA for a gene fails to be\namplified prior to sequencing, producing a \"false\" zero in the observed data.\nIn this paper, we propose a Unified RNA-Sequencing Model (URSM) for both single\ncell and bulk RNA-seq data, formulated as a hierarchical model. URSM borrows\nthe strength from both data sources and carefully models the dropouts in single\ncell data, leading to a more accurate estimation of cell type specific gene\nexpression profile. In addition, URSM naturally provides inference on the\ndropout entries in single cell data that need to be imputed for downstream\nanalyses, as well as the mixing proportions of different cell types in bulk\nsamples. We adopt an empirical Bayes approach, where parameters are estimated\nusing the EM algorithm and approximate inference is obtained by Gibbs sampling.\nSimulation results illustrate that URSM outperforms existing approaches both in\ncorrecting for dropouts in single cell data, as well as in deconvolving bulk\nsamples. We also demonstrate an application to gene expression data on fetal\nbrains, where our model successfully imputes the dropout genes and reveals cell\ntype specific expression patterns.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 15:43:42 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 17:39:57 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 22:54:05 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhu", "Lingxue", ""], ["Lei", "Jing", ""], ["Devlin", "Bernie", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1609.08039", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Dmitry Smolyakov", "title": "One-Class SVM with Privileged Information and its Application to Malware\n  Detection", "comments": "8 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of important applied problems in engineering, finance and medicine\ncan be formulated as a problem of anomaly detection. A classical approach to\nthe problem is to describe a normal state using a one-class support vector\nmachine. Then to detect anomalies we quantify a distance from a new observation\nto the constructed description of the normal class. In this paper we present a\nnew approach to the one-class classification. We formulate a new problem\nstatement and a corresponding algorithm that allow taking into account a\nprivileged information during the training phase. We evaluate performance of\nthe proposed approach using a synthetic dataset, as well as the publicly\navailable Microsoft Malware Classification Challenge dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 16:01:02 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 16:31:46 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Smolyakov", "Dmitry", ""]]}, {"id": "1609.08074", "submitter": "Kevin Schultz", "authors": "Kevin Schultz", "title": "Orientation Statistics and Quantum Information", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the engineering applications of uncertainty quantification, in\nthis work we draw connections between the notions of random quantum states and\noperations in quantum information with probability distributions commonly\nencountered in the field of orientation statistics. This approach identifies\nnatural probability distributions that can be used in the analysis, simulation,\nand inference of quantum information systems. The theory of exponential\nfamilies on Stiefel manifolds provides the appropriate generalization to the\nclassical case, and fortunately there are many existing techniques for\ninference and sampling that exist for these distributions. Furthermore, this\nviewpoint motivates a number of additional questions into the convex geometry\nof quantum operations relative to both the differential geometry of Stiefel\nmanifolds as well as the information geometry of exponential families defined\nupon them. In particular, we draw on results from convex geometry to\ncharacterize which quantum operations can be represented as the average of a\nrandom quantum operation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 17:00:06 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Schultz", "Kevin", ""]]}, {"id": "1609.08217", "submitter": "Yan Kagan Y.", "authors": "Yan Y. Kagan", "title": "Earthquake Number Forecasts Testing", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": "10.1093/gji/ggx300", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributions of earthquake numbers in two global catalogs:\nGlobal Centroid-Moment Tensor and Preliminary Determinations of Epicenters.\nThese distributions are required to develop the number test for forecasts of\nfuture seismic activity rate. A common assumption is that the numbers are\ndescribed by the Poisson distribution. In contrast to the one-parameter Poisson\ndistribution, the negative-binomial distribution (NBD) has two parameters. The\nsecond parameter characterizes the clustering or over-dispersion of a process.\nWe investigate the dependence of parameters for both distributions on the\ncatalog magnitude threshold and on temporal subdivision of catalog duration. We\nfind that for most cases of interest the Poisson distribution can be rejected\nstatistically at a high significance level in favor of the NBD. Therefore we\ninvestigate whether these distributions fit the observed distributions of\nseismicity. For this purpose we study upper statistical moments of earthquake\nnumbers (skewness and kurtosis) and compare them to the theoretical values for\nboth distributions. Empirical values for the skewness and the kurtosis increase\nfor the smaller magnitude threshold and increase with even greater intensity\nfor small temporal subdivision of catalogs. A calculation of the NBD skewness\nand kurtosis levels shows rapid increase of these upper moments levels.\nHowever, the observed catalog values of skewness and kurtosis are rising even\nfaster. This means that for small time intervals the earthquake number\ndistribution is even more heavy-tailed than the NBD predicts. Therefore for\nsmall time intervals we propose using empirical number distributions\nappropriately smoothed for testing forecasted earthquake numbers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 23:05:52 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Kagan", "Yan Y.", ""]]}, {"id": "1609.08328", "submitter": "Ma\\\"eva Biret", "authors": "Maeva Biret and Michel Broniatowski", "title": "SAFIP: a streaming algorithm for inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm which aims at the resolution of inverse\nproblems of the form f(x) = 0, for x a vector of dimension d and f an arbitrary\nfunction with mild regularity condition. The set of solutions S may be\ninfinite. This algorithm produces a good coverage of S, with a limited number\nof evaluations of the function f. It is therefore appropriate for complex\nproblems where those evaluations are costly. Various examples are presented,\nwith d varying from 2 to 10. Proofs of convergence and of coverage of S are\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 09:26:46 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Biret", "Maeva", ""], ["Broniatowski", "Michel", ""]]}, {"id": "1609.08641", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt and Jorge Mateu", "title": "Graphical modelling of multivariate spatial point processes with\n  continuous marks", "comments": "arXiv admin note: text overlap with arXiv:1607.07083", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the second in a series of papers which combine graphical\nmodelling and marked spatial point patterns. Extending the previous results of\n\\cite Eckardt (2016a), we introduce a marked spatial dependence graph model\nwhich depicts the global dependence structure of quantitatively marked\nmulti-type points that occur in space based on the marked conditional partial\nspectral coherence. Most beneficial, no structural assumption with respect to\nthe characteristics in the data are to be made prior to analysis. This approach\npresents a computationally efficient method of pattern recognition in highly\nstructured and high dimensional multi-type spatial point processes where also\nquantitative marks are available. Unlike all previous methods, our new model\npermits the simultaneous analysis of all multivariate conditional\ninterrelations. The new technique is illustrated analysing the diameter at\nbreast hight of 37 different tree species recorded at 10053 locations in Duke\nForest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:06:57 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1609.08896", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas, and Matthew R Evans", "title": "A big-data spatial, temporal and network analysis of bovine tuberculosis\n  between wildlife (badgers) and cattle", "comments": "to appear in the journal Stochastic Environmental Research & Risk\n  Assessment", "journal-ref": null, "doi": "10.1007/s00477-016-1311-x", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bovine tuberculosis (TB) poses a serious threat for agricultural industry in\nseveral countries, it involves potential interactions between wildlife and\ncattle and creates societal problems in terms of human-wildlife conflict. This\nstudy addresses connectedness network analysis, the spatial, and temporal\ndynamics of TB between cattle in farms and the European badger (Meles meles)\nusing a large dataset generated by a calibrated agent based model. Results\nshowed that infected network connectedness was lower in badgers than in cattle.\nThe contribution of an infected individual to the mean distance of disease\nspread over time was considerably lower for badger than cattle; badgers mainly\nspread the disease locally while cattle infected both locally and across longer\ndistances. The majority of badger-induced infections occurred when individual\nbadgers leave their home sett, and this was positively correlated with badger\npopulation growth rates. Point pattern analysis indicated aggregation in the\nspatial pattern of TB prevalence in badger setts across all scales. The spatial\ndistribution of farms that were not TB free was aggregated at different scales\nthan the spatial distribution of infected badgers and became random at larger\nscales. The spatial cross correlation between infected badger setts and\ninfected farms revealed that generally infected setts and farms do not coexist\nexcept at few scales. Temporal autocorrelation detected a two year infection\ncycle for badgers, while there was both within the year and longer cycles for\ninfected cattle. Temporal cross correlation indicated that infection cycles in\nbadgers and cattle are negatively correlated. The implications of these results\nfor understanding the dynamics of the disease are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:03:06 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Moustakas", "Aristides", ""], ["Evans", "Matthew R", ""]]}, {"id": "1609.09033", "submitter": "David Kaplan", "authors": "David M. Kaplan and Yixiao Sun", "title": "Smoothed estimating equations for instrumental variables quantile\n  regression", "comments": "Authors' accepted manuscript; forthcoming in Econometric Theory,\n  copyright Cambridge University Press, published version at\n  http://dx.doi.org/10.1017/S0266466615000407", "journal-ref": "Econometric Theory 33 (2017) 105-157", "doi": "10.1017/S0266466615000407", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The moment conditions or estimating equations for instrumental variables\nquantile regression involve the discontinuous indicator function. We instead\nuse smoothed estimating equations (SEE), with bandwidth $h$. We show that the\nmean squared error (MSE) of the vector of the SEE is minimized for some $h>0$,\nleading to smaller asymptotic MSE of the estimating equations and associated\nparameter estimators. The same MSE-optimal $h$ also minimizes the higher-order\ntype I error of a SEE-based $\\chi^2$ test and increases size-adjusted power in\nlarge samples. Computation of the SEE estimator also becomes simpler and more\nreliable, especially with (more) endogenous regressors. Monte Carlo simulations\ndemonstrate all of these superior properties in finite samples, and we apply\nour estimator to JTPA data. Smoothing the estimating equations is not just a\ntechnical operation for establishing Edgeworth expansions and bootstrap\nrefinements; it also brings the real benefits of having more precise estimators\nand more powerful tests. Code for the estimator, simulations, and empirical\nexamples is available from the first author's website.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:33:54 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Kaplan", "David M.", ""], ["Sun", "Yixiao", ""]]}, {"id": "1609.09035", "submitter": "David Kaplan", "authors": "Matt Goldman and David M. Kaplan", "title": "Fractional order statistic approximation for nonparametric conditional\n  quantile inference", "comments": "Authors' accepted manuscript (Journal of Econometrics); DOI TBD", "journal-ref": "Journal of Econometrics 196 (2017) 331-346", "doi": "10.1016/j.jeconom.2016.09.015", "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using and extending fractional order statistic theory, we characterize the\n$O(n^{-1})$ coverage probability error of the previously proposed confidence\nintervals for population quantiles using $L$-statistics as endpoints in Hutson\n(1999). We derive an analytic expression for the $n^{-1}$ term, which may be\nused to calibrate the nominal coverage level to get\n$O\\bigl(n^{-3/2}[\\log(n)]^3\\bigr)$ coverage error. Asymptotic power is shown to\nbe optimal. Using kernel smoothing, we propose a related method for\nnonparametric inference on conditional quantiles. This new method compares\nfavorably with asymptotic normality and bootstrap methods in theory and in\nsimulations. Code is available from the second author's website for both\nunconditional and conditional methods, simulations, and empirical examples.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:34:27 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Goldman", "Matt", ""], ["Kaplan", "David M.", ""]]}, {"id": "1609.09429", "submitter": "Marius Hofert", "authors": "Marius Hofert and Wayne Oldford", "title": "Visualizing Dependence in High-Dimensional Data: An Application to S&P\n  500 Constituent Data", "comments": "The figures had to be massively reduced in size in order for the\n  paper to fulfill the 10M limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of a zenpath and a zenplot is introduced to search and detect\ndependence in high-dimensional data for model building and statistical\ninference. By using any measure of dependence between two random variables\n(such as correlation, Spearman's rho, Kendall's tau, tail dependence etc.), a\nzenpath can construct paths through pairs of variables in different ways, which\ncan then be laid out and displayed by a zenplot. The approach is illustrated by\ninvestigating tail dependence and model fit in constituent data of the S&P 500\nduring the financial crisis of 2007-2008. The corresponding Global Industry\nClassification Standard (GICS) sector information is also addressed.\n  Zenpaths and zenplots are useful tools for exploring dependence in\nhigh-dimensional data, for example, from the realm of finance, insurance and\nquantitative risk management. All presented algorithms are implemented using\nthe R package zenplots and all examples and graphics in the paper can be\nreproduced using the accompanying demo SP500.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:04:15 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 16:25:37 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Hofert", "Marius", ""], ["Oldford", "Wayne", ""]]}, {"id": "1609.09435", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "On the statistical properties of viral misinformation in online social\n  media", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2016.11.012", "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive diffusion of online social media allows for the rapid and\nuncontrolled spreading of conspiracy theories, hoaxes, unsubstantiated claims,\nand false news. Such an impressive amount of misinformation can influence\npolicy preferences and encourage behaviors strongly divergent from recommended\npractices. In this paper, we study the statistical properties of viral\nmisinformation in online social media. By means of methods belonging to Extreme\nValue Theory, we show that the number of extremely viral posts over time\nfollows a homogeneous Poisson process, and that the interarrival times between\nsuch posts are independent and identically distributed, following an\nexponential distribution. Moreover, we characterize the uncertainty around the\nrate parameter of the Poisson process through Bayesian methods. Finally, we are\nable to derive the predictive posterior probability distribution of the number\nof posts exceeding a certain threshold of shares over a finite interval of\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:31:44 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1609.09532", "submitter": "Donatello Telesca", "authors": "Qian Li, Damla Senturk, Catherine A. Sugar, Shanali Jeste, Charlotte\n  DiStefano, Joel Frohlich, Donatello Telesca", "title": "Inferring Brain Signals Synchronicity from a Sample of EEG Readings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring patterns of synchronous brain activity from a heterogeneous sample\nof electroencephalograms (EEG) is scientifically and methodologically\nchallenging. While it is intuitively and statistically appealing to rely on\nreadings from more than one individual in order to highlight recurrent patterns\nof brain activation, pooling information across subjects presents non-trivial\nmethodological problems. We discuss some of the scientific issues associated\nwith the understanding of synchronized neuronal activity and propose a\nmethodological framework for statistical inference from a sample of EEG\nreadings. Our work builds on classical contributions in time-series, clustering\nand functional data analysis, in an effort to reframe a challenging inferential\nproblem in the context of familiar analytical techniques. Some attention is\npaid to computational issues, with a proposal based on the combination of\nmachine learning and Bayesian techniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 21:26:03 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 23:06:36 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Li", "Qian", ""], ["Senturk", "Damla", ""], ["Sugar", "Catherine A.", ""], ["Jeste", "Shanali", ""], ["DiStefano", "Charlotte", ""], ["Frohlich", "Joel", ""], ["Telesca", "Donatello", ""]]}, {"id": "1609.09619", "submitter": "Philippe Besse", "authors": "Philippe Besse (IMT), Brendan Guillouet (IMT), Jean-Michel Loubes\n  (IMT)", "title": "Big Data analytics. Three use cases with R, Python and Spark", "comments": "in French, Apprentissage Statistique et Donn{\\'e}es Massives,\n  Technip, 2017, Journ{\\'e}es d'Etudes en Statistisque", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Management and analysis of big data are systematically associated with a data\ndistributed architecture in the Hadoop and now Spark frameworks. This article\noffers an introduction for statisticians to these technologies by comparing the\nperformance obtained by the direct use of three reference environments: R,\nPython Scikit-learn, Spark MLlib on three public use cases: character\nrecognition, recommending films, categorizing products. As main result, it\nappears that, if Spark is very efficient for data munging and recommendation by\ncollaborative filtering (non-negative factorization), current implementations\nof conventional learning methods (logistic regression, random forests) in MLlib\nor SparkML do not ou poorly compete habitual use of these methods (R, Python\nScikit-learn) in an integrated or undistributed architecture\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 07:35:49 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Besse", "Philippe", "", "IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1609.09638", "submitter": "Julia Mortera Julia Mortera", "authors": "Peter J. Green and Julia Mortera", "title": "Paternity testing and other inference about relationships from DNA\n  mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for inference about relationships between contributors to\na DNA mixture and other individuals of known genotype: a basic example would be\ntesting whether a contributor to a mixture is the father of a child of known\ngenotype. The evidence for such a relationship is evaluated as the likelihood\nratio for the specified relationship versus the alternative that there is no\nsuch relationship. We analyse real casework examples from a criminal case and a\ndisputed paternity case; in both examples part of the evidence was from a DNA\nmixture. DNA samples are of varying quality and therefore present challenging\nproblems in interpretation. Our methods are based on a recent statistical model\nfor DNA mixtures, in which a Bayesian network (BN) is used as a computational\ndevice; the present work builds on that approach, but makes more explicit use\nof the BN in the modelling. The R code for the analyses presented is freely\navailable as supplementary material.\n  We show how additional information of specific genotypes relevant to the\nrelationship under analysis greatly strengthens the resulting inference. We\nfind that taking full account of the uncertainty inherent in a DNA mixture can\nyield likelihood ratios very close to what one would obtain if we had a single\nsource DNA profile. Furthermore, the methods can be readily extended to analyse\ndifferent scenarios as our methods are not limited to the particular genotyping\nkits used in the examples, to the allele frequency databases used, to the\nnumbers of contributors assumed, to the number of traces analysed\nsimultaneously, nor to the specific hypotheses tested.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 08:55:07 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 14:10:31 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Green", "Peter J.", ""], ["Mortera", "Julia", ""]]}, {"id": "1609.09743", "submitter": "Antoine Deleforge", "authors": "Antoine Deleforge (PANAMA), Florence Forbes (MISTIS)", "title": "Rectified binaural ratio: A complex T-distributed feature for robust\n  sound localization", "comments": "European Signal Processing Conference, Aug 2016, Budapest, Hungary.\n  Proceedings of the 24th European Signal Processing Conference (EUSIPCO),\n  2016, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods in binaural sound source localization rely on some kind\nof aggregation of phase-and level-difference cues in the time-frequency plane.\nWhile different ag-gregation schemes exist, they are often heuristic and suffer\nin adverse noise conditions. In this paper, we introduce the rectified binaural\nratio as a new feature for sound source local-ization. We show that for\nGaussian-process point source signals corrupted by stationary Gaussian noise,\nthis ratio follows a complex t-distribution with explicit parameters. This new\nformulation provides a principled and statistically sound way to aggregate\nbinaural features in the presence of noise. We subsequently derive two simple\nand efficient methods for robust relative transfer function and time-delay\nestimation. Experiments on heavily corrupted simulated and speech signals\ndemonstrate the robustness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 14:15:46 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Deleforge", "Antoine", "", "PANAMA"], ["Forbes", "Florence", "", "MISTIS"]]}, {"id": "1609.09816", "submitter": "Xiao Liu", "authors": "Xiao Liu, Viknesswaran Gopal, Jayant Kalagnanam", "title": "A Spatio-Temporal Modeling Approach for Weather Radar Reflectivity Data\n  and Its Applications in Tropical Southeast Asia", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather radar echoes, correlated in both space and time, are the most\nimportant input data for short-term precipitation forecast. Motivated by real\ndatasets, this paper is concerned with the spatio-temporal modeling of\ntwo-dimensional radar reflectivity fields from a sequence of radar images.\nUnder a Lagrangian integration scheme, we model the radar reflectivity data by\na spatio-temporal conditional autoregressive process which is driven by two\nhidden sub-processes. The first sub-process is the dynamic velocity field which\ndetermines the motion of the weather system, while the second sub-process\ngoverns the growth or decay of the strength of radar reflectivity. The proposed\nmethod is demonstrated, and compared with existing methods, using the real\nradar data collected from the tropical southeast Asia. Note that, since the\ntropical storms are known to be highly chaotic and extremely difficult to be\npredicted, we only focus on the modeling of reflectivity data within a\nshort-period of time and consider the short-term prediction problem based on\nthe proposed model. This is often referred to as the nowcasting issue in the\nmeteorology society.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 17:08:32 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Liu", "Xiao", ""], ["Gopal", "Viknesswaran", ""], ["Kalagnanam", "Jayant", ""]]}, {"id": "1609.09830", "submitter": "Alexander Franks", "authors": "Alexander Franks, Alexander D'Amour, Daniel Cervone and Luke Bornn", "title": "Meta-Analytics: Tools for Understanding the Statistical Properties of\n  Sports Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports, there is a constant effort to improve metrics which assess player\nability, but there has been almost no effort to quantify and compare existing\nmetrics. Any individual making a management, coaching, or gambling decision is\nquickly overwhelmed with hundreds of statistics. We address this problem by\nproposing a set of \"meta-metrics\" which can be used to identify the metrics\nthat provide the most unique, reliable, and useful information for\ndecision-makers. Specifically, we develop methods to evalute metrics based on\nthree criteria: 1) stability: does the metric measure the same thing over time\n2) discrimination: does the metric differentiate between players and 3)\nindependence: does the metric provide new information? Our methods are easy to\nimplement and widely applicable so they should be of interest to the broader\nsports community. We demonstrate our methods in analyses of both NBA and NHL\nmetrics. Our results indicate the most reliable metrics and highlight how they\nshould be used by sports analysts. The meta-metrics also provide useful\ninsights about how to best construct new metrics which provide independent and\nreliable information about athletes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 17:39:21 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Franks", "Alexander", ""], ["D'Amour", "Alexander", ""], ["Cervone", "Daniel", ""], ["Bornn", "Luke", ""]]}]