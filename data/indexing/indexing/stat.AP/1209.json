[{"id": "1209.0089", "submitter": "Aaron Clauset", "authors": "Aaron Clauset, Ryan Woodard", "title": "Estimating the historical and future probabilities of large terrorist\n  events", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS614 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1838-1865", "doi": "10.1214/12-AOAS614", "report-no": "IMS-AOAS-AOAS614", "categories": "physics.data-an cs.LG physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantities with right-skewed distributions are ubiquitous in complex social\nsystems, including political conflict, economics and social networks, and these\nsystems sometimes produce extremely large events. For instance, the 9/11\nterrorist events produced nearly 3000 fatalities, nearly six times more than\nthe next largest event. But, was this enormous loss of life statistically\nunlikely given modern terrorism's historical record? Accurately estimating the\nprobability of such an event is complicated by the large fluctuations in the\nempirical distribution's upper tail. We present a generic statistical algorithm\nfor making such estimates, which combines semi-parametric models of tail\nbehavior and a nonparametric bootstrap. Applied to a global database of\nterrorist events, we estimate the worldwide historical probability of observing\nat least one 9/11-sized or larger event since 1968 to be 11-35%. These results\nare robust to conditioning on global variations in economic development,\ndomestic versus international events, the type of weapon used and a truncated\nhistory that stops at 1998. We then use this procedure to make a data-driven\nstatistical forecast of at least one similar event over the next decade.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 12:58:35 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 05:52:57 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2014 08:38:09 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Clauset", "Aaron", ""], ["Woodard", "Ryan", ""]]}, {"id": "1209.0253", "submitter": "Jamie Hall", "authors": "Jamie Hall, Michael K. Pitt, Robert Kohn", "title": "Bayesian inference for nonlinear structural time series models", "comments": "Typo correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses a partially adapted particle filter for estimating the\nlikelihood of a nonlinear structural econometric state space models whose state\ntransition density cannot be expressed in closed form. The filter generates the\ndisturbances in the state transition equation and allows for multiple modes in\nthe conditional disturbance distribution. The particle filter produces an\nunbiased estimate of the likelihood and so can be used to carry out Bayesian\ninference in a particle Markov chain Monte Carlo framework. We show empirically\nthat when the signal to noise ratio is high, the new filter can be much more\nefficient than the standard particle filter, in the sense that it requires far\nfewer particles to give the same accuracy. The new filter is applied to several\nsimulated and real examples and in particular to a dynamic stochastic general\nequilibrium model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 06:36:47 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 00:30:42 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Hall", "Jamie", ""], ["Pitt", "Michael K.", ""], ["Kohn", "Robert", ""]]}, {"id": "1209.0364", "submitter": "Brian Williams Dr", "authors": "Brian G. Williams, Eleanor Gouws, John Hargrove, Cari van Schalkwyk,\n  and Hilmarie Brand", "title": "Pre-exposure prophylaxis (PrEP) versus treatment-as-prevention (TasP)\n  for the control of HIV: Where does the balance lie?", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anti-retroviral drugs can reduce the infectiousness of people living with HIV\nby about 96%--treatment as prevention or TasP--and can reduce the risk of being\ninfected by an HIV positive person by about 70%--pre-exposure prophylaxis or\nPrEP--raising the prospect of using anti-retroviral drugs to stop the epidemic\nof HIV. The question as to which is more effective, more affordable and more\ncost effective, and under what conditions, continues to be debated in the\nscientific literature. Here we compare TasP and PreP in order to determine the\nconditions under which each strategy is favourable. This analysis suggests that\nwhere the incidence of HIV is less than 5% or the risk-reduction under PrEP is\nless than 50%, TasP is favoured over PrEP; otherwise PrEP is favoured over\nTasP. The potential for using PreP should therefore be restricted to those\namong whom the annual incidence of HIV is greater than 5% and TasP reduces\ntransmission by more than 50%. PreP should be considered for commercial sex\nworkers, young women aged about 20 to 25 years, men-who-have-sex with men, or\nintravenous drug users, but only where the incidence of HIV is high.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 11:46:15 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Williams", "Brian G.", ""], ["Gouws", "Eleanor", ""], ["Hargrove", "John", ""], ["van Schalkwyk", "Cari", ""], ["Brand", "Hilmarie", ""]]}, {"id": "1209.0565", "submitter": "Stefano Andreon", "authors": "S. Andreon (INAF-OABrera)", "title": "The enrichment history of the intracluster medium: a Bayesian approach", "comments": "A&A, in press", "journal-ref": null, "doi": "10.1051/0004-6361/201219194", "report-no": null, "categories": "astro-ph.CO astro-ph.HE physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work measures the evolution of the iron content in galaxy clusters by a\nrigorous analysis of the data of 130 clusters at 0.1<z<1.3. This task is made\ndifficult by a) the low signal-to-noise ratio of abundance measurements and the\nupper limits, b) possible selection effects, c) boundaries in the parameter\nspace, d) non-Gaussian errors, e) the intrinsic variety of the objects studied,\nand f) abundance systematics. We introduce a Bayesian model to address all\nthese issues at the same time, thus allowing cross-talk (covariance). On\nsimulated data, the Bayesian fit recovers the input enrichment history, unlike\nin standard analysis. After accounting for a possible dependence on X-ray\ntemperature, for metal abundance systematics, and for the intrinsic variety of\nstudied objects, we found that the present-day metal content is not reached\neither at high or at low redshifts, but gradually over time: iron abundance\nincreases by a factor 1.5 in the 7 Gyr sampled by the data. Therefore, feedback\nin metal abundance does not end at high redshift. Evolution is established with\na moderate amount of evidence, 19 to 1 odds against faster or slower metal\nenrichment histories. We quantify, for the first time, the intrinsic spread in\nmetal abundance, 18+/-3 %, after correcting for the effect of evolution, X-ray\ntemperature, and metal abundance systematics. Finally, we also present an\nanalytic approximation of the X-ray temperature and metal abundance likelihood\nfunctions, which are useful for other regression fitting involving these\nparameters. The data for the 130 clusters and code used for the stochastic\ncomputation are provided with the paper.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 08:40:42 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Andreon", "S.", "", "INAF-OABrera"]]}, {"id": "1209.0624", "submitter": "R.J. Vanderbei", "authors": "Robert J. Vanderbei", "title": "Local Warming", "comments": "12 pages, 5 figures, to appear in SIAM Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using 55 years of daily average temperatures from a local weather station, I\nmade a least-absolute-deviations (LAD) regression model that accounts for three\neffects: seasonal variations, the 11-year solar cycle, and a linear trend. The\nmodel was formulated as a linear programming problem and solved using widely\navailable optimization software. The solution indicates that temperatures have\ngone up by about 2 degrees Fahrenheit over the 55 years covered by the data. It\nalso correctly identifies the known phase of the solar cycle; i.e., the date of\nthe last solar minimum. It turns out that the maximum slope of the solar cycle\nsinusoid in the regression model is about the same size as the slope produced\nby the linear trend. The fact that the solar cycle was correctly extracted by\nthe model is a strong indicator that effects of this size, in particular the\nslope of the linear trend, can be accurately determined from the 55 years of\ndata analyzed.\n  The main purpose for doing this analysis is to demonstrate that it is easy to\nfind and analyze archived temperature data for oneself. In particular, this\nproblem makes a good class project for upper-level undergraduate courses in\noptimization or in statistics.\n  It is worth noting that a similar least-squares model failed to characterize\nthe solar cycle correctly and hence even though it too indicates that\ntemperatures have been rising locally, one can be less confident in this\nresult.\n  The paper ends with a section presenting similar results from a few thousand\nsites distributed world-wide, some results from a modification of the model\nthat includes both temperature and humidity, as well as a number of suggestions\nfor future work and/or ideas for enhancements that could be used as classroom\nprojects.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 12:15:29 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Vanderbei", "Robert J.", ""]]}, {"id": "1209.0651", "submitter": "Mohamed Abdel-Hameed", "authors": "Mohamed Abdel-Hameed", "title": "Optimal Control of dams using P(M,Lambda,tau) policies when the input\n  process is an inverse Gaussian process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the P(M,lambda,tau) maintenance policy of a dam using the total\ndiscounted and long-run average costs, when the input process is inverse\nGaussian.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 14:05:27 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Abdel-Hameed", "Mohamed", ""]]}, {"id": "1209.0790", "submitter": "R.J. Vanderbei", "authors": "Robert J. Vanderbei, Gordon Scharf and Daniel Marlow", "title": "A Regression Approach to Fairer Grading", "comments": "18 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a statistical procedure to account for differences\nin grading practices from one course to another. The goal is to define a course\n\"inflatedness\" and a student \"aptitude\" that best captures one's intuitive\nnotions of these concepts.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 20:14:51 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Vanderbei", "Robert J.", ""], ["Scharf", "Gordon", ""], ["Marlow", "Daniel", ""]]}, {"id": "1209.0897", "submitter": "Frederic Pascal", "authors": "Melanie Mahot, Philippe Forster, Frederic Pascal and Jean-Philippe\n  Ovarlez", "title": "Asymptotic properties of robust complex covariance matrix estimates", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2259823", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical signal processing applications, the estimation of\nnuisance parameters and parameters of interest is strongly linked to the\nresulting performance. Generally, these applications deal with complex data.\nThis paper focuses on covariance matrix estimation problems in non-Gaussian\nenvironments and particularly, the M-estimators in the context of elliptical\ndistributions. Firstly, this paper extends to the complex case the results of\nTyler in [1]. More precisely, the asymptotic distribution of these estimators\nas well as the asymptotic distribution of any homogeneous function of degree 0\nof the M-estimates are derived. On the other hand, we show the improvement of\nsuch results on two applications: DOA (directions of arrival) estimation using\nthe MUSIC (MUltiple SIgnal Classification) algorithm and adaptive radar\ndetection based on the ANMF (Adaptive Normalized Matched Filter) test.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 09:03:47 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 14:58:37 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Mahot", "Melanie", ""], ["Forster", "Philippe", ""], ["Pascal", "Frederic", ""], ["Ovarlez", "Jean-Philippe", ""]]}, {"id": "1209.1270", "submitter": "Anna  Deluca", "authors": "Alvaro Corral and Anna Deluca and Ramon Ferrer-i-Cancho", "title": "A practical recipe to fit discrete power-law distributions", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power laws pervade statistical physics and complex systems, but,\ntraditionally, researchers in these fields have paid little attention to\nproperly fit these distributions. Who has not seen (or even shown) a log-log\nplot of a completely curved line pretending to be a power law? Recently,\nClauset et al. have proposed a method to decide if a set of values of a\nvariable has a distribution whose tail is a power law. The key of their\nprocedure is the identification of the minimum value of the variable for which\nthe fit holds, which is selected as the value for which the Kolmogorov-Smirnov\ndistance between the empirical distribution and its maximum-likelihood fit is\nminimum. However, it has been shown that this method can reject the power-law\nhypothesis even in the case of power-law simulated data. Here we propose a\nsimpler selection criterion, which is illustrated with the more involving case\nof discrete power-law distributions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 12:43:14 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Corral", "Alvaro", ""], ["Deluca", "Anna", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1209.1341", "submitter": "Xiang Zhou", "authors": "Xiang Zhou and Peter Carbonetto and Matthew Stephens", "title": "Polygenic Modeling with Bayesian Sparse Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both linear mixed models (LMMs) and sparse regression models are widely used\nin genetics applications, including, recently, polygenic modeling in\ngenome-wide association studies. These two approaches make very different\nassumptions, so are expected to perform well in different situations. However,\nin practice, for a given data set one typically does not know which assumptions\nwill be more accurate. Motivated by this, we consider a hybrid of the two,\nwhich we refer to as a \"Bayesian sparse linear mixed model\" (BSLMM) that\nincludes both these models as special cases. We address several key\ncomputational and statistical issues that arise when applying BSLMM, including\nappropriate prior specification for the hyper-parameters, and a novel Markov\nchain Monte Carlo algorithm for posterior inference. We apply BSLMM and compare\nit with other methods for two polygenic modeling applications: estimating the\nproportion of variance in phenotypes explained (PVE) by available genotypes,\nand phenotype (or breeding value) prediction. For PVE estimation, we\ndemonstrate that BSLMM combines the advantages of both standard LMMs and sparse\nregression modeling. For phenotype prediction it considerably outperforms\neither of the other two methods, as well as several other large-scale\nregression methods previously suggested for this problem. Software implementing\nour method is freely available from\nhttp://stephenslab.uchicago.edu/software.html\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 16:48:45 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 22:30:27 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Zhou", "Xiang", ""], ["Carbonetto", "Peter", ""], ["Stephens", "Matthew", ""]]}, {"id": "1209.1826", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Debapriya Sengupta", "title": "A spatio-spectral hybridization for edge preservation and noisy image\n  restoration via local parametric mixtures and Lagrangian relaxation", "comments": "29 Pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a fully unsupervised statistical method for edge\npreserving image restoration and compression using a spatial decomposition\nscheme. Smoothed maximum likelihood is used for local estimation of edge pixels\nfrom mixture parametric models of local templates. For the complementary smooth\npart the traditional L2-variational problem is solved in the Fourier domain\nwith Thin Plate Spline (TPS) regularization. It is well known that naive\nFourier compression of the whole image fails to restore a piece-wise smooth\nnoisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as\nrelative frequency histograms of samples from bi-variate densities where the\nsample sizes might be unknown. The set of discontinuities is assumed to be\ncompletely unsupervised Lebesgue-null, compact subset of the plane in the\ncontinuous formulation of the problem. Proposed spatial decomposition uses a\nwidely used topological concept, partition of unity. The decision on edge pixel\nneighborhoods are made based on the multiple testing procedure of Holms.\nStatistical summary of the final output is decomposed into two layers of\ninformation extraction, one for the subset of edge pixels and the other for the\nsmooth region. Robustness is also demonstrated by applying the technique on\nnoisy degradation of clean images.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 18:23:21 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Basu", "Kinjal", ""], ["Sengupta", "Debapriya", ""]]}, {"id": "1209.2072", "submitter": "Adityanand Guntuboyina", "authors": "Adityanand Guntuboyina and Russell Barbour and Robert Heimer", "title": "On the impossibility of constructing good population mean estimators in\n  a realistic Respondent Driven Sampling model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for population mean estimation from data collected by\nRespondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator\ntogether with a set of assumptions on the sampling model under which the\ninclusion probabilities can be determined from the information contained in the\ndata. In this paper, we argue that such set of assumptions are too simplistic\nto be realistic and that under realistic sampling models, the situation is far\nmore complicated. Specifically, we study a realistic RDS sampling model that is\nmotivated by a real world RDS dataset. We show that, for this model, the\ninclusion probabilities, which are necessary for the application of the\nHorvitz-Thompson estimator, can not be determined by the information in the\nsample alone. An implication is that, unless additional information about the\nunderlying population network is obtained, it is hopeless to conceive of a\ngeneral theory of population mean estimation from current RDS data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 17:42:25 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 18:12:42 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Barbour", "Russell", ""], ["Heimer", "Robert", ""]]}, {"id": "1209.2074", "submitter": "Sterling Sawaya", "authors": "Sterling Sawaya and Steffen Klaere", "title": "Extinction in a branching process: Why some of the fittest strategies\n  cannot guarantee survival", "comments": "Best case extrema added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fitness of a biological strategy is typically measured by its expected\nreproductive rate, the first moment of its offspring distribution. However,\nstrategies with high expected rates can also have high probabilities of\nextinction. A similar situation is found in gambling and investment, where\nstrategies with a high expected payoff can also have a high risk of ruin. We\ntake inspiration from the gambler's ruin problem to examine how extinction is\nrelated to population growth. Using moment theory we demonstrate how higher\nmoments can impact the probability of extinction. We discuss how moments can be\nused to find bounds on the extinction probability, focusing on s-convex\nordering of random variables, a method developed in actuarial science. This\napproach generates \"best case\" and \"worst case\" scenarios to provide upper and\nlower bounds on the probability of extinction. Our results demonstrate that\neven the most fit strategies can have high probabilities of extinction.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 17:47:02 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2013 03:30:12 GMT"}, {"version": "v3", "created": "Thu, 16 May 2013 03:18:01 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Sawaya", "Sterling", ""], ["Klaere", "Steffen", ""]]}, {"id": "1209.2076", "submitter": "Dennis Sun", "authors": "Dennis L. Sun, Julius O. Smith III", "title": "Estimating a Signal from a Magnitude Spectrogram via Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of recovering a signal from the magnitude of its short-time\nFourier transform (STFT) is a longstanding one in audio signal processing.\nExisting approaches rely on heuristics that often perform poorly because of the\nnonconvexity of the problem. We introduce a formulation of the problem that\nlends itself to a tractable convex program. We observe that our method yields\nbetter reconstructions than the standard Griffin-Lim algorithm. We provide an\nalgorithm and discuss practical implementation details, including how the\nmethod can be scaled up to larger examples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 17:56:02 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Sun", "Dennis L.", ""], ["Smith", "Julius O.", "III"]]}, {"id": "1209.2204", "submitter": "Henk van Elst", "authors": "Ekaterina Svetlova and Henk van Elst (Karlshochschule International\n  University)", "title": "How is non-knowledge represented in economic theory?", "comments": "18 pages, LaTeX2e, hyperlinked references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we address the question of how non-knowledge about future\nevents that influence economic agents' decisions in choice settings has been\nformally represented in economic theory up to date. To position our discussion\nwithin the ongoing debate on uncertainty, we provide a brief review of\nhistorical developments in economic theory and decision theory on the\ndescription of economic agents' choice behaviour under conditions of\nuncertainty, understood as either (i) ambiguity, or (ii) unawareness.\nAccordingly, we identify and discuss two approaches to the formalisation of\nnon-knowledge: one based on decision-making in the context of a state space\nrepresenting the exogenous world, as in Savage's axiomatisation and some\nsuccessor concepts (ambiguity as situations with unknown probabilities), and\none based on decision-making over a set of menus of potential future\nopportunities, providing the possibility of derivation of agents' subjective\nstate spaces (unawareness as situation with imperfect subjective knowledge of\nall future events possible). We also discuss impeding challenges of the\nformalisation of non-knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 19:55:46 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Svetlova", "Ekaterina", "", "Karlshochschule International\n  University"], ["van Elst", "Henk", "", "Karlshochschule International\n  University"]]}, {"id": "1209.2298", "submitter": "Nassim N. Taleb", "authors": "Nassim N. Taleb", "title": "The Future Has Thicker Tails than the Past: Model Error As Branching\n  Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ex ante forecast outcomes should be interpreted as counterfactuals (potential\nhistories), with errors as the spread between outcomes. Reapplying measurements\nof uncertainty about the estimation errors of the estimation errors of an\nestimation leads to branching counterfactuals. Such recursions of epistemic\nuncertainty have markedly different distributial properties from conventional\nsampling error. Nested counterfactuals of error rates invariably lead to fat\ntails, regardless of the probability distribution used, and to powerlaws under\nsome conditions. A mere .01% branching error rate about the STD (itself an\nerror rate), and .01% branching error rate about that error rate, etc.\n(recursing all the way) results in explosive (and infinite) higher moments than\n1. Missing any degree of regress leads to the underestimation of small\nprobabilities and concave payoffs (a standard example of which is Fukushima).\nThe paper states the conditions under which higher order rates of uncertainty\n(expressed in spreads of counterfactuals) alters the shapes the of final\ndistribution and shows which a priori beliefs about conterfactuals are needed\nto accept the reliability of conventional probabilistic methods (thin tails or\nmildly fat tails).\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 12:18:56 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Taleb", "Nassim N.", ""]]}, {"id": "1209.2433", "submitter": "James Risk", "authors": "James Risk", "title": "Correlations between Google search data and Mortality Rates", "comments": "4 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by correlations recently discovered between Google search data and\nfinancial markets, we show correlations between Google search data mortality\nrates. Words with negative connotations may provide for increased mortality\nrates, while words with positive connotations may provide for decreased\nmortality rates, and so statistical methods were employed to determine to\ninvestigate further.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 20:26:48 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2012 22:38:57 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Risk", "James", ""]]}, {"id": "1209.2486", "submitter": "Baiyang Wang", "authors": "Baiyang Wang", "title": "On sampling social networking services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article aims at summarizing the existing methods for sampling social\nnetworking services and proposing a faster confidence interval for related\nsampling methods. It also includes comparisons of common network sampling\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 03:10:53 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 05:56:44 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Wang", "Baiyang", ""]]}, {"id": "1209.2684", "submitter": "Tina Eliassi-Rad", "authors": "Michele Berlingerio, Danai Koutra, Tina Eliassi-Rad, Christos\n  Faloutsos", "title": "NetSimile: A Scalable Approach to Size-Independent Network Similarity", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of k networks, possibly with different sizes and no overlaps in\nnodes or edges, how can we quickly assess similarity between them, without\nsolving the node-correspondence problem? Analogously, how can we extract a\nsmall number of descriptive, numerical features from each graph that\neffectively serve as the graph's \"signature\"? Having such features will enable\na wealth of graph mining tasks, including clustering, outlier detection,\nvisualization, etc.\n  We propose NetSimile -- a novel, effective, and scalable method for solving\nthe aforementioned problem. NetSimile has the following desirable properties:\n(a) It gives similarity scores that are size-invariant. (b) It is scalable,\nbeing linear on the number of edges for \"signature\" vector extraction. (c) It\ndoes not need to solve the node-correspondence problem. We present extensive\nexperiments on numerous synthetic and real graphs from disparate domains, and\nshow NetSimile's superiority over baseline competitors. We also show how\nNetSimile enables several mining tasks such as clustering, visualization,\ndiscontinuity detection, network transfer learning, and re-identification\nacross networks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 18:32:55 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Berlingerio", "Michele", ""], ["Koutra", "Danai", ""], ["Eliassi-Rad", "Tina", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1209.2759", "submitter": "Adel Javanmard", "authors": "Adel Javanmard, Maya Haridasan and Li Zhang", "title": "Multi-track Map Matching", "comments": "11 pages, 8 figures, short version appears in 20th International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS\n  2012). Extended Abstract in Proceedings of the 10th international conference\n  on Mobile systems, applications, and services (MobiSys 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for matching user tracks, consisting of time-ordered\nlocation points, to paths in the road network. Previous work has focused on the\nscenario where the location data is linearly ordered and consists of fairly\ndense and regular samples. In this work, we consider the \\emph{multi-track map\nmatching}, where the location data comes from different trips on the same\nroute, each with very sparse samples. This captures the realistic scenario\nwhere users repeatedly travel on regular routes and samples are sparsely\ncollected, either due to energy consumption constraints or because samples are\nonly collected when the user actively uses a service. In the multi-track\nproblem, the total set of combined locations is only partially ordered, rather\nthan globally ordered as required by previous map-matching algorithms. We\npropose two methods, the iterative projection scheme and the graph Laplacian\nscheme, to solve the multi-track problem by using a single-track map-matching\nsubroutine. We also propose a boosting technique which may be applied to either\napproach to improve the accuracy of the estimated paths. In addition, in order\nto deal with variable sampling rates in single-track map matching, we propose a\nmethod based on a particular regularized cost function that can be adapted for\ndifferent sampling rates and measurement errors. We evaluate the effectiveness\nof our techniques for reconstructing tracks under several different\nconfigurations of sampling error and sampling rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 01:44:12 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Javanmard", "Adel", ""], ["Haridasan", "Maya", ""], ["Zhang", "Li", ""]]}, {"id": "1209.2829", "submitter": "Ruth Heller", "authors": "Ruth Heller, Daniel Yekutieli", "title": "Replicability analysis for genome-wide association studies", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS697 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 481-498", "doi": "10.1214/13-AOAS697", "report-no": "IMS-AOAS-AOAS697", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paramount importance of replicating associations is well recognized in\nthe genome-wide associaton (GWA) research community, yet methods for assessing\nreplicability of associations are scarce. Published GWA studies often combine\nseparately the results of primary studies and of the follow-up studies.\nInformally, reporting the two separate meta-analyses, that of the primary\nstudies and follow-up studies, gives a sense of the replicability of the\nresults. We suggest a formal empirical Bayes approach for discovering whether\nresults have been replicated across studies, in which we estimate the optimal\nrejection region for discovering replicated results. We demonstrate, using\nrealistic simulations, that the average false discovery proportion of our\nmethod remains small. We apply our method to six type two diabetes (T2D) GWA\nstudies. Out of 803 SNPs discovered to be associated with T2D using a typical\nmeta-analysis, we discovered 219 SNPs with replicated associations with T2D. We\nrecommend complementing a meta-analysis with a replicability analysis for GWA\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 09:32:55 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 13:23:14 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 14:07:06 GMT"}, {"version": "v4", "created": "Tue, 29 Apr 2014 13:35:05 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Heller", "Ruth", ""], ["Yekutieli", "Daniel", ""]]}, {"id": "1209.3029", "submitter": "Graham Coop", "authors": "Torsten G\\\"unther and Graham Coop", "title": "Robust identification of local adaptation from allele frequencies", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing allele frequencies among populations that differ in environment has\nlong been a tool for detecting loci involved in local adaptation. However, such\nanalyses are complicated by an imperfect knowledge of population allele\nfrequencies and neutral correlations of allele frequencies among populations\ndue to shared population history and gene flow. Here we develop a set of\nmethods to robustly test for unusual allele frequency patterns, and\ncorrelations between environmental variables and allele frequencies while\naccounting for these complications based on a Bayesian model previously\nimplemented in the software Bayenv. Using this model, we calculate a set of\n`standardized allele frequencies' that allows investigators to apply tests of\ntheir choice to multiple populations, while accounting for sampling and\ncovariance due to population history. We illustrate this first by showing that\nthese standardized frequencies can be used to calculate powerful tests to\ndetect non-parametric correlations with environmental variables, which are also\nless prone to spurious results due to outlier populations. We then demonstrate\nhow these standardized allele frequencies can be used to construct a test to\ndetect SNPs that deviate strongly from neutral population structure. This test\nis conceptually related to FST but should be more powerful as we account for\npopulation history. We also extend the model to next-generation sequencing of\npopulation pools, which is a cost-efficient way to estimate population allele\nfrequencies, but it implies an additional level of sampling noise. The utility\nof these methods is demonstrated in simulations and by re-analyzing human SNP\ndata from the HGDP populations. An implementation of our method will be\navailable from http://gcbias.org.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 20:27:09 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["G\u00fcnther", "Torsten", ""], ["Coop", "Graham", ""]]}, {"id": "1209.3522", "submitter": "Benjamin Nachman", "authors": "Benjamin Nachman and Tom Rudelius", "title": "Evidence for Conservatism in LHC SUSY Searches", "comments": "8 pages, 2 figures. v4: Published in EPJ plus", "journal-ref": "EPJ Plus, 127 12 (2012) 157", "doi": "10.1140/epjp/i2012-12157-0", "report-no": null, "categories": "stat.AP hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard in the high energy physics community for claiming discovery of\nnew physics is a $5\\sigma$ excess in the observed signal over the estimated\nbackground. While a $3\\sigma$ excess is not enough to claim discovery, it is\ncertainly enough to pique the interest of both experimentalists and theorists.\nHowever, with a large number of searches performed by both the ATLAS and CMS\ncollaborations at the LHC, one expects a nonzero number of multi-$\\sigma$\nresults simply due to statistical fluctuations in the no-signal scenario. Our\nanalysis examines the distribution of p-values for CMS and ATLAS supersymmetry\n(SUSY) searches using the full 2011 data set to determine if the collaborations\nare being overly conservative in their analyses. We find that there is a\nstatistically significant excess of `medium' $\\sigma$ values at the level of\n$p=0.005$, indicating over-conservativism in the estimation of uncertainties.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2012 21:21:58 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2012 17:56:41 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2012 00:21:58 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2012 13:25:07 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Nachman", "Benjamin", ""], ["Rudelius", "Tom", ""]]}, {"id": "1209.3534", "submitter": "Massimiliano Ignaccolo", "authors": "Massimiliano Ignaccolo, Carlo De Michele", "title": "Phase space parametrization of rain: the inadequacy of the gamma\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Gamma distribution is not an adequate fit for the\nprobability density function of drop diameters using the Kolmogorov-Smirnov\ngoodness of fit test. We propose a different parametrization of drop size\ndistributions, which not depending by any particular functional form, is based\non the adoption of standardized central moments. The first three standardized\ncentral moments are sufficient to characterize the distribution of drop\ndiamters at the ground. These parameters together with the drop count form a\n4-tuple which fully describe the variability of the drop size distributions.\nThe Cartesian product of this 4-tuple of parameters is the rainfall phase\nspace. Using disdrometer data from 10 different locations we identify\ninvariant, not depending on location, properties of the rainfall phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 00:59:10 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Ignaccolo", "Massimiliano", ""], ["De Michele", "Carlo", ""]]}, {"id": "1209.3766", "submitter": "Nikolai Gagunashvili", "authors": "N. D. Gagunashvili and M. Schmelling", "title": "Kernel based unfolding of data obtained from detectors with finite\n  resolution and limited acceptance", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kernel based procedure for correcting experimental data for distortions due\nto the finite resolution and limited detector acceptance is presented. The\nunfolding problem is known to be an ill-posed problem that can not be solved\nwithout some a priori information about solution such as, for example,\nsmoothness or positivity. In the approach presented here the true distribution\nis estimated by a weighted sum of kernels, with the width of the kernels acting\nas a regularization parameter responsible for the smoothness of the result.\nCross-validation is used to determine an optimal value for this parameter. A\nnumerical example with a simulation study of systematical and statistical\nerrors is presented to illustrate the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 14:42:12 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Gagunashvili", "N. D.", ""], ["Schmelling", "M.", ""]]}, {"id": "1209.3775", "submitter": "Henrik Brink", "authors": "Henrik Brink, Joseph W. Richards, Dovi Poznanski, Joshua S. Bloom,\n  John Rice, Sahand Negahban, Martin Wainwright", "title": "Using Machine Learning for Discovery in Synoptic Survey Imaging", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/stt1306", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern time-domain surveys continuously monitor large swaths of the sky to\nlook for astronomical variability. Astrophysical discovery in such data sets is\ncomplicated by the fact that detections of real transient and variable sources\nare highly outnumbered by bogus detections caused by imperfect subtractions,\natmospheric effects and detector artefacts. In this work we present a machine\nlearning (ML) framework for discovery of variability in time-domain imaging\nsurveys. Our ML methods provide probabilistic statements, in near real time,\nabout the degree to which each newly observed source is astrophysically\nrelevant source of variable brightness. We provide details about each of the\nanalysis steps involved, including compilation of the training and testing\nsets, construction of descriptive image-based and contextual features, and\noptimization of the feature subset and model tuning parameters. Using a\nvalidation set of nearly 30,000 objects from the Palomar Transient Factory, we\ndemonstrate a missed detection rate of at most 7.7% at our chosen\nfalse-positive rate of 1% for an optimized ML classifier of 23 features,\nselected to avoid feature correlation and over-fitting from an initial library\nof 42 attributes. Importantly, we show that our classification methodology is\ninsensitive to mis-labelled training data up to a contamination of nearly 10%,\nmaking it easier to compile sufficient training sets for accurate performance\nin future surveys. This ML framework, if so adopted, should enable the\nmaximization of scientific gain from future synoptic survey and enable fast\nfollow-up decisions on the vast amounts of streaming data produced by such\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 20:00:02 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Brink", "Henrik", ""], ["Richards", "Joseph W.", ""], ["Poznanski", "Dovi", ""], ["Bloom", "Joshua S.", ""], ["Rice", "John", ""], ["Negahban", "Sahand", ""], ["Wainwright", "Martin", ""]]}, {"id": "1209.3795", "submitter": "Raul Jimenez", "authors": "Raul Jimenez, Manuel Hidalgo", "title": "Keep an Eye on Venezuelan Elections! (Vigilad las elecciones\n  venezolanas)", "comments": "English version, pp. 1-11. Spanish version, pp. 12-23", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the 2004 recall referendum, an important opposition sector to\nPresident Chavez has questioned the integrity of the Venezuelan electoral\nsystem, and casts doubt on the legitimacy and impartiality of the upcoming 2012\npresidential elections on October 7. After carrying out a forensic analysis on\nVenezuelan elections and referendums celebrated since 1998 until 2012, we reach\ntwo controversial conclusions: on one hand, we cannot rule out the hypothesis\nof fraud in elections run by the current regime. On the other, if fraud has\nbeen committed, it has not been decisive on results of past elections. In other\nwords, the winner would have been the same in clean elections. Only in a\nscenario of tight results, as 2012 elections could be, fraud would constitute a\ndecisive factor.\n  ----\n  A partir del refer\\'endum revocatorio del 2004, un importante sector opositor\nal Presidente Ch\\'avez ha cuestionado la integridad del sistema electoral\nvenezolano y tiene dudas acerca de la legitimidad e imparcialidad de las\nfuturas elecciones presidenciales del 7 de Octubre del 2012. Practicando un\nan\\'alisis forense a elecciones y refer\\'endums venezolanos desde 1998 hasta\n2012 llegamos a dos controversiales conclusiones: por un lado, no podemos\ndescartar la hip\\'otesis de fraude en comicios administrados por el actual\nr\\'egimen. Por otro lado, de haberse cometido fraude en pasadas elecciones,\neste no ha sido determinante en los resultados. Es decir, el ganador hubiese\nsido el mismo en elecciones limpias. S\\'olo en un escenario de resultados\najustados, como pudiera ser el 2012, el fraude podr\\'ia ser determinante.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 20:42:14 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 09:04:44 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Jimenez", "Raul", ""], ["Hidalgo", "Manuel", ""]]}, {"id": "1209.3990", "submitter": "Kalyani Krishnamurthy", "authors": "Kalyani Krishnamurthy, Waheed U. Bajwa and Rebecca Willett", "title": "Level set estimation from projection measurements: Performance\n  guarantees and fast computation", "comments": "23 pages, 20 figures", "journal-ref": "SIAM J. Imaging Sciences, vol. 6, no. 4, pp. 2047-2074, Oct. 2013", "doi": "10.1137/120891927", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the level set of a function (i.e., regions where the function\nexceeds some value) is an important problem with applications in digital\nelevation mapping, medical imaging, astronomy, etc. In many applications, the\nfunction of interest is not observed directly. Rather, it is acquired through\n(linear) projection measurements, such as tomographic projections,\ninterferometric measurements, coded-aperture measurements, and random\nprojections associated with compressed sensing. This paper describes a new\nmethodology for rapid and accurate estimation of the level set from such\nprojection measurements. The key defining characteristic of the proposed\nmethod, called the projective level set estimator, is its ability to estimate\nthe level set from projection measurements without an intermediate\nreconstruction step. This leads to significantly faster computation relative to\nheuristic \"plug-in\" methods that first estimate the function, typically with an\niterative algorithm, and then threshold the result. The paper also includes a\nrigorous theoretical analysis of the proposed method, which utilizes the recent\nresults from the non-asymptotic theory of random matrices results from the\nliterature on concentration of measure and characterizes the estimator's\nperformance in terms of geometry of the measurement operator and 1-norm of the\ndiscretized function.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 15:28:42 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 21:06:51 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Krishnamurthy", "Kalyani", ""], ["Bajwa", "Waheed U.", ""], ["Willett", "Rebecca", ""]]}, {"id": "1209.4006", "submitter": "Fran\\c{c}ois Giraud", "authors": "Fran\\c{c}ois Giraud, Pierre Minvielle, Marc Sancandi, Pierre Del Moral", "title": "Rao-Blackwellised Interacting Markov Chain Monte Carlo for\n  Electromagnetic Scattering Inversion", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1088/1742-6596/386/1/012008", "report-no": null, "categories": "stat.AP math.PR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following electromagnetism (EM) inverse problem is addressed. It consists\nin estimating local radioelectric properties of materials recovering an object\nfrom the global EM scattering measurement, at various incidences and wave\nfrequencies. This large scale ill-posed inverse problem is explored by an\nintensive exploitation of an efficient 2D Maxwell solver, distributed on High\nPerformance Computing (HPC) machines. Applied to a large training data set, a\nstatistical analysis reduces the problem to a simpler probabilistic metamodel,\non which Bayesian inference can be performed. Considering the radioelectric\nproperties as a dynamic stochastic process, evolving in function of the\nfrequency, it is shown how advanced Markov Chain Monte Carlo methods, called\nSequential Monte Carlo (SMC) or interacting particles, can provide estimations\nof the EM properties of each material, and their associated uncertainties.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 16:07:25 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 16:18:52 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Giraud", "Fran\u00e7ois", ""], ["Minvielle", "Pierre", ""], ["Sancandi", "Marc", ""], ["Del Moral", "Pierre", ""]]}, {"id": "1209.4551", "submitter": "Serge Iovleff", "authors": "Serge Iovleff (LPP, INRIA Lille - Nord Europe)", "title": "Probabilistic Auto-Associative Models and Semi-Linear PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Associative models cover a large class of methods used in data analysis.\nIn this paper, we describe the generals properties of these models when the\nprojection component is linear and we propose and test an easy to implement\nProbabilistic Semi-Linear Auto- Associative model in a Gaussian setting. We\nshow it is a generalization of the PCA model to the semi-linear case. Numerical\nexperiments on simulated datasets and a real astronomical application highlight\nthe interest of this approach\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 14:42:48 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Iovleff", "Serge", "", "LPP, INRIA Lille - Nord Europe"]]}, {"id": "1209.4678", "submitter": "Taras Lazariv", "authors": "Taras Lazariv, Wolfgang Schmid, and Svitlana Zabolotska", "title": "On Control Charts for Monitoring the Variance of a Time Series", "comments": "30 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive control charts for the variance of a Gaussian process\nusing the likelihood ratio approach, the generalized likelihood ratio approach,\nthe sequential probability ratio method and a generalized sequential\nprobability ratio procedure, the Shiryaev-Roberts procedure and a generalized\nShiryaev-Roberts ap- proach. Recursive presentations for the calculation of the\ncontrol statistics are given for autoregressive processes of order 1. In an\nextensive simulation study these schemes are compared with existing control\ncharts for the variance. In order to asses the performance of the schemes both\nthe average run length and the average delay are used.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 22:34:57 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Lazariv", "Taras", ""], ["Schmid", "Wolfgang", ""], ["Zabolotska", "Svitlana", ""]]}, {"id": "1209.4690", "submitter": "Wei-Yin Loh", "authors": "Wei-Yin Loh, Wei Zheng", "title": "Regression trees for longitudinal and multiresponse data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS596 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 495-522", "doi": "10.1214/12-AOAS596", "report-no": "IMS-AOAS-AOAS596", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous algorithms for constructing regression tree models for longitudinal\nand multiresponse data have mostly followed the CART approach. Consequently,\nthey inherit the same selection biases and computational difficulties as CART.\nWe propose an alternative, based on the GUIDE approach, that treats each\nlongitudinal data series as a curve and uses chi-squared tests of the residual\ncurve patterns to select a variable to split each node of the tree. Besides\nbeing unbiased, the method is applicable to data with fixed and random time\npoints and with missing values in the response or predictor variables.\nSimulation results comparing its mean squared prediction error with that of\nMVPART are given, as well as examples comparing it with standard linear mixed\neffects and generalized estimating equation models. Conditions for asymptotic\nconsistency of regression tree function estimates are also given.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 02:07:31 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 05:34:09 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Loh", "Wei-Yin", ""], ["Zheng", "Wei", ""]]}, {"id": "1209.5026", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Matthew A. Taddy, Shane T. Jensen", "title": "Estimating Player Contribution in Hockey with Regularized Logistic\n  Regression", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a regularized logistic regression model for evaluating player\ncontributions in hockey. The traditional metric for this purpose is the\nplus-minus statistic, which allocates a single unit of credit (for or against)\nto each player on the ice for a goal. However, plus-minus scores measure only\nthe marginal effect of players, do not account for sample size, and provide a\nvery noisy estimate of performance. We investigate a related regression\nproblem: what does each player on the ice contribute, beyond aggregate team\nperformance and other factors, to the odds that a given goal was scored by\ntheir team? Due to the large-p (number of players) and imbalanced design\nsetting of hockey analysis, a major part of our contribution is a careful\ntreatment of prior shrinkage in model estimation. We showcase two recently\ndeveloped techniques -- for posterior maximization or simulation -- that make\nsuch analysis feasible. Each approach is accompanied with publicly available\nsoftware and we include the simple commands used in our analysis. Our results\nshow that most players do not stand out as measurably strong (positive or\nnegative) contributors. This allows the stars to really shine, reveals diamonds\nin the rough overlooked by earlier analyses, and argues that some of the\nhighest paid players in the league are not making contributions worth their\nexpense.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 22:42:56 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2013 20:41:57 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Taddy", "Matthew A.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1209.5242", "submitter": "Michael GB Blum", "authors": "Nicolas Duforet-Frebourg, Michael G. B. Blum", "title": "Non-stationary patterns of isolation-by-distance: inferring measures of\n  local genetic differentiation with Bayesian kriging", "comments": "In press, Evolution 2014", "journal-ref": null, "doi": "10.1111/evo.12342", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns of isolation-by-distance arise when population differentiation\nincreases with increasing geographic distances. Patterns of\nisolation-by-distance are usually caused by local spatial dispersal, which\nexplains why differences of allele frequencies between populations accumulate\nwith distance. However, spatial variations of demographic parameters such as\nmigration rate or population density can generate non-stationary patterns of\nisolation-by-distance where the rate at which genetic differentiation\naccumulates varies across space. To characterize non-stationary patterns of\nisolation-by-distance, we infer local genetic differentiation based on Bayesian\nkriging. Local genetic differentiation for a sampled population is defined as\nthe average genetic differentiation between the sampled population and fictive\nneighboring populations. To avoid defining populations in advance, the method\ncan also be applied at the scale of individuals making it relevant for\nlandscape genetics. Inference of local genetic differentiation relies on a\nmatrix of pairwise similarity or dissimilarity between populations or\nindividuals such as matrices of FST between pairs of populations. Simulation\nstudies show that maps of local genetic differentiation can reveal barriers to\ngene flow but also other patterns such as continuous variations of gene flow\nacross habitat. The potential of the method is illustrated with 2 data sets:\ngenome-wide SNP data for human Swedish populations and AFLP markers for alpine\nplant species. The software LocalDiff implementing the method is available at\nhttp://membres-timc.imag.fr/Michael.Blum/LocalDiff.html\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 12:17:15 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2012 09:36:36 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2014 08:44:31 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Duforet-Frebourg", "Nicolas", ""], ["Blum", "Michael G. B.", ""]]}, {"id": "1209.5272", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Does the specification of uncertainty hurt the progress of\n  scientometrics?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"Caveats for using statistical significance tests in research\nassessments,\"--Journal of Informetrics 7(1)(2013) 50-62, available at\narXiv:1112.2516 -- Schneider (2013) focuses on Opthof & Leydesdorff (2010) as\nan example of the misuse of statistics in the social sciences. However, our\nconclusions are theoretical since they are not dependent on the use of one\nstatistics or another. We agree with Schneider insofar as he proposes to\ndevelop further statistical instruments (such as effect sizes). Schneider\n(2013), however, argues on meta-theoretical grounds against the specification\nof uncertainty because, in his opinion, the presence of statistics would\nlegitimate decision-making. We disagree: uncertainty can also be used for\nopening a debate. Scientometric results in which error bars are suppressed for\nmeta-theoretical reasons should not be trusted.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 13:59:13 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2012 08:19:31 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "1209.5350", "submitter": "Adel Javanmard", "authors": "Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade", "title": "Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints", "comments": "38 pages, 6 figures, 2 tables, applications in topic models and\n  Bayesian networks are studied. Simulation section is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:11:02 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2012 06:34:45 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 18:25:32 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Javanmard", "Adel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1209.5356", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer and Eike C. Brechmann and Daniel Silvestrini and\n  Claudia Czado", "title": "Total loss estimation using copula-based regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint copula-based model for insurance claims and sizes. It uses\nbivariate copulae to accommodate for the dependence between these quantities.\nWe derive the general distribution of the policy loss without the restrictive\nassumption of independence. We illustrate that this distribution tends to be\nskewed and multi-modal, and that an independence assumption can lead to\nsubstantial bias in the estimation of the policy loss. Further, we extend our\nframework to regression models by combining marginal generalized linear models\nwith a copula. We show that this approach leads to a flexible class of models,\nand that the parameters can be estimated efficiently using maximum-likelihood.\nWe propose a test procedure for the selection of the optimal copula family. The\nusefulness of our approach is illustrated in a simulation study and in an\nanalysis of car insurance policies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:28:04 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Kraemer", "Nicole", ""], ["Brechmann", "Eike C.", ""], ["Silvestrini", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1209.5776", "submitter": "Michael Chertkov", "authors": "Danhua Wang, Konstantin Turitsyn, and Michael Chertkov", "title": "DistFlow ODE: Modeling, Analyzing and Controlling Long Distribution\n  Feeder", "comments": "6 pages, 6 figures, 51st IEEE Conference on Decision and Control", "journal-ref": null, "doi": "10.1109/CDC.2012.6426054", "report-no": null, "categories": "math.OC physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear feeder connecting multiple distributed loads and\ngenerators to the sub-station. Voltage is controlled directly at the\nsub-station, however, voltage down the line shifts up or down, in particular\ndepending on if the feeder operates in the power export regime or power import\nregime. Starting from this finite element description of the feeder, assuming\nthat the consumption/generation is distributed heterogeneously along the\nfeeder, and following the asymptotic homogenization approach, we derive simple\nlow-parametric ODE model of the feeder. We also explain how the homogeneous ODE\nmodeling is generalized to account for other distributed effects, e.g. for\ninverter based and voltage dependent control of reactive power. The resulting\nsystem of the DistFlow-ODEs, relating homogenized voltage to flows of real and\nreactive power along the lines, admits computationally efficient analysis in\nterms of the minimal number of the feeder line \"media\" parameters, such as the\nratio of the inductance-to-resistance densities. Exploring the space of the\nmedia and control parameters allows us to test and juxtapose different measures\nof the system performance, in particular expressed in terms of the voltage drop\nalong the feeder, power import/export from the feeder line as the whole, power\nlosses within the feeder, and critical (with respect to possible voltage\ncollapse) length of the feeder. Our most surprising funding relates to\nperformance of a feeder rich on PhotoVoltaic (PV) systems during a sunny day.\nWe observe that if the feeder is sufficiently long the DistFlow-ODEs may have\nmultiple stable solutions. The multiplicity may mean troubles for successful\nrecovery of the feeder after a very short, few periods long, fault at the head\nof the line.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 21:28:50 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Danhua", ""], ["Turitsyn", "Konstantin", ""], ["Chertkov", "Michael", ""]]}, {"id": "1209.5977", "submitter": "Alexandra Schmidt", "authors": "Joaquim H. Vianna Neto, Alexandra Mello Schmidt and Peter Guttorp", "title": "Accounting for spatially varying directional effects in spatial\n  covariance structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind direction plays an important role in the spread of pollutant levels over\na geographical region. We discuss how to include wind directional information\nin the covariance function of spatial models. We follow the spatial convolution\napproach initially proposed by Higdon and co-authors, wherein a spatial process\nis described by a convolution between a smoothing kernel and a white noise\nprocess. We propose two different ways of accounting for wind direction in the\nkernel function. For comparison purposes, we also consider a more flexible\nkernel parametrization, that makes use of latent processes which vary smoothly\nacross the region. Inference procedure follows the Bayesian paradigm, and\nuncertainty about parameter estimation is naturally accounted for when\nperforming spatial interpolation. We analyze ozone levels observed at a\nmonitoring network in the Northeast of the USA. Sam- ples from the posterior\ndistribution under our proposed models are obtained much faster when compared\nto the kernel based on latent processes. Our models provide better results, in\nterms of model fitting and spatial interpolation, when compared to simple\nisotropic and geometrical anisotropic models. Despite the small number of\nparameters, our proposed models provide fits which are comparable to those\nobtained under the kernel based on latent processes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 15:45:42 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Neto", "Joaquim H. Vianna", ""], ["Schmidt", "Alexandra Mello", ""], ["Guttorp", "Peter", ""]]}, {"id": "1209.6004", "submitter": "Sean Gerrish", "authors": "Sean M. Gerrish and David M. Blei", "title": "The Issue-Adjusted Ideal Point Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model of issue-specific voting behavior. This model can be used\nto explore lawmakers' personal voting patterns of voting by issue area,\nproviding an exploratory window into how the language of the law is correlated\nwith political support. We derive approximate posterior inference algorithms\nbased on variational methods. Across 12 years of legislative data, we\ndemonstrate both improvement in heldout prediction performance and the model's\nutility in interpreting an inherently multi-dimensional space.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 17:00:21 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Gerrish", "Sean M.", ""], ["Blei", "David M.", ""]]}, {"id": "1209.6141", "submitter": "Peter G. M. van der Heijden", "authors": "Peter G. M. van der Heijden, Joe Whittaker, Maarten Cruyff, Bart\n  Bakker, Rik van der Vliet", "title": "People born in the Middle East but residing in the Netherlands:\n  Invariant population size estimates and the role of active and passive\n  covariates", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS536 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 831-852", "doi": "10.1214/12-AOAS536", "report-no": "IMS-AOAS-AOAS536", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Including covariates in loglinear models of population registers improves\npopulation size estimates for two reasons. First, it is possible to take\nheterogeneity of inclusion probabilities over the levels of a covariate into\naccount; and second, it allows subdivision of the estimated population by the\nlevels of the covariates, giving insight into characteristics of individuals\nthat are not included in any of the registers. The issue of whether or not\nmarginalizing the full table of registers by covariates over one or more\ncovariates leaves the estimated population size estimate invariant is\nintimately related to collapsibility of contingency tables [Biometrika 70\n(1983) 567-578]. We show that, with information from two registers, population\nsize invariance is equivalent to the simultaneous collapsibility of each margin\nconsisting of one register and the covariates. We give a short path\ncharacterization of the loglinear model which describes when marginalizing over\na covariate leads to different population size estimates. Covariates that are\ncollapsible are called passive, to distinguish them from covariates that are\nnot collapsible and are termed active. We make the case that it can be useful\nto include passive covariates within the estimation model, because they allow a\nfiner description of the population in terms of these covariates. As an example\nwe discuss the estimation of the population size of people born in the Middle\nEast but residing in the Netherlands.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 07:18:41 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["van der Heijden", "Peter G. M.", ""], ["Whittaker", "Joe", ""], ["Cruyff", "Maarten", ""], ["Bakker", "Bart", ""], ["van der Vliet", "Rik", ""]]}, {"id": "1209.6167", "submitter": "Kanti V. Mardia", "authors": "Kanti V. Mardia, Emma M. Petty, Charles C. Taylor", "title": "Matching markers and unlabeled configurations in protein gels", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS544 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 853-869", "doi": "10.1214/12-AOAS544", "report-no": "IMS-AOAS-AOAS544", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlabeled shape analysis is a rapidly emerging and challenging area of\nstatistics. This has been driven by various novel applications in\nbioinformatics. We consider here the situation where two configurations are\nmatched under various constraints, namely, the configurations have a subset of\nmanually located \"markers\" with high probability of matching each other while a\nlarger subset consists of unlabeled points. We consider a plausible model and\ngive an implementation using the EM algorithm. The work is motivated by a real\nexperiment of gels for renal cancer and our approach allows for the possibility\nof missing and misallocated markers. The methodology is successfully used to\nautomatically locate and remove a grossly misallocated marker within the given\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 09:04:18 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Mardia", "Kanti V.", ""], ["Petty", "Emma M.", ""], ["Taylor", "Charles C.", ""]]}, {"id": "1209.6172", "submitter": "Spencer Hays", "authors": "Spencer Hays, Haipeng Shen, Jianhua Z. Huang", "title": "Functional dynamic factor models with application to yield curve\n  forecasting", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS551 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 870-894", "doi": "10.1214/12-AOAS551", "report-no": "IMS-AOAS-AOAS551", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasting of zero coupon bond yields for a continuum of maturities\nis paramount to bond portfolio management and derivative security pricing. Yet\na universal model for yield curve forecasting has been elusive, and prior\nattempts often resulted in a trade-off between goodness of fit and consistency\nwith economic theory. To address this, herein we propose a novel formulation\nwhich connects the dynamic factor model (DFM) framework with concepts from\nfunctional data analysis: a DFM with functional factor loading curves. This\nresults in a model capable of forecasting functional time series. Further, in\nthe yield curve context we show that the model retains economic interpretation.\nModel estimation is achieved through an expectation-maximization algorithm,\nwhere the time series parameters and factor loading curves are simultaneously\nestimated in a single step. Efficient computing is implemented and a\ndata-driven smoothing parameter is nicely incorporated. We show that our model\nperforms very well on forecasting actual yield data compared with existing\napproaches, especially in regard to profit-based assessment for an innovative\ntrading exercise. We further illustrate the viability of our model to\napplications outside of yield forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 09:34:08 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Hays", "Spencer", ""], ["Shen", "Haipeng", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "1209.6210", "submitter": "S. C. Kou", "authors": "Chao Du, S. C. Kou", "title": "Correlation analysis of enzymatic reaction of a single protein molecule", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS541 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 950-976", "doi": "10.1214/12-AOAS541", "report-no": "IMS-AOAS-AOAS541", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New advances in nano sciences open the door for scientists to study\nbiological processes on a microscopic molecule-by-molecule basis. Recent\nsingle-molecule biophysical experiments on enzyme systems, in particular,\nreveal that enzyme molecules behave fundamentally differently from what\nclassical model predicts. A stochastic network model was previously proposed to\nexplain the experimental discovery. This paper conducts detailed theoretical\nand data analyses of the stochastic network model, focusing on the correlation\nstructure of the successive reaction times of a single enzyme molecule. We\ninvestigate the correlation of experimental fluorescence intensity and the\ncorrelation of enzymatic reaction times, and examine the role of substrate\nconcentration in enzymatic reactions. Our study shows that the stochastic\nnetwork model is capable of explaining the experimental data in depth.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 12:47:11 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Du", "Chao", ""], ["Kou", "S. C.", ""]]}, {"id": "1209.6221", "submitter": "Antoine Chambaz", "authors": "Antoine Chambaz, Christophe Denis", "title": "Classification in postural style", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS542 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 977-993", "doi": "10.1214/12-AOAS542", "report-no": "IMS-AOAS-AOAS542", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article contributes to the search for a notion of postural style,\nfocusing on the issue of classifying subjects in terms of how they maintain\nposture. Longer term, the hope is to make it possible to determine on a case by\ncase basis which sensorial information is prevalent in postural control, and to\nimprove/adapt protocols for functional rehabilitation among those who show\ndeficits in maintaining posture, typically seniors. Here, we specifically\ntackle the statistical problem of classifying subjects sampled from a two-class\npopulation. Each subject (enrolled in a cohort of 54 participants) undergoes\nfour experimental protocols which are designed to evaluate potential deficits\nin maintaining posture. These protocols result in four complex trajectories,\nfrom which we can extract four small-dimensional summary measures. Because\nundergoing several protocols can be unpleasant, and sometimes painful, we try\nto limit the number of protocols needed for the classification. Therefore, we\nfirst rank the protocols by decreasing order of relevance, then we derive four\nplug-in classifiers which involve the best (i.e., more informative), the two\nbest, the three best and all four protocols. This two-step procedure relies on\nthe cutting-edge methodologies of targeted maximum likelihood learning (a\nmethodology for robust and efficient inference) and super-learning (a machine\nlearning procedure for aggregating various estimation procedures into a single\nbetter estimation procedure). A simulation study is carried out. The\nperformances of the procedure applied to the real data set (and evaluated by\nthe leave-one-out rule) go as high as an 87% rate of correct classification (47\nout of 54 subjects correctly classified), using only the best protocol.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 13:26:19 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Chambaz", "Antoine", ""], ["Denis", "Christophe", ""]]}, {"id": "1209.6241", "submitter": "Krista Gile", "authors": "Mark S. Handcock and Krista J. Gile and Corinne M. Mar", "title": "Estimating Hidden Population Size using Respondent-Driven Sampling Data", "comments": "36 pages, 7 figures, including color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is an approach to sampling design and\ninference in hard-to-reach human populations. Typically, a sampling frame is\nnot available, and population members are difficult to identify or recruit from\nbroader sampling frames. Common examples include injecting drug users, men who\nhave sex with men, and female sex workers. Most analysis of RDS data has\nfocused on estimating aggregate characteristics, such as disease prevalence.\nHowever, RDS is often conducted in settings where the population size is\nunknown and of great independent interest. This paper presents an approach to\nestimating the size of a target population based on data collected through RDS.\n  The proposed approach uses a successive sampling approximation to RDS to\nleverage information in the ordered sequence of observed personal network\nsizes. The inference uses the Bayesian framework, allowing for the\nincorporation of prior knowledge. A flexible class of priors for the population\nsize is proposed that aids elicitation. An extensive simulation study provides\ninsight into the performance of the method for estimating population size under\na broad range of conditions. A further study shows the approach also improves\nestimation of aggregate characteristics. A particular choice of the prior\nproduces interval estimates with good frequentist properties. Finally, the\nmethod demonstrates sensible results when used to estimate the numbers of\nsub-populations most at risk for HIV in two cities in El Salvador.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 14:17:41 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Handcock", "Mark S.", ""], ["Gile", "Krista J.", ""], ["Mar", "Corinne M.", ""]]}, {"id": "1209.6254", "submitter": "Krista Gile", "authors": "Krista J. Gile and Lisa G. Johnston and Matthew J. Salganik", "title": "Diagnostics for Respondent-driven Sampling", "comments": "41 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a widely used method for sampling from\nhard-to-reach human populations, especially groups most at-risk for HIV/AIDS.\nData are collected through a peer-referral process in which current sample\nmembers harness existing social networks to recruit additional sample members.\nRDS has proven to be a practical method of data collection in many difficult\nsettings and has been adopted by leading public health organizations around the\nworld. Unfortunately, inference from RDS data requires many strong assumptions\nbecause the sampling design is not fully known and is partially beyond the\ncontrol of the researcher. In this paper, we introduce diagnostic tools for\nmost of the assumptions underlying RDS inference. We also apply these\ndiagnostics in a case study of 12 populations at increased risk for HIV/AIDS.\nWe developed these diagnostics to enable RDS researchers to better understand\ntheir data and to encourage future statistical research on RDS.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 15:00:30 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Gile", "Krista J.", ""], ["Johnston", "Lisa G.", ""], ["Salganik", "Matthew J.", ""]]}, {"id": "1209.6443", "submitter": "Tian Siva Tian", "authors": "Tian Siva Tian, Jianhua Z. Huang, Haipeng Shen, Zhimin Li", "title": "A two-way regularization method for MEG source reconstruction", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS531 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1021-1046", "doi": "10.1214/11-AOAS531", "report-no": "IMS-AOAS-AOAS531", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MEG inverse problem refers to the reconstruction of the neural activity\nof the brain from magnetoencephalography (MEG) measurements. We propose a\ntwo-way regularization (TWR) method to solve the MEG inverse problem under the\nassumptions that only a small number of locations in space are responsible for\nthe measured signals (focality), and each source time course is smooth in time\n(smoothness). The focality and smoothness of the reconstructed signals are\nensured respectively by imposing a sparsity-inducing penalty and a roughness\npenalty in the data fitting criterion. A two-stage algorithm is developed for\nfast computation, where a raw estimate of the source time course is obtained in\nthe first stage and then refined in the second stage by the two-way\nregularization. The proposed method is shown to be effective on both synthetic\nand real-world examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 07:54:34 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Tian", "Tian Siva", ""], ["Huang", "Jianhua Z.", ""], ["Shen", "Haipeng", ""], ["Li", "Zhimin", ""]]}, {"id": "1209.6453", "submitter": "Omkar Muralidharan", "authors": "Omkar Muralidharan, Georges Natsoulis, John Bell, Hanlee Ji, Nancy R.\n  Zhang", "title": "Detecting mutations in mixed sample sequencing data using empirical\n  Bayes", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS538 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1047-1067", "doi": "10.1214/12-AOAS538", "report-no": "IMS-AOAS-AOAS538", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop statistically based methods to detect single nucleotide DNA\nmutations in next generation sequencing data. Sequencing generates counts of\nthe number of times each base was observed at hundreds of thousands to billions\nof genome positions in each sample. Using these counts to detect mutations is\nchallenging because mutations may have very low prevalence and sequencing error\nrates vary dramatically by genome position. The discreteness of sequencing data\nalso creates a difficult multiple testing problem: current false discovery rate\nmethods are designed for continuous data, and work poorly, if at all, on\ndiscrete data. We show that a simple randomization technique lets us use\ncontinuous false discovery rate methods on discrete data. Our approach is a\nuseful way to estimate false discovery rates for any collection of discrete\ntest statistics, and is hence not limited to sequencing data. We then use an\nempirical Bayes model to capture different sources of variation in sequencing\nerror rates. The resulting method outperforms existing detection approaches on\nexample data sets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 09:02:09 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Muralidharan", "Omkar", ""], ["Natsoulis", "Georges", ""], ["Bell", "John", ""], ["Ji", "Hanlee", ""], ["Zhang", "Nancy R.", ""]]}, {"id": "1209.6457", "submitter": "Andrew Parnell", "authors": "Andrew C. Parnell, Donald L. Phillips, Stuart Bearhop, Brice X.\n  Semmens, Eric J. Ward, Jonathan W. Moore, Andrew L. Jackson, Richard Inger", "title": "Bayesian Stable Isotope Mixing Models", "comments": "16 pages, 9 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review recent advances in Stable Isotope Mixing Models\n(SIMMs) and place them into an over-arching Bayesian statistical framework\nwhich allows for several useful extensions. SIMMs are used to quantify the\nproportional contributions of various sources to a mixture. The most widely\nused application is quantifying the diet of organisms based on the food sources\nthey have been observed to consume. At the centre of the multivariate\nstatistical model we propose is a compositional mixture of the food sources\ncorrected for various metabolic factors. The compositional component of our\nmodel is based on the isometric log ratio (ilr) transform of Egozcue (2003).\nThrough this transform we can apply a range of time series and non-parametric\nsmoothing relationships. We illustrate our models with 3 case studies based on\nreal animal dietary behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 09:23:35 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Parnell", "Andrew C.", ""], ["Phillips", "Donald L.", ""], ["Bearhop", "Stuart", ""], ["Semmens", "Brice X.", ""], ["Ward", "Eric J.", ""], ["Moore", "Jonathan W.", ""], ["Jackson", "Andrew L.", ""], ["Inger", "Richard", ""]]}, {"id": "1209.6463", "submitter": "Antonio Punzo", "authors": "Sanjeena Subedi and Antonio Punzo and Salvatore Ingrassia and Paul D.\n  McNicholas", "title": "Clustering and Classification via Cluster-Weighted Factor Analyzers", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11634-013-0124-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based clustering and classification, the cluster-weighted model\nconstitutes a convenient approach when the random vector of interest\nconstitutes a response variable Y and a set p of explanatory variables X.\nHowever, its applicability may be limited when p is high. To overcome this\nproblem, this paper assumes a latent factor structure for X in each mixture\ncomponent. This leads to the cluster-weighted factor analyzers (CWFA) model. By\nimposing constraints on the variance of Y and the covariance matrix of X, a\nnovel family of sixteen CWFA models is introduced for model-based clustering\nand classification. The alternating expectation-conditional maximization\nalgorithm, for maximum likelihood estimation of the parameters of all the\nmodels in the family, is described; to initialize the algorithm, a 5-step\nhierarchical procedure is proposed, which uses the nested structures of the\nmodels within the family and thus guarantees the natural ranking among the\nsixteen likelihoods. Artificial and real data show that these models have very\ngood clustering and classification performance and that the algorithm is able\nto recover the parameters very well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 09:50:35 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Subedi", "Sanjeena", ""], ["Punzo", "Antonio", ""], ["Ingrassia", "Salvatore", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1209.6493", "submitter": "Steven P. Lund", "authors": "Steven P. Lund, Dan Nettleton", "title": "The importance of distinct modeling strategies for gene and\n  gene-specific treatment effects in hierarchical models for microarray data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS535 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1118-1133", "doi": "10.1214/12-AOAS535", "report-no": "IMS-AOAS-AOAS535", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing microarray data, hierarchical models are often used to share\ninformation across genes when estimating means and variances or identifying\ndifferential expression. Many methods utilize some form of the two-level\nhierarchical model structure suggested by Kendziorski et al. [Stat. Med. (2003)\n22 3899-3914] in which the first level describes the distribution of latent\nmean expression levels among genes and among differentially expressed\ntreatments within a gene. The second level describes the conditional\ndistribution, given a latent mean, of repeated observations for a single gene\nand treatment. Many of these models, including those used in Kendziorski's et\nal. [Stat. Med. (2003) 22 3899-3914] EBarrays package, assume that expression\nlevel changes due to treatment effects have the same distribution as expression\nlevel changes from gene to gene. We present empirical evidence that this\nassumption is often inadequate and propose three-level hierarchical models as\nextensions to the two-level log-normal based EBarrays models to address this\ninadequacy. We demonstrate that use of our three-level models dramatically\nchanges analysis results for a variety of microarray data sets and verify the\nvalidity and improved performance of our suggested method in a series of\nsimulation studies. We also illustrate the importance of accounting for the\nuncertainty of gene-specific error variance estimates when using hierarchical\nmodels to identify differentially expressed genes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:51:55 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Lund", "Steven P.", ""], ["Nettleton", "Dan", ""]]}, {"id": "1209.6502", "submitter": "Shaoyu Li", "authors": "Shaoyu Li, Yuehua Cui", "title": "Gene-centric gene-gene interaction: A model-based kernel machine method", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS545 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1134-1161", "doi": "10.1214/12-AOAS545", "report-no": "IMS-AOAS-AOAS545", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the natural variation for a complex trait can be explained by\nvariation in DNA sequence levels. As part of sequence variation, gene-gene\ninteraction has been ubiquitously observed in nature, where its role in shaping\nthe development of an organism has been broadly recognized. The identification\nof interactions between genetic factors has been progressively pursued via\nstatistical or machine learning approaches. A large body of currently adopted\nmethods, either parametrically or nonparametrically, predominantly focus on\npairwise single marker interaction analysis. As genes are the functional units\nin living organisms, analysis by focusing on a gene as a system could\npotentially yield more biologically meaningful results. In this work, we\nconceptually propose a gene-centric framework for genome-wide gene-gene\ninteraction detection. We treat each gene as a testing unit and derive a\nmodel-based kernel machine method for two-dimensional genome-wide scanning of\ngene-gene interactions. In addition to the biological advantage, our method is\nstatistically appealing because it reduces the number of hypotheses tested in a\ngenome-wide scan. Extensive simulation studies are conducted to evaluate the\nperformance of the method. The utility of the method is further demonstrated\nwith applications to two real data sets. Our method provides a conceptual\nframework for the identification of gene-gene interactions which could shed\nnovel light on the etiology of complex diseases.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 12:40:32 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Li", "Shaoyu", ""], ["Cui", "Yuehua", ""]]}]