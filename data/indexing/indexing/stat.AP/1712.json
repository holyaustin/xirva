[{"id": "1712.00063", "submitter": "Alexis Hannart", "authors": "Alexis Hannart and Philippe Naveau", "title": "Probabilities of causation of climate changes", "comments": null, "journal-ref": null, "doi": "10.1175/JCLI-D-17-0304.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple changes in Earth's climate system have been observed over the past\ndecades. Determining how likely each of these changes are to have been caused\nby human influence, is important for decision making on mitigation and\nadaptation policy. Here we describe an approach for deriving the probability\nthat anthropogenic forcings have caused a given observed change. The proposed\napproach is anchored into causal counterfactual theory (Pearl 2009) which has\nbeen introduced recently, and was in fact partly used already, in the context\nof extreme weather event attribution (EA). We argue that these concepts are\nalso relevant, and can be straightforwardly extended to, the context of\ndetection and attribution of long term trends associated to climate change\n(D&A). For this purpose, and in agreement with the principle of\n\"fingerprinting\" applied in the conventional D&A framework, a trajectory of\nchange is converted into an event occurrence defined by maximizing the causal\nevidence associated to the forcing under scrutiny. Other key assumptions used\nin the conventional D&A framework, in particular those related to numerical\nmodels error, can also be adapted conveniently to this approach. Our proposal\nthus allows to bridge the conventional framework with the standard causal\ntheory, in an attempt to improve the quantification of causal probabilities. An\nillustration suggests that our approach is prone to yield a significantly\nhigher estimate of the probability that anthropogenic forcings have caused the\nobserved temperature change, thus supporting more assertive causal claims.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 20:24:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Hannart", "Alexis", ""], ["Naveau", "Philippe", ""]]}, {"id": "1712.00067", "submitter": "Kris Sankaran", "authors": "Kris Sankaran, Susan P. Holmes", "title": "Inference of Dynamic Regimes in the Microbiome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been performed to characterize the dynamics and stability\nof the microbiome across a range of environmental contexts [Costello et al.,\n2012, Faust et al., 2015]. For example, it is often of interest to identify\ntime intervals within which certain subsets of taxa have an interesting pattern\nof behavior. Viewed abstractly, these problems often have a flavor not just of\ntime series modeling but also of regime detection, a problem with a rich\nhistory across a variety of applications, including speech recognition [Fox et\nal., 2011], finance [Lee, 2009], EEG analysis [Camilleri et al., 2014], and\ngeophysics [Weatherley and Mora, 2002]. In spite of the parallels, regime\ndetection methods are rarely used in microbiome analysis, most likely due to\nthe fact that references for these methods are scattered across several\nliteratures, descriptions are inaccessible outside limited research\ncommunities, and implementations are difficult to come across.\n  We distill the core ideas of different regime detection methods, provide\nexample applications, and share reproducible code, making these techniques more\naccessible to microbiome researchers. We re-analyze data of Dethlefsen and\nRelman [2011], a study of the effects of antibiotics on the microbiome, using\nClassification and Regression Trees (CART) [Breiman et al., 1984], Hidden\nMarkov Models (HMMs) [Rabiner and Juang, 1986], Bayesian nonparametric HMMs\n[Teh and Jordan, 2010, Fox et al., 2008], mixtures of Gaussian Processes (GPs)\n[Rasmussen and Ghahramani, 2002], switching dynamical systems [Linderman et\nal., 2016], and multiple changepoint detection [Fan and Mackey, 2015]. Along\nthe way, we summarize each method, their relevance to the microbiome, and\ntradeoffs associated with using them. Ultimately, our goal is to describe types\nof temporal or regime switching structure that can be incorporated into studies\nof microbiome dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 20:31:36 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Sankaran", "Kris", ""], ["Holmes", "Susan P.", ""]]}, {"id": "1712.00103", "submitter": "Sangeetika Ruchi", "authors": "Sangeetika Ruchi and Svetlana Dubinkina", "title": "Application of ensemble transform data assimilation methods for\n  parameter estimation in reservoir modelling", "comments": null, "journal-ref": null, "doi": "10.5194/npg-25-731-2018", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years data assimilation methods have been developed to obtain\nestimations of uncertain model parameters by taking into account a few\nobservations of a model state. The most reliable methods of MCMC are\ncomputationally expensive. Sequential ensemble methods such as ensemble Kalman\nfilers and particle filters provide a favourable alternative. However, Ensemble\nKalman Filter has an assumption of Gaussianity. Ensemble Transform Particle\nFilter does not have this assumption and has proven to be highly beneficial for\nan initial condition estimation and a small number of parameter estimation in\nchaotic dynamical systems with non-Gaussian distributions. In this paper we\nemploy Ensemble Transform Particle Filter (ETPF) and Ensemble Transform Kalman\nFilter (ETKF) for parameter estimation in nonlinear problems with 1, 5, and\n2500 uncertain parameters and compare them to importance sampling (IS). We\nprove that the updated parameters obtained by ETPF lie within the range of an\ninitial ensemble, which is not the case for ETKF. We examine the performance of\nETPF and ETKF in a twin experiment setup and observe that for a small number of\nuncertain parameters (1 and 5) ETPF performs comparably to ETKF in terms of the\nmean estimation. For a large number of uncertain parameters (2500) ETKF is\nrobust with respect to the initial ensemble while ETPF is sensitive due to\nsampling error. Moreover, for the high-dimensional test problem ETPF gives an\nincrease in the root mean square error after data assimilation is performed.\nThis is resolved by applying distance-based localization, which however\ndeteriorates a posterior estimation of the leading mode by largely increasing\nthe variance. A possible remedy is instead of applying localization to use only\nleading modes that are well estimated by ETPF, which demands a knowledge at\nwhich mode to truncate.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 22:19:16 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 15:28:50 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Ruchi", "Sangeetika", ""], ["Dubinkina", "Svetlana", ""]]}, {"id": "1712.00117", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga, David J. Albers, Marija Vlajic Wheeler, Anna Druet,\n  Hans Raffauf and No\\'emie Elhadad", "title": "Towards Personalized Modeling of the Female Hormonal Cycle: Experiments\n  with Mechanistic Models and Gaussian Processes", "comments": "Accepted at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel task for machine learning in healthcare,\nnamely personalized modeling of the female hormonal cycle. The motivation for\nthis work is to model the hormonal cycle and predict its phases in time, both\nfor healthy individuals and for those with disorders of the reproductive\nsystem. Because there are individual differences in the menstrual cycle, we are\nparticularly interested in personalized models that can account for individual\nidiosyncracies, towards identifying phenotypes of menstrual cycles. As a first\nstep, we consider the hormonal cycle as a set of observations through time. We\nuse a previously validated mechanistic model to generate realistic hormonal\npatterns, and experiment with Gaussian process regression to estimate their\nvalues over time. Specifically, we are interested in the feasibility of\npredicting menstrual cycle phases under varying learning conditions: number of\ncycles used for training, hormonal measurement noise and sampling rates, and\ninformed vs. agnostic sampling of hormonal measurements. Our results indicate\nthat Gaussian processes can help model the female menstrual cycle. We discuss\nthe implications of our experiments in the context of modeling the female\nmenstrual cycle.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:24:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Albers", "David J.", ""], ["Wheeler", "Marija Vlajic", ""], ["Druet", "Anna", ""], ["Raffauf", "Hans", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "1712.00131", "submitter": "Marcel Ausloos", "authors": "Jing Shi, Marcel Ausloos, Tingting Zhu", "title": "Benford's law first significant digit and distribution distances for\n  testing the reliability of financial reports in developing countries", "comments": "22 pages, 34 references, 4 figures, 7 tables; to be published in\n  Physica A", "journal-ref": null, "doi": "10.1016/j.physa.2017.11.017", "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a common suspicion about reported financial data, in 10 industrial\nsectors of the 6 so called \"main developing countries\" over the time interval\n[2000-2014]. These data are examined through Benford's law first significant\ndigit and through distribution distances tests. It is shown that several\nvisually anomalous data have to be a priori removed. Thereafter, the\ndistributions much better follow the first digit significant law, indicating\nthe usefulness of a Benford's law test from the research starting line. The\nsame holds true for distance tests. A few outliers are pointed out.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 23:59:17 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Shi", "Jing", ""], ["Ausloos", "Marcel", ""], ["Zhu", "Tingting", ""]]}, {"id": "1712.00182", "submitter": "Furong Sun", "authors": "Furong Sun, Robert B. Gramacy, Benjamin Haaland, Earl Lawrence, and\n  Andrew Walker", "title": "Emulating satellite drag from large simulation experiments", "comments": "44 pages; 16 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining accurate estimates of satellite drag coefficients in low Earth\norbit is a crucial component in positioning and collision avoidance. Simulators\ncan produce accurate estimates, but their computational expense is much too\nlarge for real-time application. A pilot study showed that Gaussian process\n(GP) surrogate models could accurately emulate simulations. However, cubic\nruntime for training GPs means that they could only be applied to a narrow\nrange of input configurations to achieve the desired level of accuracy. In this\npaper we show how extensions to the local approximate Gaussian Process (laGP)\nmethod allow accurate full-scale emulation. The new methodological\ncontributions, which involve a multi-level global/local modeling approach, and\na set-wise approach to local subset selection, are shown to perform well in\nbenchmark and synthetic data settings. We conclude by demonstrating that our\nmethod achieves the desired level of accuracy, besting simpler viable (i.e.,\ncomputationally tractable) global and local modeling approaches, when trained\non seventy thousand core hours of drag simulations for two real-world\nsatellites: the Hubble space telescope (HST) and the gravity recovery and\nclimate experiment (GRACE).\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:07:26 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 14:57:52 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 01:45:04 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 23:31:28 GMT"}, {"version": "v5", "created": "Sat, 22 Jun 2019 19:36:51 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sun", "Furong", ""], ["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""], ["Lawrence", "Earl", ""], ["Walker", "Andrew", ""]]}, {"id": "1712.00203", "submitter": "Mehrdad Gharib Shirangi", "authors": "Mehrdad G Shirangi", "title": "Closed-loop field development with multipoint geostatistics and\n  statistical performance assessment", "comments": "accepted for publication in Journal of Computational Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Closed-loop field development (CLFD) optimization is a comprehensive\nframework for optimal development of subsurface resources. CLFD involves three\nmajor steps: 1) optimization of full development plan based on current set of\nmodels, 2) drilling new wells and collecting new spatial and temporal\n(production) data, 3) model calibration based on all data. This process is\nrepeated until the optimal number of wells is drilled. This work introduces an\nefficient CLFD implementation for complex systems described by multipoint\ngeostatistics (MPS). Model calibration is accomplished in two steps:\nconditioning to spatial data by a geostatistical simulation method, and\nconditioning to production data by optimization-based PCA. A statistical\nprocedure is presented to assess the performance of CLFD. Methodology is\napplied to an oil reservoir example for 25 different true-model cases.\nApplication of a single-step of CLFD, improved the true NPV in 64%--80% of\ncases. The full CLFD procedure (with three steps) improved the true NPV in 96%\nof cases, with an average improvement of 37%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 06:08:59 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 04:42:56 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Shirangi", "Mehrdad G", ""]]}, {"id": "1712.00528", "submitter": "Ru Zhang", "authors": "Ru Zhang, Yanjun Liu and James T. Townsend", "title": "A Theoretical Study of Process Dependence for Standard Two-Process\n  Serial Models and Standard Two-Process Parallel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we differentiate and characterize the standard two-process\nserial models and the standard two process parallel models by investigating the\nbehavior of (conditional) distributions of the total completion times and\nsurvivals of intercompletion times without assuming any particular forms for\nthe distributions of processing times. We address our argument through\nmathematical proofs and computational methods. It is found that for the\nstandard two-process serial models, positive dependence between the total\ncompletion times does not hold if no specific distributional forms are imposed\nto the processing times. By contrast, for the standard two-process parallel\nmodels the total completion times are independent. According to different\nnature of process dependence, one can distinguish a standard two process serial\nmodel from a standard two-process parallel model. We also find that in standard\ntwo-process parallel models the monotonicity of survival function of the\nintercompletion time of stage 2 conditional on the completion of stage 1\ndepends on the monotonicity of the hazard function of processing time. We also\nfind that the survival of intercompletion time(s) from stage 1 to stage 2 is\nincreasing when the ratio of hazard function meets certain criterion. Then the\nempirical finding that the intercompletion time is grown with the growth of the\nnumber of recalled words can be accounted by standard parallel models. We also\nfind that if the cumulative hazard function is concave or linear, the survival\nfrom stage 1 to stage 2 is increasing.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 00:43:15 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhang", "Ru", ""], ["Liu", "Yanjun", ""], ["Townsend", "James T.", ""]]}, {"id": "1712.00546", "submitter": "Arindam Fadikar", "authors": "Arindam Fadikar, Dave Higdon, Jiangzhuo Chen, Brian Lewis, Srini\n  Venkatramanan, and Madhav Marathe", "title": "Calibrating a Stochastic Agent Based Model Using Quantile-based\n  Emulation", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": "10.1137/17M1161233", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a number of cases, the Quantile Gaussian Process (QGP) has proven\neffective in emulating stochastic, univariate computer model output (Plumlee\nand Tuo, 2014). In this paper, we develop an approach that uses this emulation\napproach within a Bayesian model calibration framework to calibrate an\nagent-based model of an epidemic. In addition, this approach is extended to\nhandle the multivariate nature of the model output, which gives a time series\nof the count of infected individuals. The basic modeling approach is adapted\nfrom Higdon et al. (2008), using a basis representation to capture the\nmultivariate model output. The approach is motivated with an example taken from\nthe 2015 Ebola Challenge workshop which simulated an ebola epidemic to evaluate\nmethodology.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 04:20:54 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fadikar", "Arindam", ""], ["Higdon", "Dave", ""], ["Chen", "Jiangzhuo", ""], ["Lewis", "Brian", ""], ["Venkatramanan", "Srini", ""], ["Marathe", "Madhav", ""]]}, {"id": "1712.00563", "submitter": "Gabriel Erion", "authors": "Gabriel Erion, Hugh Chen, Scott M. Lundberg, Su-In Lee", "title": "Anesthesiologist-level forecasting of hypoxemia with only SpO2 data\n  using deep learning", "comments": "To be presented at Machine Learning for Health Workshop: 31st\n  Conference on Neural Information Processing Systems (NIPS 2017), Long Beach,\n  CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a deep learning model trained only on a patient's blood oxygenation\ndata (measurable with an inexpensive fingertip sensor) to predict impending\nhypoxemia (low blood oxygen) more accurately than trained anesthesiologists\nwith access to all the data recorded in a modern operating room. We also\nprovide a simple way to visualize the reason why a patient's risk is low or\nhigh by assigning weight to the patient's past blood oxygen values. This work\nhas the potential to provide cutting-edge clinical decision support in\nlow-resource settings, where rates of surgical complication and death are\nsubstantially greater than in high-resource areas.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 07:27:28 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Erion", "Gabriel", ""], ["Chen", "Hugh", ""], ["Lundberg", "Scott M.", ""], ["Lee", "Su-In", ""]]}, {"id": "1712.00602", "submitter": "Marcel Ausloos", "authors": "ManYing Kang and Marcel Ausloos", "title": "An Inverse Problem Study: Credit Risk Ratings as a Determinant of\n  Corporate Governance and Capital Structure in Emerging Markets: Evidence from\n  Chinese Listed Companies", "comments": "25 pages, 6 tables", "journal-ref": "Economies 2017, 5, 41", "doi": "10.3390/economies5040041", "report-no": null, "categories": "q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit risk rating is shown to be a relevant determinant in order to estimate\ngood corporate governance and to self-optimize capital structure. The\nconclusion is argued from a study on a selected (and justified) sample of (182)\ncompanies listed on the Shanghai Stock Exchange and the Shenzhen Stock Exchange\nand which use the same Shanghai Brilliance Credit Rating & Investors Service\nCompany assessment criteria, for their credit ratings, from 2010 to 2015.\nPractically, 3 debt ratios are examined in terms of 11 characteristic\nvariables. Moreover, any relationship between credit rating and corporate\ngovernance can be thought to be an interesting finding. The relationship\nbetween credit rating and leverage is not as evident as that found by other\nresearchers from different countries; it is significantly positively related to\nthe outside director, firm size, tangible assets and firm age, and CEO and\nchairman office plurality. However, leverage is found to be negatively\ncorrelated with board size, profitability, growth opportunity, and non-debt tax\nshield. Credit rating is positively associated with leverage, but in a less\nsignificant way. CEO-Board chairship duality is insignificantly related to\nleverage. The non-debt tax shield is significantly correlated with leverage.\nThe correlation coefficient between CEO duality and auditor is positive but\nweakly significant, but seems not consistent with expectations. Finally,\nprofitability cause could be regarded as an interesting finding. Indeed, there\nis an inverse correlation between profitability and total debt (Notice that the\nresult supports the pecking order theory). In conclusion, it appears that\ncredit rating has less effect on the so listed large Chinese companies than in\nother countries. Nevertheless, the perspective of assessing credit risk rating\nby relevant agencies is indubitably a recommended time dependent leverage\ndeterminant.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 12:55:25 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Kang", "ManYing", ""], ["Ausloos", "Marcel", ""]]}, {"id": "1712.00642", "submitter": "Xiao Wu", "authors": "Xiao Wu, Danielle Braun, Marianthi-Anna Kioumourtzoglou, Christine\n  Choirat, Qian Di and Francesca Dominici", "title": "Causal inference in the context of an error prone exposure: air\n  pollution and mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for estimating causal effects when the exposure is\nmeasured with error and confounding adjustment is performed via a generalized\npropensity score (GPS). Using validation data, we propose a regression\ncalibration (RC)-based adjustment for a continuous error-prone exposure\ncombined with GPS to adjust for confounding (RC-GPS). The outcome analysis is\nconducted after transforming the corrected continuous exposure into a\ncategorical exposure. We consider confounding adjustment in the context of GPS\nsubclassification, inverse probability treatment weighting (IPTW) and matching.\nIn simulations with varying degrees of exposure error and confounding bias,\nRC-GPS eliminates bias from exposure error and confounding compared to standard\napproaches that rely on the error-prone exposure. We applied RC-GPS to a rich\ndata platform to estimate the causal effect of long-term exposure to fine\nparticles ($PM_{2.5}$) on mortality in New England for the period from 2000 to\n2012. The main study consists of $2,202$ zip codes covered by $217,660$ 1km\n$\\times$ 1km grid cells with yearly mortality rates, yearly $PM_{2.5}$ averages\nestimated from a spatio-temporal model (error-prone exposure) and several\npotential confounders. The internal validation study includes a subset of 83\n1km $\\times$ 1km grid cells within 75 zip codes from the main study with\nerror-free yearly $PM_{2.5}$ exposures obtained from monitor stations. Under\nassumptions of non-interference and weak unconfoundedness, using matching we\nfound that exposure to moderate levels of $PM_{2.5}$ ($8 <$ $PM_{2.5}$ $\\leq\n10\\ {\\rm \\mu g/m^3}$) causes a $2.8\\%$ ($95\\%$ CI: $0.6\\%, 3.6\\%$) increase in\nall-cause mortality compared to low exposure ($PM_{2.5}$ $\\leq 8\\ {\\rm \\mu\ng/m^3}$).\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 17:25:32 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 21:26:51 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Wu", "Xiao", ""], ["Braun", "Danielle", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Choirat", "Christine", ""], ["Di", "Qian", ""], ["Dominici", "Francesca", ""]]}, {"id": "1712.01263", "submitter": "Tanner Fiez", "authors": "Tanner Fiez, Lillian Ratliff", "title": "Data-Driven Spatio-Temporal Analysis of Curbside Parking Demand: A\n  Case-Study in Seattle", "comments": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid expansion of urban areas in recent years, management of curbside\nparking has become increasingly important. To mitigate congestion, while\nmeeting a city's diverse needs, performance-based pricing schemes have received\na significant amount of attention. However, several recent studies suggest\nlocation, time-of-day, and awareness of policies are the primary factors that\ndrive parking decisions. In light of this, we provide an extensive data-driven\nstudy of the spatio-temporal characteristics of curbside parking. This work\nadvances the understanding of where and when to set pricing policies, as well\nas where to target information and incentives to drivers looking to park.\nHarnessing data provided by the Seattle Department of Transportation, we\ndevelop a Gaussian mixture model based technique to identify zones with similar\nspatial parking demand as quantified by spatial autocorrelation. In support of\nthis technique, we introduce a metric based on the repeatability of our\nGaussian mixture model to investigate temporal consistency.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 21:09:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Fiez", "Tanner", ""], ["Ratliff", "Lillian", ""]]}, {"id": "1712.01521", "submitter": "Wei Xiao", "authors": "Wei Xiao", "title": "An Online Algorithm for Nonparametric Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric correlations such as Spearman's rank correlation and Kendall's\ntau correlation are widely applied in scientific and engineering fields. This\npaper investigates the problem of computing nonparametric correlations on the\nfly for streaming data. Standard batch algorithms are generally too slow to\nhandle real-world big data applications. They also require too much memory\nbecause all the data need to be stored in the memory before processing. This\npaper proposes a novel online algorithm for computing nonparametric\ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and\nis quite suitable for edge devices, where only limited memory and processing\npower are available. You can seek a balance between speed and accuracy by\nchanging the number of cutpoints specified in the algorithm. The online\nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster\nthan the corresponding batch algorithm, and it can compute them based either on\nall past observations or on fixed-size sliding windows.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 08:19:11 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Xiao", "Wei", ""]]}, {"id": "1712.01555", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt and Jorge Mateu", "title": "Second-order and local characteristics of network intensity functions", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an increase of interest in the spatial analysis\nof structured point patterns over networks whose analysis is challenging\nbecause of geometrical complexities and unique methodological problems. In this\ncontext, it is essential to incorporate the network specificity into the\nanalysis as the locations of events are restricted to areas covered by line\nsegments. Relying on concepts originating from graph theory, we extend the\nnotions of first-order network intensity functions to second-order and local\nnetwork intensity functions. We consider two types of local indicators of\nnetwork association functions which can be understood as adaptations of the\nprimary ideas of local analysis on the plane. We develop the node-wise and\ncross-hierarchical type of local functions. A real dataset on urban\ndisturbances is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 10:12:39 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1712.01900", "submitter": "Paul Patrone", "authors": "Paul N. Patrone, Anthony J. Kearsley, Andrew M. Dienstfrey", "title": "The Role of Data Analysis in Uncertainty Quantification: Case Studies\n  for Materials Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational materials science, mechanical properties are typically\nextracted from simulations by means of analysis routines that seek to mimic\ntheir experimental counterparts. However, simulated data often exhibit\nuncertainties that can propagate into final predictions in unexpected ways.\nThus, modelers require data analysis tools that (i) address the problems posed\nby simulated data, and (ii) facilitate uncertainty quantification. In this\nmanuscript, we discuss three case studies in materials modeling where careful\ndata analysis can be leveraged to address specific instances of these issues.\nAs a unifying theme, we highlight the idea that attention to physical and\nmathematical constraints surrounding the generation of computational data can\nsignificantly enhance its analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 20:12:44 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Patrone", "Paul N.", ""], ["Kearsley", "Anthony J.", ""], ["Dienstfrey", "Andrew M.", ""]]}, {"id": "1712.01995", "submitter": "Bahman Moghimi", "authors": "Bahman Moghimi, Abolfazl Safikhani, Camille Kamga, Wei Hao, JiaQi Ma", "title": "Short-Term Prediction of Signal Cycle in Actuated-Controlled Corridor\n  Using Sparse Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic signals as part of intelligent transportation systems can play a\nsignificant role toward making cities smart. Conventionally, most traffic\nlights are designed with fixed-time control, which induces a lot of slack time\n(unused green time). Actuated traffic lights control traffic flow in real time\nand are more responsive to the variation of traffic demands. For an isolated\nsignal, a family of time series models such as autoregressive integrated moving\naverage (ARIMA) models can be beneficial for predicting the next cycle length.\nHowever, when there are multiple signals placed along a corridor with different\nspacing and configurations, the cycle length variation of such signals is not\njust related to each signal's values, but it is also affected by the platoon of\nvehicles coming from neighboring intersections. In this paper, a multivariate\ntime series model is developed to analyze the behavior of signal cycle lengths\nof multiple intersections placed along a corridor in a fully actuated setup.\nFive signalized intersections have been modeled along a corridor, with\ndifferent spacing among them, together with multiple levels of traffic demand.\nTo tackle the high-dimensional nature of the problem, penalized least squares\nmethod are utilized in the estimation procedure to output sparse models. Two\nproposed sparse time series methods captured the signal data reasonably well,\nand outperformed the conventional vector autoregressive (VAR) model - in some\ncases up to 17% - as well as being more powerful than univariate models such as\nARIMA.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 01:26:12 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 18:56:59 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Moghimi", "Bahman", ""], ["Safikhani", "Abolfazl", ""], ["Kamga", "Camille", ""], ["Hao", "Wei", ""], ["Ma", "JiaQi", ""]]}, {"id": "1712.02001", "submitter": "Abolfazl Safikhani", "authors": "Sabiheh Sadat Faghih, Abolfazl Safikhani, Bahman Moghimi, Camille\n  Kamga", "title": "Predicting Short-Term Uber Demand Using Spatio-Temporal Modeling: A New\n  York City Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for e-hailing services is growing rapidly, especially in large\ncities. Uber is the first and popular e-hailing company in the United Stated\nand New York City. A comparison of the demand for yellow-cabs and Uber in NYC\nin 2014 and 2015 shows that the demand for Uber has increased. However, this\ndemand may not be distributed uniformly either spatially or temporally. Using\nspatio-temporal time series models can help us to better understand the demand\nfor e-hailing services and to predict it more accurately. This paper analyzes\nthe prediction performance of one temporal model (vector autoregressive (VAR))\nand two spatio-temporal models (Spatial-temporal autoregressive (STAR); least\nabsolute shrinkage and selection operator applied on STAR (LASSO-STAR)) and for\ndifferent scenarios (based on the number of time and space lags), and applied\nto both rush hours and non-rush hours periods. The results show the need of\nconsidering spatial models for taxi demand.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 01:44:52 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 19:55:04 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 17:53:36 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Faghih", "Sabiheh Sadat", ""], ["Safikhani", "Abolfazl", ""], ["Moghimi", "Bahman", ""], ["Kamga", "Camille", ""]]}, {"id": "1712.02183", "submitter": "Diego Marcondes", "authors": "Diego Marcondes, Cl\\'audia Peixoto and Ana Carolina Maia", "title": "A survey of a hurdle model for heavy-tailed data based on the\n  generalized lambda distribution", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": "10.1080/03610926.2018.1549251", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey we present an extensive research of the vast literature about\nthe Generalized Lambda Distribution (GLD) and propose a hurdle, or two-way,\nmodel whose associated distribution is the GLD in order to meet the demand for\na highly flexible model of heavy-tailed data with excess of zeros. We apply the\ndeveloped models to a dataset consisting of yearly healthcare expenses, a\ntypical example of heavy-tailed data with excess of zeros. The fitted models\nare compared with models based on the Generalised Pareto Distribution and it is\nestablished that the GLD models perform best.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 13:40:33 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 16:27:38 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 14:30:21 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 13:50:03 GMT"}, {"version": "v5", "created": "Thu, 20 Sep 2018 14:50:09 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Marcondes", "Diego", ""], ["Peixoto", "Cl\u00e1udia", ""], ["Maia", "Ana Carolina", ""]]}, {"id": "1712.02195", "submitter": "Ranjan Maitra", "authors": "Alejandro Murua and Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model", "comments": "18 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is important in statistical modeling and inference in many\napplications, however its normalizing constant, mean number of active vertices\nand mean spin interaction are intractable. We provide accurate approximations\nthat make it possible to calculate these quantities numerically. Simulation\nstudies indicate good performance when compared to Markov Chain Monte Carlo\nmethods and at a tiny fraction of the time. The methodology is also used to\nperform Bayesian inference in a functional Magnetic Resonance Imaging\nactivation detection experiment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 14:24:34 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 00:10:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Murua", "Alejandro", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.02224", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Paolo Cintia and Dino Pedreschi and Fosca\n  Giannotti and Albert-Laszlo Barabasi", "title": "Human Perception of Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.AI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are routinely asked to evaluate the performance of other individuals,\nseparating success from failure and affecting outcomes from science to\neducation and sports. Yet, in many contexts, the metrics driving the human\nevaluation process remain unclear. Here we analyse a massive dataset capturing\nplayers' evaluations by human judges to explore human perception of performance\nin soccer, the world's most popular sport. We use machine learning to design an\nartificial judge which accurately reproduces human evaluation, allowing us to\ndemonstrate how human observers are biased towards diverse contextual features.\nBy investigating the structure of the artificial judge, we uncover the aspects\nof the players' behavior which attract the attention of human judges,\ndemonstrating that human evaluation is based on a noticeability heuristic where\nonly feature values far from the norm are considered to rate an individual's\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 09:13:32 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""], ["Pedreschi", "Dino", ""], ["Giannotti", "Fosca", ""], ["Barabasi", "Albert-Laszlo", ""]]}, {"id": "1712.02326", "submitter": "Ricardo Ehlers", "authors": "David S. Dias, Ricardo S. Ehlers", "title": "Stochastic Volatily Models using Hamiltonian Monte Carlo Methods and\n  Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study using the Bayesian approach in stochastic\nvolatility models for modeling financial time series, using Hamiltonian Monte\nCarlo methods (HMC). We propose the use of other distributions for the errors\nin the observation equation of stochastic volatility models, besides the\nGaussian distribution, to address problems as heavy tails and asymmetry in the\nreturns. Moreover, we use recently developed information criteria WAIC and LOO\nthat approximate the cross-validation methodology, to perform the selection of\nmodels. Throughout this work, we study the quality of the HMC methods through\nexamples, simulation studies and applications to real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:48:50 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Dias", "David S.", ""], ["Ehlers", "Ricardo S.", ""]]}, {"id": "1712.02379", "submitter": "Todd Kuffner", "authors": "Liang Hong and Todd A. Kuffner and Ryan Martin", "title": "On overfitting and post-selection uncertainty assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression context, when the relevant subset of explanatory variables is\nuncertain, it is common to use a data-driven model selection procedure.\nClassical linear model theory, applied naively to the selected sub-model, may\nnot be valid because it ignores the selected sub-model's dependence on the\ndata. We provide an explanation of this phenomenon, in terms of overfitting,\nfor a class of model selection criteria.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:14:30 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Hong", "Liang", ""], ["Kuffner", "Todd A.", ""], ["Martin", "Ryan", ""]]}, {"id": "1712.02476", "submitter": "Dilanka Shenal Dedduwakumara", "authors": "Dilanka S. Dedduwakumara, Luke A. Prendergast", "title": "Confidence Intervals for Quantiles from Histograms and Other Grouped\n  Data", "comments": "19 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval estimation of quantiles has been treated by many in the literature.\nHowever, to the best of our knowledge there has been no consideration for\ninterval estimation when the data are available in grouped format. Motivated by\nthis, we introduce several methods to obtain confidence intervals for quantiles\nwhen only grouped data is available. Our preferred method for interval\nestimation is to approximate the underlying density using the Generalized\nLambda Distribution (GLD) to both estimate the quantiles and variance of the\nquantile estimators. We compare the GLD method with some other methods that we\nalso introduce which are based on a frequency approximation approach and a\nlinear interpolation approximation of the density. Our methods are strongly\nsupported by simulations showing that excellent coverage can be achieved for a\nwide number of distributions. These distributions include highly-skewed\ndistributions such as the log-normal, Dagum and Singh-Maddala distributions. We\nalso apply our methods to real data and show that inference can be carried out\non published outcomes that have been summarized only by a histogram. Our\nmethods are therefore useful for a broad range of applications. We have also\ncreated a web application that can be used to conveniently calculate the\nestimators.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 02:42:52 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Dedduwakumara", "Dilanka S.", ""], ["Prendergast", "Luke A.", ""]]}, {"id": "1712.02595", "submitter": "Robert Richardson", "authors": "Robert R. Richardson, Christoph R. Birkl, Michael A. Osborne and David\n  A. Howey", "title": "Gaussian Process Regression for In-situ Capacity Estimation of\n  Lithium-ion Batteries", "comments": "12 pages, 10 figures, submitted to IEEE Transactions on Industrial\n  Informatics", "journal-ref": null, "doi": "10.1109/TII.2018.2794997", "report-no": "TII-17-1314", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate on-board capacity estimation is of critical importance in\nlithium-ion battery applications. Battery charging/discharging often occurs\nunder a constant current load, and hence voltage vs. time measurements under\nthis condition may be accessible in practice. This paper presents a data-driven\ndiagnostic technique, Gaussian Process regression for In-situ Capacity\nEstimation (GP-ICE), which estimates battery capacity using voltage\nmeasurements over short periods of galvanostatic operation. Unlike previous\nworks, GP-ICE does not rely on interpreting the voltage-time data as\nIncremental Capacity (IC) or Differential Voltage (DV) curves. This overcomes\nthe need to differentiate the voltage-time data (a process which amplifies\nmeasurement noise), and the requirement that the range of voltage measurements\nencompasses the peaks in the IC/DV curves. GP-ICE is applied to two datasets,\nconsisting of 8 and 20 cells respectively. In each case, within certain voltage\nranges, as little as 10 seconds of galvanostatic operation enables capacity\nestimates with approximately 2-3% RMSE.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 12:48:36 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 16:46:07 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Richardson", "Robert R.", ""], ["Birkl", "Christoph R.", ""], ["Osborne", "Michael A.", ""], ["Howey", "David A.", ""]]}, {"id": "1712.02728", "submitter": "Cam Bermudez", "authors": "Camilo Bermudez, Varvara N Probst, Larry T Davis, Thomas Lasko,\n  Bennett A Landman", "title": "Opportunities for Mining Radiology Archives for Pediatric Control Images", "comments": "MASI Brief Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large database of brain imaging data from healthy, normal controls is\nuseful to describe physiologic and pathologic structural changes at a\npopulation scale. In particular, these data can provide information about\nstructural changes throughout development and aging. However, scarcity of\ncontrol data as well as technical challenges during imaging acquisition has\nmade it difficult to collect large amounts of data in a healthy pediatric\npopulation. In this study, we search the medical record at Vanderbilt\nUniversity Medical Center for pediatric patients who received brain imaging,\neither CT or MRI, according to 7 common complaints: headache, seizure, altered\nlevel of consciousness, nausea and vomiting, dizziness, head injury, and gait\nabnormalities in order to find the percent of studies that demonstrated\npathologic findings. Using a text-search based algorithm, we show that an\naverage of 59.3% of MRI studies and 37.3% of CT scans are classified as normal,\nresulting in the production of thousands of normal images. These results\nsuggest there is a wealth of pediatric imaging control data which can be used\nto create normative descriptions of development as well as to establish\nbiomarkers for disease.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:19:20 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Bermudez", "Camilo", ""], ["Probst", "Varvara N", ""], ["Davis", "Larry T", ""], ["Lasko", "Thomas", ""], ["Landman", "Bennett A", ""]]}, {"id": "1712.02749", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster, Paul G. Constantine, T.J. Sullivan", "title": "Exact active subspace Metropolis-Hastings, with applications to the\n  Lorenz-96 system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of active subspaces to inform a\nMetropolis-Hastings algorithm, thereby aggressively reducing the computational\ndimension of the sampling problem. We show that the original formulation, as\nproposed by Constantine, Kent, and Bui-Thanh (SIAM J. Sci. Comput.,\n38(5):A2779-A2805, 2016), possesses asymptotic bias. Using pseudo-marginal\narguments, we develop an asymptotically unbiased variant. Our algorithm is\napplied to a synthetic multimodal target distribution as well as a Bayesian\nformulation of a parameter inference problem for a Lorenz-96 system.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:53:57 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Schuster", "Ingmar", ""], ["Constantine", "Paul G.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1712.02860", "submitter": "Amir Ahmadi Javid", "authors": "Amir Ahmadi-Javid and Mohsen Ebadi", "title": "Remarks on Bayesian Control Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE math.OC math.PR q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a considerable amount of ongoing research on the use of Bayesian\ncontrol charts for detecting a shift from a good quality distribution to a bad\nquality distribution in univariate and multivariate processes. It is widely\nclaimed that Bayesian control charts are economically optimal; see, for\nexample, Calabrese (1995) [Bayesian process control for attributes. Management\nScience, DOI: 10.1287/mnsc.41.4.637] and Makis (2008) [Multivariate Bayesian\ncontrol chart. Operations Research, DOI: 10.1287/opre.1070.0495]. Some\nresearchers also generalize the optimality of controls defined based on\nposterior probabilities to the class of partially observable Markov decision\nprocesses. This note points out that the existing Bayesian control charts\ncannot generally be optimal because many years ago an analytical counterexample\nwas provided by Taylor (1965) [Markovian sequential replacement processes. The\nAnnals of Mathematical Statistics, DOI: 10.1214/aoms/1177699796].\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 21:10:27 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 13:22:34 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Ahmadi-Javid", "Amir", ""], ["Ebadi", "Mohsen", ""]]}, {"id": "1712.02964", "submitter": "Amir Nikooienejad", "authors": "Amir Nikooienejad, Wenyi Wang and Valen E. Johnson", "title": "Bayesian Variable Selection For Survival Data Using Inverse Moment\n  Priors", "comments": "33 pages, 11 figures", "journal-ref": "Annals of Applied Statistics 14.2 (2020): 809-828", "doi": "10.1214/20-AOAS1325", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient variable selection in high-dimensional cancer genomic studies is\ncritical for discovering genes associated with specific cancer types and for\npredicting response to treatment. Censored survival data is prevalent in such\nstudies. In this article we introduce a Bayesian variable selection procedure\nthat uses a mixture prior composed of a point mass at zero and an inverse\nmoment prior in conjunction with the partial likelihood defined by the Cox\nproportional hazard model. The procedure is implemented in the R package\nBVSNLP, which supports parallel computing and uses a stochastic search method\nto explore the model space. Bayesian model averaging is used for prediction.\nThe proposed algorithm provides better performance than other variable\nselection procedures in simulation studies, and appears to provide more\nconsistent variable selection when applied to actual genomic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:52:52 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 22:19:47 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 07:45:30 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 20:55:07 GMT"}, {"version": "v5", "created": "Mon, 4 Jun 2018 21:52:26 GMT"}, {"version": "v6", "created": "Wed, 9 Oct 2019 04:47:34 GMT"}, {"version": "v7", "created": "Sat, 14 Mar 2020 20:18:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nikooienejad", "Amir", ""], ["Wang", "Wenyi", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1712.03058", "submitter": "Theresa Stocks", "authors": "Theresa Stocks", "title": "Iterated filtering methods for Markov process epidemic models", "comments": "This manuscript is a preprint of a chapter to appear in the Handbook\n  of Infectious Disease Data Analysis, Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman \\& Hall/CRC, 2018. Please use the book for\n  possible citations. Corrected typo in the references and modified second\n  example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic epidemic models have proven valuable for public health decision\nmakers as they provide useful insights into the understanding and prevention of\ninfectious diseases. However, inference for these types of models can be\ndifficult because the disease spread is typically only partially observed e.g.\nin form of reported incidences in given time periods. This chapter discusses\nhow to perform likelihood-based inference for partially observed Markov\nepidemic models when it is relatively easy to generate samples from the Markov\ntransmission model while the likelihood function is intractable. The first part\nof the chapter reviews the theoretical background of inference for partially\nobserved Markov processes (POMP) via iterated filtering. In the second part of\nthe chapter the performance of the method and associated practical difficulties\nare illustrated on two examples. In the first example a simulated outbreak data\nset consisting of the number of newly reported cases aggregated by week is\nfitted to a POMP where the underlying disease transmission model is assumed to\nbe a simple Markovian SIR model. The second example illustrates possible model\nextensions such as seasonal forcing and over-dispersion in both, the\ntransmission and observation model, which can be used, e.g., when analysing\nroutinely collected rotavirus surveillance data. Both examples are implemented\nusing the R-package pomp (King et al., 2016) and the code is made available\nonline.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 13:44:34 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 09:08:34 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 10:18:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Stocks", "Theresa", ""]]}, {"id": "1712.03120", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Abhishek Pratap, Thanneer M Perumal, Meghasyam\n  Tummalacherla, Brian M Bot, Andrew D Trister, Stephen H Friend, Lara\n  Mangravite, Larsson Omberg", "title": "Learning Disease vs Participant Signatures: a permutation test approach\n  to detect identity confounding in machine learning diagnostic applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Saeb et al (2017) showed that, in diagnostic machine learning\napplications, having data of each subject randomly assigned to both training\nand test sets (record-wise data split) can lead to massive underestimation of\nthe cross-validation prediction error, due to the presence of \"subject identity\nconfounding\" caused by the classifier's ability to identify subjects, instead\nof recognizing disease. To solve this problem, the authors recommended the\nrandom assignment of the data of each subject to either the training or the\ntest set (subject-wise data split). The adoption of subject-wise split has been\ncriticized in Little et al (2017), on the basis that it can violate assumptions\nrequired by cross-validation to consistently estimate generalization error. In\nparticular, adopting subject-wise splitting in heterogeneous data-sets might\nlead to model under-fitting and larger classification errors. Hence, Little et\nal argue that perhaps the overestimation of prediction errors with subject-wise\ncross-validation, rather than underestimation with record-wise\ncross-validation, is the reason for the discrepancies between prediction error\nestimates generated by the two splitting strategies. In order to shed light on\nthis controversy, we focus on simpler classification performance metrics and\ndevelop permutation tests that can detect identity confounding. By focusing on\npermutation tests, we are able to evaluate the merits of record-wise and\nsubject-wise data splits under more general statistical dependencies and\ndistributional structures of the data, including situations where\ncross-validation breaks down. We illustrate the application of our tests using\nsynthetic and real data from a Parkinson's disease study.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:23:58 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 23:32:58 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Pratap", "Abhishek", ""], ["Perumal", "Thanneer M", ""], ["Tummalacherla", "Meghasyam", ""], ["Bot", "Brian M", ""], ["Trister", "Andrew D", ""], ["Friend", "Stephen H", ""], ["Mangravite", "Lara", ""], ["Omberg", "Larsson", ""]]}, {"id": "1712.03304", "submitter": "Pedro Ramos", "authors": "Pedro Luiz Ramos, Diego Nascimento, Camila Cocolo, M\\'arcio Jos\\'e\n  Nicola, Carlos Alonso, Luiz Gustavo Ribeiro, Andr\\'e Ennes, Francisco Louzada", "title": "Reliability-centered maintenance: analyzing failure in harvest sugarcane\n  machine using some generalizations of the Weibull distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we considered five generalizations of the standard Weibull\ndistribution to describe the lifetime of two important components of harvest\nsugarcane machines. The harvesters considered in the analysis does the harvest\nof an average of 20 tons of sugarcane per hour and their malfunction may lead\nto major losses, therefore, an effective maintenance approach is of main\ninteresting for cost savings. For the considered distributions, the\nmathematical background is presented. Maximum likelihood is used for parameter\nestimation. Further, different discrimination procedures were used to obtain\nthe best fit for each component. At the end, we propose a maintenance\nscheduling for the components of the harvesters using predictive analysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 22:51:02 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Ramos", "Pedro Luiz", ""], ["Nascimento", "Diego", ""], ["Cocolo", "Camila", ""], ["Nicola", "M\u00e1rcio Jos\u00e9", ""], ["Alonso", "Carlos", ""], ["Ribeiro", "Luiz Gustavo", ""], ["Ennes", "Andr\u00e9", ""], ["Louzada", "Francisco", ""]]}, {"id": "1712.03512", "submitter": "Vladimir Bochkarev", "authors": "Inna A. Belashova and Vladimir V. Bochkarev", "title": "Comparative analysis of criteria for filtering time series of word usage\n  frequencies", "comments": "10 pages, 4 figures. This report was presented at ITISE 2017\n  (International work-conference on Time Series), September 18-20th 2017,\n  Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method of nonlinear wavelet thresholding of time\nseries. The Ramachandran-Ranganathan runs test is used to assess the quality of\napproximation. To minimize the objective function, it is proposed to use\ngenetic algorithms - one of the stochastic optimization methods. The suggested\nmethod is tested both on the model series and on the word frequency series\nusing the Google Books Ngram data. It is shown that method of filtering which\nuses the runs criterion shows significantly better results compared with the\nstandard wavelet thresholding. The method can be used when quality of filtering\nis of primary importance but not the speed of calculations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 12:04:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Belashova", "Inna A.", ""], ["Bochkarev", "Vladimir V.", ""]]}, {"id": "1712.03553", "submitter": "Jason Poulos", "authors": "Jason Poulos, Shuxi Zeng", "title": "RNN-based counterfactual prediction, with an application to homestead\n  policy and public schooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for estimating the effect of a policy\nintervention on an outcome over time. We train recurrent neural networks (RNNs)\non the history of control unit outcomes to learn a useful representation for\npredicting future outcomes. The learned representation of control units is then\napplied to the treated units for predicting counterfactual outcomes. RNNs are\nspecifically structured to exploit temporal dependencies in panel data, and are\nable to learn negative and nonlinear interactions between control unit\noutcomes. We apply the method to the problem of estimating the long-run impact\nof U.S. homestead policy on public school spending.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 16:00:18 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 23:14:22 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 18:47:57 GMT"}, {"version": "v4", "created": "Fri, 11 May 2018 17:51:34 GMT"}, {"version": "v5", "created": "Sat, 13 Apr 2019 00:30:08 GMT"}, {"version": "v6", "created": "Wed, 30 Sep 2020 20:14:51 GMT"}, {"version": "v7", "created": "Mon, 17 May 2021 13:56:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Poulos", "Jason", ""], ["Zeng", "Shuxi", ""]]}, {"id": "1712.03636", "submitter": "Wanyun Shao", "authors": "Wanyun Shao, Maaz Gardezi, Siyuan Xian", "title": "Examining the Effects of Objective Hurricane Risks and Community\n  Resilience on Risk Perceptions of Hurricanes at the County Level in the U.S.\n  Gulf Coast: An Innovative Approach", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community risk perceptions can influence their abilities to cope with coastal\nhazards such as hurricanes and coastal flooding.Our study presents an initial\neffort to examine the relationship between community resilience and risk\nperception at the county level, through innovative construction of aggregate\nvariables. Utilizing the 2012 Gulf Coast Climate Change Survey merged with\nhistorical hurricane data and community resilience indicators, we first apply a\nspatial statistical model to construct a county level risk perception indicator\nbased on survey responses. Next, we employ regression to reveal the\nrelationship between contextual hurricane risk factors and community\nresilience, on one hand, and county level perceptions of hurricane risks, on\nthe other. Results of this study are directly applicable in the policy making\ndomain as many hazard mitigation plans and adaptation policies are designed and\nimplemented at the county level. Specifically, two major findings stand out.\nFirst, the contextual hurricane risks represented by peak height of storm surge\nassociated with the last hurricane landfall and land area exposed to historical\nstorm surge flooding positively affect county level risk perceptions. This\nindicates that hurricanes another threat wind risks need to be clearly\ncommunicated with the public and fully incorporated into hazard mitigation\nplans and adaptation policies. Second, two components of community resilience\nhigher levels of economic resilience and community capital are found to lead to\nheightened perceptions of hurricane risks, which suggests that concerted\nefforts are needed to raise awareness of hurricane risks among counties with\nless economic and community capitals.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 03:41:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Shao", "Wanyun", ""], ["Gardezi", "Maaz", ""], ["Xian", "Siyuan", ""]]}, {"id": "1712.03686", "submitter": "Maria Perez-Ortiz", "authors": "Maria Perez-Ortiz and Rafal K. Mantiuk", "title": "A practical guide and software for analysing pairwise comparison\n  experiments", "comments": "Code available at https://github.com/mantiuk/pwcmp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular strategies to capture subjective judgments from humans involve\nthe construction of a unidimensional relative measurement scale, representing\norder preferences or judgments about a set of objects or conditions. This\ninformation is generally captured by means of direct scoring, either in the\nform of a Likert or cardinal scale, or by comparative judgments in pairs or\nsets. In this sense, the use of pairwise comparisons is becoming increasingly\npopular because of the simplicity of this experimental procedure. However, this\nstrategy requires non-trivial data analysis to aggregate the comparison ranks\ninto a quality scale and analyse the results, in order to take full advantage\nof the collected data. This paper explains the process of translating pairwise\ncomparison data into a measurement scale, discusses the benefits and\nlimitations of such scaling methods and introduces a publicly available\nsoftware in Matlab. We improve on existing scaling methods by introducing\noutlier analysis, providing methods for computing confidence intervals and\nstatistical testing and introducing a prior, which reduces estimation error\nwhen the number of observers is low. Most of our examples focus on image\nquality assessment.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 09:21:36 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 14:43:34 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Perez-Ortiz", "Maria", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1712.03797", "submitter": "Daniel Kosiorowski", "authors": "Daniel Kosiorowski, Dominik Mielczarek, Jerzy. P. Rydlewski", "title": "Forecasting of a Hierarchical Functional Time Series on Example of\n  Macromodel for Day and Night Air Pollution in Silesia Region: A Critical\n  Overview", "comments": "arXiv admin note: text overlap with arXiv:1710.02669", "journal-ref": "Central European Journal of Economic Modelling and Econometrics,\n  10: 53-73 (2018)", "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In economics we often face a system, which intrinsically imposes a structure\nof hierarchy of its components, i.e., in modelling trade accounts related to\nforeign exchange or in optimization of regional air protection policy.\n  A problem of reconciliation of forecasts obtained on different levels of\nhierarchy has been addressed in the statistical and econometric literature for\nmany times and concerns bringing together forecasts obtained independently at\ndifferent levels of hierarchy.\n  This paper deals with this issue in case of a hierarchical functional time\nseries. We present and critically discuss a state of art and indicate\nopportunities of an application of these methods to a certain environment\nprotection problem. We critically compare the best predictor known from the\nliterature with our own original proposal. Within the paper we study a\nmacromodel describing a day and night air pollution in Silesia region divided\ninto five subregions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 18:36:37 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy. P.", ""]]}, {"id": "1712.03834", "submitter": "Luiz Gustavo De Andrade Alves", "authors": "Luiz G A Alves, Haroldo V Ribeiro, Francisco A Rodrigues", "title": "Crime prediction through urban metrics and statistical learning", "comments": "Accepted for publication in Physica A", "journal-ref": "Physica A 505, 435-443 (2018)", "doi": "10.1016/j.physa.2018.03.084", "report-no": null, "categories": "physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the causes of crime is a longstanding issue in researcher's\nagenda. While it is a hard task to extract causality from data, several linear\nmodels have been proposed to predict crime through the existing correlations\nbetween crime and urban metrics. However, because of non-Gaussian distributions\nand multicollinearity in urban indicators, it is common to find controversial\nconclusions about the influence of some urban indicators on crime. Machine\nlearning ensemble-based algorithms can handle well such problems. Here, we use\na random forest regressor to predict crime and quantify the influence of urban\nindicators on homicides. Our approach can have up to 97% of accuracy on crime\nprediction, and the importance of urban indicators is ranked and clustered in\ngroups of equal influence, which are robust under slightly changes in the data\nsample analyzed. Our results determine the rank of importance of urban\nindicators to predict crime, unveiling that unemployment and illiteracy are the\nmost important variables for describing homicides in Brazilian cities. We\nfurther believe that our approach helps in producing more robust conclusions\nregarding the effects of urban indicators on crime, having potential\napplications for guiding public policies for crime control.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 13:38:23 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 12:47:52 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Alves", "Luiz G A", ""], ["Ribeiro", "Haroldo V", ""], ["Rodrigues", "Francisco A", ""]]}, {"id": "1712.04058", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, Jay Belsky, Eszter Szekely, Keith F.\n  Widaman, Michael Pluess, Celia Greenwood and Ashley Wazana", "title": "Distinguishing differential susceptibility, diathesis-stress and vantage\n  sensitivity: beyond the single gene and environment model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, two main approaches exist to distinguish differential\nsusceptibility from diathesis-stress and vantage sensitivity in genotype x\nenvironment interaction (GxE) research: Regions of significance (RoS) and\ncompetitive-confirmatory approaches. Each is limited by their\nsingle-gene/single-environment foci given that most phenotypes are the product\nof multiple interacting genetic and environmental factors. We thus addressed\nthese two concerns in a recently developed R package (LEGIT) for constructing\nGxE interaction models with latent genetic and environmental scores using\nalternating optimization. Herein we test, by means of computer simulation,\ndiverse GxE models in the context of both single and multiple genes and\nenvironments. Results indicate that the RoS and competitive-confirmatory\napproaches were highly accurate when the sample size was large, whereas the\nlatter performed better in small samples and for small effect sizes. The\nconfirmatory approach generally had good accuracy (a) when effect size was\nmoderate and N >= 500 and (b) when effect size was large and N >= 250, whereas\nRoS performed poorly. Computational tools to determine the type of GxE of\nmultiple genes and environments are provided as extensions in our LEGIT R\npackage.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 22:45:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 21:00:35 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 17:51:26 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""], ["Belsky", "Jay", ""], ["Szekely", "Eszter", ""], ["Widaman", "Keith F.", ""], ["Pluess", "Michael", ""], ["Greenwood", "Celia", ""], ["Wazana", "Ashley", ""]]}, {"id": "1712.04075", "submitter": "Won Chang", "authors": "Won Chang, Jiali Wang, Julian Marohnic, Rao Kotamarthi, Elisabeth J.\n  Moyer", "title": "Diagnosing added value of convection-permitting regional models using\n  precipitation event identification and tracking", "comments": null, "journal-ref": null, "doi": "10.1007/s00382-018-4294-0", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical downscaling with high-resolution regional climate models may offer\nthe possibility of realistically reproducing precipitation and weather events\nin climate simulations. As resolutions fall to order kilometers, the use of\nexplicit rather than parametrized convection may offer even greater fidelity.\nHowever, these increased model resolutions both allow and require increasingly\ncomplex diagnostics for evaluating model fidelity. In this study we use a suite\nof dynamically downscaled simulations of the summertime U.S. (WRF driven by\nNCEP) with systematic variations in parameters and treatment of convection as a\ntest case for evaluation of model precipitation. In particular, we use a novel\nrainstorm identification and tracking algorithm that allocates essentially all\nrainfall to individual precipitation events (Chang et al. 2016). This approach\nallows multiple insights, including that, at least in these runs, model wet\nbias is driven by excessive areal extent of precipitating events. Biases are\ntime-dependent, producing excessive diurnal cycle amplitude. We show that this\neffect is produced not by new production of events but by excessive enlargement\nof long-lived precipitation events during daytime, and that in the domain\naverage, precipitation biases appear best represented as additive offsets. Of\nall model configurations evaluated, convection-permitting simulations most\nconsistently reduced biases in precipitation event characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 00:02:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Chang", "Won", ""], ["Wang", "Jiali", ""], ["Marohnic", "Julian", ""], ["Kotamarthi", "Rao", ""], ["Moyer", "Elisabeth J.", ""]]}, {"id": "1712.04078", "submitter": "Gillian Raab", "authors": "Gillian M. Raab and Beata Nowok and Chris Dibben", "title": "Guidelines for Producing Useful Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on our experiences of helping staff of the Scottish Longitudinal\nStudy to create synthetic extracts that can be released to users. In\nparticular, we focus on how the synthesis process can be tailored to produce\nsynthetic extracts that will provide users with similar results to those that\nwould be obtained from the original data. We make recommendations for synthesis\nmethods and illustrate how the staff creating synthetic extracts can evaluate\ntheir utility at the time they are being produced. We discuss measures of\nutility for synthetic data and show that one tabular utility measure is exactly\nequivalent to a measure calculated from a propensity score. The methods are\nillustrated by using the R package $synthpop$ to create synthetic versions of\ndata from the 1901 Census of Scotland.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 00:16:45 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Raab", "Gillian M.", ""], ["Nowok", "Beata", ""], ["Dibben", "Chris", ""]]}, {"id": "1712.04088", "submitter": "Manoel Santos Neto", "authors": "Danillo Xavier and Manoel Santos-Neto and Marcelo Bourguignon and Vera\n  Tomazella", "title": "Zero-Modified Poisson-Lindley distribution with applications in\n  zero-inflated and zero-deflated count data", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main object of this article is to present an extension of the\nzero-inflated Poisson-Lindley distribution, called of zero-modified\nPoisson-Lindley. The additional parameter $\\pi$ of the zero-modified\nPoisson-Lindley has a natural interpretation in terms of either\nzero-deflated/inflated proportion. Inference is dealt with by using the\nlikelihood approach. In particular the maximum likelihood estimators of the\ndistribution's parameter are compared in small and large samples. We also\nconsider an alternative bias-correction mechanism based on Efron's bootstrap\nresampling. The model is applied to real data sets and found to perform better\nthan other competing models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 01:18:15 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:03:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Xavier", "Danillo", ""], ["Santos-Neto", "Manoel", ""], ["Bourguignon", "Marcelo", ""], ["Tomazella", "Vera", ""]]}, {"id": "1712.04594", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Finite-Sample Optimal Estimation and Inference on Average Treatment\n  Effects Under Unconfoundedness", "comments": "45 pages, plus supplemental materials (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference on average treatment effects under\nunconfoundedness conditional on the realizations of the treatment variable and\ncovariates. Given nonparametric smoothness and/or shape restrictions on the\nconditional mean of the outcome variable, we derive estimators and confidence\nintervals (CIs) that are optimal in finite samples when the regression errors\nare normal with known variance. In contrast to conventional CIs, our CIs use a\nlarger critical value that explicitly takes into account the potential bias of\nthe estimator. When the error distribution is unknown, feasible versions of our\nCIs are valid asymptotically, even when $\\sqrt{n}$-inference is not possible\ndue to lack of overlap, or low smoothness of the conditional mean. We also\nderive the minimum smoothness conditions on the conditional mean that are\nnecessary for $\\sqrt{n}$-inference. When the conditional mean is restricted to\nbe Lipschitz with a large enough bound on the Lipschitz constant, the optimal\nestimator reduces to a matching estimator with the number of matches set to\none. We illustrate our methods in an application to the National Supported Work\nDemonstration.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 02:57:02 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 15:47:17 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 16:38:30 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 16:16:48 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 15:59:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1712.04723", "submitter": "Jialiang Mao", "authors": "Jialiang Mao, Yuhan Chen, Li Ma", "title": "Bayesian graphical compositional regression for microbiome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in microbiome studies is to test the existence of and give\ncharacterization to differences in the microbiome composition across groups of\nsamples. Important challenges of this problem include the large within-group\nheterogeneities among samples and the existence of potential confounding\nvariables that, when ignored, increase the chance of false discoveries and\nreduce the power for identifying true differences. We propose a probabilistic\nframework to overcome these issues by combining three ideas: (i) a phylogenetic\ntree-based decomposition of the cross-group comparison problem into a series of\nlocal tests, (ii) a graphical model that links the local tests to allow\ninformation sharing across taxa, and (iii) a Bayesian testing strategy that\nincorporates covariates and integrates out the within-group variation, avoiding\npotentially unstable point estimates. We derive an efficient inference\nalgorithm based on numerical integration and junction-tree message passing,\nconduct extensive simulation studies to investigate the performance of our\napproach, and compare it to state-of-the-art methods in a number of\nrepresentative settings. We then apply our method to the American Gut data to\nanalyze the association of dietary habits and human's gut microbiome\ncomposition in the presence of covariates, and illustrate the importance of\nincorporating covariates in microbiome cross-group comparison.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 12:06:21 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 19:08:17 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 20:46:00 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mao", "Jialiang", ""], ["Chen", "Yuhan", ""], ["Ma", "Li", ""]]}, {"id": "1712.04775", "submitter": "Clementine Barreyre", "authors": "Cl\\'ementine Barreyre, B\\'eatrice Laurent (IMT), Jean-Michel Loubes\n  (IMT), Bertrand Cabon, Lo\\\"ic Boussouf", "title": "Multiple testing for outlier detection in functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel procedure for outlier detection in functional data, in a\nsemi-supervised framework. As the data is functional, we consider the\ncoefficients obtained after projecting the observations onto orthonormal bases\n(wavelet, PCA). A multiple testing procedure based on the two-sample test is\ndefined in order to highlight the levels of the coefficients on which the\noutliers appear as significantly different to the normal data. The selected\ncoefficients are then called features for the outlier detection, on which we\ncompute the Local Outlier Factor to highlight the outliers. This procedure to\nselect the features is applied on simulated data that mimic the behaviour of\nspace telemetries, and compared with existing dimension reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:07:55 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Barreyre", "Cl\u00e9mentine", "", "IMT"], ["Laurent", "B\u00e9atrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Cabon", "Bertrand", ""], ["Boussouf", "Lo\u00efc", ""]]}, {"id": "1712.04784", "submitter": "Ana Nora Donaldson", "authors": "Angelo Passalacqua, Stephen Dunne, Tim J Newton, Nairn HF Wilson and\n  Ana Nora Donaldson", "title": "Quantifying a qualitative framework of patients perceptions, attitudes\n  and behavior relevant to oral health related quality of life", "comments": "19 pages including 7 pages containing 5 tables. Total of 7,024 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a qualitative study, Gregory, Gibson and Robinson proposed a framework of\nitems grouped in seven dimensions reflecting oral health related perceptions,\nattitudes and behavior to encompass what is relevant when patients assess their\nown oral health related quality of life (OHRQOL). The aim of this study is to\nquantify the dimensions of Gregory relevance framework and confirm, or\notherwise, the influence on the change in the OHIP-14, an instrument widely\nused to measure OHRQOL. The study was observational and longitudinal with the\nOHIP-14 measured before a tooth extraction, and two and four weeks thereafter.\nStatistical methods of analysis consisted of generalised estimating equations.\nResponsiveness (or sensitivity to change) of the OHIP-14 was established in our\npatient population. The dimensions of Gregory relevance framework featured\nsignificantly in the models. Patients trust in dental products, perception of\nown oral health as normal in relation to the average person, preference for\nnatural teeth, character bias in judgement and control by adherence to dentists\ninstructions, were all found to be significant factors in the longitudinal\nchange of the OHIP-14. Borderline significance was found in terms of dental\nanxiety and symptoms. Perceptions, behaviour and attitudes, rather than\nsocio-demographic characteristics or oral health related knowledge, influence\nchange in OHRQOL. Trusting that the dentist values the patient as a person and\nthe importance the patient gives to having good oral health, are not found\nsignificant, yet adhering to dentists advice has a beneficial effect on OHRQOL.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 16:13:49 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Passalacqua", "Angelo", ""], ["Dunne", "Stephen", ""], ["Newton", "Tim J", ""], ["Wilson", "Nairn HF", ""], ["Donaldson", "Ana Nora", ""]]}, {"id": "1712.05063", "submitter": "Alexander Cloninger", "authors": "Jonathan Bates, Alexander Cloninger", "title": "Outcome Based Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to reduce variance in treatment effect estimates in the\nsetting of high-dimensional data. In particular, we introduce an approach for\nlearning a metric to be used in matching treatment and control groups. The\nmetric reduces variance in treatment effect estimates by weighting covariates\nrelated to the outcome and filtering out unrelated covariates.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 00:44:12 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Bates", "Jonathan", ""], ["Cloninger", "Alexander", ""]]}, {"id": "1712.05135", "submitter": "Seksan Kiatsupaibul", "authors": "Seksan Kiatsupaibul and Pariyakorn Maneekul", "title": "A Statistical Model with Qualitative Input", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical estimation model with qualitative input provides a mechanism to\nfuse human intuition in a form of qualitative information into a quantitative\nstatistical model. We investigate statistical properties and devise a numerical\ncomputation method for a model subclass with a uniform correlation structure.\nWe show that, within this subclass, qualitative information can be as objective\nas quantitative information. We also show that the correlation between each\npair of variables controls the accuracy of the statistical estimate. An\napplication to portfolio selection is discussed. The correlation, although\ncompromising the accuracy of the statistical estimation, affects the\nperformance of the portfolio in a minimal way.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 09:25:07 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Kiatsupaibul", "Seksan", ""], ["Maneekul", "Pariyakorn", ""]]}, {"id": "1712.05229", "submitter": "Federica Nicolussi", "authors": "Federica Nicolussi and Manuela Cazzaro", "title": "Context-specific independencies for ordinal variables in chain\n  regression models", "comments": "21 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we handle with categorical (ordinal) variables and we focus on\nthe (in)dependence relationship under the marginal, conditional and\ncontext-specific perspective. If the first two are well known, the last one\nconcerns independencies holding only in a subspace of the outcome space. We\ntake advantage from the Hierarchical Multinomial Marginal models and provide\nseveral original results about the representation of context-specific\nindependencies through these models. By considering the graphical aspect, we\ntake advantage from the chain graphical models. The resultant graphical model\nis a so-called \"stratified\" chain graphical model with labelled arcs. New\nMarkov properties are provided. Furthermore, we consider the graphical models\nunder the regression poit of view. Here we provide simplification of the\nregression parameters due to the context-specific independencies. Finally, an\napplication about the innovation degree of the Italian enterprises is provided.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:04:50 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 15:29:14 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Nicolussi", "Federica", ""], ["Cazzaro", "Manuela", ""]]}, {"id": "1712.05293", "submitter": "Jianan Cao", "authors": "Jianan Cao, David J. Farnham, Upmanu Lall", "title": "Spatial-temporal wind field prediction by Artificial Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1603.07285 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of near surface wind speed is becoming increasingly vital for\nthe operation of electrical energy grids as the capacity of installed wind\npower grows. The majority of predictive wind speed modeling has focused on\npoint-based time-series forecasting. Effectively balancing demand and supply in\nthe presence of distributed wind turbine electricity generation, however,\nrequires the prediction of wind fields in space and time. Additionally,\npredictions of full wind fields are particularly useful for future power\nplanning such as the optimization of electricity power supply systems. In this\npaper, we propose a composite artificial neural network (ANN) model to predict\nthe 6-hour and 24-hour ahead average wind speed over a large area (~3.15*106\nkm2). The ANN model consists of a convolutional input layer, a Long Short-Term\nMemory (LSTM) hidden layer, and a transposed convolutional layer as the output\nlayer. We compare the ANN model with two non-parametric models, a null\npersistence model and a mean value model, and find that the ANN model has\nsubstantially smaller error than each of these models. Additionally, the ANN\nmodel also generally performs better than integrated autoregressive moving\naverage models, which are trained for optimal performance in specific\nlocations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 17:00:15 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Cao", "Jianan", ""], ["Farnham", "David J.", ""], ["Lall", "Upmanu", ""]]}, {"id": "1712.05527", "submitter": "Gery Geenens", "authors": "Gery Geenens and Richard Dunn", "title": "A nonparametric copula approach to conditional Value-at-Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-at-Risk and its conditional allegory, which takes into account the\navailable information about the economic environment, form the centrepiece of\nthe Basel framework for the evaluation of market risk in the banking sector. In\nthis paper, a new nonparametric framework for estimating this conditional\nValue-at-Risk is presented. A nonparametric approach is particularly pertinent\nas the traditionally used parametric distributions have been shown to be\ninsufficiently robust and flexible in most of the equity-return data sets\nobserved in practice. The method extracts the quantile of the conditional\ndistribution of interest, whose estimation is based on a novel estimator of the\ndensity of the copula describing the dynamic dependence observed in the series\nof returns. Real-world back-testing analyses demonstrate the potential of the\napproach, whose performance may be superior to its industry counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 04:27:25 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 03:20:40 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Geenens", "Gery", ""], ["Dunn", "Richard", ""]]}, {"id": "1712.05748", "submitter": "Emma Pierson", "authors": "Emma Pierson, Tim Althoff, Jure Leskovec", "title": "Modeling Individual Cyclic Variation in Human Behavior", "comments": "Accepted at WWW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cycles are fundamental to human health and behavior. However, modeling cycles\nin time series data is challenging because in most cases the cycles are not\nlabeled or directly observed and need to be inferred from multidimensional\nmeasurements taken over time. Here, we present CyHMMs, a cyclic hidden Markov\nmodel method for detecting and modeling cycles in a collection of\nmultidimensional heterogeneous time series data. In contrast to previous cycle\nmodeling methods, CyHMMs deal with a number of challenges encountered in\nmodeling real-world cycles: they can model multivariate data with discrete and\ncontinuous dimensions; they explicitly model and are robust to missing data;\nand they can share information across individuals to model variation both\nwithin and between individual time series. Experiments on synthetic and\nreal-world health-tracking data demonstrate that CyHMMs infer cycle lengths\nmore accurately than existing methods, with 58% lower error on simulated data\nand 63% lower error on real-world data compared to the best-performing\nbaseline. CyHMMs can also perform functions which baselines cannot: they can\nmodel the progression of individual features/symptoms over the course of the\ncycle, identify the most variable features, and cluster individual time series\ninto groups with distinct characteristics. Applying CyHMMs to two real-world\nhealth-tracking datasets -- of menstrual cycle symptoms and physical activity\ntracking data -- yields important insights including which symptoms to expect\nat each point during the cycle. We also find that people fall into several\ngroups with distinct cycle patterns, and that these groups differ along\ndimensions not provided to the model. For example, by modeling missing data in\nthe menstrual cycles dataset, we are able to discover a medically relevant\ngroup of birth control users even though information on birth control is not\ngiven to the model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 16:51:16 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 17:39:50 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 23:07:43 GMT"}, {"version": "v4", "created": "Fri, 20 Apr 2018 08:02:38 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Pierson", "Emma", ""], ["Althoff", "Tim", ""], ["Leskovec", "Jure", ""]]}, {"id": "1712.05865", "submitter": "Anusha Lalitha", "authors": "Anusha Lalitha, Nancy Ronquillo, Tara Javidi", "title": "Improved Target Acquisition Rates with Feedback Codes", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2850751", "report-no": null, "categories": "cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of acquiring an unknown target location\n(among a finite number of locations) via a sequence of measurements, where each\nmeasurement consists of simultaneously probing a group of locations. The\nresulting observation consists of a sum of an indicator of the target's\npresence in the probed region, and a zero mean Gaussian noise term whose\nvariance is a function of the measurement vector. An equivalence between the\ntarget acquisition problem and channel coding over a binary input additive\nwhite Gaussian noise (BAWGN) channel with state and feedback is established.\nUtilizing this information theoretic perspective, a two-stage adaptive target\nsearch strategy based on the sorted Posterior Matching channel coding strategy\nis proposed. Furthermore, using information theoretic converses, the\nfundamental limits on the target acquisition rate for adaptive and non-adaptive\nstrategies are characterized. As a corollary to the non-asymptotic upper bound\nof the expected number of measurements under the proposed two-stage strategy,\nand to non-asymptotic lower bound of the expected number of measurements for\noptimal non-adaptive search strategy, a lower bound on the adaptivity gain is\nobtained. The adaptivity gain is further investigated in different asymptotic\nregimes of interest.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 22:27:25 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Lalitha", "Anusha", ""], ["Ronquillo", "Nancy", ""], ["Javidi", "Tara", ""]]}, {"id": "1712.05879", "submitter": "Gabriel Phelan", "authors": "Gabriel C. Phelan, John T. Whelan", "title": "Hierarchical Bayesian Bradley-Terry for Applications in Major League\n  Baseball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem faced in statistical inference is drawing conclusions from\npaired comparisons, in which two objects compete and one is declared the\nvictor. A probabilistic approach to such a problem is the Bradley-Terry model,\nfirst studied by Zermelo in 1929 and rediscovered by Bradley and Terry in 1952.\nOne obvious area of application for such a model is sporting events, and in\nparticular Major League Baseball. With this in mind, we describe a hierarchical\nBayesian version of Bradley-Terry suitable for use in ranking and prediction\nproblems, and compare results from these application domains to standard\nmaximum likelihood approaches. Our Bayesian methods outperform the MLE-based\nanalogues, while being simple to construct, implement, and interpret.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 00:03:06 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Phelan", "Gabriel C.", ""], ["Whelan", "John T.", ""]]}, {"id": "1712.05997", "submitter": "Amir Karami", "authors": "Amir Karami", "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag of words (BOW) represents a corpus in a matrix whose elements are the\nfrequency of words. However, each row in the matrix is a very high-dimensional\nsparse vector. Dimension reduction (DR) is a popular method to address sparsity\nand high-dimensionality issues. Among different strategies to develop DR\nmethod, Unsupervised Feature Transformation (UFT) is a popular strategy to map\nall words on a new basis to represent BOW. The recent increase of text data and\nits challenges imply that DR area still needs new perspectives. Although a wide\nrange of methods based on the UFT strategy has been developed, the fuzzy\napproach has not been considered for DR based on this strategy. This research\ninvestigates the application of fuzzy clustering as a DR method based on the\nUFT strategy to collapse BOW matrix to provide a lower-dimensional\nrepresentation of documents instead of the words in a corpus. The quantitative\nevaluation shows that fuzzy clustering produces superior performance and\nfeatures to Principal Components Analysis (PCA) and Singular Value\nDecomposition (SVD), two popular DR methods based on the UFT strategy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 17:57:57 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Karami", "Amir", ""]]}, {"id": "1712.06414", "submitter": "Misha Teplitskiy", "authors": "Feng Shi, Misha Teplitskiy, Eamon Duede, James Evans", "title": "The Wisdom of Polarized Crowds", "comments": null, "journal-ref": "Nature Human Behavior. 2019", "doi": "10.1038/s41562-019-0541-6", "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As political polarization in the United States continues to rise, the\nquestion of whether polarized individuals can fruitfully cooperate becomes\npressing. Although diversity of individual perspectives typically leads to\nsuperior team performance on complex tasks, strong political perspectives have\nbeen associated with conflict, misinformation and a reluctance to engage with\npeople and perspectives beyond one's echo chamber. It is unclear whether\nself-selected teams of politically diverse individuals will create higher or\nlower quality outcomes. In this paper, we explore the effect of team political\ncomposition on performance through analysis of millions of edits to Wikipedia's\nPolitical, Social Issues, and Science articles. We measure editors' political\nalignments by their contributions to conservative versus liberal articles. A\nsurvey of editors validates that those who primarily edit liberal articles\nidentify more strongly with the Democratic party and those who edit\nconservative ones with the Republican party. Our analysis then reveals that\npolarized teams---those consisting of a balanced set of politically diverse\neditors---create articles of higher quality than politically homogeneous teams.\nThe effect appears most strongly in Wikipedia's Political articles, but is also\nobserved in Social Issues and even Science articles. Analysis of article \"talk\npages\" reveals that politically polarized teams engage in longer, more\nconstructive, competitive, and substantively focused but linguistically diverse\ndebates than political moderates. More intense use of Wikipedia policies by\npolitically diverse teams suggests institutional design principles to help\nunleash the power of politically polarized teams.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:40:29 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Shi", "Feng", ""], ["Teplitskiy", "Misha", ""], ["Duede", "Eamon", ""], ["Evans", "James", ""]]}, {"id": "1712.06575", "submitter": "Nicolas Behr", "authors": "Nicolas Behr, G\\'erard H. E. Duchamp and Karol A. Penson", "title": "Combinatorics of chemical reaction systems", "comments": "33+12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.CO math.MP math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a concise stochastic mechanics framework for chemical reaction\nsystems that allows to formulate evolution equations for three general types of\ndata: the probability generating functions, the exponential moment generating\nfunctions and the factorial moment generating functions. This formulation\nconstitutes an intimate synergy between techniques of statistical physics and\nof combinatorics. We demonstrate how to analytically solve the evolution\nequations for all six elementary types of single-species chemical reactions by\neither combinatorial normal-ordering techniques, or, for the binary reactions,\nby means of Sobolev-Jacobi orthogonal polynomials. The former set of results in\nparticular highlights the relationship between infinitesimal generators of\nstochastic evolution and parametric transformations of probability\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 18:42:47 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 17:11:13 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Behr", "Nicolas", ""], ["Duchamp", "G\u00e9rard H. E.", ""], ["Penson", "Karol A.", ""]]}, {"id": "1712.06631", "submitter": "Anis Davoudi", "authors": "Anis Davoudi, Duane B. Corbett, Tezcan Ozrazgat-Baslanti, Azra\n  Bihorac, Scott C. Brakenridge, Todd M. Manini, Parisa Rashidi", "title": "Activity and Circadian Rhythm of Sepsis Patients in the Intensive Care\n  Unit", "comments": "4 pages, IEEE Biomedical and Health Informatics (BHI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early mobilization of critically ill patients in the Intensive Care Unit\n(ICU) can prevent adverse outcomes such as delirium and post-discharge physical\nimpairment. To date, no studies have characterized activity of sepsis patients\nin the ICU using granular actigraphy data. This study characterizes the\nactivity of sepsis patients in the ICU to aid in future mobility interventions.\nWe have compared the actigraphy features of 24 patients in four groups: Chronic\nCritical Illness (CCI) sepsis patients in the ICU, Rapid Recovery (RR) sepsis\npatients in the ICU, non-sepsis ICU patients (control-ICU), and healthy\nsubjects. We used several statistical and circadian rhythm features extracted\nfrom the patients' actigraphy data collected over a five-day period. Our\nresults show that the four groups are significantly different in terms of\nactivity features. In addition, we observed that the CCI and control-ICU\npatients show less regularity in their circadian rhythm compared to the RR\npatients. These results show the potential of using actigraphy data for guiding\nmobilization practices, classifying sepsis recovery subtype, as well as for\ntracking patients' recovery.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 19:17:04 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Davoudi", "Anis", ""], ["Corbett", "Duane B.", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bihorac", "Azra", ""], ["Brakenridge", "Scott C.", ""], ["Manini", "Todd M.", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1712.06718", "submitter": "Ruitao Lin", "authors": "Haitao Pan, Ruitao Lin, and Ying Yuan", "title": "Statistical Properties of the Keyboard Design with Extension to\n  Drug-Combination Trials", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The keyboard design is a novel phase I dose-finding method that is simple and\nhas good operating characteristics. This paper studies theoretical properties\nof the keyboard design, including the optimality of its decision rules,\ncoherence in dose transition, and convergence to the target dose. Establishing\nthese theoretical properties explains the mechanism of the design and provides\nassurance to practitioners regarding the behavior of the keyboard design. We\nfurther extend the keyboard design to dual-agent dose-finding trials, which\ninherit the same statistical properties and simplicity as the single-agent\nkeyboard design. Extensive simulations are conducted to evaluate the\nperformance of the proposed keyboard drug-combination design using a novel,\nrandom two-dimensional dose--toxicity scenario generating algorithm. The\nsimulation results confirm the desirable and competitive operating\ncharacteristics of the keyboard design as established by the theoretical study.\nAn R Shiny application is developed to facilitate implementing the keyboard\ncombination design in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 23:23:58 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Pan", "Haitao", ""], ["Lin", "Ruitao", ""], ["Yuan", "Ying", ""]]}, {"id": "1712.06797", "submitter": "Daniel Felix Ahelegbey", "authors": "Daniel Ahelegbey, Luis Carvalho and Eric Kolaczyk", "title": "A Bayesian Covariance Graphical And Latent Position Model For\n  Multivariate Financial Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current understanding holds that financial contagion is driven mainly by the\nsystem-wide interconnectedness of institutions. A distinction has been made\nbetween systematic and idiosyncratic channels of contagion, with shocks\ntransmitted through the latter expected to be substantially more likely to lead\nto systemic crisis than through the former. Idiosyncratic connectivity is\nthought to be driven not simply by obviously shared characteristics among\ninstitutions, but more by latent characteristics that lead to the holding of\nrelated securities. We develop a graphical model for multivariate financial\ntime series with interest in uncovering the latent positions of nodes in a\nnetwork intended to capture idiosyncratic relationships. We propose a\nhierarchical model consisting of a VAR, a covariance graphical model (CGM) and\na latent position model (LPM). The VAR enables us to extract useful information\non the idiosyncratic components, which are used by the CGM to model the network\nand the LPM uncovers the spatial position of the nodes. We also develop a\nMarkov chain Monte Carlo algorithm that iterates between sampling parameters of\nthe CGM and the LPM, using samples from the latter to update prior information\nfor covariance graph selection. We show empirically that modeling the\nidiosyncratic channel of contagion using our approach can relate latent\ninstitutional features to systemic vulnerabilities prior to a crisis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 06:16:33 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 18:18:07 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Ahelegbey", "Daniel", ""], ["Carvalho", "Luis", ""], ["Kolaczyk", "Eric", ""]]}, {"id": "1712.06905", "submitter": "Kamal Captain", "authors": "Kamal Captain and Manjunath Joshi", "title": "SNR Wall for Cooperative Spectrum Sensing Using Generalized Energy\n  Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive radio (CR) is a promising scheme to improve the spectrum\nutilization. Spectrum sensing (SS) is one of the main tasks of CR. Cooperative\nspectrum sensing (CSS) is used in CR to improve detection capability. Due to\nits simplicity and low complexity, sensing based on energy detection known as\nconventional energy detection (CED) is widely adopted. CED can be generalized\nby changing the squaring operation of the amplitude of received samples by an\narbitrary positive power p which is referred to as the generalized energy\ndetector (GED). The performance of GED degrades when there exists noise\nuncertainty (NU). In this paper, we investigate the performance of CSS by\nconsidering the noise NU when all the secondary users (SUs) employ GED. We\nderive the signal to noise ratio (SNR) wall for CSS for both hard and soft\ndecision combining. All the derived expressions are validated using Monte Carlo\n(MC) simulations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 12:50:23 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Captain", "Kamal", ""], ["Joshi", "Manjunath", ""]]}, {"id": "1712.07319", "submitter": "Yuanjun Gao", "authors": "Yuanjun Gao, Jack Goetz, Rahul Mazumder, Matthew Connelly", "title": "Mining Events with Declassified Diplomatic Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 1973 the State Department has been using electronic records systems to\npreserve classified communications. Recently, approximately 1.9 million of\nthese records from 1973-77 have been made available by the U.S. National\nArchives. While some of these communication streams have periods witnessing an\nacceleration in the rate of transmission; others do not show any notable\npatterns in communication intensity. Given the sheer volume of these\ncommunications -- far greater than what had been available until now --\nscholars need automated statistical techniques to identify the communications\nthat warrant closer study. We develop a statistical framework that can\nsemi-automatically identify from a large corpus of documents a handful that\nhistorians would consider more interesting electronic records. Our approach\nbrings together related but distinct statistical concepts from nonparametric\nsignal estimation and statistical hypothesis testing -- which when put together\nhelp us identify and analyze various geometrical aspects of the communication\nstreams. Dominant periods of heightened and sustained activities aka bursts, as\nidentified through these methods, correspond well with historical events\nrecognized by standard reference works on the 1970s.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 04:39:28 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Gao", "Yuanjun", ""], ["Goetz", "Jack", ""], ["Mazumder", "Rahul", ""], ["Connelly", "Matthew", ""]]}, {"id": "1712.07325", "submitter": "Lingzhou Xue", "authors": "Kevin H. Lee, Lingzhou Xue and David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal\n  Exponential-Family Random Graph Models", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are a general language for describing time-evolving complex\nsystems, and discrete time network models provide an emerging statistical\ntechnique for various applications. It is a fundamental research question to\ndetect the community structure in time-evolving networks. However, due to\nsignificant computational challenges and difficulties in modeling communities\nof time-evolving networks, there is little progress in the current literature\nto effectively find communities in time-evolving networks. In this work, we\npropose a novel model-based clustering framework for time-evolving networks\nbased on discrete time exponential-family random graph models. To choose the\nnumber of communities, we use conditional likelihood to construct an effective\nmodel selection criterion. Furthermore, we propose an efficient variational\nexpectation-maximization (EM) algorithm to find approximate maximum likelihood\nestimates of network parameters and mixing proportions. By using variational\nmethods and minorization-maximization (MM) techniques, our method has appealing\nscalability for large-scale time-evolving networks. The power of our method is\ndemonstrated in simulation studies and empirical applications to international\ntrade networks and the collaboration networks of a large American research\nuniversity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:36:00 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lee", "Kevin H.", ""], ["Xue", "Lingzhou", ""], ["Hunter", "David R.", ""]]}, {"id": "1712.07683", "submitter": "Daniel Gamermann Dr.", "authors": "D. Gamermann, J. Triana, R. Jaime", "title": "A comprehensive statistical study of metabolic and protein-protein\n  interaction network properties", "comments": "13 pages, 4 figures, 9 tables", "journal-ref": null, "doi": "10.1016/j.physa.2019.122204", "report-no": null, "categories": "q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mathematical properties of graphs underling biological\nsystems could give hints on the evolutionary mechanisms behind these\nstructures. In this article we perform a complete statistical analysis over\nthousands of graphs representing metabolic and protein-protein interaction\n(PPI) networks. First, we investigate the quality of fits obtained for the\nnodes degree distributions to power-law functions. This analysis suggests that\na power-law distribution poorly describes the data except for the far right\ntail in the case of PPI networks. Next we obtain descriptive statistics for the\nmain graph parameters and try to identify the properties that deviate from the\nexpected values had the networks been built by randomly linking nodes with the\nsame degree distribution. This survey identifies the properties of biological\nnetworks which are not solely the result of their degree distribution, but\nemerge from yet unidentified mechanisms other than those that drive these\ndistributions. The findings suggest that, while PPI networks have properties\nthat differ from their expected values in their randomized versions with great\nstatistical significance, the differences for metabolic networks have a smaller\nstatistical significance, though it is possible to identify some drift.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 19:38:32 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 17:50:05 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Gamermann", "D.", ""], ["Triana", "J.", ""], ["Jaime", "R.", ""]]}, {"id": "1712.07800", "submitter": "Lingzhou Xue", "authors": "Amal Agarwal and Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks with\n  Application to Water Pollution Analysis", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water pollution is a major global environmental problem, and it poses a great\nenvironmental risk to public health and biological diversity. This work is\nmotivated by assessing the potential environmental threat of coal mining\nthrough increased sulfate concentrations in river networks, which do not belong\nto any simple parametric distribution. However, existing network models mainly\nfocus on binary or discrete networks and weighted networks with known\nparametric weight distributions. We propose a principled nonparametric weighted\nnetwork model based on exponential-family random graph models and local\nlikelihood estimation and study its model-based clustering with application to\nlarge-scale water pollution network analysis. We do not require any parametric\ndistribution assumption on network weights. The proposed method greatly extends\nthe methodology and applicability of statistical network models. Furthermore,\nit is scalable to large and complex networks in large-scale environmental\nstudies. The power of our proposed methods is demonstrated in simulation\nstudies and a real application to sulfate pollution network analysis in Ohio\nwatershed located in Pennsylvania, United States.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:36:10 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 03:31:53 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Agarwal", "Amal", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1712.07809", "submitter": "Jin Wang", "authors": "Jin Wang, Javier Cabrera, Kwok-Leung Tsui, Hainan Guo, Monique Bakker,\n  John B. Kostis", "title": "Predicting Surgery Duration from a New Perspective: Evaluation from a\n  Database on Thoracic Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND: Clinical factors influence surgery duration. This study also\ninvestigated non-clinical effects. METHODS: 22 months of data about thoracic\noperations in a large hospital in China were reviewed. Linear and nonlinear\nregression models were used to predict the duration of the operations.\nInteractions among predictors were also considered. RESULTS: Surgery duration\ndecreased with the number of operations a surgeon performed in a day (P<0.001).\nAlso, it was found that surgery duration decreased with the number of\noperations allocated to an OR as long as there were no more than four surgeries\nper day in the OR (P<0.001), but increased with the number of operations if it\nwas more than four (P<0.01). The duration of surgery was affected by its\nposition in a sequence of surgeries performed by a surgeon. In addition,\nsurgeons exhibited different patterns of the effects of surgery type for\nsurgeries in different positions in the day. CONCLUSIONS: Surgery duration was\naffected not only by clinical effects but also some non-clinical effects.\nScheduling and allocation decisions significantly influenced surgery duration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 06:31:03 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Wang", "Jin", ""], ["Cabrera", "Javier", ""], ["Tsui", "Kwok-Leung", ""], ["Guo", "Hainan", ""], ["Bakker", "Monique", ""], ["Kostis", "John B.", ""]]}, {"id": "1712.07879", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Aky{\\i}ld{\\i}z", "title": "A probabilistic interpretation of replicator-mutator dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we investigate the relationship between probabilistic updating\nmechanisms and discrete-time replicator-mutator dynamics. We consider the\nrecently shown connection between Bayesian updating and replicator dynamics and\nextend it to the replicator-mutator dynamics by considering prediction and\nfiltering recursions in hidden Markov models (HMM). We show that it is possible\nto understand the evolution of the frequency vector of a population under the\nreplicator-mutator equation as a posterior predictive inference procedure in an\nHMM. This view enables us to derive a natural dual version of the\nreplicator-mutator equation, which corresponds to updating the filtering\ndistribution. Finally, we conclude with the implications of the interpretation\nand with some comments related to the recent discussions about evolution and\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 11:15:25 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Aky\u0131ld\u0131z", "\u00d6mer Deniz", ""]]}, {"id": "1712.07968", "submitter": "Hau-tieng Wu", "authors": "Hau-tieng Wu, Yi-Wen Liu", "title": "Analyzing transient-evoked otoacoustic emissions by concentration of\n  frequency and time", "comments": null, "journal-ref": null, "doi": "10.1121/1.5047749", "report-no": null, "categories": "eess.SP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear part of transient evoked (TE) otoacoustic emission (OAE) is\nthought to be generated via coherent reflection near the characteristic place\nof constituent wave components. Because of the tonotopic organization of the\ncochlea, high frequency emissions return earlier than low frequencies; however,\ndue to the random nature of coherent reflection, the instantaneous frequency\n(IF) and amplitude envelope of TEOAEs both fluctuate. Multiple reflection\ncomponents and synchronized spontaneous emissions can further make it difficult\nto extract the IF by linear transforms. In this paper, we propose to model\nTEOAEs as a sum of {\\em intrinsic mode-type functions} and analyze it by a\n{nonlinear-type time-frequency analysis} technique called concentration of\nfrequency and time (ConceFT). When tested with synthetic OAE signals {with\npossibly multiple oscillatory components}, the present method is able to\nproduce clearly visualized traces of individual components on the\ntime-frequency plane. Further, when the signal is noisy, the proposed method is\ncompared with existing linear and bilinear methods in its accuracy for\nestimating the fluctuating IF. Results suggest that ConceFT outperforms the\nbest of these methods in terms of optimal transport distance, reducing the\nerror by 10 to {21\\%} when the signal to noise ratio is 10 dB or below.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 12:30:14 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 10:45:20 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Wu", "Hau-tieng", ""], ["Liu", "Yi-Wen", ""]]}, {"id": "1712.08101", "submitter": "Sebastiaan H\\\"oppner", "authors": "Sebastiaan H\\\"oppner, Eugen Stripling, Bart Baesens, Seppe vanden\n  Broucke and Tim Verdonck", "title": "Profit Driven Decision Trees for Churn Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer retention campaigns increasingly rely on predictive models to detect\npotential churners in a vast customer base. From the perspective of machine\nlearning, the task of predicting customer churn can be presented as a binary\nclassification problem. Using data on historic behavior, classification\nalgorithms are built with the purpose of accurately predicting the probability\nof a customer defecting. The predictive churn models are then commonly selected\nbased on accuracy related performance measures such as the area under the ROC\ncurve (AUC). However, these models are often not well aligned with the core\nbusiness requirement of profit maximization, in the sense that, the models fail\nto take into account not only misclassification costs, but also the benefits\noriginating from a correct classification. Therefore, the aim is to construct\nchurn prediction models that are profitable and preferably interpretable too.\nThe recently developed expected maximum profit measure for customer churn\n(EMPC) has been proposed in order to select the most profitable churn model. We\npresent a new classifier that integrates the EMPC metric directly into the\nmodel construction. Our technique, called ProfTree, uses an evolutionary\nalgorithm for learning profit driven decision trees. In a benchmark study with\nreal-life data sets from various telecommunication service providers, we show\nthat ProfTree achieves significant profit improvements compared to classic\naccuracy driven tree-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:31:58 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["H\u00f6ppner", "Sebastiaan", ""], ["Stripling", "Eugen", ""], ["Baesens", "Bart", ""], ["Broucke", "Seppe vanden", ""], ["Verdonck", "Tim", ""]]}, {"id": "1712.08211", "submitter": "Eric Tramel", "authors": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy,\n  Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of interactions between treatment effects and patient descriptors\nin clinical trials is critical for optimizing the drug development process. The\nincreasing volume of data accumulated in clinical trials provides a unique\nopportunity to discover new biomarkers and further the goal of personalized\nmedicine, but it also requires innovative robust biomarker detection methods\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\nof novel univariate statistical tests, based on the theory of random walks,\nwhich are able to capture non-linear and non-monotonic covariate-treatment\ninteractions. We also propose a novel combined test, which leverages the power\nof all of our proposed univariate tests into a single general-case tool. We\npresent results for both synthetic trials as well as real-world clinical\ntrials, where we compare our method with state-of-the-art techniques and\ndemonstrate the utility and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 21:09:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Goujaud", "Baptiste", ""], ["Tramel", "Eric W.", ""], ["Courtiol", "Pierre", ""], ["Zaslavskiy", "Mikhail", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1712.08235", "submitter": "Nigul Olspert", "authors": "N. Olspert, J. Pelt, M. J. K\\\"apyl\\\"a, J. Lehtinen", "title": "Estimating activity cycles with probabilistic methods I. Bayesian\n  Generalised Lomb-Scargle Periodogram with Trend", "comments": null, "journal-ref": "A&A 615, A111 (2018)", "doi": "10.1051/0004-6361/201732524", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Period estimation is one of the central topics in astronomical time series\nanalysis, where data is often unevenly sampled. Especially challenging are\nstudies of stellar magnetic cycles, as there the periods looked for are of the\norder of the same length than the datasets themselves. The datasets often\ncontain trends, the origin of which is either a real long-term cycle or an\ninstrumental effect, but these effects cannot be reliably separated, while they\ncan lead to erroneous period determinations if not properly handled. In this\nstudy we aim at developing a method that can handle the trends properly, and by\nperforming extensive set of testing, we show that this is the optimal procedure\nwhen contrasted with methods that do not include the trend directly to the\nmodel. The effect of the form of the noise (whether constant or\nheteroscedastic) on the results is also investigated. We introduce a Bayesian\nGeneralised Lomb-Scargle Periodogram with Trend (BGLST), which is a\nprobabilistic linear regression model using Gaussian priors for the\ncoefficients and uniform prior for the frequency parameter. We show, using\nsynthetic data, that when there is no prior information on whether and to what\nextent the true model of the data contains a linear trend, the introduced BGLST\nmethod is preferable to the methods which either detrend the data or leave the\ndata untrended before fitting the periodic model. Whether to use noise with\ndifferent than constant variance in the model depends on the density of the\ndata sampling as well as on the true noise type of the process.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:16:14 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 13:12:16 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Olspert", "N.", ""], ["Pelt", "J.", ""], ["K\u00e4pyl\u00e4", "M. J.", ""], ["Lehtinen", "J.", ""]]}, {"id": "1712.08238", "submitter": "Karthik Dinakar", "authors": "Chelsea Barabas, Karthik Dinakar, Joichi Ito, Madars Virza, Jonathan\n  Zittrain", "title": "Interventions over Predictions: Reframing the Ethical Debate for\n  Actuarial Risk Assessment", "comments": "Accepted paper (not camera-ready version) of FATML 2018 conference,\n  Fairness, Accountability and Transparency in Machine Learning, 2018,\n  Proceedings of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actuarial risk assessments might be unduly perceived as a neutral way to\ncounteract implicit bias and increase the fairness of decisions made at almost\nevery juncture of the criminal justice system, from pretrial release to\nsentencing, parole and probation. In recent times these assessments have come\nunder increased scrutiny, as critics claim that the statistical techniques\nunderlying them might reproduce existing patterns of discrimination and\nhistorical biases that are reflected in the data. Much of this debate is\ncentered around competing notions of fairness and predictive accuracy, resting\non the contested use of variables that act as \"proxies\" for characteristics\nlegally protected against discrimination, such as race and gender. We argue\nthat a core ethical debate surrounding the use of regression in risk\nassessments is not simply one of bias or accuracy. Rather, it's one of purpose.\nIf machine learning is operationalized merely in the service of predicting\nindividual future crime, then it becomes difficult to break cycles of\ncriminalization that are driven by the iatrogenic effects of the criminal\njustice system itself. We posit that machine learning should not be used for\nprediction, but rather to surface covariates that are fed into a causal model\nfor understanding the social, structural and psychological drivers of crime. We\npropose an alternative application of machine learning and causal inference\naway from predicting risk scores to risk mitigation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:27:39 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 20:52:15 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Barabas", "Chelsea", ""], ["Dinakar", "Karthik", ""], ["Ito", "Joichi", ""], ["Virza", "Madars", ""], ["Zittrain", "Jonathan", ""]]}, {"id": "1712.08240", "submitter": "Nigul Olspert", "authors": "N. Olspert, J. Lehtinen, M. J. K\\\"apyl\\\"a, J. Pelt, A. Grigorievskiy", "title": "Estimating activity cycles with probabilistic methods II. The Mount\n  Wilson Ca H&K data", "comments": null, "journal-ref": null, "doi": "10.1051/0004-6361/201732525", "report-no": null, "categories": "astro-ph.SR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debate over the existence of branches in the stellar activity-rotation\ndiagrams continues. Application of modern time series analysis tools to study\nthe mean cycle periods in chromospheric activity index is lacking. We develop\nsuch models, based on Gaussian processes, for one-dimensional time series and\napply it to the extended Mount Wilson Ca H&K sample. Our main aim is to study\nhow the previously commonly used assumption of strict harmonicity of the\nstellar cycles as well as handling of the linear trends affects the results. We\nintroduce three methods of different complexity, starting with the simple\nBayesian harmonic model and followed by Gaussian Process models with periodic\nand quasi-periodic covariance functions. We confirm the existence of two\npopulations in the activity-period diagram. We find only one significant trend\nin the inactive population, namely that the cycle periods get shorter with\nincreasing rotation. This is in contrast with earlier studies, that postulate\nthe existence of trends in both of the populations. In terms of rotation to\ncycle period ratio, our data is consistent with only two activity branches such\nthat the active branch merges together with the transitional one. The retrieved\nstellar cycles are uniformly distributed over the R'HK activity index,\nindicating that the operation of stellar large-scale dynamos carries smoothly\nover the Vaughan-Preston gap. At around the solar activity index, however,\nindications of a disruption in the cyclic dynamo action are seen. Our study\nshows that stellar cycle estimates depend significantly on the model applied.\nSuch model-dependent aspects include the improper treatment of linear trends,\nwhile the assumption of strict harmonicity can result in the appearance of\ndouble cyclicities that seem more likely to be explained by the\nquasi-periodicity of the cycles.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 22:40:45 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 08:20:50 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 07:25:26 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 06:53:50 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Olspert", "N.", ""], ["Lehtinen", "J.", ""], ["K\u00e4pyl\u00e4", "M. J.", ""], ["Pelt", "J.", ""], ["Grigorievskiy", "A.", ""]]}, {"id": "1712.08243", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Maryan Morel, Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Agathe Guilloux,\n  Fanny Leroy", "title": "ConvSCCS: convolutional self-controlled case series model for lagged\n  adverse event detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of large databases of electronic health\nrecords (EHRs) comes the chance of enhancing health risks screening. Most\npost-marketing detections of adverse drug reaction (ADR) rely on physicians'\nspontaneous reports, leading to under reporting. To take up this challenge, we\ndevelop a scalable model to estimate the effect of multiple longitudinal\nfeatures (drug exposures) on a rare longitudinal outcome. Our procedure is\nbased on a conditional Poisson model also known as self-controlled case series\n(SCCS). We model the intensity of outcomes using a convolution between\nexposures and step functions, that are penalized using a combination of\ngroup-Lasso and total-variation. This approach does not require the\nspecification of precise risk periods, and allows to study in the same model\nseveral exposures at the same time. We illustrate the fact that this approach\nimproves the state-of-the-art for the estimation of the relative risks both on\nsimulations and on a cohort of diabetic patients, extracted from the large\nFrench national health insurance database (SNIIRAM), a SQL database built\naround medical reimbursements of more than 65 million people. This work has\nbeen done in the context of a research partnership between Ecole Polytechnique\nand CNAMTS (in charge of SNIIRAM).\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 23:06:22 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 23:27:15 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Morel", "Maryan", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Guilloux", "Agathe", ""], ["Leroy", "Fanny", ""]]}, {"id": "1712.08329", "submitter": "Antoine Lejay", "authors": "Antoine Lejay (TOSCA, IECL), Paolo Pigato (WIAS)", "title": "A threshold model for local volatility: evidence of leverage and mean\n  reversion effects on historical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In financial markets, low prices are generally associated with high\nvolatilities and vice-versa, this well known stylized fact usually being\nreferred to as leverage effect. We propose a local volatility model, given by a\nstochastic differential equation with piecewise constant coefficients, which\naccounts of leverage and mean-reversion effects in the dynamics of the prices.\nThis model exhibits a regime switch in the dynamics accordingly to a certain\nthreshold. It can be seen as a continuous-time version of the Self-Exciting\nThreshold Autoregressive (SETAR) model. We propose an estimation procedure for\nthe volatility and drift coefficients as well as for the threshold level.\nParameters estimated on the daily prices of 348 stocks of NYSE and S\\&P 500, on\ndifferent time windows, show consistent empirical evidence for leverageeffects.\nMean-reversion effects are also detected, most markedly in crisis periods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 07:42:13 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 13:08:17 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 06:32:15 GMT"}, {"version": "v4", "created": "Fri, 22 Feb 2019 11:12:52 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Lejay", "Antoine", "", "TOSCA, IECL"], ["Pigato", "Paolo", "", "WIAS"]]}, {"id": "1712.08837", "submitter": "Ze Jin", "authors": "Ze Jin, Benjamin B. Risk, David S. Matteson", "title": "Optimization and Testing in Linear Non-Gaussian Component Analysis", "comments": "33 pages, 3 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:37:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 20:23:30 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""]]}, {"id": "1712.08871", "submitter": "Xing He", "authors": "Fan Yang, Xing He, Robert Caiming Qiu, Zenan Ling", "title": "A Data-driven Approach to Multi-event Analytics in Large-scale Power\n  Systems Using Factor Model", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-event detection and recognition in real time is of challenge for a\nmodern grid as its feature is usually non-identifiable. Based on factor model,\nthis paper porposes a data-driven method as an alternative solution under the\nframework of random matrix theory. This method maps the raw data into a\nhigh-dimensional space with two parts: 1) the principal components (factors,\nmapping event signals); and 2) time series residuals (bulk, mapping\nwhite/non-Gaussian noises). The spatial information is extracted form factors,\nand the termporal infromation from residuals. Taking both spatial-tempral\ncorrelation into account, this method is able to reveal the multi-event: its\ncomponents and their respective details, e.g., occurring time. Case studies\nbased on the standard IEEE 118-bus system validate the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 03:22:24 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Yang", "Fan", ""], ["He", "Xing", ""], ["Qiu", "Robert Caiming", ""], ["Ling", "Zenan", ""]]}, {"id": "1712.08883", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, Changshui Zhang, Yi Zhang", "title": "Traffic Flow Forecasting Using a Spatio-Temporal Bayesian Network\n  Predictor", "comments": null, "journal-ref": "The 15th International Conference on Artificial Neural Networks\n  (ICANN), 2005, pp. 273-278", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel predictor for traffic flow forecasting, namely spatio-temporal\nBayesian network predictor, is proposed. Unlike existing methods, our approach\nincorporates all the spatial and temporal information available in a\ntransportation network to carry our traffic flow forecasting of the current\nsite. The Pearson correlation coefficient is adopted to rank the input\nvariables (traffic flows) for prediction, and the best-first strategy is\nemployed to select a subset as the cause nodes of a Bayesian network. Given the\nderived cause nodes and the corresponding effect node in the spatio-temporal\nBayesian network, a Gaussian Mixture Model is applied to describe the\nstatistical relationship between the input and output. Finally, traffic flow\nforecasting is performed under the criterion of Minimum Mean Square Error\n(M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing\ndemonstrate the effectiveness of our presented spatio-temporal Bayesian network\npredictor.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 07:29:49 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Sun", "Shiliang", ""], ["Zhang", "Changshui", ""], ["Zhang", "Yi", ""]]}, {"id": "1712.08894", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Ben Placek, Daniel Angerhausen, Jennifer L. Carter,\n  Bryan D'Angelo, Anthony D. Gai, Bertrand Carado", "title": "EXONEST: The Bayesian Exoplanetary Explorer", "comments": "30 pages, 8 figures, 5 tables. Presented at the 37th International\n  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and\n  Engineering (MaxEnt 2017) in Jarinu/SP Brasil", "journal-ref": "Entropy, 19(10), 559, 2017. doi: 10.3390/e19100559", "doi": "10.3390/e19100559", "report-no": null, "categories": "astro-ph.EP astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fields of astronomy and astrophysics are currently engaged in an\nunprecedented era of discovery as recent missions have revealed thousands of\nexoplanets orbiting other stars. While the Kepler Space Telescope mission has\nenabled most of these exoplanets to be detected by identifying transiting\nevents, exoplanets often exhibit additional photometric effects that can be\nused to improve the characterization of exoplanets. The EXONEST Exoplanetary\nExplorer is a Bayesian exoplanet inference engine based on nested sampling and\noriginally designed to analyze archived Kepler Space Telescope and CoRoT\n(Convection Rotation et Transits plan\\'etaires) exoplanet mission data. We\ndiscuss the EXONEST software package and describe how it accommodates\nplug-and-play models of exoplanet-associated photometric effects for the\npurpose of exoplanet detection, characterization and scientific hypothesis\ntesting. The current suite of models allows for both circular and eccentric\norbits in conjunction with photometric effects, such as the primary transit and\nsecondary eclipse, reflected light, thermal emissions, ellipsoidal variations,\nDoppler beaming and superrotation. We discuss our new efforts to expand the\ncapabilities of the software to include more subtle photometric effects\ninvolving reflected and refracted light. We discuss the EXONEST inference\nengine design and introduce our plans to port the current MATLAB-based EXONEST\nsoftware package over to the next generation Exoplanetary Explorer, which will\nbe a Python-based open source project with the capability to employ third-party\nplug-and-play models of exoplanet-related photometric effects.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 09:21:27 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Placek", "Ben", ""], ["Angerhausen", "Daniel", ""], ["Carter", "Jennifer L.", ""], ["D'Angelo", "Bryan", ""], ["Gai", "Anthony D.", ""], ["Carado", "Bertrand", ""]]}, {"id": "1712.09149", "submitter": "Miles Ott", "authors": "Miles Q. Ott, Krista J. Gile, Matthew T. Harrison, Lisa G. Johnston,\n  Joseph W. Hogan", "title": "Reduced Bias for respondent driven sampling: accounting for non-uniform\n  edge sampling probabilities in people who inject drugs in Mauritius", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People who inject drugs are an important population to study in order to\nreduce transmission of blood-borne illnesses including HIV and Hepatitis. In\nthis paper we estimate the HIV and Hepatitis C prevalence among people who\ninject drugs, as well as the proportion of people who inject drugs who are\nfemale in Mauritius. Respondent driven sampling (RDS), a widely adopted\nlink-tracing sampling design used to collect samples from hard-to-reach human\npopulations, was used to collect this sample. The random walk approximation\nunderlying many common RDS estimators assumes that each social relation (edge)\nin the underlying social network has an equal probability of being traced in\nthe collection of the sample. This assumption does not hold in practice. We\nshow that certain RDS estimators are sensitive to the violation of this\nassumption. In order to address this limitation in current methodology, and the\nimpact it may have on prevalence estimates, we present a new method for\nimproving RDS prevalence estimators using estimated edge inclusion\nprobabilities, and apply this to data from Mauritius.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 00:36:23 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ott", "Miles Q.", ""], ["Gile", "Krista J.", ""], ["Harrison", "Matthew T.", ""], ["Johnston", "Lisa G.", ""], ["Hogan", "Joseph W.", ""]]}, {"id": "1712.09210", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas, Anneta Voutsela, and Stelios Katsanevakis", "title": "Sampling alien species inside and outside protected areas: does it\n  matter?", "comments": "accepted journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of alien species presences are generally more readily available in\nprotected than non-protected areas due to higher sampling efforts inside\nprotected areas. Are the results and conclusions based on analyses of data\ncollected in protected areas representative of wider non-protected regions? We\naddress this question by analysing some recently published data of alien plants\nin Greece. Mixed effects models were used with alien species presences in 8.25\nx 8.25 km cells as dependent variable and the percentage of protected area, as\nwell as the agricultural and artificial land cover types richness (as\nindicators of human presence) as independent variables. In addition, the\nspatial cross-correlation between the percentage of protected area and alien\nspecies richness was examined across scales. Results indicated that the\npercentage of protected area per cell is a poor predictor of alien species\nrichness. Spatial analysis indicated that cells with higher percentage of\nprotected areas have slightly less alien species than cells with lower\npercentage of protected areas. This result is likely to be driven by the\noverall negative correlation between habitat protection and anthropogenic\nactivities. Thus, the conclusions deduced by data deriving from protected areas\nare likely to hold true for patterns of alien species in non-protected areas\nwhen the human pressures are accounted for.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 08:48:30 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Moustakas", "Aristides", ""], ["Voutsela", "Anneta", ""], ["Katsanevakis", "Stelios", ""]]}, {"id": "1712.09694", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "On the estimation of correlation in a binary sequence model", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary sequence generated by thresholding a hidden continuous\nsequence. The hidden variables are assumed to have a compound symmetry\ncovariance structure with a single parameter characterizing the common\ncorrelation. We study the parameter estimation problem under such one-parameter\nmodels. We demonstrate that maximizing the likelihood function does not yield\nconsistent estimates for the correlation. We then formally prove the\nnonestimability of the parameter by deriving a non-vanishing minimax lower\nbound. This counter-intuitive phenomenon provides an interesting insight that\none-bit information of each latent variable is not sufficient to consistently\nrecover their common correlation. On the other hand, we further show that\ntrinary data generated from the hidden variables can consistently estimate the\ncorrelation with parametric convergence rate. Thus we reveal a phase transition\nphenomenon regarding the discretization of latent continuous variables while\npreserving the estimability of the correlation. Numerical experiments are\nperformed to validate the conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 22:19:19 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1712.09816", "submitter": "Marco Oesting", "authors": "Sebastian Engelke and Raphael de Fondeville and Marco Oesting", "title": "Extremal Behavior of Aggregated Data with an Application to Downscaling", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of spatially aggregated data from a stochastic process $X$\nmay exhibit a different tail behavior than its marginal distributions. For a\nlarge class of aggregating functionals $\\ell$ we introduce the $\\ell$-extremal\ncoefficient that quantifies this difference as a function of the extremal\nspatial dependence in $X$. We also obtain the joint extremal dependence for\nmultiple aggregation functionals applied to the same process. Explicit formulas\nfor the $\\ell$-extremal coefficients and multivariate dependence structures are\nderived in important special cases. The results provide a theoretical link\nbetween the extremal distribution of the aggregated data and the corresponding\nunderlying process, which we exploit to develop a method for statistical\ndownscaling. We apply our framework to downscale daily temperature maxima in\nthe south of France from a gridded data set and use our model to generate high\nresolution maps of the warmest day during the 2003 heatwave.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:40:00 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Engelke", "Sebastian", ""], ["de Fondeville", "Raphael", ""], ["Oesting", "Marco", ""]]}, {"id": "1712.09853", "submitter": "Ying Zhu", "authors": "Ying Zhu, Tom Fearn, D.Wayne Chicken, Martin R. Austwick, Santosh K.\n  Somasundaram, Charles A. Mosse, Benjamin Clark, Irving J. Bigio, Mohammed\n  R.S. Keshtgar, Stephen G. Bown", "title": "A Partially Supervised Bayesian Image Classification Model with\n  Applications in Diagnosis of Sentinel Lymph Node Metastases in Breast Cancer", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A method has been developed for the analysis of images of sentinel lymph\nnodes generated by a spectral scanning device. The aim is to classify the\nnodes, excised during surgery for breast cancer, as normal or metastatic. The\ndata from one node constitute spectra at 86 wavelengths for each pixel of a\n20*20 grid. For the analysis, the spectra are reduced to scores on two factors,\none derived externally from a linear discriminant analysis using spectra taken\nmanually from known normal and metastatic tissue, and one derived from the node\nunder investigation to capture variability orthogonal to the external factor.\nThen a three-group mixture model (normal, metastatic, non-nodal background)\nusing multivariate t distributions is fitted to the scores, with external data\nbeing used to specify informative prior distributions for the parameters of the\nthree distributions. A Markov random field prior imposes smoothness on the\nimage generated by the model. Finally, the node is classified as metastatic if\nany one pixel in this smoothed image is classified as metastatic. The model\nparameters were tuned on a training set of nodes, and then the tuned model was\ntested on a separate validation set of nodes, achieving satisfactory\nsensitivity and specificity. The aim in developing the analysis was to allow\nflexibility in the way each node is modelled whilst still using external\ninformation. The Bayesian framework employed is ideal for this.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 13:31:47 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zhu", "Ying", ""], ["Fearn", "Tom", ""], ["Chicken", "D. Wayne", ""], ["Austwick", "Martin R.", ""], ["Somasundaram", "Santosh K.", ""], ["Mosse", "Charles A.", ""], ["Clark", "Benjamin", ""], ["Bigio", "Irving J.", ""], ["Keshtgar", "Mohammed R. S.", ""], ["Bown", "Stephen G.", ""]]}, {"id": "1712.10035", "submitter": "Soumen Dey", "authors": "Soumen Dey, Mohan Delampady, K. Ullas Karanth and Arjun M. Gopalaswamy", "title": "A spatially explicit capture recapture model for partially identified\n  individuals when trap detection rate is less than one", "comments": "This draft has been submitted to a journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially explicit capture recapture (SECR) models have gained enormous\npopularity to solve abundance estimation problems in ecology. In this study, we\ndevelop a novel Bayesian SECR model that disentangles the process of animal\nmovement through a detector from the process of recording data by a detector in\nthe face of imperfect detection. We integrate this complexity into an advanced\nversion of a recent SECR model involving partially identified individuals\n(Royle, 2015). We assess the performance of our model over a range of realistic\nsimulation scenarios and demonstrate that estimates of population size $N$\nimprove when we utilize the proposed model relative to the model that does not\nexplicitly estimate trap detection probability (Royle, 2015). We confront and\ninvestigate the proposed model with a spatial capture-recapture data set from a\ncamera trapping survey on tigers (\\textit{Panthera tigris}) in Nagarahole,\nsouthern India. Trap detection probability is estimated at 0.489 and therefore\njustifies the necessity to utilize our model in field situations. We discuss\npossible extensions, future work and relevance of our model to other\nstatistical applications beyond ecology.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 19:36:02 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Dey", "Soumen", ""], ["Delampady", "Mohan", ""], ["Karanth", "K. Ullas", ""], ["Gopalaswamy", "Arjun M.", ""]]}, {"id": "1712.10284", "submitter": "Dhaval Adjodah", "authors": "Dhaval Adjodah, Shi Kai Chong, Yan Leng, Peter Krafft, Alex Pentland", "title": "Large-Scale Experiment on the Importance of Social Learning and\n  Unimodality in the Wisdom of the Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we build on previous research to understand the conditions\nwithin which the Wisdom of the Crowd (WoC) improves or worsens as a result of\nshowing individuals the predictions of their peers. Our main novel\ncontributions are: 1) a dataset of unprecedented size and detail; 2) we observe\nthe novel effect of the importance of the unimodality of the social information\nshown to individuals: if one does not see only one clear peak in the\ndistribution of the crowd's predictions, the WoC is worsened after social\nexposure; and 3) we estimate social learning weights that we use to show that\nthere exists individuals who are much better at learning from the crowd and can\nbe filtered to improve collective accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 17:22:15 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Adjodah", "Dhaval", ""], ["Chong", "Shi Kai", ""], ["Leng", "Yan", ""], ["Krafft", "Peter", ""], ["Pentland", "Alex", ""]]}]