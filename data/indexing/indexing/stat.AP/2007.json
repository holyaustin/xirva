[{"id": "2007.00137", "submitter": "Ioannis Papastathopoulos", "authors": "Justin A. Kasin and Ioannis Papastathopoulos", "title": "A spatial Poisson hurdle model with application to wildfires", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling wildfire occurrences is important for disaster management including\nprevention, detection and suppression of large catastrophic events. We present\na spatial Poisson hurdle model for exploring geographical variation of monthly\ncounts of wildfire occurrences and apply it to Indonesia and Australia. The\nmodel includes two a priori independent spatially structured latent effects\nthat account for residual spatial variation in the probability of wildfire\noccurrence, and the positive count rate given an occurrence. Inference is\nprovided by empirical Bayes using the Laplace approximation to the marginal\nposterior which provides fast inference for latent Gaussian models with sparse\nstructures. In both cases, our model matched several empirically known facts\nabout wildfires. We conclude that elevation, percentage tree cover, relative\nhumidity, surface temperature, and the interaction between humidity and\ntemperature to be important predictors of monthly counts of wildfire\noccurrences. Further, our findings show opposing effects for surface\ntemperature and its interaction with relative humidity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 22:31:37 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kasin", "Justin A.", ""], ["Papastathopoulos", "Ioannis", ""]]}, {"id": "2007.00161", "submitter": "Ransalu Senanayake", "authors": "Ransalu Senanayake, Maneekwan Toyungyernsub, Mingyu Wang, Mykel J.\n  Kochenderfer, and Mac Schwager", "title": "Directional Primitives for Uncertainty-Aware Motion Estimation in Urban\n  Environments", "comments": "The 23rd IEEE International Conference on Intelligent Transportation\n  Systems. September, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can use driving data collected over a long period of time to extract rich\ninformation about how vehicles behave in different areas of the roads. In this\npaper, we introduce the concept of directional primitives, which is a\nrepresentation of prior information of road networks. Specifically, we\nrepresent the uncertainty of directions using a mixture of von Mises\ndistributions and associated speeds using gamma distributions. These\nlocation-dependent primitives can be combined with motion information of\nsurrounding vehicles to predict their future behavior in the form of\nprobability distributions. Experiments conducted on highways, intersections,\nand roundabouts in the Carla simulator, as well as real-world urban driving\ndatasets, indicate that primitives lead to better uncertainty-aware motion\nestimation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 00:22:31 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Senanayake", "Ransalu", ""], ["Toyungyernsub", "Maneekwan", ""], ["Wang", "Mingyu", ""], ["Kochenderfer", "Mykel J.", ""], ["Schwager", "Mac", ""]]}, {"id": "2007.00180", "submitter": "Hamed Nikbakht", "authors": "Konstantinos G. Papakonstantinou and Hamed Nikbakht", "title": "Hamiltonian MCMC methods for estimating rare events probabilities in\n  high-dimensional problems", "comments": "arXiv admin note: text overlap with arXiv:1909.03575", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient estimation of rare events probabilities is of\nsignificant importance, since often the occurrences of such events have\nwidespread impacts. The focus in this work is on precisely quantifying these\nprobabilities, often encountered in reliability analysis of complex engineering\nsystems, based on an introduced framework termed Approximate Sampling Target\nwith Post-processing Adjustment (ASTPA), which herein is integrated with and\nsupported by gradient-based Hamiltonian Markov Chain Monte Carlo (HMCMC)\nmethods. The basic idea is to construct a relevant target distribution by\nweighting the high-dimensional random variable space through a one-dimensional\noutput likelihood model, using the limit-state function. To sample from this\ntarget distribution, we exploit HMCMC algorithms, a family of MCMC methods that\nadopts physical system dynamics, rather than solely using a proposal\nprobability distribution, to generate distant sequential samples, and we\ndevelop a new Quasi-Newton mass preconditioned HMCMC scheme (QNp-HMCMC), which\nis particularly efficient and suitable for high-dimensional spaces. To\neventually compute the rare event probability, an original post-sampling step\nis devised using an inverse importance sampling procedure based on the already\nobtained samples. The statistical properties of the estimator are analyzed as\nwell, and the performance of the proposed methodology is examined in detail and\ncompared against Subset Simulation in a series of challenging low- and\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:56:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Papakonstantinou", "Konstantinos G.", ""], ["Nikbakht", "Hamed", ""]]}, {"id": "2007.00296", "submitter": "Sothea Has", "authors": "Sothea Has (LPSM)", "title": "A Kernel-based Consensual Aggregation for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a kernel-based consensual aggregation method\nfor regression problems. We aim to flexibly combine individual regression\nestimators $r_1, r_2, \\ldots, r_M$ using a weighted average where the weights\nare defined based on some kernel function to build a target prediction. This\nwork extends the context of Biau et al. (2016) to a more general kernel-based\nframework. We show that this more general configuration also inherits the\nconsistency of the basic consistent estimators. Moreover, an optimization\nmethod based on gradient descent algorithm is proposed to efficiently and\nrapidly estimate the key parameter of the strategy. The numerical experiments\ncarried out on several simulated and real datasets are also provided to\nillustrate the speed-up of gradient descent algorithm in estimating the key\nparameter and the improvement of overall performance of the method with the\nintroduction of smoother kernel functions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:43:30 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 07:59:44 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 08:54:23 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 09:02:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Has", "Sothea", "", "LPSM"]]}, {"id": "2007.00437", "submitter": "Fengqing Chao Dr.", "authors": "Fengqing Chao, Samir KC, Hernando Ombao", "title": "Levels and trends in the sex ratio at birth in seven provinces of Nepal\n  between 1980 and 2016 with probabilistic projections to 2050: a Bayesian\n  modeling approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sex ratio at birth (SRB; ratio of male to female births) in Nepal has\nbeen reported without imbalance on the national level. However, the national\nSRB could mask the disparity within the country. Given the demographic and\ncultural heterogeneities in Nepal, it is crucial to model Nepal SRB on the\nsubnational level. Prior studies on subnational SRB in Nepal are mostly based\non reporting observed values from surveys and census, and no study has provided\nprobabilistic projections. We aim to estimate and project SRB for the seven\nprovinces of Nepal from 1980 to 2050 using a Bayesian modeling approach. We\ncompiled an extensive database on provincial SRB of Nepal, consisting 2001,\n2006, 2011, and 2016 Nepal Demographic and Health Surveys and 2011 Census. We\nadopted a Bayesian hierarchical time series model to estimate and project the\nprovincial SRB, with a focus on modelling the potential SRB imbalance. In 2016,\nthe highest SRB is estimated in Province 5 at 1.102 with a 95% credible\ninterval (1.044, 1.127) and the lowest SRB is in Province 2 at 1.053 (1.035,\n1.109). The SRB imbalance probabilities in all provinces are generally low and\nvary from 16% in Province 2 to 81% in Province 5. SRB imbalances are estimated\nto have begun at the earliest in 2001 in Province 5 with a 95% credible\ninterval (1992, 2022) and the latest in 2017 (1998, 2040) in Province 2. We\nproject SRB in all provinces to begin converging back to the national baseline\nin the mid-2030s. Our findings imply that the majority of provinces in Nepal\nhave low risks of SRB imbalance for the period 1980-2016. However, we identify\na few provinces with higher probabilities of having SRB inflation. The\nprojected SRB is an important illustration of potential future prenatal sex\ndiscrimination and shows the need to monitor SRB in provinces with higher\npossibilities of SRB imbalance.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:40:38 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 12:40:59 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chao", "Fengqing", ""], ["KC", "Samir", ""], ["Ombao", "Hernando", ""]]}, {"id": "2007.00480", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Shamsuddin Ladha, Shailesh Deshpande, Edward Curry", "title": "Computational Model for Urban Growth Using Socioeconomic Latent\n  Parameters", "comments": "12 pages", "journal-ref": "ECML PKDD 2018 Lecture Notes in Computer Science vol 11329\n  Springer Cham", "doi": "10.1007/978-3-030-13453-2_6", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land use land cover changes (LULCC) are generally modeled using multi-scale\nspatio-temporal variables. Recently, Markov Chain (MC) has been used to model\nLULCC. However, the model is derived from the proportion of LULCC observed over\na given period and it does not account for temporal factors such as\nmacro-economic, socio-economic, etc. In this paper, we present a richer model\nbased on Hidden Markov Model (HMM), grounded in the common knowledge that\neconomic, social and LULCC processes are tightly coupled. We propose a HMM\nwhere LULCC classes represent hidden states and temporal fac-tors represent\nemissions that are conditioned on the hidden states. To our knowledge, HMM has\nnot been used in LULCC models in the past. We further demonstrate its\nintegration with other spatio-temporal models such as Logistic Regression. The\nintegrated model is applied on the LULCC data of Pune district in the state of\nMaharashtra (India) to predict and visualize urban LULCC over the past 14\nyears. We observe that the HMM integrated model has improved prediction\naccuracy as compared to the corresponding MC integrated model\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:38:17 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Yadav", "Piyush", ""], ["Ladha", "Shamsuddin", ""], ["Deshpande", "Shailesh", ""], ["Curry", "Edward", ""]]}, {"id": "2007.00499", "submitter": "Pavao Santak", "authors": "Pavao Santak, Gareth Conduit", "title": "Enhancing NEMD with automatic shear rate sampling to model viscosity and\n  correction of systematic errors in modelling density: Application to linear\n  and light branched alkanes", "comments": null, "journal-ref": "J. Chem. Phys. 153, 014102 (2020)", "doi": "10.1063/5.0004377", "report-no": null, "categories": "physics.comp-ph physics.chem-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform molecular dynamics simulations to model density as a function of\ntemperature for 74 alkanes with 5 to 10 carbon atoms and non-equilibrium\nmolecular dynamics simulations in the NVT ensemble to model kinematic viscosity\nof 10 linear alkanes as a function of molecular weight, pressure, and\ntemperature. To model density, we perform simulations in the NPT ensemble\nbefore applying correction factors to exploit the systematic error in the\nSciPCFF force field, and compare results to experimental values, obtaining an\naverage absolute deviation of 3.4g/l at 25 $^{\\circ}$ C and of 7.2g/l at 100\n$^{\\circ}$ C. We develop a sampling algorithm that automatically selects good\nshear rates at which to perform viscosity simulations in the NVT ensemble and\nuse Carreau model with weighted least squares regression to extrapolate\nNewtonian viscosity. Viscosity simulations are performed at experimental\ndensities and show an excellent agreement with experimental viscosities, with\nan average percent deviation of -1% and an average absolute percent deviation\nof 5%. Future plans to study and apply the sampling algorithm are outlined.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:56:43 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Santak", "Pavao", ""], ["Conduit", "Gareth", ""]]}, {"id": "2007.00756", "submitter": "Andre Nguyen", "authors": "Nicole E. Kogan, Leonardo Clemente, Parker Liautaud, Justin Kaashoek,\n  Nicholas B. Link, Andre T. Nguyen, Fred S. Lu, Peter Huybers, Bernd Resch,\n  Clemens Havas, Andreas Petutschnig, Jessica Davis, Matteo Chinazzi, Backtosch\n  Mustafa, William P. Hanage, Alessandro Vespignani, Mauricio Santillana", "title": "An Early Warning Approach to Monitor COVID-19 Activity with Multiple\n  Digital Traces in Near Real-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-pharmaceutical interventions (NPIs) have been crucial in curbing COVID-19\nin the United States (US). Consequently, relaxing NPIs through a phased\nre-opening of the US amid still-high levels of COVID-19 susceptibility could\nlead to new epidemic waves. This calls for a COVID-19 early warning system.\nHere we evaluate multiple digital data streams as early warning indicators of\nincreasing or decreasing state-level US COVID-19 activity between January and\nJune 2020. We estimate the timing of sharp changes in each data stream using a\nsimple Bayesian model that calculates in near real-time the probability of\nexponential growth or decay. Analysis of COVID-19-related activity on social\nnetwork microblogs, Internet searches, point-of-care medical software, and a\nmetapopulation mechanistic model, as well as fever anomalies captured by smart\nthermometer networks, shows exponential growth roughly 2-3 weeks prior to\ncomparable growth in confirmed COVID-19 cases and 3-4 weeks prior to comparable\ngrowth in COVID-19 deaths across the US over the last 6 months. We further\nobserve exponential decay in confirmed cases and deaths 5-6 weeks after\nimplementation of NPIs, as measured by anonymized and aggregated human mobility\ndata from mobile phones. Finally, we propose a combined indicator for\nexponential growth in multiple data streams that may aid in developing an early\nwarning system for future COVID-19 outbreaks. These efforts represent an\ninitial exploratory framework, and both continued study of the predictive power\nof digital indicators as well as further development of the statistical\napproach are needed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:04:47 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 21:04:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kogan", "Nicole E.", ""], ["Clemente", "Leonardo", ""], ["Liautaud", "Parker", ""], ["Kaashoek", "Justin", ""], ["Link", "Nicholas B.", ""], ["Nguyen", "Andre T.", ""], ["Lu", "Fred S.", ""], ["Huybers", "Peter", ""], ["Resch", "Bernd", ""], ["Havas", "Clemens", ""], ["Petutschnig", "Andreas", ""], ["Davis", "Jessica", ""], ["Chinazzi", "Matteo", ""], ["Mustafa", "Backtosch", ""], ["Hanage", "William P.", ""], ["Vespignani", "Alessandro", ""], ["Santillana", "Mauricio", ""]]}, {"id": "2007.00807", "submitter": "Juan Carlos Castro-Palacio", "authors": "M. E. Iglesias-Mart\\'inez, M. Hernaiz-Guijarro, J. C. Castro-Palacio,\n  P. Fern\\'andez-de-C\\'ordoba, J. M. Isidro, E. Navarro-Pardo", "title": "Machinery Failure Approach and Spectral Analysis to study the Reaction\n  Time Dynamics over Consecutive Visual Stimuli", "comments": "5 figures, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reaction times of individuals over consecutive visual stimuli have been\nstudied using spectral analysis and a failure machinery approach. The used\ntools include the fast Fourier transform and a spectral entropy analysis. The\nresults indicate that the reaction times produced by the independently\nresponding individuals to visual stimuli appear to be correlated. The spectral\nanalysis and the entropy of the spectrum yield that there are features of\nsimilarity in the response times of each participant and among them.\nFurthermore, the analysis of the mistakes made by the participants during the\nreaction time experiments concluded that they follow a behavior which is\nconsistent with the MTBF (Mean Time Between Failures) model, widely used in\nindustry for the predictive diagnosis of electrical machines and equipment.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:13:13 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 06:58:35 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Iglesias-Mart\u00ednez", "M. E.", ""], ["Hernaiz-Guijarro", "M.", ""], ["Castro-Palacio", "J. C.", ""], ["Fern\u00e1ndez-de-C\u00f3rdoba", "P.", ""], ["Isidro", "J. M.", ""], ["Navarro-Pardo", "E.", ""]]}, {"id": "2007.00848", "submitter": "Marcos Prates O", "authors": "Fernanda L. Schumacher, Clecio S. Ferreira, Marcos O. Prates, Alberto\n  Lachos, Victor H. Lachos", "title": "A robust nonlinear mixed-effects model for COVID-19 deaths data", "comments": "11 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of complex longitudinal data such as COVID-19 deaths is\nchallenging due to several inherent features: (i) Similarly-shaped profiles\nwith different decay patterns; (ii) Unexplained variation among repeated\nmeasurements within each country, these repeated measurements may be viewed as\nclustered data since they are taken on the same country at roughly the same\ntime; (iii) Skewness, outliers or skew-heavy-tailed noises are possibly\nembodied within response variables. This article formulates a robust nonlinear\nmixed-effects model based in the class of scale mixtures of skew-normal\ndistributions for modeling COVID-19 deaths, which allows the analysts to model\nsuch data in the presence of the above described features simultaneously. An\nefficient EM-type algorithm is proposed to carry out maximum likelihood\nestimation of model parameters. The bootstrap method is used to determine\ninherent characteristics of the nonlinear individual profiles such as\nconfidence interval of the predicted deaths and fitted curves. The target is to\nmodel COVID-19 deaths curves from some Latin American countries since this\nregion is the new epicenter of the disease. Moreover, since a mixed-effect\nframework borrows information from the population-average effects, in our\nanalysis we include some countries from Europe and North America that are in a\nmore advanced stage of their COVID-19 deaths curve.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:56:35 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 20:31:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Schumacher", "Fernanda L.", ""], ["Ferreira", "Clecio S.", ""], ["Prates", "Marcos O.", ""], ["Lachos", "Alberto", ""], ["Lachos", "Victor H.", ""]]}, {"id": "2007.00929", "submitter": "Peter van der Heijden", "authors": "Peter G.M. van der Heijden, Maarten Cruyff, Paul A. Smith, Christine\n  Bycroft, Patrick Graham and Nathaniel Matheson-Dunning", "title": "Multiple system estimation using covariates having missing values and\n  measurement error: estimating the size of the M\\=aori population in New\n  Zealand", "comments": "34 pages, 4 figures, submitted to JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate use of two or more linked registers, or lists, for both\npopulation size estimation and to investigate the relationship between\nvariables appearing on all or only some registers. This relationship is usually\nnot fully known because some individuals appear in only some registers, and\nsome are not in any register. These two problems have been solved\nsimultaneously using the EM algorithm. We extend this approach to estimate the\nsize of the indigenous M\\=aori population in New Zealand, leading to several\ninnovations: (1) the approach is extended to four registers (including the\npopulation census), where the reporting of M\\=aori status differs between\nregisters; (2) some individuals in one or more registers have missing\nethnicity, and we adapt the approach to handle this additional missingness; (3)\nsome registers cover subsets of the population by design. We discuss under\nwhich assumptions such structural undercoverage can be ignored and provide a\ngeneral result; (4) we treat the M\\=aori indicator in each register as a\nvariable measured with error, and embed a latent class model in the multiple\nsystem estimation to estimate the population size of a latent variable,\ninterpreted as the true M\\=aori status. Finally, we discuss estimating the\nM\\=aori population size from administrative data only. Supplementary materials\nfor our article are available online.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:11:13 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["van der Heijden", "Peter G. M.", ""], ["Cruyff", "Maarten", ""], ["Smith", "Paul A.", ""], ["Bycroft", "Christine", ""], ["Graham", "Patrick", ""], ["Matheson-Dunning", "Nathaniel", ""]]}, {"id": "2007.01100", "submitter": "Xiao Huang", "authors": "Xiao Huang, Zhenlong Li, Yuqin Jiang, Xiaoming Li, Dwayne Porter", "title": "Twitter, human mobility, and COVID-19", "comments": "27 pages, 9 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The outbreak of COVID-19 highlights the need for a more harmonized, less\nprivacy-concerning, easily accessible approach to monitoring the human mobility\nthat has been proved to be associated with the viral transmission. In this\nstudy, we analyzed 587 million tweets worldwide to see how global collaborative\nefforts in reducing human mobility are reflected from the user-generated\ninformation at the global, country, and the U.S. state scale. Considering the\nmultifaceted nature of mobility, we propose two types of distance: the\nsingle-day distance and the cross-day distance. To quantify the responsiveness\nin certain geographical regions, we further propose a mobility-based responsive\nindex (MRI) that captures the overall degree of mobility changes within a time\nwindow. The results suggest that mobility patterns obtained from Twitter data\nare amendable to quantitatively reflect the mobility dynamics. Globally, the\nproposed two distances had greatly deviated from their baselines after March\n11, 2020, when WHO declared COVID-19 as a pandemic. The considerably less\nperiodicity after the declaration suggests that the protection measures have\nobviously affected people's travel routines. The country scale comparisons\nreveal the discrepancies in responsiveness, evidenced by the contrasting\nmobility patterns in different epidemic phases. We find that the triggers of\nmobility changes correspond well with the national announcements of mitigation\nmeasures. In the U.S., the influence of the COVID-19 pandemic on mobility is\ndistinct. However, the impacts varied substantially among states. The strong\nmobility recovering momentum is further fueled by the Black Lives Matter\nprotests, potentially fostering the second wave of infections in the U.S.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 23:21:03 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Huang", "Xiao", ""], ["Li", "Zhenlong", ""], ["Jiang", "Yuqin", ""], ["Li", "Xiaoming", ""], ["Porter", "Dwayne", ""]]}, {"id": "2007.01171", "submitter": "Laurens Deprez", "authors": "Laurens Deprez, Katrien Antonio, Robert Boute", "title": "Pricing service maintenance contracts using predictive analytics", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2020.08.022", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more manufacturers shift their focus from selling products to end\nsolutions, full-service maintenance contracts gain traction in the business\nworld. These contracts cover all maintenance related costs during a\npredetermined horizon in exchange for a fixed service fee and relieve customers\nfrom uncertain maintenance costs. To guarantee profitability, the service fees\nshould at least cover the expected costs during the contract horizon. As these\nexpected costs may depend on several machine-dependent characteristics, e.g.\noperational environment, the service fees should also be differentiated based\non these characteristics. If not, customers that are less prone to high\nmaintenance costs will not buy into or renege on the contract. The latter can\nlead to adverse selection and leave the service provider with a\nmaintenance-heavy portfolio, which may be detrimental to the profitability of\nthe service contracts. We contribute to the literature with a data-driven\ntariff plan based on the calibration of predictive models that take into\naccount the different machine profiles. This conveys to the service provider\nwhich machine profiles should be attracted at which price. We demonstrate the\nadvantage of a differentiated tariff plan and show how it better protects\nagainst adverse selection.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:56:54 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Deprez", "Laurens", ""], ["Antonio", "Katrien", ""], ["Boute", "Robert", ""]]}, {"id": "2007.01238", "submitter": "Gustau Camps-Valls", "authors": "Gustau Camps-Valls and Dino Sejdinovic and Jakob Runge and Markus\n  Reichstein", "title": "A Perspective on Gaussian Processes for Earth Observation", "comments": "1 figure", "journal-ref": "National Science Review, Volume 6, Issue 4, July 2019, Pages\n  616-618", "doi": "10.1093/nsr/nwz028", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth observation (EO) by airborne and satellite remote sensing and in-situ\nobservations play a fundamental role in monitoring our planet. In the last\ndecade, machine learning and Gaussian processes (GPs) in particular has\nattained outstanding results in the estimation of bio-geo-physical variables\nfrom the acquired images at local and global scales in a time-resolved manner.\nGPs provide not only accurate estimates but also principled uncertainty\nestimates for the predictions, can easily accommodate multimodal data coming\nfrom different sensors and from multitemporal acquisitions, allow the\nintroduction of physical knowledge, and a formal treatment of uncertainty\nquantification and error propagation. Despite great advances in forward and\ninverse modelling, GP models still have to face important challenges that are\nrevised in this perspective paper. GP models should evolve towards data-driven\nphysics-aware models that respect signal characteristics, be consistent with\nelementary laws of physics, and move from pure regression to observational\ncausal inference.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:44:11 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Camps-Valls", "Gustau", ""], ["Sejdinovic", "Dino", ""], ["Runge", "Jakob", ""], ["Reichstein", "Markus", ""]]}, {"id": "2007.01340", "submitter": "Branden Olson", "authors": "Julia Fukuyama, Branden J Olson, Frederick A Matsen IV", "title": "Lack of evidence for a substantial rate of templated mutagenesis in B\n  cell diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B cell receptor sequences diversify through mutations introduced by\npurpose-built cellular machinery. A recent paper has concluded that a\n\"templated mutagenesis\" process is a major contributor to somatic\nhypermutation, and therefore immunoglobulin diversification, in mice and\nhumans. In this proposed process, mutations in the immunoglobulin locus are\nintroduced by copying short segments from other immunoglobulin genes. If true,\nthis would overturn decades of research on B cell diversification, and would\nrequire a complete re-write of computational methods to analyze B cell data for\nthese species.\n  In this paper, we re-evaluate the templated mutagenesis hypothesis. By\napplying the original inferential method using potential donor templates absent\nfrom B cell genomes, we obtain estimates of the methods's false positive rates.\nWe find false positive rates of templated mutagenesis in murine and human\nimmunoglobulin loci that are similar to or even higher than the original rate\ninferences, and by considering the bases used in substitution we find evidence\nthat if templated mutagenesis occurs, it is at a low rate. We also show that\nthe statistically significant results in the original paper can easily result\nfrom a slight misspecification of the null model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:50:09 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fukuyama", "Julia", ""], ["Olson", "Branden J", ""], ["Matsen", "Frederick A", "IV"]]}, {"id": "2007.01370", "submitter": "Thomas Webster", "authors": "Thomas F. Webster and Marc G. Weisskopf", "title": "Epidemiology of exposure to mixtures: we cant be casual about causality\n  when using or testing methods", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: There is increasing interest in approaches for analyzing the\neffect of exposure mixtures on health. A key issue is how to simultaneously\nanalyze often highly collinear components of the mixture, which can create\nproblems such as confounding by co-exposure and co-exposure amplification bias\n(CAB). Evaluation of novel mixtures methods, typically using synthetic data, is\ncritical to their ultimate utility. Objectives: This paper aims to answer two\nquestions. How do causal models inform the interpretation of statistical models\nand the creation of synthetic data used to test them? Are novel mixtures\nmethods susceptible to CAB? Methods: We use directed acyclic graphs (DAGs) and\nlinear models to derive closed form solutions for model parameters to examine\nhow underlying causal assumptions affect the interpretation of model results.\nResults: The same beta coefficients estimated by a statistical model can have\ndifferent interpretations depending on the assumed causal structure. Similarly,\nthe method used to simulate data can have implications for the underlying DAG\n(and vice versa), and therefore the identification of the parameter being\nestimated with an analytic approach. We demonstrate that methods that can\nreproduce results of linear regression, such as Bayesian kernel machine\nregression and the new quantile g-computation approach, will be subject to CAB.\nHowever, under some conditions, estimates of an overall effect of the mixture\nis not subject to CAB and even has reduced uncontrolled bias. Discussion: Just\nas DAGs encode a priori subject matter knowledge allowing identification of\nvariable control needed to block analytic bias, we recommend explicitly\nidentifying DAGs underlying synthetic data created to test statistical mixtures\napproaches. Estimates of the total effect of a mixture is an important but\nrelatively underexplored topic that warrants further investigation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:14:34 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Webster", "Thomas F.", ""], ["Weisskopf", "Marc G.", ""]]}, {"id": "2007.01484", "submitter": "Yanxun Xu", "authors": "Yuliang Li, Yang Ni, Leah H. Rubin, Amanda B. Spence, Yanxun Xu", "title": "BAGEL: A Bayesian Graphical Model for Inferring Drug Effect\n  Longitudinally on Depression in People with HIV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access and adherence to antiretroviral therapy (ART) has transformed the face\nof HIV infection from a fatal to a chronic disease. However, ART is also known\nfor its side effects. Studies have reported that ART is associated with\ndepressive symptomatology. Large-scale HIV clinical databases with individuals'\nlongitudinal depression records, ART medications, and clinical characteristics\noffer researchers unprecedented opportunities to study the effects of ART drugs\non depression over time. We develop BAGEL, a Bayesian graphical model to\ninvestigate longitudinal effects of ART drugs on a range of depressive symptoms\nwhile adjusting for participants' demographic, behavior, and clinical\ncharacteristics, and taking into account the heterogeneous population through a\nBayesian nonparametric prior. We evaluate BAGEL through simulation studies.\nApplication to a dataset from the Women's Interagency HIV Study yields\ninterpretable and clinically useful results. BAGEL not only can improve our\nunderstanding of ART drugs effects on disparate depression symptoms, but also\nhas clinical utility in guiding informed and effective treatment selection to\nfacilitate precision medicine in HIV.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:44:58 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Li", "Yuliang", ""], ["Ni", "Yang", ""], ["Rubin", "Leah H.", ""], ["Spence", "Amanda B.", ""], ["Xu", "Yanxun", ""]]}, {"id": "2007.01516", "submitter": "Deepak Sharma", "authors": "Deepak Sharma, Audrey Durand, Marc-Andr\\'e Legault, Louis-Philippe\n  Lemieux Perreault, Audrey Lema\\c{c}on, Marie-Pierre Dub\\'e, Joelle Pineau", "title": "Deep interpretability for GWAS", "comments": "Accepted at ICML 2020 workshop on ML Interpretability for Scientific\n  Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-Wide Association Studies are typically conducted using linear models\nto find genetic variants associated with common diseases. In these studies,\nassociation testing is done on a variant-by-variant basis, possibly missing out\non non-linear interaction effects between variants. Deep networks can be used\nto model these interactions, but they are difficult to train and interpret on\nlarge genetic datasets. We propose a method that uses the gradient based deep\ninterpretability technique named DeepLIFT to show that known diabetes genetic\nrisk factors can be identified using deep models along with possibly novel\nassociations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 06:49:31 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sharma", "Deepak", ""], ["Durand", "Audrey", ""], ["Legault", "Marc-Andr\u00e9", ""], ["Perreault", "Louis-Philippe Lemieux", ""], ["Lema\u00e7on", "Audrey", ""], ["Dub\u00e9", "Marie-Pierre", ""], ["Pineau", "Joelle", ""]]}, {"id": "2007.01675", "submitter": "Michael Chappell", "authors": "Michael A. Chappell, Martin S. Craig, Mark W. Woolrich", "title": "Stochastic Variational Bayesian Inference for a Nonlinear Forward Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Bayes (VB) has been used to facilitate the calculation of the\nposterior distribution in the context of Bayesian inference of the parameters\nof nonlinear models from data. Previously an analytical formulation of VB has\nbeen derived for nonlinear model inference on data with additive gaussian noise\nas an alternative to nonlinear least squares. Here a stochastic solution is\nderived that avoids some of the approximations required of the analytical\nformulation, offering a solution that can be more flexibly deployed for\nnonlinear model inference problems. The stochastic VB solution was used for\ninference on a biexponential toy case and the algorithmic parameter space\nexplored, before being deployed on real data from a magnetic resonance imaging\nstudy of perfusion. The new method was found to achieve comparable parameter\nrecovery to the analytic solution and be competitive in terms of computational\nspeed despite being reliant on sampling.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:30:50 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Chappell", "Michael A.", ""], ["Craig", "Martin S.", ""], ["Woolrich", "Mark W.", ""]]}, {"id": "2007.01796", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Stacy Rosenbaum, Elizabeth Archie, Susan Alberts, Fan Li", "title": "Causal Mediation Analysis for Sparse and Irregular Longitudinal Data", "comments": "28 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis seeks to investigate how the treatment effect of an\nexposure on outcomes is mediated through intermediate variables. Although many\napplications involve longitudinal data, the existing methods are not directly\napplicable to settings where the mediator and outcome are measured on sparse\nand irregular time grids. We extend the existing causal mediation framework\nfrom a functional data analysis perspective, viewing the sparse and irregular\nlongitudinal data as realizations of underlying smooth stochastic processes. We\ndefine causal estimands of direct and indirect effects accordingly and provide\ncorresponding identification assumptions. For estimation and inference, we\nemploy a functional principal component analysis approach for dimension\nreduction and use the first few functional principal components instead of the\nwhole trajectories in the structural equation models. We adopt the Bayesian\nparadigm to accurately quantify the uncertainties. The operating\ncharacteristics of the proposed methods are examined via simulations. We apply\nthe proposed methods to a longitudinal data set from a wild baboon population\nin Kenya to investigate the causal relationships between early adversity,\nstrength of social bonds between animals, and adult glucocorticoid hormone\nconcentrations. We find that early adversity has a significant direct effect (a\n9-14% increase) on females' glucocorticoid concentrations across adulthood, but\nfind little evidence that these effects were mediated by weak social bonds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:36:09 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:50:04 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 02:28:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zeng", "Shuxi", ""], ["Rosenbaum", "Stacy", ""], ["Archie", "Elizabeth", ""], ["Alberts", "Susan", ""], ["Li", "Fan", ""]]}, {"id": "2007.01903", "submitter": "Max Biggs", "authors": "Max Biggs, Wei Sun, Markus Ettl", "title": "Model Distillation for Revenue Optimization: Interpretable Personalized\n  Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven pricing strategies are becoming increasingly common, where\ncustomers are offered a personalized price based on features that are\npredictive of their valuation of a product. It is desirable for this pricing\npolicy to be simple and interpretable, so it can be verified, checked for\nfairness, and easily implemented. However, efforts to incorporate machine\nlearning into a pricing framework often lead to complex pricing policies which\nare not interpretable, resulting in slow adoption in practice. We present a\ncustomized, prescriptive tree-based algorithm that distills knowledge from a\ncomplex black-box machine learning algorithm, segments customers with similar\nvaluations and prescribes prices in such a way that maximizes revenue while\nmaintaining interpretability. We quantify the regret of a resulting policy and\ndemonstrate its efficacy in applications with both synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:33:23 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:22:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Biggs", "Max", ""], ["Sun", "Wei", ""], ["Ettl", "Markus", ""]]}, {"id": "2007.01935", "submitter": "Jingyi Jessica Li", "authors": "Jingyi Jessica Li and Xin Tong", "title": "Statistical hypothesis testing versus machine-learning binary\n  classification: distinctions and guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making binary decisions is a common data analytical task in scientific\nresearch and industrial applications. In data sciences, there are two related\nbut distinct strategies: hypothesis testing and binary classification. In\npractice, how to choose between these two strategies can be unclear and rather\nconfusing. Here we summarize key distinctions between these two strategies in\nthree aspects and list five practical guidelines for data analysts to choose\nthe appropriate strategy for specific analysis needs. We demonstrate the use of\nthose guidelines in a cancer driver gene prediction example.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 20:56:54 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 04:06:10 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Li", "Jingyi Jessica", ""], ["Tong", "Xin", ""]]}, {"id": "2007.01979", "submitter": "Ronal Arela-Bobadilla", "authors": "Ronal Arela-Bobadilla", "title": "Excess deaths hidden 100 days after the quarantine in Peru by COVID-19", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To make an estimate of the excess deaths caused by COVID-19 in the\nnon-violent mortality of Peru, controlling for the effect of quarantine.\nMethods: Analysis of longitudinal data from the departments of Peru using\nofficial public information from the National Death Information System and the\nMinistry of Health of Peru. The analysis is performed between January 1, 2018\nand June 23, 2020 (100 days of quarantine). The daily death rate per million\ninhabitants has been used. The days in which the departments were quarantined\nwith a limit number of accumulated cases of COVID-19 were used to estimate the\nquarantine impact. Three limits were established for cases: less than 1, 10 and\n100 cases. Result: In Peru, the daily death rate per million inhabitants\ndecreased by -1.89 (95% CI: -2.70; -1.07) on quarantine days and without\nCOVID-19 cases. When comparing this result with the total number of non-violent\ndeaths, the excess deaths during the first 100 days of quarantine is 36,230.\nThis estimate is 1.12 times the estimate with data from 2019 and 4.2 times the\ndeaths officers by COVID-19. Conclusion: Quarantine reduced nonviolent deaths;\nhowever, they are overshadowed by the increase as a direct or indirect cause of\nthe pandemic. Therefore, the difference between the number of current deaths\nand that of past years underestimates the real excess of deaths.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 01:16:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Arela-Bobadilla", "Ronal", ""]]}, {"id": "2007.02010", "submitter": "Yanwei Fu", "authors": "Yanwei Fu, Chen Liu, Donghao Li, Xinwei Sun, Jinshan Zeng, Yuan Yao", "title": "DessiLBI: Exploring Structural Sparsity of Deep Networks via\n  Differential Inclusion Paths", "comments": "conference , 23 pages https://github.com/corwinliu9669/dS2LBI", "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization is ubiquitous nowadays in training neural networks to\nbenefit both optimization in seeking global optima and generalization in\nreducing prediction error. However, compressive networks are desired in many\nreal world applications and direct training of small networks may be trapped in\nlocal optima. In this paper, instead of pruning or distilling\nover-parameterized models to compressive ones, we propose a new approach based\non differential inclusions of inverse scale spaces. Specifically, it generates\na family of models from simple to complex ones that couples a pair of\nparameters to simultaneously train over-parameterized deep models and\nstructural sparsity on weights of fully connected and convolutional layers.\nSuch a differential inclusion scheme has a simple discretization, proposed as\nDeep structurally splitting Linearized Bregman Iteration (DessiLBI), whose\nglobal convergence analysis in deep learning is established that from any\ninitializations, algorithmic iterations converge to a critical point of\nempirical risks. Experimental evidence shows that DessiLBI achieve comparable\nand even better performance than the competitive optimizers in exploring the\nstructural sparsity of several widely used backbones on the benchmark datasets.\nRemarkably, with early stopping, DessiLBI unveils \"winning tickets\" in early\nepochs: the effective sparse structure with comparable test accuracy to fully\ntrained over-parameterized models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 04:40:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fu", "Yanwei", ""], ["Liu", "Chen", ""], ["Li", "Donghao", ""], ["Sun", "Xinwei", ""], ["Zeng", "Jinshan", ""], ["Yao", "Yuan", ""]]}, {"id": "2007.02014", "submitter": "Clayton Miller", "authors": "Prageeth Jayathissa, Matias Quintana, Mahmoud Abdelrahman, and Clayton\n  Miller", "title": "Humans-as-a-sensor for buildings: Intensive longitudinal indoor comfort\n  models", "comments": null, "journal-ref": "Buildings 2020, 10(10), 174", "doi": "10.3390/buildings10100174", "report-no": null, "categories": "cs.HC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating and optimising human comfort within the built environment is\nchallenging due to the large number of physiological, psychological and\nenvironmental variables that affect occupant comfort preference. Human\nperception could be helpful to capture these disparate phenomena and\ninterpreting their impact; the challenge is collecting spatially and temporally\ndiverse subjective feedback in a scalable way. This paper presents a\nmethodology to collect intensive longitudinal subjective feedback of\ncomfort-based preference using micro ecological momentary assessments on a\nsmartwatch platform. An experiment with 30 occupants over two weeks produced\n4,378 field-based surveys for thermal, noise, and acoustic preference. The\noccupants and the spaces in which they left feedback were then clustered\naccording to these preference tendencies. These groups were used to create\ndifferent feature sets with combinations of environmental and physiological\nvariables, for use in a multi-class classification task. These classification\nmodels were trained on a feature set that was developed from time-series\nattributes, environmental and near-body sensors, heart rate, and the historical\npreferences of both the individual and the comfort group assigned. The most\naccurate model had multi-class classification F1 micro scores of 64%, 80% and\n86% for thermal, light, and noise preference, respectively. The discussion\noutlines how these models can enhance comfort preference prediction when\nsupplementing data from installed sensors. The approach presented prompts\nreflection on how the building analysis community evaluates, controls, and\ndesigns indoor environments through balancing the measurement of variables with\nstrategically asking for occupant preferences in an intensive longitudinal way.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 05:52:56 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 06:16:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Jayathissa", "Prageeth", ""], ["Quintana", "Matias", ""], ["Abdelrahman", "Mahmoud", ""], ["Miller", "Clayton", ""]]}, {"id": "2007.02103", "submitter": "Yihuang Kang", "authors": "Bowen Kuo, Yihuang Kang, Pinghsung Wu, Sheng-Tai Huang, Yajie Huang", "title": "Discovering Drug-Drug and Drug-Disease Interactions Inducing Acute\n  Kidney Injury Using Deep Rule Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patients with Acute Kidney Injury (AKI) increase mortality, morbidity, and\nlong-term adverse events. Therefore, early identification of AKI may improve\nrenal function recovery, decrease comorbidities, and further improve patients'\nsurvival. To control certain risk factors and develop targeted prevention\nstrategies are important to reduce the risk of AKI. Drug-drug interactions and\ndrug-disease interactions are critical issues for AKI. Typical statistical\napproaches cannot handle the complexity of drug-drug and drug-disease\ninteractions. In this paper, we propose a novel learning algorithm, Deep Rule\nForests (DRF), which discovers rules from multilayer tree models as the\ncombinations of drug usages and disease indications to help identify such\ninteractions. We found that several disease and drug usages are considered\nhaving significant impact on the occurrence of AKI. Our experimental results\nalso show that the DRF model performs comparatively better than typical\ntree-based and other state-of-the-art algorithms in terms of prediction\naccuracy and model interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:10:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kuo", "Bowen", ""], ["Kang", "Yihuang", ""], ["Wu", "Pinghsung", ""], ["Huang", "Sheng-Tai", ""], ["Huang", "Yajie", ""]]}, {"id": "2007.02105", "submitter": "Edsel Pena", "authors": "T. KIm and B. Lieberman and G. Luta and E. Pena", "title": "Prediction Regions for Poisson and Over-Dispersed Poisson Regression\n  Models with Applications to Forecasting Number of Deaths during the COVID-19\n  Pandemic", "comments": "There are 16 Figures with some containing one to four plot panels.\n  The appendix section are supplementary materials. Without these supplementary\n  materials, there are 35 pages in this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the current Coronavirus Disease (COVID-19) pandemic, which is\ndue to the SARS-CoV-2 virus, and the important problem of forecasting daily\ndeaths and cumulative deaths, this paper examines the construction of\nprediction regions or intervals under the Poisson regression model and for an\nover-dispersed Poisson regression model. For the Poisson regression model,\nseveral prediction regions are developed and their performance are compared\nthrough simulation studies. The methods are applied to the problem of\nforecasting daily and cumulative deaths in the United States (US) due to\nCOVID-19. To examine their performance relative to what actually happened,\ndaily deaths data until May 15th were used to forecast cumulative deaths by\nJune 1st. It was observed that there is over-dispersion in the observed data\nrelative to the Poisson regression model. An over-dispersed Poisson regression\nmodel is therefore proposed. This new model builds on frailty ideas in Survival\nAnalysis and over-dispersion is quantified through an additional parameter. The\nPoisson regression model is a hidden model in this over-dispersed Poisson\nregression model and obtains as a limiting case when the over-dispersion\nparameter increases to infinity. A prediction region for the cumulative number\nof US deaths due to COVID-19 by July 16th, given the data until July 2nd, is\npresented. Finally, the paper discusses limitations of proposed procedures and\nmentions open research problems, as well as the dangers and pitfalls when\nforecasting on a long horizon, with focus on this pandemic where events, both\nforeseen and unforeseen, could have huge impacts on point predictions and\nprediction regions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:20:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 01:45:23 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["KIm", "T.", ""], ["Lieberman", "B.", ""], ["Luta", "G.", ""], ["Pena", "E.", ""]]}, {"id": "2007.02192", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Debdeep Pati, Bani K. Mallick", "title": "Tail-adaptive Bayesian shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens and\nthousands of predictors. Under diverse sparsity regimes, the nature of signal\ndetection is associated with a tail behaviour of a prior. A desirable tail\nbehaviour is called tail-adaptive shrinkage property where tail-heaviness of a\nprior gets adaptively larger (or smaller) as a sparsity level increases (or\ndecreases) to accommodate more (or less) signals. We propose a\nglobal-local-tail (GLT) Gaussian mixture distribution to ensure this property\nand provide accurate inference under diverse sparsity regimes. Incorporating a\npeaks-over-threshold method in extreme value theory, we develop an automated\ntail learning algorithm for the GLT prior. We compare the performance of the\nGLT prior to the Horseshoe in two gene expression datasets and numerical\nexamples. Results suggest that varying tail rule is advantageous over fixed\ntail rule under diverse sparsity domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:40:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:47:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lee", "Se Yoon", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2007.02222", "submitter": "Yishu Xue", "authors": "Zhihua Ma, Yishu Xue, Guanyu Hu", "title": "Geographically Weighted Regression Analysis for Spatial Economics Data:\n  a Bayesian Recourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The geographically weighted regression (GWR) is a well-known statistical\napproach to explore spatial non-stationarity of the regression relationship in\nspatial data analysis. In this paper, we discuss a Bayesian recourse of GWR.\nBayesian variable selection based on spike-and-slab prior, bandwidth selection\nbased on range prior, and model assessment using a modified deviance\ninformation criterion and a modified logarithm of pseudo-marginal likelihood\nare fully discussed in this paper. Usage of the graph distance in modeling\nareal data is also introduced. Extensive simulation studies are carried out to\nexamine the empirical performance of the proposed methods with both small and\nlarge number of location scenarios, and comparison with the classical\nfrequentist GWR is made. The performance of variable selection and estimation\nof the proposed methodology under different circumstances are satisfactory. We\nfurther apply the proposed methodology in analysis of a province-level\nmacroeconomic data of 30 selected provinces in China. The estimation and\nvariable selection results reveal insights about China's economy that are\nconvincing and agree with previous studies and facts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 01:36:50 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ma", "Zhihua", ""], ["Xue", "Yishu", ""], ["Hu", "Guanyu", ""]]}, {"id": "2007.02359", "submitter": "Roberto Rondinelli", "authors": "Luca De Benedictis, Roberto Rondinelli and Veronica Vinciotti", "title": "The network structure of cultural distances", "comments": "60 pages, 67 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making use of the information from the World Value Survey (Wave 6), and\noperationalizing a definition of national culture that encompasses both the\nrelevance of specific cultural traits and the interdependence among them, this\npaper proposes a methodology to reveal the latent structure of a national\nculture and to measure a cultural distance between countries that takes into\naccount the network structure of national cultural traits. Exploiting the\npossibilities offered by Copula graphical models for ordinal and categorical\ndata, this paper infers the structure of the cultural network of 54 countries\nand proposes a new summary measure of national cultural distances. The new\nnetwork index of cultural distance shows that, compared to the methodology used\nby Inglehart and Welzel (2005), the world appears to be more culturally\nheterogeneous than what it was previously measured.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:06:50 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 13:40:06 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 10:41:29 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["De Benedictis", "Luca", ""], ["Rondinelli", "Roberto", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "2007.02371", "submitter": "Luca Pappalardo", "authors": "Giuliano Cornacchia, Giulio Rossetti, Luca Pappalardo", "title": "Modelling Human Mobility considering Spatial,Temporal and Social\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling human mobility is crucial in several areas, from urban planning to\nepidemic modeling, traffic forecasting, and what-if analysis. On the one hand,\nexisting models focus mainly on reproducing the spatial and temporal dimensions\nof human mobility, while the social aspect, though it influences human\nmovements significantly, is often neglected. On the other hand, those models\nthat capture some social aspects of human mobility have trivial and unrealistic\nspatial and temporal mechanisms. In this paper, we propose STS-EPR, a modeling\nframework that embeds mechanisms to capture the spatial, temporal, and social\naspects together. Our experiments show that STS-EPR outperforms existing\nspatial-temporal or social models on a set of standard mobility metrics and\nthat it can be used with a limited amount of information without any\nsignificant loss of realism. STS-EPR, which is open-source and tested on open\ndata, is a step towards the design of mechanistic models that can capture all\nthe aspects of human mobility in a comprehensive way.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:04:45 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cornacchia", "Giuliano", ""], ["Rossetti", "Giulio", ""], ["Pappalardo", "Luca", ""]]}, {"id": "2007.02455", "submitter": "Xuekui Zhang", "authors": "Li Xing, Songwan Joun, Kurt Mackey, Mary Lesperance, and Xuekui Zhang", "title": "Handling high correlations in the feature gene selection using\n  Single-Cell RNA sequencing data", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Selecting feature genes and predicting cells' phenotype are\ntypical tasks in the analysis of scRNA-seq data. Many algorithms were developed\nfor these tasks, but high correlations among genes create challenges\nspecifically in scRNA-seq analysis, which are not well addressed. Highly\ncorrelated genes lead to unreliable prediction models due to technical\nproblems, such as multi-collinearity. Most importantly, when a causal gene\n(whose variants have a true biological effect on the phenotype) is highly\ncorrelated with other genes, most algorithms select one of them in a\ndata-driven manner. The correlation structure among genes could change\nsubstantially. Hence, it is critical to build a prediction model based on\ncausal genes.\n  Results: To address the issues discussed above, we propose a grouping\nalgorithm that can be integrated into prediction models. Using real benchmark\nscRNA-seq data and simulated cell phenotypes, we show our novel method\nsignificantly outperforms standard models in both prediction and feature\nselection. Our algorithm reports the whole group of correlated genes, allowing\nresearchers to either use their common pattern as a more robust predictor or\nconduct follow-up studies to identify the causal genes in the group.\n  Availability: An R package is being developed and will be available on the\nComprehensive R Archive Network (CRAN) when the paper is published.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 22:14:03 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 20:19:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Xing", "Li", ""], ["Joun", "Songwan", ""], ["Mackey", "Kurt", ""], ["Lesperance", "Mary", ""], ["Zhang", "Xuekui", ""]]}, {"id": "2007.02476", "submitter": "Richard Valliant", "authors": "Lingxiao Wang, Richard Valliant, and Yan Li", "title": "Adjusted Logistic Propensity Weighting Methods for Population Inference\n  using Nonprobability Volunteer-Based Epidemiologic Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many epidemiologic studies forgo probability sampling and turn to\nnonprobability volunteer-based samples because of cost, response burden, and\ninvasiveness of biological samples. However, finite population inference is\ndifficult to make from the nonprobability samples due to the lack of population\nrepresentativeness. Aiming for making inferences at the population level using\nnonprobability samples, various inverse propensity score weighting (IPSW)\nmethods have been studied with the propensity defined by the participation rate\nof population units in the nonprobability sample. In this paper, we propose an\nadjusted logistic propensity weighting (ALP) method to estimate the\nparticipation rates for nonprobability sample units. Compared to existing IPSW\nmethods, the proposed ALP method is easy to implement by ready-to-use software\nwhile producing approximately unbiased estimators for population quantities\nregardless of the nonprobability sample rate. The efficiency of the ALP\nestimator can be further improved by scaling the survey sample weights in\npropensity estimation. Taylor linearization variance estimators are proposed\nfor ALP estimators of finite population means that account for all sources of\nvariability. The proposed ALP methods are evaluated numerically via simulation\nstudies and empirically using the na\\\"ive unweighted National Health and\nNutrition Examination Survey III sample, while taking the 1997 National Health\nInterview Survey as the reference, to estimate the 15-year mortality rates.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:13:58 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 23:15:27 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 21:50:38 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wang", "Lingxiao", ""], ["Valliant", "Richard", ""], ["Li", "Yan", ""]]}, {"id": "2007.02510", "submitter": "Kamanchi Chandramouli", "authors": "Chandramouli Kamanchi and Gopinath Ashok Kumar and Nachiappan Sundaram\n  and Ravindra Babu T and Chaithanya Bandi", "title": "An Application of Newsboy Problem in Supply Chain Optimisation of Online\n  Fashion E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a supply chain optimization model deployed in an online fashion\ne-commerce company in India called Myntra. Our model is simple, elegant and\neasy to put into service. The model utilizes historic data and predicts the\nquantity of Stock Keeping Units (SKUs) to hold so that the metrics \"Fulfilment\nIndex\" and \"Utilization Index\" are optimized. We present the mathematics\ncentral to our model as well as compare the performance of our model with\nbaseline regression based solutions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:27:42 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kamanchi", "Chandramouli", ""], ["Kumar", "Gopinath Ashok", ""], ["Sundaram", "Nachiappan", ""], ["T", "Ravindra Babu", ""], ["Bandi", "Chaithanya", ""]]}, {"id": "2007.02541", "submitter": "Feng Zhao", "authors": "Feng Zhao", "title": "Moments of the multivariate Beta distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend Beta distribution to 2 by 2 matrix and give the\nanalytical formula for its moments. Our analytical formula can be used to\nanalyze the asymptotic behavior of Beta distribution for 2 by 2 matrix.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 05:56:17 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 11:42:28 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 12:28:34 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhao", "Feng", ""]]}, {"id": "2007.02552", "submitter": "Guillaume Chauvet", "authors": "Val\\'erie Gar\\`es (IRMAR), Guillaume Chauvet (IRMAR), David Hajage", "title": "Closed-form variance estimators for weighted and stratified\n  dose-response function estimators using generalized propensity score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods are widely used in observational studies for\nevaluating marginal treatment effects. The generalized propensity score (GPS)\nis an extension of the propensity score framework, historically developed in\nthe case of binary exposures, for use with quantitative or continuous\nexposures. In this paper, we proposed variance esti-mators for treatment effect\nestimators on continuous outcomes. Dose-response functions (DRF) were estimated\nthrough weighting on the inverse of the GPS, or using stratification. Variance\nestimators were evaluated using Monte Carlo simulations. Despite the use of\nstabilized weights, the variability of the weighted estimator of the DRF was\nparticularly high, and none of the variance estimators (a bootstrap-based\nestimator, a closed-form estimator especially developped to take into account\nthe estimation step of the GPS, and a sandwich estimator) were able to\nadequately capture this variability, resulting in coverages below to the\nnominal value, particularly when the proportion of the variation in the\nquantitative exposure explained by the covariates was 1 large. The stratified\nestimator was more stable, and variance estima-tors (a bootstrap-based\nestimator, a pooled linearized estimator, and a pooled model-based estimator)\nmore efficient at capturing the empirical variability of the parameters of the\nDRF. The pooled variance estimators tended to overestimate the variance,\nwhereas the bootstrap estimator, which intrinsically takes into account the\nestimation step of the GPS, resulted in correct variance estimations and\ncoverage rates. These methods were applied to a real data set with the aim of\nassessing the effect of maternal body mass index on newborn birth weight.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 06:45:06 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gar\u00e8s", "Val\u00e9rie", "", "IRMAR"], ["Chauvet", "Guillaume", "", "IRMAR"], ["Hajage", "David", ""]]}, {"id": "2007.02613", "submitter": "Roi Naveiro", "authors": "David Banks, V\\'ictor Gallego, Roi Naveiro, David R\\'ios Insua", "title": "Adversarial Risk Analysis (Overview)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial risk analysis (ARA) is a relatively new area of research that\ninforms decision-making when facing intelligent opponents and uncertain\noutcomes. It enables an analyst to express her Bayesian beliefs about an\nopponent's utilities, capabilities, probabilities and the type of strategic\ncalculation that the opponent is using. Within that framework, the analyst then\nsolves the problem from the perspective of the opponent while placing\nsubjective probability distributions on all unknown quantities. This produces a\ndistribution over the actions of the opponent that permits the analyst to\nmaximize her expected utility. This overview covers conceptual, modeling,\ncomputational and applied issues in ARA.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:59:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Banks", "David", ""], ["Gallego", "V\u00edctor", ""], ["Naveiro", "Roi", ""], ["Insua", "David R\u00edos", ""]]}, {"id": "2007.02726", "submitter": "Yasin Simsek", "authors": "Cem Cakmakli and Yasin Simsek", "title": "Bridging the COVID-19 Data and the Epidemiological Model using Time\n  Varying Parameter SIRD Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the canonical model of epidemiology, SIRD model, to allow\nfor time varying parameters for real-time measurement of the stance of the\nCOVID-19 pandemic. Time variation in model parameters is captured using the\ngeneralized autoregressive score modelling structure designed for the typically\ndaily count data related to pandemic. The resulting specification permits a\nflexible yet parsimonious model structure with a very low computational cost.\nThis is especially crucial at the onset of the pandemic when the data is scarce\nand the uncertainty is abundant. Full sample results show that countries\nincluding US, Brazil and Russia are still not able to contain the pandemic with\nthe US having the worst performance. Furthermore, Iran and South Korea are\nlikely to experience the second wave of the pandemic. A real-time exercise show\nthat the proposed structure delivers timely and precise information on the\ncurrent stance of the pandemic ahead of the competitors that use rolling\nwindow. This, in turn, transforms into accurate short-term predictions of the\nactive cases. We further modify the model to allow for unreported cases.\nResults suggest that the effects of the presence of these cases on the\nestimation results diminish towards the end of sample with the increasing\nnumber of testing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 12:31:55 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 10:21:36 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cakmakli", "Cem", ""], ["Simsek", "Yasin", ""]]}, {"id": "2007.02733", "submitter": "Shibaji Chakraborty", "authors": "S. Chakraborty and S. K. Morley", "title": "Probabilistic Prediction of Geomagnetic Storms and the K$_{\\textrm{p}}$\n  Index", "comments": "The article has been accepted for publication in the Journal of Space\n  Weather and Space Climate (JSWSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.space-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geomagnetic activity is often described using summary indices to summarize\nthe likelihood of space weather impacts, as well as when parameterizing space\nweather models. The geomagnetic index $\\text{K}_\\text{p}$ in particular, is\nwidely used for these purposes. Current state-of-the-art forecast models\nprovide deterministic $\\text{K}_\\text{p}$ predictions using a variety of\nmethods -- including empirically-derived functions, physics-based models, and\nneural networks -- but do not provide uncertainty estimates associated with the\nforecast. This paper provides a sample methodology to generate a 3-hour-ahead\n$\\text{K}_\\text{p}$ prediction with uncertainty bounds and from this provide a\nprobabilistic geomagnetic storm forecast. Specifically, we have used a\ntwo-layered architecture to separately predict storm ($\\text{K}_\\text{p}\\geq\n5^-$) and non-storm cases. As solar wind-driven models are limited in their\nability to predict the onset of transient-driven activity we also introduce a\nmodel variant using solar X-ray flux to assess whether simple models including\nproxies for solar activity can improve the predictions of geomagnetic storm\nactivity with lead times longer than the L1-to-Earth propagation time. By\ncomparing the performance of these models we show that including\noperationally-available information about solar irradiance enhances the ability\nof predictive models to capture the onset of geomagnetic storms and that this\ncan be achieved while also enabling probabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:14:12 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chakraborty", "S.", ""], ["Morley", "S. K.", ""]]}, {"id": "2007.02763", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in", "title": "Yield curve and macroeconomy interaction: evidence from the\n  non-parametric functional lagged regression approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewing a yield curve as a sparse collection of measurements on a latent\ncontinuous random function allows us to model it statistically as a sparsely\nobserved functional time series. Doing so, we use the state-of-the-art methods\nin non-parametric statistical inference for sparsely observed functional time\nseries to analyse the lagged regression dependence of the US Treasury yield\ncurve on US macroeconomic variables. Our non-parametric analysis confirms\nprevious findings established under parametric assumptions, namely a strong\nimpact of the federal funds rate on the short end of the yield curve and a\nmoderate effect of the annual inflation on the longer end of the yield curve.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:07:38 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""]]}, {"id": "2007.02789", "submitter": "J\\\"orn Diedrichsen", "authors": "J\\\"orn Diedrichsen, Eva Berlot, Marieke Mur, Heiko H. Sch\\\"utt,\n  Mahdiyar Shahbazi, Nikolaus Kriegeskorte", "title": "Comparing representational geometries using whitened\n  unbiased-distance-matrix similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representational similarity analysis (RSA) tests models of brain computation\nby investigating how neural activity patterns reflect experimental conditions.\nInstead of predicting activity patterns directly, the models predict the\ngeometry of the representation, as defined by the representational\ndissimilarity matrix (RDM), which captures to what extent experimental\nconditions are associated with similar or dissimilar activity patterns. RSA\ntherefore first quantifies the representational geometry by calculating a\ndissimilarity measure for each pair of conditions, and then compares the\nestimated representational dissimilarities to those predicted by each model.\nHere we address two central challenges of RSA: First, dissimilarity measures\nsuch as the Euclidean, Mahalanobis, and correlation distance, are biased by\nmeasurement noise, which can lead to incorrect inferences. Unbiased\ndissimilarity estimates can be obtained by crossvalidation, at the price of\nincreased variance. Second, the pairwise dissimilarity estimates are not\nstatistically independent, and ignoring this dependency makes model comparison\nstatistically suboptimal. We present an analytical expression for the mean and\n(co)variance of both biased and unbiased estimators of the squared Euclidean\nand Mahalanobis distance, allowing us to quantify the bias-variance trade-off.\nWe also use the analytical expression of the covariance of the dissimilarity\nestimates to whiten the RDM estimation errors. This results in a new criterion\nfor RDM similarity, the whitened unbiased RDM cosine similarity (WUC), which\nallows for near-optimal model selection combined with robustness to correlated\nmeasurement noise.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:43:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 19:19:13 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 14:21:05 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 22:07:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Diedrichsen", "J\u00f6rn", ""], ["Berlot", "Eva", ""], ["Mur", "Marieke", ""], ["Sch\u00fctt", "Heiko H.", ""], ["Shahbazi", "Mahdiyar", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "2007.03016", "submitter": "Yajuan Si", "authors": "Yajuan Si, Steve Heeringa, David Johnson, Roderick Little, Wenshuo\n  Liu, Fabian Pfeffer and Trivellore Raghunathan", "title": "Multiple Imputation with Massive Data: An Application to the Panel Study\n  of Income Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is a popular and well-established method for\nhandling missing data in multivariate data sets, but its practicality for use\nin massive and complex data sets has been questioned. One such data set is the\nPanel Study of Income Dynamics (PSID), a longstanding and extensive survey of\nhousehold income and wealth in the United States. Missing data for this survey\nare currently handled using traditional hot deck methods. We use a sequential\nregression/ chained-equation approach, using the software IVEware, to multiply\nimpute cross-sectional wealth data in the 2013 PSID, and compare analyses of\nthe resulting imputed data with results from the current hot deck approach.\nPractical difficulties, such as non-normally distributed variables, skip\npatterns, categorical variables with many levels, and multicollinearity, are\ndescribed together with our approaches to overcoming them. We evaluate the\nimputation quality and validity with internal diagnostics and external\nbenchmarking data. MI produces improvements over the existing hot deck approach\nby helping preserve correlation structures with efficiency gains. We recommend\nthe practical implementation of MI and expect greater gains when the fraction\nof missing information is large.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:05:17 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 23:14:43 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 02:35:36 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Si", "Yajuan", ""], ["Heeringa", "Steve", ""], ["Johnson", "David", ""], ["Little", "Roderick", ""], ["Liu", "Wenshuo", ""], ["Pfeffer", "Fabian", ""], ["Raghunathan", "Trivellore", ""]]}, {"id": "2007.03031", "submitter": "Barry Loneck PhD", "authors": "Barry Loneck and Igor Zurbenko", "title": "Theoretical and Practical Limits of Kolmogorov-Zurbenko Periodograms\n  with DiRienzo-Zurbenko Algorithm Smoothing in the Spectral Analysis of Time\n  Series Data", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kolomogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm\nsmoothing is the state-of-the-art method for spectral analysis of time series\ndata. Because this approach assumes that a sinusoidal model underlies\ntime-series data and because its algorithms are adaptive in nature, it is\nsuperior to traditional use of autoregressive integral moving average (ARIMA)\nalgorithms. This article begins with a presentation of its statistical\nderivation and development followed by instructions for accessing and utilizing\nthis approach within the R statistical program platform. The discussion then\nturns to a presentation of its theoretical and practical limits with regard to\nsensitivity (i.e., ability to detect weak signals), accuracy (i.e., ability to\ncorrectly identify signal frequencies), resolution (i.e., ability to resolve\nsignals with close frequencies), and robustness with respect to missing data\n(i.e., sensitivity and accuracy despite high levels of missingness). Next using\na simulated time series in which two signals close in frequency are embedded in\nsignificant amounts of random noise, the predictive power of this approach is\ncompared to the traditional ARIMA approach, with support also garnered for its\nbeing robust even in the face of significant levels of missing data. The\narticle concludes with brief descriptions of studies across a range of\nscientific disciplines that have capitalized on the power of the\nKolmogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm smoothing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:28:55 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Loneck", "Barry", ""], ["Zurbenko", "Igor", ""]]}, {"id": "2007.03100", "submitter": "Emiliano Valdez", "authors": "Banghee So and Jean-Philippe Boucher and Emiliano A. Valdez", "title": "Cost-sensitive Multi-class AdaBoost for Understanding Driving Behavior\n  with Telematics", "comments": "27 pages, 9 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered with telematics technology, insurers can now capture a wide range of\ndata, such as distance traveled, how drivers brake, accelerate or make turns,\nand travel frequency each day of the week, to better decode driver's behavior.\nSuch additional information helps insurers improve risk assessments for\nusage-based insurance (UBI), an increasingly popular industry innovation. In\nthis article, we explore how to integrate telematics information to better\npredict claims frequency. For motor insurance during a policy year, we\ntypically observe a large proportion of drivers with zero claims, a less\nproportion with exactly one claim, and far lesser with two or more claims. We\nintroduce the use of a cost-sensitive multi-class adaptive boosting (AdaBoost)\nalgorithm, which we call SAMME.C2, to handle such imbalances. To calibrate\nSAMME.C2 algorithm, we use empirical data collected from a telematics program\nin Canada and we find improved assessment of driving behavior with telematics\nrelative to traditional risk variables. We demonstrate our algorithm can\noutperform other models that can handle class imbalances: SAMME, SAMME with\nSMOTE, RUSBoost, and SMOTEBoost. The sampled data on telematics were\nobservations during 2013-2016 for which 50,301 are used for training and\nanother 21,574 for testing. Broadly speaking, the additional information\nderived from vehicle telematics helps refine risk classification of drivers of\nUBI.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:26:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["So", "Banghee", ""], ["Boucher", "Jean-Philippe", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2007.03238", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, P. Fearnhead", "title": "Innovative And Additive Outlier Robust Kalman Filtering With A Robust\n  Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CE-BASS, a particle mixture Kalman filter which is\nrobust to both innovative and additive outliers, and able to fully capture\nmulti-modality in the distribution of the hidden state. Furthermore, the\nparticle sampling approach re-samples past states, which enables CE-BASS to\nhandle innovative outliers which are not immediately visible in the\nobservations, such as trend changes. The filter is computationally efficient as\nwe derive new, accurate approximations to the optimal proposal distributions\nfor the particles. The proposed algorithm is shown to compare well with\nexisting approaches and is applied to both machine temperature and server data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:11:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2007.03297", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Yang Yang", "title": "Forecasting Australian subnational age-specific mortality rates", "comments": "27 pages, 9 figures", "journal-ref": "Journal of Population Research (2020)", "doi": "10.1007/s12546-020-09250-0", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling sub-national mortality rates, it is important to incorporate\nany possible correlation among sub-populations to improve forecast accuracy.\nMoreover, forecasts at the sub-national level should aggregate consistently\nacross the forecasts at the national level. In this study, we apply a grouped\nmultivariate functional time series to forecast Australian regional and remote\nage-specific mortality rates and reconcile forecasts in a group structure using\nvarious methods. Our proposed method compares favorably to a grouped univariate\nfunctional time series forecasting method by comparing one-step-ahead to\nfive-step-ahead point forecast accuracy. Thus, we demonstrate that joint\nmodeling of sub-populations with similar mortality patterns can improve point\nforecast accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:28:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shang", "Han Lin", ""], ["Yang", "Yang", ""]]}, {"id": "2007.03303", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Rapha\\\"el Nedellec,\n  Yannig Goude", "title": "qgam: Bayesian non-parametric quantile regression modelling in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) are flexible non-linear regression models,\nwhich can be fitted efficiently using the approximate Bayesian methods provided\nby the mgcv R package. While the GAM methods provided by mgcv are based on the\nassumption that the response distribution is modelled parametrically, here we\ndiscuss more flexible methods that do not entail any parametric assumption. In\nparticular, this article introduces the qgam package, which is an extension of\nmgcv providing fast calibrated Bayesian methods for fitting quantile GAMs\n(QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of\nKoenker (2005), rather than on a likelihood function, hence jointly achieving\nsatisfactory accuracy of the quantile point estimates and coverage of the\ncorresponding credible intervals requires adopting the specialized Bayesian\nfitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here\nwe detail how this framework is implemented in qgam and we provide examples\nillustrating how the package should be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:32:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""], ["Zaffran", "Margaux", ""], ["Nedellec", "Rapha\u00ebl", ""], ["Goude", "Yannig", ""]]}, {"id": "2007.03488", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein, Carola Doerr, Daan van den Berg, Jakob\n  Bossek, Sowmya Chandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal\n  Kerschke, William La Cava, Manuel Lopez-Ibanez, Katherine M. Malan, Jason H.\n  Moore, Boris Naujoks, Patryk Orzechowski, Vanessa Volz, Markus Wagner, Thomas\n  Weise", "title": "Benchmarking in Optimization: Best Practice and Open Issues", "comments": "Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PF math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey compiles ideas and recommendations from more than a dozen\nresearchers with different backgrounds and from different institutes around the\nworld. Promoting best practice in benchmarking is its main goal. The article\ndiscusses eight essential topics in benchmarking: clearly stated goals,\nwell-specified problems, suitable algorithms, adequate performance measures,\nthoughtful analysis, effective and efficient designs, comprehensible\npresentations, and guaranteed reproducibility. The final goal is to provide\nwell-accepted guidelines (rules) that might be useful for authors and\nreviewers. As benchmarking in optimization is an active and evolving field of\nresearch this manuscript is meant to co-evolve over time by means of periodic\nupdates.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:20:26 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:36:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""], ["Doerr", "Carola", ""], ["Berg", "Daan van den", ""], ["Bossek", "Jakob", ""], ["Chandrasekaran", "Sowmya", ""], ["Eftimov", "Tome", ""], ["Fischbach", "Andreas", ""], ["Kerschke", "Pascal", ""], ["La Cava", "William", ""], ["Lopez-Ibanez", "Manuel", ""], ["Malan", "Katherine M.", ""], ["Moore", "Jason H.", ""], ["Naujoks", "Boris", ""], ["Orzechowski", "Patryk", ""], ["Volz", "Vanessa", ""], ["Wagner", "Markus", ""], ["Weise", "Thomas", ""]]}, {"id": "2007.03682", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Daniel H\\\"orcher, Daniel J. Graham", "title": "A Dynamic Choice Model with Heterogeneous Decision Rules: Application in\n  Estimating the User Cost of Rail Crowding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowding valuation of subway riders is an important input to various\nsupply-side decisions of transit operators. The crowding cost perceived by a\ntransit rider is generally estimated by capturing the trade-off that the rider\nmakes between crowding and travel time while choosing a route. However,\nexisting studies rely on static compensatory choice models and fail to account\nfor inertia and the learning behaviour of riders. To address these challenges,\nwe propose a new dynamic latent class model (DLCM) which (i) assigns riders to\nlatent compensatory and inertia/habit classes based on different decision\nrules, (ii) enables transitions between these classes over time, and (iii)\nadopts instance-based learning theory to account for the learning behaviour of\nriders. We use the expectation-maximisation algorithm to estimate DLCM, and the\nmost probable sequence of latent classes for each rider is retrieved using the\nViterbi algorithm. The proposed DLCM can be applied in any choice context to\ncapture the dynamics of decision rules used by a decision-maker. We demonstrate\nits practical advantages in estimating the crowding valuation of an Asian\nmetro's riders. To calibrate the model, we recover the daily route preferences\nand in-vehicle crowding experiences of regular metro riders using a\ntwo-month-long smart card and vehicle location data. The results indicate that\nthe average rider follows the compensatory rule on only 25.5% of route choice\noccasions. DLCM estimates also show an increase of 47% in metro riders'\nvaluation of travel time under extremely crowded conditions relative to that\nunder uncrowded conditions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:26:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bansal", "Prateek", ""], ["H\u00f6rcher", "Daniel", ""], ["Graham", "Daniel J.", ""]]}, {"id": "2007.03699", "submitter": "Michael Muthukrishna", "authors": "Carl Falk and Michael Muthukrishna", "title": "Parsimony in Model Selection: Tools for Assessing Fit Propensity", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theories can be represented as statistical models for empirical testing.\nThere is a vast literature on model selection and multimodel inference that\nfocuses on how to assess which statistical model, and therefore which theory,\nbest fits the available data. For example, given some data, one can compare\nmodels on various information criterion or other fit statistics. However, what\nthese indices fail to capture is the full range of counterfactuals. That is,\nsome models may fit the given data better not because they represent a more\ncorrect theory, but simply because these models have more fit propensity - a\ntendency to fit a wider range of data, even nonsensical data, better. Current\napproaches fall short in considering the principle of parsimony (Occam's\nRazor), often equating it with the number of model parameters. Here we offer a\ntoolkit for researchers to better study and understand parsimony through the\nfit propensity of Structural Equation Models. We provide an R package\n(ockhamSEM) built on the popular lavaan package. To illustrate the importance\nof evaluating fit propensity, we use ockhamSEM to investigate the factor\nstructure of the Rosenberg Self-Esteem Scale.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:00:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Falk", "Carl", ""], ["Muthukrishna", "Michael", ""]]}, {"id": "2007.03722", "submitter": "David Ginsbourger", "authors": "Trygve Olav Fossum, C\\'edric Travelletti, Jo Eidsvik, David\n  Ginsbourger, Kanna Rajan", "title": "Learning excursion sets of vector-valued Gaussian random fields for\n  autonomous ocean sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving and optimizing oceanographic sampling is a crucial task for marine\nscience and maritime resource management. Faced with limited resources in\nunderstanding processes in the water-column, the combination of statistics and\nautonomous systems provide new opportunities for experimental design. In this\nwork we develop efficient spatial sampling methods for characterizing regions\ndefined by simultaneous exceedances above prescribed thresholds of several\nresponses, with an application focus on mapping coastal ocean phenomena based\non temperature and salinity measurements. Specifically, we define a design\ncriterion based on uncertainty in the excursions of vector-valued Gaussian\nrandom fields, and derive tractable expressions for the expected integrated\nBernoulli variance reduction in such a framework. We demonstrate how this\ncriterion can be used to prioritize sampling efforts at locations that are\nambiguous, making exploration more effective. We use simulations to study and\ncompare properties of the considered approaches, followed by results from field\ndeployments with an autonomous underwater vehicle as part of a study mapping\nthe boundary of a river plume. The results demonstrate the potential of\ncombining statistical methods and robotic platforms to effectively inform and\nexecute data-driven environmental sampling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:23:46 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 13:32:27 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Fossum", "Trygve Olav", ""], ["Travelletti", "C\u00e9dric", ""], ["Eidsvik", "Jo", ""], ["Ginsbourger", "David", ""], ["Rajan", "Kanna", ""]]}, {"id": "2007.03732", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Space-time smoothing models for sub-national measles routine\n  immunization coverage estimation with complex survey data", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial advances in global measles vaccination, measles disease\nburden remains high in many low- and middle-income countries. A key public\nhealth strategy for controling measles in such high-burden settings is to\nconduct supplementary immunization activities (SIAs) in the form of mass\nvaccination campaigns, in addition to delivering scheduled vaccination through\nroutine immunization (RI) programs. To achieve balanced implementations of RI\nand SIAs, robust measurement of sub-national RI-specific coverage is crucial.\nIn this paper, we develop a space-time smoothing model for estimating\nRI-specific coverage of the first dose of measles-containing-vaccines (MCV1) at\nsub-national level using complex survey data. The application that motivated\nthis work is estimation of the RI-specific MCV1 coverage in Nigeria's 36 states\nand the Federal Capital Territory. Data come from four Demographic and Health\nSurveys, three Multiple Indicator Cluster Surveys, and two National Nutrition\nand Health Surveys conducted in Nigeria between 2003 and 2018. Our method\nincorporates information from the SIA calendar published by the World Health\nOrganization and accounts for the impact of SIAs on the overall MCV1 coverage,\nas measured by cross-sectional surveys. The model can be used to analyze data\nfrom multiple surveys with different data collection schemes and construct\ncoverage estimates with uncertainty that reflects the various sampling designs.\nImplementation of our method can be done efficiently using integrated nested\nLaplace approximation (INLA).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:44:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2007.03788", "submitter": "Andrei Zinovyev Dr.", "authors": "Sergey E. Golovenkin, Jonathan Bac, Alexander Chervov, Evgeny M.\n  Mirkes, Yuliya V. Orlova, Emmanuel Barillot, Alexander N. Gorban, and Andrei\n  Zinovyev", "title": "Trajectories, bifurcations and pseudotime in large clinical datasets:\n  applications to myocardial infarction and diabetes data", "comments": null, "journal-ref": "GigaScience, Volume 9, Issue 11, 2020, giaa128,", "doi": "10.1093/gigascience/giaa128", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large observational clinical datasets become increasingly available for\nmining associations between various disease traits and administered therapy.\nThese datasets can be considered as representations of the landscape of all\npossible disease conditions, in which a concrete pathology develops through a\nnumber of stereotypical routes, characterized by `points of no return' and\n`final states' (such as lethal or recovery states). Extracting this information\ndirectly from the data remains challenging, especially in the case of\nsynchronic (with a short-term follow up) observations. Here we suggest a\nsemi-supervised methodology for the analysis of large clinical datasets,\ncharacterized by mixed data types and missing values, through modeling the\ngeometrical data structure as a bouquet of bifurcating clinical trajectories.\nThe methodology is based on application of elastic principal graphs which can\naddress simultaneously the tasks of dimensionality reduction, data\nvisualization, clustering, feature selection and quantifying the geodesic\ndistances (pseudotime) in partially ordered sequences of observations. The\nmethodology allows positioning a patient on a particular clinical trajectory\n(pathological scenario) and characterizing the degree of progression along it\nwith a qualitative estimate of the uncertainty of the prognosis. Overall, our\npseudo-time quantification-based approach gives a possibility to apply the\nmethods developed for dynamical disease phenotyping and illness trajectory\nanalysis (diachronic data analysis) to synchronic observational data. We\ndeveloped a tool $ClinTrajan$ for clinical trajectory analysis implemented in\nPython programming language. We test the methodology in two large publicly\navailable datasets: myocardial infarction complications and readmission of\ndiabetic patients data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:04:55 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 21:57:46 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Golovenkin", "Sergey E.", ""], ["Bac", "Jonathan", ""], ["Chervov", "Alexander", ""], ["Mirkes", "Evgeny M.", ""], ["Orlova", "Yuliya V.", ""], ["Barillot", "Emmanuel", ""], ["Gorban", "Alexander N.", ""], ["Zinovyev", "Andrei", ""]]}, {"id": "2007.03792", "submitter": "Alexander Kaizer", "authors": "Alexander M. Kaizer, Joseph S. Koopmeiners, Nan Chen, Brian P. Hobbs", "title": "Statistical design considerations for trials that study multiple\n  indications", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breakthroughs in cancer biology have defined new research programs\nemphasizing the development of therapies that target specific pathways in tumor\ncells. Innovations in clinical trial design have followed with master protocols\ndefined by inclusive eligibility criteria and evaluations of multiple therapies\nand/or histologies. Consequently, characterization of subpopulation\nheterogeneity has become central to the formulation and selection of a study\ndesign. However, this transition to master protocols has led to challenges in\nidentifying the optimal trial design and proper calibration of hyperparameters.\nWe often evaluate a range of null and alternative scenarios, however there has\nbeen little guidance on how to synthesize the potentially disparate\nrecommendations for what may be optimal. This may lead to the selection of\nsuboptimal designs and statistical methods that do not fully accommodate the\nsubpopulation heterogeneity. This article proposes novel optimization criteria\nfor calibrating and evaluating candidate statistical designs of master\nprotocols in the presence of the potential for treatment effect heterogeneity\namong enrolled patient subpopulations. The framework is applied to demonstrate\nthe statistical properties of conventional study designs when treatments offer\nheterogeneous benefit as well as identify optimal designs devised to monitor\nthe potential for heterogeneity among patients with differing clinical\nindications using Bayesian modeling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:20:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kaizer", "Alexander M.", ""], ["Koopmeiners", "Joseph S.", ""], ["Chen", "Nan", ""], ["Hobbs", "Brian P.", ""]]}, {"id": "2007.03870", "submitter": "Sujay Mukhoti", "authors": "Soham Ghosh, Mamta Sahare and Sujay Mukhoti", "title": "A new generalized newsvendor model with random demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newsvendor problem is an extensively researched topic in inventory\nmanagement. In this class of inventory problems, shortage and excess costs are\nconsidered to be proportional to the quantity lost. But, for critical goods or\ncommodities, inventory decision is a typical example where, excess or shortage\nmay lead to greater losses than merely the total cost. Such a problem has not\nbeen discussed much in the literature. Moreover, majority of the existing\nliterature assumes the demand distribution to be completely known. In this\npaper, we propose a generalization of the newsvendor problem for critical goods\nor commodities with higher shortage or excess losses but of same degree. We\nalso assume that, the parameters of the demand distribution are unknown. We\nalso discuss different estimators of the optimal order quantity based on a\nrandom sample of demand. In particular, we provide different estimators based\non (i) full sample and (ii) broken sample data (i.e with single order\nstatistic). We also report comparison of the estimators using simulated bias\nand mean square error (MSE).\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 03:02:37 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ghosh", "Soham", ""], ["Sahare", "Mamta", ""], ["Mukhoti", "Sujay", ""]]}, {"id": "2007.04005", "submitter": "Kirien Whan", "authors": "Simon Veldkamp, Kirien Whan, Sjoerd Dirksen and Maurice Schmeits", "title": "Statistical post-processing of wind speed forecasts using convolutional\n  neural networks", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": "10.1175/MWR-D-20-0219.1", "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical post-processing methods for probabilistic weather\nforecasting are not capable of using full spatial patterns from the numerical\nweather prediction (NWP) model. In this paper we incorporate spatial wind speed\ninformation by using convolutional neural networks (CNNs) and obtain\nprobabilistic wind speed forecasts in the Netherlands for 48 hours ahead, based\non KNMI's deterministic Harmonie-Arome NWP model. The probabilistic forecasts\nfrom the CNNs are shown to have higher Brier skill scores for medium to higher\nwind speeds, as well as a better continuous ranked probability score (CRPS) and\nlogarithmic score, than the forecasts from fully connected neural networks and\nquantile regression forests. As a secondary result, we have compared the CNNs\nusing 3 different density estimation methods (quantized softmax (QS), kernel\nmixture networks, and fitting a truncated normal distribution), and found the\nprobabilistic forecasts based on the QS method to be best.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:18:07 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 11:49:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Veldkamp", "Simon", ""], ["Whan", "Kirien", ""], ["Dirksen", "Sjoerd", ""], ["Schmeits", "Maurice", ""]]}, {"id": "2007.04009", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Frank Schaarschmidt", "title": "A Tukey type trend test for repeated carcinogenicity bioassays,\n  motivated by multiple glyphosate studies", "comments": "1 fig", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the last two decades, significant methodological progress to the\nsimultaneous inference of simple and complex randomized designs, particularly\nproportions as endpoints, occurred. This includes: i) the new Tukey trend test\napproach, ii) multiple contrast tests for binomial proportions, iii) multiple\ncontrast tests for poly-k estimates, and Add-1 approximation for one-sided\ninference. This report focus on a new Tukey type trend test to evaluate\nrepeated long-term carcinogenicity bioassays which was motivated by multiple\nglyphosate studies. Notice, it is not the aim here to contribute to the\nevaluation of Glyphosate and its controversies. By means of the CRAN-packages\ntukeytrend, MCPAN, multcomp the real data analysis is straightforward possible.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:26:47 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Schaarschmidt", "Frank", ""]]}, {"id": "2007.04037", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Deborah Blacker, Eric B. Larson, Sebastien Haneuse", "title": "Modeling semi-competing risks data as a longitudinal bivariate process", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adult Changes in Thought (ACT) study is a long-running prospective study\nof incident all-cause dementia and Alzheimer's disease (AD). As the cohort\nages, death (a terminal event) is a prominent competing risk for AD (a\nnon-terminal event), although the reverse is not the case. As such, analyses of\ndata from ACT can be placed within the semi-competing risks framework. Central\nto semi-competing risks, and in contrast to standard competing risks, is that\none can learn about the dependence structure between the two events. To-date,\nhowever, most methods for semi-competing risks treat dependence as a nuisance\nand not a potential source of new clinical knowledge. We propose a novel\nregression-based framework that views the two time-to-event outcomes through\nthe lens of a longitudinal bivariate process on a partition of the time scale.\nA key innovation of the framework is that dependence is represented in two\ndistinct forms, $\\textit{local}$ and $\\textit{global}$ dependence, both of\nwhich have intuitive clinical interpretations. Estimation and inference are\nperformed via penalized maximum likelihood, and can accommodate right\ncensoring, left truncation and time-varying covariates. The framework is used\nto investigate the role of gender and having $\\ge$1 APOE-$\\epsilon4$ allele on\nthe joint risk of AD and death.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:28:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Nevo", "Daniel", ""], ["Blacker", "Deborah", ""], ["Larson", "Eric B.", ""], ["Haneuse", "Sebastien", ""]]}, {"id": "2007.04168", "submitter": "Michael Grayling", "authors": "Michael J. Grayling and Adrian P. Mander", "title": "Two-stage single-arm trials are rarely reported adequately", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Two-stage single-arm trial designs are commonly used in phase II\noncology to infer treatment effects for a binary primary outcome (e.g., tumour\nresponse). It is imperative that such studies be designed, analysed, and\nreported effectively. However, there is little available evidence on whether\nthis is the case, particularly for key statistical considerations. We therefore\ncomprehensively review such trials, examining in particular quality of\nreporting. Methods: Published oncology trials that utilised \"Simon's two-stage\ndesign\" over a 5 year period were identified and reviewed. Articles were\nevaluated on whether they reported sufficient design details, such as the\nrequired sample size, and analysis details, such as a confidence interval (CI).\nThe articles that did not adjust their inference for the incorporation of an\ninterim analysis were re-analysed to evaluate the impact on their reported\npoint estimate and CI. Results: Four hundred and twenty five articles that\nreported the results of a single treatment arm were included. Of these, 47.5%\nprovided the five components that ensure design reproducibility. Only 1.2% and\n2.1% reported an adjusted point estimate or CI, respectively. Just 55.3% of\ntrials provided the final stage rejection bound, indicating many trials did not\ntest the hypothesis the design is constructed to assess. Re-analysis of the\ntrials suggests that reported point estimates underestimated treatment effects\nand that reported CIs were too narrow. Conclusion: Key design details of\ntwo-stage single-arm trials are often unreported. Whilst inference is regularly\nperformed, it is rarely done so in a way that removes the bias introduced by\nthe interim analysis. In order to maximise their value, future studies must\nimprove the way that they are analysed and reported.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:55:29 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Grayling", "Michael J.", ""], ["Mander", "Adrian P.", ""]]}, {"id": "2007.04410", "submitter": "Aditi Shenvi", "authors": "F.O.Bunnin, A.Shenvi, J.Q.Smith", "title": "Network Modelling of Criminal Collaborations with Dynamic Bayesian\n  Steady Evolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The threat status and criminal collaborations of potential terrorists are\nhidden but give rise to observable behaviours and communications. Terrorists,\nwhen acting in concert, need to communicate to organise their plots. The\nauthorities utilise such observable behaviour and communication data to inform\ntheir investigations and policing. We present a dynamic latent network model\nthat integrates real-time communications data with prior knowledge on\nindividuals. This model estimates and predicts the latent strength of criminal\ncollaboration between individuals to assist in the identification of potential\ncells and the measurement of their threat levels. We demonstrate how, by\nassuming certain plausible conditional independences across the measurements\nassociated with this population, the network model can be combined with models\nof individual suspects to provide fast transparent algorithms to predict group\nattacks. The methods are illustrated using a simulated example involving the\nthreat posed by a cell suspected of plotting an attack.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:23:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Bunnin", "F. O.", ""], ["Shenvi", "A.", ""], ["Smith", "J. Q.", ""]]}, {"id": "2007.04431", "submitter": "Xianping Du", "authors": "Xianping Du, Hongyi Xu, Feng Zhu", "title": "Understanding the effect of hyperparameter optimization on machine\n  learning models for structure design problems", "comments": "43 pages, 15 figures,8 tables, Accepted by the Computer-aided design", "journal-ref": "Computer-Aided Design (2021): 103013", "doi": "10.1016/j.cad.2021.103013", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To relieve the computational cost of design evaluations using expensive\nfinite element simulations, surrogate models have been widely applied in\ncomputer-aided engineering design. Machine learning algorithms (MLAs) have been\nimplemented as surrogate models due to their capability of learning the complex\ninterrelations between the design variables and the response from big datasets.\nTypically, an MLA regression model contains model parameters and\nhyperparameters. The model parameters are obtained by fitting the training\ndata. Hyperparameters, which govern the model structures and the training\nprocesses, are assigned by users before training. There is a lack of systematic\nstudies on the effect of hyperparameters on the accuracy and robustness of the\nsurrogate model. In this work, we proposed to establish a hyperparameter\noptimization (HOpt) framework to deepen our understanding of the effect. Four\nfrequently used MLAs, namely Gaussian Process Regression (GPR), Support Vector\nMachine (SVM), Random Forest Regression (RFR), and Artificial Neural Network\n(ANN), are tested on four benchmark examples. For each MLA model, the model\naccuracy and robustness before and after the HOpt are compared. The results\nshow that HOpt can generally improve the performance of the MLA models in\ngeneral. HOpt leads to few improvements in the MLAs accuracy and robustness for\ncomplex problems, which are featured by high-dimensional mixed-variable design\nspace. The HOpt is recommended for the design problems with intermediate\ncomplexity. We also investigated the additional computational costs incurred by\nHOpt. The training cost is closely related to the MLA architecture. After HOpt,\nthe training cost of ANN and RFR is increased more than that of the GPR and\nSVM. To sum up, this study benefits the selection of HOpt method for the\ndifferent types of design problems based on their complexity.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:57:34 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 22:14:58 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Du", "Xianping", ""], ["Xu", "Hongyi", ""], ["Zhu", "Feng", ""]]}, {"id": "2007.04446", "submitter": "Brian Lucena", "authors": "Brian Lucena", "title": "StructureBoost: Efficient Gradient Boosting for Structured Categorical\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting methods based on Structured Categorical Decision Trees\n(SCDT) have been demonstrated to outperform numerical and one-hot-encodings on\nproblems where the categorical variable has a known underlying structure.\nHowever, the enumeration procedure in the SCDT is infeasible except for\ncategorical variables with low or moderate cardinality. We propose and\nimplement two methods to overcome the computational obstacles and efficiently\nperform Gradient Boosting on complex structured categorical variables. The\nresulting package, called StructureBoost, is shown to outperform established\npackages such as CatBoost and LightGBM on problems with categorical predictors\nthat contain sophisticated structure. Moreover, we demonstrate that\nStructureBoost can make accurate predictions on unseen categorical values due\nto its knowledge of the underlying structure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:37:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lucena", "Brian", ""]]}, {"id": "2007.04509", "submitter": "Briana Stephenson", "authors": "Briana Stephenson, Amy Herring, Andrew Olshan", "title": "Supervised Robust Profile Clustering", "comments": "30 pages, 3 figures, Supplementary materials (5 figures, 1 table)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many studies, dimension reduction methods are used to profile participant\ncharacteristics. For example, nutrition epidemiologists often use latent class\nmodels to characterize dietary patterns. One challenge with such approaches is\nunderstanding subtle variations in patterns across subpopulations. Robust\nProfile Clustering (RPC) provides a dual flexible clustering model, where\nparticipants may cluster at two levels: (1) globally, where participants are\nclustered according to behaviors shared across an overall population, and (2)\nlocally, where individual behaviors can deviate and cluster in subpopulations.\nWe link clusters to a health outcome using a joint model. This model is used to\nderive dietary patterns in the United States and evaluate case proportion of\norofacial clefts. Using dietary consumption data from the 1997-2009 National\nBirth Defects Prevention Study, a population-based case-control study, we\ndetermine how maternal dietary profiles are associated with an orofacial cleft\namong offspring. Results indicated that mothers who consumed a high proportion\nof fruits and vegetables compared to meats, such as chicken and beef, had lower\nodds delivering a child with an orofacial cleft defect.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:13:05 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Stephenson", "Briana", ""], ["Herring", "Amy", ""], ["Olshan", "Andrew", ""]]}, {"id": "2007.04553", "submitter": "Feiyu Jiang", "authors": "Feiyu Jiang, Zifeng Zhao, Xiaofeng Shao", "title": "Time Series Analysis of COVID-19 Infection Curve: A Change-Point\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the trajectory of the cumulative confirmed cases and\ndeaths of COVID-19 (in log scale) via a piecewise linear trend model. The model\nnaturally captures the phase transitions of the epidemic growth rate via\nchange-points and further enjoys great interpretability due to its\nsemiparametric nature. On the methodological front, we advance the nascent\nself-normalization (SN) technique (Shao, 2010) to testing and estimation of a\nsingle change-point in the linear trend of a nonstationary time series. We\nfurther combine the SN-based change-point test with the NOT algorithm\n(Baranowski et al., 2019) to achieve multiple change-point estimation. Using\nthe proposed method, we analyze the trajectory of the cumulative COVID-19 cases\nand deaths for 30 major countries and discover interesting patterns with\npotentially relevant implications for effectiveness of the pandemic responses\nby different countries. Furthermore, based on the change-point detection\nalgorithm and a flexible extrapolation function, we design a simple two-stage\nforecasting scheme for COVID-19 and demonstrate its promising performance in\npredicting cumulative deaths in the U.S.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:46:13 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jiang", "Feiyu", ""], ["Zhao", "Zifeng", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2007.04558", "submitter": "Dehan Kong", "authors": "Dengdeng Yu, Linbo Wang, Dehan Kong, Hongtu Zhu", "title": "Beyond Scalar Treatment: A Causal Analysis of Hippocampal Atrophy on\n  Behavioral Deficits in Alzheimer's Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is a progressive form of dementia that results in\nproblems with memory, thinking and behavior. It often starts with abnormal\naggregation and deposition of beta-amyloid and tau, followed by neuronal damage\nsuch as atrophy of the hippocampi, and finally leads to behavioral deficits.\nDespite significant progress in finding biomarkers associated with behavioral\ndeficits, the underlying causal mechanism remains largely unknown. Here we\ninvestigate whether and how hippocampal atrophy contributes to behavioral\ndeficits based on a large-scale observational study conducted by the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). As a key novelty, we use 2D\nrepresentations of the hippocampi, which allows us to better understand atrophy\nassociated with different subregions. It, however, introduces methodological\nchallenges as existing causal inference methods are not well suited for\nexploiting structural information embedded in the 2D exposures. Moreover, our\ndata contain more than 6 million clinical and genetic covariates, necessitating\nappropriate confounder selection methods. We hence develop a novel two-step\ncausal inference approach tailored for our ADNI data application. Analysis\nresults suggest that atrophy of CA1 and subiculum subregions may cause more\nsevere behavioral deficits compared to CA2 and CA3 subregions. We further\nevaluate our method using simulations and provide theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:08:38 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yu", "Dengdeng", ""], ["Wang", "Linbo", ""], ["Kong", "Dehan", ""], ["Zhu", "Hongtu", ""]]}, {"id": "2007.04697", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova", "title": "Open Data Quality Evaluation: A Comparative Analysis of Open Data in\n  Latvia", "comments": "24 pages, 2 tables, 3 figures, Baltic J. Modern Computing", "journal-ref": "Baltic J. Modern Computing, Vol. 6(2018), No. 4, 363-386", "doi": "10.22364/bjmc.2018.6.4.04", "report-no": null, "categories": "cs.DB cs.CY cs.IR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays open data is entering the mainstream - it is free available for\nevery stakeholder and is often used in business decision-making. It is\nimportant to be sure data is trustable and error-free as its quality problems\ncan lead to huge losses. The research discusses how (open) data quality could\nbe assessed. It also covers main points which should be considered developing a\ndata quality management solution. One specific approach is applied to several\nLatvian open data sets. The research provides a step-by-step open data sets\nanalysis guide and summarizes its results. It is also shown there could exist\ndifferences in data quality depending on data supplier (centralized and\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\nguarantee data quality problems absence. There are also underlined common data\nquality problems detected not only in Latvian open data but also in open data\nof 3 European countries.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:43:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nikiforova", "Anastasija", ""]]}, {"id": "2007.04727", "submitter": "Dr. Wolfgang A. Rolke", "authors": "Wolfgang Rolke", "title": "Supplemental Studies for Simultaneous Goodness-of-Fit Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Testing to see whether a given data set comes from some specified\ndistribution is among the oldest types of problems in Statistics. Many such\ntests have been developed and their performance studied. The general result has\nbeen that while a certain test might perform well, aka have good power, in one\nsituation it will fail badly in others. This is not a surprise given the great\nmany ways in which a distribution can differ from the one specified in the null\nhypothesis. It is therefore very difficult to decide a priori which test to\nuse. The obvious solution is not to rely on any one test but to run several of\nthem. This however leads to the problem of simultaneous inference, that is, if\nseveral tests are done even if the null hypothesis were true, one of them is\nlikely to reject it anyway just by random chance. In this paper we present a\nmethod that yields a p value that is uniform under the null hypothesis no\nmatter how many tests are run. This is achieved by adjusting the p value via\nsimulation. While this adjustment method is not new, it has not previously been\nused in the context of goodness-of-fit testing. We present a number of\nsimulation studies that show the uniformity of the p value and others that show\nthat this test is superior to any one test if the power is averaged over a\nlarge number of cases.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:53:27 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 14:06:23 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Rolke", "Wolfgang", ""]]}, {"id": "2007.04743", "submitter": "James Unwin", "authors": "Yunseo Choi and James Unwin", "title": "Racial Impact on Infections and Deaths due to COVID-19 in New York City", "comments": "6 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redlining is the discriminatory practice whereby institutions avoided\ninvestment in certain neighborhoods due to their demographics. Here we explore\nthe lasting impacts of redlining on the spread of COVID-19 in New York City\n(NYC). Using data available through the Home Mortgage Disclosure Act, we\nconstruct a redlining index for each NYC census tract via a multi-level\nlogistical model. We compare this redlining index with the COVID-19 statistics\nfor each NYC Zip Code Tabulation Area. Accurate mappings of the pandemic would\naid the identification of the most vulnerable areas and permit the most\neffective allocation of medical resources, while reducing ethnic health\ndisparities.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 12:27:59 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Choi", "Yunseo", ""], ["Unwin", "James", ""]]}, {"id": "2007.04767", "submitter": "Dominic Magirr Dr", "authors": "Dominic Magirr", "title": "Non-proportional hazards in immuno-oncology: is an old perspective\n  needed?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental concept in two-arm non-parametric survival analysis is the\ncomparison of observed versus expected numbers of events on one of the\ntreatment arms (the choice of which arm is arbitrary), where the expectation is\ntaken assuming that the true survival curves in the two arms are identical.\nThis concept is at the heart of the counting-process theory that provides a\nrigorous basis for methods such as the log-rank test. It is natural, therefore,\nto maintain this perspective when extending the log-rank test to deal with\nnon-proportional hazards, for example by considering a weighted sum of the\n\"observed - expected\" terms, where larger weights are given to time periods\nwhere the hazard ratio is expected to favour the experimental treatment. In\ndoing so, however, one may stumble across some rather subtle issues, related to\nthe difficulty in ascribing a causal interpretation to hazard ratios, that may\nlead to strange conclusions. An alternative approach is to view non-parametric\nsurvival comparisons as permutation tests. With this perspective, one can\neasily improve on the efficiency of the log-rank test, whilst thoroughly\ncontrolling the false positive rate. In particular, for the field of\nimmuno-oncology, where researchers often anticipate a delayed treatment effect,\nsample sizes could be substantially reduced without loss of power.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:15:19 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Magirr", "Dominic", ""]]}, {"id": "2007.04793", "submitter": "Tom Needham", "authors": "Xiaoyang Guo, Aditi Basu Bal, Tom Needham, Anuj Srivastava", "title": "Statistical shape analysis of brain arterial networks (BAN)", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.00287", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structures of brain arterial networks (BANs) - that are complex arrangements\nof individual arteries, their branching patterns, and inter-connectivities -\nplay an important role in characterizing and understanding brain physiology.\nOne would like tools for statistically analyzing the shapes of BANs, i.e.\nquantify shape differences, compare population of subjects, and study the\neffects of covariates on these shapes. This paper mathematically represents and\nstatistically analyzes BAN shapes as elastic shape graphs. Each elastic shape\ngraph is made up of nodes that are connected by a number of 3D curves, and\nedges, with arbitrary shapes. We develop a mathematical representation, a\nRiemannian metric and other geometrical tools, such as computations of\ngeodesics, means and covariances, and PCA for analyzing elastic graphs and\nBANs. This analysis is applied to BANs after separating them into four\ncomponents -- top, bottom, left, and right. This framework is then used to\ngenerate shape summaries of BANs from 92 subjects, and to study the effects of\nage and gender on shapes of BAN components. We conclude that while gender\neffects require further investigation, the age has a clear, quantifiable effect\non BAN shapes. Specifically, we find an increased variance in BAN shapes as age\nincreases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:46:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Bal", "Aditi Basu", ""], ["Needham", "Tom", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2007.04951", "submitter": "Thomas Burnett", "authors": "Thomas Burnett, Franz K\\\"onig and Thomas Jaki", "title": "Adding experimental treatment arms to Multi-Arm Multi-Stage platform\n  trials in progress", "comments": "28 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Arm Multi-Stage (MAMS) platform trials are an efficient tool for the\ncomparison of several treatments. Suppose we wish to add a treatment to a trial\nalready in progress, to access the benefits of a MAMS design. How should this\nbe done?\n  The MAMS framework requires pre-planned options for how the trial proceeds at\neach stage in order to control the family-wise error rate. Thus, it is\ndifficult to make both planned and unplanned design modifications. The\nconditional error approach is a tool that allows unplanned design modifications\nwhile maintaining the overall error rate. In this work, we use the conditional\nerror approach to allow adding new arms to a MAMS trial in progress.\n  We demonstrate the principles of incorporating additional hypotheses into the\ntesting structure. Using this framework, we show how to update the testing\nprocedure for a MAMS trial in progress to incorporate additional treatment\narms. Simulations illustrate the possible operating characteristics of such\nprocedures using a fixed rule for how and when the design modification is made.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:26:45 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 14:21:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Burnett", "Thomas", ""], ["K\u00f6nig", "Franz", ""], ["Jaki", "Thomas", ""]]}, {"id": "2007.05010", "submitter": "Prashant Shekhar", "authors": "Prashant Shekhar, Beata Csatho, Tony Schenk, Carolyn Roberts and Abani\n  Patra", "title": "ALPS: A Unified Framework for Modeling Time Series of Land Ice Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling time series is a research focus in cryospheric sciences because of\nthe complexity and multiscale nature of events of interest. Highly non-uniform\nsampling of measurements from different sensors with different levels of\naccuracy, as is typical for measurements of ice sheet elevations, makes the\nproblem even more challenging. In this paper, we propose a spline-based\napproximation framework (ALPS - Approximation by Localized Penalized Splines)\nfor modeling time series of land ice changes. The localized support of the\nB-spline basis functions enable robustness to non-uniform sampling, a\nconsiderable improvement over other global and piecewise local models. With\nfeatures like, discrete-coordinate-difference-based penalization and two-level\noutlier detection, ALPS further guarantees the stability and quality of\napproximations. ALPS incorporates rigorous model uncertainty estimates with all\napproximations. As demonstrated by examples, ALPS performs well for a variety\nof data sets, including time series of ice sheet thickness, elevation,\nvelocity, and terminus locations. The robust estimation of time series and\ntheir derivatives facilitates new applications, such as the reconstruction of\nhigh-resolution elevation change records by fusing sparsely sampled time series\nof ice sheet thickness changes with modeled firn thickness changes, and the\nanalysis of the relationship between different outlet glacier observations to\ngain new insight into processes and forcing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:07:42 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Shekhar", "Prashant", ""], ["Csatho", "Beata", ""], ["Schenk", "Tony", ""], ["Roberts", "Carolyn", ""], ["Patra", "Abani", ""]]}, {"id": "2007.05035", "submitter": "Jonas S. Juul", "authors": "Jonas L. Juul and Kaare Gr{\\ae}sb{\\o}ll and Lasse Engbo Christiansen\n  and Sune Lehmann", "title": "Fixed-time descriptive statistics underestimate extremes of epidemic\n  curve ensembles", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": "10.1038/s41567-020-01121-y", "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across the world, scholars are racing to predict the spread of the novel\ncoronavirus, COVID-19. Such predictions are often pursued by numerically\nsimulating epidemics with a large number of plausible combinations of relevant\nparameters. It is essential that any forecast of the epidemic trajectory\nderived from the resulting ensemble of simulated curves is presented with\nconfidence intervals that communicate the uncertainty associated with the\nforecast. Here we argue that the state-of-the-art approach for summarizing\nensemble statistics does not capture crucial epidemiological information. In\nparticular, the current approach systematically suppresses information about\nthe projected trajectory peaks. The fundamental problem is that each time step\nis treated separately in the statistical analysis. We suggest using curve-based\ndescriptive statistics to summarize trajectory ensembles. The results presented\nallow researchers to report more representative confidence intervals, resulting\nin more realistic projections of epidemic trajectories and -- in turn -- enable\nbetter decision making in the face of the current and future pandemics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:16:18 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Juul", "Jonas L.", ""], ["Gr\u00e6sb\u00f8ll", "Kaare", ""], ["Christiansen", "Lasse Engbo", ""], ["Lehmann", "Sune", ""]]}, {"id": "2007.05057", "submitter": "Tom Lovett", "authors": "Tom Lovett, Mark Briers, Marcos Charalambides, Radka Jersakova, James\n  Lomax and Chris Holmes", "title": "Inferring proximity from Bluetooth Low Energy RSSI with Unscented Kalman\n  Smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 pandemic has resulted in a variety of approaches for managing\ninfection outbreaks in international populations. One example is mobile phone\napplications, which attempt to alert infected individuals and their contacts by\nautomatically inferring two key components of infection risk: the proximity to\nan individual who may be infected, and the duration of proximity. The former\ncomponent, proximity, relies on Bluetooth Low Energy (BLE) Received Signal\nStrength Indicator(RSSI) as a distance sensor, and this has been shown to be\nproblematic; not least because of unpredictable variations caused by different\ndevice types, device location on-body, device orientation, the local\nenvironment and the general noise associated with radio frequency propagation.\nIn this paper, we present an approach that infers posterior probabilities over\ndistance given sequences of RSSI values. Using a single-dimensional Unscented\nKalman Smoother (UKS) for non-linear state space modelling, we outline several\nGaussian process observation transforms, including: a generative model that\ndirectly captures sources of variation; and a discriminative model that learns\na suitable observation function from training data using both distance and\ninfection risk as optimisation objective functions. Our results show that good\nrisk prediction can be achieved in $\\mathcal{O}(n)$ time on real-world data\nsets, with the UKS outperforming more traditional classification methods\nlearned from the same training data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:47:02 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Lovett", "Tom", ""], ["Briers", "Mark", ""], ["Charalambides", "Marcos", ""], ["Jersakova", "Radka", ""], ["Lomax", "James", ""], ["Holmes", "Chris", ""]]}, {"id": "2007.05114", "submitter": "Andrea Arnold", "authors": "Leah Mitchell, Andrea Arnold", "title": "Analyzing the Effects of Observation Function Selection in Ensemble\n  Kalman Filtering for Epidemic Models", "comments": "29 pages, 13 figures. This is the accepted manuscript of an article\n  published in Mathematical Biosciences. The published journal article is\n  available online at https://doi.org/10.1016/j.mbs.2021.108655", "journal-ref": "Mathematical Biosciences 339 (2021) 108655", "doi": "10.1016/j.mbs.2021.108655", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Ensemble Kalman Filter (EnKF) is a popular sequential data assimilation\nmethod that has been increasingly used for parameter estimation and forecast\nprediction in epidemiological studies. The observation function plays a\ncritical role in the EnKF framework, connecting the unknown system variables\nwith the observed data. Key differences in observed data and modeling\nassumptions have led to the use of different observation functions in the\nepidemic modeling literature. In this work, we present a novel computational\nanalysis demonstrating the effects of observation function selection when using\nthe EnKF for state and parameter estimation in this setting. In examining the\nuse of four epidemiologically-inspired observation functions of different forms\nin connection with the classic Susceptible-Infectious-Recovered (SIR) model, we\nshow how incorrect observation modeling assumptions (i.e., fitting incidence\ndata with a prevalence model, or neglecting under-reporting) can lead to\ninaccurate filtering estimates and forecast predictions. Results demonstrate\nthe importance of choosing an observation function that well interprets the\navailable data on the corresponding EnKF estimates in several filtering\nscenarios, including state estimation with known parameters, and combined state\nand parameter estimation with both constant and time-varying parameters.\nNumerical experiments further illustrate how modifying the observation noise\ncovariance matrix in the filter can help to account for uncertainty in the\nobservation function in certain cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:11:48 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:57:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mitchell", "Leah", ""], ["Arnold", "Andrea", ""]]}, {"id": "2007.05117", "submitter": "Zehang Li", "authors": "Zehang Richard Li, Bryan D Martin, Tracy Qi Dong, Geir-Arne Fuglstad,\n  John Paige, Andrea Riebler, Samuel Clark, Jon Wakefield", "title": "Space-Time Smoothing of Demographic and Health Indicators using the R\n  Package SUMMER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of complex survey data, and the continued need\nfor estimates of demographic and health indicators at a fine spatial and\ntemporal scale, which leads to issues of data sparsity, has led to the need for\nspatio-temporal smoothing methods that acknowledge the manner in which the data\nwere collected. The open source R package SUMMER implements a variety of\nmethods for spatial or spatio-temporal smoothing of survey data. The emphasis\nis on small-area estimation. We focus primarily on indicators in a low and\nmiddle-income countries context. Our methods are particularly useful for data\nfrom Demographic Health Surveys and Multiple Indicator Cluster Surveys. We\nbuild upon functions within the survey package, and use INLA for fast Bayesian\ncomputation. This paper includes a brief overview of these methods and\nillustrates the workflow of accessing and processing surveys, estimating\nsubnational child mortality rates, and visualizing results with both simulated\ndata and DHS surveys.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:17:39 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Zehang Richard", ""], ["Martin", "Bryan D", ""], ["Dong", "Tracy Qi", ""], ["Fuglstad", "Geir-Arne", ""], ["Paige", "John", ""], ["Riebler", "Andrea", ""], ["Clark", "Samuel", ""], ["Wakefield", "Jon", ""]]}, {"id": "2007.05124", "submitter": "Cyrus DiCiccio", "authors": "Cyrus DiCiccio, Sriram Vasudevan, Kinjal Basu, Krishnaram Kenthapadi,\n  and Deepak Agarwal", "title": "Evaluating Fairness Using Permutation Tests", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are central to people's lives and impact society in\nways as fundamental as determining how people access information. The gravity\nof these models imparts a responsibility to model developers to ensure that\nthey are treating users in a fair and equitable manner. Before deploying a\nmodel into production, it is crucial to examine the extent to which its\npredictions demonstrate biases. This paper deals with the detection of bias\nexhibited by a machine learning model through statistical hypothesis testing.\nWe propose a permutation testing methodology that performs a hypothesis test\nthat a model is fair across two groups with respect to any given metric. There\nare increasingly many notions of fairness that can speak to different aspects\nof model fairness. Our aim is to provide a flexible framework that empowers\npractitioners to identify significant biases in any metric they wish to study.\nWe provide a formal testing mechanism as well as extensive experiments to show\nhow this method works in practice.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:49:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["DiCiccio", "Cyrus", ""], ["Vasudevan", "Sriram", ""], ["Basu", "Kinjal", ""], ["Kenthapadi", "Krishnaram", ""], ["Agarwal", "Deepak", ""]]}, {"id": "2007.05276", "submitter": "Nan Zhang", "authors": "Nan Zhang, Daniel J. Graham, Daniel H\\\"orcher, Prateek Bansal", "title": "A Causal Inference Approach to Measure the Vulnerability of Urban Metro\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transit operators need vulnerability measures to understand the level of\nservice degradation under disruptions. This paper contributes to the literature\nwith a novel causal inference approach for estimating station-level\nvulnerability in metro systems. The empirical analysis is based on large-scale\ndata on historical incidents and population-level passenger demand. This\nanalysis thus obviates the need for assumptions made by previous studies on\nhuman behaviour and disruption scenarios. We develop four empirical\nvulnerability metrics based on the causal impact of disruptions on travel\ndemand, average travel speed and passenger flow distribution. Specifically, the\nproposed metrics based on the irregularity in passenger flow distribution\nextends the scope of vulnerability measurement to the entire trip distribution,\ninstead of just analysing the disruption impact on the entry or exit demand\n(that is, moments of the trip distribution). The unbiased estimates of\ndisruption impact are obtained by adopting a propensity score matching method,\nwhich adjusts for the confounding biases caused by non-random occurrence of\ndisruptions. An application of the proposed framework to the London Underground\nindicates that the vulnerability of a metro station depends on the location,\ntopology, and other characteristics. We find that, in 2013, central London\nstations are more vulnerable in terms of travel demand loss. However, the loss\nof average travel speed and irregularity in relative passenger flows reveal\nthat passengers from outer London stations suffer from longer individual delays\ndue to lack of alternative routes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:40:58 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:47:28 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 12:51:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Nan", ""], ["Graham", "Daniel J.", ""], ["H\u00f6rcher", "Daniel", ""], ["Bansal", "Prateek", ""]]}, {"id": "2007.05304", "submitter": "Kristian Miok", "authors": "Kristian Miok, Blaz Skrlj, Daniela Zaharie and Marko Robnik-Sikonja", "title": "To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate\n  Speech Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech is an important problem in the management of user-generated\ncontent. To remove offensive content or ban misbehaving users, content\nmoderators need reliable hate speech detectors. Recently, deep neural networks\nbased on the transformer architecture, such as the (multilingual) BERT model,\nachieve superior performance in many natural language classification tasks,\nincluding hate speech detection. So far, these methods have not been able to\nquantify their output in terms of reliability. We propose a Bayesian method\nusing Monte Carlo dropout within the attention layers of the transformer models\nto provide well-calibrated reliability estimates. We evaluate and visualize the\nresults of the proposed approach on hate speech detection problems in several\nlanguages. Additionally, we test if affective dimensions can enhance the\ninformation extracted by the BERT model in hate speech classification. Our\nexperiments show that Monte Carlo dropout provides a viable mechanism for\nreliability estimation in transformer networks. Used within the BERT model, it\nofers state-of-the-art classification performance and can detect less trusted\npredictions. Also, it was observed that affective dimensions extracted using\nsentic computing methods can provide insights toward interpretation of emotions\ninvolved in hate speech. Our approach not only improves the classification\nperformance of the state-of-the-art multilingual BERT model but the computed\nreliability scores also significantly reduce the workload in an inspection of\nofending cases and reannotation campaigns. The provided visualization helps to\nunderstand the borderline outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:09:00 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 10:02:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:19:23 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 10:28:12 GMT"}, {"version": "v5", "created": "Fri, 25 Sep 2020 07:30:47 GMT"}, {"version": "v6", "created": "Mon, 7 Dec 2020 08:01:25 GMT"}, {"version": "v7", "created": "Thu, 17 Dec 2020 09:43:00 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Miok", "Kristian", ""], ["Skrlj", "Blaz", ""], ["Zaharie", "Daniela", ""], ["Robnik-Sikonja", "Marko", ""]]}, {"id": "2007.05385", "submitter": "Owen Ward", "authors": "Owen G. Ward, Zhen Huang, Andrew Davison, Tian Zheng", "title": "Next Waves in Veridical Network Embedding", "comments": null, "journal-ref": null, "doi": "10.1002/sam.11486", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding nodes of a large network into a metric (e.g., Euclidean) space has\nbecome an area of active research in statistical machine learning, which has\nfound applications in natural and social sciences. Generally, a representation\nof a network object is learned in a Euclidean geometry and is then used for\nsubsequent tasks regarding the nodes and/or edges of the network, such as\ncommunity detection, node classification and link prediction. Network embedding\nalgorithms have been proposed in multiple disciplines, often with\ndomain-specific notations and details. In addition, different measures and\ntools have been adopted to evaluate and compare the methods proposed under\ndifferent settings, often dependent of the downstream tasks. As a result, it is\nchallenging to study these algorithms in the literature systematically.\nMotivated by the recently proposed Veridical Data Science (VDS) framework, we\npropose a framework for network embedding algorithms and discuss how the\nprinciples of predictability, computability and stability apply in this\ncontext. The utilization of this framework in network embedding holds the\npotential to motivate and point to new directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:41:48 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ward", "Owen G.", ""], ["Huang", "Zhen", ""], ["Davison", "Andrew", ""], ["Zheng", "Tian", ""]]}, {"id": "2007.05470", "submitter": "A. John Woodill", "authors": "A. John Woodill, Maria Kavanaugh, Michael Harte, and James R. Watson", "title": "Predicting Illegal Fishing on the Patagonia Shelf from Oceanographic\n  Seascapes", "comments": "27 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the world's most important fisheries are experiencing increases in\nillegal fishing, undermining efforts to sustainably conserve and manage fish\nstocks. A major challenge to ending illegal, unreported, and unregulated (IUU)\nfishing is improving our ability to identify whether a vessel is fishing\nillegally and where illegal fishing is likely to occur in the ocean. However,\nmonitoring the oceans is costly, time-consuming, and logistically challenging\nfor maritime authorities to patrol. To address this problem, we use vessel\ntracking data and machine learning to predict illegal fishing on the Patagonian\nShelf, one of the world's most productive regions for fisheries. Specifically,\nwe focus on Chinese fishing vessels, which have consistently fished illegally\nin this region. We combine vessel location data with oceanographic seascapes --\nclasses of oceanic areas based on oceanographic variables -- as well as other\nremotely sensed oceanographic variables to train a series of machine learning\nmodels of varying levels of complexity. These models are able to predict\nwhether a Chinese vessel is operating illegally with 69-96% confidence,\ndepending on the year and predictor variables used. These results offer a\npromising step towards preempting illegal activities, rather than reacting to\nthem forensically.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:31:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Woodill", "A. John", ""], ["Kavanaugh", "Maria", ""], ["Harte", "Michael", ""], ["Watson", "James R.", ""]]}, {"id": "2007.05542", "submitter": "Christopher Finlay", "authors": "Chris Finlay and Bruce A. Bassett", "title": "Climate & BCG: Effects on COVID-19 Death Growth Rates", "comments": "17 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple studies have suggested the spread of COVID-19 is affected by factors\nsuch as climate, BCG vaccinations, pollution and blood type. We perform a joint\nstudy of these factors using the death growth rates of 40 regions worldwide\nwith both machine learning and Bayesian methods. We find weak, non-significant\n(< 3$\\sigma$) evidence for temperature and relative humidity as factors in the\nspread of COVID-19 but little or no evidence for BCG vaccination prevalence or\n$\\text{PM}_{2.5}$ pollution. The only variable detected at a statistically\nsignificant level (>3$\\sigma$) is the rate of positive COVID-19 tests, with\nhigher positive rates correlating with higher daily growth of deaths.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:00:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Finlay", "Chris", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "2007.05825", "submitter": "Patricio Foncea", "authors": "Patricio Foncea, Susana Mondschein, Ragheb Massouh", "title": "Safer working spaces at coronavirus time: A novel use of antibody tests", "comments": "24 pages, 6 figures, preliminary preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SARS-CoV-2 pandemic has transformed the way that the world functions.\nHealth issues and population safety have driven countries' economies to a\ncritical state; therefore, sustaining economic activity while keeping workers\nsafe has become a worldwide goal. In this paper, we present a novel safety\nprotocol based on rapid antibody testing (ABT). Using discrete event\nsimulation, we evaluated its performance on the cumulative number of infected\nworkers, effective reproductive number ($R_e$), and active work force within a\ncompany. Using a synthetic experiment, we showed that ABT twice a week (ABT 3)\nperformed the best, detecting 5.7% of infected workers, compared to 16.9% when\nno ABT was applied. $R_e$ was reduced from 1.75 to 0.84, with a slight decrease\nin the active workers within the firm. A sensitivity analysis on the duration\nof the shedding period and sensitivity of ABT was performed and led to the same\nqualitative conclusions. We applied this protocol in a Chilean winery: the\nestimation of the initial $R_e$ of 1.3 was reduced to 0.7 when the ABT 3\nprotocol was implemented, with a 27% decrease in the number of infected\nworkers. Although ABT is not approved for COVID-19 diagnosis, our study shows\nthat upgraded safety standards can already be implemented in workspaces.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 18:21:04 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 21:04:26 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 20:39:06 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Foncea", "Patricio", ""], ["Mondschein", "Susana", ""], ["Massouh", "Ragheb", ""]]}, {"id": "2007.05894", "submitter": "Manar Samad", "authors": "Manar D. Samad and Sumen Sen", "title": "A Probabilistic Approach to Identifying Run Scoring Advantage in the\n  Order of Playing Cricket", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the game of cricket, the result of coin toss is assumed to be one of the\ndeterminants of match outcome. The decision to bat first after winning the toss\nis often taken to make the best use of superior pitch conditions and set a big\ntarget for the opponent. However, the opponent may fail to show their natural\nbatting performance in the second innings due to a number of factors, including\ndeteriorated pitch conditions and excessive pressure of chasing a high target\nscore. The advantage of batting first has been highlighted in the literature\nand expert opinions, however, the effect of batting and bowling order on match\noutcome has not been investigated well enough to recommend a solution to any\npotential bias. This study proposes a probability theory-based model to study\nvenue-specific scoring and chasing characteristics of teams under different\nmatch outcomes. A total of 1117 one-day international matches held in ten\npopular venues are analyzed to show substantially high scoring advantage and\nlikelihood when the winning team bat in the first innings. Results suggest that\nthe same 'bat-first' winning team is very unlikely to score or chase such a\nhigh score if they were to bat in the second innings. Therefore, the coin toss\ndecision may favor one team over the other. A Bayesian model is proposed to\nrevise the target score for each venue such that the winning and scoring\nlikelihood is equal regardless of the toss decision. The data and source codes\nhave been shared publicly for future research in creating competitive match\noutcomes by eliminating the advantage of batting order in run scoring.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 03:23:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Samad", "Manar D.", ""], ["Sen", "Sumen", ""]]}, {"id": "2007.05940", "submitter": "Xiuwen Wang", "authors": "Xinyun Chen and Xiuwen Wang", "title": "Perfect Sampling of Multivariate Hawkes Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an extension of self-exciting Hawkes process, the multivariate Hawkes\nprocess models counting processes of different types of random events with\nmutual excitement. In this paper, we present a perfect sampling algorithm that\ncan generate i.i.d. stationary sample paths of multivariate Hawkes process\nwithout any transient bias. In addition, we provide an explicit expression of\nalgorithm complexity in model and algorithm parameters and provide numerical\nschemes to find the optimal parameter set that minimizes the complexity of the\nperfect sampling algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 08:50:04 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:35:39 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Xinyun", ""], ["Wang", "Xiuwen", ""]]}, {"id": "2007.06084", "submitter": "Fabio Priuli", "authors": "Francesco Toraldo, Fabio S. Priuli", "title": "Bayesian probabilistic models for corporate context, with an application\n  to internal audit activities", "comments": "34 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a business case carried out in Poste Italiane, in\nthe context of fair performance evaluations of human resources engaged in\ninternal audit activities. In addition to the development of a Bayesian network\nsupporting the goal of the Internal Audit unit of Poste Italiane, the work has\nled to the development of a methodological approach to advanced analytics in\ncorporate context, whose usefulness goes well beyond the specific use case\ndescribed here. We thus present the different stages of such analytical\nstrategy, from feature selection, to model structure inference and model\nselection, as a general toolbox that allows a completely transparent and\nexplainable process to support data-driven decisions in business environments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:15:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Toraldo", "Francesco", ""], ["Priuli", "Fabio S.", ""]]}, {"id": "2007.06101", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Olanrewaju Akande and Quanli Wang", "title": "Multiple Imputation and Synthetic Data Generation with the R package\n  NPBayesImputeCat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contexts, missing data and disclosure control are ubiquitous and\nchallenging issues. In particular at statistical agencies, the respondent-level\ndata they collect from surveys and censuses can suffer from high rates of\nmissingness. Furthermore, agencies are obliged to protect respondents' privacy\nwhen publishing the collected data for public use. The NPBayesImputeCat R\npackage, introduced in this paper, provides routines to i) create multiple\nimputations for missing data, and ii) create synthetic data for statistical\ndisclosure control, for multivariate categorical data, with or without\nstructural zeros. We describe the Dirichlet process mixture of products of\nmultinomial distributions model used in the package, and illustrate various\nuses of the package using data samples from the American Community Survey\n(ACS). We also compare results of the missing data imputation to the mice R\npackage and those of the synthetic data generation to the synthpop R package.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:44:27 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:34:47 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Hu", "Jingchen", ""], ["Akande", "Olanrewaju", ""], ["Wang", "Quanli", ""]]}, {"id": "2007.06136", "submitter": "Han Yan", "authors": "Han Yan, Jiexing Wu, Yang Li, Jun S. Liu", "title": "Bayesian Bi-clustering Methods with Applications in Computational\n  Biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-clustering is a useful approach in analyzing biological data when\nobservations come from heterogeneous groups and have a large number of\nfeatures. We outline a general Bayesian approach in tackling bi-clustering\nproblems in moderate to high dimensions, and propose three Bayesian\nbi-clustering models on categorical data, which increase in complexities in\ntheir modeling of the distributions of features across bi-clusters. Our\nproposed methods apply to a wide range of scenarios: from situations where data\nare cluster-distinguishable only among a small subset of features but masked by\na large amount of noise, to situations where different groups of data are\nidentified by different sets of features or data exhibit hierarchical\nstructures. Through simulation studies, we show that our methods outperform\nexisting (bi-)clustering methods in both identifying clusters and recovering\nfeature distributional patterns across bi-clusters. We apply our methods to two\ngenetic datasets, though the area of application of our methods is even\nbroader.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 00:11:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 23:56:24 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Yan", "Han", ""], ["Wu", "Jiexing", ""], ["Li", "Yang", ""], ["Liu", "Jun S.", ""]]}, {"id": "2007.06160", "submitter": "Shuaimin Kang", "authors": "Shuaimin Kang, Krista Gile and Megan Price", "title": "Nested Dirichlet Process For Population Size Estimation From Multi-list\n  Recapture Data", "comments": "24 pages, 9 figures, submitted to Biometrics for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneity of response patterns is important in estimating the size of a\nclosed population from multiple recapture data when capture patterns are\ndifferent over time and location. In this paper, we extend the non-parametric\none layer latent class model for multiple recapture data proposed by\nManrique-Vallier (2016) to a nested latent class model with the first layer\nmodeling individual heterogeneity and the second layer modeling location-time\ndifferences. Location-time groups with similar recording patterns are in the\nsame top layer latent class and individuals within each top layer class are\ndependent. The nested latent class model incorporates hierarchical\nheterogeneity into the modeling to estimate population size from multi-list\nrecapture data. This approach leads to more accurate population size estimation\nand reduced uncertainty. We apply the method to estimating casualties from the\nSyrian conflict.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:52:19 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kang", "Shuaimin", ""], ["Gile", "Krista", ""], ["Price", "Megan", ""]]}, {"id": "2007.06336", "submitter": "Bram Burger", "authors": "Bram Burger, Marc Vaudel, Harald Barsnes", "title": "On the importance of block randomisation when designing proteomics\n  experiments", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomisation is used in experimental design to reduce the prevalence of\nunanticipated confounders. Complete randomisation can however create unbalanced\ndesigns, for example, grouping all samples of the same condition in the same\nbatch. Block randomisation is an approach that can prevent severe imbalances in\nsample allocation with respect to both known and unknown confounders. This\nfeature provides the reader with an introduction to blocking and randomisation,\ninsights into how to effectively organise samples during experimental design,\nwith special considerations with respect to proteomics.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:12:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Burger", "Bram", ""], ["Vaudel", "Marc", ""], ["Barsnes", "Harald", ""]]}, {"id": "2007.06357", "submitter": "Phillip Murray", "authors": "Phillip Murray, Riccardo Passeggeri, Almut E.D. Veraart and Mikko S.\n  Pakkanen", "title": "Feasible Inference for Stochastic Volatility in Brownian Semistationary\n  Processes", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the finite sample behaviour of a number of estimators\nfor the integrated power volatility process of a Brownian semistationary\nprocess in the non semi-martingale setting. We establish three consistent\nfeasible estimators for the integrated volatility, two derived from parametric\nmethods and one non-parametrically. We then use a simulation study to compare\nthe convergence properties of the estimators to one another, and to a benchmark\nof an infeasible estimator. We further establish bounds for the asymptotic\nvariance of the infeasible estimator and assess whether a central limit theorem\nwhich holds for the infeasible estimator can be translated into a feasible\nlimit theorem for the non-parametric estimator.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:06:42 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 14:00:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murray", "Phillip", ""], ["Passeggeri", "Riccardo", ""], ["Veraart", "Almut E. D.", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "2007.06380", "submitter": "Victor Churchill", "authors": "Victor Churchill and Anne Gelb", "title": "Synthetic Aperture Radar Image Formation with Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) is a day or night any-weather imaging modality\nthat is an important tool in remote sensing. Most existing SAR image formation\nmethods result in a maximum a posteriori image which approximates the\nreflectivity of an unknown ground scene. This single image provides no\nquantification of the certainty with which the features in the estimate should\nbe trusted. In addition, finding the mode is generally not the best way to\ninterrogate a posterior. This paper addresses these issues by introducing a\nsampling framework to SAR image formation. A hierarchical Bayesian model is\nconstructed using conjugate priors that directly incorporate coherent imaging\nand the problematic speckle phenomenon which is known to degrade image quality.\nSamples of the resulting posterior as well as parameters governing speckle and\nnoise are obtained using a Gibbs sampler. These samples may then be used to\ncompute estimates, and also to derive other statistics like variance which aid\nin uncertainty quantification. The latter information is particularly important\nin SAR, where ground truth images even for synthetically-created examples are\ntypically unknown. An example result using real-world data shows that the\nsampling-based approach introduced here to SAR image formation provides\nparameter-free estimates with improved contrast and significantly reduced\nspeckle, as well as unprecedented uncertainty quantification information.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:43:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Churchill", "Victor", ""], ["Gelb", "Anne", ""]]}, {"id": "2007.06414", "submitter": "Gabrielle Kelly Dr", "authors": "L.M. White and G.E. Kelly", "title": "Epidemic modelling of bovine tuberculosis in cattle herds and badgers in\n  Ireland", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bovine tuberculosis, a disease that affects cattle and badgers in Ireland,\nwas studied via stochastic epidemic modeling using incidence data from the Four\nArea Project (Griffin et al., 2005). The Four Area Project was a large scale\nfield trial conducted in four diverse farming regions of Ireland over a\nfive-year period (1997-2002) to evaluate the impact of badger culling on bovine\ntuberculosis incidence in cattle herds.\n  Based on the comparison of several models, the model with no between-herd\ntransmission and badger-to-herd transmission proportional to the total number\nof infected badgers culled was best supported by the data.\n  Detailed model validation was conducted via model prediction, identifiability\nchecks and sensitivity analysis.\n  The results suggest that badger-to-cattle transmission is of more importance\nthan between-herd transmission and that if there was no badger-to-herd\ntransmission, levels of bovine tuberculosis in cattle herds in Ireland could\ndecrease considerably.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:47:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["White", "L. M.", ""], ["Kelly", "G. E.", ""]]}, {"id": "2007.06476", "submitter": "Daniel Iong", "authors": "Daniel Iong, Qingyuan Zhao, Yang Chen", "title": "A Latent Mixture Model for Heterogeneous Causal Mechanisms in Mendelian\n  Randomization", "comments": "38 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian Randomization (MR) is a popular method in epidemiology and genetics\nthat uses genetic variation as instrumental variables for causal inference.\nExisting MR methods usually assume most genetic variants are valid instrumental\nvariables that identify a common causal effect. There is a general lack of\nawareness that this effect homogeneity assumption can be violated when there\nare multiple causal pathways involved, even if all the instrumental variables\nare valid. In this article, we introduce a latent mixture model MR-PATH that\ngroups instruments that yield similar causal effect estimates together. We\ndevelop a Monte-Carlo EM algorithm to fit this mixture model, derive\napproximate confidence intervals for uncertainty quantification, and adopt a\nmodified Bayesian Information Criterion (BIC) for model selection. We verify\nthe efficacy of the Monte-Carlo EM algorithm, confidence intervals, and model\nselection criterion using numerical simulations. We identify potential\nmechanistic heterogeneity when applying our method to estimate the effect of\nhigh-density lipoprotein cholesterol on coronary heart disease and the effect\nof adiposity on type II diabetes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:14:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 14:07:55 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Iong", "Daniel", ""], ["Zhao", "Qingyuan", ""], ["Chen", "Yang", ""]]}, {"id": "2007.06566", "submitter": "Seth Flaxman", "authors": "Michaela A. C. Vollmer, Ben Glampson, Thomas A. Mellan, Swapnil\n  Mishra, Luca Mercuri, Ceire Costello, Robert Klaber, Graham Cooke, Seth\n  Flaxman, Samir Bhatt", "title": "A unified machine learning approach to time series forecasting applied\n  to demand at emergency departments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There were 25.6 million attendances at Emergency Departments (EDs) in England\nin 2019 corresponding to an increase of 12 million attendances over the past\nten years. The steadily rising demand at EDs creates a constant challenge to\nprovide adequate quality of care while maintaining standards and productivity.\nManaging hospital demand effectively requires an adequate knowledge of the\nfuture rate of admission. Using 8 years of electronic admissions data from two\nmajor acute care hospitals in London, we develop a novel ensemble methodology\nthat combines the outcomes of the best performing time series and machine\nlearning approaches in order to make highly accurate forecasts of demand, 1, 3\nand 7 days in the future. Both hospitals face an average daily demand of 208\nand 106 attendances respectively and experience considerable volatility around\nthis mean. However, our approach is able to predict attendances at these\nemergency departments one day in advance up to a mean absolute error of +/- 14\nand +/- 10 patients corresponding to a mean absolute percentage error of 6.8%\nand 8.6% respectively. Our analysis compares machine learning algorithms to\nmore traditional linear models. We find that linear models often outperform\nmachine learning methods and that the quality of our predictions for any of the\nforecasting horizons of 1, 3 or 7 days are comparable as measured in MAE. In\naddition to comparing and combining state-of-the-art forecasting methods to\npredict hospital demand, we consider two different hyperparameter tuning\nmethods, enabling a faster deployment of our models without compromising\nperformance. We believe our framework can readily be used to forecast a wide\nrange of policy relevant indicators.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:59:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Vollmer", "Michaela A. C.", ""], ["Glampson", "Ben", ""], ["Mellan", "Thomas A.", ""], ["Mishra", "Swapnil", ""], ["Mercuri", "Luca", ""], ["Costello", "Ceire", ""], ["Klaber", "Robert", ""], ["Cooke", "Graham", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""]]}, {"id": "2007.06635", "submitter": "Mehrdad Naderi Dr", "authors": "Elham Mirfarah and Mehrdad Naderi and Ding-Geng Chen", "title": "Mixture of linear experts model for censored data: A novel approach with\n  scale-mixture of normal distributions", "comments": "21 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical mixture of linear experts (MoE) model is one of the widespread\nstatistical frameworks for modeling, classification, and clustering of data.\nBuilt on the normality assumption of the error terms for mathematical and\ncomputational convenience, the classical MoE model has two challenges: 1) it is\nsensitive to atypical observations and outliers, and 2) it might produce\nmisleading inferential results for censored data. The paper is then aimed to\nresolve these two challenges, simultaneously, by proposing a novel robust MoE\nmodel for model-based clustering and discriminant censored data with the\nscale-mixture of normal class of distributions for the unobserved error terms.\nBased on this novel model, we develop an analytical expectation-maximization\n(EM) type algorithm to obtain the maximum likelihood parameter estimates.\nSimulation studies are carried out to examine the performance, effectiveness,\nand robustness of the proposed methodology. Finally, real data is used to\nillustrate the superiority of the new model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:15:39 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mirfarah", "Elham", ""], ["Naderi", "Mehrdad", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2007.06772", "submitter": "Bo Zhang", "authors": "Bo Zhang, Siyu Heng, Emily J. MacKay, Ting Ye", "title": "Bridging preference-based instrumental variable studies and\n  cluster-randomized encouragement experiments: study design, noncompliance,\n  and average cluster effect ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods are widely used in medical and social science\nresearch to draw causal conclusions when the treatment and outcome are\nconfounded by unmeasured confounding variables. One important feature of such\nstudies is that the instrumental variable is often applied at the cluster\nlevel, e.g., hospitals' or physicians' preference for a certain treatment where\neach hospital or physician naturally defines a cluster. This paper proposes to\nembed such observational instrumental variable data into a cluster-randomized\nencouragement experiment using statistical matching. Potential outcomes and\ncausal assumptions underpinning the design are formalized and examined. Testing\nprocedures for two commonly-used estimands, Fisher's sharp null hypothesis and\nthe pooled effect ratio, are extended to the current setting. We then introduce\na novel cluster-heterogeneous proportional treatment effect model and the\nrelevant estimand: the average cluster effect ratio. This new estimand is\nadvantageous over the structural parameter in a constant proportional treatment\neffect model in that it allows treatment heterogeneity, and is advantageous\nover the pooled effect ratio estimand in that it is immune to Simpson's\nparadox. We develop an asymptotically valid randomization-based testing\nprocedure for this new estimand based on solving a mixed integer\nquadratically-constrained optimization problem. The proposed design and\ninferential methods are applied to a study of the effect of using\ntransesophageal echocardiography during CABG surgery on patients' 30-day\nmortality rate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:13:12 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 21:59:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Bo", ""], ["Heng", "Siyu", ""], ["MacKay", "Emily J.", ""], ["Ye", "Ting", ""]]}, {"id": "2007.06971", "submitter": "Surajit Ray", "authors": "Abhirup Banerjee, Surajit Ray, Bart Vorselaars, Joanne Kitson, Michail\n  Mamalakis, Simonne Weeks, Mark Baker, Louise S. Mackenzie", "title": "Use of Machine Learning and Artificial Intelligence to predict\n  SARS-CoV-2 infection from Full Blood Counts in a population", "comments": null, "journal-ref": "International Immunopharmacology, Volume 86,2020,106705,", "doi": "10.1016/j.intimp.2020.106705", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since December 2019 the novel coronavirus SARS-CoV-2 has been identified as\nthe cause of the pandemic COVID-19. Early symptoms overlap with other common\nconditions such as common cold and Influenza, making early screening and\ndiagnosis are crucial goals for health practitioners. The aim of the study was\nto use machine learning (ML), an artificial neural network (ANN) and a simple\nstatistical test to identify SARS-CoV-2 positive patients from full blood\ncounts without knowledge of symptoms or history of the individuals. The dataset\nincluded in the analysis and training contains anonymized full blood counts\nresults from patients seen at the Hospital Israelita Albert Einstein, at S\\~ao\nPaulo, Brazil, and who had samples collected to perform the SARS-CoV-2 rt-PCR\ntest during a visit to the hospital. Patient data was anonymised by the\nhospital, clinical data was standardized to have a mean of zero and a unit\nstandard deviation. This data was made public with the aim to allow researchers\nto develop ways to enable the hospital to rapidly predict and potentially\nidentify SARS-CoV-2 positive patients. We find that with full blood counts\nrandom forest, shallow learning and a flexible ANN model predict SARS-CoV-2\npatients with high accuracy between populations on regular wards (AUC = 94-95%)\nand those not admitted to hospital or in the community (AUC=80-86%). Here, AUC\nis the Area Under the receiver operating characteristics Curve and a measure\nfor model performance. Moreover, a simple linear combination of 4 blood counts\ncan be used to have an AUC of 85% for patients within the community. The\nnormalised data of different blood parameters from SARS-CoV-2 positive patients\nexhibit a decrease in platelets, leukocytes, eosinophils, basophils and\nlymphocytes, and an increase in monocytes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 11:08:22 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 10:22:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Banerjee", "Abhirup", ""], ["Ray", "Surajit", ""], ["Vorselaars", "Bart", ""], ["Kitson", "Joanne", ""], ["Mamalakis", "Michail", ""], ["Weeks", "Simonne", ""], ["Baker", "Mark", ""], ["Mackenzie", "Louise S.", ""]]}, {"id": "2007.07056", "submitter": "Yichen Jia", "authors": "Yichen Jia and Jong-Hyeon Jeong", "title": "Deep Learning for Quantile Regression under Right Censoring:\n  DeepQuantreg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational prediction algorithm of neural network, or deep learning,\nhas drawn much attention recently in statistics as well as in image recognition\nand natural language processing. Particularly in statistical application for\ncensored survival data, the loss function used for optimization has been mainly\nbased on the partial likelihood from Cox's model and its variations to utilize\nexisting neural network library such as Keras, which was built upon the open\nsource library of TensorFlow. This paper presents a novel application of the\nneural network to the quantile regression for survival data with right\ncensoring, which is adjusted by the inverse of the estimated censoring\ndistribution in the check function. The main purpose of this work is to show\nthat the deep learning method could be flexible enough to predict nonlinear\npatterns more accurately compared to existing quantile regression methods such\nas traditional linear quantile regression and nonparametric quantile regression\nwith total variation regularization, emphasizing practicality of the method for\ncensored survival data. Simulation studies were performed to generate nonlinear\ncensored survival data and compare the deep learning method with existing\nquantile regression methods in terms of prediction accuracy. The proposed\nmethod is illustrated with two publicly available breast cancer data sets with\ngene signatures. The method has been built into a package and is freely\navailable at \\url{https://github.com/yicjia/DeepQuantreg}.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:31:11 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:35:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jia", "Yichen", ""], ["Jeong", "Jong-Hyeon", ""]]}, {"id": "2007.07156", "submitter": "Dean Karlen", "authors": "Dean Karlen", "title": "Characterizing the spread of CoViD-19", "comments": "17 pages, 10 figures. Submitted to Statistical Methods in Medical\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the beginning of the epidemic, daily reports of CoViD-19 cases,\nhospitalizations, and deaths from around the world have been publicly\navailable. This paper describes methods to characterize broad features of the\nspread of the disease, with relatively long periods of constant transmission\nrates, using a new population modeling framework based on discrete-time\ndifference equations. Comparative parameters are chosen for their weak\ndependence on model assumptions. Approaches for their point and interval\nestimation, accounting for additional sources of variance in the case data, are\npresented. These methods provide a basis to quantitatively assess the impact of\nchanges to social distancing policies using publicly available data. As\nexamples, data from Ontario and German states are analyzed using this\nframework. German case data show a small increase in transmission rates\nfollowing the relaxation of lock-down rules on May 6, 2020. By combining case\nand death data from Germany, the mean and standard deviation of the time from\ninfection to death are estimated.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:20:40 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Karlen", "Dean", ""]]}, {"id": "2007.07369", "submitter": "Joonas P\\\"a\\\"akk\\\"onen", "authors": "Joonas P\\\"a\\\"akk\\\"onen", "title": "Ordinal Regression with Fenton-Wilkinson Order Statistics: A Case Study\n  of an Orienteering Race", "comments": "10 pages, 5 figures, to be published in Proc. ICSDMS 2020:\n  International Conference on Sports Data Mining and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports, individuals and teams are typically interested in final rankings.\nFinal results, such as times or distances, dictate these rankings, also known\nas places. Places can be further associated with ordered random variables,\ncommonly referred to as order statistics. In this work, we introduce a simple,\nyet accurate order statistical ordinal regression function that predicts relay\nrace places with changeover-times. We call this function the Fenton-Wilkinson\nOrder Statistics model. This model is built on the following educated\nassumption: individual leg-times follow log-normal distributions. Moreover, our\nkey idea is to utilize Fenton-Wilkinson approximations of changeover-times\nalongside an estimator for the total number of teams as in the notorious German\ntank problem. This original place regression function is sigmoidal and thus\ncorrectly predicts the existence of a small number of elite teams that\nsignificantly outperform the rest of the teams. Our model also describes how\nplace increases linearly with changeover-time at the inflection point of the\nlog-normal distribution function. With real-world data from Jukola 2019, a\nmassive orienteering relay race, the model is shown to be highly accurate even\nwhen the size of the training set is only 5% of the whole data set. Numerical\nresults also show that our model exhibits smaller place prediction\nroot-mean-square-errors than linear regression, mord regression and Gaussian\nprocess regression.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:37:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["P\u00e4\u00e4kk\u00f6nen", "Joonas", ""]]}, {"id": "2007.07471", "submitter": "You-Gan Wang", "authors": "Qibin Duan and Jinran Wu and Gaojun Wu and You-Gan Wang", "title": "Predication of Inflection Point and Outbreak Size of COVID-19 in New\n  Epicentres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) had caused more that 8 million\ninfections as of middle June 2020. Recently, Brazil has become a new epicentre\nof COVID-19, while India and African region are potential epicentres. This\nstudy aims to predict the inflection point and outbreak size of these\nnew/potential epicentres at the early phase of the epidemics by borrowing\ninformation from more `mature' curves from other countries. We modeled the\ncumulative cases to the well-known sigmoid growth curves to describe the\nepidemic trends under the mixed-effect models and using the four-parameter\nlogistic model after power transformations. African region is predicted to have\nthe largest total outbreak size of 3.9 million cases (2.2 to 6 million), and\nthe inflection will come around September 13, 2020. Brazil and India are\npredicted to have a similar final outbreak size of around 2.5 million cases\n(1.1 to 4.3 million), with the inflection points arriving June 23 and July 26,\nrespectively. We conclude in Brazil, India, and African the epidemics of COVI19\nhave not yet passed the inflection points; these regions potentially can take\nover USA in terms of outbreak size\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 04:10:39 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Duan", "Qibin", ""], ["Wu", "Jinran", ""], ["Wu", "Gaojun", ""], ["Wang", "You-Gan", ""]]}, {"id": "2007.07799", "submitter": "Charles Truong", "authors": "Flavien Quijoux, Charles Truong, Ali\\'enor Vienne-Jumeau, Laurent\n  Oudre, Fran\\c{c}ois BERTIN-HUGAULT, Philippe ZAWIEJA, Marie LEFEVRE,\n  Pierre-Paul VIDAL, Damien RICARD", "title": "Meta-analysis parameters computation: a Python approach to facilitate\n  the crossing of experimental conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is a data aggregation method that establishes an overall and\nobjective level of evidence based on the results of several studies. It is\nnecessary to maintain a high level of homogeneity in the aggregation of data\ncollected from a systematic literature review. However, the current tools do\nnot allow a cross-referencing of the experimental conditions that could explain\nthe heterogeneity observed between studies. This article aims at proposing a\nPython programming code containing several functions allowing the analysis and\nrapid visualization of data from many studies, while allowing the possibility\nof cross-checking the results by experimental condition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:42:28 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Quijoux", "Flavien", ""], ["Truong", "Charles", ""], ["Vienne-Jumeau", "Ali\u00e9nor", ""], ["Oudre", "Laurent", ""], ["BERTIN-HUGAULT", "Fran\u00e7ois", ""], ["ZAWIEJA", "Philippe", ""], ["LEFEVRE", "Marie", ""], ["VIDAL", "Pierre-Paul", ""], ["RICARD", "Damien", ""]]}, {"id": "2007.07881", "submitter": "Fei Wan", "authors": "Fei Wan", "title": "Statistical analysis of two arm randomized pre-post design with one\n  post-treatment measurement", "comments": "38 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized pre-post designs, with outcomes measured at baseline and\nfollow-ups, have been commonly used to compare the clinical effectiveness of\ntwo competing treatments. There are vast, but often conflicting, amount of\ninformation in current literature about the best analytic methods for pre-post\ndesign. It is challenging for applied researchers to make an informed choice.\nWe discuss six methods commonly used in literature: one way analysis of\nvariance (ANOVA), analysis of covariance main effect and interaction models on\npost-treatment measurement (ANCOVA I and II), ANOVA on change score between\nbaseline and post-treatment measurements, repeated measures and constrained\nrepeated measures models (cRM) on baseline and post-treatment measurements as\njoint outcomes. We review a number of study endpoints in pre-post designs and\nidentify the difference in post-treatment measurement as the common treatment\neffect that all six methods target. We delineate the underlying differences and\nlinks between these competing methods in homogeneous and heterogeneous study\npopulation. We demonstrate that ANCOVA and cRM outperform other alternatives\nbecause their treatment effect estimators have the smallest variances. cRM has\ncomparable performance to ANCOVA I main effect model in homogeneous scenario\nand to ANCOVA II interaction model in heterogeneous scenario. In spite of that,\nANCOVA has several advantages over cRM, including treating baseline measurement\nas covariate because it is not an outcome by definition, the convenience of\nincorporating other baseline variables and handling complex heteroscedasticity\npatterns in a linear regression framework.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:56:07 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wan", "Fei", ""]]}, {"id": "2007.07886", "submitter": "Ruoqi Liu", "authors": "Qianlong Wen, Ruoqi Liu and Ping Zhang", "title": "Clinical connectivity map for drug repurposing: using laboratory tests\n  to bridge drugs and diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drug repurposing has attracted increasing attention from both the\npharmaceutical industry and the research community. Many existing computational\ndrug repurposing methods rely on preclinical data (e.g., chemical structures,\ndrug targets), resulting in translational problems for clinical trials. In this\nstudy, we propose a clinical connectivity map framework for drug repurposing by\nleveraging laboratory tests to analyze complementarity between drugs and\ndiseases. We establish clinical drug effect vectors (i.e., drug-laboratory test\nassociations) by applying a continuous self-controlled case series model on a\nlongitudinal electronic health record data. We establish clinical disease sign\nvectors (i.e., disease-laboratory test associations) by applying a Wilcoxon\nrank sum test on a large-scale national survey data. Finally, we compute a\nrepurposing possibility score for each drug-disease pair by applying a dot\nproduct-based scoring function on clinical disease sign vectors and clinical\ndrug effect vectors. We comprehensively evaluate 392 drugs for 6 important\nchronic diseases (e.g., asthma, coronary heart disease, type 2 diabetes, etc.).\nWe discover not only known associations between diseases and drugs but also\nmany hidden drug-disease associations. Moreover, we are able to explain the\npredicted drug-disease associations via the corresponding complementarity\nbetween laboratory tests of drug effect vectors and disease sign vectors. The\nproposed clinical connectivity map framework uses laboratory tests from\nelectronic clinical information to bridge drugs and diseases, which is\nexplainable and has better translational power than existing computational\nmethods. Experimental results demonstrate the effectiveness of the proposed\nframework and suggest that our method could help identify drug repurposing\nopportunities, which will benefit patients by offering more effective and safer\ntreatments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:08:43 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 18:26:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wen", "Qianlong", ""], ["Liu", "Ruoqi", ""], ["Zhang", "Ping", ""]]}, {"id": "2007.08047", "submitter": "Guanyu Hu", "authors": "Guanyu Hu, Junxian Geng", "title": "Heterogeneity Learning for SIRS model: an Application to the COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a Bayesian Heterogeneity Learning approach for\nSusceptible-Infected-Removal-Susceptible (SIRS) model that allows underlying\nclustering patterns for transmission rate, recovery rate, and loss of immunity\nrate for the latest coronavirus (COVID-19) among different regions. Our\nproposed method provides simultaneously inference on parameter estimation and\nclustering information which contains both number of clusters and cluster\nconfigurations. Specifically, our key idea is to formulates the SIRS model into\na hierarchical form and assign the Mixture of Finite mixtures priors for\nheterogeneity learning. The properties of the proposed models are examined and\na Markov chain Monte Carlo sampling algorithm is used to sample from the\nposterior distribution. Extensive simulation studies are carried out to examine\nempirical performance of the proposed methods. We further apply the proposed\nmethodology to analyze the state level COVID-19 data in U.S.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:38:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hu", "Guanyu", ""], ["Geng", "Junxian", ""]]}, {"id": "2007.08189", "submitter": "Juha Karvanen", "authors": "Juha Karvanen, Santtu Tikka, Antti Hyttinen", "title": "Do-search -- a tool for causal inference and study design with multiple\n  data sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological evidence is based on multiple data sources including clinical\ntrials, cohort studies, surveys, registries and expert opinions. Merging\ninformation from different sources opens up new possibilities for the\nestimation of causal effects. We show how causal effects can be identified and\nestimated by combining experiments and observations in real and realistic\nscenarios. As a new tool, we present do-search, a recently developed\nalgorithmic approach that can determine the identifiability of a causal effect.\nThe approach is based on do-calculus, and it can utilize data with non-trivial\nmissing data and selection bias mechanisms. When the effect is identifiable,\ndo-search outputs an identifying formula on which numerical estimation can be\nbased. When the effect is not identifiable, we can use do-search to recognize\nadditional data sources and assumptions that would make the effect\nidentifiable. Throughout the paper, we consider the effect of salt-adding\nbehavior on blood pressure mediated by the salt intake as an example. The\nidentifiability of this effect is resolved in various scenarios with different\nassumptions on confounding. There are scenarios where the causal effect is\nidentifiable from a chain of experiments but not from survey data, as well as\nscenarios where the opposite is true. As an illustration, we use survey data\nfrom NHANES 2013--2016 and the results from a meta-analysis of randomized\ncontrolled trials and estimate the reduction in average systolic blood pressure\nunder an intervention where the use of table salt is discontinued.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:56:46 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Karvanen", "Juha", ""], ["Tikka", "Santtu", ""], ["Hyttinen", "Antti", ""]]}, {"id": "2007.08190", "submitter": "Kelly Van Lancker", "authors": "Kelly Van Lancker, Oliver Dukes and Stijn Vansteelandt", "title": "Principled Selection of Baseline Covariates to Account for Censoring in\n  Randomized Trials with a Survival Endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of randomized trials with time-to-event endpoints is nearly\nalways plagued by the problem of censoring. As the censoring mechanism is\nusually unknown, analyses typically employ the assumption of non-informative\ncensoring. While this assumption usually becomes more plausible as more\nbaseline covariates are being adjusted for, such adjustment also raises\nconcerns. Pre-specification of which covariates will be adjusted for (and how)\nis difficult, thus prompting the use of data-driven variable selection\nprocedures, which may impede valid inferences to be drawn. The adjustment for\ncovariates moreover adds concerns about model misspecification, and the fact\nthat each change in adjustment set, also changes the censoring assumption and\nthe treatment effect estimand. In this paper, we discuss these concerns and\npropose a simple variable selection strategy that aims to produce a valid test\nof the null in large samples. The proposal can be implemented using\noff-the-shelf software for (penalized) Cox regression, and is empirically found\nto work well in simulation studies and real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:57:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Van Lancker", "Kelly", ""], ["Dukes", "Oliver", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2007.08209", "submitter": "Andreas Apfelbeck", "authors": "Andreas Apfelbeck, Stefan Wegner, Roman Henze, Ferit K\\\"u\\c{c}\\\"ukay", "title": "Prediction of the Subjective Impression of Passenger Car Roll Dynamics\n  on the Driver Based on Frequency-Domain Characteristic Values", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": "10.1080/00423114.2020.1814357", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characteristic values are essential for the design and assessment of driving\ndynamics during the early stages of the development process of passenger cars.\nCompared to other aspects of vehicle dynamics however, the relationship between\nmeasurable parameters and the subjective perception of vehicle roll dynamics\nhas not been researched extensively. In this paper, a study is presented in\nwhich several variants of a vehicle with an electronically controlled\nsuspension were rated by test subjects regarding its roll dynamics and measured\nin a standardized driving manoeuvre. The resulting subjective ratings and\nobjective characteristic values are then used to derive models to predict the\nsubjective liking of several roll dynamics aspects based on objective\nfrequency-domain parameters. Finally, the resulting prediction models are\nvalidated using measurements of additional vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:33:08 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 09:55:15 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 15:22:15 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Apfelbeck", "Andreas", ""], ["Wegner", "Stefan", ""], ["Henze", "Roman", ""], ["K\u00fc\u00e7\u00fckay", "Ferit", ""]]}, {"id": "2007.08406", "submitter": "Norman Fenton Prof", "authors": "Norman Fenton, Martin Neil, Steven Frazier", "title": "The role of collider bias in understanding statistics on racially biased\n  policing", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contradictory conclusions have been made about whether unarmed blacks are\nmore likely to be shot by police than unarmed whites using the same data. The\nproblem is that, by relying only on data of 'police encounters', there is the\npossibility that genuine bias can be hidden. We provide a causal Bayesian\nnetwork model to explain this bias, which is called collider bias or Berkson's\nparadox, and show how the different conclusions arise from the same model and\ndata. We also show that causal Bayesian networks provide the ideal formalism\nfor considering alternative hypotheses and explanations of bias.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:26:23 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Fenton", "Norman", ""], ["Neil", "Martin", ""], ["Frazier", "Steven", ""]]}, {"id": "2007.08511", "submitter": "Sarah Elizabeth Heaps", "authors": "Naomi E. Hannaford, Sarah E. Heaps, Tom M. W. Nye, Tom A. Williams and\n  T. Martin Embley", "title": "Incorporating compositional heterogeneity into Lie Markov models for\n  phylogenetic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetics uses alignments of molecular sequence data to learn about\nevolutionary trees. Substitutions in sequences are modelled through a\ncontinuous-time Markov process, characterised by an instantaneous rate matrix,\nwhich standard models assume is time-reversible and stationary. These\nassumptions are biologically questionable and induce a likelihood function\nwhich is invariant to a tree's root position. This hampers inference because a\ntree's biological interpretation depends critically on where it is rooted.\nRelaxing both assumptions, we introduce a model whose likelihood can\ndistinguish between rooted trees. The model is non-stationary, with step\nchanges in the instantaneous rate matrix at each speciation event. Exploiting\nrecent theoretical work, each rate matrix belongs to a non-reversible family of\nLie Markov models. These models are closed under matrix multiplication, so our\nextension offers the conceptually appealing property that a tree and all its\nsub-trees could have arisen from the same family of non-stationary models.\n  We adopt a Bayesian approach, describe an MCMC algorithm for posterior\ninference and provide software. The biological insight that our model can\nprovide is illustrated through an analysis in which non-reversible but\nstationary, and non-stationary but reversible models cannot identify a\nplausible root.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:58:33 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 07:24:50 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hannaford", "Naomi E.", ""], ["Heaps", "Sarah E.", ""], ["Nye", "Tom M. W.", ""], ["Williams", "Tom A.", ""], ["Embley", "T. Martin", ""]]}, {"id": "2007.08569", "submitter": "Daniele Durante", "authors": "Sirio Legramanti, Tommaso Rigon, Daniele Durante and David B. Dunson", "title": "Extended Stochastic Block Models with Application to Criminal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably learning group structure among nodes in network data is challenging\nin modern applications. We are motivated by covert networks encoding\nrelationships among criminals. These data are subject to measurement errors and\nexhibit a complex combination of an unknown number of core-periphery,\nassortative and disassortative structures that may unveil the internal\narchitecture of the criminal organization. The coexistence of such noisy block\nstructures limits the reliability of community detection algorithms routinely\napplied to criminal networks, and requires extensions of model-based solutions\nto realistically characterize the node partition process, incorporate\ninformation from node attributes, and provide improved strategies for\nestimation, uncertainty quantification, model selection and prediction. To\naddress these goals, we develop a novel class of extended stochastic block\nmodels (ESBM) that infer groups of nodes having common connectivity patterns\nvia Gibbs-type priors on the partition process. This choice encompasses several\nrealistic priors for criminal networks, covering solutions with fixed, random\nand infinite number of possible groups, and facilitates inclusion of node\nattributes in a principled manner. Among the new alternatives in our class, we\nfocus on the Gnedin process as a realistic prior that allows the number of\ngroups to be finite, random and subject to a reinforcement process coherent\nwith the modular structures in organized crime. A collapsed Gibbs sampler is\nproposed for the whole ESBM class, and refined strategies for estimation,\nprediction, uncertainty quantification and model selection are outlined. ESBM\nperformance is illustrated in realistic simulations and in an application to an\nItalian Mafia network, where we learn key block patterns revealing a complex\nhierarchical structure of the organization, mostly hidden from state-of-the-art\nalternative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:06:16 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:51:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Legramanti", "Sirio", ""], ["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "2007.08594", "submitter": "Steffen Ventz", "authors": "Steffen Ventz, Rahul Mazumder, Lorenzo Trippa", "title": "Integration of Survival Data from Multiple Studies", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a statistical procedure that integrates survival data from\nmultiple biomedical studies, to improve the accuracy of predictions of survival\nor other events, based on individual clinical and genomic profiles, compared to\nmodels developed leveraging only a single study or meta-analytic methods. The\nmethod accounts for potential differences in the relation between predictors\nand outcomes across studies, due to distinct patient populations, treatments\nand technologies to measure outcomes and biomarkers. These differences are\nmodeled explicitly with study-specific parameters. We use hierarchical\nregularization to shrink the study-specific parameters towards each other and\nto borrow information across studies. Shrinkage of the study-specific\nparameters is controlled by a similarity matrix, which summarizes differences\nand similarities of the relations between covariates and outcomes across\nstudies. We illustrate the method in a simulation study and using a collection\nof gene-expression datasets in ovarian cancer. We show that the proposed model\nincreases the accuracy of survival prediction compared to alternative\nmeta-analytic methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:52:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ventz", "Steffen", ""], ["Mazumder", "Rahul", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2007.08962", "submitter": "Panayotis Papoutsis", "authors": "Panayotis Papoutsis (LPSM, LMJL, ECN), Bertrand Michel (LMJL), Anne\n  Philippe (LMJL), Tarn Duong", "title": "Bayesian hierarchical models for the prediction of the driver flow and\n  passenger waiting times in a stochastic carpooling service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carpooling is an integral component in smart carbon-neutral cities, in\nparticular to facilitate homework commuting. We study an innovative carpooling\nservice developed by the start-up Ecov which specialises in homework commutes\nin peri-urban and rural regions. When a passenger makes a carpooling request, a\ndesignated driver is not assigned as in a traditional carpooling service;\nrather the passenger waits for the first driver, from a population of\nnon-professional drivers who are already en route, to arrive. We propose a\ntwo-stage Bayesian hierarchical model to overcome the considerable\ndifficulties, due to the sparsely observed driver and passenger data from an\nembryonic stochastic carpooling service, to deliver high-quality predictions of\ndriver flow and passenger waiting times. The first stage focuses on the driver\nflow, whose predictions are aggregated at the daily level to compensate the\ndata sparsity. The second stage processes this single daily driver flow into\nsub-daily (e.g. hourly) predictions of the passenger waiting times. We\ndemonstrate that our model mostly outperforms frequentist and non-hierarchical\nBayesian methods for observed data from operational carpooling service in Lyon,\nFrance and we also validated our model on simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:12:22 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Papoutsis", "Panayotis", "", "LPSM, LMJL, ECN"], ["Michel", "Bertrand", "", "LMJL"], ["Philippe", "Anne", "", "LMJL"], ["Duong", "Tarn", ""]]}, {"id": "2007.08974", "submitter": "Romain Narci", "authors": "Romain Narci, Maud Delattre, Catherine Lar\\'edo, Elisabeta Vergu", "title": "Inference for partially observed epidemic dynamics guided by Kalman\n  filtering techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent development of methods dealing with partially observed\nepidemic dynamics (unobserved model coordinates, discrete and noisy outbreak\ndata), limitations remain in practice, mainly related to the quantity of\naugmented data and calibration of numerous tuning parameters. In particular, as\ncoordinates of dynamic epidemic models are coupled, the presence of unobserved\ncoordinates leads to a statistically difficult problem. The aim is to propose\nan easy-to-use and general inference method that is able to tackle these\nissues. First, using the properties of epidemics in large populations, a\ntwo-layer model is constructed. Via a diffusion-based approach, a Gaussian\napproximation of the epidemic density-dependent Markovian jump process is\nobtained, representing the state model. The observational model, consisting of\nnoisy observations of certain model coordinates, is approximated by Gaussian\ndistributions. Then, an inference method based on an approximate likelihood\nusing Kalman filtering recursion is developed to estimate parameters of both\nthe state and observational models. The performance of estimators of key model\nparameters is assessed on simulated data of SIR epidemic dynamics for different\nscenarios with respect to the population size and the number of observations.\nThis performance is compared with that obtained using the well-known maximum\niterated filtering method. Finally, the inference method is applied to a real\ndata set on an influenza outbreak in a British boarding school in 1978.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:38:45 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 10:47:31 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 14:42:41 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Narci", "Romain", ""], ["Delattre", "Maud", ""], ["Lar\u00e9do", "Catherine", ""], ["Vergu", "Elisabeta", ""]]}, {"id": "2007.09015", "submitter": "Chris Dent", "authors": "S. Awara and M. Lynch and S. Pfenninger and K. Schell and R. Sioshansi\n  and I. Staffell and N. Samaan and S.H. Tindemans and A.L. Wilson and S.\n  Zachary and H. Zareipour and C.J. Dent", "title": "Capacity Value of Solar Power and Other Variable Generation", "comments": "8 pages, 3 figures, submitted to IEEE TPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews methods that are used for adequacy risk assessment\nconsidering solar power and for assessment of the capacity value of solar\npower. The properties of solar power are described as seen from the perspective\nof the power-system operator, comparing differences in energy availability and\ncapacity factors with those of wind power. Methodologies for risk calculations\nconsidering variable generation are surveyed, including the probability\nbackground, statistical-estimation approaches, and capacity-value metrics.\nIssues in incorporating variable generation in capacity markets are described,\nfollowed by a review of applied studies considering solar power. Finally,\nrecommendations for further research are presented.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:10:23 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Awara", "S.", ""], ["Lynch", "M.", ""], ["Pfenninger", "S.", ""], ["Schell", "K.", ""], ["Sioshansi", "R.", ""], ["Staffell", "I.", ""], ["Samaan", "N.", ""], ["Tindemans", "S. H.", ""], ["Wilson", "A. L.", ""], ["Zachary", "S.", ""], ["Zareipour", "H.", ""], ["Dent", "C. J.", ""]]}, {"id": "2007.09056", "submitter": "Luke Keele", "authors": "Luke Keele and Eli Ben-Michael and Avi Feller and Rachel Kelz and Luke\n  Miratrix", "title": "Hospital Quality Risk Standardization via Approximate Balancing Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing outcomes across hospitals, often to identify underperforming\nhospitals, is a critical task in health services research. However, naive\ncomparisons of average outcomes, such as surgery complication rates, can be\nmisleading because hospital case mixes differ -- a hospital's overall\ncomplication rate may be lower due to more effective treatments or simply\nbecause the hospital serves a healthier population overall. In this paper, we\ndevelop a method of ``direct standardization'' where we re-weight each hospital\npatient population to be representative of the overall population and then\ncompare the weighted averages across hospitals. Adapting methods from survey\nsampling and causal inference, we find weights that directly control for\nimbalance between the hospital patient mix and the target population, even\nacross many patient attributes. Critically, these balancing weights can also be\ntuned to preserve sample size for more precise estimates. We also derive\nprincipled measures of statistical precision, and use outcome modeling and\nBayesian shrinkage to increase precision and account for variation in hospital\nsize. We demonstrate these methods using claims data from Pennsylvania,\nFlorida, and New York, estimating standardized hospital complication rates for\ngeneral surgery patients. We conclude with a discussion of how to detect low\nperforming hospitals.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:32:19 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 15:45:44 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Keele", "Luke", ""], ["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Kelz", "Rachel", ""], ["Miratrix", "Luke", ""]]}, {"id": "2007.09117", "submitter": "Michelle Anzarut Ms", "authors": "Michelle Anzarut, Luis Felipe Gonz\\'alez, Sonia Mendiz\\'abal and\n  Mar\\'ia Teresa Ortiz", "title": "Estimating COVID-19 cases and reproduction number in Mexico", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we fit a semi-mechanistic Bayesian hierarchical model to\ndescribe the Mexican COVID-19 epidemic. We obtain two epidemiological measures:\nthe number of infections and the reproduction number. Estimations are based on\ndeath data. Hence, we expect our estimates to be more accurate than the attack\nrates estimated from the reported number of cases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:12:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Anzarut", "Michelle", ""], ["Gonz\u00e1lez", "Luis Felipe", ""], ["Mendiz\u00e1bal", "Sonia", ""], ["Ortiz", "Mar\u00eda Teresa", ""]]}, {"id": "2007.09128", "submitter": "Biagio Palumbo", "authors": "Christian Capezza, Fabio Centofanti, Antonio Lepore, Biagio Palumbo", "title": "Functional clustering methods for resistance spot welding process data\n  in the automotive industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of resistance spot welding (RSW) joints of metal sheets in\nthe automotive industry is typically based on costly and lengthy off-line tests\nthat are unfeasible on the full production, especially on large scale. However,\nthe massive industrial digitalization triggered by the industry 4.0 framework\nmakes available, for every produced joint, on-line RSW process parameters, such\nas, in particular, the so-called dynamic resistance curve (DRC), which is\nrecognized as the full technological signature of the spot welds. Motivated by\nthis context, the present paper means to show the potentiality and the\npractical applicability to clustering methods of the functional data approach\nthat avoids the need for arbitrary and often controversial feature extraction\nto find out homogeneous groups of DRCs, which likely pertain to spot welds\nsharing common mechanical and metallurgical properties. We intend is to provide\nan essential hands-on overview of the most promising functional clustering\nmethods, and to apply the latter to the DRCs collected from the RSW process at\nhand, even if they could go far beyond the specific application hereby\ninvestigated. The methods analyzed are demonstrated to possibly support\npractitioners along the identification of the mapping relationship between\nprocess parameters and the final quality of RSW joints as well as, more\nspecifically, along the priority assignment for off-line testing of welded\nspots and the welding tool wear analysis. The analysis code, that has been\ndeveloped through the software environment R, and the DRC data set are made\nopenly available online at https://github.com/unina-sfere/funclustRSW/\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:38:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Capezza", "Christian", ""], ["Centofanti", "Fabio", ""], ["Lepore", "Antonio", ""], ["Palumbo", "Biagio", ""]]}, {"id": "2007.09181", "submitter": "Siddharth Dixit Mr.", "authors": "Siddharth Dixit, Meghna Chaudhary, Niteesh Sahni", "title": "Network Learning Approaches to study World Happiness", "comments": "13 Pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations in its 2011 resolution declared the pursuit of happiness a\nfundamental human goal and proposed public and economic policies centered\naround happiness. In this paper we used 2 types of computational strategies\nviz. Predictive Modelling and Bayesian Networks (BNs) to model the processed\nhistorical happiness index data of 156 nations published by UN since 2012. We\nattacked the problem of prediction using General Regression Neural Networks\n(GRNNs) and show that it out performs other state of the art predictive models.\nTo understand causal links amongst key features that have been proven to have a\nsignificant impact on world happiness, we first used a manual discretization\nscheme to discretize continuous variables into 3 levels viz. Low, Medium and\nHigh. A consensus World Happiness BN structure was then fixed after\namalgamating information by learning 10000 different BNs using bootstrapping.\nLastly, exact inference through conditional probability queries was used on\nthis BN to unravel interesting relationships among the important features\naffecting happiness which would be useful in policy making.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:29:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dixit", "Siddharth", ""], ["Chaudhary", "Meghna", ""], ["Sahni", "Niteesh", ""]]}, {"id": "2007.09246", "submitter": "Jonathan Marc Bearak", "authors": "Jonathan Marc Bearak, Anna Popinchalk, Bela Ganatra, Ann-Beth Moller,\n  \\\"Ozge Tun\\c{c}alp, Cynthia Beavin, Lorraine Kwok, Leontine Alkema", "title": "Global estimation of unintended pregnancy and abortion using a Bayesian\n  hierarchical random walk model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unintended pregnancy and abortion estimates are needed to inform and motivate\ninvestment in global health programmes and policies. Variability in the\navailability and reliability of data poses challenges for producing estimates.\nWe developed a Bayesian model that simultaneously estimates incidence of\nunintended pregnancy and abortion for 195 countries and territories. Our\nmodelling strategy was informed by the proximate determinants of fertility with\n(i) incidence of unintended pregnancy defined by the number of women (grouped\nby marital and contraceptive use status) and their respective pregnancy rates,\nand (ii) abortion incidence defined by group-specific pregnancies and\npropensities to have an abortion. Hierarchical random walk models are used to\nestimate country-group-period-specific pregnancy rates and propensities to\nabort.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:31:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bearak", "Jonathan Marc", ""], ["Popinchalk", "Anna", ""], ["Ganatra", "Bela", ""], ["Moller", "Ann-Beth", ""], ["Tun\u00e7alp", "\u00d6zge", ""], ["Beavin", "Cynthia", ""], ["Kwok", "Lorraine", ""], ["Alkema", "Leontine", ""]]}, {"id": "2007.09417", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang and Maryclare Griffin and David S. Matteson", "title": "Modeling a Nonlinear Biophysical Trend Followed by Long-Memory\n  Equilibrium with Unknown Change Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements of many biological processes are characterized by an initial\ntrend period followed by an equilibrium period. Scientists may wish to quantify\nfeatures of the two periods, as well as the timing of the change point.\nSpecifically, we are motivated by problems in the study of electrical\ncell-substrate impedance sensing (ECIS) data. ECIS is a popular new technology\nwhich measures cell behavior non-invasively. Previous studies using ECIS data\nhave found that different cell types can be classified by their equilibrium\nbehavior. However, it can be challenging to identify when equilibrium has been\nreached, and to quantify the relevant features of cells' equilibrium behavior.\nIn this paper, we assume that measurements during the trend period are\nindependent deviations from a smooth nonlinear function of time, and that\nmeasurements during the equilibrium period are characterized by a simple long\nmemory model. We propose a method to simultaneously estimate the parameters of\nthe trend and equilibrium processes and locate the change point between the\ntwo. We find that this method performs well in simulations and in practice.\nWhen applied to ECIS data, it produces estimates of change points and measures\nof cell equilibrium behavior which offer improved classification of infected\nand uninfected cells.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 12:24:26 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 10:27:37 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Wenyu", ""], ["Griffin", "Maryclare", ""], ["Matteson", "David S.", ""]]}, {"id": "2007.09505", "submitter": "Dhaval Adjodah", "authors": "Dhaval Adjodah, Yan Leng, Shi Kai Chong, P. M. Krafft, Esteban Moro,\n  Alex Pentland", "title": "Social Learning and the Accuracy-Risk Trade-off in the Wisdom of the\n  Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we design and deploy crowdsourced prediction platforms for real-world\napplications where risk is an important dimension of prediction performance? To\nanswer this question, we conducted a large online Wisdom of the Crowd study\nwhere participants predicted the prices of real financial assets (e.g. S&P\n500). We observe a Pareto frontier between accuracy of prediction and risk, and\nfind that this trade-off is mediated by social learning i.e. as social learning\nis increasingly leveraged, it leads to lower accuracy but also lower risk. We\nalso observe that social learning leads to superior accuracy during one of our\nrounds that occurred during the high market uncertainty of the Brexit vote. Our\nresults have implications for the design of crowdsourced prediction platforms:\nfor example, they suggest that the performance of the crowd should be more\ncomprehensively characterized by using both accuracy and risk (as is standard\nin financial and statistical forecasting), in contrast to prior work where risk\nof prediction has been overlooked.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 19:32:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Adjodah", "Dhaval", ""], ["Leng", "Yan", ""], ["Chong", "Shi Kai", ""], ["Krafft", "P. M.", ""], ["Moro", "Esteban", ""], ["Pentland", "Alex", ""]]}, {"id": "2007.09577", "submitter": "Feng Li", "authors": "Xiaoqian Wang, Yanfei Kang, Rob J Hyndman, and Feng Li", "title": "Distributed ARIMA Models for Ultra-long Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing forecasts for ultra-long time series plays a vital role in various\nactivities, such as investment decisions, industrial production arrangements,\nand farm management. This paper develops a novel distributed forecasting\nframework to tackle challenges associated with forecasting ultra-long time\nseries by utilizing the industry-standard MapReduce framework. The proposed\nmodel combination approach facilitates distributed time series forecasting by\ncombining the local estimators of ARIMA (AutoRegressive Integrated Moving\nAverage) models delivered from worker nodes and minimizing a global loss\nfunction. In this way, instead of unrealistically assuming the data generating\nprocess (DGP) of an ultra-long time series stays invariant, we make assumptions\nonly on the DGP of subseries spanning shorter time periods. We investigate the\nperformance of the proposed distributed ARIMA models on an electricity demand\ndataset. Compared to ARIMA models, our approach results in significantly\nimproved forecasting accuracy and computational efficiency both in point\nforecasts and prediction intervals, especially for longer forecast horizons.\nMoreover, we explore some potential factors that may affect the forecasting\nperformance of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:23:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Xiaoqian", ""], ["Kang", "Yanfei", ""], ["Hyndman", "Rob J", ""], ["Li", "Feng", ""]]}, {"id": "2007.09631", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Claiming trend in toxicological and pharmacological dose-response\n  studies: an overview on statistical methods and related R-Software", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There are very different statistical methods for demonstrating a trend in\npharmacological experiments. Here, the focus is on sparse models with only one\nparameter to be estimated and interpreted: the increase in the regression model\nand the difference to control in the contrast model. This provides both\np-values and confidence intervals for an appropriate effect size. A combined\ntest consisting of the Tukey regression approach and the multiple contrast test\naccording to Williams is recommended, which can be generalized to the\ngeneralized linear (mixed effect) model. Thus numerous variable types occurring\nin pharmacology/toxicology can be adequately evaluated. Software is available\nthrough CRAN packages. The most significant limitation of this approach is for\ndesigns with very small sample sizes, often in pharmacology/toxicology.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 09:13:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2007.09726", "submitter": "Jeong-Soo Park", "authors": "Yonggwan Shin, Youngsaeng Lee, Juntae Choi, Jeong-Soo Park", "title": "Integration of max-stable processes and Bayesian model averaging to\n  predict extreme climatic events in multi-model ensembles", "comments": null, "journal-ref": "Stochastic Environmental Research and Risk Assessment 33, 2019,\n  47-57", "doi": "10.1007/s00477-018-1629-7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projections of changes in extreme climate are sometimes predicted by using\nmulti-model ensemble methods such as Bayesian model averaging (BMA) embedded\nwith the generalized extreme value (GEV) distribution. BMA is a popular method\nfor combining the forecasts of individual simulation models by weighted\naveraging and characterizing the uncertainty induced by simulating the model\nstructure. This method is referred to as the GEV-embedded BMA. It is, however,\nbased on a point-wise analysis of extreme events, which means it overlooks the\nspatial dependency between nearby grid cells. Instead of a point-wise model, a\nspatial extreme model such as the max-stable process (MSP) is often employed to\nimprove precision by considering spatial dependency. We propose an approach\nthat integrates the MSP into BMA, which is referred to as the MSP-BMA herein.\nThe superiority of the proposed method over the GEV-embedded BMA is\ndemonstrated by using extreme rainfall intensity data on the Korean peninsula\nfrom Coupled Model Intercomparison Project Phase 5 (CMIP5) multi-models. The\nreanalysis data called APHRODITE (Asian Precipitation Highly-Resolved\nObservational Data Integration Towards Evaluation, v1101) and 17 CMIP5 models\nare examined for 10 grid boxes in Korea. In this example, the MSP-BMA achieves\na variance reduction over the GEV-embedded BMA. The bias inflation by MSP-BMA\nover the GEV-embedded BMA is also discussed. A by-product technical advantage\nof the MSP-BMA is that tedious `regridding' is not required before and after\nthe analysis while it should be done for the GEV-embedded BMA.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 17:30:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Shin", "Yonggwan", ""], ["Lee", "Youngsaeng", ""], ["Choi", "Juntae", ""], ["Park", "Jeong-Soo", ""]]}, {"id": "2007.09755", "submitter": "Sami Belkacem", "authors": "Sami Belkacem", "title": "COVID-19 Data Analysis and Forecasting: Algeria and the World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel coronavirus disease 2019 COVID-19 has been leading the world into a\nprominent crisis. As of May 19, 2020, the virus had spread to 215 countries\nwith more than 4,622,001 confirmed cases and 311,916 reported deaths worldwide,\nincluding Algeria with 7201 cases and 555 deaths. Analyze and forecast COVID-19\ncases and deaths growth could be useful in many ways, governments could\nestimate medical equipment and take appropriate policy responses, and experts\ncould approximate the peak and the end of the disease. In this work, we first\ntrain a time series Prophet model to analyze and forecast the number of\nCOVID-19 cases and deaths in Algeria based on the previously reported numbers.\nThen, to better understand the spread and the properties of the COVID-19, we\ninclude external factors that may contribute to accelerate/slow the spread of\nthe virus, construct a dataset from reliable sources, and conduct a large-scale\ndata analysis considering 82 countries worldwide. The evaluation results show\nthat the time series Prophet model accurately predicts the number of cases and\ndeaths in Algeria with low RMSE scores of 218.87 and 4.79 respectively, while\nthe forecast suggests that the total number of cases and deaths are expected to\nincrease in the coming weeks. Moreover, the worldwide data-driven analysis\nreveals several correlations between the increase/decrease in the number of\ncases and deaths and external factors that may contribute to accelerate/slow\nthe spread of the virus such as geographic, climatic, health, economic, and\ndemographic factors.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:10:48 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 09:28:52 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Belkacem", "Sami", ""]]}, {"id": "2007.09780", "submitter": "Anurag Mishra", "authors": "Abhishek Srivastava, Anurag Mishra, Trusha Jayant Parekh, Sampreeti\n  Jena", "title": "Implementing Stepped Pooled Testing for Rapid COVID-19 Detection", "comments": "6 pages, including three tables and four figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19, a viral respiratory pandemic, has rapidly spread throughout the\nglobe. Large scale and rapid testing of the population is required to contain\nthe disease, but such testing is prohibitive in terms of resources, cost and\ntime. Recently RT-PCR based pooled testing has emerged as a promising way to\nboost testing efficiency. We introduce a stepped pooled testing strategy, a\nprobability driven approach which significantly reduces the number of tests\nrequired to identify infected individuals in a large population. Our\ncomprehensive methodology incorporates the effect of false negative and\npositive rates to accurately determine not only the efficiency of pooling but\nalso it's accuracy. Under various plausible scenarios, we show that this\napproach significantly reduces the cost of testing and also reduces the\neffective false positive rate of tests when compared to a strategy of testing\nevery individual of a population. We also outline an optimization strategy to\nobtain the pool size that maximizes the efficiency of pooling given the\ndiagnostic protocol parameters and local infection conditions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 20:47:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Srivastava", "Abhishek", ""], ["Mishra", "Anurag", ""], ["Parekh", "Trusha Jayant", ""], ["Jena", "Sampreeti", ""]]}, {"id": "2007.09811", "submitter": "Liangyu Zhu", "authors": "Liangyu Zhu, Wenbin Lu, Michael R. Kosorok, Rui Song", "title": "Kernel Assisted Learning for Personalized Dose Finding", "comments": "Accepted for KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individualized dose rule recommends a dose level within a continuous safe\ndose range based on patient level information such as physical conditions,\ngenetic factors and medication histories. Traditionally, personalized dose\nfinding process requires repeating clinical visits of the patient and frequent\nadjustments of the dosage. Thus the patient is constantly exposed to the risk\nof underdosing and overdosing during the process. Statistical methods for\nfinding an optimal individualized dose rule can lower the costs and risks for\npatients. In this article, we propose a kernel assisted learning method for\nestimating the optimal individualized dose rule. The proposed methodology can\nalso be applied to all other continuous decision-making problems. Advantages of\nthe proposed method include robustness to model misspecification and capability\nof providing statistical inference for the estimated parameters. In the\nsimulation studies, we show that this method is capable of identifying the\noptimal individualized dose rule and produces favorable expected outcomes in\nthe population. Finally, we illustrate our approach using data from a warfarin\ndosing study for thrombosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:03:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Liangyu", ""], ["Lu", "Wenbin", ""], ["Kosorok", "Michael R.", ""], ["Song", "Rui", ""]]}, {"id": "2007.09845", "submitter": "Spencer Woody", "authors": "Spencer Woody, Carlos M. Carvalho, P. Richard Hahn, and Jared S.\n  Murray", "title": "Estimating heterogeneous effects of continuous exposures using Bayesian\n  tree ensembles: revisiting the impact of abortion rates on crime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In estimating the causal effect of a continuous exposure or treatment, it is\nimportant to control for all confounding factors. However, most existing\nmethods require parametric specification for how control variables influence\nthe outcome or generalized propensity score, and inference on treatment effects\nis usually sensitive to this choice. Additionally, it is often the goal to\nestimate how the treatment effect varies across observed units. To address this\ngap, we propose a semiparametric model using Bayesian tree ensembles for\nestimating the causal effect of a continuous treatment of exposure which (i)\ndoes not require a priori parametric specification of the influence of control\nvariables, and (ii) allows for identification of effect modification by\npre-specified moderators. The main parametric assumption we make is that the\neffect of the exposure on the outcome is linear, with the steepness of this\nrelationship determined by a nonparametric function of the moderators, and we\nprovide heuristics to diagnose the validity of this assumption. We apply our\nmethods to revisit a 2001 study of how abortion rates affect incidence of\ncrime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:35:54 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Woody", "Spencer", ""], ["Carvalho", "Carlos M.", ""], ["Hahn", "P. Richard", ""], ["Murray", "Jared S.", ""]]}, {"id": "2007.10152", "submitter": "Ruoqi Liu", "authors": "Ruoqi Liu, Lai Wei, Ping Zhang", "title": "When deep learning meets causal inference: a computational framework for\n  drug repurposing from real-world data", "comments": "This version has been removed by arXiv administrators due to claim of\n  copyright infringement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drug repurposing is an effective strategy to identify new uses for existing\ndrugs, providing the quickest possible transition from bench to bedside.\nExisting methods for drug repurposing that mainly focus on pre-clinical\ninformation may exist translational issues when applied to human beings. Real\nworld data (RWD), such as electronic health records and insurance claims,\nprovide information on large cohorts of users for many drugs. Here we present\nan efficient and easily-customized framework for generating and testing\nmultiple candidates for drug repurposing using a retrospective analysis of\nRWDs. Building upon well-established causal inference and deep learning\nmethods, our framework emulates randomized clinical trials for drugs present in\na large-scale medical claims database. We demonstrate our framework in a case\nstudy of coronary artery disease (CAD) by evaluating the effect of 55\nrepurposing drug candidates on various disease outcomes. We achieve 6 drug\ncandidates that significantly improve the CAD outcomes but not have been\nindicated for treating CAD, paving the way for drug repurposing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:30:56 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Liu", "Ruoqi", ""], ["Wei", "Lai", ""], ["Zhang", "Ping", ""]]}, {"id": "2007.10160", "submitter": "Zhenzhong Wang", "authors": "Zhenzhong Wang, Zhengyuan Zhu, Cindy Yu", "title": "Variable Selection in Macroeconomic Forecasting with Many Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the data-rich environment, using many economic predictors to forecast a\nfew key variables has become a new trend in econometrics. The commonly used\napproach is factor augment (FA) approach. In this paper, we pursue another\ndirection, variable selection (VS) approach, to handle high-dimensional\npredictors. VS is an active topic in statistics and computer science. However,\nit does not receive as much attention as FA in economics. This paper introduces\nseveral cutting-edge VS methods to economic forecasting, which includes: (1)\nclassical greedy procedures; (2) l1 regularization; (3) gradient descent with\nsparsification and (4) meta-heuristic algorithms. Comprehensive simulation\nstudies are conducted to compare their variable selection accuracy and\nprediction performance under different scenarios. Among the reviewed methods, a\nmeta-heuristic algorithm called sequential Monte Carlo algorithm performs the\nbest. Surprisingly the classical forward selection is comparable to it and\nbetter than other more sophisticated algorithms. In addition, we apply these VS\nmethods on economic forecasting and compare with the popular FA approach. It\nturns out for employment rate and CPI inflation, some VS methods can achieve\nconsiderable improvement over FA, and the selected predictors can be well\nexplained by economic theories.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:33:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Zhenzhong", ""], ["Zhu", "Zhengyuan", ""], ["Yu", "Cindy", ""]]}, {"id": "2007.10163", "submitter": "Gimenez Olivier", "authors": "Daniel Turek, Claudia Wehrhahn, Olivier Gimenez", "title": "Bayesian Non-Parametric Detection Heterogeneity in Ecological Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection heterogeneity is inherent to ecological data, arising from factors\nsuch as varied terrain or weather conditions, inconsistent sampling effort, or\nheterogeneity of individuals themselves. Incorporating additional covariates\ninto a statistical model is one approach for addressing heterogeneity, but is\nno guarantee that any set of measurable covariates will adequately address the\nheterogeneity, and the presence of unmodelled heterogeneity has been shown to\nproduce biases in the resulting inferences. Other approaches for addressing\nheterogeneity include the use of random effects, or finite mixtures of\nhomogeneous subgroups. Here, we present a non-parametric approach for modelling\ndetection heterogeneity for use in a Bayesian hierarchical framework. We employ\na Dirichlet process mixture which allows a flexible number of population\nsubgroups without the need to pre-specify this number of subgroups as in a\nfinite mixture. We describe this non-parametric approach, then consider its use\nfor modelling detection heterogeneity in two common ecological motifs:\ncapture-recapture and occupancy modelling. For each, we consider a homogeneous\nmodel, finite mixture models, and the non-parametric approach. We compare these\napproaches using two simulation studies, and observe the non-parametric\napproach as the most reliable method for addressing varying degrees of\nheterogeneity. We also present two real-data examples, and compare the\ninferences resulting from each modelling approach. Analyses are carried out\nusing the \\texttt{nimble} package for \\texttt{R}, which provides facilities for\nBayesian non-parametric models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:40:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Turek", "Daniel", ""], ["Wehrhahn", "Claudia", ""], ["Gimenez", "Olivier", ""]]}, {"id": "2007.10164", "submitter": "Amir Weiss", "authors": "Amir Weiss and Boaz Nadler", "title": "\"Self-Wiener\" Filtering: Non-Iterative Data-Driven Robust Deconvolution\n  of Deterministic Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of robust deconvolution, and particularly\nthe recovery of an unknown deterministic signal convolved with a known filter\nand corrupted by additive noise. We present a novel, non-iterative data-driven\napproach. Specifically, our algorithm works in the frequency-domain, where it\ntries to mimic the optimal unrealizable Wiener-like filter as if the unknown\ndeterministic signal were known. This leads to a threshold-type regularized\nestimator, where the threshold value at each frequency is found in a fully\ndata-driven manner. We provide an analytical performance analysis, and derive\napproximate closed-form expressions for the residual Mean Squared Error (MSE)\nof our proposed estimator in the low and high Signal-to-Noise Ratio (SNR)\nregimes. We show analytically that in the low SNR regime our method provides\nenhanced noise suppression, and in the high SNR regime it approaches the\nperformance of the optimal unrealizable solution. Further, as we demonstrate in\nsimulations, our solution is highly suitable for (approximately) bandlimited or\nfrequency-domain sparse signals, and provides a significant gain of several dBs\nrelative to other methods in the resulting MSE.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:41:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Weiss", "Amir", ""], ["Nadler", "Boaz", ""]]}, {"id": "2007.10183", "submitter": "Linyi Zou", "authors": "Linyi Zou, Hui Guo and Carlo Berzuini", "title": "Overlapping-sample Mendelian randomisation with multiple exposures: A\n  Bayesian approach", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Mendelian randomization (MR) has been widely applied to causal\ninference in medical research. It uses genetic variants as instrumental\nvariables (IVs) to investigate putative causal relationship between an exposure\nand an outcome. Traditional MR methods have dominantly focussed on a two-sample\nsetting in which IV-exposure association study and IV-outcome association study\nare independent. However, it is not uncommon that participants from the two\nstudies fully overlap (one-sample) or partly overlap (overlapping-sample).\nMethods: We proposed a method that is applicable to all the three sample\nsettings. In essence, we converted a two- or overlapping- sample problem to a\none-sample problem where data of some or all of the individuals were\nincomplete. Assume that all individuals were drawn from the same population and\nunmeasured data were missing at random. Then the unobserved data were treated\nau pair with the model parameters as unknown quantities, and thus, could be\nimputed iteratively conditioning on the observed data and estimated parameters\nusing Markov chain Monte Carlo. We generalised our model to allow for\npleiotropy and multiple exposures and assessed its performance by a number of\nsimulations using four metrics: mean, standard deviation, coverage and power.\nResults: Higher sample overlapping rate and stronger instruments led to\nestimates with higher precision and power. Pleiotropy had a notably negative\nimpact on the estimates. Nevertheless, overall the coverages were high and our\nmodel performed well in all the sample settings. Conclusions: Our model offers\nthe flexibility of being applicable to any of the sample settings, which is an\nimportant addition to the MR literature which has restricted to one- or two-\nsample scenarios. Given the nature of Bayesian inference, it can be easily\nextended to more complex MR analysis in medical research.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:18:33 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 14:48:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zou", "Linyi", ""], ["Guo", "Hui", ""], ["Berzuini", "Carlo", ""]]}, {"id": "2007.10306", "submitter": "Stephen Pfohl", "authors": "Stephen R. Pfohl, Agata Foryciarz, Nigam H. Shah", "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk\n  Prediction", "comments": "Published in the Journal of Biomedical Informatics\n  (https://doi.org/10.1016/j.jbi.2020.103621). Version 3 updates\n  acknowledgements and fixes typos", "journal-ref": "Journal of Biomedical Informatics, Volume 113, January 2021,\n  103621", "doi": "10.1016/j.jbi.2020.103621", "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning to guide clinical decision making has the\npotential to worsen existing health disparities. Several recent works frame the\nproblem as that of algorithmic fairness, a framework that has attracted\nconsiderable attention and criticism. However, the appropriateness of this\nframework is unclear due to both ethical as well as technical considerations,\nthe latter of which include trade-offs between measures of fairness and model\nperformance that are not well-understood for predictive models of clinical\noutcomes. To inform the ongoing debate, we conduct an empirical study to\ncharacterize the impact of penalizing group fairness violations on an array of\nmeasures of model performance and group fairness. We repeat the analyses across\nmultiple observational healthcare databases, clinical outcomes, and sensitive\nattributes. We find that procedures that penalize differences between the\ndistributions of predictions across groups induce nearly-universal degradation\nof multiple performance metrics within groups. On examining the secondary\nimpact of these procedures, we observe heterogeneity of the effect of these\nprocedures on measures of fairness in calibration and ranking across\nexperimental conditions. Beyond the reported trade-offs, we emphasize that\nanalyses of algorithmic fairness in healthcare lack the contextual grounding\nand causal awareness necessary to reason about the mechanisms that lead to\nhealth disparities, as well as about the potential of algorithmic fairness\nmethods to counteract those mechanisms. In light of these limitations, we\nencourage researchers building predictive models for clinical use to step\noutside the algorithmic fairness frame and engage critically with the broader\nsociotechnical context surrounding the use of machine learning in healthcare.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:46:31 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:25:57 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:28:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pfohl", "Stephen R.", ""], ["Foryciarz", "Agata", ""], ["Shah", "Nigam H.", ""]]}, {"id": "2007.10492", "submitter": "P.-A. Absil", "authors": "P.-A. Absil, Ousmane Diao, Mouhamadou Diallo", "title": "Assessment of COVID-19 hospitalization forecasts from a simplified SIR\n  model", "comments": "Paper home page: https://sites.uclouvain.be/absil/2020.05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the SH model, a simplified version of the well-known SIR\ncompartmental model of infectious diseases. With optimized parameters and\ninitial conditions, this time-invariant two-parameter two-dimensional model is\nable to fit COVID-19 hospitalization data over several months with high\naccuracy (mean absolute percentage error below 15%). Moreover, we observed\nthat, when the model is trained on a suitable two-week period around the\nhospitalization peak for Belgium, it forecasts the subsequent three-month\ndecrease with mean absolute percentage error below 10%. However, when it is\ntrained in the increase phase, it is less successful at forecasting the\nsubsequent evolution.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:42:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Absil", "P. -A.", ""], ["Diao", "Ousmane", ""], ["Diallo", "Mouhamadou", ""]]}, {"id": "2007.10550", "submitter": "Zachary Terner", "authors": "Zachary Terner and Alexander Franks", "title": "Modeling Player and Team Performance in Basketball", "comments": "25 pages, 3 figures, supplement included before bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, analytics has started to revolutionize the game of\nbasketball: quantitative analyses of the game inform team strategy, management\nof player health and fitness, and how teams draft, sign, and trade players. In\nthis review, we focus on methods for quantifying and characterizing basketball\ngameplay. At the team level, we discuss methods for characterizing team\nstrategy and performance, while at the player level, we take a deep look into a\nmyriad of tools for player evaluation. This includes metrics for overall player\nvalue, defensive ability, and shot modeling, and methods for understanding\nperformance over multiple seasons via player production curves. We conclude\nwith a discussion on the future of basketball analytics, and in particular\nhighlight the need for causal inference in sports.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:30:17 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Terner", "Zachary", ""], ["Franks", "Alexander", ""]]}, {"id": "2007.10559", "submitter": "Harrison Quick", "authors": "Harrison Quick, Guangzi Song, and Loni Tabb", "title": "Evaluating the Informativeness of the Besag-York-Molli\\'e CAR Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the conditional autoregressive framework proposed by Besag, York,\nand Molli\\'e (1991; BYM) is ubiquitous in Bayesian disease mapping and spatial\nepidemiology. While it is understood that Bayesian inference is based on a\ncombination of the information contained in the data and the information\ncontributed by the model, quantifying the contribution of the model relative to\nthe information in the data is often non-trivial. Here, we provide a measure of\nthe contribution of the BYM framework by first considering the simple\nPoisson-gamma setting in which quantifying the prior's contribution is quite\nclear. We then propose a relationship between gamma and lognormal priors that\nwe then extend to cover the framework proposed by BYM. Following a brief\nsimulation study in which we illustrate the accuracy of our lognormal\napproximation of the gamma prior, we analyze a dataset comprised of\ncounty-level heart disease-related death data across the United States. In\naddition to demonstrating the potential for the BYM framework to correspond to\na highly informative prior specification, we also illustrate the sensitivity of\ndeath rate estimates to changes in the informativeness of the BYM framework.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:54:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Quick", "Harrison", ""], ["Song", "Guangzi", ""], ["Tabb", "Loni", ""]]}, {"id": "2007.10667", "submitter": "Juste Raimbault", "authors": "Juste Raimbault, Julien Perret and Romain Reuillon", "title": "A scala library for spatial sensitivity analysis", "comments": "GISRUK 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sensitivity analysis and validation of simulation models require specific\napproaches in the case of spatial models. We describe the spatialdata scala\nlibrary providing such tools, including synthetic generators for urban\nconfigurations at different scales, spatial networks, and spatial point\nprocesses. These can be used to parametrize geosimulation models on synthetic\nconfigurations, and evaluate the sensitivity of model outcomes to spatial\nconfiguration. The library also includes methods to perturb real data, and\nspatial statistics indicators, urban form indicators, and network indicators.\nIt is embedded into the OpenMOLE platform for model exploration, fostering the\napplication of such methods without technical constraints.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:00:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Raimbault", "Juste", ""], ["Perret", "Julien", ""], ["Reuillon", "Romain", ""]]}, {"id": "2007.10677", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Gautier Marti and Sumanta Ray and Saumyadipta Pyne", "title": "Clustering patterns connecting COVID-19 dynamics and Human mobility\n  using optimal transport", "comments": "16 pages", "journal-ref": "Sankhya B (16th March 2021)", "doi": "10.1007/s13571-021-00255-0", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social distancing and stay-at-home are among the few measures that are known\nto be effective in checking the spread of a pandemic such as COVID-19 in a\ngiven population. The patterns of dependency between such measures and their\neffects on disease incidence may vary dynamically and across different\npopulations. We described a new computational framework to measure and compare\nthe temporal relationships between human mobility and new cases of COVID-19\nacross more than 150 cities of the United States with relatively high incidence\nof the disease. We used a novel application of Optimal Transport for computing\nthe distance between the normalized patterns induced by bivariate time series\nfor each pair of cities. Thus, we identified 10 clusters of cities with similar\ntemporal dependencies, and computed the Wasserstein barycenter to describe the\noverall dynamic pattern for each cluster. Finally, we used city-specific\nsocioeconomic covariates to analyze the composition of each cluster.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:32:49 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Nielsen", "Frank", ""], ["Marti", "Gautier", ""], ["Ray", "Sumanta", ""], ["Pyne", "Saumyadipta", ""]]}, {"id": "2007.10727", "submitter": "Matthieu Garcin", "authors": "Ayoub Ammy-Driss and Matthieu Garcin", "title": "Efficiency of the financial markets during the COVID-19 crisis:\n  time-varying parameters of fractional stable dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.GN stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the impact of COVID-19 on financial markets. It\nfocuses on the evolution of the market efficiency, using two efficiency\nindicators: the Hurst exponent and the memory parameter of a fractional\nL\\'evy-stable motion. The second approach combines, in the same model of\ndynamic, an alpha-stable distribution and a dependence structure between price\nreturns. We provide a dynamic estimation method for the two efficiency\nindicators. This method introduces a free parameter, the discount factor, which\nwe select so as to get the best alpha-stable density forecasts for observed\nprice returns. The application to stock indices during the COVID-19 crisis\nshows a strong loss of efficiency for US indices. On the opposite, Asian and\nAustralian indices seem less affected and the inefficiency of these markets\nduring the COVID-19 crisis is even questionable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:39:41 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:16:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ammy-Driss", "Ayoub", ""], ["Garcin", "Matthieu", ""]]}, {"id": "2007.10882", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Renato Luiz de Freitas Cunha, Bruno Silva", "title": "Estimating crop yields with remote sensing and deep learning", "comments": "6 pages, 2 figures. Accepted for publication at 2020 Latin American\n  GRSS & ISPRS Remote Sensing Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the accuracy of crop yield estimates may allow improvements in the\nwhole crop production chain, allowing farmers to better plan for harvest, and\nfor insurers to better understand risks of production, to name a few\nadvantages. To perform their predictions, most current machine learning models\nuse NDVI data, which can be hard to use, due to the presence of clouds and\ntheir shadows in acquired images, and due to the absence of reliable crop masks\nfor large areas, especially in developing countries. In this paper, we present\na deep learning model able to perform pre-season and in-season predictions for\nfive different crops. Our model uses crop calendars, easy-to-obtain remote\nsensing data and weather forecast information to provide accurate yield\nestimates.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:09:11 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cunha", "Renato Luiz de Freitas", ""], ["Silva", "Bruno", ""]]}, {"id": "2007.10929", "submitter": "Zhijian Li", "authors": "Zhijian Li, Yunling Zheng, Jack Xin, and Guofa Zhou", "title": "A Recurrent Neural Network and Differential Equation Based\n  Spatiotemporal Infectious Disease Model with Application to COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreaks of Coronavirus Disease 2019 (COVID-19) have impacted the world\nsignificantly. Modeling the trend of infection and real-time forecasting of\ncases can help decision making and control of the disease spread. However,\ndata-driven methods such as recurrent neural networks (RNN) can perform poorly\ndue to limited daily samples in time. In this work, we develop an integrated\nspatiotemporal model based on the epidemic differential equations (SIR) and\nRNN. The former after simplification and discretization is a compact model of\ntemporal infection trend of a region while the latter models the effect of\nnearest neighboring regions. The latter captures latent spatial information.\n%that is not publicly reported. We trained and tested our model on COVID-19\ndata in Italy, and show that it out-performs existing temporal models (fully\nconnected NN, SIR, ARIMA) in 1-day, 3-day, and 1-week ahead forecasting\nespecially in the regime of limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:04:57 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 08:26:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Li", "Zhijian", ""], ["Zheng", "Yunling", ""], ["Xin", "Jack", ""], ["Zhou", "Guofa", ""]]}, {"id": "2007.11049", "submitter": "Nikola Surjanovic", "authors": "Nikola Surjanovic, Richard Lockhart, and Thomas M. Loughin", "title": "A Generalized Hosmer-Lemeshow Goodness-of-Fit Test for a Family of\n  Generalized Linear Models", "comments": "37 pages; modified/updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) are used within a vast number of application\ndomains. However, formal goodness of fit (GOF) tests for the overall fit of the\nmodel$-$so-called \"global\" tests$-$seem to be in wide use only for certain\nclasses of GLMs. In this paper we develop and apply a new global\ngoodness-of-fit test, similar to the well-known and commonly used\nHosmer-Lemeshow (HL) test, that can be used with a wide variety of GLMs. The\ntest statistic is a variant of the HL test statistic, but we rigorously derive\nan asymptotically correct sampling distribution of the test statistic using\nmethods of Stute and Zhu (2002). Our new test is relatively straightforward to\nimplement and interpret. We demonstrate the test on a real data set, and\ncompare the performance of our new test with other global GOF tests for GLMs,\nfinding that our test provides competitive or comparable power in various\nsimulation settings. Our test also avoids the use of kernel-based estimators,\nused in various GOF tests for regression, thereby avoiding the issues of\nbandwidth selection and the curse of dimensionality. Since the asymptotic\nsampling distribution is known, a bootstrap procedure for the calculation of a\np-value is also not necessary, and we therefore find that performing our test\nis computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:17:59 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 03:33:41 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Surjanovic", "Nikola", ""], ["Lockhart", "Richard", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "2007.11103", "submitter": "Kathryn Taylor", "authors": "Kathryn S. Taylor and James W. Taylor", "title": "A Comparison of Aggregation Methods for Probabilistic Forecasts of\n  COVID-19 Mortality in the United States", "comments": "32 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has placed forecasting models at the forefront of\nhealth policy making. Predictions of mortality and hospitalization help\ngovernments meet planning and resource allocation challenges. In this paper, we\nconsider the weekly forecasting of the cumulative mortality due to COVID-19 at\nthe national and state level in the U.S. Optimal decision-making requires a\nforecast of a probability distribution, rather than just a single point\nforecast. Interval forecasts are also important, as they can support decision\nmaking and provide situational awareness. We consider the case where\nprobabilistic forecasts have been provided by multiple forecasting teams, and\nwe aggregate the forecasts to extract the wisdom of the crowd. With only\nlimited information available regarding the historical accuracy of the\nforecasting teams, we consider aggregation (i.e. combining) methods that do not\nrely on a record of past accuracy. In this empirical paper, we evaluate the\naccuracy of aggregation methods that have been previously proposed for interval\nforecasts and predictions of probability distributions. These include the use\nof the simple average, the median, and trimming methods, which enable robust\nestimation and allow the aggregate forecast to reduce the impact of a tendency\nfor the forecasting teams to be under- or overconfident. We use data that has\nbeen made publicly available from the COVID-19 Forecast Hub. While the simple\naverage performed well for the high mortality series, we obtained greater\naccuracy using the median and certain trimming methods for the low and medium\nmortality series. It will be interesting to see if this remains the case as the\npandemic evolves.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:33:31 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 14:19:01 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Taylor", "Kathryn S.", ""], ["Taylor", "James W.", ""]]}, {"id": "2007.11123", "submitter": "Peng Liu", "authors": "Peng Liu, Yusi Fang, Zhao Ren, Lu Tang and George C. Tseng", "title": "Outcome-Guided Disease Subtyping for High-Dimensional Omics Data", "comments": "29 pages in total, 4 figures, 2 tables and 1 supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput microarray and sequencing technology have been used to\nidentify disease subtypes that could not be observed otherwise by using\nclinical variables alone. The classical unsupervised clustering strategy\nconcerns primarily the identification of subpopulations that have similar\npatterns in gene features. However, as the features corresponding to irrelevant\nconfounders (e.g. gender or age) may dominate the clustering process, the\nresulting clusters may or may not capture clinically meaningful disease\nsubtypes. This gives rise to a fundamental problem: can we find a subtyping\nprocedure guided by a pre-specified disease outcome? Existing methods, such as\nsupervised clustering, apply a two-stage approach and depend on an arbitrary\nnumber of selected features associated with outcome. In this paper, we propose\na unified latent generative model to perform outcome-guided disease subtyping\nconstructed from omics data, which improves the resulting subtypes concerning\nthe disease of interest. Feature selection is embedded in a regularization\nregression. A modified EM algorithm is applied for numerical computation and\nparameter estimation. The proposed method performs feature selection, latent\nsubtype characterization and outcome prediction simultaneously. To account for\npossible outliers or violation of mixture Gaussian assumption, we incorporate\nrobust estimation using adaptive Huber or median-truncated loss function.\nExtensive simulations and an application to complex lung diseases with\ntranscriptomic and clinical data demonstrate the ability of the proposed method\nto identify clinically relevant disease subtypes and signature genes suitable\nto explore toward precision medicine.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:56:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Yusi", ""], ["Ren", "Zhao", ""], ["Tang", "Lu", ""], ["Tseng", "George C.", ""]]}, {"id": "2007.11358", "submitter": "Ludwig Hothorn", "authors": "Charlotte Vogel, Frank Schaarschmidt, Christian Ritz, Franz Koenig and\n  Ludwig A. Hothorn", "title": "Model-based simultaneous inference for multiple subgroups and multiple\n  endpoints", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Various methodological options exist on evaluating differences in both\nsubgroups and the overall population. Most desirable is the simultaneous study\nof multiple endpoints in several populations. We investigate a newer method\nusing multiple marginal models (mmm) which allows flexible handling of multiple\nendpoints, including continuous, binary or time-to-event data. This paper\nexplores the performance of mmm in contrast to the standard Bonferroni approach\nvia simulation. Mainly these methods are compared on the basis of their\nfamilywise error rate and power under different scenarios, varying in sample\nsize and standard deviation. Additionally, it is shown that the method can deal\nwith overlapping subgroup definitions and different combinations of endpoints\nmay be assumed. The reanalysis of a clinical example shows a practical\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:14:04 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Vogel", "Charlotte", ""], ["Schaarschmidt", "Frank", ""], ["Ritz", "Christian", ""], ["Koenig", "Franz", ""], ["Hothorn", "Ludwig A.", ""]]}, {"id": "2007.11379", "submitter": "Mirko Fiacchini", "authors": "Mirko Fiacchini and Mazen Alamir", "title": "The Ockham's razor applied to COVID-19 model fitting French data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-based simple model for fitting the available data\nof the Covid-19 pandemic evolution in France. The time series concerning the 13\nregions of mainland France have been considered for fitting and validating the\nmodel. An extremely simple, two-dimensional model with only two parameters\ndemonstrated to be able to reproduce the time series concerning the number of\ndaily demises caused by Covid-19, the hospitalizations, intensive care and\nemergency accesses, the daily number of positive test and other indicators, for\nthe different French regions. These results might contribute to stimulate a\ndebate on the suitability of much more complex models for reproducing and\nforecasting the pandemic evolution since, although relevant from a mechanistic\npoint of view, they could lead to nonidentifiability issues.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 05:48:41 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Fiacchini", "Mirko", ""], ["Alamir", "Mazen", ""]]}, {"id": "2007.11418", "submitter": "Christopher Geoga", "authors": "Christopher J. Geoga and Mihai Anitescu and Michael L. Stein", "title": "Flexible nonstationary spatio-temporal modeling of high-frequency\n  monitoring data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many physical datasets are generated by collections of instruments that make\nmeasurements at regular time intervals. For such regular monitoring data, we\nextend the framework of half-spectral covariance functions to the case of\nnonstationarity in space and time and demonstrate that this method provides a\nnatural and tractable way to incorporate complex behaviors into a covariance\nmodel. Further, we use this method with fully time-domain computations to\nobtain bona fide maximum likelihood estimators---as opposed to using\nWhittle-type likelihood approximations, for example---that can still be\ncomputed efficiently. We apply this method to very high-frequency Doppler LIDAR\nvertical wind velocity measurements, demonstrating that the model can\nexpressively capture the extreme nonstationarity of dynamics above and below\nthe atmospheric boundary layer and, more importantly, the interaction of the\nprocess dynamics across it.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:28:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Geoga", "Christopher J.", ""], ["Anitescu", "Mihai", ""], ["Stein", "Michael L.", ""]]}, {"id": "2007.11638", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu (1 and 2), Emma J. McCoy (1) ((1) Imperial College\n  London, (2) ASOS.com)", "title": "An Evaluation Framework for Personalization Strategy Experiment Designs", "comments": "Presented in the AdKDD 2020 workshop, in conjunction with The 26th\n  ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2020. Main\n  paper: 7 pages, 2 figures, 2 tables, Supplementary document: 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Controlled Experiments (OCEs) are the gold standard in evaluating the\neffectiveness of changes to websites. An important type of OCE evaluates\ndifferent personalization strategies, which present challenges in low test\npower and lack of full control in group assignment. We argue that getting the\nright experiment setup -- the allocation of users to treatment/analysis groups\n-- should take precedence of post-hoc variance reduction techniques in order to\nenable the scaling of the number of experiments. We present an evaluation\nframework that, along with a few simple rule of thumbs, allow experimenters to\nquickly compare which experiment setup will lead to the highest probability of\ndetecting a treatment effect under their particular circumstance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:21:39 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 18:58:45 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Liu", "C. H. Bryan", "", "1 and 2"], ["McCoy", "Emma J.", ""]]}, {"id": "2007.11674", "submitter": "Jorge Gaxiola", "authors": "J.A. Gaxiola-Tirado", "title": "Using EEG-based brain connectivity for the study of brain dynamics in\n  brain-computer interfaces", "comments": "in Spanish", "journal-ref": "Revista Doctorado UMH, 2020", "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of brain connectivity aims to understand the emergence of\nfunctional networks into the brain. This information can be used in the process\nof electroencephalographic (EEG) signal analysis and classification for a\nbraincomputer interface (BCI). These systems provide an alternative channel of\ncommunication and control to people with motor impairments. In this article,\nfour strategies for using the brain connectivity in a BCI environment as a tool\nto obtain a deeper understanding of the cerebral mechanisms are proposed, with\nthe principal aim of developing a scheme oriented to neuro-rehabilitation of\ngait in combination with different neurotechnologies and exoskeletons. This\nscheme would allow improving current schemes and/or to design new control\nstrategies, as well as rehabilitation approaches.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 18:05:14 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gaxiola-Tirado", "J. A.", ""]]}, {"id": "2007.11698", "submitter": "Vittorio Lippi", "authors": "V. Lippi, L. Assl\\\"ander, E. Akcay, and T. Mergner", "title": "Body sway responses to pseudorandom support surface translations of\n  vestibular loss subjects resemble those of vestibular able subjects", "comments": null, "journal-ref": null, "doi": "10.1016/j.neulet.2020.135271", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body sway responses evoked by a horizontal acceleration of a level and firm\nsupport surface are particular in that the vestibular information on body-space\nangle BS resembles the proprioceptive information on body-foot angle BF. We\ncompared corresponding eyes-closed responses of vestibular-able (VA) and\nvestibular-loss (VL) subjects, postulating a close correspondence. In\ncontradistinction to previous studies, we used an unpredictable (pseudorandom)\nstimulus and found that the eyes-closed and eyes-open responses of the VA\nclosely resembled those of the VL subjects, as expected. We further conclude\nthat the vestibular signals coding head linear translation in VA subjects has\nin this case too little functional relevance to cause a notable difference\nbetween the subject groups.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:04:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lippi", "V.", ""], ["Assl\u00e4nder", "L.", ""], ["Akcay", "E.", ""], ["Mergner", "T.", ""]]}, {"id": "2007.11700", "submitter": "Claudia Wehrhahn", "authors": "Claudia Wehrhahn, Ruth Fuentes-Garc\\'ia, Rams\\'es H. Mena, Fabrizio\n  Leisen, Maria Elena Gonz\\'alez-Villalpando, and Clicerio\n  Gonz\\'alez-Villalpando", "title": "A Copula-based Fully Bayesian Nonparametric Evaluation of Cardiovascular\n  Risk Markers in the Mexico City Diabetes Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease lead the cause of death world wide and several studies\nhave been carried out to understand and explore cardiovascular risk markers in\nnormoglycemic and diabetic populations. In this work, we explore the\nassociation structure between hyperglycemic markers and cardiovascular risk\nmarkers controlled by triglycerides, body mass index, age and gender, for the\nnormoglycemic population in The Mexico City Diabetes Study. Understanding the\nassociation structure could contribute to the assessment of additional\ncardiovascular risk markers in this low income urban population with a high\nprevalence of classic cardiovascular risk biomarkers. The association structure\nis measured by conditional Kendall's tau, defined through conditional copula\nfunctions. The latter are in turn modeled under a fully Bayesian nonparametric\napproach, which allows the complete shape of the copula function to vary for\ndifferent values of the controlled covariates.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:13:19 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 21:33:16 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wehrhahn", "Claudia", ""], ["Fuentes-Garc\u00eda", "Ruth", ""], ["Mena", "Rams\u00e9s H.", ""], ["Leisen", "Fabrizio", ""], ["Gonz\u00e1lez-Villalpando", "Maria Elena", ""], ["Gonz\u00e1lez-Villalpando", "Clicerio", ""]]}, {"id": "2007.11811", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Avleen S. Bijral", "title": "On a Bernoulli Autoregression Framework for Link Discovery and\n  Prediction", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic prediction framework for binary sequences that is based\non a Bernoulli generalization of the auto-regressive process. Our approach\nlends itself easily to variants of the standard link prediction problem for a\nsequence of time dependent networks. Focusing on this dynamic network link\nprediction/recommendation task, we propose a novel problem that exploits\nadditional information via a much larger sequence of auxiliary networks and has\nimportant real-world relevance. To allow discovery of links that do not exist\nin the available data, our model estimation framework introduces a\nregularization term that presents a trade-off between the conventional link\nprediction and this discovery task. In contrast to existing work our stochastic\ngradient based estimation approach is highly efficient and can scale to\nnetworks with millions of nodes. We show extensive empirical results on both\nactual product-usage based time dependent networks and also present results on\na Reddit based data set of time dependent sentiment sequences.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:58:22 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bijral", "Avleen S.", ""]]}, {"id": "2007.11896", "submitter": "Atalanti Mastakouri", "authors": "Atalanti A. Mastakouri and Bernhard Sch\\\"olkopf", "title": "Causal analysis of Covid-19 spread in Germany", "comments": "correction of two citations", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the causal relations among German regions in terms of\nthe spread of Covid-19 since the beginning of the pandemic, taking into account\nthe restriction policies that were applied by the different federal states. We\npropose and prove a new theorem for a causal feature selection method for time\nseries data, robust to latent confounders, which we subsequently apply on\nCovid-19 case numbers. We present findings about the spread of the virus in\nGermany and the causal impact of restriction measures, discussing the role of\nvarious policies in containing the spread. Since our results are based on\nrather limited target time series (only the numbers of reported cases), care\nshould be exercised in interpreting them. However, it is encouraging that\nalready such limited data seems to contain causal signals. This suggests that\nas more data becomes available, our causal approach may contribute towards\nmeaningful causal analysis of political interventions on the development of\nCovid-19, and thus also towards the development of rational and data-driven\nmethodologies for choosing interventions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:08:27 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 08:50:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mastakouri", "Atalanti A.", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2007.11972", "submitter": "Yuxiao Li", "authors": "Yuxiao Li, Ying Sun, Brian J Reich", "title": "DeepKriging: Spatially Dependent Deep Neural Networks for Spatial\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spatial statistics, a common objective is to predict the values of a\nspatial process at unobserved locations by exploiting spatial dependence. In\ngeostatistics, Kriging provides the best linear unbiased predictor using\ncovariance functions and is often associated with Gaussian processes. However,\nwhen considering non-linear prediction for non-Gaussian and categorical data,\nthe Kriging prediction is not necessarily optimal, and the associated variance\nis often overly optimistic. We propose to use deep neural networks (DNNs) for\nspatial prediction. Although DNNs are widely used for general classification\nand prediction, they have not been studied thoroughly for data with spatial\ndependence. In this work, we propose a novel neural network structure for\nspatial prediction by adding an embedding layer of spatial coordinates with\nbasis functions. We show in theory that the proposed DeepKriging method has\nmultiple advantages over Kriging and classical DNNs only with spatial\ncoordinates as features. We also provide density prediction for uncertainty\nquantification without any distributional assumption and apply the method to\nPM$_{2.5}$ concentrations across the continental United States.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:38:53 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 08:15:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Yuxiao", ""], ["Sun", "Ying", ""], ["Reich", "Brian J", ""]]}, {"id": "2007.12031", "submitter": "Jeong-Soo Park", "authors": "Yire Shin, Piyapatr Busababodhin, Jeong-Soo Park", "title": "The r-largest four parameter kappa distribution", "comments": "6 figures, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized extreme value distribution (GEVD) has been widely used to\nmodel the extreme events in many areas. It is however limited to using only\nblock maxima, which motivated to model the GEVD dealing with $r$-largest order\nstatistics (rGEVD). The rGEVD which uses more than one extreme per block can\nsignificantly improves the performance of the GEVD. The four parameter kappa\ndistribution (K4D) is a generalization of some three-parameter distributions\nincluding the GEVD. It can be useful in fitting data when three parameters in\nthe GEVD are not sufficient to capture the variability of the extreme\nobservations. The K4D still uses only block maxima. In this study, we thus\nextend the K4D to deal with $r$-largest order statistics as analogy as the GEVD\nis extended to the rGEVD. The new distribution is called the $r$-largest four\nparameter kappa distribution (rK4D). We derive a joint probability density\nfunction (PDF) of the rK4D, and the marginal and conditional cumulative\ndistribution functions and PDFs. The maximum likelihood method is considered to\nestimate parameters. The usefulness and some practical concerns of the rK4D are\nillustrated by applying it to Venice sea-level data. This example study shows\nthat the rK4D gives better fit but larger variances of the parameter estimates\nthan the rGEVD. Some new $r$-largest distributions are derived as special cases\nof the rK4D, such as the $r$-largest logistic (rLD), generalized logistic\n(rGLD), and generalized Gumbel distributions (rGGD).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:19:32 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Shin", "Yire", ""], ["Busababodhin", "Piyapatr", ""], ["Park", "Jeong-Soo", ""]]}, {"id": "2007.12242", "submitter": "Ruili Huang", "authors": "Hu Zhu, Catherine Z. Chen, Srilatha Sakamuru, Anton Simeonov, Mathew\n  D. Hall, Menghang Xia, Wei Zheng, Ruili Huang", "title": "Mining of high throughput screening database reveals AP-1 and autophagy\n  pathways as potential targets for COVID-19 therapeutics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent global pandemic of Coronavirus Disease 2019 (COVID-19) caused by\nthe new coronavirus SARS-CoV-2 presents an urgent need for new therapeutic\ncandidates. Many efforts have been devoted to screening existing drug libraries\nwith the hope to repurpose approved drugs as potential treatments for COVID-19.\nHowever, the antiviral mechanisms of action for the drugs found active in these\nphenotypic screens are largely unknown. To deconvolute the viral targets for\nmore effective anti-COVID-19 drug development, we mined our in-house database\nof approved drug screens against 994 assays and compared their activity\nprofiles with the drug activity profile in a cytopathic effect (CPE) assay of\nSARS-CoV-2. We found that the autophagy and AP-1 signaling pathway activity\nprofiles are significantly correlated with the anti-SARS-CoV-2 activity\nprofile. In addition, a class of neurology/psychiatry drugs was found\nsignificantly enriched with anti-SARS-CoV-2 activity. Taken together, these\nresults have provided new insights into SARS-CoV-2 infection and potential\ntargets for COVID-19 therapeutics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:30:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhu", "Hu", ""], ["Chen", "Catherine Z.", ""], ["Sakamuru", "Srilatha", ""], ["Simeonov", "Anton", ""], ["Hall", "Mathew D.", ""], ["Xia", "Menghang", ""], ["Zheng", "Wei", ""], ["Huang", "Ruili", ""]]}, {"id": "2007.12419", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn, Atiar M. Rahman and Frank Schaarschmidt", "title": "A versatile trend test for the evaluation of tumor incidences in\n  long-term carcinogenicity bioassays", "comments": "6 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For the evaluation of carcinogenicity bioassays a new trend test is proposed\nwhich is based on a maximum of arithmetic, ordinal, and logarithmic regression\nscores as well as the Williams-type contrasts for either crude proportions or\nmore appropriate poly3-estimates for the tumor-by-time relationships. This test\nprovides an almost appropriate power for most shapes of dose-response\nrelationships (including for possible downturn effect at high(er) dose(s)),\ncommon signs of significance (p-value, confidence limits) and the information\non the probable shape. Related software is easily available within the\nCRAN-packages tukeytrend, MCPAN, multcomp.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:16:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Rahman", "Atiar M.", ""], ["Schaarschmidt", "Frank", ""]]}, {"id": "2007.12644", "submitter": "Ian Fellows", "authors": "Ian E. Fellows, Rachel B. Slayton and Avi J. Hakim", "title": "The COVID-19 Pandemic, Community Mobility and the Effectiveness of\n  Non-pharmaceutical Interventions: The United States of America, February to\n  May 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The impact of individual non-pharmaceutical interventions (NPI)\nsuch as state-wide stay-at-home orders, school closures and gathering size\nlimitations, on the COVID-19 epidemic is unknown. Understanding the impact that\nabove listed NPI have on disease transmission is critical for policy makers,\nparticularly as case counts increase again in some areas.\n  Methods: Using a Bayesian framework, we reconstructed the incidence and\ntime-varying reproductive number (Rt) curves to investigate the relationship\nbetween Rt, individual mobility as measured by Google Community Mobility\nReports, and NPI.\n  Results: We found a strong relationship between reproductive number and\nmobility, with each 10% drop in mobility being associated with an expected\n10.2% reduction in Rt compared to baseline. The effects of limitations on the\nsize of gatherings, school and business closures, and stay-at-home orders were\ndominated by the trend over time, which was associated with a 48% decrease in\nthe reproductive number, adjusting for the NPI.\n  Conclusions: We found that the decrease in mobility associated with time may\nbe due to individuals changing their behavior in response to perceived risk or\nexternal factors.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:18:44 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Fellows", "Ian E.", ""], ["Slayton", "Rachel B.", ""], ["Hakim", "Avi J.", ""]]}, {"id": "2007.12769", "submitter": "Weijia Zhang", "authors": "Weijia Zhang, Jiuyong Li, Lin Liu", "title": "A unified survey of treatment effect heterogeneity modeling and uplift\n  modeling", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in many fields of scientific research is to determine how\nan outcome would be affected by an action, or to measure the effect of an\naction (a.k.a treatment effect). In recent years, a need for estimating the\nheterogeneous treatment effects conditioning on the different characteristics\nof individuals has emerged from research fields such as personalized\nhealthcare, social science, and online marketing. To meet the need, researchers\nand practitioners from different communities have developed algorithms by\ntaking the treatment effect heterogeneity modeling approach and the uplift\nmodeling approach, respectively. In this paper, we provide a unified survey of\nthese two seemingly disconnected yet closely related approaches under the\npotential outcome framework. We then provide a structured survey of existing\nmethods by emphasizing on their inherent connections with a set of unified\nnotations to make comparisons of the different methods easy. We then review the\nmain applications of the surveyed methods in personalized marketing,\npersonalized medicine, and social studies. Finally, we summarize the existing\nsoftware packages and present discussions based on the use of methods on\nsynthetic, semi-synthetic and real world data sets and provide some general\nguidelines for choosing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:16:02 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 13:26:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Weijia", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""]]}, {"id": "2007.12843", "submitter": "Jorge Gaxiola", "authors": "Jorge Antonio Gaxiola Tirado", "title": "Preliminary Assessment of hands motor imagery in theta- and beta-bands\n  for Brain-Machine-Interfaces using functional connectivity analysis", "comments": "Article accepted and presented at IGS2019- YOUR BRAIN ON ART", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of time- and frequency-based features has proven effective in the\nprocess of classifying mental tasks in Brain Computer Interfaces (BCIs). Still,\nmost of those methods provide little insight about the underlying brain\nactivity and functions. Thus, a better understanding of the mechanisms and\ndynamics of brain activity, is necessary in order to obtain useful and\ninformative features for BCIs. In the present study, the objective is to\ninvestigate the differences in functional connectivity of two motor imagery\ntasks, through a partial directed coherence (PDC) analysis, which is a\nfrequency-domain metric that provides information about directionality in the\ninteraction between signals recorded at different channels. Four healthy\nsubjects participated in this study, two mental tasks were evaluated:\nImagination of the movement of the right hand or left hand. We carry out the\ndifferentiation of these tasks through two different approaches: on one hand,\nthe traditional one based on spectral power; on the other hand, an approach\nbased on PDC. The results showed that EEG-based PDC analysis provides\nadditional information and it can potentially improve the feature selection\nmainly in the beta frequency band.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 03:18:02 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tirado", "Jorge Antonio Gaxiola", ""]]}, {"id": "2007.12852", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari and Mingyuan Zhou", "title": "Graph Gamma Process Generalized Linear Dynamical Systems", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce graph gamma process (GGP) linear dynamical systems to model\nreal-valued multivariate time series. For temporal pattern discovery, the\nlatent representation under the model is used to decompose the time series into\na parsimonious set of multivariate sub-sequences. In each sub-sequence,\ndifferent data dimensions often share similar temporal patterns but may exhibit\ndistinct magnitudes, and hence allowing the superposition of all sub-sequences\nto exhibit diverse behaviors at different data dimensions. We further\ngeneralize the proposed model by replacing the Gaussian observation layer with\nthe negative binomial distribution to model multivariate count time series.\nGenerated from the proposed GGP is an infinite dimensional directed sparse\nrandom graph, which is constructed by taking the logical OR operation of\ncountably infinite binary adjacency matrices that share the same set of\ncountably infinite nodes. Each of these adjacency matrices is associated with a\nweight to indicate its activation strength, and places a finite number of edges\nbetween a finite subset of nodes belonging to the same node community. We use\nthe generated random graph, whose number of nonzero-degree nodes is finite, to\ndefine both the sparsity pattern and dimension of the latent state transition\nmatrix of a (generalized) linear dynamical system. The activation strength of\neach node community relative to the overall activation strength is used to\nextract a multivariate sub-sequence, revealing the data pattern captured by the\ncorresponding community. On both synthetic and real-world time series, the\nproposed nonparametric Bayesian dynamic models, which are initialized at\nrandom, consistently exhibit good predictive performance in comparison to a\nvariety of baseline models, revealing interpretable latent state transition\npatterns and decomposing the time series into distinctly behaved sub-sequences.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 04:16:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kalantari", "Rahi", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2007.12919", "submitter": "Ly Antoine PhD", "authors": "Dimitri Delcaillau, Antoine Ly, Franck Vermet, Aliz\\'e Papp", "title": "Interpretabilit\\'e des mod\\`eles : \\'etat des lieux des m\\'ethodes et\n  application \\`a l'assurance", "comments": "25 pages without appendix, submitted to BFA, French preprint before\n  English paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since May 2018, the General Data Protection Regulation (GDPR) has introduced\nnew obligations to industries. By setting a legal framework, it notably imposes\nstrong transparency on the use of personal data. Thus, people must be informed\nof the use of their data and must consent the usage of it. Data is the raw\nmaterial of many models which today make it possible to increase the quality\nand performance of digital services. Transparency on the use of data also\nrequires a good understanding of its use through different models. The use of\nmodels, even if efficient, must be accompanied by an understanding at all\nlevels of the process that transform data (upstream and downstream of a model),\nthus making it possible to define the relationships between the individual's\ndata and the choice that an algorithm could make based on the analysis of the\nlatter. (For example, the recommendation of one product or one promotional\noffer or an insurance rate representative of the risk.) Models users must\nensure that models do not discriminate against and that it is also possible to\nexplain its result. The widening of the panel of predictive algorithms - made\npossible by the evolution of computing capacities -- leads scientists to be\nvigilant about the use of models and to consider new tools to better understand\nthe decisions deduced from them . Recently, the community has been particularly\nactive on model transparency with a marked intensification of publications over\nthe past three years. The increasingly frequent use of more complex algorithms\n(\\textit{deep learning}, Xgboost, etc.) presenting attractive performances is\nundoubtedly one of the causes of this interest. This article thus presents an\ninventory of methods of interpreting models and their uses in an insurance\ncontext.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 12:18:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Delcaillau", "Dimitri", ""], ["Ly", "Antoine", ""], ["Vermet", "Franck", ""], ["Papp", "Aliz\u00e9", ""]]}, {"id": "2007.12974", "submitter": "Andrew Yiu", "authors": "Andrew Yiu (1), Robert J. B. Goudie (1), Stephen J. Sharp (2), Paul J.\n  Newcombe (1) and Brian D. M. Tom (1) ((1) MRC Biostatistics Unit, University\n  of Cambridge, UK, (2) MRC Epidemiology Unit, University of Cambridge, UK)", "title": "A Bayesian framework for case-cohort Cox regression: application to\n  dietary epidemiology", "comments": "34 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The case-cohort study design bypasses resource constraints by collecting\ncertain expensive covariates for only a small subset of the full cohort.\nWeighted Cox regression is the most widely used approach for analyzing\ncase-cohort data within the Cox model, but is inefficient. Alternative\napproaches based on multiple imputation and nonparametric maximum likelihood\nsuffer from incompatibility and computational issues respectively. We introduce\na novel Bayesian framework for case-cohort Cox regression that avoids the\naforementioned problems. Users can include auxiliary variables to help predict\nthe unmeasured expensive covariates with a prediction model of their choice,\nwhile the models for the nuisance parameters are nonparametrically specified\nand integrated out. Posterior sampling can be carried out using procedures\nbased on the pseudo-marginal MCMC algorithm. The method scales effectively to\nlarge, complex datasets, as demonstrated in our application: investigating the\nassociations between saturated fatty acids and type 2 diabetes using the\nEPIC-Norfolk study. As part of our analysis, we also develop a new approach for\nhandling compositional data in the Cox model, leading to more reliable and\ninterpretable results compared to previous studies. The performance of our\nmethod is illustrated with extensive simulations, including the use of\nsynthetic data generated by resampling from the application dataset. The code\nused to produce the results in this paper can be found at\nhttps://github.com/andrewyiu/bayes_cc .\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 16:52:43 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yiu", "Andrew", ""], ["Goudie", "Robert J. B.", ""], ["Sharp", "Stephen J.", ""], ["Newcombe", "Paul J.", ""], ["Tom", "Brian D. M.", ""]]}, {"id": "2007.13037", "submitter": "Celso Cabral", "authors": "C. R. B. Cabral, N. L. de Souza, J. Le\\~ao", "title": "Bayesian Measurement Error Models Using Finite Mixtures of Scale\n  Mixtures of Skew-Normal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proposal to deal with the non-normality issue in the context of\nregression models with measurement errors when both the response and the\nexplanatory variable are observed with error. We extend the normal model by\njointly modeling the unobserved covariate and the random errors by a finite\nmixture of scale mixture of skew-normal distributions. This approach allows us\nto model data with great flexibility, accommodating skewness, heavy tails, and\nmulti-modality.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 00:29:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cabral", "C. R. B.", ""], ["de Souza", "N. L.", ""], ["Le\u00e3o", "J.", ""]]}, {"id": "2007.13255", "submitter": "Milad Asgari Mehrabadi", "authors": "Milad Asgari Mehrabadi, Nikil Dutt and Amir M. Rahmani", "title": "The Causality Inference of Public Interest in Restaurants and Bars on\n  COVID-19 Daily Cases in the US: A Google Trends Analysis", "comments": "6 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 coronavirus pandemic has affected virtually every region of the\nglobe. At the time of conducting this study, the number of daily cases in the\nUnited States is more than any other country, and the trend is increasing in\nmost of its states. Google trends provide public interest in various topics\nduring different periods. Analyzing these trends using data mining methods\nmight provide useful insights and observations regarding the COVID-19 outbreak.\nThe objective of this study was to consider the predictive ability of different\nsearch terms (i.e., bars and restaurants) with regards to the increase of daily\ncases in the US. We considered the causation of two different search query\ntrends, namely restaurant and bars, on daily positive cases in top-10\nstates/territories of the United States with the highest and lowest daily new\npositive cases. In addition, to measure the linear relation of different\ntrends, we used Pearson correlation. Our results showed for states/territories\nwith higher numbers of daily cases, the historical trends in search queries\nrelated to bars and restaurants, which mainly happened after re-opening,\nsignificantly affect the daily new cases, on average. California, for example,\nhad most searches for restaurants on June 7th, 2020, which affected the number\nof new cases within two weeks after the peak with the P-value of .004 for\nGranger's causality test. Although a limited number of search queries were\nconsidered, Google search trends for restaurants and bars showed a significant\neffect on daily new cases for regions with higher numbers of daily new cases in\nthe United States. We showed that such influential search trends could be used\nas additional information for prediction tasks in new cases of each region.\nThis prediction can help healthcare leaders manage and control the impact of\nCOVID-19 outbreaks on society and be prepared for the outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:29:06 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mehrabadi", "Milad Asgari", ""], ["Dutt", "Nikil", ""], ["Rahmani", "Amir M.", ""]]}, {"id": "2007.13351", "submitter": "Subhadip Maji", "authors": "Subhadip Maji, Raghav Bali, Sree Harsha Ankem and Kishore V Ayyadevara", "title": "A Simple and Interpretable Predictive Model for Healthcare", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning based models are currently dominating most state-of-the-art\nsolutions for disease prediction. Existing works employ RNNs along with\nmultiple levels of attention mechanisms to provide interpretability. These deep\nlearning models, with trainable parameters running into millions, require huge\namounts of compute and data to train and deploy. These requirements are\nsometimes so huge that they render usage of such models as unfeasible. We\naddress these challenges by developing a simpler yet interpretable non-deep\nlearning based model for application to EHR data. We model and showcase our\nwork's results on the task of predicting first occurrence of a diagnosis, often\noverlooked in existing works. We push the capabilities of a tree based model\nand come up with a strong baseline for more sophisticated models. Its\nperformance shows an improvement over deep learning based solutions (both, with\nand without the first-occurrence constraint) all the while maintaining\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:13:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Maji", "Subhadip", ""], ["Bali", "Raghav", ""], ["Ankem", "Sree Harsha", ""], ["Ayyadevara", "Kishore V", ""]]}, {"id": "2007.13446", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen and Kristine B Walhovd and Anders M Fjell", "title": "A recipe for accurate estimation of lifespan brain trajectories,\n  distinguishing longitudinal and cohort effects", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2020.117596", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating how different parts of the brain develop\nand change throughout the lifespan, and how these trajectories are affected by\ngenetic and environmental factors. Estimation of these lifespan trajectories is\nstatistically challenging, since their shapes are typically highly nonlinear,\nand although true change can only be quantified by longitudinal examinations,\nas follow-up intervals in neuroimaging studies typically cover less than 10 \\%\nof the lifespan, use of cross-sectional information is necessary. Linear mixed\nmodels (LMMs) and structural equation models (SEMs) commonly used in\nlongitudinal analysis rely on assumptions which are typically not met with\nlifespan data, in particular when the data consist of observations combined\nfrom multiple studies. While LMMs require a priori specification of a\npolynomial functional form, SEMs do not easily handle data with unstructured\ntime intervals between measurements. Generalized additive mixed models (GAMMs)\noffer an attractive alternative, and in this paper we propose various ways of\nformulating GAMMs for estimation of lifespan trajectories of 12 brain regions,\nusing a large longitudinal dataset and realistic simulation experiments. We\nshow that GAMMs are able to more accurately fit lifespan trajectories,\ndistinguish longitudinal and cross-sectional effects, and estimate effects of\ngenetic and environmental exposures. Finally, we discuss and contrast questions\nrelated to lifespan research which strictly require repeated measures data and\nquestions which can be answered with a single measurement per participant, and\nin the latter case, which simplifying assumptions that need to be made. The\nexamples are accompanied with R code, providing a tutorial for researchers\ninterested in using GAMMs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:35:33 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 06:56:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Walhovd", "Kristine B", ""], ["Fjell", "Anders M", ""]]}, {"id": "2007.13454", "submitter": "Jan Markus Brauner", "authors": "Mrinank Sharma, S\\\"oren Mindermann, Jan Markus Brauner, Gavin Leech,\n  Anna B. Stephenson, Tom\\'a\\v{s} Gaven\\v{c}iak, Jan Kulveit, Yee Whye Teh,\n  Leonid Chindelevitch, Yarin Gal", "title": "How Robust are the Estimated Effects of Nonpharmaceutical Interventions\n  against COVID-19?", "comments": null, "journal-ref": "NeurIPS 2020, Advances in Neural Information Processing Systems 33", "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To what extent are effectiveness estimates of nonpharmaceutical interventions\n(NPIs) against COVID-19 influenced by the assumptions our models make? To\nanswer this question, we investigate 2 state-of-the-art NPI effectiveness\nmodels and propose 6 variants that make different structural assumptions. In\nparticular, we investigate how well NPI effectiveness estimates generalise to\nunseen countries, and their sensitivity to unobserved factors. Models that\naccount for noise in disease transmission compare favourably. We further\nevaluate how robust estimates are to different choices of epidemiological\nparameters and data. Focusing on models that assume transmission noise, we find\nthat previously published results are remarkably robust across these variables.\nFinally, we mathematically ground the interpretation of NPI effectiveness\nestimates when certain common assumptions do not hold.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:49:54 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:47:54 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 15:35:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sharma", "Mrinank", ""], ["Mindermann", "S\u00f6ren", ""], ["Brauner", "Jan Markus", ""], ["Leech", "Gavin", ""], ["Stephenson", "Anna B.", ""], ["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["Kulveit", "Jan", ""], ["Teh", "Yee Whye", ""], ["Chindelevitch", "Leonid", ""], ["Gal", "Yarin", ""]]}, {"id": "2007.13530", "submitter": "Andreas Wagner", "authors": "Enislay Ramentol, Florian Schirra, Andreas Wagner", "title": "Short- and long-term forecasting of electricity prices using embedding\n  of calendar information in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity prices strongly depend on seasonality on different time scales,\ntherefore any forecasting of electricity prices has to account for it. Neural\nnetworks proved successful in forecasting, but complicated architectures like\nLSTM are used to integrate the seasonal behavior. This paper shows that simple\nneural networks architectures like DNNs with an embedding layer for seasonality\ninformation deliver not only a competitive but superior forecast. The embedding\nbased processing of calendar information additionally opens up new applications\nfor neural networks in electricity trading like the generation of price forward\ncurves. Besides the theoretical foundation, this paper also provides an\nempirical multi-year study on the German electricity market for both\napplications and derives economical insights from the embedding layer. The\nstudy shows that in short-term price-forecasting the mean absolute error of the\nproposed neural networks with embedding layer is only about half of the mean\nabsolute forecast error of state-of-the-art LSTM approaches. The predominance\nof the proposed approach is also supported by a statistical analysis using\nFriedman and Holm's tests.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:07:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ramentol", "Enislay", ""], ["Schirra", "Florian", ""], ["Wagner", "Andreas", ""]]}, {"id": "2007.13553", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (LNE, L2S), Julien Bect (L2S, GdR MASCOT-NUM), S\\'everine\n  Demeyer (LNE), Nicolas Fischer (LNE), Damien Marquis (LNE), Emmanuel Vazquez\n  (L2S, GdR MASCOT-NUM)", "title": "Sequential design of multi-fidelity computer experiments: maximizing the\n  rate of stepwise uncertainty reduction", "comments": "Technometrics, Taylor & Francis", "journal-ref": null, "doi": "10.1080/00401706.2021.1935324", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the sequential design of experiments for\n(deterministic or stochastic) multi-fidelity numerical simulators, that is,\nsimulators that offer control over the accuracy of simulation of the physical\nphenomenon or system under study. Very often, accurate simulations correspond\nto high computational efforts whereas coarse simulations can be obtained at a\nsmaller cost. In this setting, simulation results obtained at several levels of\nfidelity can be combined in order to estimate quantities of interest (the\noptimal value of the output, the probability that the output exceeds a given\nthreshold...) in an efficient manner. To do so, we propose a new Bayesian\nsequential strategy called Maximal Rate of Stepwise Uncertainty Reduction\n(MR-SUR), that selects additional simulations to be performed by maximizing the\nratio between the expected reduction of uncertainty and the cost of simulation.\nThis generic strategy unifies several existing methods, and provides a\nprincipled approach to develop new ones. We assess its performance on several\nexamples, including a computationally intensive problem of fire safety analysis\nwhere the quantity of interest is the probability of exceeding a tenability\nthreshold during a building fire.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:34:12 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:37:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE, L2S"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Marquis", "Damien", "", "LNE"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2007.13570", "submitter": "Rahul Roy", "authors": "Rahul Roy, Trivikram Dokka, David A. Ellis, Esther Dudek, Paul\n  Barnfather", "title": "Understanding controlled EV charging impacts using scenario-based\n  forecasting models", "comments": "22 pages (including references and appendix); revised title to\n  include new analysis and results", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrification of transport is a key strategy in reducing carbon emissions.\nMany countries have adopted policies of complete but gradual transformation to\nelectric vehicles (EVs). However, mass EV adoption also means a spike in load\n(kW), which in turn can disrupt existing electricity infrastructure. Smart or\ncontrolled charging is widely seen as a potential solution to alleviate this\nstress on existing networks. Learning from the recent EV trials in the UK and\nelsewhere we take into account two key aspects which are largely ignored in\ncurrent research: EVs actually charging at any given time and wide range of EV\ntypes, especially battery capacity-wise. Taking a minimalistic scenario-based\napproach, we study forecasting models for mean number of active chargers and\nmean EV consumption for distinct scenarios. Focusing on residential charging\nthe models we consider range from simple regression models to more advanced\nmachine and deep learning models such as XGBoost and LSTMs. We then use these\nmodels to evaluate the impacts of different levels of future EV penetration on\na specimen distribution transformer that captures typical real-world scenarios.\nIn doing so, we also initiate the study of different types of controlled\ncharging when fully controlled charging is not possible. This aligns with the\noutcomes from recent trials which show that a sizeable proportion of EV owners\nmay not prefer fully controlled centralized charging. We study two possible\ncontrol regimes and show that one is more beneficial from load-on-transformer\npoint of view, while the other may be preferred for other objectives. We show\nthat a minimum of 60% control is required to ensure that transformers are not\noverloaded during peak hours.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 08:37:09 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 14:40:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Roy", "Rahul", ""], ["Dokka", "Trivikram", ""], ["Ellis", "David A.", ""], ["Dudek", "Esther", ""], ["Barnfather", "Paul", ""]]}, {"id": "2007.13712", "submitter": "Hsin-Hsiung Huang", "authors": "Chih-Wei Chen, Charles Harrison, and Hsin-Hsiung Huang", "title": "The Unsupervised Method of Vessel Movement Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world application scenarios, it is crucial for marine navigators and\nsecurity analysts to predict vessel movement trajectories at sea based on the\nAutomated Identification System (AIS) data in a given time span. This article\npresents an unsupervised method of ship movement trajectory prediction which\nrepresents the data in a three-dimensional space which consists of time\ndifference between points, the scaled error distance between the tested and its\npredicted forward and backward locations, and the space-time angle. The\nrepresentation feature space reduces the search scope for the next point to a\ncollection of candidates which fit the local path prediction well, and\ntherefore improve the accuracy. Unlike most statistical learning or deep\nlearning methods, the proposed clustering-based trajectory reconstruction\nmethod does not require computationally expensive model training. This makes\nreal-time reliable and accurate prediction feasible without using a training\nset. Our results show that the most prediction trajectories accurately consist\nof the true vessel paths.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:45:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 03:46:27 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 15:42:43 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Chih-Wei", ""], ["Harrison", "Charles", ""], ["Huang", "Hsin-Hsiung", ""]]}, {"id": "2007.13734", "submitter": "Chris von Csefalvay", "authors": "Chris von Csefalvay", "title": "Statistical dynamics of social distancing in SARS-CoV-2 as a\n  differential game", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel coronavirus SARS-CoV-2 has rapidly emerged as a significant threat\nto global public health, in particular because -- as is not uncommon with novel\npathogens -- there is no effective pharmaceutical treatment or prophylaxis to\nthe viral syndrome it causes. In the absence of such specific treatment\nmodalities, the mainstay of public health response rests on non-pharmaceutical\ninterventions (NPIs), such as social distancing. This paper contributes to the\nunderstanding of social distancing against SARS-CoV-2 by quantitatively\nanalysing the statistical dynamics of disease propagation as a differential\ngame, and estimating the relative costs of distancing versus not distancing,\nidentifying marginal utility of distancing based on known population\nepidemiological data about SARS-CoV-2 and concluding that unless the costs of\ndistancing vastly exceed the cost of illness per unit time, social distancing\nremains a dominant strategy. These findings can assist in solidly anchoring\npublic health responses based on social distancing within a quantitative\nframework attesting to their effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:58:39 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["von Csefalvay", "Chris", ""]]}, {"id": "2007.13741", "submitter": "Jing Xu", "authors": "Jing Xu, Xiaoxi Yan, Caroline Figueroa, Joseph Jay Williams, Bibhas\n  Chakraborty", "title": "Multi-Level Micro-Randomized Trial: Detecting the Proximal Effect of\n  Messages on Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advancements in mobile devices have made it possible to deliver\nmobile health interventions to individuals. A novel intervention framework that\nemerges from such advancements is the just-in-time adaptive intervention\n(JITAI), where it aims to suggest the right support to the individual \"just in\ntime\", when their needs arise, thus having proximal, near future effects. The\nmicro-randomized trial (MRT) design was proposed recently to test the proximal\neffects of these JITAIs. In an MRT, participants are repeatedly randomized to\none of the intervention options of various in the intervention components, at a\nscale of hundreds or thousands of decision time points over the course of the\nstudy. However, the extant MRT framework only tests the proximal effect of\ntwo-level intervention components (e.g. control vs intervention). In this\npaper, we propose a novel version of MRT design with multiple levels per\nintervention component, which we call \"multi-level micro-randomized trial\"\n(MLMRT) design. The MLMRT extends the existing MRT design by allowing\nmulti-level intervention components, and the addition of more levels to the\ncomponents during the study period. We apply generalized estimating equation\ntype methodology on the longitudinal data arising from an MLMRT to develop the\nnovel test statistics for assessing the proximal effects and deriving the\nassociated sample size calculators. We conduct simulation studies to evaluate\nthe sample size calculators based on both power and precision. We have\ndeveloped an R shiny application of the sample size calculators. This proposed\ndesign is motivated by our involvement in the Diabetes and Mental Health\nAdaptive Notification Tracking and Evaluation (DIAMANTE) study. This study uses\na novel mobile application, also called \"DIAMANTE\", which delivers adaptive\ntext messages to encourage physical activity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:53:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Xu", "Jing", ""], ["Yan", "Xiaoxi", ""], ["Figueroa", "Caroline", ""], ["Williams", "Joseph Jay", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "2007.13821", "submitter": "Adrian Bayer", "authors": "Adrian E. Bayer, Uros Seljak", "title": "The look-elsewhere effect from a unified Bayesian and frequentist\n  perspective", "comments": "21 pages, 7 figures", "journal-ref": "JCAP 10 (2020) 009", "doi": "10.1088/1475-7516/2020/10/009", "report-no": null, "categories": "physics.data-an astro-ph.CO astro-ph.IM hep-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching over a large parameter space for anomalies such as events,\npeaks, objects, or particles, there is a large probability that spurious\nsignals with seemingly high significance will be found. This is known as the\nlook-elsewhere effect and is prevalent throughout cosmology, (astro)particle\nphysics, and beyond. To avoid making false claims of detection, one must\naccount for this effect when assigning the statistical significance of an\nanomaly. This is typically accomplished by considering the trials factor, which\nis generally computed numerically via potentially expensive simulations. In\nthis paper we develop a continuous generalization of the Bonferroni and Sidak\ncorrections by applying the Laplace approximation to evaluate the Bayes factor,\nand in turn relating the trials factor to the prior-to-posterior volume ratio.\nWe use this to define a test statistic whose frequentist properties have a\nsimple interpretation in terms of the global $p$-value, or statistical\nsignificance. We apply this method to various physics-based examples and show\nit to work well for the full range of $p$-values, i.e. in both the asymptotic\nand non-asymptotic regimes. We also show that this method naturally accounts\nfor other model complexities such as additional degrees of freedom,\ngeneralizing Wilks' theorem. This provides a fast way to quantify statistical\nsignificance in light of the look-elsewhere effect, without resorting to\nexpensive simulations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:25:49 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bayer", "Adrian E.", ""], ["Seljak", "Uros", ""]]}, {"id": "2007.13847", "submitter": "Claire Donnat", "authors": "Claire Donnat, Nina Miolane, Freddy Bunbury, Jack Kreindler", "title": "A Bayesian Hierarchical Network for Combining Heterogeneous Data Sources\n  in Medical Diagnoses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Diagnosis has shown stellar performance in providing accurate\nmedical diagnoses across multiple testing modalities (medical images,\nelectrophysiological signals, etc.). While this field has typically focused on\nfully harvesting the signal provided by a single (and generally extremely\nreliable) modality, fewer efforts have utilized imprecise data lacking reliable\nground truth labels. In this unsupervised, noisy setting, the robustification\nand quantification of the diagnosis uncertainty become paramount, thus posing a\nnew challenge: how can we combine multiple sources of information -- often\nthemselves with vastly varying levels of precision and uncertainty -- to\nprovide a diagnosis estimate with confidence bounds? Motivated by a concrete\napplication in antibody testing, we devise a Stochastic\nExpectation-Maximization algorithm that allows the principled integration of\nheterogeneous, and potentially unreliable, data types. Our Bayesian formalism\nis essential in (a) flexibly combining these heterogeneous data sources and\ntheir corresponding levels of uncertainty, (b) quantifying the degree of\nconfidence associated with a given diagnostic, and (c) dealing with the missing\nvalues that typically plague medical data. We quantify the potential of this\napproach on simulated data, and showcase its practicality by deploying it on a\nreal COVID-19 immunity study.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:22:57 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 20:34:27 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Donnat", "Claire", ""], ["Miolane", "Nina", ""], ["Bunbury", "Freddy", ""], ["Kreindler", "Jack", ""]]}, {"id": "2007.13902", "submitter": "Kirk Bansak", "authors": "Jeremy Ferwerda, Nicholas Adams-Cohen, Kirk Bansak, Jennifer Fei,\n  Duncan Lawrence, Jeremy M. Weinstein, Jens Hainmueller", "title": "Leveraging the Power of Place: A Data-Driven Decision Helper to Improve\n  the Location Decisions of Economic Immigrants", "comments": "51 pages (including appendix), 13 figures. Immigration Policy Lab\n  (IPL) Working Paper Series, Working Paper No. 20-06", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of countries have established programs to attract immigrants\nwho can contribute to their economy. Research suggests that an immigrant's\ninitial arrival location plays a key role in shaping their economic success.\nYet immigrants currently lack access to personalized information that would\nhelp them identify optimal destinations. Instead, they often rely on\navailability heuristics, which can lead to the selection of sub-optimal landing\nlocations, lower earnings, elevated outmigration rates, and concentration in\nthe most well-known locations. To address this issue and counteract the effects\nof cognitive biases and limited information, we propose a data-driven decision\nhelper that draws on behavioral insights, administrative data, and machine\nlearning methods to inform immigrants' location decisions. The decision helper\nprovides personalized location recommendations that reflect immigrants'\npreferences as well as data-driven predictions of the locations where they\nmaximize their expected earnings given their profile. We illustrate the\npotential impact of our approach using backtests conducted with administrative\ndata that links landing data of recent economic immigrants from Canada's\nExpress Entry system with their earnings retrieved from tax records.\nSimulations across various scenarios suggest that providing location\nrecommendations to incoming economic immigrants can increase their initial\nearnings and lead to a mild shift away from the most populous landing\ndestinations. Our approach can be implemented within existing institutional\nstructures at minimal cost, and offers governments an opportunity to harness\ntheir administrative data to improve outcomes for economic immigrants.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:02:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Ferwerda", "Jeremy", ""], ["Adams-Cohen", "Nicholas", ""], ["Bansak", "Kirk", ""], ["Fei", "Jennifer", ""], ["Lawrence", "Duncan", ""], ["Weinstein", "Jeremy M.", ""], ["Hainmueller", "Jens", ""]]}, {"id": "2007.14052", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "A. F. L\\'opez-Lopera, D. Idier, J. Rohmer, F. Bachoc", "title": "Multi-Output Gaussian Processes with Functional Data: A Study on Coastal\n  Flood Hazard Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing coastal flood Forecast and Early-Warning Systems do not\nmodel the flood, but instead, rely on the prediction of hydrodynamic conditions\nat the coast and on expert judgment. Recent scientific contributions are now\ncapable to precisely model flood events, even in situations where wave\novertopping plays a significant role. Such models are nevertheless\ncostly-to-evaluate and surrogate ones need to be exploited for substantial\ncomputational savings. For the latter models, the hydro-meteorological forcing\nconditions (inputs) or flood events (outputs) are conveniently parametrised\ninto scalar representations. However, they neglect the fact that inputs are\nactually functions (more precisely, time series), and that floods spatially\npropagate inland. Here, we introduce a multi-output Gaussian process model\naccounting for both criteria. On various examples, we test its versatility for\nboth learning spatial maps and inferring unobserved ones. We demonstrate that\nefficient implementations are obtained by considering tensor-structured data\nand/or sparse-variational approximations. Finally, the proposed framework is\napplied on a coastal application aiming at predicting flood events. We conclude\nthat accurate predictions are obtained in the order of minutes rather than the\ncouples of days required by dedicated hydrodynamic simulators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:15:17 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:34:30 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["L\u00f3pez-Lopera", "A. F.", ""], ["Idier", "D.", ""], ["Rohmer", "J.", ""], ["Bachoc", "F.", ""]]}, {"id": "2007.14109", "submitter": "Alessandro Gasparini", "authors": "Emma C. Martin and Alessandro Gasparini and Michael J. Crowther", "title": "merlin: An R package for Mixed Effects Regression for Linear, Nonlinear\n  and User-defined models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package merlin performs flexible joint modelling of hierarchical\nmulti-outcome data. Increasingly, multiple longitudinal biomarker measurements,\npossibly censored time-to-event outcomes and baseline characteristics are\navailable. However, there is limited software that allows all of this\ninformation to be incorporated into one model. In this paper, we present merlin\nwhich allows for the estimation of models with unlimited numbers of continuous,\nbinary, count and time-to-event outcomes, with unlimited levels of nested\nrandom effects. A wide variety of link functions, including the expected value,\nthe gradient and shared random effects, are available in order to link the\ndifferent outcomes in a biologically plausible way. The accompanying\npredict.merlin function allows for individual and population level predictions\nto be made from even the most complex models. There is the option to specify\nuser-defined families, making merlin ideal for methodological research. The\nflexibility of merlin is illustrated using an example in patients followed up\nafter heart valve replacement, beginning with a linear model, and finishing\nwith a joint multiple longitudinal and competing risks survival model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:26:00 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Martin", "Emma C.", ""], ["Gasparini", "Alessandro", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2007.14296", "submitter": "Adam Davey", "authors": "Adam Davey and Ting Dai", "title": "A systematic approach to identify and evaluate missing data patterns and\n  mechanisms in multivariate educational, social, and behavioral research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods for addressing missing data have become much more accessible to\napplied researchers. However, little guidance exists to help researchers\nsystematically identify plausible missing data mechanisms in order to ensure\nthat these methods are appropriately applied. Two considerations motivate the\npresent study. First, psychological research is typically characterized by a\nlarge number of potential response variables that may be observed across\nmultiple waves of data collection. This situation makes it more challenging to\nidentify plausible missing data mechanisms than is the case in other fields\nsuch as biostatistics where a small number of dependent variables is typically\nof primary interest and the main predictor of interest is statistically\nindependent of other covariates. Second, there is growing recognition of the\nimportance of systematic approaches to sensitivity analyses for treatment of\nmissing data in psychological science. We develop and apply a systematic\napproach for reducing a large number of observed patterns and demonstrate how\nthese can be used to explore potential missing data mechanisms within\nmultivariate contexts. A large scale simulation study is used to guide\nsuggestions for which approaches are likely to be most accurate as a function\nof sample size, number of factors, number of indicators per factor, and\nproportion of missing data. Three applications of this approach to data\nexamples suggest that the method appears useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:11:03 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Davey", "Adam", ""], ["Dai", "Ting", ""]]}, {"id": "2007.14299", "submitter": "Rapha\\\"elle Momal", "authors": "Rapha\\\"elle Momal, St\\'ephane Robin, Christophe Ambroise", "title": "Accounting for missing actors in interaction network inference from\n  abundance data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network inference aims at unraveling the dependency structure relating\njointly observed variables. Graphical models provide a general framework to\ndistinguish between marginal and conditional dependency. Unobserved variables\n(missing actors) may induce apparent conditional dependencies.In the context of\ncount data, we introduce a mixture of Poisson log-normal distributions with\ntree-shaped graphical models, to recover the dependency structure, including\nmissing actors. We design a variational EM algorithm and assess its performance\non synthetic data. We demonstrate the ability of our approach to recover\nenvironmental drivers on two ecological datasets. The corresponding R package\nis available from github.com/Rmomal/nestor.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:15:19 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Momal", "Rapha\u00eblle", ""], ["Robin", "St\u00e9phane", ""], ["Ambroise", "Christophe", ""]]}, {"id": "2007.14336", "submitter": "Anne Woods", "authors": "Anne Woods (1), Craig Meyer (2), Brian Sauer (3 and 4), Beth Cohen (2\n  and 5) ((1) Northern California Institute for Research and Education (2)\n  University of California, San Francisco (3) University of Utah, Division of\n  Epidemiology, Salt Lake City, Utah (4) Salt Lake City Veterans Affairs\n  Medical Center, Health Services Research and Development (IDEAS) Center (5)\n  San Francisco VA Medical Center)", "title": "Mining Time-Stamped Electronic Health Records Using Referenced Sequences", "comments": "Submitted to The American Statistician on July 27, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records (EHRs) are typically stored as time-stamped\nencounter records. Observing temporal relationship between medical records is\nan integral part of interpreting the information. Hence, statistical analysis\nof EHRs requires that clinically informed time-interdependent analysis\nvariables (TIAV) be created. Often, formulation and creation of these variables\nare iterative and requiring custom codes. We describe a technique of using\nsequences of time-referenced entities as the building blocks for TIAVs. These\nsequences represent different aspects of patient's medical history in a\ncontiguous fashion. To illustrate the principles and applications of the\nmethod, we provide examples using Veterans Health Administration's research\ndatabases. In the first example, sequences representing medication exposure\nwere used to assess patient selection criteria for a treatment comparative\neffectiveness study. In the second example, sequences of Charlson Comorbidity\nconditions and clinical settings of inpatient or outpatient were used to create\nvariables with which data anomalies and trends were revealed. The third example\ndemonstrated the creation of an analysis variable derived from the temporal\ndependency of medication exposure and comorbidity. Complex time-interdependent\nanalysis variables can be created from the sequences with simple, reusable\ncodes, hence enable unscripted or automation of TIAV creation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:12:20 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Woods", "Anne", "", "3 and 4"], ["Meyer", "Craig", "", "3 and 4"], ["Sauer", "Brian", "", "3 and 4"], ["Cohen", "Beth", "", "2\n  and 5"]]}, {"id": "2007.14520", "submitter": "David Ledbetter", "authors": "David Ledbetter, Eugene Laksana, Melissa Aczon, Randall Wetzel", "title": "Improving Recurrent Neural Network Responsiveness to Acute Clinical\n  Events", "comments": "7 pages, 5 figures, 4 figures, 2 pages of Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models in acute care settings must be able to immediately\nrecognize precipitous changes in a patient's status when presented with data\nreflecting such changes. Recurrent neural networks (RNNs) have become common\nfor training and deploying clinical decision support models. They frequently\nexhibit a delayed response to acute events. New information must propagate\nthrough the RNN's cell state memory before the total impact is reflected in the\nmodel's predictions. This work presents input data perseveration as a method of\ntraining and deploying an RNN model to make its predictions more responsive to\nnewly acquired information: input data is replicated during training and\ndeployment. Each replication of the data input impacts the cell state and\noutput of the RNN, but only the output at the final replication is maintained\nand broadcast as the prediction for evaluation and deployment purposes. When\npresented with data reflecting acute events, a model trained and deployed with\ninput perseveration responds with more pronounced immediate changes in\npredictions and maintains globally robust performance. Such a characteristic is\ncrucial in predictive models for an intensive care unit.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 23:10:46 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ledbetter", "David", ""], ["Laksana", "Eugene", ""], ["Aczon", "Melissa", ""], ["Wetzel", "Randall", ""]]}, {"id": "2007.14694", "submitter": "Michail Tsagris", "authors": "Ioanna Papadaki and Michail Tsagris", "title": "Estimating NBA players salary share according to their performance on\n  court: A machine learning approach", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is customary for researchers and practitioners to fit linear models in\norder to predict NBA player's salary based on the players' performance on\ncourt. On the contrary, we focus on the players salary share (with regards to\nthe team payroll) by first selecting the most important determinants or\nstatistics (years of experience in the league, games played, etc.) and then\nutilise them to predict the player salaries by employing a non linear Random\nForest machine learning algorithm. We externally evaluate our salary\npredictions, thus we avoid the phenomenon of over-fitting observed in most\npapers. Overall, using data from three distinct periods, 2017-2019 we identify\nthe important factors that achieve very satisfactory salary predictions and we\ndraw useful conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:21:27 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 19:15:30 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 20:15:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Papadaki", "Ioanna", ""], ["Tsagris", "Michail", ""]]}, {"id": "2007.14810", "submitter": "Andrea Cappozzo", "authors": "Andrea Cappozzo, Francesca Greselin, Thomas Brendan Murphy", "title": "Robust variable selection for model-based learning in presence of\n  adulteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying the most discriminating features when performing\nsupervised learning has been extensively investigated. In particular, several\nmethods for variable selection in model-based classification have been\nproposed. Surprisingly, the impact of outliers and wrongly labeled units on the\ndetermination of relevant predictors has received far less attention, with\nalmost no dedicated methodologies available in the literature. In the present\npaper, we introduce two robust variable selection approaches: one that embeds a\nrobust classifier within a greedy-forward selection procedure and the other\nbased on the theory of maximum likelihood estimation and irrelevance. The\nformer recasts the feature identification as a model selection problem, while\nthe latter regards the relevant subset as a model parameter to be estimated.\nThe benefits of the proposed methods, in contrast with non-robust solutions,\nare assessed via an experiment on synthetic data. An application to a\nhigh-dimensional classification problem of contaminated spectroscopic data\nconcludes the paper.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:49:13 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 14:59:26 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Cappozzo", "Andrea", ""], ["Greselin", "Francesca", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2007.14841", "submitter": "Vadim Balashov", "authors": "Vadim S. Balashov, Yuxing Yan, Xiaodi Zhu", "title": "Who Manipulates Data During Pandemics? Evidence from Newcomb-Benford Law", "comments": "37 pages, 11 tables, 2 Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the Newcomb-Benford law to test if countries have manipulated reported\ndata during the COVID-19 pandemic. We find that democratic countries, countries\nwith the higher gross domestic product (GDP) per capita, higher healthcare\nexpenditures, and better universal healthcare coverage are less likely to\ndeviate from the Newcomb-Benford law. The relationship holds for the cumulative\nnumber of reported deaths and total cases but is more pronounced for the death\ntoll. The findings are robust for second-digit tests, for a sub-sample of\ncountries with regional data, and in relation to the previous swine flu (H1N1)\n2009-2010 pandemic. The paper further highlights the importance of independent\nsurveillance data verification projects.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 13:58:46 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 18:47:03 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 11:59:48 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 17:15:19 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Balashov", "Vadim S.", ""], ["Yan", "Yuxing", ""], ["Zhu", "Xiaodi", ""]]}, {"id": "2007.14874", "submitter": "Lennart Oelschl\\\"ager", "authors": "Lennart Oelschl\\\"ager and Timo Adam", "title": "Detecting bearish and bullish markets in financial time series using\n  hierarchical hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets exhibit alternating periods of rising and falling prices.\nStock traders seeking to make profitable investment decisions have to account\nfor those trends, where the goal is to accurately predict switches from bullish\ntowards bearish markets and vice versa. Popular tools for modeling financial\ntime series are hidden Markov models, where a latent state process is used to\nexplicitly model switches among different market regimes. In their basic form,\nhowever, hidden Markov models are not capable of capturing both short- and\nlong-term trends, which can lead to a misinterpretation of short-term price\nfluctuations as changes in the long-term trend. In this paper, we demonstrate\nhow hierarchical hidden Markov models can be used to draw a comprehensive\npicture of financial markets, which can contribute to the development of more\nsophisticated trading strategies. The feasibility of the suggested approach is\nillustrated in two real-data applications, where we model data from two major\nstock indices, the Deutscher Aktienindex and the Standard & Poor's 500.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:50:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Oelschl\u00e4ger", "Lennart", ""], ["Adam", "Timo", ""]]}, {"id": "2007.14900", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis, Lambros Mertzanis, Athina Panotopoulou, Ioannis\n  Papageorgiou, and Maria Skoularidou", "title": "Bayesian Context Trees: Modelling and exact inference for discrete time\n  series", "comments": "50 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Bayesian modelling framework for the class of higher-order,\nvariable-memory Markov chains, and introduce an associated collection of\nmethodological tools for exact inference with discrete time series. We show\nthat a version of the context tree weighting algorithm can compute the prior\npredictive likelihood exactly (averaged over both models and parameters), and\ntwo related algorithms are introduced, which identify the a posteriori most\nlikely models and compute their exact posterior probabilities. All three\nalgorithms are deterministic and have linear-time complexity. A family of\nvariable-dimension Markov chain Monte Carlo samplers is also provided,\nfacilitating further exploration of the posterior. The performance of the\nproposed methods in model selection, Markov order estimation and prediction is\nillustrated through simulation experiments and real-world applications with\ndata from finance, genetics, neuroscience, and animal communication.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:16:49 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Mertzanis", "Lambros", ""], ["Panotopoulou", "Athina", ""], ["Papageorgiou", "Ioannis", ""], ["Skoularidou", "Maria", ""]]}, {"id": "2007.14910", "submitter": "Soham Poddar", "authors": "Soham Poddar, Mainack Mondal and Saptarshi Ghosh", "title": "A Survey on Disaster: Understanding the After-effects of Super-cyclone\n  Amphan and Helping Hand of Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The super-cyclonic storm \"Amphan\" hit Eastern India, specifically the state\nof West Bengal, Odisha and parts of Bangladesh in May 2020, and caused severe\ndamage to the regions. In this study, we aim to understand the self-reported\neffects of this natural disaster on residents of the state of West Bengal. To\nthat end, we conducted an online survey to understand the effects of the\ncyclone. In total, 201 participants (spanning five districts) from the\nworst-affected state of West Bengal participated in the survey. This report\ndescribes our findings from the survey, with respect to the damages caused by\nthe cyclone, how it affected the population in various districts of West\nBengal, and how prepared the authorities were in responding to the disaster. We\nfound that the participants were most adversely affected in this disaster due\nto disruption of services like electricity, phone and internet (as opposed to\nuprooting of trees and water-logging). Furthermore, we found that receiving\nresponses to Amphan-related queries is highly positively correlated with the\nfavorable perception of people about preparedness of authorities. Additionally,\nwe study the usage of online social media by the affected population in the\ndays immediately after the disaster. Our results strongly suggest how social\nmedia platforms can help authorities to better prepare for future disasters. In\nsummary, our study analyzes self-reported data collected from grassroots, and\nbrings out several key insights that can help authorities deal better with\ndisaster events in future.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:30:26 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Poddar", "Soham", ""], ["Mondal", "Mainack", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2007.14961", "submitter": "Mario Beraha", "authors": "Mario Beraha, Matteo Pegoraro, Riccardo Peli and Alessandra Guglielmi", "title": "Spatially dependent mixture models via the Logistic Multivariate CAR\n  prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of spatially dependent areal data, where for each\narea independent observations are available, and propose to model the density\nof each area through a finite mixture of Gaussian distributions. The spatial\ndependence is introduced via a novel joint distribution for a collection of\nvectors in the simplex, that we term logisticMCAR. We show that salient\nfeatures of the logisticMCAR distribution can be described analytically, and\nthat a suitable augmentation scheme based on the P\\'olya-Gamma identity allows\nto derive an efficient Markov Chain Monte Carlo algorithm. When compared to\ncompetitors, our model has proved to better estimate densities in different\n(disconnected) areal locations when they have different characteristics. We\ndiscuss an application on a real dataset of Airbnb listings in the city of\nAmsterdam, also showing how to easily incorporate for additional covariate\ninformation in the model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:09:34 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:35:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Beraha", "Mario", ""], ["Pegoraro", "Matteo", ""], ["Peli", "Riccardo", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2007.14975", "submitter": "Mikael Kuusela", "authors": "Pratik Patil, Mikael Kuusela and Jonathan Hobbs", "title": "Objective frequentist uncertainty quantification for atmospheric CO$_2$\n  retrievals", "comments": "39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steadily increasing amount of atmospheric carbon dioxide (CO$_2$) is\naffecting the global climate system and threatening the long-term\nsustainability of Earth's ecosystem. In order to better understand the sources\nand sinks of CO$_2$, NASA operates the Orbiting Carbon Observatory-2 & 3\nsatellites to monitor CO$_2$ from space. These satellites make passive radiance\nmeasurements of the sunlight reflected off the Earth's surface in different\nspectral bands, which are then inverted to obtain estimates of the atmospheric\nCO$_2$ concentration. In this work, we first analyze the current operational\nretrieval procedure, which uses prior knowledge in the form of probability\ndistributions on the relevant atmospheric state variables to regularize the\nunderlying ill-posed inverse problem, and demonstrate that the resulting\nuncertainties might be poorly calibrated both at individual locations and over\na spatial region. To alleviate these issues, we propose a new method that uses\nknown physical constraints on the state variables and direct inversion of the\ntarget functionals of the CO$_2$ profile to construct well-calibrated\nfrequentist confidence intervals based on convex programming. Furthermore, we\nstudy the influence of individual nuisance state variables on the length of the\nconfidence intervals and identify certain key variables that can greatly reduce\nthe final uncertainty given additional deterministic or probabilistic\nconstraints, and develop a principled framework to incorporate such information\ninto our method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:41:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Patil", "Pratik", ""], ["Kuusela", "Mikael", ""], ["Hobbs", "Jonathan", ""]]}, {"id": "2007.15031", "submitter": "David Mori\\~na Prof.", "authors": "Gilma Hern\\'andez-Herrera, Albert Navarro and David Mori\\~na", "title": "Regression-based imputation of explanatory discrete missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of missing values is a strategy for handling non-responses in\nsurveys or data loss in measurement processes, which may be more effective than\nignoring them. When the variable represents a count, the literature dealing\nwith this issue is scarce. Likewise, if problems of over- or under-dispersion\nare observed, generalisations of the Poisson distribution are recommended for\ncarrying out imputation. In order to assess the performance of various\nregression models in the imputation of a discrete variable compared to\nclassical counting models, this work presents a comprehensive simulation study\nconsidering a variety of scenarios and real data. To do so we compared the\nresults of estimations using only complete data, and using imputations based on\nthe Poisson, negative binomial, Hermite, and COMPoisson distributions, and the\nZIP and ZINB models for excesses of zeros. The results of this work reveal that\nthe COMPoisson distribution provides in general better results in any\ndispersion scenario, especially when the amount of missing information is\nlarge. When the variable presenting missing values is a count, the most widely\nused method is to assume that a classical Poisson model is the best alternative\nto impute the missing counts; however, in real-life research this assumption is\nnot always correct, and it is common to find count variables exhibiting\noverdispersion or underdispersion, for which the Poisson model is no longer the\nbest to use in imputation. In several of the scenarios considered the\nperformance of the methods analysed differs, something which indicates that it\nis important to analyse dispersion and the possible presence of excess zeros\nbefore deciding on the imputation method to use. The COMPoisson model performs\nwell as it is flexible regarding the handling of counts with characteristics of\nover- and under-dispersion, as well as with equidispersion.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:02:50 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hern\u00e1ndez-Herrera", "Gilma", ""], ["Navarro", "Albert", ""], ["Mori\u00f1a", "David", ""]]}, {"id": "2007.15039", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Elizabeth Chou, Catie McVey, Yin-Chen Hsieh, Sabrina Enriquez, Fushing\n  Hsieh", "title": "Extreme-K categorical samples problem", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With histograms as its foundation, we develop Categorical Exploratory Data\nAnalysis (CEDA) under the extreme-$K$ sample problem, and illustrate its\nuniversal applicability through four 1D categorical datasets. Given a sizable\n$K$, CEDA's ultimate goal amounts to discover by data's information content via\ncarrying out two data-driven computational tasks: 1) establish a tree geometry\nupon $K$ populations as a platform for discovering a wide spectrum of patterns\namong populations; 2) evaluate each geometric pattern's reliability. In CEDA\ndevelopments, each population gives rise to a row vector of categories\nproportions. Upon the data matrix's row-axis, we discuss the pros and cons of\nEuclidean distance against its weighted version for building a binary\nclustering tree geometry. The criterion of choice rests on degrees of\nuniformness in column-blocks framed by this binary clustering tree. Each\ntree-leaf (population) is then encoded with a binary code sequence, so is\ntree-based pattern. For evaluating reliability, we adopt row-wise multinomial\nrandomness to generate an ensemble of matrix mimicries, so an ensemble of\nmimicked binary trees. Reliability of any observed pattern is its recurrence\nrate within the tree ensemble. A high reliability value means a deterministic\npattern. Our four applications of CEDA illuminate four significant aspects of\nextreme-$K$ sample problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:12:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chou", "Elizabeth", ""], ["McVey", "Catie", ""], ["Hsieh", "Yin-Chen", ""], ["Enriquez", "Sabrina", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2007.15172", "submitter": "Emiliano Valdez", "authors": "Shuang Yin and Dipak K. Dey and Emiliano A. Valdez and Guojun Gan and\n  Jeyaraj Vadiveloo", "title": "Skewed link regression models for imbalanced binary response with\n  applications to life insurance", "comments": "25 pages, 7 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a portfolio of life insurance policies observed for a stated period of\ntime, e.g., one year, mortality is typically a rare event. When we examine the\noutcome of dying or not from such portfolios, we have an imbalanced binary\nresponse. The popular logistic and probit regression models can be\ninappropriate for imbalanced binary response as model estimates may be biased,\nand if not addressed properly, it can lead to serious adverse predictions. In\nthis paper, we propose the use of skewed link regression models (Generalized\nExtreme Value, Weibull, and Fre\\`chet link models) as more superior models to\nhandle imbalanced binary response. We adopt a fully Bayesian approach for the\ngeneralized linear models (GLMs) under the proposed link functions to help\nbetter explain the high skewness. To calibrate our proposed Bayesian models, we\nuse a real dataset of death claims experience drawn from a life insurance\ncompany's portfolio. Bayesian estimates of parameters were obtained using the\nMetropolis-Hastings algorithm and for Bayesian model selection and comparison,\nthe Deviance Information Criterion (DIC) statistic has been used. For our\nmortality dataset, we find that these skewed link models are more superior than\nthe widely used binary models with standard link functions. We evaluate the\npredictive power of the different underlying models by measuring and comparing\naggregated death counts and death benefits.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:17:34 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yin", "Shuang", ""], ["Dey", "Dipak K.", ""], ["Valdez", "Emiliano A.", ""], ["Gan", "Guojun", ""], ["Vadiveloo", "Jeyaraj", ""]]}, {"id": "2007.15179", "submitter": "Linchuan Xu", "authors": "Kenji Yamanishi, Linchuan Xu, Ryo Yuki, Shintaro Fukushima, Chuan-hao\n  Lin", "title": "Detecting Change Signs with Differential MDL Change Statistics for\n  COVID-19 Pandemic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the issue of detecting changes and their signs from a\ndata stream. For example, when given time series of COVID-19 cases in a region,\nwe may raise early warning signals of outbreaks by detecting signs of changes\nin the cases. We propose a novel methodology to address this issue. The key\nidea is to employ a new information-theoretic notion, which we call the\ndifferential minimum description length change statistics (D-MDL), for\nmeasuring the scores of change sign. We first give a fundamental theory for\nD-MDL. We then demonstrate its effectiveness using synthetic datasets. We apply\nit to detecting early warning signals of the COVID-19 epidemic. We empirically\ndemonstrate that D-MDL is able to raise early warning signals of events such as\nsignificant increase/decrease of cases. Remarkably, for about $64\\%$ of the\nevents of significant increase of cases in 37 studied countries, our method can\ndetect warning signals as early as nearly six days on average before the\nevents, buying considerably long time for making responses. We further relate\nthe warning signals to the basic reproduction number $R0$ and the timing of\nsocial distancing. The results showed that our method can effectively monitor\nthe dynamics of $R0$, and confirmed the effectiveness of social distancing at\ncontaining the epidemic in a region. We conclude that our method is a promising\napproach to the pandemic analysis from a data science viewpoint. The software\nfor the experiments is available at\nhttps://github.com/IbarakikenYukishi/differential-mdl-change-statistics. An\nonline detection system is available at\nhttps://ibarakikenyukishi.github.io/d-mdl-html/index.html\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:52:30 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 06:29:00 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yamanishi", "Kenji", ""], ["Xu", "Linchuan", ""], ["Yuki", "Ryo", ""], ["Fukushima", "Shintaro", ""], ["Lin", "Chuan-hao", ""]]}, {"id": "2007.15326", "submitter": "Harrison Wilde", "authors": "Harrison Wilde, Lucia Lushi Chen, Austin Nguyen, Zoe Kimpel, Joshua\n  Sidgwick, Adolfo De Unanue, Davide Veronese, Bilal Mateen, Rayid Ghani,\n  Sebastian Vollmer", "title": "A Recommendation and Risk Classification System for Connecting Rough\n  Sleepers to Essential Outreach Services", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rough sleeping is a chronic problem faced by some of the most disadvantaged\npeople in modern society. This paper describes work carried out in partnership\nwith Homeless Link, a UK-based charity, in developing a data-driven approach to\nassess the quality of incoming alerts from members of the public aimed at\nconnecting people sleeping rough on the streets with outreach service\nproviders. Alerts are prioritised based on the predicted likelihood of\nsuccessfully connecting with the rough sleeper, helping to address capacity\nlimitations and to quickly, effectively, and equitably process all of the\nalerts that they receive. Initial evaluation concludes that our approach\nincreases the rate at which rough sleepers are found following a referral by at\nleast 15\\% based on labelled data, implying a greater overall increase when the\nalerts with unknown outcomes are considered, and suggesting the benefit in a\ntrial taking place over a longer period to assess the models in practice. The\ndiscussion and modelling process is done with careful considerations of ethics,\ntransparency and explainability due to the sensitive nature of the data in this\ncontext and the vulnerability of the people that are affected.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:14:46 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wilde", "Harrison", ""], ["Chen", "Lucia Lushi", ""], ["Nguyen", "Austin", ""], ["Kimpel", "Zoe", ""], ["Sidgwick", "Joshua", ""], ["De Unanue", "Adolfo", ""], ["Veronese", "Davide", ""], ["Mateen", "Bilal", ""], ["Ghani", "Rayid", ""], ["Vollmer", "Sebastian", ""]]}, {"id": "2007.15419", "submitter": "Michael Pfarrhofer", "authors": "Martin Feldkircher, Florian Huber and Michael Pfarrhofer", "title": "Measuring the Effectiveness of US Monetary Policy during the COVID-19\n  Recession", "comments": "JEL: E52, E58, H12; Keywords: Unconventional monetary policy, mixed\n  frequency model, monetary policy effectiveness", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 recession that started in March 2020 led to an unprecedented\ndecline in economic activity across the globe. To fight this recession, policy\nmakers in central banks engaged in expansionary monetary policy. This paper\nasks whether the measures adopted by the US Federal Reserve (Fed) have been\neffective in boosting real activity and calming financial markets. To measure\nthese effects at high frequencies, we propose a novel mixed frequency vector\nautoregressive (MF-VAR) model. This model allows us to combine weekly and\nmonthly information within an unified framework. Our model combines a set of\nmacroeconomic aggregates such as industrial production, unemployment rates and\ninflation with high frequency information from financial markets such as stock\nprices, interest rate spreads and weekly information on the Feds balance sheet\nsize. The latter set of high frequency time series is used to dynamically\ninterpolate the monthly time series to obtain weekly macroeconomic measures. We\nuse this setup to simulate counterfactuals in absence of monetary stimulus. The\nresults show that the monetary expansion caused higher output growth and stock\nmarket returns, more favorable long-term financing conditions and a\ndepreciation of the US dollar compared to a no-policy benchmark scenario.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:28:14 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2007.15475", "submitter": "Roland Ramsahai", "authors": "Roland R. Ramsahai", "title": "Connecting actuarial judgment to probabilistic learning techniques with\n  graph theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have been widely used in applications ranging from medical\nexpert systems to natural language processing. Their popularity partly arises\nsince they are intuitive representations of complex inter-dependencies among\nvariables with efficient algorithms for performing computationally intensive\ninference in high-dimensional models. It is argued that the formalism is very\nuseful for applications in the modelling of non-life insurance claims data. It\nis also shown that actuarial models in current practice can be expressed\ngraphically to exploit the advantages of the approach. More general models are\nproposed within the framework to demonstrate the potential use of graphical\nmodels for probabilistic learning with telematics and other dynamic actuarial\ndata. The discussion also demonstrates throughout that the intuitive nature of\nthe models allows the inclusion of qualitative knowledge or actuarial judgment\nin analyses.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 13:24:40 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ramsahai", "Roland R.", ""]]}, {"id": "2007.15554", "submitter": "Prasannavenkatesan Theerthagiri", "authors": "Prasannavenkatesan Theerthagiri", "title": "Forecasting Hyponatremia in hospitalized patients Using Multilayer\n  Perceptron and Multivariate Linear Regression Techniques", "comments": "19 pages, 5 figure", "journal-ref": "Concurrency and Computation: Practice and Experience, 2021", "doi": "10.1002/cpe.6248", "report-no": null, "categories": "q-bio.QM cs.LG cs.NE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The percentage of patients hospitalized due to hyponatremia is getting\nhigher. Hyponatremia is the deficiency of sodium electrolyte in the human\nserum. This deficiency might indulge adverse effects and also associated with\nlonger hospital stay or mortality, if it wasnt actively treated and managed.\nThis work predicts the futuristic sodium levels of patients based on their\nhistory of health problems using multilayer perceptron and multivariate linear\nregression algorithm. This work analyses the patients age, information about\nother disease such as diabetes, pneumonia, liver-disease, malignancy,\npulmonary, sepsis, SIADH, and sodium level of the patient during admission to\nthe hospital. The results of the proposed MLP algorithm is compared with MLR\nalgorithm based results. The MLP prediction results generates 23-72 of higher\nprediction results than MLR algorithm. Thus, proposed MLR algorithm has\nproduced 57.1 of reduced mean squared error rate than the MLR results on\npredicting future sodium ranges of patients. Further, proposed MLR algorithm\nproduces 27-50 of higher prediction precision rate.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:47:24 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 04:16:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Theerthagiri", "Prasannavenkatesan", ""]]}, {"id": "2007.15769", "submitter": "Ning Xu", "authors": "Ning Xu, Timothy C.G. Fisher, Jian Hong", "title": "Instrument variable detection with graph learning : an application to\n  high dimensional GIS-census data for house pricing", "comments": "introduction rewritten; detailed graph learning and variable\n  selection procedure explained", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endogeneity bias and instrument variable validation have always been\nimportant topics in statistics and econometrics. In the era of big data, such\nissues typically combine with dimensionality issues and, hence, require even\nmore attention. In this paper, we merge two well-known tools from machine\nlearning and biostatistics---variable selection algorithms and probablistic\ngraphs---to estimate house prices and the corresponding causal structure using\n2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional\ndatabase consisting of local school data, GIS information, census data, house\ncharacteristics and other socio-economic records. Using \"big data\", we show\nthat it is possible to perform a data-driven instrument selection efficiently\nand purge out the invalid instruments. Our approach improves the sparsity of\nvariable selection, stability and robustness in the presence of high\ndimensionality, complicated causal structures and the consequent\nmulticollinearity, and recovers a sparse and intuitive causal structure. The\napproach also reveals an efficiency and effectiveness in endogeneity detection,\ninstrument validation, weak instrument pruning and the selection of valid\ninstruments. From the perspective of machine learning, the estimation results\nboth align with and confirms the facts of Sydney house market, the classical\neconomic theories and the previous findings of simultaneous equations modeling.\nMoreover, the estimation results are consistent with and supported by classical\neconometric tools such as two-stage least square regression and different\ninstrument tests. All the code may be found at\n\\url{https://github.com/isaac2math/solar_graph_learning}.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 23:11:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:24:22 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Ning", ""], ["Fisher", "Timothy C. G.", ""], ["Hong", "Jian", ""]]}, {"id": "2007.15812", "submitter": "Yushu Shi", "authors": "Yushu Shi, Liangliang Zhang, Kim-Anh Do, Robert Jenq, Christine\n  Peterson", "title": "Sparse tree-based clustering of microbiome data to characterize\n  microbiome heterogeneity in pancreatic cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a keen interest in characterizing variation in the microbiome across\ncancer patients, given increasing evidence of its important role in determining\ntreatment outcomes. Here our goal is to discover subgroups of patients with\nsimilar microbiome profiles. We propose a novel unsupervised clustering\napproach in the Bayesian framework that innovates over existing model-based\nclustering approaches, such as the Dirichlet multinomial mixture model, in\nthree key respects: we incorporate feature selection, learn the appropriate\nnumber of clusters from the data, and integrate information on the tree\nstructure relating the observed features. We compare the performance of our\nproposed method to existing methods on simulated data designed to mimic real\nmicrobiome data. We then illustrate results obtained for our motivating data\nset, a clinical study aimed at characterizing the tumor microbiome of\npancreatic cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:34:05 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 21:04:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shi", "Yushu", ""], ["Zhang", "Liangliang", ""], ["Do", "Kim-Anh", ""], ["Jenq", "Robert", ""], ["Peterson", "Christine", ""]]}, {"id": "2007.15814", "submitter": "Dandan Chen", "authors": "Dandan Chen (University of Illinois, Urbana-Champaign)", "title": "Performance of Multi-group DIF Methods in Assessing Cross-Country Score\n  Comparability of International Large-Scale Assessments", "comments": "29 pages, 6 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Standardized large-scale testing can be a debatable topic, in which test\nfairness sits at its very core. This study found that two out of five recent\nmulti-group DIF detection methods are capable of capturing both the uniform and\nnonuniform DIF that affects test fairness. Still, no prior research has\ndemonstrated the relative performance of these two methods when they are\ncompared with each other. These two methods are the improved Wald test and the\ngeneralized logistic regression procedure. This study assessed the\ncommonalities and differences between two sets of empirical results from these\ntwo methods with the latest TIMSS math score data. The primary conclusion was\nthat the improved Wald test is relatively more established than the generalized\nlogistic regression procedure for multi-group DIF analysis. Empirical results\nfrom this study may inform the selection of a multi-group DIF method in the\nILSA score analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:42:19 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Chen", "Dandan", "", "University of Illinois, Urbana-Champaign"]]}, {"id": "2007.15862", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha", "title": "A Note on Particle Gibbs Method and its Extensions and Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional state trajectories of state-space models pose challenges for\nBayesian inference. Particle Gibbs (PG) methods have been widely used to sample\nfrom the posterior of a state space model. Basically, particle Gibbs is a\nParticle Markov Chain Monte Carlo (PMCMC) algorithm that mimics the Gibbs\nsampler by drawing model parameters and states from their conditional\ndistributions.\n  This tutorial provides an introductory view on Particle Gibbs (PG) method and\nits extensions and variants, and illustrates through several examples of\ninference in non-linear state space models (SSMs). We also implement PG\nSamplers in two different programming languages: Python and Rust. Comparison of\nrun-time performance of Python and Rust programs are also provided for various\nPG methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 06:15:03 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 04:39:50 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "2007.15896", "submitter": "Marco Stefanucci", "authors": "Marco Stefanucci, Stefano Mazzuco", "title": "Analyzing Cause-Specific Mortality Trends using Compositional Functional\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamics of cause--specific mortality rates among countries by\nconsidering them as compositions of functions. We develop a novel framework for\nsuch data structure, with particular attention to functional PCA. The\napplication of this method to a subset of the WHO mortality database reveals\nthe main modes of variation of cause--specific rates over years for men and\nwomen and enables us to perform clustering in the projected subspace. The\nresults give many insights of the ongoing trends, only partially explained by\npast literature, that the considered countries are undergoing. We are also able\nto show the different evolution of cause of death undergone by men and women:\nfor example, we can see that while lung cancer incidence is stabilizing for\nmen, it is still increasing for women.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 08:19:22 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Stefanucci", "Marco", ""], ["Mazzuco", "Stefano", ""]]}, {"id": "2007.15955", "submitter": "Zhuozhao Zhan", "authors": "Osama Almalik, Zhuozhao Zhan, and Edwin R. van den Heuvel", "title": "Copas' method is sensitive to different mechanisms of publication bias", "comments": "1 Figure, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copas' method corrects a pooled estimate from an aggregated data\nmeta-analysis for publication bias. Its performance has been studied for one\nparticular mechanism of publication bias. We show through simulations that\nCopas' method is not robust against other realistic mechanisms. This questions\nthe usefulness of Copas' method, since publication bias mechanisms are\ntypically unknown in practice.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 10:57:08 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Almalik", "Osama", ""], ["Zhan", "Zhuozhao", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2007.15991", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich and Tim Friede", "title": "Causal inference methods for small non-randomized studies: Methods and\n  an application in COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual development cycles are too slow for the development of vaccines,\ndiagnostics and treatments in pandemics such as the ongoing SARS-CoV-2\npandemic. Given the pressure in such a situation, there is a risk that findings\nof early clinical trials are overinterpreted despite their limitations in terms\nof size and design. Motivated by a non-randomized open-label study\ninvestigating the efficacy of hydroxychloroquine in patients with COVID-19, we\ndescribe in a unified fashion various alternative approaches to the analysis of\nnon-randomized studies. A widely used tool to reduce the impact of\ntreatment-selection bias are so-called propensity score (PS) methods.\nConditioning on the propensity score allows one to replicate the design of a\nrandomized controlled trial, conditional on observed covariates. Extensions\ninclude the g-computation approach, which is less frequently applied, in\nparticular in clinical studies. Moreover, doubly robust estimators provide\nadditional advantages. Here, we investigate the properties of propensity score\nbased methods including three variations of doubly robust estimators in small\nsample settings, typical for early trials, in a simulation study. R code for\nthe simulations is provided.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 12:00:14 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 10:45:43 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Friedrich", "Sarah", ""], ["Friede", "Tim", ""]]}, {"id": "2007.16031", "submitter": "Xin Gao", "authors": "Xin Gao, Li Li, Li Luo", "title": "Decomposition of the Total Effect for Two Mediators: A Natural\n  Counterfactual Interaction Effect Framework", "comments": "112 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2004.06054", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis has been used in many disciplines to explain the mechanism\nor process that underlies an observed relationship between an exposure variable\nand an outcome variable via the inclusion of mediators. Decompositions of the\ntotal causal effect of an exposure variable into effects characterizing\nmediation pathways and interactions have gained an increasing amount of\ninterest in the last decade. In this work, we develop decompositions for\nscenarios where the two mediators are causally sequential or non-sequential.\nCurrent developments in this area have primarily focused on either\ndecompositions without interaction components or with interactions but assuming\nno causally sequential order between the mediators. We propose a new concept\ncalled natural counterfactual interaction effect that captures the two-way and\nthree-way interactions for both scenarios that extend the two-way mediated\ninteractions in literature. We develop a unified approach for decomposing the\ntotal effect into the effects that are due to mediation only, interaction only,\nboth mediation and interaction, neither mediation nor interaction within the\ncounterfactual framework. Finally, we illustrate the proposed decomposition\nmethod using a real data analysis where the two mediators are causally\nsequential.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:10:09 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Gao", "Xin", ""], ["Li", "Li", ""], ["Luo", "Li", ""]]}, {"id": "2007.16058", "submitter": "Giacomo De Nicola", "authors": "Giacomo De Nicola, Marc Schneble, G\\\"oran Kauermann and Ursula Berger", "title": "Regional now- and forecasting for data reported with delay: Towards\n  surveillance of COVID-19 infections", "comments": "5 Figures, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governments around the world continue to act to contain and mitigate the\nspread of COVID-19. The rapidly evolving situation compels officials and\nexecutives to continuously adapt policies and social distancing measures\ndepending on the current state of the spread of the disease. In this context,\nit is crucial for policymakers to have a firm grasp on what the current state\nof the pandemic is as well as to have an idea of how the infective situation is\ngoing to unfold in the next days. However, as in many other situations of\ncompulsorily-notifiable diseases and beyond, cases are reported with delay to a\ncentral register, with this delay deferring an up-to-date view of the state of\nthings. We provide a stable tool for monitoring current infection levels as\nwell as predicting infection numbers in the immediate future at the regional\nlevel. We accomplish this through nowcasting of cases that have not yet been\nreported as well as through predictions of future infections. We apply our\nmodel to German data, for which our focus lies in predicting and explain\ninfectious behavior by district.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:16:42 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 17:29:56 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["De Nicola", "Giacomo", ""], ["Schneble", "Marc", ""], ["Kauermann", "G\u00f6ran", ""], ["Berger", "Ursula", ""]]}, {"id": "2007.16059", "submitter": "Antonio Elias", "authors": "Antonio El\\'ias (1 and 2), Ra\\'ul Jim\\'enez (1 and 2), Joe Yukich (2\n  and 3) ((1) Department of Statistics, Universidad Carlos III de Madrid, (2)\n  UC3M-Santander Big Data Institute, Universidad Carlos III de Madrid, (3)\n  Department of Mathematics, Lehigh University)", "title": "Localization processes for functional data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative to $k$-nearest neighbors for functional data\nwhereby the approximating neighboring curves are piecewise functions built from\na functional sample. Using a locally defined distance function that satisfies\nstabilization criteria, we establish pointwise and global approximation results\nin function spaces when the number of data curves is large enough. We exploit\nthis feature to develop the asymptotic theory when a finite number of curves is\nobserved at time-points given by an i.i.d. sample whose cardinality increases\nup to infinity. We use these results to investigate the problem of estimating\nunobserved segments of a partially observed functional data sample as well as\nto study the problem of functional classification and outlier detection. For\nsuch problems, our methods are competitive with and sometimes superior to\nbenchmark predictions in the field.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:19:09 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:58:34 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["El\u00edas", "Antonio", "", "1 and 2"], ["Jim\u00e9nez", "Ra\u00fal", "", "1 and 2"], ["Yukich", "Joe", "", "2\n  and 3"]]}, {"id": "2007.16094", "submitter": "Katarzyna Adamczyk-Chauvat", "authors": "Katarzyna Adamczyk-Chauvat (MaIAGE), Mouna Kassa (INSA Rennes), Ki\\^en\n  Ki\\^eu (MaIAGE), Julien Papa\\\"ix (BIOSP), Radu S. Stoica (IECL)", "title": "Gibbsian T-tessellation model for agricultural landscape\n  characterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of planar tessellations, named T-tessellations, was introduced in\n([10]). A model was proposed to be considered as a completely random\nT-tessellation model (CRTT) and its Gibbsian variants were discussed. A general\nsimulation algorithm of Metropolis-Hastings-Green type was derived for model\nsimulation, involving three local transformations of T-tessellations. The\ncurrent paper focuses on statistical inference for Gibbs models of\nT-tessellations. Statistical methods originated from point pattern analysis are\nimplemented on the example of three agricultural landscapes approximated by\nT-tessellations. The choice of model statistics is guided by their capacity to\nhighlight the differences between the landscape patterns. Model parameters are\nestimated by Monte Carlo Maximum Likelihood method, yielding a baseline for\nlandscapes comparison. In the last part of the paper a global envelope test\nbased on the empty-space function is proposed for assessing the goodness-of-fit\nof the model.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:14:49 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Adamczyk-Chauvat", "Katarzyna", "", "MaIAGE"], ["Kassa", "Mouna", "", "INSA Rennes"], ["Ki\u00eau", "Ki\u00ean", "", "MaIAGE"], ["Papa\u00efx", "Julien", "", "BIOSP"], ["Stoica", "Radu S.", "", "IECL"]]}, {"id": "2007.16096", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb, Yaneer Bar-Yam, and Pasquale Cirillo", "title": "On Single Point Forecasts for Fat-Tailed Variables", "comments": "Accepted, International Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss common errors and fallacies when using naive \"evidence based\"\nempiricism and point forecasts for fat-tailed variables, as well as the\ninsufficiency of using naive first-order scientific methods for tail risk\nmanagement. We use the COVID-19 pandemic as the background for the discussion\nand as an example of a phenomenon characterized by a multiplicative nature, and\nwhat mitigating policies must result from the statistical properties and\nassociated risks. In doing so, we also respond to the points raised by\nIoannidis et al. (2020).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:20:16 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Taleb", "Nassim Nicholas", ""], ["Bar-Yam", "Yaneer", ""], ["Cirillo", "Pasquale", ""]]}, {"id": "2007.16109", "submitter": "Parijat Dube", "authors": "Samuel Ackerman, Parijat Dube, Eitan Farchi", "title": "Sequential Drift Detection in Deep Learning Classifiers", "comments": "11 pages + appendix, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize neural network embeddings to detect data drift by formulating the\ndrift detection within an appropriate sequential decision framework. This\nenables control of the false alarm rate although the statistical tests are\nrepeatedly applied. Since change detection algorithms naturally face a tradeoff\nbetween avoiding false alarms and quick correct detection, we introduce a loss\nfunction which evaluates an algorithm's ability to balance these two concerns,\nand we use it in a series of experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:46:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ackerman", "Samuel", ""], ["Dube", "Parijat", ""], ["Farchi", "Eitan", ""]]}]