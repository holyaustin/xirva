[{"id": "0910.0063", "submitter": "Srikanth Jagabathula", "authors": "Vivek F. Farias, Srikanth Jagabathula, Devavrat Shah", "title": "A Nonparametric Approach to Modeling Choice with Limited Data", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central push in operations models over the last decade has been the\nincorporation of models of customer choice. Real world implementations of many\nof these models face the formidable stumbling block of simply identifying the\n`right' model of choice to use. Thus motivated, we visit the following problem:\nFor a `generic' model of consumer choice (namely, distributions over preference\nlists) and a limited amount of data on how consumers actually make decisions\n(such as marginal information about these distributions), how may one predict\nrevenues from offering a particular assortment of choices? We present a\nframework to answer such questions and design a number of tractable algorithms\nfrom a data and computational standpoint for the same. This paper thus takes a\nsignificant step towards `automating' the crucial task of choice model\nselection in the context of operational decision problems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 00:42:56 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2009 13:04:08 GMT"}, {"version": "v3", "created": "Thu, 21 Oct 2010 17:11:29 GMT"}, {"version": "v4", "created": "Tue, 21 Jun 2011 23:25:22 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Farias", "Vivek F.", ""], ["Jagabathula", "Srikanth", ""], ["Shah", "Devavrat", ""]]}, {"id": "0910.0483", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis, Aikaterini Mitrokotsa", "title": "Statistical Decision Making for Authentication and Intrusion Detection", "comments": "13 pages, 2 figures, to be presented at ICMLA 2009", "journal-ref": null, "doi": null, "report-no": "IAS-UVA-09-02", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User authentication and intrusion detection differ from standard\nclassification problems in that while we have data generated from legitimate\nusers, impostor or intrusion data is scarce or non-existent. We review existing\ntechniques for dealing with this problem and propose a novel alternative based\non a principled statistical decision-making view point. We examine the\ntechnique on a toy problem and validate it on complex real-world data from an\nRFID based access control system. The results indicate that it can\nsignificantly outperform the classical world model approach. The method could\nbe more generally useful in other decision-making scenarios where there is a\nlack of adversary data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 19:43:40 GMT"}], "update_date": "2009-12-26", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Mitrokotsa", "Aikaterini", ""]]}, {"id": "0910.1233", "submitter": "Dylan S. Small", "authors": "Dylan S. Small, Paul R. Rosenbaum", "title": "Error-free milestones in error prone measurements", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS233 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 881-901", "doi": "10.1214/08-AOAS233", "report-no": "IMS-AOAS-AOAS233", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictor variable or dose that is measured with substantial error may\npossess an error-free milestone, such that it is known with negligible error\nwhether the value of the variable is to the left or right of the milestone.\nSuch a milestone provides a basis for estimating a linear relationship between\nthe true but unknown value of the error-free predictor and an outcome, because\nthe milestone creates a strong and valid instrumental variable. The inferences\nare nonparametric and robust, and in the simplest cases, they are exact and\ndistribution free. We also consider multiple milestones for a single predictor\nand milestones for several predictors whose partial slopes are estimated\nsimultaneously. Examples are drawn from the Wisconsin Longitudinal Study, in\nwhich a BA degree acts as a milestone for sixteen years of education, and the\nbinary indicator of military service acts as a milestone for years of service.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2009 12:18:25 GMT"}], "update_date": "2009-10-08", "authors_parsed": [["Small", "Dylan S.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "0910.1426", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "Are a set of microarrays independent of each other?", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS236 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 922-942", "doi": "10.1214/09-AOAS236", "report-no": "IMS-AOAS-AOAS236", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having observed an $m\\times n$ matrix $X$ whose rows are possibly correlated,\nwe wish to test the hypothesis that the columns are independent of each other.\nOur motivation comes from microarray studies, where the rows of $X$ record\nexpression levels for $m$ different genes, often highly correlated, while the\ncolumns represent $n$ individual microarrays, presumably obtained\nindependently. The presumption of independence underlies all the familiar\npermutation, cross-validation and bootstrap methods for microarray analysis, so\nit is important to know when independence fails. We develop nonparametric and\nnormal-theory testing methods. The row and column correlations of $X$ interact\nwith each other in a way that complicates test procedures, essentially by\nreducing the accuracy of the relevant estimators.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 06:44:30 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "0910.1430", "submitter": "Ming Yuan", "authors": "Ming Yuan", "title": "State price density estimation via nonparametric mixtures", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS246 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 963-984", "doi": "10.1214/09-AOAS246", "report-no": "IMS-AOAS-AOAS246", "categories": "q-fin.CP q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric estimation of the state price density encapsulated\nin option prices. Unlike usual density estimation problems, we only observe\noption prices and their corresponding strike prices rather than samples from\nthe state price density. We propose to model the state price density directly\nwith a nonparametric mixture and estimate it using least squares. We show that\nalthough the minimization is taken over an infinitely dimensional function\nspace, the minimizer always admits a finite dimensional representation and can\nbe computed efficiently. We also prove that the proposed estimate of the state\nprice density function converges to the truth at a ``nearly parametric'' rate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 07:42:06 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Yuan", "Ming", ""]]}, {"id": "0910.1432", "submitter": "Bradley P. Carlin", "authors": "Shengde Liang, Bradley P. Carlin, Alan E. Gelfand", "title": "Analysis of Minnesota colon and rectum cancer point patterns with\n  spatial and nonspatial covariate information", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS240 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 943-962", "doi": "10.1214/09-AOAS240", "report-no": "IMS-AOAS-AOAS240", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colon and rectum cancer share many risk factors, and are often tabulated\ntogether as ``colorectal cancer'' in published summaries. However, recent work\nindicating that exercise, diet, and family history may have differential\nimpacts on the two cancers encourages analyzing them separately, so that\ncorresponding public health interventions can be more efficiently targeted. We\nanalyze colon and rectum cancer data from the Minnesota Cancer Surveillance\nSystem from 1998--2002 over the 16-county Twin Cities (Minneapolis--St. Paul)\nmetro and exurban area. The data consist of two marked point patterns, meaning\nthat any statistical model must account for randomness in the observed\nlocations, and expected positive association between the two cancer patterns.\nOur model extends marked spatial point pattern analysis in the context of a log\nGaussian Cox process to accommodate spatially referenced covariates (local\npoverty rate and location within the metro area), individual-level risk factors\n(patient age and cancer stage), and related interactions. We obtain smoothed\nmaps of marginal log-relative intensity surfaces for colon and rectum cancer,\nand uncover significant age and stage differences between the two groups. This\nencourages more aggressive colon cancer screening in the inner Twin Cities and\ntheir southern and western exurbs, where our model indicates higher colon\ncancer relative intensity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 07:52:13 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Liang", "Shengde", ""], ["Carlin", "Bradley P.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "0910.1455", "submitter": "Peihua Qiu", "authors": "Peihua Qiu, Rong Yang, Michael Potegal", "title": "Statistical modeling of the time course of tantrum anger", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS242 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1013-1034", "doi": "10.1214/09-AOAS242", "report-no": "IMS-AOAS-AOAS242", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although anger is an important emotion that underlies much overt aggression\nat great social cost, little is known about how to quantify anger or to specify\nthe relationship between anger and the overt behaviors that express it. This\npaper proposes a novel statistical model which provides both a metric for the\nintensity of anger and an approach to determining the quantitative relationship\nbetween anger intensity and the specific behaviors that it controls. From\nobserved angry behaviors, we reconstruct the time course of the latent anger\nintensity and the linkage between anger intensity and the probability of each\nangry behavior. The data on which this analysis is based consist of observed\ntantrums had by 296 children in the Madison WI area during the period\n1994--1996. For each tantrum, eight angry behaviors were recorded as occurring\nor not within each consecutive 30-second unit. So, the data can be\ncharacterized as a multivariate, binary, longitudinal (MBL) dataset with a\nlatent variable (anger intensity) involved. Data such as these are common in\nbiomedical, psychological and other areas of the medical and social sciences.\nThus, the proposed modeling approach has broad applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 10:06:59 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Qiu", "Peihua", ""], ["Yang", "Rong", ""], ["Potegal", "Michael", ""]]}, {"id": "0910.1479", "submitter": "David Rossell", "authors": "David Rossell", "title": "GaGa: A parsimonious and flexible model for differential expression\n  analysis", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS244 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1035-1051", "doi": "10.1214/09-AOAS244", "report-no": "IMS-AOAS-AOAS244", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models are a powerful tool for high-throughput data with a small\nto moderate number of replicates, as they allow sharing information across\nunits of information, for example, genes. We propose two such models and show\nits increased sensitivity in microarray differential expression applications.\nWe build on the gamma--gamma hierarchical model introduced by Kendziorski et\nal. [Statist. Med. 22 (2003) 3899--3914] and Newton et al. [Biostatistics 5\n(2004) 155--176], by addressing important limitations that may have hampered\nits performance and its more widespread use. The models parsimoniously describe\nthe expression of thousands of genes with a small number of hyper-parameters.\nThis makes them easy to interpret and analytically tractable. The first model\nis a simple extension that improves the fit substantially with almost no\nincrease in complexity. We propose a second extension that uses a mixture of\ngamma distributions to further improve the fit, at the expense of increased\ncomputational burden. We derive several approximations that significantly\nreduce the computational cost. We find that our models outperform the original\nformulation of the model, as well as some other popular methods for\ndifferential expression analysis. The improved performance is specially\nnoticeable for the small sample sizes commonly encountered in high-throughput\nexperiments. Our methods are implemented in the freely available Bioconductor\ngaga package.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 11:50:21 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Rossell", "David", ""]]}, {"id": "0910.1490", "submitter": "Andrew O. Finley", "authors": "Andrew O. Finley, Sudipto Banerjee, Ronald E. McRoberts", "title": "Hierarchical spatial models for predicting tree species assemblages\n  across large domains", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS250 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1052-1079", "doi": "10.1214/09-AOAS250", "report-no": "IMS-AOAS-AOAS250", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially explicit data layers of tree species assemblages, referred to as\nforest types or forest type groups, are a key component in large-scale\nassessments of forest sustainability, biodiversity, timber biomass, carbon\nsinks and forest health monitoring. This paper explores the utility of coupling\ngeoreferenced national forest inventory (NFI) data with readily available and\nspatially complete environmental predictor variables through spatially-varying\nmultinomial logistic regression models to predict forest type groups across\nlarge forested landscapes. These models exploit underlying spatial associations\nwithin the NFI plot array and the spatially-varying impact of predictor\nvariables to improve the accuracy of forest type group predictions. The\nrichness of these models incurs onerous computational burdens and we discuss\ndimension reducing spatial processes that retain the richness in modeling. We\nillustrate using NFI data from Michigan, USA, where we provide a comprehensive\nanalysis of this large study area and demonstrate improved prediction with\nassociated measures of uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 12:46:25 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""], ["McRoberts", "Ronald E.", ""]]}, {"id": "0910.1656", "submitter": "Ian L. Dryden", "authors": "Ian L. Dryden, Alexey Koloydenko, Diwei Zhou", "title": "Non-Euclidean statistics for covariance matrices, with applications to\n  diffusion tensor imaging", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS249 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1102-1123", "doi": "10.1214/09-AOAS249", "report-no": "IMS-AOAS-AOAS249", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of covariance matrix data is considered and, in\nparticular, methodology is discussed which takes into account the non-Euclidean\nnature of the space of positive semi-definite symmetric matrices. The main\nmotivation for the work is the analysis of diffusion tensors in medical image\nanalysis. The primary focus is on estimation of a mean covariance matrix and,\nin particular, on the use of Procrustes size-and-shape space. Comparisons are\nmade with other estimation techniques, including using the matrix logarithm,\nmatrix square root and Cholesky decomposition. Applications to diffusion tensor\nimaging are considered and, in particular, a new measure of fractional\nanisotropy called Procrustes Anisotropy is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 06:23:19 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Dryden", "Ian L.", ""], ["Koloydenko", "Alexey", ""], ["Zhou", "Diwei", ""]]}, {"id": "0910.1660", "submitter": "Ming-Hui Chen", "authors": "Sungduk Kim, Yingmei Xi, Ming-Hui Chen", "title": "A new latent cure rate marker model for survival data", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS238 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1124-1146", "doi": "10.1214/09-AOAS238", "report-no": "IMS-AOAS-AOAS238", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address an important risk classification issue that arises in clinical\npractice, we propose a new mixture model via latent cure rate markers for\nsurvival data with a cure fraction. In the proposed model, the latent cure rate\nmarkers are modeled via a multinomial logistic regression and patients who\nshare the same cure rate are classified into the same risk group. Compared to\navailable cure rate models, the proposed model fits better to data from a\nprostate cancer clinical trial. In addition, the proposed model can be used to\ndetermine the number of risk groups and to develop a predictive classification\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 06:56:47 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Kim", "Sungduk", ""], ["Xi", "Yingmei", ""], ["Chen", "Ming-Hui", ""]]}, {"id": "0910.1664", "submitter": "Paul Joyce", "authors": "Erkan Ozge Buzbas, Paul Joyce", "title": "Maximum likelihood estimates under $\\mathbf{k}$-allele models with\n  selection can be numerically unstable", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS237 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1147-1162", "doi": "10.1214/09-AOAS237", "report-no": "IMS-AOAS-AOAS237", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stationary distribution of allele frequencies under a variety of\nWright--Fisher $k$-allele models with selection and parent independent mutation\nis well studied. However, the statistical properties of maximum likelihood\nestimates of parameters under these models are not well understood. Under each\nof these models there is a point in data space which carries the strongest\npossible signal for selection, yet, at this point, the likelihood is unbounded.\nThis result remains valid even if all of the mutation parameters are assumed to\nbe known. Therefore, standard simulation approaches used to approximate the\nsampling distribution of the maximum likelihood estimate produce numerically\nunstable results in the presence of substantial selection. We describe the\nBayesian alternative where the posterior distribution tends to produce more\naccurate and reliable interval estimates for the selection intensity at a\nlocus.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 07:17:26 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Buzbas", "Erkan Ozge", ""], ["Joyce", "Paul", ""]]}, {"id": "0910.1667", "submitter": "Elizabeth R. Brown", "authors": "Elizabeth R. Brown", "title": "Assessing the association between trends in a biomarker and risk of\n  event with an application in pediatric HIV/AIDS", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS251 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1163-1182", "doi": "10.1214/09-AOAS251", "report-no": "IMS-AOAS-AOAS251", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new joint longitudinal and survival model aimed at estimating\nthe association between the risk of an event and the change in and history of a\nbiomarker that is repeatedly measured over time. We use cubic B-splines models\nfor the longitudinal component that lend themselves to straight-forward\nformulations of the slope and integral of the trajectory of the biomarker. The\nmodel is applied to data collected in a long term follow-up study of HIV\ninfected infants in Uganda. Estimation is carried out using MCMC methods. We\nalso explore using the deviance information criteria, the conditional\npredictive ordinate and ROC curves for model selection and evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 07:43:45 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Brown", "Elizabeth R.", ""]]}, {"id": "0910.1683", "submitter": "Asger Hobolth", "authors": "Asger Hobolth, Eric A. Stone", "title": "Simulation from endpoint-conditioned, continuous-time Markov chains on a\n  finite state space, with applications to molecular evolution", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS247 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1204-1231", "doi": "10.1214/09-AOAS247", "report-no": "IMS-AOAS-AOAS247", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of serially-sampled data often begin with the assumption that the\nobservations represent discrete samples from a latent continuous-time\nstochastic process. The continuous-time Markov chain (CTMC) is one such\ngenerative model whose popularity extends to a variety of disciplines ranging\nfrom computational finance to human genetics and genomics. A common theme among\nthese diverse applications is the need to simulate sample paths of a CTMC\nconditional on realized data that is discretely observed. Here we present a\ngeneral solution to this sampling problem when the CTMC is defined on a\ndiscrete and finite state space. Specifically, we consider the generation of\nsample paths, including intermediate states and times of transition, from a\nCTMC whose beginning and ending states are known across a time interval of\nlength $T$. We first unify the literature through a discussion of the three\npredominant approaches: (1) modified rejection sampling, (2) direct sampling,\nand (3) uniformization. We then give analytical results for the complexity and\nefficiency of each method in terms of the instantaneous transition rate matrix\n$Q$ of the CTMC, its beginning and ending states, and the length of sampling\ntime $T$. In doing so, we show that no method dominates the others across all\nmodel specifications, and we give explicit proof of which method prevails for\nany given $Q,T,$ and endpoints. Finally, we introduce and compare three\napplications of CTMCs to demonstrate the pitfalls of choosing an inefficient\nsampler.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 08:49:32 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Hobolth", "Asger", ""], ["Stone", "Eric A.", ""]]}, {"id": "0910.1723", "submitter": "Julien  Chiquet Dr.", "authors": "Camille Charbonnier, Julien Chiquet, Christophe Ambroise", "title": "Weighted-Lasso for Structured Network Inference from Time Course Data", "comments": null, "journal-ref": "Statistical Applications in Genetics and Molecular Biology: Vol. 9\n  : Iss. 1, Article 15, 2010.", "doi": "10.2202/1544-6115.1519", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weighted-Lasso method to infer the parameters of a first-order\nvector auto-regressive model that describes time course expression data\ngenerated by directed gene-to-gene regulation networks. These networks are\nassumed to own a prior internal structure of connectivity which drives the\ninference method. This prior structure can be either derived from prior\nbiological knowledge or inferred by the method itself. We illustrate the\nperformance of this structure-based penalization both on synthetic data and on\ntwo canonical regulatory networks, first yeast cell cycle regulation network by\nanalyzing Spellman et al's dataset and second E. coli S.O.S. DNA repair network\nby analysing U. Alon's lab data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 12:12:54 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2009 15:16:02 GMT"}], "update_date": "2010-04-05", "authors_parsed": [["Charbonnier", "Camille", ""], ["Chiquet", "Julien", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0910.2034", "submitter": "Hugo Zanghi", "authors": "Hugo Zanghi, Franck Picard, Vincent Miele, Christophe Ambroise", "title": "Strategies for online inference of model-based clustering in large and\n  growing networks", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS359 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 687-714", "doi": "10.1214/10-AOAS359", "report-no": "IMS-AOAS-AOAS359", "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we adapt online estimation strategies to perform model-based\nclustering on large networks. Our work focuses on two algorithms, the first\nbased on the SAEM algorithm, and the second on variational methods. These two\nstrategies are compared with existing approaches on simulated and real data. We\nuse the method to decipher the connexion structure of the political websphere\nduring the US political campaign in 2008. We show that our online EM-based\nalgorithms offer a good trade-off between precision and speed, when estimating\nparameters for mixture distributions in the context of random graphs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 19:36:16 GMT"}, {"version": "v2", "created": "Wed, 10 Nov 2010 09:02:00 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Zanghi", "Hugo", ""], ["Picard", "Franck", ""], ["Miele", "Vincent", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0910.2090", "submitter": "Jun S. Liu", "authors": "W. Evan Johnson, X. Shirley Liu, Jun S. Liu", "title": "Doubly stochastic continuous-time hidden Markov approach for analyzing\n  genome tiling arrays", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS248 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 3, 1183-1203", "doi": "10.1214/09-AOAS248", "report-no": "IMS-AOAS-AOAS248", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarrays have been developed that tile the entire nonrepetitive genomes of\nmany different organisms, allowing for the unbiased mapping of active\ntranscription regions or protein binding sites across the entire genome. These\ntiling array experiments produce massive correlated data sets that have many\nexperimental artifacts, presenting many challenges to researchers that require\ninnovative analysis methods and efficient computational algorithms. This paper\npresents a doubly stochastic latent variable analysis method for transcript\ndiscovery and protein binding region localization using tiling array data. This\nmodel is unique in that it considers actual genomic distance between probes.\nAdditionally, the model is designed to be robust to cross-hybridized and\nnonresponsive probes, which can often lead to false-positive results in\nmicroarray experiments. We apply our model to a transcript finding data set to\nillustrate the consistency of our method. Additionally, we apply our method to\na spike-in experiment that can be used as a benchmark data set for researchers\ninterested in developing and comparing future tiling array methods. The results\nindicate that our method is very powerful, accurate and can be used on a single\nsample and without control experiments, thus defraying some of the overhead\ncost of conducting experiments on tiling arrays.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2009 06:10:39 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Johnson", "W. Evan", ""], ["Liu", "X. Shirley", ""], ["Liu", "Jun S.", ""]]}, {"id": "0910.2098", "submitter": "Pierre Latouche", "authors": "Pierre Latouche, Etienne Birmel\\'e, Christophe Ambroise", "title": "Overlapping stochastic block models with application to the French\n  political blogosphere", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS382 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 309-336", "doi": "10.1214/10-AOAS382", "report-no": "IMS-AOAS-AOAS382", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems in nature and in society are often represented as networks,\ndescribing the rich set of interactions between objects of interest. Many\ndeterministic and probabilistic clustering methods have been developed to\nanalyze such structures. Given a network, almost all of them partition the\nvertices into disjoint clusters, according to their connection profile.\nHowever, recent studies have shown that these techniques were too restrictive\nand that most of the existing networks contained overlapping clusters. To\ntackle this issue, we present in this paper the Overlapping Stochastic Block\nModel. Our approach allows the vertices to belong to multiple clusters, and, to\nsome extent, generalizes the well-known Stochastic Block Model [Nowicki and\nSnijders (2001)]. We show that the model is generically identifiable within\nclasses of equivalence and we propose an approximate inference procedure, based\non global and local variational techniques. Using toy data sets as well as the\nFrench Political Blogosphere network and the transcriptional network of\nSaccharomyces cerevisiae, we compare our work with other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2009 07:27:54 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2010 09:05:54 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2010 14:26:28 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2011 08:42:29 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Latouche", "Pierre", ""], ["Birmel\u00e9", "Etienne", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0910.2107", "submitter": "Christophe Ambroise", "authors": "Hugo Zanghi, Stevenn Volant, Christophe Ambroise", "title": "Clustering based on Random Graph Model embedding Vertex Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets with interactions between objects are common to numerous\nscientific fields (i.e. social science, internet, biology...). The interactions\nnaturally define a graph and a common way to explore or summarize such dataset\nis graph clustering. Most techniques for clustering graph vertices just use the\ntopology of connections ignoring informations in the vertices features. In this\npaper, we provide a clustering algorithm exploiting both types of data based on\na statistical model with latent structure characterizing each vertex both by a\nvector of features as well as by its connectivity. We perform simulations to\ncompare our algorithm with existing approaches, and also evaluate our method\nwith real datasets based on hyper-textual documents. We find that our algorithm\nsuccessfully exploits whatever information is found both in the connectivity\npattern and in the features.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2009 08:51:43 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Zanghi", "Hugo", ""], ["Volant", "Stevenn", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0910.2145", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen", "title": "Node harvest", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS367 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 2049-2072", "doi": "10.1214/10-AOAS367", "report-no": "IMS-AOAS-AOAS367", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When choosing a suitable technique for regression and classification with\nmultivariate predictor variables, one is often faced with a tradeoff between\ninterpretability and high predictive accuracy. To give a classical example,\nclassification and regression trees are easy to understand and interpret. Tree\nensembles like Random Forests provide usually more accurate predictions. Yet\ntree ensembles are also more difficult to analyze than single trees and are\noften criticized, perhaps unfairly, as `black box' predictors. Node harvest is\ntrying to reconcile the two aims of interpretability and predictive accuracy by\ncombining positive aspects of trees and tree ensembles. Results are very sparse\nand interpretable and predictive accuracy is extremely competitive, especially\nfor low signal-to-noise data. The procedure is simple: an initial set of a few\nthousand nodes is generated randomly. If a new observation falls into just a\nsingle node, its prediction is the mean response of all training observation\nwithin this node, identical to a tree-like prediction. A new observation falls\ntypically into several nodes and its prediction is then the weighted average of\nthe mean responses across all these nodes. The only role of node harvest is to\n`pick' the right nodes from the initial large ensemble of nodes by choosing\nnode weights, which amounts in the proposed algorithm to a quadratic\nprogramming problem with linear inequality constraints. The solution is sparse\nin the sense that only very few nodes are selected with a nonzero weight. This\nsparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to\nselect a tuning parameter for optimal predictive accuracy. Node harvest can\nhandle mixed data and missing values and is shown to be simple to interpret and\ncompetitive in predictive accuracy on a variety of data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2009 12:12:46 GMT"}, {"version": "v2", "created": "Fri, 7 Jan 2011 09:02:24 GMT"}], "update_date": "2011-01-10", "authors_parsed": [["Meinshausen", "Nicolai", ""]]}, {"id": "0910.2585", "submitter": "Thomas Brendan Murphy", "authors": "Thomas Brendan Murphy, Nema Dean, Adrian E. Raftery", "title": "Variable selection and updating in model-based discriminant analysis for\n  high dimensional data with food authenticity applications", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS279 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 1, 396-421", "doi": "10.1214/09-AOAS279", "report-no": "IMS-AOAS-AOAS279", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food authenticity studies are concerned with determining if food samples have\nbeen correctly labeled or not. Discriminant analysis methods are an integral\npart of the methodology for food authentication. Motivated by food authenticity\napplications, a model-based discriminant analysis method that includes variable\nselection is presented. The discriminant analysis model is fitted in a\nsemi-supervised manner using both labeled and unlabeled data. The method is\nshown to give excellent classification performance on several high-dimensional\nmulticlass food authenticity data sets with more variables than observations.\nThe variables selected by the proposed method provide information about which\nvariables are meaningful for classification purposes. A headlong search\nstrategy for variable selection is shown to be efficient in terms of\ncomputation and achieves excellent classification performance. In applications\nto several food authenticity data sets, our proposed method outperformed\ndefault implementations of Random Forests, AdaBoost, transductive SVMs and\nBayesian Multinomial Regression by substantial margins.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2009 10:39:44 GMT"}, {"version": "v2", "created": "Thu, 7 Oct 2010 12:51:36 GMT"}], "update_date": "2010-10-08", "authors_parsed": [["Murphy", "Thomas Brendan", ""], ["Dean", "Nema", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "0910.4443", "submitter": "Tom Britton", "authors": "Tom Britton", "title": "Stochastic epidemic models: a survey", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a survey paper on stochastic epidemic models. A simple\nstochastic epidemic model is defined and exact and asymptotic model properties\n(relying on a large community) are presented. The purpose of modelling is\nillustrated by studying effects of vaccination and also in terms of inference\nprocedures for important parameters, such as the basic reproduction number and\nthe critical vaccination coverage. Several generalizations towards realism,\ne.g. multitype and household epidemic models, are also presented, as is a model\nfor endemic diseases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2009 06:09:32 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Britton", "Tom", ""]]}, {"id": "0910.4817", "submitter": "Patricia Gautier", "authors": "Ivana Roche (INIST), Dominique Besagni (INIST), Claire Fran\\c{c}ois\n  (INIST), Marianne H\\\"orlesberger (ARCS), Edgar L Schiebel (ARCS)", "title": "Identification and Characterisation of Technological Topics in the Field\n  of Molecular Biology", "comments": null, "journal-ref": "10th International Conference on Science and Technology\n  Indicators,, Austria (2008)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on methodological approaches for characterising the\nspecific topics within a technological field based on scientific literature\ndata. We introduce a diachronic clustering analysis approach and some\nbibliometric indicators. The results are visualised with the software-tool\nStanalyst [1]. We are applying our methods to the field \"Molecular Biology\".\nThis field has grown a great deal in the last decade.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2009 07:24:53 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Roche", "Ivana", "", "INIST"], ["Besagni", "Dominique", "", "INIST"], ["Fran\u00e7ois", "Claire", "", "INIST"], ["H\u00f6rlesberger", "Marianne", "", "ARCS"], ["Schiebel", "Edgar L", "", "ARCS"]]}, {"id": "0910.5449", "submitter": "David Friedenberg", "authors": "David A. Friedenberg and Christopher R. Genovese", "title": "Straight to the Source: Detecting Aggregate Objects in Astronomical\n  Images with Proper Error Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of telescopes will acquire terabytes of image data on a\nnightly basis. Collectively, these large images will contain billions of\ninteresting objects, which astronomers call sources. The astronomers' task is\nto construct a catalog detailing the coordinates and other properties of the\nsources. The source catalog is the primary data product for most telescopes and\nis an important input for testing new astrophysical theories, but to construct\nthe catalog one must first detect the sources. Existing algorithms for catalog\ncreation are effective at detecting sources, but do not have rigorous\nstatistical error control. At the same time, there are several multiple testing\nprocedures that provide rigorous error control, but they are not designed to\ndetect sources that are aggregated over several pixels. In this paper, we\npropose a technique that does both, by providing rigorous statistical error\ncontrol on the aggregate objects themselves rather than the pixels. We\ndemonstrate the effectiveness of this approach on data from the Chandra X-ray\nObservatory Satellite. Our technique effectively controls the rate of false\nsources, yet still detects almost all of the sources detected by procedures\nthat do not have such rigorous error control and have the advantage of\nadditional data in the form of follow up observations, which will not be\navailable for upcoming large telescopes. In fact, we even detect a new source\nthat was missed by previous studies. The statistical methods developed in this\npaper can be extended to problems beyond Astronomy, as we will illustrate with\nan example from Neuroimaging.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2009 18:31:46 GMT"}], "update_date": "2009-10-29", "authors_parsed": [["Friedenberg", "David A.", ""], ["Genovese", "Christopher R.", ""]]}]