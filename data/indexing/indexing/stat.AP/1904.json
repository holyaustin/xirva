[{"id": "1904.00117", "submitter": "Jean Feng", "authors": "Jean Feng, William S DeWitt III, Aaron McKenna, Noah Simon, Amy\n  Willis, Frederick A Matsen IV", "title": "Estimation of cell lineage trees by maximum-likelihood phylogenetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CRISPR technology has enabled large-scale cell lineage tracing for complex\nmulticellular organisms by mutating synthetic genomic barcodes during\norganismal development. However, these sophisticated biological tools currently\nuse ad-hoc and outmoded computational methods to reconstruct the cell lineage\ntree from the mutated barcodes. Because these methods are agnostic to the\nbiological mechanism, they are unable to take full advantage of the data's\nstructure. We propose a statistical model for the mutation process and develop\na procedure to estimate the tree topology, branch lengths, and mutation\nparameters by iteratively applying penalized maximum likelihood estimation. In\ncontrast to existing techniques, our method estimates time along each branch,\nrather than number of mutation events, thus providing a detailed account of\ntissue-type differentiation. Via simulations, we demonstrate that our method is\nsubstantially more accurate than existing approaches. Our reconstructed trees\nalso better recapitulate known aspects of zebrafish development and reproduce\nsimilar results across fish replicates.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 23:27:36 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Feng", "Jean", ""], ["DeWitt", "William S", "III"], ["McKenna", "Aaron", ""], ["Simon", "Noah", ""], ["Willis", "Amy", ""], ["Matsen", "Frederick A", "IV"]]}, {"id": "1904.00148", "submitter": "Daniel Spencer", "authors": "Daniel Spencer, Rajarshi Guhaniyogi, Raquel Prado", "title": "Bayesian Mixed Effect Sparse Tensor Response Regression Model with Joint\n  Estimation of Activation and Connectivity", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain activation and connectivity analyses in task-based functional magnetic\nresonance imaging (fMRI) experiments with multiple subjects are currently at\nthe forefront of data-driven neuroscience. In such experiments, interest often\nlies in understanding activation of brain voxels due to external stimuli and\nstrong association or connectivity between the measurements on a set of\npre-specified group of brain voxels, also known as regions of interest (ROI).\nThis article proposes a joint Bayesian additive mixed modeling framework that\nsimultaneously assesses brain activation and connectivity patterns from\nmultiple subjects. In particular, fMRI measurements from each individual\nobtained in the form of a multi-dimensional array/tensor at each time are\nregressed on functions of the stimuli. We impose a low-rank PARAFAC\ndecomposition on the tensor regression coefficients corresponding to the\nstimuli to achieve parsimony. Multiway stick breaking shrinkage priors are\nemployed to infer activation patterns and associated uncertainties in each\nvoxel. Further, the model introduces region specific random effects which are\njointly modeled with a Bayesian Gaussian graphical prior to account for the\nconnectivity among pairs of ROIs. Empirical investigations under various\nsimulation studies demonstrate the effectiveness of the method as a tool to\nsimultaneously assess brain activation and connectivity. The method is then\napplied to a multi-subject fMRI dataset from a balloon-analog risk-taking\nexperiment in order to make inference about how the brain processes risk.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 04:53:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Spencer", "Daniel", ""], ["Guhaniyogi", "Rajarshi", ""], ["Prado", "Raquel", ""]]}, {"id": "1904.00495", "submitter": "Weining Shen", "authors": "Wei Hu, Tianyu Pan, Dehan Kong, Weining Shen", "title": "Nonparametric Matrix Response Regression with Application to Brain\n  Imaging Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of neuroimaging technologies, a great effort has been\ndedicated recently to investigate the dynamic changes in brain activity.\nExamples include time course calcium imaging and dynamic brain functional\nconnectivity. In this paper, we propose a novel nonparametric matrix response\nregression model to characterize the nonlinear association between 2D image\noutcomes and predictors such as time and patient information. Our estimation\nprocedure can be formulated as a nuclear norm regularization problem, which can\ncapture the underlying low-rank structure of the dynamic 2D images. We present\na computationally efficient algorithm, derive the asymptotic theory and show\nthat the method outperforms other existing approaches in simulations. We then\napply the proposed method to a calcium imaging study for estimating the change\nof fluorescent intensities of neurons, and an electroencephalography study for\na comparison in the dynamic connectivity covariance matrices between alcoholic\nand control individuals. For both studies, the method leads to a substantial\nimprovement in prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 22:12:57 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 13:42:07 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 06:11:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Hu", "Wei", ""], ["Pan", "Tianyu", ""], ["Kong", "Dehan", ""], ["Shen", "Weining", ""]]}, {"id": "1904.00661", "submitter": "Richard Torkar", "authors": "Richard Torkar, Robert Feldt, Carlo A. Furia", "title": "Bayesian data analysis in empirical software engineering---The case of\n  missing data", "comments": "34 pages, 15 figures. Chapter in the book Contemporary Empirical\n  Methods in Software Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian data analysis (BDA) is today used by a multitude of research\ndisciplines. These disciplines use BDA as a way to embrace uncertainty by using\nmultilevel models and making use of all available information at hand. In this\nchapter, we first introduce the reader to BDA and then provide an example from\nempirical software engineering, where we also deal with a common issue in our\nfield, i.e., missing data.\n  The example we make use of presents the steps done when conducting state of\nthe art statistical analysis. First, we need to understand the problem we want\nto solve. Second, we conduct causal analysis. Third, we analyze\nnon-identifiability. Fourth, we conduct missing data analysis. Finally, we do a\nsensitivity analysis of priors. All this before we design our statistical\nmodel. Once we have a model, we present several diagnostics one can use to\nconduct sanity checks.\n  We hope that through these examples, the reader will see the advantages of\nusing BDA. This way, we hope Bayesian statistics will become more prevalent in\nour field, thus partly avoiding the reproducibility crisis we have seen in\nother disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:40:06 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 16:32:04 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 11:15:28 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Torkar", "Richard", ""], ["Feldt", "Robert", ""], ["Furia", "Carlo A.", ""]]}, {"id": "1904.01116", "submitter": "Yue Wei", "authors": "Yue Wei, Yi Liu, Wei Chen and Ying Ding", "title": "Gene-based Association Analysis for Bivariate Time-to-event Data through\n  Functional Regression with Copula Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several gene-based association tests for time-to-event traits have been\nproposed recently, to detect whether a gene region (containing multiple\nvariants), as a set, is associated with the survival outcome. However, for\nbivariate survival outcomes, to the best of our knowledge, there is no\nstatistical method that can be directly applied for gene-based association\nanalysis. Motivated by a genetic study to discover gene regions associated with\nthe progression of a bilateral eye disease, Age-related Macular Degeneration\n(AMD), we implement a novel functional regression method under the copula\nframework. Specifically, the effects of variants within a gene region are\nmodeled through a functional linear model, which then contributes to the\nmarginal survival functions within the copula. Generalized score test and\nlikelihood ratio test statistics are derived to test for the association\nbetween bivariate survival traits and the genetic region. Extensive simulation\nstudies are conducted to evaluate the type-I error control and power\nperformance of the proposed approach, with comparisons to several existing\nmethods for a single survival trait, as well as the marginal Cox functional\nregression model using the robust sandwich estimator for bivariate survival\ntraits. Finally, we apply our method to a large AMD study, the Age-related Eye\nDisease Study (AREDS), to identify gene regions that are associated with AMD\nprogression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:29:41 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wei", "Yue", ""], ["Liu", "Yi", ""], ["Chen", "Wei", ""], ["Ding", "Ying", ""]]}, {"id": "1904.01128", "submitter": "Xiao Liu", "authors": "Xiao Liu, Rong Pan", "title": "Analysis of Large Heterogeneous Repairable System Reliability Data with\n  Static System Attributes and Dynamic Sensor Measurement in Big Data\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Big Data environment, one pressing challenge facing engineers is to\nperform reliability analysis for a large fleet of heterogeneous repairable\nsystems with covariates. In addition to static covariates, which include\ntime-invariant system attributes such as nominal operating conditions,\ngeo-locations, etc., the recent advances of sensing technologies have also made\nit possible to obtain dynamic sensor measurement of system operating and\nenvironmental conditions. As a common practice in the Big Data environment, the\nmassive reliability data are typically stored in some distributed storage\nsystems. Leveraging the power of modern statistical learning, this paper\ninvestigates a statistical approach which integrates the Random Forests\nalgorithm and the classical data analysis methodologies for repairable system\nreliability, such as the nonparametric estimator for the Mean Cumulative\nFunction and the parametric models based on the Nonhomogeneous Poisson Process.\nWe show that the proposed approach effectively addresses some common challenges\narising from practice, including system heterogeneity, covariate selection,\nmodel specification and data locality due to the distributed data storage. The\nlarge sample properties as well as the uniform consistency of the proposed\nestimator is established. Two numerical examples and a case study are presented\nto illustrate the application of the proposed approach. The strengths of the\nproposed approach are demonstrated by comparison studies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:13:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Liu", "Xiao", ""], ["Pan", "Rong", ""]]}, {"id": "1904.01199", "submitter": "Munir Hiabu", "authors": "Stephan M. Bischofberger, Munir Hiabu, Alex Isakson", "title": "Continuous chain-ladder with paid data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous-time framework for the prediction of outstanding\nliabilities, in which chain-ladder development factors arise as a histogram\nestimator of a cost-weighted hazard function running in reversed development\ntime. We use this formulation to show that under our assumptions on the\nindividual data chain-ladder is consistent. Consistency is understood in the\nsense that both the number of observed claims grows to infinity and the level\nof aggregation tends to zero. We propose alternatives to chain-ladder\ndevelopment factors by replacing the histogram estimator with kernel smoothers\nand by estimating a cost-weighted density instead of a cost-weighted hazard.\nFinally, we provide a real-data example and a simulation study confirming the\nstrengths of the proposed alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:49:18 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 04:22:30 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Bischofberger", "Stephan M.", ""], ["Hiabu", "Munir", ""], ["Isakson", "Alex", ""]]}, {"id": "1904.01280", "submitter": "Yuxin He", "authors": "Yuxin He, Yang Zhao, and Kwok Leung Tsui", "title": "An Analysis of Factors Influencing Metro Station Ridership: Insights\n  from Taipei Metro", "comments": "6 pages. 2018 21st International Conference on Intelligent\n  Transportation Systems (ITSC). IEEE, 2018", "journal-ref": "2018 21st International Conference on Intelligent Transportation\n  Systems (ITSC). IEEE, 2018", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel demand analysis at the planning stage is important for metro system\ndevelopment. In practice, travel demand can be affected by various factors.\nThis paper focuses on investigating the factors influencing Taipei metro\nridership at station level over varying time periods. Ordinary Least Square\n(OLS) multiple regression models with backward stepwise feature selection are\nemployed to identify the influencing factors, including land use, social\neconomic, accessibility, network structure information, etc. Network structure\nfactors are creatively quantified based on complex network theory to accurately\nmeasure the related information. To enhance goodness-of-fit, the dummy variable\ndistinguishing transportation hub is incorporated in the modeling. The main\nfindings in this paper are three-fold: First, there is no distinct difference\nbetween influencing factors of boarding and those of alighting; Second,\nridership is significantly associated with the number of nearby shopping malls,\ndistance to city center, days since opening, nearby bus stations and dummy\nvariable for transportation hub; Finally, the ridership on weekdays is mainly\naffected by commuting activities, while the ridership on weekends is driven by\ncommercial access.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:38:16 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 01:33:29 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["He", "Yuxin", ""], ["Zhao", "Yang", ""], ["Tsui", "Kwok Leung", ""]]}, {"id": "1904.01378", "submitter": "Yuxin He", "authors": "Yuxin He, Yang Zhao, and Kwok Leung Tsui", "title": "An Adapted Geographically Weighted Lasso(Ada-GWL) model for estimating\n  metro ridership", "comments": "41pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridership estimation at station level plays a critical role in metro\ntransportation planning. Among various existing ridership estimation methods,\ndirect demand model has been recognized as an effective approach. However,\nexisting direct demand models including Geographically Weighted Regression\n(GWR) have rarely included local model selection in ridership estimation. In\npractice, acquiring insights into metro ridership under multiple influencing\nfactors from a local perspective is important for passenger volume management\nand transportation planning operations adapting to local conditions. In this\nstudy, we propose an Adapted Geographically Weighted Lasso (Ada-GWL) framework\nfor modelling metro ridership, which performs regression-coefficient shrinkage\nand local model selection. It takes metro network connection intermedia into\naccount and adopts network-based distance metric instead of Euclidean-based\ndistance metric, making it so-called adapted to the context of metro networks.\nThe real-world case of Shenzhen Metro is used to validate the superiority of\nour proposed model. The results show that the Ada-GWL model performs the best\ncompared with the global model (Ordinary Least Square (OLS), GWR, GWR\ncalibrated with network-based distance metric and GWL in terms of estimation\nerror of the dependent variable and goodness-of-fit. Through understanding the\nvariation of each coefficient across space (elasticities) and variables\nselection of each station, it provides more realistic conclusions based on\nlocal analysis. Besides, through clustering analysis of the stations according\nto the regression coefficients, clusters' functional characteristics are found\nto be in compliance with the facts of the functional land use policy of\nShenzhen. These results of the proposed Ada-GWL model demonstrate a great\nspatial explanatory power in transportation planning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 12:44:59 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["He", "Yuxin", ""], ["Zhao", "Yang", ""], ["Tsui", "Kwok Leung", ""]]}, {"id": "1904.01493", "submitter": "Edilberto Cepeda-Cuervo", "authors": "Edilberto Cepeda-Cuervo", "title": "New ITEM response models: application to school bullying data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  School bullying victimization is a variable that cannot be measured directly.\nTaking into account that this variable has a lower bound, given by the absence\nof bullying victimization, this paper proposes IRT logistic models, where the\nlatent parameter ranges from $0$ to $\\infty$ or from $0$ to a positive real\nnumber R, defining the IRT parameters and proposing an empirical anchor\nprocedure. As the academic abilities and the school bullying victimization can\nbe explained due to associated factors such as habits, sex, socioeconomic level\nand education level of parents, IRT regression models are proposed to make\njoint inferences about individual and school characteristic effects. Results\nfrom the application of the proposed models to the Bogot\\'a school bullying\ndataset are presented. The need for testing based in statistical models\nincreases in different fields.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:32:16 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Cepeda-Cuervo", "Edilberto", ""]]}, {"id": "1904.01524", "submitter": "Kevin Layer", "authors": "Kevin Layer, Andrew L. Johnson, Robin C. Sickles, Gary D. Ferrier", "title": "Direction Selection in Stochastic Directional Distance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers rely on the distance function to model multiple product\nproduction using multiple inputs. A stochastic directional distance function\n(SDDF) allows for noise in potentially all input and output variables. Yet,\nwhen estimated, the direction selected will affect the functional estimates\nbecause deviations from the estimated function are minimized in the specified\ndirection. The set of identified parameters of a parametric SDDF can be\nnarrowed via data-driven approaches to restrict the directions considered. We\ndemonstrate a similar narrowing of the identified parameter set for a shape\nconstrained nonparametric method, where the shape constraints impose standard\nfeatures of a cost function such as monotonicity and convexity.\n  Our Monte Carlo simulation studies reveal significant improvements, as\nmeasured by out of sample radial mean squared error, in functional estimates\nwhen we use a directional distance function with an appropriately selected\ndirection. From our Monte Carlo simulations we conclude that selecting a\ndirection that is approximately orthogonal to the estimated function in the\ncentral region of the data gives significantly better estimates relative to the\ndirections commonly used in the literature. For practitioners, our results\nimply that selecting a direction vector that has non-zero components for all\nvariables that may have measurement error provides a significant improvement in\nthe estimator's performance. We illustrate these results using cost and\nproduction data from samples of approximately 500 US hospitals per year\noperating in 2007, 2008, and 2009, respectively, and find that the shape\nconstrained nonparametric methods provide a significant increase in flexibility\nover second order local approximation parametric methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:14:01 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 16:45:51 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Layer", "Kevin", ""], ["Johnson", "Andrew L.", ""], ["Sickles", "Robin C.", ""], ["Ferrier", "Gary D.", ""]]}, {"id": "1904.01628", "submitter": "Adam Lauretig", "authors": "Adam M. Lauretig", "title": "Identification, Interpretability, and Bayesian Word Embeddings", "comments": "Accepted to the Third Workshop on Natural Language Processing and\n  Computational Social Science at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have recently turned to analyzing text using tools from\nnatural language processing like word embeddings to measure concepts like\nideology, bias, and affinity. However, word embeddings are difficult to use in\nthe regression framework familiar to social scientists: embeddings are are\nneither identified, nor directly interpretable. I offer two advances on\nstandard embedding models to remedy these problems. First, I develop Bayesian\nWord Embeddings with Automatic Relevance Determination priors, relaxing the\nassumption that all embedding dimensions have equal weight. Second, I apply\nwork identifying latent variable models to anchor the dimensions of the\nresulting embeddings, identifying them, and making them interpretable and\nusable in a regression. I then apply this model and anchoring approach to two\ncases, the shift in internationalist rhetoric in the American presidents'\ninaugural addresses, and the relationship between bellicosity in American\nforeign policy decision-makers' deliberations. I find that inaugural addresses\nbecame less internationalist after 1945, which goes against the conventional\nwisdom, and that an increase in bellicosity is associated with an increase in\nhostile actions by the United States, showing that elite deliberations are not\ncheap talk, and helping confirm the validity of the model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:12:35 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Lauretig", "Adam M.", ""]]}, {"id": "1904.01659", "submitter": "Moritz N. Lang", "authors": "Moritz N. Lang and Georg J. Mayr and Reto Stauffer and Achim Zeileis", "title": "Bivariate Gaussian models for wind vectors in a distributional\n  regression framework", "comments": null, "journal-ref": "Adv. Stat. Clim. Meteorol. Oceanogr. 5 (2019) 115-132", "doi": "10.5194/ascmo-5-115-2019", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new probabilistic post-processing method for wind vectors is presented in a\ndistributional regression framework employing the bivariate Gaussian\ndistribution. In contrast to previous studies all parameters of the\ndistribution are simultaneously modeled, namely the means and variances for\nboth wind components and also the correlation coefficient between them\nemploying flexible regression splines. To capture a possible mismatch between\nthe predicted and observed wind direction, ensemble forecasts of both wind\ncomponents are included using flexible two-dimensional smooth functions. This\nencompasses a smooth rotation of the wind direction conditional on the season\nand the forecasted ensemble wind direction.\n  The performance of the new method is tested for stations located in plains,\nmountain foreland, and within an alpine valley employing ECMWF ensemble\nforecasts as explanatory variables for all distribution parameters. The\nrotation-allowing model shows distinct improvements in terms of predictive\nskill for all sites compared to a baseline model that post-processes each wind\ncomponent separately. Moreover, different correlation specifications are tested\nand small improvements compared to the model setup with no estimated\ncorrelation could be found for stations located in alpine valleys.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:34:17 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Lang", "Moritz N.", ""], ["Mayr", "Georg J.", ""], ["Stauffer", "Reto", ""], ["Zeileis", "Achim", ""]]}, {"id": "1904.01668", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Joseph W. Hogan", "title": "Causal comparative effectiveness analysis of dynamic continuous-time\n  treatment initiation rules with sparsely measured outcomes and death", "comments": "Accepted by Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence supporting the current World Health Organization recommendations of\nearly antiretroviral therapy (ART) initiation for adolescents is inconclusive.\nWe leverage a large observational data and compare, in terms of mortality and\nCD4 cell count, the dynamic treatment initiation rules for HIV-infected\nadolescents. Our approaches extend the marginal structural model for estimating\noutcome distributions under dynamic treatment regimes (DTR), developed in\nRobins et al. (2008), to allow the causal comparisons of both specific regimes\nand regimes along a continuum. Furthermore, we propose strategies to address\nthree challenges posed by the complex data set: continuous-time measurement of\nthe treatment initiation process; sparse measurement of longitudinal outcomes\nof interest, leading to incomplete data; and censoring due to dropout and\ndeath. We derive a weighting strategy for continuous time treatment initiation;\nuse imputation to deal with missingness caused by sparse measurements and\ndropout; and define a composite outcome that incorporates both death and CD4\ncount as a basis for comparing treatment regimes. Our analysis suggests that\nimmediate ART initiation leads to lower mortality and higher median values of\nthe composite outcome, relative to other initiation rules.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:07:49 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hu", "Liangyuan", ""], ["Hogan", "Joseph W.", ""]]}, {"id": "1904.01676", "submitter": "S. Stanley Young", "authors": "S. Stanley Young and Warren B. Kindzierski", "title": "Evaluation of a meta-analysis of air quality and heart attacks, a case\n  study", "comments": "46 pages", "journal-ref": "Critical Reviews in Toxicology 28/03/2019", "doi": "10.1080/10408444.2019.1576587", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally acknowledged that claims from observational studies often\nfail to replicate. An exploratory study was undertaken to assess the\nreliability of base studies used in meta-analysis of short-term air\nquality-myocardial infarction risk and to judge the reliability of statistical\nevidence from meta-analysis that uses data from observational studies. A highly\ncited meta-analysis paper examining whether short-term air quality exposure\ntriggers myocardial infarction was evaluated as a case study. The paper\nconsidered six air quality components - carbon monoxide, nitrogen dioxide,\nsulfur dioxide, particulate matter 10 and 2.5 micrometers in diameter (PM10 and\nPM2.5), and ozone. The number of possible questions and statistical models at\nissue in each of 34 base papers used were estimated and p-value plots for each\nof the air components were constructed to evaluate the effect heterogeneity of\np-values used from the base papers. Analysis search spaces (number of\nstatistical tests possible) in the base papers were large, median of 12,288,\ninterquartile range: 2,496 to 58,368, in comparison to actual statistical test\nresults presented. Statistical test results taken from the base papers may not\nprovide unbiased measures of effect for meta-analysis. Shapes of p-value plots\nfor the six air components were consistent with the possibility of analysis\nmanipulation to obtain small p-values in several base papers. Results suggest\nthe appearance of heterogeneous, researcher-generated p-values used in the\nmeta-analysis rather than unbiased evidence of real effects for air quality. We\nconclude that this meta-analysis does not provide reliable evidence for an\nassociation of air quality components with myocardial risk.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:36:14 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Young", "S. Stanley", ""], ["Kindzierski", "Warren B.", ""]]}, {"id": "1904.01832", "submitter": "Andreas Mittermeier", "authors": "Andreas Mittermeier, Birgit Ertl-Wagner, Jens Ricke, Olaf Dietrich,\n  Michael Ingrisch", "title": "Bayesian Pharmacokinetic Modeling of Dynamic Contrast-Enhanced Magnetic\n  Resonance Imaging: Validation and Application", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": "10.1088/1361-6560/ab3a5a", "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracer-kinetic analysis of dynamic contrast-enhanced magnetic resonance\nimaging data is commonly performed with the well-known Tofts model and\nnonlinear least squares (NLLS) regression. This approach yields point estimates\nof model parameters, uncertainty of these estimates can be assessed e.g. by an\nadditional bootstrapping analysis. Here, we present a Bayesian probabilistic\nmodeling approach for tracer-kinetic analysis with a Tofts model, which yields\nposterior probability distributions of perfusion parameters and therefore\npromises a robust and information-enriched alternative based on a framework of\nprobability distributions. In this manuscript, we use the Quantitative Imaging\nBiomarkers Alliance (QIBA) Tofts phantom to evaluate the Bayesian Tofts Model\n(BTM) against a bootstrapped NLLS approach. Furthermore, we demonstrate how\nBayesian posterior probability distributions can be employed to assess\ntreatment response in a breast cancer DCE-MRI dataset using Cohen's d. Accuracy\nand precision of the BTM posterior distributions were validated and found to be\nin good agreement with the NLLS approaches, and assessment of therapy response\nwith respect to uncertainty in parameter estimates was found to be excellent.\nIn conclusion, the Bayesian modeling approach provides an elegant means to\ndetermine uncertainty via posterior distributions within a single step and\nprovides honest information about changes in parameter estimates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:18:09 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Mittermeier", "Andreas", ""], ["Ertl-Wagner", "Birgit", ""], ["Ricke", "Jens", ""], ["Dietrich", "Olaf", ""], ["Ingrisch", "Michael", ""]]}, {"id": "1904.01932", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Joseph W. Hogan, Ann W. Mwangi, Abraham Siika", "title": "Modeling the Causal Effect of Treatment Initiation Time on Survival:\n  Application to HIV/TB Co-infection", "comments": "Published in Biometrics", "journal-ref": "Biometrics 2018; 74(2): 703-713", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timing of antiretroviral therapy (ART) initiation for HIV and\ntuberculosis (TB) co-infected patients needs to be considered carefully. CD4\ncell count can be used to guide decision making about when to initiate ART.\nEvidence from recent randomized trials and observational studies generally\nsupports early initiation but does not provide information about effects of\ninitiation time on a continuous scale. In this paper, we develop and apply a\nhighly flexible structural proportional hazards model for characterizing the\neffect of treatment initiation time on a survival distribution. The model can\nbe fitted using a weighted partial likelihood score function. Construction of\nboth the score function and the weights must accommodate censoring of the\ntreatment initiation time, the outcome, or both. The methods are applied to\ndata on 4903 individuals with HIV/TB co-infection, derived from electronic\nhealth records in a large HIV care program in Kenya. We use a model formulation\nthat flexibly captures the joint effects of ART initiation time and ART\nduration using natural cubic splines. The model is used to generate survival\ncurves corresponding to specific treatment initiation times; and to identify\noptimal times for ART initiation for subgroups defined by CD4 count at time of\nTB diagnosis. Our findings potentially provide \"higher resolution\" information\nabout the relationship between ART timing and mortality, and about the\ndifferential effect of ART timing within CD4 subgroups.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:57:31 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hu", "Liangyuan", ""], ["Hogan", "Joseph W.", ""], ["Mwangi", "Ann W.", ""], ["Siika", "Abraham", ""]]}, {"id": "1904.02052", "submitter": "Philipp M. Maier", "authors": "Philipp M. Maier, Sina Keller", "title": "Estimating Chlorophyll a Concentrations of Several Inland Waters with\n  Hyperspectral Data and Machine Learning Models", "comments": "Accepted at ISPRS Geospatial Week 2019 in Enschede", "journal-ref": null, "doi": "10.5194/isprs-annals-IV-2-W5-609-2019", "report-no": null, "categories": "cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water is a key component of life, the natural environment and human health.\nFor monitoring the conditions of a water body, the chlorophyll a concentration\ncan serve as a proxy for nutrients and oxygen supply. In situ measurements of\nwater quality parameters are often time-consuming, expensive and limited in\nareal validity. Therefore, we apply remote sensing techniques. During field\ncampaigns, we collected hyperspectral data with a spectrometer and in situ\nmeasured chlorophyll a concentrations of 13 inland water bodies with different\nspectral characteristics. One objective of this study is to estimate\nchlorophyll a concentrations of these inland waters by applying three machine\nlearning regression models: Random Forest, Support Vector Machine and an\nArtificial Neural Network. Additionally, we simulate four different\nhyperspectral resolutions of the spectrometer data to investigate the effects\non the estimation performance. Furthermore, the application of first order\nderivatives of the spectra is evaluated in turn to the regression performance.\nThis study reveals the potential of combining machine learning approaches and\nremote sensing data for inland waters. Each machine learning model achieves an\nR2-score between 80 % to 90 % for the regression on chlorophyll a\nconcentrations. The random forest model benefits clearly from the applied\nderivatives of the spectra. In further studies, we will focus on the\napplication of machine learning models on spectral satellite data to enhance\nthe area-wide estimation of chlorophyll a concentration for inland waters.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:16:36 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Maier", "Philipp M.", ""], ["Keller", "Sina", ""]]}, {"id": "1904.02058", "submitter": "Sung J. Choi", "authors": "Sung J. Choi, M. Eric Johnson", "title": "Do Hospital Data Breaches Reduce Patient Care Quality?", "comments": "32 pages, 6 figures, 4 tables, presented at the Workshop on the\n  Economics of Information Security 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To estimate the relationship between a hospital data breach and\nhospital quality outcome\n  Materials and Methods: Hospital data breaches reported to the U.S. Department\nof Health and Human Services breach portal and the Privacy Rights Clearinghouse\ndatabase were merged with the Medicare Hospital Compare data to assemble a\npanel of non-federal acutecare inpatient hospitals for years 2011 to 2015. The\nstudy panel included 2,619 hospitals. Changes in 30-day AMI mortality rate\nfollowing a hospital data breach were estimated using a multivariate regression\nmodel based on a difference-in-differences approach.\n  Results: A data breach was associated with a 0.338[95% CI, 0.101-0.576]\npercentage point increase in the 30-day AMI mortality rate in the year\nfollowing the breach and a 0.446[95% CI, 0.164-0.729] percentage point increase\ntwo years after the breach. For comparison, the median 30-day AMI mortality\nrate has been decreasing about 0.4 percentage points annually since 2011 due to\nprogress in care. The magnitude of the breach impact on hospitals' AMI\nmortality rates was comparable to a year's worth historical progress in\nreducing AMI mortality rates.\n  Conclusion: Hospital data breaches significantly increased the 30-day\nmortality rate for AMI. Data breaches may disrupt the processes of care that\nrely on health information technology. Financial costs to repair a breach may\nalso divert resources away from patient care. Thus breached hospitals should\ncarefully focus investments in security procedures, processes, and health\ninformation technology that jointly lead to better data security and improved\npatient outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:26:12 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Choi", "Sung J.", ""], ["Johnson", "M. Eric", ""]]}, {"id": "1904.02145", "submitter": "Frederic Barraquand", "authors": "Frederic Barraquand and Olivier Gimenez", "title": "Fitting stochastic predator-prey models using both population density\n  and kill rate data", "comments": null, "journal-ref": null, "doi": "10.1016/j.tpb.2021.01.003", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most mechanistic predator-prey modelling has involved either parameterization\nfrom process rate data or inverse modelling. Here, we take a median road: we\naim at identifying the potential benefits of combining datasets, when both\npopulation growth and predation processes are viewed as stochastic. We fit a\ndiscrete-time, stochastic predator-prey model of the Leslie type to simulated\ntime series of densities and kill rate data. Our model has both environmental\nstochasticity in the growth rates and interaction stochasticity, i.e., a\nstochastic functional response. We examine what the kill rate data brings to\nthe quality of the estimates, and whether estimation is possible (for various\ntime series lengths) solely with time series of population counts or biomass\ndata. Both Bayesian and frequentist estimation are performed, providing\nmultiple ways to check model identifiability. The Fisher Information Matrix\nsuggests that models with and without kill rate data are all identifiable,\nalthough correlations remain between parameters that belong to the same\nfunctional form. However, our results show that if the attractor is a fixed\npoint in the absence of stochasticity, identifying parameters in practice\nrequires kill rate data as a complement to the time series of population\ndensities, due to the relatively flat likelihood. Only noisy limit cycle\nattractors can be identified directly from population count data (as in inverse\nmodelling), although even in this case, adding kill rate data - including in\nsmall amounts - can make the estimates much more precise. Overall, we show that\nunder process stochasticity in interaction rates, interaction data might be\nessential to obtain identifiable dynamical models for multiple species. These\nresults may extend to other biotic interactions than predation, for which\nsimilar models combining interaction rates and population counts could be\ndeveloped.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:59:57 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 22:04:37 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 14:09:57 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Barraquand", "Frederic", ""], ["Gimenez", "Olivier", ""]]}, {"id": "1904.02219", "submitter": "Abhik Ghosh PhD", "authors": "Elena Castilla, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "Robust semiparametric inference for polytomous logistic regression with\n  complex survey design", "comments": "Preprint; Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing polytomous response from a complex survey scheme, like stratified\nor cluster sampling is very crucial in several socio-economics applications. We\npresent a class of minimum quasi weighted density power divergence estimators\nfor the polytomous logistic regression model with such a complex survey. This\nfamily of semiparametric estimators is a robust generalization of the maximum\nquasi weighted likelihood estimator exploiting the advantages of the popular\ndensity power divergence measure. Accordingly robust estimators for the design\neffects are also derived. Robust testing of general linear hypotheses on the\nregression coefficients are proposed using the new estimators. Their asymptotic\ndistributions and robustness properties are theoretically studied and also\nempirically validated through a numerical example and an extensive Monte Carlo\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:47:17 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Castilla", "Elena", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1904.02235", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Christian Kroer, Adam Lerer", "title": "Robust Multi-agent Counterfactual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using logged data to make predictions about what\nwould happen if we changed the `rules of the game' in a multi-agent system.\nThis task is difficult because in many cases we observe actions individuals\ntake but not their private information or their full reward functions. In\naddition, agents are strategic, so when the rules change, they will also change\ntheir actions. Existing methods (e.g. structural estimation, inverse\nreinforcement learning) make counterfactual predictions by constructing a model\nof the game, adding the assumption that agents' behavior comes from optimizing\ngiven some goals, and then inverting observed actions to learn agent's\nunderlying utility function (a.k.a. type). Once the agent types are known,\nmaking counterfactual predictions amounts to solving for the equilibrium of the\ncounterfactual environment. This approach imposes heavy assumptions such as\nrationality of the agents being observed, correctness of the analyst's model of\nthe environment/parametric form of the agents' utility functions, and various\nother conditions to make point identification possible. We propose a method for\nanalyzing the sensitivity of counterfactual conclusions to violations of these\nassumptions. We refer to this method as robust multi-agent counterfactual\nprediction (RMAC). We apply our technique to investigating the robustness of\ncounterfactual claims for classic environments in market design: auctions,\nschool choice, and social choice. Importantly, we show RMAC can be used in\nregimes where point identification is impossible (e.g. those which have\nmultiple equilibria or non-injective maps from type distributions to outcomes).\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 20:49:37 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Kroer", "Christian", ""], ["Lerer", "Adam", ""]]}, {"id": "1904.02519", "submitter": "Thea Roksv{\\aa}g", "authors": "Thea Roksv{\\aa}g, Ingelin Steinsland and Kolbj{\\o}rn Engeland", "title": "A geostatistical two field model that combines point observations and\n  nested areal observations, and quantifies long-term spatial variability -- A\n  case study of annual runoff predictions in the Voss area", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12492", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, annual runoff is estimated by using a Bayesian geostatistical\nmodel for interpolation of hydrological data of different spatial support. That\nis, streamflow observations from catchments (areal data), and precipitation and\nevaporation data (point data). The model contains one climatic spatial effect\nthat is common for all years under study, and one year specific spatial effect.\nHence, the framework enables a quantification of the spatial variability that\nis due to long-term weather patterns and processes. This can contribute to a\nbetter understanding of biases and uncertainties in environmental modeling. By\nusing integrated nested Laplace approximations (INLA) and the stochastic\npartial differential equation approach (SPDE) to spatial modeling, the two\nfield model is computationally feasible and fast. The suggested model is tested\nby predicting 10 years of annual runoff around Voss in Norway and through a\nsimulation study. We find that on average we benefit from combining point and\nareal data compared to using only one of the data types, and that the\ninteraction between nested areal data and point data gives a spatial model that\ntakes us beyond smoothing. Another finding is that when climatic effects\ndominate over annual effects, systematic under- and overestimation of runoff\nover time can be expected. On the other hand, a dominating climatic spatial\neffect implies that short records of runoff from an otherwise ungauged\ncatchment can lead to large improvements in the predictability of runoff.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:44:39 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:39:59 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 15:22:21 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 08:37:57 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Roksv\u00e5g", "Thea", ""], ["Steinsland", "Ingelin", ""], ["Engeland", "Kolbj\u00f8rn", ""]]}, {"id": "1904.02846", "submitter": "Wenying Ji", "authors": "Yudi Chen, Qi Wang, Wenying Ji", "title": "A Bayesian-Based Approach for Public Sentiment Modeling", "comments": null, "journal-ref": null, "doi": "10.1109/WSC40007.2019.9004846", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public sentiment is a direct public-centric indicator for the success of\neffective action planning. Despite its importance, systematic modeling of\npublic sentiment remains untapped in previous studies. This research aims to\ndevelop a Bayesian-based approach for quantitative public sentiment modeling,\nwhich is capable of incorporating uncertainty and guiding the selection of\npublic sentiment measures. This study comprises three steps: (1) quantifying\nprior sentiment information and new sentiment observations with Dirichlet\ndistribution and multinomial distribution respectively; (2) deriving the\nposterior distribution of sentiment probabilities through incorporating the\nDirichlet distribution and multinomial distribution via Bayesian inference; and\n(3) measuring public sentiment through aggregating sampled sets of sentiment\nprobabilities with an application-based measure. A case study on Hurricane\nHarvey is provided to demonstrate the feasibility and applicability of the\nproposed approach. The developed approach also has the potential to be\ngeneralized to model various types of probability-based measures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 01:57:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Yudi", ""], ["Wang", "Qi", ""], ["Ji", "Wenying", ""]]}, {"id": "1904.02921", "submitter": "Igor Koval", "authors": "Igor Koval (ARAMIS, CMAP, ICM), St\\'ephanie Allassonni\\`ere (CRC -\n  UMR-S 1138), Stanley Durrleman (ICM, ARAMIS)", "title": "Simulation of virtual cohorts increases predictive accuracy of cognitive\n  decline in MCI subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict the progression of biomarkers, notably in NDD, is\nlimited by the size of the longitudinal data sets, in terms of number of\npatients, number of visits per patients and total follow-up time. To this end,\nwe introduce a data augmentation technique that is able to reproduce the\nvariability seen in a longitudinal training data set and simulate continuous\nbiomarkers trajectories for any number of virtual patients. Thanks to this\nsimulation framework, we propose to transform the training set into a simulated\ndata set with more patients, more time-points per patient and longer follow-up\nduration. We illustrate this approach on the prediction of the MMSE of MCI\nsubjects of the ADNI data set. We show that it allows to reach predictions with\nerrors comparable to the noise in the data, estimated in test/retest studies,\nachieving a improvement of 37% of the mean absolute error compared to the same\nnon-augmented model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:01:39 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Koval", "Igor", "", "ARAMIS, CMAP, ICM"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC -\n  UMR-S 1138"], ["Durrleman", "Stanley", "", "ICM, ARAMIS"]]}, {"id": "1904.03054", "submitter": "Lionel Barnett", "authors": "Lionel Barnett and Anil K. Seth", "title": "Inferring the temporal structure of directed functional connectivity in\n  neural systems: some extensions to Granger causality", "comments": "Accepted for presentation at the special session \"Brain connectivity\n  and neuronal system identification: theory and applications to brain state\n  decoding\", and for publication in conference proceedings, by the 9th Workshop\n  on Brain-Machine Interface (BMI) at IEEE Systems, Man and Cybernetics (SMC)\n  2019, Bari, Italy, October 6-9, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural processes in the brain operate at a range of temporal scales. Granger\ncausality, the most widely-used neuroscientific tool for inference of directed\nfunctional connectivity from neurophsyiological data, is traditionally deployed\nin the form of one-step-ahead prediction regardless of the data sampling rate,\nand as such yields only limited insight into the temporal structure of the\nunderlying neural processes. We introduce Granger causality variants based on\nmulti-step, infinite-future and single-lag prediction, which facilitate a more\ndetailed and systematic temporal analysis of information flow in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 13:30:28 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:11:30 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Barnett", "Lionel", ""], ["Seth", "Anil K.", ""]]}, {"id": "1904.03397", "submitter": "Oliver Stoner", "authors": "Oliver Stoner, Theo Economou", "title": "Multivariate Hierarchical Frameworks for Modelling Delayed Reporting in\n  Count Data", "comments": "Biometrics (2019)", "journal-ref": null, "doi": "10.1111/biom.13188", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields and applications count data can be subject to delayed\nreporting. This is where the total count, such as the number of disease cases\ncontracted in a given week, may not be immediately available, instead arriving\nin parts over time. For short term decision making, the statistical challenge\nlies in predicting the total count based on any observed partial counts, along\nwith a robust quantification of uncertainty. In this article we discuss\nprevious approaches to modelling delayed reporting and present a multivariate\nhierarchical framework where the count generating process and delay mechanism\nare modelled simultaneously. Unlike other approaches, the framework can also be\neasily adapted to allow for the presence of under-reporting in the final\nobserved count. To compare our approach with existing frameworks, one of which\nwe extend to potentially improve predictive performance, we present a case\nstudy of reported dengue fever cases in Rio de Janeiro. Based on both\nwithin-sample and out-of-sample posterior predictive model checking and\narguments of interpretability, adaptability, and computational efficiency, we\ndiscuss the advantages and disadvantages of each modelling framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:29:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Stoner", "Oliver", ""], ["Economou", "Theo", ""]]}, {"id": "1904.03401", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento", "title": "Idealize - A Notion of Idea Strength", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Entrepreneurs frequently thrive on looking for ways to test business\nideas, without giving too much information. Recent techniques in startup\ndevelopment promote the use of surveys to measure the potential client's\ninterest. In this preliminary report, we describe the concept behind Idealize,\na Shiny R application to measure the local trend strength of a potential idea.\nAdditionally, the system might provide a relative distance to the capital city\nof the country. The tests were made for the United States of America, i.e.,\nmade available regarding native English language. This report shows some of the\ntests results with this system.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:44:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""]]}, {"id": "1904.03444", "submitter": "Avinash Achar", "authors": "B. Dhivyabharathi, B. Anil Kumar, Avinash Achar, Lelitha Vanajakshi", "title": "Bus Travel Time Prediction: A Lognormal Auto-Regressive (AR) Modeling\n  Approach", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing real time information about the arrival time of the transit buses\nhas become inevitable in urban areas to make the system more user-friendly and\nadvantageous over various other transportation modes. However, accurate\nprediction of arrival time of buses is still a challenging problem in\ndynamically varying traffic conditions especially under heterogeneous traffic\ncondition without lane discipline. One broad approach researchers have adopted\nover the years is to segment the entire bus route into segments and work with\nthese segment travel times as the data input (from GPS traces) for prediction.\nThis paper adopts this approach and proposes predictive modelling approaches\nwhich fully exploit the temporal correlations in the bus GPS data.\nSpecifically, we propose two approaches: (a) classical time-series approach\nemploying a seasonal AR model (b)unconventional linear, non-stationary AR\napproach. The second approach is a novel technique and exploits the notion of\npartial correlation for learning from data. A detailed analysis of the marginal\ndistributions of the data from Indian conditions (used here), revealed a\npredominantly log-normal behavior. This aspect was incorporated into the above\nproposed predictive models and statistically optimal prediction schemes in the\nlognormal sense are utilized for all predictions. Both the above temporal\npredictive modeling approaches predict ahead in time at each segment\nindependently. For real-time bus travel time prediction, one however needs to\npredict across multiple segments ahead in space. Towards a complete solution,\nthe study also proposes an intelligent procedure to perform (real-time)\nmulti-section ahead travel-time predictions based on either of the above\nproposed temporal models. Results showed a clear improvement in prediction\naccuracy using the proposed methods,\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:34:44 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Dhivyabharathi", "B.", ""], ["Kumar", "B. Anil", ""], ["Achar", "Avinash", ""], ["Vanajakshi", "Lelitha", ""]]}, {"id": "1904.03524", "submitter": "Md. Noor-E-Alam", "authors": "Md Mahmudul Hasan, Md. Noor-E-Alam, Mehul Rakeshkumar Patel, Alicia\n  Sasser Modestino, Leon D. Sanchez, Gary Young", "title": "A Big Data Analytics Framework to Predict the Risk of Opioid Use\n  Disorder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overdose related to prescription opioids have reached an epidemic level in\nthe US, creating an unprecedented national crisis. This has been exacerbated\npartly due to the lack of tools for physicians to help predict the risk of\nwhether a patient will develop opioid use disorder. Little is known about how\nmachine learning can be applied to a big-data platform to ensure an informed,\nsustained and judicious prescribing of opioids, in particular for commercially\ninsured population. This study explores Massachusetts All Payer Claims Data, a\nde-identified healthcare dataset, and proposes a machine learning framework to\nexamine how na\\\"ive users develop opioid use disorder. We perform several\nfeature selections techniques to identify influential demographic and clinical\nfeatures associated with opioid use disorder from a class imbalanced analytic\nsample. We then compare the predictive power of four well-known machine\nlearning algorithms: Logistic Regression, Random Forest, Decision Tree, and\nGradient Boosting to predict the risk of opioid use disorder. The study results\nshow that the Random Forest model outperforms the other three algorithms while\ndetermining the features, some of which are consistent with prior clinical\nfindings. Moreover, alongside the higher predictive accuracy, the proposed\nframework is capable of extracting some risk factors that will add significant\nknowledge to what is already known in the extant literature. We anticipate that\nthis study will help healthcare practitioners improve the current prescribing\npractice of opioids and contribute to curb the increasing rate of opioid\naddiction and overdose.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 20:21:54 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:06:15 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 02:05:13 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hasan", "Md Mahmudul", ""], ["Noor-E-Alam", "Md.", ""], ["Patel", "Mehul Rakeshkumar", ""], ["Modestino", "Alicia Sasser", ""], ["Sanchez", "Leon D.", ""], ["Young", "Gary", ""]]}, {"id": "1904.03546", "submitter": "Felipe Vaca", "authors": "Felipe Vaca-Ram\\'irez", "title": "Robustness of urban road networks based on spatial topological patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, road network vulnerability assessment has received an\nincreasing attention. On one hand, it is due to the significant advances in\nNetwork Science and the potentialities that its tools offer. On the other hand,\nit is due to its utility for urban planning and emergency response. Despite\nthese facts and the increasingly available data, related work is still sparse\nin Latin America, even more so in Ecuador. Due to its geographical, historical,\nand social characteristics, the city of Quito is considered as a case study. At\nfirst, the spatial distributions of several topological centrality measures are\nanalyzed. As expected, there are hotspots where high values of these measures\nconcentrate. These results serve to further simulate several strategies for\ndisconnecting the urban road network. Finally, we observe that centrality-based\nstrategies are more effective than randomly-based strategies in disconnecting\nthe network.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 23:36:58 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Vaca-Ram\u00edrez", "Felipe", ""]]}, {"id": "1904.03702", "submitter": "Mikkel Bennedsen", "authors": "Mikkel Bennedsen", "title": "Designing a statistical procedure for monitoring global carbon dioxide\n  emissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the Paris Agreement of $2015$, most countries have agreed to reduce\ntheir carbon dioxide (CO$_2$) emissions according to individually set\nNationally Determined Contributions. However, national CO$_2$ emissions are\nreported by individual countries and cannot be directly measured or verified by\nthird parties. Inherent weaknesses in the reporting methodology may\nmisrepresent, typically an under-reporting of, the total national emissions.\nThis paper applies the theory of sequential testing to design a statistical\nmonitoring procedure that can be used to detect systematic under-reportings of\nCO$_2$ emissions. Using simulations, we investigate how the proposed sequential\ntesting procedure can be expected to work in practice. We find that, if\nemissions are reported faithfully, the test is correctly sized, while, if\nemissions are under-reported, detection time can be sufficiently fast to help\ninform the $5$ yearly global \"stocktake\" of the Paris Agreement. We recommend\nthe monitoring procedure be applied going forward as part of a larger portfolio\nof methods designed to verify future global CO$_2$ emissions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 18:03:59 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 06:03:43 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 16:27:15 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 06:26:31 GMT"}, {"version": "v5", "created": "Thu, 25 Feb 2021 13:11:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bennedsen", "Mikkel", ""]]}, {"id": "1904.03717", "submitter": "Ricardo Ehlers", "authors": "Ian M Danilevicz and Ricardo S Ehlers", "title": "Bayesian influence diagnostics using normalizing functional Bregman\n  divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideally, any statistical inference should be robust to local influences.\nAlthough there are simple ways to check about leverage points in independent\nand linear problems, more complex models require more sophisticated methods.\nKullback-Leiber and Bregman divergences were already applied in Bayesian\ninference to measure the isolated impact of each observation in a model. We\nextend these ideas to models for dependent data and with non-normal probability\ndistributions such as time series, spatial models and generalized linear\nmodels. We also propose a strategy to rescale the functional Bregman divergence\nto lie in the (0,1) interval thus facilitating interpretation and comparison.\nThis is accomplished with a minimal computational effort and maintaining all\ntheoretical properties. For computational efficiency, we take advantage of\nHamiltonian Monte Carlo methods to draw samples from the posterior distribution\nof model parameters. The resulting Markov chains are then directly connected\nwith Bregman calculus, which results in fast computation. We check the\npropositions in both simulated and empirical studies.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:33:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Danilevicz", "Ian M", ""], ["Ehlers", "Ricardo S", ""]]}, {"id": "1904.03720", "submitter": "Xichen She", "authors": "Xichen She, Yaya Zhai, Ricardo Henao, Christopher W. Woods, Geoffrey\n  S. Ginsburg, Peter X.K. Song, Alfred O. Hero", "title": "An unsupervised transfer learning algorithm for sleep monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To develop multisensor-wearable-device sleep monitoring algorithms\nthat are robust to health disruptions affecting sleep patterns. Methods: We\ndevelop an unsupervised transfer learning algorithm based on a multivariate\nhidden Markov model and Fisher's linear discriminant analysis, adaptively\nadjusting to sleep pattern shift by training on dynamics of sleep/wake states.\nThe proposed algorithm operates, without requiring a priori information about\ntrue sleep/wake states, by establishing an initial training set with hidden\nMarkov model and leveraging a taper window mechanism to learn the sleep pattern\nin an incremental fashion. Our domain-adaptation algorithm is applied to a\ndataset collected in a human viral challenge study to identify sleep/wake\nperiods of both uninfected and infected participants. Results: The algorithm\nsuccessfully detects sleep/wake sessions in subjects whose sleep patterns are\ndisrupted by respiratory infection (H3N2 flu virus). Pre-symptomatic features\nbased on the detected periods are found to be strongly predictive of both\ninfection status (AUC = 0.844) and infection onset time (AUC = 0.885),\nindicating the effectiveness and usefulness of the algorithm. Conclusion: Our\nmethod can effectively detect sleep/wake states in the presence of sleep\npattern shift. Significance: Utilizing integrated multisensor signal processing\nand adaptive training schemes, our algorithm is able to capture key sleep\npatterns in ambulatory monitoring, leading to better automated sleep assessment\nand prediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:42:20 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["She", "Xichen", ""], ["Zhai", "Yaya", ""], ["Henao", "Ricardo", ""], ["Woods", "Christopher W.", ""], ["Ginsburg", "Geoffrey S.", ""], ["Song", "Peter X. K.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1904.03825", "submitter": "Boris Ryabko", "authors": "K.S. Chirikhin, B.Ya. Ryabko", "title": "Application of data compression techniques to time series forecasting", "comments": "Accepted. International Symposium on Forecasting, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we show that standard well-known file compression programs\n(zlib, bzip2, etc.) are able to forecast real-world time series data well. The\nstrength of our approach is its ability to use a set of data compression\nalgorithms and \"automatically\" choose the best one of them during the process\nof forecasting. Besides, modern data-compressors are able to find many kinds of\nlatent regularities using some methods of artificial intelligence (for example,\nsome data-compressors are based on finding the smallest formal grammar that\ndescribes the time series). Thus, our approach makes it possible to apply some\nparticular methods of artificial intelligence for time-series forecasting.\n  As examples of the application of the proposed method, we made forecasts for\nthe monthly T-index and the Kp-index time series using standard compressors. In\nboth cases, we used the Mean Absolute Error (MAE) as an accuracy measure.\n  For the monthly T-index time series, we made 18 forecasts beyond the\navailable data for each month since January 2011 to July 2017. We show that, in\ncomparison with the forecasts made by the Australian Bureau of Meteorology, our\nmethod more accurately predicts one value ahead.\n  The Kp-index time series consists of 3-hour values ranging from 0 to 9. For\neach day from February 4, 2018 to March 28, 2018, we made forecasts for 24\nvalues ahead. We compared our forecasts with the forecasts made by the Space\nWeather Prediction Center (SWPC). The results showed that the accuracy of our\nmethod is similar to the accuracy of the SWPC's method. As in the previous\ncase, we also obtained more accurate one-step forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:59:18 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chirikhin", "K. S.", ""], ["Ryabko", "B. Ya.", ""]]}, {"id": "1904.03836", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "A Fast MCMC for the Uniform Sampling of Binary Matrices with Fixed\n  Margins", "comments": null, "journal-ref": "Electronic Journal of Statistics 2020", "doi": "10.1214/20-EJS1702", "report-no": null, "categories": "stat.CO cs.DS math.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform sampling of binary matrix with fixed margins is an important and\ndifficult problem in statistics, computer science, ecology and so on. The\nwell-known swap algorithm would be inefficient when the size of the matrix\nbecomes large or when the matrix is too sparse/dense.\n  Here we propose the Rectangle Loop algorithm, a Markov chain Monte Carlo\nalgorithm to sample binary matrices with fixed margins uniformly. Theoretically\nthe Rectangle Loop algorithm is better than the swap algorithm in Peskun's\norder. Empirically studies also demonstrates the Rectangle Loop algorithm is\nremarkablely more efficient than the swap algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:41:15 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 02:22:32 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "1904.03987", "submitter": "Ivan Ramirez", "authors": "Iv\\'an Ram\\'irez Morales, Daniel Rivero Cebri\\'an, Enrique Fern\\'andez\n  Blanco, Alejandro Pazos Sierra", "title": "Early warning in egg production curves from commercial hens: A SVM\n  approach", "comments": null, "journal-ref": "Early warning in egg production curves from commercial hens: A SVM\n  approach, Computers and Electronics in Agriculture, Volume 121, 2016, Pages\n  169-179, ISSN 0168-1699, https://doi.org/10.1016/j.compag.2015.12.009", "doi": "10.1016/j.compag.2015.12.009", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence allows the improvement of our daily life, for\ninstance, speech and handwritten text recognition, real time translation and\nweather forecasting are common used applications. In the livestock sector,\nmachine learning algorithms have the potential for early detection and warning\nof problems, which represents a significant milestone in the poultry industry.\nProduction problems generate economic loss that could be avoided by acting in a\ntimely manner. In the current study, training and testing of support vector\nmachines are addressed, for an early detection of problems in the production\ncurve of commercial eggs, using farm's egg production data of 478,919 laying\nhens grouped in 24 flocks. Experiments using support vector machines with a 5\nk-fold cross-validation were performed at different previous time intervals, to\nalert with up to 5 days of forecasting interval, whether a flock will\nexperience a problem in production curve. Performance metrics such as accuracy,\nspecificity, sensitivity, and positive predictive value were evaluated,\nreaching 0-day values of 0.9874, 0.9876, 0.9783 and 0.6518 respectively on\nunseen data (test-set). The optimal forecasting interval was from zero to three\ndays, performance metrics decreases as the forecasting interval is increased.\nIt should be emphasized that this technique was able to issue an alert a day in\nadvance, achieving an accuracy of 0.9854, a specificity of 0.9865, a\nsensitivity of 0.9333 and a positive predictive value of 0.6135. This novel\napplication embedded in a computer system of poultry management is able to\nprovide significant improvements in early detection and warning of problems\nrelated to the production curve.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:16:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Morales", "Iv\u00e1n Ram\u00edrez", ""], ["Cebri\u00e1n", "Daniel Rivero", ""], ["Blanco", "Enrique Fern\u00e1ndez", ""], ["Sierra", "Alejandro Pazos", ""]]}, {"id": "1904.04018", "submitter": "Julie Dugdale", "authors": "Sabri Ghazi, Julie Dugdale, Tarek Khadir", "title": "Modelling Air Pollution Crises Using Multi-agent Simulation", "comments": null, "journal-ref": "Hawaii International Conference on System Sciences, Jan 2016,\n  Kauai, United States", "doi": null, "report-no": null, "categories": "stat.AP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an agent based approach for simulating the control of an\nair pollution crisis. A Gaussian Plum air pollution dispersion model (GPD) is\ncombined with an Artificial Neural Network (ANN) to predict the concentration\nlevels of three different air pollutants. The two models (GPM and ANN) are\nintegrated with a MAS (multi-agent system). The MAS models pollutant sources\ncontrollers and air pollution monitoring agencies as software agents. The\npopulation of agents cooperates with each other in order to reduce their\nemissions and control the air pollution. Leaks or natural sources of pollution\nare modelled as uncontrolled sources. A cooperation strategy is simulated and\nits impact on air pollution evolution is assessed and compared. The simulation\nscenario is built using data about Annaba (a city in NorthEast Algeria). The\nsimulation helps to compare and assess the efficiency of policies to control\nair pollution during crises, and takes in to account uncontrolled sources.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:48:33 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ghazi", "Sabri", ""], ["Dugdale", "Julie", ""], ["Khadir", "Tarek", ""]]}, {"id": "1904.04083", "submitter": "Eike Petersen", "authors": "Herbert Buchner and Eike Petersen and Marcus Eger and Philipp\n  Rostalski", "title": "Convolutive Blind Source Separation on Surface EMG Signals for\n  Respiratory Diagnostics and Medical Ventilation Control", "comments": null, "journal-ref": "2016 38th Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC)", "doi": "10.1109/EMBC.2016.7591513", "report-no": null, "categories": "eess.SP cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electromyogram (EMG) is an important tool for assessing the activity of a\nmuscle and thus also a valuable measure for the diagnosis and control of\nrespiratory support. In this article we propose convolutive blind source\nseparation (BSS) as an effective tool to pre-process surface electromyogram\n(sEMG) data of the human respiratory muscles. Specifically, the problem of\ndiscriminating between inspiratory, expiratory and cardiac muscle activity is\naddressed, which currently poses a major obstacle for the clinical use of sEMG\nfor adaptive ventilation control. It is shown that using the investigated\nbroadband algorithm, a clear separation of these components can be achieved.\nThe algorithm is based on a generic framework for BSS that utilizes multiple\nstatistical signal characteristics. Apart from a four-channel FIR structure,\nthere are no further restrictive assumptions on the demixing system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:08:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Buchner", "Herbert", ""], ["Petersen", "Eike", ""], ["Eger", "Marcus", ""], ["Rostalski", "Philipp", ""]]}, {"id": "1904.04091", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J. Reich, Joseph Guinness, Dipankar Bandyopadhyay,\n  Liangsuo Ma, F. Gerard Moeller", "title": "Geostatistical Modeling of Positive Definite Matrices: An Application to\n  Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13445", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical modeling for continuous point-referenced data has been\nextensively applied to neuroimaging because it produces efficient and valid\nstatistical inference. However, diffusion tensor imaging (DTI), a neuroimaging\ncharacterizing the brain structure produces a positive definite (p.d.) matrix\nfor each voxel. Current geostatistical modeling has not been extended to p.d.\nmatrices because introducing spatial dependence among positive definite\nmatrices properly is challenging. In this paper, we use the spatial Wishart\nprocess, a spatial stochastic process (random field) where each p.d.\nmatrix-variate marginally follows a Wishart distribution, and spatial\ndependence between random matrices is induced by latent Gaussian processes.\nThis process is valid on an uncountable collection of spatial locations and is\nalmost surely continuous, leading to a reasonable means of modeling spatial\ndependence. Motivated by a DTI dataset of cocaine users, we propose a spatial\nmatrix-variate regression model based on the spatial Wishart process. A\nproblematic issue is that the spatial Wishart process has no closed-form\ndensity function. Hence, we propose approximation methods to obtain a feasible\nworking model. A local likelihood approximation method is also applied to\nachieve fast computation. The simulation studies and real data analysis\ndemonstrate that the working model produces reliable inference and improved\nperformance compared to other methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:28:11 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 04:48:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J.", ""], ["Guinness", "Joseph", ""], ["Bandyopadhyay", "Dipankar", ""], ["Ma", "Liangsuo", ""], ["Moeller", "F. Gerard", ""]]}, {"id": "1904.04137", "submitter": "Hamed Sadeghi", "authors": "Mathieu Ravaut, Hamed Sadeghi, Kin Kwan Leung, Maksims Volkovs, Laura\n  C. Rosella", "title": "Diabetes Mellitus Forecasting Using Population Health Data in Ontario,\n  Canada", "comments": "18 pages, 3 figures, 8 Tables, Submitted to 2019 ML for Healthcare\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging health administrative data (HAD) datasets for predicting the risk\nof chronic diseases including diabetes has gained a lot of attention in the\nmachine learning community recently. In this paper, we use the largest health\nrecords datasets of patients in Ontario,Canada. Provided by the Institute of\nClinical Evaluative Sciences (ICES), this database is age, gender and\nethnicity-diverse. The datasets include demographics, lab measurements,drug\nbenefits, healthcare system interactions, ambulatory and hospitalizations\nrecords. We perform one of the first large-scale machine learning studies with\nthis data to study the task of predicting diabetes in a range of 1-10 years\nahead, which requires no additional screening of individuals.In the best setup,\nwe reach a test AUC of 80.3 with a single-model trained on an observation\nwindow of 5 years with a one-year buffer using all datasets. A subset of top 15\nfeatures alone (out of a total of 963) could provide a test AUC of 79.1. In\nthis paper, we provide extensive machine learning model performance and feature\ncontribution analysis, which enables us to narrow down to the most important\nfeatures useful for diabetes forecasting. Examples include chronic conditions\nsuch as asthma and hypertension, lab results, diagnostic codes in insurance\nclaims, age and geographical information.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 15:46:38 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ravaut", "Mathieu", ""], ["Sadeghi", "Hamed", ""], ["Leung", "Kin Kwan", ""], ["Volkovs", "Maksims", ""], ["Rosella", "Laura C.", ""]]}, {"id": "1904.04148", "submitter": "Weisi Guo", "authors": "Weisi Guo", "title": "Common Statistical Patterns in Urban Terrorism", "comments": null, "journal-ref": "under review, Apr 2019", "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying reasons behind modern terrorism are seemingly complex and\nintangible. Despite diverse causal mechanisms, research has shown that there\nexists general statistical patterns at the global scale that can shed light on\nhuman confrontation behaviour. Whilst many policing and counter-terrorism\noperations are conducted at a city level, there has been a lack of research in\nbuilding city-level resolution prediction engines based on statistical\npatterns. For the first time, the paper shows that there exists general\ncommonalities between global cities under terrorist attacks. By examining over\n30,000 geo-tagged terrorism acts over 7000 cities worldwide from 2002 to today,\nthe results shows the following. All cities experience attacks $A$ that are\nuncorrelated to the population and separated by a time interval $t$ that is\nnegative exponentially distributed $\\sim \\exp(-A^{-1})$, with a death-toll per\nattack that follows a power law distribution. The prediction parameters yield a\nhigh confidence of explaining up to 87\\% of the variations in frequency and\n89\\% in the death-toll data. These findings show that the aggregate statistical\nbehaviour of terror attacks are seemingly random and memoryless for all global\ncities. The enabled the author to develop a data-driven city-specific\nprediction system and we quantify its information theoretic uncertainty and\ninformation loss. Further analysis show that there appears to be an increase in\nthe uncertainty over the predictability of attacks, challenging our ability to\ndevelop effective counter-terrorism capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:00:58 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Guo", "Weisi", ""]]}, {"id": "1904.04376", "submitter": "Taufik Abrao PhD", "authors": "Victor Croisfelt Rodrigues, Jose Carlos Marinello Filho and Taufik\n  Abrao", "title": "Randomized Kaczmarz Algorithm for Massive MIMO Systems with Channel\n  Estimation and Spatial Correlation", "comments": "36 pages, 5 figures, full paper", "journal-ref": null, "doi": "10.1002/dac.4158", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To exploit the benefits of massive multiple-input multiple-output (M-MIMO)\ntechnology in scenarios where base stations (BSs) need to be cheap and equipped\nwith simple hardware, the computational complexity of classical signal\nprocessing schemes for spatial multiplexing of users shall be reduced. This\ncalls for suboptimal designs that perform well the combining/precoding steps\nand simultaneously achieve low computational complexities. An approach based on\nthe iterative Kaczmarz algorithm (KA) has been recently investigated, assuring\nwell execution without the knowledge of second order moments of the wireless\nchannels in the BS, and with easiness, since no tuning parameters, besides the\nnumber of iterations, are required. In fact, the randomized version of KA (rKA)\nhas been used in this context due to global convergence properties. Herein,\nmodifications are proposed on this first rKA-based attempt, aiming to improve\nits performance-complexity trade-off solution for M-MIMO systems. We observe\nthat long-term channel effects degrade the rate of convergence of the rKA-based\nschemes. This issue is then tackled herein by means of a hybrid rKA\ninitialization proposal that lands within the region of convexity of the\nalgorithm and assures fairness to the communication system. The effectiveness\nof our proposal is illustrated through numerical results which bring more\nrealistic system conditions in terms of channel estimation and spatial\ncorrelation than those used so far. We also characterize the computational\ncomplexity of the proposed rKA scheme, deriving upper bounds for the number of\niterations. A case study focused on a dense urban application scenario is used\nto gather new insights on the feasibility of the proposed scheme to cope with\nthe inserted BS constraints.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:53:21 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 01:41:55 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Rodrigues", "Victor Croisfelt", ""], ["Filho", "Jose Carlos Marinello", ""], ["Abrao", "Taufik", ""]]}, {"id": "1904.04459", "submitter": "Alireza Ebrahimvandi", "authors": "Alireza Ebrahimvandi, Niyousha Hosseinichimeh", "title": "System modeling of a health issue: the case of preterm birth in Ohio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm birth rate (PBR) stands out as a major public health concern in the\nU.S. However, effective policies for mitigating the problem is largely unknown.\nThe complexities of the problem raise critical questions: Why is PBR increasing\ndespite the massive investment for reducing it? What policies can decrease it?\nTo address these questions, we develop a causal loop diagram to investigate\nmechanisms underlying high preterm rate in a community. Our boundary is broad\nand includes medical and education systems, as well as living conditions such\nas crime rate and housing price. Then, we built a simulation model and divided\nthe population into two groups based on their chance of delivering a preterm\nbaby. We calibrated the model using the historical data of a case study,\nCuyahoga Ohio, from 1995 to 2017. Prior studies mostly applied reductionist\napproaches to determine factors associated with high preterm rate at the\nindividual level. Our simulation model examines the reciprocal influences of\nmultiple factors and investigates the effect of different resource allocation\nscenarios on the PBR. Results show that, in the case of Cuyahoga county with\none of the highest rates of PBR in the U.S., estimated preterm birth rates will\nnot be lower than the rates of 1995 during the next five years.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:18:05 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Ebrahimvandi", "Alireza", ""], ["Hosseinichimeh", "Niyousha", ""]]}, {"id": "1904.04488", "submitter": "Arnald Puy", "authors": "Arnald Puy, Samuele Lo Piano, Andrea Saltelli", "title": "A sensitivity analysis of the PAWN sensitivity index", "comments": "The paper has been accepted for publication in Environmental Modeling\n  and Software (use the journal version to cite the paper)", "journal-ref": "Environmental Modeling & Software 127, 104679 (2020)", "doi": "10.1016/j.envsoft.2020.104679", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The PAWN index is gaining traction among the modelling community as a\nsensitivity measure. However, the robustness to its design parameters has not\nyet been scrutinized: the size ($N$) and sampling ($\\varepsilon$) of the model\noutput, the number of conditioning intervals ($n$) or the summary statistic\n($\\theta$). Here we fill this gap by running a sensitivity analysis of a\nPAWN-based sensitivity analysis. We compare the results with the design\nuncertainties of the Sobol' total-order index ($S_{Ti}^*$). Unlike in\n$S_{Ti}^*$, the design uncertainties in PAWN create non-negligible chances of\nproducing biased results when ranking or screening inputs. The dependence of\nPAWN upon ($N,n,\\varepsilon, \\theta$) is difficult to tame, as these parameters\ninteract with one another. Even in an ideal setting in which the optimum choice\nfor ($N,n,\\varepsilon, \\theta$) is known in advance, PAWN might not allow to\ndistinguish an influential, non-additive model input from a truly\nnon-influential model input.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:50:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 09:27:08 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 17:29:08 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Puy", "Arnald", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1904.04551", "submitter": "David Frazier", "authors": "David T. Frazier and Christopher Drovandi", "title": "Robust Approximate Bayesian Inference with Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is now an established method for\nconducting approximate Bayesian inference in models where, due to the\nintractability of the likelihood function, exact Bayesian approaches are either\ninfeasible or computationally too demanding. Implicit in the application of BSL\nis the assumption that the data generating process (DGP) can produce simulated\nsummary statistics that capture the behaviour of the observed summary\nstatistics. We demonstrate that if this compatibility between the actual and\nassumed DGP is not satisfied, i.e., if the model is misspecified, BSL can yield\nunreliable parameter inference. To circumvent this issue, we propose a new BSL\napproach that can detect the presence of model misspecification, and\nsimultaneously deliver useful inferences even under significant model\nmisspecification. Two simulated and two real data examples demonstrate the\nperformance of this new approach to BSL, and document its superior accuracy\nover standard BSL when the assumed model is misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:16:22 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 05:53:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 07:23:53 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1904.04609", "submitter": "Karthik Sriram", "authors": "Karthik Sriram and Peng Shi", "title": "A new perspective from a Dirichlet model for forecasting outstanding\n  liabilities of nonlife insurers", "comments": null, "journal-ref": null, "doi": "10.1111/jori.12311", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the outstanding claim liabilities to set adequate reserves is\ncritical for a nonlife insurer's solvency. Chain-Ladder and\nBornhuetter-Ferguson are two prominent actuarial approaches used for this task.\nThe selection between the two approaches is often ad hoc due to different\nunderlying assumptions. We introduce a Dirichlet model that provides a common\nstatistical framework for the two approaches, with some appealing properties.\nDepending on the type of information available, the model inference naturally\nleads to either Chain-Ladder or Bornhuetter-Ferguson prediction. Using claims\ndata on Worker's compensation insurance from several US insurers, we discuss\nboth frequentist and Bayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 11:49:40 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Sriram", "Karthik", ""], ["Shi", "Peng", ""]]}, {"id": "1904.04636", "submitter": "Valerie Livina N.", "authors": "Valerie Livina, Adam Lewis, Martin Wickham", "title": "Tipping point analysis of electrical resistance data with early warning\n  signals of failure for predictive maintenance", "comments": "13 pages, 5 figures. Accepted for publication in Journal of\n  Electronic Testing (Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply tipping point analysis to measurements of electronic components\ncommonly used in applications in the automotive or aviation industries and\ndemonstrate early warning signals based on scaling properties of resistance\ntime series. The analysis utilises the statistical physics framework with\nstochastic modelling by representing the measured time series as a composition\nof deterministic and stochastic components estimated from measurements. The\nearly warning signals are observed much earlier than those estimated from\nconventional techniques, such as threshold-based failure detection, or bulk\nestimates used in Weibull failure analysis. The introduced techniques may be\nuseful for predictive maintenance of power electronics, with industrial\napplications. We suggest that this approach can be applied to various\nelectromagnetic measurements in power systems and energy applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:24:42 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 19:12:39 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 12:13:08 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Livina", "Valerie", ""], ["Lewis", "Adam", ""], ["Wickham", "Martin", ""]]}, {"id": "1904.04699", "submitter": "Sen Hu", "authors": "Sen Hu, T Brendan Murphy, Adrian O'Hagan", "title": "Bivariate Gamma Mixture of Experts Models for Joint Insurance Claims\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general insurance, risks from different categories are often modeled\nindependently and their sum is regarded as the total risk the insurer takes on\nin exchange for a premium. The dependence from multiple risks is generally\nneglected even when correlation could exist, for example a single car accident\nmay result in claims from multiple risk categories. It is desirable to take the\ncovariance of different categories into consideration in modeling in order to\nbetter predict future claims and hence allow greater accuracy in ratemaking. In\nthis work multivariate severity models are investigated using mixture of\nexperts models with bivariate gamma distributions, where the dependence\nstructure is modeled directly using a GLM framework, and covariates can be\nplaced in both gating and expert networks. Furthermore, parsimonious\nparameterisations are considered, which leads to a family of bivariate gamma\nmixture of experts models. It can be viewed as a model-based clustering\napproach that clusters policyholders into sub-groups with different\ndependencies, and the parameters of the mixture models are dependent on the\ncovariates. Clustering is shown to be important in separating the data into\nsub-groupings where strong dependence is often present, even if the overall\ndata set exhibits only weak dependence. In doing so, the correlation within\ndifferent components features prominently in the model. It is shown that, by\napplying to both simulated data and a real-world Irish GI insurer data set,\nclaim predictions can be improved.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:29:39 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hu", "Sen", ""], ["Murphy", "T Brendan", ""], ["O'Hagan", "Adrian", ""]]}, {"id": "1904.04793", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar, L\\'eo Neufcourt, Taps Maiti and Frederi Viens", "title": "Bayesian averaging of computer models with domain discrepancies: a\n  nuclear physics perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nucl-th physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies Bayesian model averaging (BMA) in the context of\ncompeting expensive computer models in a typical nuclear physics setup. While\nit is well known that BMA accounts for the additional uncertainty of the model\nitself, we show that it also decreases the posterior variance of the prediction\nerrors via an explicit decomposition. We extend BMA to the situation where the\ncompeting models are defined on non-identical study regions. Any model's local\nforecasting difficulty is offset by predictions obtained from the average\nmodel, thus extending individual models to the full domain. We illustrate our\nmethodology via pedagogical simulations and applications to forecasting nuclear\nobservables, which exhibit convincing improvements in both the BMA prediction\nerror and empirical coverage probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:15:35 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 18:07:48 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Neufcourt", "L\u00e9o", ""], ["Maiti", "Taps", ""], ["Viens", "Frederi", ""]]}, {"id": "1904.04901", "submitter": "Andrea Cremaschi", "authors": "Andrea Cremaschi, Arnoldo Frigessi, Kjetil Task\\'en, Manuela Zucknick", "title": "A Bayesian approach to study synergistic interaction effects in in-vitro\n  drug combination experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer translational research, increasing effort is devoted to the study\nof the combined effect of two drugs when they are administered simultaneously.\nIn this paper, we introduce a new approach to estimate the part of the effect\nof the two drugs due to the interaction of the compounds, i.e. which is due to\nsynergistic or antagonistic effects of the two drugs, compared to a reference\nvalue representing the condition when the combined compounds do not interact,\ncalled zero-interaction. We describe an in-vitro cell viability experiment as a\nrandom experiment, by interpreting cell viability as the probability of a cell\nin the experiment to be viable after treatment, and including information\nrelated to different exposure conditions. We propose a flexible Bayesian spline\nregression framework for modelling the viability surface of two drugs combined\nas a function of the concentrations. Since the proposed approach is based on a\nstatistical model, it allows to include replicates of the experiments, to\nevaluate the uncertainty of the estimates, and to perform prediction. We test\nthe model fit and prediction performance on a simulation study, and on an\novarian cancer cell dataset. Posterior estimates of the zero-interaction level\nand of the synergy term, obtained via adaptive MCMC algorithms, are used to\ncompute interpretable measures of efficacy of the combined experiment,\nincluding relative volume under the surface (rVUS) measures to summarise the\nzero-interaction and synergy terms and a bi-variate alternative to the\nwell-known EC50 measure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:22:48 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 01:40:39 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cremaschi", "Andrea", ""], ["Frigessi", "Arnoldo", ""], ["Task\u00e9n", "Kjetil", ""], ["Zucknick", "Manuela", ""]]}, {"id": "1904.05026", "submitter": "Sabine Plancoulaine", "authors": "Sabine Plancoulaine, Camille Stagnara, Sophie Flori, Flora\n  Bat-Pitault, Jian-Sheng Lin, Hugues Patural (SNA-EPIS), Patricia Franco\n  (CRNL)", "title": "Early features associated with the neurocognitive development at 36\n  months of age: the AuBE study", "comments": null, "journal-ref": "Sleep Medicine, Elsevier, 2017, 30, pp.222-228", "doi": "10.1016/j.sleep.2016.10.015", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Few studies on the relations between sleep quantity and/or\nquality and cognition were conducted among pre-schoolers from healthy general\npopulation. We aimed at identifying, among 3 years old children, early factors\nassociated with intelligence quotient estimated through Weschler Preschool and\nPrimary Scale Intelligence-III test and its indicators: full-scale-, verbal-\nand performance-intelligence quotients and their sub-scale scores. Methods. We\nincluded 194 children from the French birth-cohort AuBE with both available\nWeschler Preschool and Primary Scale Intelligence-III scores at 3y and sleep\ndata. Information was collected through self-questionnaires at birth, 6, 12, 18\nand 24 months. A day/night sleep ratio was calculated. Results. Mean scores\nwere in normal ranges for verbal-, performance- and full-scale-intelligence\nquotients. In multivariate models, being a $\\ge$3 born-child and watching\ntelevision $\\ge$1 hour/day at 24 months were negatively associated with all\nintelligence quotient scores while collective care arrangement was positively\nassociated. Night waking at 6 and frequent snoring at 18 months were negatively\nassociated with performance intelligence quotient, some subscales and\nfull-scale-intelligence quotient contrary to day/night sleep ratio at 12\nmonths. No association was observed between early sleep characteristics and\nverbal intelligence quotient. Conclusion. We showed that early features\nincluding infant sleep characteristics influence intelligence quotient scores\nat 3 years old. Some of these may be accessible to prevention.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:01:47 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Plancoulaine", "Sabine", "", "SNA-EPIS"], ["Stagnara", "Camille", "", "SNA-EPIS"], ["Flori", "Sophie", "", "SNA-EPIS"], ["Bat-Pitault", "Flora", "", "SNA-EPIS"], ["Lin", "Jian-Sheng", "", "SNA-EPIS"], ["Patural", "Hugues", "", "SNA-EPIS"], ["Franco", "Patricia", "", "CRNL"]]}, {"id": "1904.05042", "submitter": "Sabine Plancoulaine", "authors": "Sabine Plancoulaine (INSERM, CRESS - U1153), Eve Reynaud (CRESS -\n  U1153, INSERM), Anne Forhan (CRESS - U1153, INSERM), Sandrine Lioret (CRESS -\n  U1153, INSERM), Barbara Heude (CRESS - U1153, INSERM), Marie-Aline Charles\n  (CRESS - U1153, INSERM)", "title": "Night sleep duration trajectories and associated factors among preschool\n  children from the EDEN cohort", "comments": null, "journal-ref": "Sleep Medicine, Elsevier, 2018, 48, pp.194-201", "doi": "10.1016/j.sleep.2018.03.030", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Sleep duration may vary inter-individually and intra-individually\nover time. We aimed at both identifying night-sleep duration (NSD) trajectories\namong preschoolers and studying associated factors. Methods. NSD were collected\nwithin the French birth-cohort study EDEN at ages 2, 3 and 5-6 years through\nparental questionnaires, and were used to model NSD trajectories among 1205\nchildren. Familial socioeconomic factors, maternal sociodemographic, health and\nlifestyle characteristics as well as child health, lifestyle, and sleep\ncharacteristics at birth and/or at age 2 years were investigated in association\nwith NSD using multinomial logistic regressions. Results. Five distinct NSD\ntrajectories were identified: short (SS, <10h, 4.9%), medium-low (MLS, <11h,\n47.8%), medium-high (MHS, $\\approx$11h30, 37.2%), long (LS, $\\ge$11h30, 4.5%)\nand changing (CS, i.e. $\\ge$11h30 then <11h, 5.6%) NSD trajectories.\nMultivariable analyses showed in particular that, compared to the MHS\ntrajectory, factors associated with increased risk for belonging to SS\ntrajectory were male gender, first child, maternal age and working status,\nnight-waking, parental presence when falling asleep, television-viewing\nduration and both the `Processed and fast foods' and the `Baby food' dietary\npatterns at age 2 years. Factors positively associated with the CS trajectory\nwere maternal smoking, bottle-feeding at night and the `Processed and fast\nfoods' dietary pattern at age 2 years whereas child's activity and emotionality\nscores at age 1 year were negatively associated. Conclusion. We identified\ndistinct NSD trajectories among preschoolers and associated early life factors.\nSome of them may reflect less healthy lifestyle, providing cues for early\nmulti-behavioral prevention interventions\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:56:13 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Plancoulaine", "Sabine", "", "INSERM, CRESS - U1153"], ["Reynaud", "Eve", "", "CRESS -\n  U1153, INSERM"], ["Forhan", "Anne", "", "CRESS - U1153, INSERM"], ["Lioret", "Sandrine", "", "CRESS -\n  U1153, INSERM"], ["Heude", "Barbara", "", "CRESS - U1153, INSERM"], ["Charles", "Marie-Aline", "", "CRESS - U1153, INSERM"]]}, {"id": "1904.05043", "submitter": "Sabine Plancoulaine", "authors": "Eve Reynaud (INSERM, CRESS - U1153), Anne Forhan (INSERM, CRESS -\n  U1153), Barbara Heude (INSERM, CRESS - U1153), Marie-Aline Charles (INSERM,\n  CRESS - U1153), Sabine Plancoulaine (INSERM, CRESS - U1153)", "title": "Association of night-waking and inattention/hyperactivity symptoms\n  trajectories in preschool-aged children", "comments": null, "journal-ref": "Scientific Reports, Nature Publishing Group, 2018, 8 (1)", "doi": "10.1038/s41598-018-33811-2", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To study the longitudinal associations between\ninattention/hyperactivity symptoms and night-waking in preschool-years, in\nlight of their joint evolution.Study design: Within the French birth-cohort\nstudy EDEN, repeated measures of 1342 children's night-waking and\ninattention/hyperactivity symptoms were collected at age 2, 3 and 5-6 through\nquestionnaires. Trajectories were computed using group-based modeling. Logistic\nregressions, adjusted for confounding factors, were used to measure the\nassociation between trajectories and to determine risk factors for belonging to\nthe identified joint trajectories.Results: Two night-waking trajectories were\nobserved, 20% of the children had a trajectory of `common night-waking', and\n80% a trajectory of `rare night-waking'. The children were distributed in three\ninattention/hyperactivity trajectories, a low (47%), medium (40%) and high one\n(13%). Both night-waking and inattention/hyperactivity trajectories showed\npersistence of difficulties in preschool years. The risk of presenting a high\ninattention/hyperactivity trajectory compared to a low one was of\n4.19[2.68-6.53] for common night-wakers, compared to rare night-wakers. Factors\nassociated with joint trajectories were parent's education level and history of\nchildhood behavioral problems, and the child's gender, night-sleep duration and\ncollective care at 2 years of age.Conclusion: Results suggest that children\npresenting behavioral difficulties would benefit from a systematic\ninvestigation of their sleep quality and conversely.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:00:44 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Reynaud", "Eve", "", "INSERM, CRESS - U1153"], ["Forhan", "Anne", "", "INSERM, CRESS -\n  U1153"], ["Heude", "Barbara", "", "INSERM, CRESS - U1153"], ["Charles", "Marie-Aline", "", "INSERM,\n  CRESS - U1153"], ["Plancoulaine", "Sabine", "", "INSERM, CRESS - U1153"]]}, {"id": "1904.05062", "submitter": "Carla Rampichini", "authors": "Leonardo Grilli, Maria Francesca Marino, Omar Paccagnella, Carla\n  Rampichini", "title": "Multiple imputation and selection of ordinal level 2 predictors in\n  multilevel models. An analysis of the relationship between student ratings\n  and teacher beliefs and practices", "comments": "Presented at the 12th International Multilevel Conference is held\n  April 9-10, 2019 , Utrecht", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is motivated by the analysis of the relationship between ratings\nand teacher practices and beliefs, which are measured via a set of binary and\nordinal items collected by a specific survey with nearly half missing\nrespondents. The analysis, which is based on a two-level random effect model,\nmust face two about the items measuring teacher practices and beliefs: (i)\nthese items level 2 predictors severely affected by missingness; (ii) there is\nredundancy in the number of items and the number of categories of their\nmeasurement scale. tackle the first issue by considering a multiple imputation\nstrategy based on information at both level 1 and level 2. For the second\nissue, we consider regularization techniques for ordinal predictors, also\naccounting for the multilevel data structure. The proposed solution combines\nexisting methods in an original way to solve specific problem at hand, but it\nis generally applicable to settings requiring to select predictors affected by\nmissing values. The results obtained with the final model out that some teacher\npractices and beliefs are significantly related to ratings about teacher\nability to motivate students.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:37:33 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 22:59:46 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Grilli", "Leonardo", ""], ["Marino", "Maria Francesca", ""], ["Paccagnella", "Omar", ""], ["Rampichini", "Carla", ""]]}, {"id": "1904.05070", "submitter": "Sabine Plancoulaine", "authors": "Chu Yan Yong (INSERM, CRESS - U1153), Eve Reynaud (INSERM, CRESS -\n  U1153), Anne Forhan (INSERM, CRESS - U1153), Partricia Dargent-Molina\n  (INSERM, CRESS - U1153), Barbara Heude (INSERM, CRESS - U1153), Marie-Aline\n  Charles (INSERM, CRESS - U1153), Sabine Plancoulaine (INSERM, CRESS - U1153)", "title": "Cord-blood vitamin D level and night sleep duration in preschoolers in\n  the EDEN mother-child birth cohort", "comments": null, "journal-ref": "Sleep Medicine, Elsevier, 2019, 53, pp.70-74", "doi": "10.1016/j.sleep.2018.09.017", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: 25-hydroxyvitamin D (25OHD) deficiency has been associated with\nsleep disorders in adults. Only three cross-sectional studies were performed in\nchildren and showed an association between 25OHD deficiency and both\nobstructive sleep apnea syndrome and primary snoring. No longitudinal study has\nbeen performed in children from the general population. We analyzed the\nassociation between cord-blood vitamin D level at birth and night-sleep\nduration trajectories for children between 2 and 5-6 years old in a\nnon-clinical cohort.Method: We included 264 children from the French EDEN\nmother-child birth-cohort with both cord-blood 25OHD level determined by\nradio-immunoassay at birth, and night-sleep trajectories for children between 2\nand 5-6 years old obtained by the group-based trajectory modeling method.\nAssociations between 25OHD and sleep trajectories were assessed by multinomial\nlogistic regression adjusted for maternal and child characteristics.Results:\nThe trajectories short sleep (<10h30/night), medium-low sleep\n(10h30-11h00/night), medium-high sleep ($\\approx$11h30/night), long sleep\n($\\ge$11h30/night) and changing sleep (decreased from $\\ge$11h30 to\n10h30-11h00/night) represented 5%, 46%, 37%, 4% and 8% of the children,\nrespectively. The mean 25OHD level was 19 ng/ml (SD=11, range 3 to 63). It was\n12 (SD=7), 20 (SD=11), 19 (SD=10), 14 (SD=7) and 16 (SD=8) ng/ml for children\nwith short, medium-low, medium-high, long and changing sleep trajectories,\nrespectively. On adjusted analysis, for each 1-ng/ml decrease in 25OHD level,\nthe odds of belonging to the short sleep versus medium-high sleep trajectory\nwas increased (odds ratio =1.12, 95% confidence interval [1.01-1.25]). We found\nno other significant association between 25OHD level and other\ntrajectories.Conclusion: Low 25OHD level at birth may be associated with\nincreased probability of being a persistent short sleeper in preschool years.\nThese results need confirmation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:56:50 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Yong", "Chu Yan", "", "INSERM, CRESS - U1153"], ["Reynaud", "Eve", "", "INSERM, CRESS -\n  U1153"], ["Forhan", "Anne", "", "INSERM, CRESS - U1153"], ["Dargent-Molina", "Partricia", "", "INSERM, CRESS - U1153"], ["Heude", "Barbara", "", "INSERM, CRESS - U1153"], ["Charles", "Marie-Aline", "", "INSERM, CRESS - U1153"], ["Plancoulaine", "Sabine", "", "INSERM, CRESS - U1153"]]}, {"id": "1904.05270", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "Kinga Kita, {\\L}ukasz Kidzi\\'nski", "title": "Google Street View image of a house predicts car accident risk of its\n  resident", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road traffic injuries are a leading cause of death worldwide. Proper\nestimation of car accident risk is critical for appropriate allocation of\nresources in healthcare, insurance, civil engineering, and other industries. We\nshow how images of houses are predictive of car accidents. We analyze 20,000\naddresses of insurance company clients, collect a corresponding house image\nusing Google Street View, and annotate house features such as age, type, and\ncondition. We find that this information substantially improves car accident\nrisk prediction compared to the state-of-the-art risk model of the insurance\ncompany and could be used for price discrimination. From this perspective,\npublic availability of house images raises legal and social concerns, as they\ncan be a proxy of ethnicity, religion and other sensitive data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:16:31 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kita", "Kinga", ""], ["Kidzi\u0144ski", "\u0141ukasz", ""]]}, {"id": "1904.05305", "submitter": "Giuseppe Pernagallo Dr.", "authors": "Davide Bennato, Giuseppe Pernagallo and Benedetto Torrisi", "title": "A Classification Algorithm to Recognize Fake News Websites", "comments": "11 pages, 3 figures, 4 tables", "journal-ref": "International Conference on Data Science and Social Research, 2020", "doi": "10.1007/978-3-030-51222-4_25", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Fake news' is information that generally spreads on the web, which only\nmimics the form of reliable news media content. The phenomenon has assumed\nuncontrolled proportions in recent years rising the concern of authorities and\ncitizens. In this paper we present a classifier able to distinguish a reliable\nsource from a fake news website. We have prepared a dataset made of 200 fake\nnews websites and 200 reliable websites from all over the world and used as\npredictors information potentially available on websites, such as the presence\nof a 'contact us' section or a secured connection. The algorithm is based on\nlogistic regression, whereas further analyses were carried out using\ntetrachoric correlation coefficients for dichotomous variables and chi-square\ntests. This framework offers a concrete solution to attribute a 'reliability\nscore' to news website, defined as the probability that a source is reliable or\nnot, and on this probability a user can decide if the news is worth sharing or\nnot.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:14:22 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Bennato", "Davide", ""], ["Pernagallo", "Giuseppe", ""], ["Torrisi", "Benedetto", ""]]}, {"id": "1904.05312", "submitter": "Angelos Alexopoulos Dr", "authors": "Angelos Alexopoulos, Petros Dellaportas, Omiros Papaspiliopoulos", "title": "Bayesian prediction of jumps in large panels of time series data", "comments": "49 pages, 27 figures, 4 tables", "journal-ref": null, "doi": "10.1214/21-BA1268", "report-no": null, "categories": "q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a new look at the problem of disentangling the volatility and jumps\nprocesses of daily stock returns. We first provide a computational framework\nfor the univariate stochastic volatility model with Poisson-driven jumps that\noffers a competitive inference alternative to the existing tools. This\nmethodology is then extended to a large set of stocks for which we assume that\ntheir unobserved jump intensities co-evolve in time through a dynamic factor\nmodel. To evaluate the proposed modelling approach we conduct out-of-sample\nforecasts and we compare the posterior predictive distributions obtained from\nthe different models. We provide evidence that joint modelling of jumps\nimproves the predictive ability of the stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:59:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:57:45 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 15:24:56 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 11:29:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Dellaportas", "Petros", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1904.05313", "submitter": "Shanshan Chen", "authors": "Shanshan Chen, Robert Perera, Matthew M. Engelhard, Jessica R.\n  Lunsford-Avery, Scott H. Kollins, Bernard F. Fuemmeler", "title": "A Generic Algorithm for Sleep-Wake Cycle Detection using Unlabeled\n  Actigraphy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key component when analyzing actigraphy data for sleep studies is\nsleep-wake cycle detection. Most detection algorithms rely on accurate sleep\ndiary labels to generate supervised classifiers, with parameters optimized for\na particular dataset. However, once the actigraphy trackers are deployed in the\nfield, labels for training models and validating detection accuracy are often\nnot available.\n  In this paper, we propose a generic, training-free algorithm to detect\nsleep-wake cycles from minute-by-minute actigraphy. Leveraging a robust\nnonlinear parametric model, our proposed method refines the detection region by\nsearching for a single change point within bounded regions defined by the\nparametric model. Challenged by the absence of ground truth labels, we also\npropose an evaluation metric dedicated to this problem. Tested on week-long\nactigraphy from 112 children, the results show that the proposed algorithm\nimproves on the baseline model consistently and significantly (p<3e-15).\nMoreover, focusing on the commonality in human circadian rhythm captured by\nactigraphy, the proposed method is generic to data collected by various\nactigraphy trackers, circumventing the laborious label collection step in\ndeveloping customized classifiers for sleep detection.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 02:24:12 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Shanshan", ""], ["Perera", "Robert", ""], ["Engelhard", "Matthew M.", ""], ["Lunsford-Avery", "Jessica R.", ""], ["Kollins", "Scott H.", ""], ["Fuemmeler", "Bernard F.", ""]]}, {"id": "1904.05327", "submitter": "Laurence Brandenberger", "authors": "Philip Leifeld and Laurence Brandenberger", "title": "Endogenous Coalition Formation in Policy Debates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political actors form coalitions around their joint policy beliefs in order\nto influence the policy process on contentious issues such as climate change or\npopulation aging. The present article explains the formation and maintenance of\ncoalitions by focusing on the ways that actors adopt policy beliefs from other\nactors. A policy debate is a complex system that exhibits network dependencies\nboth in cross-sectional and longitudinal ways when actors contribute\nideological statements to the debate. In such a temporal network, learning of\npolicy beliefs matters in three complementary ways: positive reciprocity\nthrough bonding relationships within coalitions, innovation across coalitions\nthrough bridging relationships, and negative reciprocity through repulsion, or\npolarization, of adversarial coalitions by reinforcement of conflictual\nrelationships. We test this theory of endogenous coalition formation in policy\ndebates using a novel inferential technique combining network and event history\nanalysis and find systematic evidence for the interplay of the three coalition\nformation mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 13:03:14 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Leifeld", "Philip", ""], ["Brandenberger", "Laurence", ""]]}, {"id": "1904.05333", "submitter": "Francesco Sanna Passino", "authors": "Francesco Sanna Passino and Nicholas A. Heard", "title": "Bayesian estimation of the latent dimension and communities in\n  stochastic blockmodels", "comments": null, "journal-ref": "Statistics and Computing 30(5), 1291-1307 (2020)", "doi": "10.1007/s11222-020-09946-6", "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding of adjacency or Laplacian matrices of undirected graphs is\na common technique for representing a network in a lower dimensional latent\nspace, with optimal theoretical guarantees. The embedding can be used to\nestimate the community structure of the network, with strong consistency\nresults in the stochastic blockmodel framework. One of the main practical\nlimitations of standard algorithms for community detection from spectral\nembeddings is that the number of communities and the latent dimension of the\nembedding must be specified in advance. In this article, a novel Bayesian model\nfor simultaneous and automatic selection of the appropriate dimension of the\nlatent space and the number of blocks is proposed. Extensions to directed and\nbipartite graphs are discussed. The model is tested on simulated and real world\nnetwork data, showing promising performance for recovering latent community\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 08:22:15 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 09:55:51 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 08:54:28 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Passino", "Francesco Sanna", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "1904.05375", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Greg Ver Steeg, Chantal M. W. Tax, Paul M. Thompson", "title": "Scanner Invariant Representations for Diffusion MRI Harmonization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In the present work we describe the correction of diffusion-weighted\nMRI for site and scanner biases using a novel method based on invariant\nrepresentation.\n  Theory and Methods: Pooled imaging data from multiple sources are subject to\nvariation between the sources. Correcting for these biases has become very\nimportant as imaging studies increase in size and multi-site cases become more\ncommon. We propose learning an intermediate representation invariant to\nsite/protocol variables, a technique adapted from information theory-based\nalgorithmic fairness; by leveraging the data processing inequality, such a\nrepresentation can then be used to create an image reconstruction that is\nuninformative of its original source, yet still faithful to underlying\nstructures. To implement this, we use a deep learning method based on\nvariational auto-encoders (VAE) to construct scanner invariant encodings of the\nimaging data.\n  Results: To evaluate our method, we use training data from the 2018 MICCAI\nComputational Diffusion MRI (CDMRI) Challenge Harmonization dataset. Our\nproposed method shows improvements on independent test data relative to a\nrecently published baseline method on each subtask, mapping data from three\ndifferent scanning contexts to and from one separate target scanning context.\n  Conclusion: As imaging studies continue to grow, the use of pooled multi-site\nimaging will similarly increase. Invariant representation presents a strong\ncandidate for the harmonization of these data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:10:19 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 19:11:39 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Moyer", "Daniel", ""], ["Steeg", "Greg Ver", ""], ["Tax", "Chantal M. W.", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1904.05636", "submitter": "Julie Rendlov\\'a", "authors": "Julie Rendlov\\'a, Karel Hron, Kamila Fa\\v{c}evicov\\'a and Peter\n  Filzmoser", "title": "Robust Principal Component Analysis for Compositional Tables", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data table which is arranged according to two factors can often be\nconsidered as a compositional table. An example is the number of unemployed\npeople, split according to gender and age classes. Analyzed as compositions,\nthe relevant information would consist of ratios between different cells of\nsuch a table. This is particularly useful when analyzing several compositional\ntables jointly, where the absolute numbers are in very different ranges, e.g.\nif unemployment data are considered from different countries. Within the\nframework of the logratio methodology, compositional tables can be decomposed\ninto independent and interactive parts, and orthonormal coordinates can be\nassigned to these parts. However, these coordinates usually require some prior\nknowledge about the data, and they are not easy to handle for exploring the\nrelationships between the given factors.\n  Here we propose a special choice of coordinates with a direct relation to\ncentered logratio (clr) coefficients, which are particularly useful for an\ninterpretation in terms of the original cells of the tables. With these\ncoordinates, robust principal component analysis (PCA) is performed for\ndimension reduction, allowing to investigate the relationships between the\nfactors. The link between orthonormal coordinates and clr coefficients enables\nto apply robust PCA, which would otherwise suffer from the singularity of clr\ncoefficients.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:30:06 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Rendlov\u00e1", "Julie", ""], ["Hron", "Karel", ""], ["Fa\u010devicov\u00e1", "Kamila", ""], ["Filzmoser", "Peter", ""]]}, {"id": "1904.06007", "submitter": "Seyed Soheil Hosseini", "authors": "Seyed Soheil Hosseini, Nick Wormald, and Tianhai Tian", "title": "A Weight-based Information Filtration Algorithm for Stock-Correlation\n  Networks", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms have been proposed to filter information on a complete\ngraph of correlations across stocks to build a stock-correlation network. Among\nthem the planar maximally filtered graph (PMFG) algorithm uses $3n-6$ edges to\nbuild a graph whose features include a high frequency of small cliques and a\ngood clustering of stocks. We propose a new algorithm which we call\nproportional degree (PD) to filter information on the complete graph of\nnormalised mutual information (NMI) across stocks. Our results show that the PD\nalgorithm produces a network showing better homogeneity with respect to\ncliques, as compared to economic sectoral classification than its PMFG\ncounterpart. We also show that the partition of the PD network obtained through\nnormalised spectral clustering (NSC) agrees better with the NSC of the complete\ngraph than the corresponding one obtained from PMFG. Finally, we show that the\nclusters in the PD network are more robust with respect to the removal of\nrandom sets of edges than those in the PMFG network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 02:16:45 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Hosseini", "Seyed Soheil", ""], ["Wormald", "Nick", ""], ["Tian", "Tianhai", ""]]}, {"id": "1904.06048", "submitter": "Jun-Ichi Takeshita", "authors": "Jun-ichi Takeshita, Yuto Arai, Mayu Ogawa, Xiao-Nan Lu, and Tomomichi\n  Suzuki", "title": "New statistic for detecting laboratory effects in ORDANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study defines a new statistic for detecting laboratory effects in\nthe analysis of ordinal variation (ORDANOVA). The ORDANOVA is an analysis\nmethod similar to one-way analysis of variance for analysing ordinal data\nobtained from interlaboratory comparison studies. In this paper, we present an\napproximate continuous distribution for the new statistic for the case of an\narbitrary number of ordinal levels, and we demonstrate that $alpha$-percentiles\nof the distribution are suitable criteria for conducting statistical tests. In\naddition, a real example involving data from an interlaboratory comparison\nstudy is analysed using the proposed statistic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:08:11 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 13:43:27 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Takeshita", "Jun-ichi", ""], ["Arai", "Yuto", ""], ["Ogawa", "Mayu", ""], ["Lu", "Xiao-Nan", ""], ["Suzuki", "Tomomichi", ""]]}, {"id": "1904.06127", "submitter": "Alastair Gregory", "authors": "Alastair Gregory, Din-Houn Lau, Alex Tessier and Pan Zhang", "title": "A streaming feature-based compression method for data from instrumented\n  infrastructure", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of civil engineering applications are utilising data\nacquired from infrastructure instrumented with sensing devices. This data has\nan important role in monitoring the response of these structures to excitation,\nand evaluating structural health. In this paper we seek to monitor\npedestrian-events (such as a person walking) on a footbridge using strain and\nacceleration data. The rate of this data acquisition and the number of sensing\ndevices make the storage and analysis of this data a computational challenge.\nWe introduce a streaming method to compress the sensor data, whilst preserving\nkey patterns and features (unique to different sensor types) corresponding to\npedestrian-events. Numerical demonstrations of the methodology on data obtained\nfrom strain sensors and accelerometers on the pedestrian footbridge are\nprovided to show the trade-off between compression and accuracy during and\nin-between periods of pedestrian-events.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:36:40 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gregory", "Alastair", ""], ["Lau", "Din-Houn", ""], ["Tessier", "Alex", ""], ["Zhang", "Pan", ""]]}, {"id": "1904.06136", "submitter": "Andrea Cappozzo", "authors": "Andrea Cappozzo, Francesca Greselin, Thomas Brendan Murphy", "title": "A robust approach to model-based classification based on trimming and\n  constraints", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-019-00371-w", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard classification framework a set of trustworthy learning data are\nemployed to build a decision rule, with the final aim of classifying unlabelled\nunits belonging to the test set. Therefore, unreliable labelled observations,\nnamely outliers and data with incorrect labels, can strongly undermine the\nclassifier performance, especially if the training size is small. The present\nwork introduces a robust modification to the Model-Based Classification\nframework, employing impartial trimming and constraints on the ratio between\nthe maximum and the minimum eigenvalue of the group scatter matrices. The\nproposed method effectively handles noise presence in both response and\nexploratory variables, providing reliable classification even when dealing with\ncontaminated datasets. A robust information criterion is proposed for model\nselection. Experiments on real and simulated data, artificially adulterated,\nare provided to underline the benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:50:29 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 07:33:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Cappozzo", "Andrea", ""], ["Greselin", "Francesca", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1904.06384", "submitter": "Michael Levine", "authors": "Jiexin Duan, Michael Levine, Junxiang Luo, Yongming Qu", "title": "Estimation of group means in generalized linear mixed models", "comments": "26 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we investigate the concept of the mean response for a\ntreatment group mean as well as its estimation and prediction for generalized\nlinear models with a subject-wise random effect. Generalized linear models are\ncommonly used to analyze categorical data. The model-based mean for a treatment\ngroup usually estimates the response at the mean covariate. However, the mean\nresponse for the treatment group for studied population is at least equally\nimportant in the context of clinical trials. New methods were proposed to\nestimate such a mean response in generalized linear models; however, this has\nonly been done when there are no random effects in the model. We suggest that,\nin a generalized linear mixed model (GLMM), there are at least two possible\ndefinitions of a treatment group mean response that can serve as\nestimation/prediction targets. The estimation of these treatment group means is\nimportant for healthcare professionals to be able to understand the absolute\nbenefit versus risk. For both of these treatment group means, we propose a new\nset of methods that suggests how to estimate/predict both of them in a GLMM\nmodels with a univariate subject-wise random effect. Our methods also suggest\nan easy way of constructing corresponding confidence and prediction intervals\nfor both possible treatment group means. Simulations show that proposed\nconfidence and prediction intervals provide correct empirical coverage\nprobability under most circumstances. Proposed methods have also been applied\nto analyze hypoglycemia data from diabetes clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 19:21:30 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 20:27:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Duan", "Jiexin", ""], ["Levine", "Michael", ""], ["Luo", "Junxiang", ""], ["Qu", "Yongming", ""]]}, {"id": "1904.06819", "submitter": "Robert Foster", "authors": "Robert C. Foster, Brian Weaver, James Gattiker", "title": "Applications of Quantum Annealing in Statistics", "comments": "Corrected a few typos, added a few references, clarified some points,\n  added information about potential polynomial speedup of quantum annealing and\n  effect of difference between ground and excited energy states", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation offers exciting new possibilities for statistics. This\npaper explores the use of the D-Wave machine, a specialized type of quantum\ncomputer, which performs quantum annealing. A general description of quantum\nannealing through the use of the D-Wave is given, along with technical issues\nto be encountered. Quantum annealing is used to perform maximum likelihood\nestimation, generate an experimental design, and perform matrix inversion.\nThough the results show that quantum computing is still at an early stage which\nis not yet superior to classical computation, there is promise for quantum\ncomputation in the future.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:59:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 17:49:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Foster", "Robert C.", ""], ["Weaver", "Brian", ""], ["Gattiker", "James", ""]]}, {"id": "1904.06941", "submitter": "Lewis Mitchell", "authors": "Vanessa Glenny, Jonathan Tuke, Nigel Bean, Lewis Mitchell", "title": "A framework for streamlined statistical prediction using topic models", "comments": "Proceedings of the 2019 Joint SIGHUM Workshop on Computational\n  Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature\n  (LaTeCH-CLfL `19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Humanities and Social Sciences, there is increasing interest in\napproaches to information extraction, prediction, intelligent linkage, and\ndimension reduction applicable to large text corpora. With approaches in these\nfields being grounded in traditional statistical techniques, the need arises\nfor frameworks whereby advanced NLP techniques such as topic modelling may be\nincorporated within classical methodologies. This paper provides a classical,\nsupervised, statistical learning framework for prediction from text, using\ntopic models as a data reduction method and the topics themselves as\npredictors, alongside typical statistical tools for predictive modelling. We\napply this framework in a Social Sciences context (applied animal behaviour) as\nwell as a Humanities context (narrative analysis) as examples of this\nframework. The results show that topic regression models perform comparably to\ntheir much less efficient equivalents that use individual words as predictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:06:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Glenny", "Vanessa", ""], ["Tuke", "Jonathan", ""], ["Bean", "Nigel", ""], ["Mitchell", "Lewis", ""]]}, {"id": "1904.07192", "submitter": "Maurice Schmeits", "authors": "Kilian Bakker, Kirien Whan, Wouter Knap and Maurice Schmeits", "title": "Comparison of statistical post-processing methods for probabilistic NWP\n  forecasts of solar radiation", "comments": "https://doi.org/10.1016/j.solener.2019.08.044", "journal-ref": "Solar Energy, volume 191, 2019, pages 138-150", "doi": "10.1016/j.solener.2019.08.044", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased usage of solar energy places additional importance on forecasts\nof solar radiation. Solar panel power production is primarily driven by the\namount of solar radiation and it is therefore important to have accurate\nforecasts of solar radiation. Accurate forecasts that also give information on\nthe forecast uncertainties can help users of solar energy to make better solar\nradiation based decisions related to the stability of the electrical grid. To\nachieve this, we apply statistical post-processing techniques that determine\nrelationships between observations of global radiation (made within the KNMI\nnetwork of automatic weather stations in the Netherlands) and forecasts of\nvarious meteorological variables from the numerical weather prediction (NWP)\nmodel HARMONIE-AROME (HA) and the atmospheric composition model CAMS. Those\nrelationships are used to produce probabilistic forecasts of global radiation.\nWe compare 7 different statistical post-processing methods, consisting of two\nparametric and five non-parametric methods. We find that all methods are able\nto generate probabilistic forecasts that improve the raw global radiation\nforecast from HA according to the root mean squared error (on the median) and\nthe potential economic value. Additionally, we show how important the\npredictors are in the different regression methods. We also compare the\nregression methods using various probabilistic scoring metrics, namely the\ncontinuous ranked probability skill score, the Brier skill score and\nreliability diagrams. We find that quantile regression and generalized random\nforests generally perform best. In (near) clear sky conditions the\nnon-parametric methods have more skill than the parametric ones.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:08:58 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 10:29:50 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bakker", "Kilian", ""], ["Whan", "Kirien", ""], ["Knap", "Wouter", ""], ["Schmeits", "Maurice", ""]]}, {"id": "1904.07208", "submitter": "Jessica Young PhD", "authors": "Jessica G. Young, Sarah J. Willis, Katherine Hsu, Michael Klompas,\n  Julia L. Marcus", "title": "More testing or more disease? A counterfactual approach to explaining\n  observed increases in positive tests over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observed gonorrhea case rates (number of positive tests per 100,000\nindividuals) increased by 75 percent in the United States between 2009 and\n2017, predominantly among men. However, testing recommendations by the Centers\nfor Disease Control and Prevention (CDC) have also changed over this period\nwith more frequent screening for sexually transmitted infections (STIs)\nrecommended among men who have sex with men (MSM) who are sexually active. In\nthis and similar disease surveillance settings, a common question is whether\nobserved increases in the overall proportion of positive tests over time is due\nonly to increased testing of diseased individuals, increased underlying disease\nor both. By placing this problem within a counterfactual framework, we can\ncarefully consider untestable assumptions under which this question may be\nanswered and, in turn, a principled approach to statistical analysis. This\nreport outlines this thought process.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:26:58 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Young", "Jessica G.", ""], ["Willis", "Sarah J.", ""], ["Hsu", "Katherine", ""], ["Klompas", "Michael", ""], ["Marcus", "Julia L.", ""]]}, {"id": "1904.07231", "submitter": "Jean Faber", "authors": "Jean Faber, Priscila C. Antoneli, Guillem Via, Noemi S. Ara\\'ujo,\n  Daniel J. L. L. Pinheiro, Esper Cavalheiro", "title": "Critical elements for connectivity analysis of brain networks", "comments": "39 pages, 14 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, new and important perspectives were introduced in the field\nof neuroimaging with the emergence of the connectionist approach. In this new\ncontext, it is important to know not only which brain areas are activated by a\nparticular stimulus but, mainly, how these areas are structurally and\nfunctionally connected, distributed, and organized in relation to other areas.\nAdditionally, the arrangement of the network elements, i.e., its topology, and\nthe dynamics they give rise to are also important. This new approach is called\nconnectomics. It brings together a series of techniques and methodologies\ncapable of systematizing, from the different types of signals and images of the\nnervous system, how neuronal units to brain areas are connected. Through this\napproach, the different patterns of connectivity can be graphically and\nmathematically represented by the so-called connectomes. The connectome uses\nquantitative metrics to evaluate structural and functional information from\nimages of neural tracts and pathways or signals from the metabolic and/or\nelectrophysiologic activity of cell populations or brain areas. Besides, with\nadequate treatment of this information, it is also possible to infer causal\nrelationships. In this way, structural and functional evaluations are\ncomplementary descriptions which, together, represent the anatomic and\nphysiologic neural properties, establishing a new paradigm for understanding\nhow the brain functions by looking at brain connections. Here, we highlight\nfive critical elements of a network that allows an integrative analysis,\nfocusing mainly on a functional description. These elements include; (i) the\nproperties of its nodes; (ii) the metrics for connectivity and coupling between\nnodes; (iii) the network topologies; (iv) the network dynamics and (v) the\ninterconnections between different domains and scales of network\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:34:36 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Faber", "Jean", ""], ["Antoneli", "Priscila C.", ""], ["Via", "Guillem", ""], ["Ara\u00fajo", "Noemi S.", ""], ["Pinheiro", "Daniel J. L. L.", ""], ["Cavalheiro", "Esper", ""]]}, {"id": "1904.07282", "submitter": "Hongming Li", "authors": "Hongming Li, Mohamad Habes, David A. Wolk, Yong Fan", "title": "A deep learning model for early prediction of Alzheimer's disease\n  dementia based on hippocampal MRI", "comments": "Accepted for publication in Alzheimer's & Dementia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction: It is challenging at baseline to predict when and which\nindividuals who meet criteria for mild cognitive impairment (MCI) will\nultimately progress to Alzheimer's disease (AD) dementia. Methods: A deep\nlearning method is developed and validated based on MRI scans of 2146 subjects\n(803 for training and 1343 for validation) to predict MCI subjects' progression\nto AD dementia in a time-to-event analysis setting. Results: The deep learning\ntime-to-event model predicted individual subjects' progression to AD dementia\nwith a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects\nwith follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a\nC-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from\n18-54 months (quartiles: [18, 36,54]). The predicted progression risk also\nclustered individual subjects into subgroups with significant differences in\ntheir progression time to AD dementia (p<0.0002). Improved performance for\npredicting progression to AD dementia (C-index=0.864) was obtained when the\ndeep learning based progression risk was combined with baseline clinical\nmeasures. Conclusion: Our method provides a cost effective and accurate means\nfor prognosis and potentially to facilitate enrollment in clinical trials with\nindividuals likely to progress within a specific temporal period.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:37:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Li", "Hongming", ""], ["Habes", "Mohamad", ""], ["Wolk", "David A.", ""], ["Fan", "Yong", ""]]}, {"id": "1904.07295", "submitter": "Maja von Cube", "authors": "Maja von Cube, Martin Schumacher, Hein Putter, Jean-Francois Timsit,\n  Cornelis van der Velde, Martin Wolkewitz", "title": "The population-attributable fraction for time-dependent exposures using\n  dynamic prediction and landmarking", "comments": "A revised version has been submitted", "journal-ref": "Biometrical Journal, 2019", "doi": "10.1002/bimj.201800252", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The public health impact of a harmful exposure can be quantified by the\npopulation-attributable fraction (PAF). The PAF describes the attributable risk\ndue to an exposure and is often interpreted as the proportion of preventable\ncases if the exposure could be extinct. Difficulties in the definition and\ninterpretation of the PAF arise when the exposure of interest depends on time.\nThen, the definition of exposed and unexposed individuals is not\nstraightforward. We propose dynamic prediction and landmarking to define and\nestimate a PAF in this data situation. Two estimands are discussed which are\nbased on two hypothetical interventions that could prevent the exposure in\ndifferent ways. Considering the first estimand, at each landmark the estimation\nproblem is reduced to a time-independent setting. Then, estimation is simply\nperformed by using a generalized-linear model accounting for the current\nexposure state and further (time-varying) covariates. The second estimand is\nbased on counterfactual outcomes, estimation can be performed using\npseudo-values or inverse-probability weights. The approach is explored in a\nsimulation study and applied on two data examples. First, we study a large\nFrench database of intensive care unit patients to estimate the\npopulation-benefit of a pathogen-specific intervention that could prevent\nventilator-associated pneumonia caused by the pathogen Pseudomonas aeruginosa.\nMoreover, we quantify the population-attributable burden of locoregional and\ndistant recurrence in breast cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:10:54 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["von Cube", "Maja", ""], ["Schumacher", "Martin", ""], ["Putter", "Hein", ""], ["Timsit", "Jean-Francois", ""], ["van der Velde", "Cornelis", ""], ["Wolkewitz", "Martin", ""]]}, {"id": "1904.07408", "submitter": "Albee Ling", "authors": "Albee Y. Ling, Maria E. Montez-Rath, Maya B. Mathur, Kris Kapphahn,\n  Manisha Desai", "title": "How to apply multiple imputation in propensity score matching with\n  partially observed confounders: a simulation study and practical\n  recommendations", "comments": null, "journal-ref": "Journal of Modern Applied Statistical Methods, 19(1), eP3439\n  (2020)", "doi": "10.22237/jmasm/1608552120", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching (PSM) has been widely used to mitigate confounding\nin observational studies, although complications arise when the covariates used\nto estimate the PS are only partially observed. Multiple imputation (MI) is a\npotential solution for handling missing covariates in the estimation of the PS.\nUnfortunately, it is not clear how to best apply MI strategies in the context\nof PSM. We conducted a simulation study to compare the performances of popular\nnon-MI missing data methods and various MI-based strategies under different\nmissing data mechanisms (MDMs). We found that commonly applied missing data\nmethods resulted in biased and inefficient estimates, and we observed large\nvariation in performance across MI-based strategies. Based on our findings, we\nrecommend 1) deriving the PS after applying MI (referred to as MI-derPassive);\n2) conducting PSM within each imputed data set followed by averaging the\ntreatment effects to arrive at one summarized finding (INT-within) for mild\nMDMs and averaging the PSs across multiply imputed datasets before obtaining\none treatment effect using PSM (INT-across) for more complex MDMs; 3) a\nbootstrapped-based variance to account for uncertainty of PS estimation,\nmatching, and imputation; and 4) inclusion of key auxiliary variables in the\nimputation model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:20:31 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ling", "Albee Y.", ""], ["Montez-Rath", "Maria E.", ""], ["Mathur", "Maya B.", ""], ["Kapphahn", "Kris", ""], ["Desai", "Manisha", ""]]}, {"id": "1904.07414", "submitter": "Francois Meyer", "authors": "Peter Wills and Francois G. Meyer", "title": "Metrics for Graph Comparison: A Practitioner's Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of graph structure is a ubiquitous task in data analysis and\nmachine learning, with diverse applications in fields such as neuroscience,\ncyber security, social network analysis, and bioinformatics, among others.\nDiscovery and comparison of structures such as modular communities, rich clubs,\nhubs, and trees in data in these fields yields insight into the generative\nmechanisms and functional properties of the graph.\n  Often, two graphs are compared via a pairwise distance measure, with a small\ndistance indicating structural similarity and vice versa. Common choices\ninclude spectral distances (also known as $\\lambda$ distances) and distances\nbased on node affinities. However, there has of yet been no comparative study\nof the efficacy of these distance measures in discerning between common graph\ntopologies and different structural scales.\n  In this work, we compare commonly used graph metrics and distance measures,\nand demonstrate their ability to discern between common topological features\nfound in both random graph models and empirical datasets. We put forward a\nmulti-scale picture of graph structure, in which the effect of global and local\nstructure upon the distance measures is considered. We make recommendations on\nthe applicability of different distance measures to empirical graph data\nproblem based on this multi-scale view. Finally, we introduce the Python\nlibrary NetComp which implements the graph distances used in this work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:32:28 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 00:42:12 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Wills", "Peter", ""], ["Meyer", "Francois G.", ""]]}, {"id": "1904.07632", "submitter": "Shixiong Wang", "authors": "Shixiong Wang, Chongshou Li and Andrew Lim", "title": "Why Are the ARIMA and SARIMA not Sufficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoregressive moving average (ARMA) model takes the significant position\nin time series analysis for a wide-sense stationary time series. The difference\noperator and seasonal difference operator, which are bases of ARIMA and SARIMA\n(Seasonal ARIMA), respectively, were introduced to remove the trend and\nseasonal component so that the original non-stationary time series could be\ntransformed into a wide-sense stationary one, which could then be handled by\nBox-Jenkins methodology. However, such difference operators are more practical\nexperiences than exact theories by now. In this paper, we investigate the power\nof the (resp. seasonal) difference operator from the perspective of spectral\nanalysis, linear system theory and digital filtering, and point out the\ncharacteristics and limitations of (resp. seasonal) difference operator.\nBesides, the general method that transforms a non-stationary (the\nnon-stationarity in the mean sense) stochastic process to be wide-sense\nstationary will be presented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:01:15 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 02:24:00 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 06:25:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Shixiong", ""], ["Li", "Chongshou", ""], ["Lim", "Andrew", ""]]}, {"id": "1904.07672", "submitter": "James Hodges", "authors": "Liying Luo, James S. Hodges", "title": "Constraints in Random Effects Age-Period-Cohort Models", "comments": "Submitted to \"Sociological Methodology\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random effects (RE) models have been widely used to study the contextual\neffects of structures such as neighborhood or school. The RE approach has\nrecently been applied to age-period-cohort (APC) models that are unidentified\nbecause the predictors are exactly linearly dependent. However, it has not been\nfully understood how the RE specification identifies these otherwise\nunidentified APC models. We address this challenge by first making explicit\nthat RE-APC models have greater -- not less -- rank deficiency than the\ntraditional fixed-effects model, followed by two empirical examples. We then\nprovide intuition and a mathematical proof to explain that for APC models with\none RE, treating one effect as an RE is equivalent to constraining the\nestimates of that effect's linear component and the random intercept to be\nzero. For APC models with two RE's, the effective constraints implied by the\nmodel depend on the true (i.e., in the data-generating mechanism) non-linear\ncomponents of the effects that are modeled as RE's, so that the estimated\nlinear components of the RE's are determined by the true non-linear components\nof those effects. In conclusion, RE-APC models impose arbitrary though highly\nobscure constraints and thus do not differ qualitatively from other constrained\nAPC estimators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:55:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Luo", "Liying", ""], ["Hodges", "James S.", ""]]}, {"id": "1904.07688", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano,\n  Taha H. Rashidi", "title": "P\\'olygamma Data Augmentation to address Non-conjugacy in the Bayesian\n  Estimation of Mixed Multinomial Logit Models", "comments": "arXiv admin note: text overlap with arXiv:1904.03647", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Gibbs sampler of Mixed Multinomial Logit (MMNL) models involves\nsampling from conditional densities of utility parameters using\nMetropolis-Hastings (MH) algorithm due to unavailability of conjugate prior for\nlogit kernel. To address this non-conjugacy concern, we propose the application\nof P\\'olygamma data augmentation (PG-DA) technique for the MMNL estimation. The\nposterior estimates of the augmented and the default Gibbs sampler are similar\nfor two-alternative scenario (binary choice), but we encounter empirical\nidentification issues in the case of more alternatives ($J \\geq 3$).\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 16:38:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Bierlaire", "Michel", ""], ["Daziano", "Ricardo A.", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1904.07701", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Paul D. W. Kirk", "title": "Multiple kernel learning for integrative consensus clustering of 'omic\n  datasets", "comments": "Manuscript: 18 pages, 6 figures. Supplement: 29 pages, 19 figures.\n  This version contains additional simulation studies and comparisons to other\n  methods. For associated R code, see https://CRAN.R-project.org/package=klic\n  and https://github.com/acabassi/klic-pancancer-analysis", "journal-ref": null, "doi": "10.1093/bioinformatics/btaa593", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diverse applications - particularly in tumour subtyping - have demonstrated\nthe importance of integrative clustering techniques for combining information\nfrom multiple data sources. Cluster-Of-Clusters Analysis (COCA) is one such\napproach that has been widely applied in the context of tumour subtyping.\nHowever, the properties of COCA have never been systematically explored, and\nits robustness to the inclusion of noisy datasets, or datasets that define\nconflicting clustering structures, is unclear. We rigorously benchmark COCA,\nand present Kernel Learning Integrative Clustering (KLIC) as an alternative\nstrategy. KLIC frames the challenge of combining clustering structures as a\nmultiple kernel learning problem, in which different datasets each provide a\nweighted contribution to the final clustering. This allows the contribution of\nnoisy datasets to be down-weighted relative to more informative datasets. We\ncompare the performances of KLIC and COCA in a variety of situations through\nsimulation studies. We also present the output of KLIC and COCA in real data\napplications to cancer subtyping and transcriptional module discovery. R\npackages \"klic\" and \"coca\" are available on the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:33:32 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 16:15:23 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 09:35:40 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 18:12:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1904.07758", "submitter": "Thevaa Chandereng", "authors": "Thevaa Chandereng, Rick Chappell", "title": "Robust Blocked Response-Adaptive Randomization Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most clinical trials, patients are randomized with equal probability among\ntreatments to obtain an unbiased estimate of the treatment effect.\nResponse-adaptive randomization (RAR) has been proposed for ethical reasons,\nwhere the randomization ratio is tilted successively to favor the better\nperforming treatment. However, the substantial disagreement regarding bias due\nto time-trends in adaptive randomization is not fully recognized. The type-I\nerror is inflated in the traditional Bayesian RAR approaches when a time-trend\nis present. In our approach, patients are assigned in blocks and the\nrandomization ratio is recomputed for blocks rather than traditional adaptive\nrandomization where it is done per patient. We further investigate the design\nwith a range of scenarios for both frequentist and Bayesian designs. We compare\nour method with equal randomization and with different numbers of blocks\nincluding the traditional RAR design where randomization ratio is altered\npatient by patient basis. The analysis is stratified if there are two or more\npatients in each block. Small blocks should be avoided due to the possibility\nof not acquiring any information from the $\\mu_i$. On the other hand, RAR with\nlarge blocks has a good balance between efficiency and treating more subjects\nto the better-performing treatment, while retaining blocked RAR's unique\nunbiasedness.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:27:52 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:21:44 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Chandereng", "Thevaa", ""], ["Chappell", "Rick", ""]]}, {"id": "1904.07990", "submitter": "Gunnar R\\\"atsch", "authors": "Stephanie L. Hyland and Martin Faltys and Matthias H\\\"user and Xinrui\n  Lyu and Thomas Gumbsch and Crist\\'obal Esteban and Christian Bock and Max\n  Horn and Michael Moor and Bastian Rieck and Marc Zimmermann and Dean Bodenham\n  and Karsten Borgwardt and Gunnar R\\\"atsch and Tobias M. Merz", "title": "Machine learning for early prediction of circulatory failure in the\n  intensive care unit", "comments": "5 main figures, 1 main table, 13 supplementary figures, 5\n  supplementary tables; 250ppi images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive care clinicians are presented with large quantities of patient\ninformation and measurements from a multitude of monitoring systems. The\nlimited ability of humans to process such complex information hinders\nphysicians to readily recognize and act on early signs of patient\ndeterioration. We used machine learning to develop an early warning system for\ncirculatory failure based on a high-resolution ICU database with 240 patient\nyears of data. This automatic system predicts 90.0% of circulatory failure\nevents (prevalence 3.1%), with 81.8% identified more than two hours in advance,\nresulting in an area under the receiver operating characteristic curve of 94.0%\nand area under the precision-recall curve of 63.0%. The model was externally\nvalidated in a large independent patient cohort.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 21:18:17 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 16:02:18 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Faltys", "Martin", ""], ["H\u00fcser", "Matthias", ""], ["Lyu", "Xinrui", ""], ["Gumbsch", "Thomas", ""], ["Esteban", "Crist\u00f3bal", ""], ["Bock", "Christian", ""], ["Horn", "Max", ""], ["Moor", "Michael", ""], ["Rieck", "Bastian", ""], ["Zimmermann", "Marc", ""], ["Bodenham", "Dean", ""], ["Borgwardt", "Karsten", ""], ["R\u00e4tsch", "Gunnar", ""], ["Merz", "Tobias M.", ""]]}, {"id": "1904.08063", "submitter": "Alex Stivala", "authors": "Alex Stivala, Garry Robins, Alessandro Lomi", "title": "Exponential random graph model parameter estimation for very large\n  directed networks", "comments": "Minor correction of discussion of coverage and Type II error rate", "journal-ref": null, "doi": "10.1371/journal.pone.0227804", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models (ERGMs) are widely used for modeling social\nnetworks observed at one point in time. However the computational difficulty of\nERGM parameter estimation has limited the practical application of this class\nof models to relatively small networks, up to a few thousand nodes at most,\nwith usually only a few hundred nodes or fewer. In the case of undirected\nnetworks, snowball sampling can be used to find ERGM parameter estimates of\nlarger networks via network samples, and recently published improvements in\nERGM network distribution sampling and ERGM estimation algorithms have allowed\nERGM parameter estimates of undirected networks with over one hundred thousand\nnodes to be made. However the implementations of these algorithms to date have\nbeen limited in their scalability, and also restricted to undirected networks.\nHere we describe an implementation of the recently published Equilibrium\nExpectation (EE) algorithm for ERGM parameter estimation of large directed\nnetworks. We test it on some simulated networks, and demonstrate its\napplication to an online social network with over 1.6 million nodes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:16:53 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 00:46:47 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 21:32:52 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Stivala", "Alex", ""], ["Robins", "Garry", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1904.08232", "submitter": "Sarah Lemler", "authors": "Charlotte Dion (SU, LPSM UMR 8001), Sarah Lemler (MICS)", "title": "Nonparametric drift estimation for diffusions with jumps driven by a\n  Hawkes process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a 1-dimensional diffusion process X with jumps. The particularity\nof this model relies in the jumps which are driven by a multidimensional Hawkes\nprocess denoted N. This article is dedicated to the study of a nonparametric\nestimator of the drift coefficient of this original process. We construct\nestimators based on discrete observations of the process X in a high frequency\nframework with a large horizon time and on the observations of the process N.\nThe proposed nonparametric estimator is built from a least squares contrast\nprocedure on subspace spanned by trigonometric basis vectors. We obtain\nadaptive results that are comparable with the one obtained in the nonparametric\nregression context. We finally conduct a simulation study in which we first\nfocus on the implementation of the process and then on showing the good\nbehavior of the estimator.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:44:39 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 11:03:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Dion", "Charlotte", "", "SU, LPSM UMR 8001"], ["Lemler", "Sarah", "", "MICS"]]}, {"id": "1904.08420", "submitter": "Yongcheng Qi", "authors": "Yizeng Li and Yongcheng Qi", "title": "Adjusted Empirical Likelihood Method for the Tail Index of A\n  Heavy-Tailed Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical likelihood is a well-known nonparametric method in statistics and\nhas been widely applied in statistical inference. The method has been employed\nby Lu and Peng (2002) to constructing confidence intervals for the tail index\nof a heavy-tailed distribution. It is demonstrated in Lu and Peng (2002) that\nthe empirical likelihood-based confidence intervals performs better than\nconfidence intervals based on normal approximation in terms of the coverage\nprobability. In general, the empirical likelihood method can be hindered by its\nimprecision in the coverage probability when the sample size is small. This may\ncause a serious undercoverage issue when we apply the empirical likelihood to\nthe tail index as only a very small portion of observations can be used in the\nestimation of the tail index. In this paper, we employ an adjusted empirical\nlikelihood method, developed by Chen et al. (2008) and Liu and Chen (2010), to\nconstructing confidence intervals of the tail index so as to achieve a better\naccuracy. We conduct a simulation study to compare the performance of the\nadjusted empirical likelihood method and the normal approximation method. Our\nsimulation results indicate that the adjusted empirical likelihood method\noutperforms other methods in terms of the coverage probability and length of\nconfidence intervals. We also apply the adjusted empirical likelihood method to\na real data set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:55:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Yizeng", ""], ["Qi", "Yongcheng", ""]]}, {"id": "1904.08507", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Arnab Maity, Timothy Clark, Stefano B Longo", "title": "Variable Selection in Functional Linear Concurrent Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for variable selection in functional linear\nconcurrent regression. Our research is motivated by a fisheries footprint study\nwhere the goal is to identify important time-varying socio-structural drivers\ninfluencing patterns of seafood consumption, and hence fisheries footprint,\nover time, as well as estimating their dynamic effects. We develop a variable\nselection method in functional linear concurrent regression extending the\nclassically used scalar on scalar variable selection methods like LASSO, SCAD,\nand MCP. We show in functional linear concurrent regression the variable\nselection problem can be addressed as a group LASSO, and their natural\nextension; group SCAD or a group MCP problem. Through simulations, we\nillustrate our method, particularly with group SCAD or group MCP penalty, can\npick out the relevant variables with high accuracy and has minuscule false\npositive and false negative rate even when data is observed sparsely, is\ncontaminated with noise and the error process is highly non-stationary. We also\ndemonstrate two real data applications of our method in studies of dietary\ncalcium absorption and fisheries footprint in the selection of influential\ntime-varying covariates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:14:24 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:45:57 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ghosal", "Rahul", ""], ["Maity", "Arnab", ""], ["Clark", "Timothy", ""], ["Longo", "Stefano B", ""]]}, {"id": "1904.08522", "submitter": "Vitor Augusto Possebom", "authors": "Vitor Possebom", "title": "Sharp Bounds for the Marginal Treatment Effect with Sample Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I analyze treatment effects in situations when agents endogenously select\ninto the treatment group and into the observed sample. As a theoretical\ncontribution, I propose pointwise sharp bounds for the marginal treatment\neffect (MTE) of interest within the always-observed subpopulation under\nmonotonicity assumptions. Moreover, I impose an extra mean dominance assumption\nto tighten the previous bounds. I further discuss how to identify those bounds\nwhen the support of the propensity score is either continuous or discrete.\nUsing these results, I estimate bounds for the MTE of the Job Corps Training\nProgram on hourly wages for the always-employed subpopulation and find that it\nis decreasing in the likelihood of attending the program within the\nNon-Hispanic group. For example, the Average Treatment Effect on the Treated is\nbetween \\$.33 and \\$.99 while the Average Treatment Effect on the Untreated is\nbetween \\$.71 and \\$3.00.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:21:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Possebom", "Vitor", ""]]}, {"id": "1904.08672", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio, Bernard Rachet, Roch Giorgi, Camille Maringe,\n  Aurelien Belot", "title": "On models for the estimation of the excess mortality hazard in case of\n  insufficiently stratified life tables", "comments": "to appear in Biostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer epidemiology using population-based data, regression models for the\nexcess mortality hazard is a useful method to estimate cancer survival and to\ndescribe the association between prognosis factors and excess mortality. This\nmethod requires expected mortality rates from general population life tables:\neach cancer patient is assigned an expected (background) mortality rate\nobtained from the life tables, typically at least according to their age and\nsex, from the population they belong to. However, those life tables may be\ninsufficiently stratified, as some characteristics such as deprivation,\nethnicity, and comorbidities, are not available in the life tables for a number\nof countries. This may affect the background mortality rate allocated to each\npatient, and it has been shown that not including relevant information for\nassigning an expected mortality rate to each patient induces a bias in the\nestimation of the regression parameters of the excess hazard model. We propose\ntwo parametric corrections in excess hazard regression models, including a\nsingle-parameter or a random effect (frailty), to account for possible\nmismatches in the life table and thus misspecification of the background\nmortality rate. In an extensive simulation study, the good statistical\nperformance of the proposed approach is demonstrated, and we illustrate their\nuse on real population-based data of lung cancer patients. We present\nconditions and limitations of these methods, and provide some recommendations\nfor their use in practice.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 10:29:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Rachet", "Bernard", ""], ["Giorgi", "Roch", ""], ["Maringe", "Camille", ""], ["Belot", "Aurelien", ""]]}, {"id": "1904.08692", "submitter": "Maja von Cube", "authors": "Maja von Cube and Martin Schumacher and Sebastien Bailly and\n  Jean-Francois Timsit and Alain Lepape and Anne Savey and Anais Machut and\n  Martin Wolkewitz", "title": "The population-attributable fraction for time-dependent exposures and\n  competing risks - A discussion on estimands", "comments": "A revision has been submitted", "journal-ref": "Stat Med, 2019; 38: 3880-3895", "doi": "10.1002/sim.8208", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population-attributable fraction (PAF) quantifies the public health\nimpact of a harmful exposure. Despite being a measure of significant importance\nan estimand accommodating complicated time-to-event data is not clearly\ndefined. We discuss current estimands of the PAF used to quantify the public\nhealth impact of an internal time-dependent exposure for data subject to\ncompeting outcomes. To overcome some limitations, we proposed a novel estimand\nwhich is based on dynamic prediction by landmarking. In a profound simulation\nstudy, we discuss interpretation and performance of the various estimands and\ntheir estimators. The methods are applied to a large French database to\nestimate the health impact of ventilator-associated pneumonia for patients in\nintensive care.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:13:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["von Cube", "Maja", ""], ["Schumacher", "Martin", ""], ["Bailly", "Sebastien", ""], ["Timsit", "Jean-Francois", ""], ["Lepape", "Alain", ""], ["Savey", "Anne", ""], ["Machut", "Anais", ""], ["Wolkewitz", "Martin", ""]]}, {"id": "1904.08834", "submitter": "Ajay Tiwari", "authors": "Ajay K. Tiwari, Richard J. Morton, Stephane R\\'egnier, and James A.\n  McLaughlin", "title": "Damping of Propagating Kink Waves in the Solar Corona", "comments": "Accepted for publication in The Astrophysical Journal", "journal-ref": null, "doi": "10.3847/1538-4357/ab164b", "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alfv\\'enic waves have gained renewed interest since the existence of\nubiquitous propagating kink waves were discovered in the corona. {It has long\nbeen suggested that Alfv\\'enic} waves play an important role in coronal heating\nand the acceleration of the solar wind. To this effect, it is imperative to\nunderstand the mechanisms that enable their energy to be transferred to the\nplasma. Mode conversion via resonant absorption is believed to be one of the\nmain mechanisms for kink wave damping, and is considered to play a key role in\nthe process of energy transfer. This study examines the damping of propagating\nkink waves in quiescent coronal loops using the Coronal Multi-channel\nPolarimeter (CoMP). A coherence-based method is used to track the Doppler\nvelocity signal of the waves, enabling us to investigate the spatial evolution\nof velocity perturbations. The power ratio of outward to inward propagating\nwaves is used to estimate the associated damping lengths and quality factors.\nTo enable accurate estimates of these quantities, {we provide the first\nderivation of a likelihood function suitable for fitting models to the ratio of\ntwo power spectra obtained from discrete Fourier transforms. Maximum likelihood\nestimation is used to fit an exponential damping model to the observed\nvariation in power ratio as a function of frequency.} We confirm earlier\nindications that propagating kink waves are undergoing frequency dependent\ndamping. Additionally, we find that the rate of damping decreases, or\nequivalently the damping length increases, for longer coronal loops that reach\nhigher in the corona.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:17:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tiwari", "Ajay K.", ""], ["Morton", "Richard J.", ""], ["R\u00e9gnier", "Stephane", ""], ["McLaughlin", "James A.", ""]]}, {"id": "1904.08870", "submitter": "Antonia Gieschen", "authors": "Antonia Gieschen, Jake Ansell, Raffaella Calabrese, Belen\n  Martin-Barragan", "title": "Modelling antimicrobial prescriptions in Scotland: A spatio-temporal\n  clustering approach", "comments": "27 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016 the British government acknowledged the importance of reducing\nantimicrobial prescriptions in order to avoid the long-term harmful effects of\nover-prescription. Prescription needs are highly dependent on factors that have\na spatio-temporal component, such as the presence of a bacterial outbreak and\nthe population density. In this context, density-based clustering algorithms\nare flexible tools to analyse data by searching for group structures. The case\nof Scotland presents an additional challenge due to the diversity of population\ndensities under the area of study. We present here a spatio-temporal clustering\napproach for highlighting the behaviour of general practitioners (GPs) in\nScotland. Particularly, we consider the density-based spatial clustering of\napplications with noise algorithm (DBSCAN) due to its ability to include both\nspatial and temporal data, as well as its flexibility to be extended with\nfurther variables. We extend this approach into two directions. For the\ntemporal analysis, we use dynamic time warping to measure the dissimilarity\nbetween warped and shifted time series. For the spatial component, we introduce\na new way of weighting spatial distances with continuous weights derived from a\nKDE-based process. This makes our approach suitable for cases involving spatial\nclusters with differing densities, which is a well-known issue for the original\nDBSCAN. We show an improved performance compared to both the latter and the\npopular k-means algorithm on simulated, as well as empirical data, presenting\nevidence for the ability to cluster more elements correctly and deliver\nactionable insights.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:26:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Gieschen", "Antonia", ""], ["Ansell", "Jake", ""], ["Calabrese", "Raffaella", ""], ["Martin-Barragan", "Belen", ""]]}, {"id": "1904.08931", "submitter": "Yawen Guan", "authors": "Veronica J. Berrocal, Yawen Guan, Amanda Muyskens, Haoyu Wang, Brian J\n  Reich, James A. Mulholland and Howard H. Chang", "title": "A comparison of statistical and machine learning methods for creating\n  national daily maps of ambient PM$_{2.5}$ concentration", "comments": null, "journal-ref": null, "doi": "10.1016/j.atmosenv.2019.117130", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical problem in air pollution epidemiology is exposure assessment for\nindividuals for which health data are available. Due to the sparsity of\nmonitoring sites and the limited temporal frequency with which measurements of\nair pollutants concentrations are collected (for most pollutants, once every 3\nor 6 days), epidemiologists have been moving away from characterizing ambient\nair pollution exposure solely using measurements. In the last few years,\nsubstantial research efforts have been placed in developing statistical methods\nor machine learning techniques to generate estimates of air pollution at finer\nspatial and temporal scales (daily, usually) with complete coverage. Some of\nthese methods include: geostatistical techniques, such as kriging; spatial\nstatistical models that use the information contained in air quality model\noutputs (statistical downscaling models); linear regression modeling approaches\nthat leverage the information in GIS covariates (land use regression); or\nmachine learning methods that mine the information contained in relevant\nvariables (neural network and deep learning approaches). Although some of these\nexposure modeling approaches have been used in several air pollution\nepidemiological studies, it is not clear how much the predicted exposures\ngenerated by these methods differ, and which method generates more reliable\nestimates. In this paper, we aim to address this gap by evaluating a variety of\nexposure modeling approaches, comparing their predictive performance and\ncomputational difficulty. Using PM$_{2.5}$ in year 2011 over the continental\nU.S. as case study, we examine the methods' performances across seasons, rural\nvs urban settings, and levels of PM$_{2.5}$ concentrations (low, medium, high).\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:14:11 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Berrocal", "Veronica J.", ""], ["Guan", "Yawen", ""], ["Muyskens", "Amanda", ""], ["Wang", "Haoyu", ""], ["Reich", "Brian J", ""], ["Mulholland", "James A.", ""], ["Chang", "Howard H.", ""]]}, {"id": "1904.08937", "submitter": "Barak Brill", "authors": "Barak Brill, Amnon Amir, Ruth Heller", "title": "Testing for differential abundance in compositional counts data, with\n  application to microbiome studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying which taxa in our microbiota are associated with traits of\ninterest is important for advancing science and health. However, the\nidentification is challenging because the measured vector of taxa counts (by\namplicon sequencing) is compositional, so a change in the abundance of one\ntaxon in the microbiota induces a change in the number of sequenced counts\nacross all taxa. The data is typically sparse, with zero counts present either\ndue to biological variance or limited sequencing depth (technical zeros). For\nlow abundance taxa, the chance for technical zeros is non-negligible. We show\nthat existing methods designed to identify differential abundance for\ncompositional data may have an inflated number of false positives due to\nimproper handling of the zero counts. We introduce a novel non-parametric\napproach which provides valid inference even when the fraction of zero counts\nis substantial. Our approach uses a set of reference taxa that are\nnon-differentially abundant, which can be estimated from the data or from\noutside information. We show the usefulness of our approach via simulations, as\nwell as on three different data sets: a Crohn's disease study, the Human\nMicrobiome Project, and an experiment with 'spiked-in' bacteria.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:39:58 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 11:44:29 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 11:12:02 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 12:56:08 GMT"}, {"version": "v5", "created": "Mon, 30 Mar 2020 12:17:34 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Brill", "Barak", ""], ["Amir", "Amnon", ""], ["Heller", "Ruth", ""]]}, {"id": "1904.08978", "submitter": "Nathaniel Price", "authors": "Nathaniel B. Price, Mathieu Balesdent, S\\'ebastien Defoort, Rodolphe\n  Le Riche, Nam H. Kim, Raphael T. Haftka", "title": "Safety-margin-based design and redesign considering mixed epistemic\n  model uncertainty and aleatory parameter uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the initial design stage engineers often rely on low-fidelity models that\nhave high epistemic uncertainty. Traditional safety-margin-based deterministic\ndesign resorts to testing (e.g. prototype experiment, evaluation of\nhigh-fidelity simulation, etc.) to reduce epistemic uncertainty and achieve\ntargeted levels of safety. Testing is used to calibrate models and prescribe\nredesign when tests are not passed. After calibration, reduced epistemic model\nuncertainty can be leveraged through redesign to restore safety or improve\ndesign performance; however, redesign may be associated with substantial costs\nor delays. In this paper, a methodology is described for optimizing the\nsafety-margin-based design, testing, and redesign process to allow the designer\nto tradeoff between the risk of future redesign and the possible performance\nand reliability benefits. The proposed methodology represents the epistemic\nmodel uncertainty with a Kriging surrogate and is applicable in a wide range of\ndesign problems. The method is illustrated on a cantilever beam bending example\nand then a sounding rocket example. It is shown that redesign acts as a type of\nquality control measure to prevent an undesirable initial design from being\naccepted as the final design. It is found that the optimal design/redesign\nstrategy for maximizing expected design performance includes not only redesign\nto correct an initial design that is later revealed to be unsafe, but also\nredesign to improve performance when an initial design is later revealed to be\ntoo conservative (e.g. too heavy).\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:25:21 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Price", "Nathaniel B.", ""], ["Balesdent", "Mathieu", ""], ["Defoort", "S\u00e9bastien", ""], ["Riche", "Rodolphe Le", ""], ["Kim", "Nam H.", ""], ["Haftka", "Raphael T.", ""]]}, {"id": "1904.09241", "submitter": "Abdullah Alshelahi", "authors": "Abdullah AlShelahi, Jingxing Wang, Mingdi You, Eunshin Byon, Romesh\n  Saigal", "title": "An Alternative Data-Driven Prediction Approach Based on Real Option\n  Theories", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijpe.2019.107605", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new prediction model for time series data by\nintegrating a time-varying Geometric Brownian Motion model with a pricing\nmechanism used in financial engineering. Typical time series models such as\nAuto-Regressive Integrated Moving Average assumes a linear correlation\nstructure in time series data. When a stochastic process is highly volatile,\nsuch an assumption can be easily violated, leading to inaccurate predictions.\nWe develop a new prediction model that can flexibly characterize a time-varying\nvolatile process without assuming linearity. We formulate the prediction\nproblem as an optimization problem with unequal overestimation and\nunderestimation costs. Based on real option theories developed in finance, we\nsolve the optimization problem and obtain a predicted value, which can minimize\nthe expected prediction cost. We evaluate the proposed approach using multiple\ndatasets obtained from real-life applications including manufacturing, finance,\nand environment. The numerical results demonstrate that the proposed model\nshows competitive prediction capability, compared with alternative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:50:39 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["AlShelahi", "Abdullah", ""], ["Wang", "Jingxing", ""], ["You", "Mingdi", ""], ["Byon", "Eunshin", ""], ["Saigal", "Romesh", ""]]}, {"id": "1904.09245", "submitter": "Shixiong Wang", "authors": "Shixiong Wang, Chongshou Li, and Andrew Lim", "title": "Deep Pattern of Time Series and Its Applications in Estimation,\n  Forecasting, Fault Diagnosis and Target Tracking", "comments": null, "journal-ref": "Published in the IEEE Transactions on Instrumentation and\n  Measurement in Feb 2021, with the adapted version", "doi": "10.1109/TIM.2021.3059321", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information contained in a time series is more than what the values\nthemselves are. In this paper, the Time-variant Local Autocorrelated Polynomial\nmodel with Kalman filter is proposed to model the underlying dynamics of a time\nseries (or signal) and mine the deep pattern of it, except estimating the\ninstantaneous mean function (also known as trend function), including: (1)\nidentifying and predicting the peak and valley values of a time series; (2)\nreporting and forecasting the current changing pattern (increasing or\ndecreasing pattern of the trend, and how fast it changes). We will show that it\nis this deep pattern that allows us to make higher-accuracy estimation and\nforecasting for a time series, to easily detect the anomalies (faults) of a\nsensor, and to track a highly-maneuvering target.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:53:32 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 15:40:07 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 06:16:14 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 02:58:46 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Shixiong", ""], ["Li", "Chongshou", ""], ["Lim", "Andrew", ""]]}, {"id": "1904.09394", "submitter": "Abbas Khalili Professor", "authors": "Annaliza McGillivray, Abbas Khalili, and David A. Stephens", "title": "Estimating Sparse Networks with Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical modelling techniques based on sparse selection have been applied to\ninfer complex networks in many fields, including biology and medicine,\nengineering, finance, and social sciences. One structural feature of some of\nthe networks in such applications that poses a challenge for statistical\ninference is the presence of a small number of strongly interconnected nodes in\na network which are called hubs. For example, in microbiome research hubs or\nmicrobial taxa play a significant role in maintaining stability of the\nmicrobial community structure. In this paper, we investigate the problem of\nestimating sparse networks in which there are a few highly connected hub nodes.\nMethods based on L1-regularization have been widely used for performing sparse\nselection in the graphical modelling context. However, while these methods\nencourage sparsity, they do not take into account structural information of the\nnetwork. We introduce a new method for estimating networks with hubs that\nexploits the ability of (inverse) covariance selection methods to include\nstructural information about the underlying network. Our proposed method is a\nweighted lasso approach with novel row/column sum weights, which we refer to as\nthe hubs weighted graphical lasso. We establish large sample properties of the\nmethod when the number of parameters diverges with the sample size, and\nevaluate its finite sample performance via extensive simulations. We illustrate\nthe method with an application to microbiome data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 03:32:12 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 01:56:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["McGillivray", "Annaliza", ""], ["Khalili", "Abbas", ""], ["Stephens", "David A.", ""]]}, {"id": "1904.09480", "submitter": "Ionas Erb", "authors": "Ionas Erb", "title": "Partial Correlations in Compositional Data Analysis", "comments": "11 pages, 1 figure, submitted to CoDaWork 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial correlations quantify linear association between two variables\nadjusting for the influence of the remaining variables. They form the backbone\nfor graphical models and are readily obtained from the inverse of the\ncovariance matrix. For compositional data, the covariance structure is\nspecified from log ratios of variables, so unless we try to \"open\" the data via\na normalization, this implies changes in the definition and interpretation of\npartial correlations. In the present work, we elucidate how results derived by\nAitchison (1986) lead to a natural definition of partial correlation that has a\nnumber of advantages over current measures of association. For this, we show\nthat the residuals of log-ratios between a variable with a reference, when\nadjusting for all remaining variables including the reference, are\nreference-independent. Since the reference itself can be controlled for,\ncorrelations between residuals are defined for the variables directly without\nthe necessity to recur to ratios except when specifying which variables are\npartialled out. Thus, perhaps surprisingly, partial correlations do not have\nthe problems commonly found with measures of pairwise association on\ncompositional data. They are well-defined between two variables, are properly\nscaled, and allow for negative association. By design, they are\nsubcompositionally incoherent, but they share this property with conventional\npartial correlations (where results change when adjusting for the influence of\nfewer variables). We discuss the equivalence with normalization-based\napproaches whenever the normalizing variables are controlled for. We also\ndiscuss the partial variances and correlations we obtain from a previously\nstudied data set of Roman glass cups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 18:59:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Erb", "Ionas", ""]]}, {"id": "1904.09514", "submitter": "Mohammad Nabhan", "authors": "Mohammad Nabhan, Yajun Mei and Jianjun Shi", "title": "High Dimensional Process Monitoring Using Robust Sparse Probabilistic\n  Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional data has introduced challenges that are difficult to address\nwhen attempting to implement classical approaches of statistical process\ncontrol. This has made it a topic of interest for research due in recent years.\nHowever, in many cases, data sets have underlying structures, such as in\nadvanced manufacturing systems. If extracted correctly, efficient methods for\nprocess control can be developed. This paper proposes a robust sparse\ndimensionality reduction approach for correlated high-dimensional process\nmonitoring to address the aforementioned issues. The developed monitoring\ntechnique uses robust sparse probabilistic PCA to reduce the dimensionality of\nthe data stream while retaining interpretability. The proposed methodology\nutilizes Bayesian variational inference to obtain the estimates of a\nprobabilistic representation of PCA. Simulation studies were conducted to\nverify the efficacy of the proposed methodology. Furthermore, we conducted a\ncase study for change detection for in-line Raman spectroscopy to validate the\nefficiency of our proposed method in a practical scenario.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 23:28:41 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nabhan", "Mohammad", ""], ["Mei", "Yajun", ""], ["Shi", "Jianjun", ""]]}, {"id": "1904.09525", "submitter": "Pei-Chun Su", "authors": "Pei-Chun Su, Stephen Miller, Salim Idriss, Piers Barker, and Hau-Tieng\n  Wu", "title": "Recovery of the fetal electrocardiogram for morphological analysis from\n  two trans-abdominal channels via optimal shrinkage", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm to recover fetal electrocardiogram (ECG) for\nboth the fetal heart rate analysis and morphological analysis of its waveform\nfrom two or three trans-abdominal maternal ECG channels. We design an algorithm\nbased on the optimal-shrinkage and the nonlocal Euclidean median under the\nwave-shape manifold model. For the fetal heart rate analysis, the algorithm is\nevaluated on publicly available database, 2013 PhyioNet/Computing in Cardiology\nChallenge, set A. For the morphological analysis, we propose to simulate\nsemi-real databases by mixing the MIT-BIH Normal Sinus Rhythm Database and\nMITDB Arrhythmia Database. For the fetal R peak detection, the proposed\nalgorithm outperforms all algorithms under comparison. For the morphological\nanalysis, the algorithm provides an encouraging result in recovery of the fetal\nECG waveform, including PR, QT and ST intervals, even when the fetus has\narrhythmia. To the best of our knowledge, this is the first work focusing on\nrecovering the fetal ECG for morphological analysis from two or three channels\nwith an algorithm potentially applicable for continuous fetal\nelectrocardiographic monitoring, which creates the potential for long term\nmonitoring purpose.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 01:31:44 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 04:19:26 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Su", "Pei-Chun", ""], ["Miller", "Stephen", ""], ["Idriss", "Salim", ""], ["Barker", "Piers", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1904.09609", "submitter": "Ranjan Maitra", "authors": "Nicholas S. Berry and Ranjan Maitra", "title": "TiK-means: $K$-means clustering for skewed groups", "comments": "15 pages, 6 figures, to appear in Statistical Analysis and Data\n  Mining - The ASA Data Science Journal", "journal-ref": "Statistical Analysis and Data Mining -- The ASA Data Science\n  Journal, 2019, volume 12, number 3, pages 223-233", "doi": "10.1002/sam11416", "report-no": null, "categories": "stat.ML astro-ph.HE cs.CV cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-means algorithm is extended to allow for partitioning of skewed\ngroups. Our algorithm is called TiK-Means and contributes a $K$-means type\nalgorithm that assigns observations to groups while estimating their\nskewness-transformation parameters. The resulting groups and transformation\nreveal general-structured clusters that can be explained by inverting the\nestimated transformation. Further, a modification of the jump statistic chooses\nthe number of groups. Our algorithm is evaluated on simulated and real-life\ndatasets and then applied to a long-standing astronomical dispute regarding the\ndistinct kinds of gamma ray bursts.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:32:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berry", "Nicholas S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1904.09662", "submitter": "John Kang", "authors": "John Kang, James T. Coates, Robert L. Strawderman, Barry S.\n  Rosenstein, Sarah L. Kerns", "title": "Genomics models in radiotherapy: from mechanistic to machine learning", "comments": "32 pages, 3 figures, 3 tables", "journal-ref": "Medical Physics 2020", "doi": "10.1002/mp.13751", "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning provides a broad framework for addressing high-dimensional\nprediction problems in classification and regression. While machine learning is\noften applied for imaging problems in medical physics, there are many efforts\nto apply these principles to biological data towards questions of radiation\nbiology. Here, we provide a review of radiogenomics modeling frameworks and\nefforts towards genomically-guided radiotherapy. We first discuss medical\noncology efforts to develop precision biomarkers. We next discuss similar\nefforts to create clinical assays for normal tissue or tumor radiosensitivity.\nWe then discuss modeling frameworks for radiosensitivity and the evolution of\nmachine learning to create predictive models for radiogenomics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 21:27:58 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 18:54:12 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 19:53:27 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kang", "John", ""], ["Coates", "James T.", ""], ["Strawderman", "Robert L.", ""], ["Rosenstein", "Barry S.", ""], ["Kerns", "Sarah L.", ""]]}, {"id": "1904.09699", "submitter": "Xueying Tang", "authors": "Xueying Tang, Zhi Wang, Qiwei He, Jingchen Liu, Zhiliang Ying", "title": "Latent Feature Extraction for Process Data via Multidimensional Scaling", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-based interactive items have become prevalent in recent educational\nassessments. In such items, the entire human-computer interactive process is\nrecorded in a log file and is known as the response process. This paper aims at\nextracting useful information from response processes. In particular, we\nconsider an exploratory latent variable analysis for process data. Latent\nvariables are extracted through a multidimensional scaling framework and can be\nempirically proved to contain more information than classic binary responses in\nterms of out-of-sample prediction of many variables.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 02:23:21 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tang", "Xueying", ""], ["Wang", "Zhi", ""], ["He", "Qiwei", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1904.09829", "submitter": "Riccardo Franco", "authors": "Riccardo Franco", "title": "First steps to a constructor theory of cognition", "comments": "arXiv admin note: text overlap with arXiv:1405.5563,\n  arXiv:1507.03287, arXiv:1601.06610 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article applies the conceptual framework of constructor theory of\ninformation to cognition theory. The main result of this work is that cognition\ntheory, in specific situations concerning for example the conjunction fallacy\nheuristic, requires the use of superinformation media, just as quantum theory.\nThis result entails that quantum and cognition theories can be considered as\nelements of a general class of superinformation-based subsidiary theories.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:39:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Franco", "Riccardo", ""]]}, {"id": "1904.09883", "submitter": "Benjamin Campbell", "authors": "Benjamin W. Campbell", "title": "Measuring and Assessing Latent Variation in Alliance Design and\n  Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alliance literature is bifurcated between an empirically-driven approach\nutilizing rigorous data, and a theoretically-motivated approach offering a rich\nconceptualization of alliances. Within the strength of one, lays the weakness\nof the other. While the former invokes a non-comprehensive view of alliances\nthat emphasizes capability aggregation, the latter does not provide a\nsystematic or rigorous approach to uncover empirical insights. I unify these\nperspectives, enumerating the roles a state can adopt within the alliance\nnetwork and considering the relationship between that role and how they design\ntheir local alliance network to accomplish role-based objectives. To uncover\nthis variation, I employ a novel methodological tool, the ego-TERGM. Results\nindicate that states form alliances to accomplish a variety of foreign policy\nobjectives beyond capability aggregation, including the consolidation of\nnon-security ties and the pursuit of domestic reforms in addition to\nsecurity-based motivations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 14:02:44 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Campbell", "Benjamin W.", ""]]}, {"id": "1904.10046", "submitter": "Priyam Das", "authors": "Raju Maiti, Jialiang Li, Priyam Das, Lei Feng, Derek Hausenloy, Bibhas\n  Chakraborty", "title": "A distribution-free smoothed combination method of biomarkers to improve\n  diagnostic accuracy in multi-category classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results from multiple diagnostic tests are usually combined to improve the\noverall diagnostic accuracy. For binary classification, maximization of the\nempirical estimate of the area under the receiver operating characteristic\n(ROC) curve is widely adopted to produce the optimal linear combination of\nmultiple biomarkers. In the presence of large number of biomarkers, this method\nproves to be computationally expensive and difficult to implement since it\ninvolves maximization of a discontinuous, non-smooth function for which\ngradient-based methods cannot be used directly. Complexity of this problem\nincreases when the classification problem becomes multi-category. In this\narticle, we develop a linear combination method that maximizes a smooth\napproximation of the empirical Hypervolume Under Manifolds (HUM) for\nmulti-category outcome. We approximate HUM by replacing the indicator function\nwith the sigmoid function or normal cumulative distribution function (CDF).\nWith the above smooth approximations, efficient gradient-based algorithms can\nbe employed to obtain better solution with less computing time. We show that\nunder some regularity conditions, the proposed method yields consistent\nestimates of the coefficient parameters. We also derive the asymptotic\nnormality of the coefficient estimates. We conduct extensive simulations to\nexamine our methods. Under different simulation scenarios, the proposed methods\nare compared with other existing methods and are shown to outperform them in\nterms of diagnostic accuracy. The proposed method is illustrated using two real\nmedical data sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 19:48:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Maiti", "Raju", ""], ["Li", "Jialiang", ""], ["Das", "Priyam", ""], ["Feng", "Lei", ""], ["Hausenloy", "Derek", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "1904.10118", "submitter": "Taiane Prass", "authors": "Taiane Schaedler Prass, S\\'ilvia Regina Costa Lopes, Jos\\'e G.\n  D\\'orea, Rejane C. Marques and Katiane G. Brand\\~ao", "title": "Amazon Forest Fires Between 2001 and 2006 and Birth Weight in Porto\n  Velho", "comments": null, "journal-ref": "Bulletin of Environmental Contamination and Toxicology, July 2012,\n  Volume 89, Issue 1, pp 1-7", "doi": "10.1007/s00128-012-0621-z", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birth weight data (22,012 live-births) from a public hospital in Porto Velho\n(Amazon) was used in multiple statistical models to assess the effects of\nforest-fire smoke on human reproductive outcome. Mean birth weights for girls\n(3,139 g) and boys (3,393 g) were considered statistically different (p-value <\n2.2e-16). Among all models analyzed, the means were considered statistically\ndifferent only when treated as a function of month and year (p-value = 0.0989,\ngirls and 0.0079, boys) . The R 2 statistics indicate that the regression\nmodels considered are able to explain 65 % (girls) and 54 % (boys) of the\nvariation of the mean birth weight.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 01:48:03 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 01:09:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Prass", "Taiane Schaedler", ""], ["Lopes", "S\u00edlvia Regina Costa", ""], ["D\u00f3rea", "Jos\u00e9 G.", ""], ["Marques", "Rejane C.", ""], ["Brand\u00e3o", "Katiane G.", ""]]}, {"id": "1904.10152", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Zhengyuan Zhu, Shuiqing Yin", "title": "Identifying Precipitation Regimes in China Using Model-Based Clustering\n  of Spatial Functional Data", "comments": "4 pages, 3 figures", "journal-ref": "Published in Proceedings of the 6th International Workshop on\n  Climate Informatics (2016)", "doi": "10.5065/D6K072N6", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of precipitation regimes is important for many purposes\nsuch as agricultural planning, water resource management, and return period\nestimation. Since precipitation and other related meteorological data typically\nexhibit spatial dependency and different characteristics at different time\nscales, clustering such data presents unique challenges. In this paper, we\ndevelop a flexible model-based approach to cluster multi-scale spatial\nfunctional data to address such problems. The underlying clustering model is a\nfunctional linear model , and the cluster memberships are assumed to be a\nrealization from a Markov random field with geographic covariates. The\nmethodology is applied to a precipitation data from China to identify\nprecipitation regimes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:01:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhang", "Haozhe", ""], ["Zhu", "Zhengyuan", ""], ["Yin", "Shuiqing", ""]]}, {"id": "1904.10172", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Massimiliano Pastore, and Gianmarco Alto\\`e", "title": "ssMousetrack: Analysing computerized tracking data via Bayesian\n  state-space models in {R}", "comments": null, "journal-ref": "Math. Comput. Appl. 2020, 25(3), 41", "doi": "10.3390/mca25030041", "report-no": null, "categories": "stat.CO stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances have provided new settings to enhance\nindividual-based data collection and computerized-tracking data have became\ncommon in many behavioral and social research. By adopting instantaneous\ntracking devices such as computer-mouse, wii, and joysticks, such data provide\nnew insights for analysing the dynamic unfolding of response process.\nssMousetrack is a R package for modeling and analysing computerized-tracking\ndata by means of a Bayesian state-space approach. The package provides a set of\nfunctions to prepare data, fit the model, and assess results via simple\ndiagnostic checks. This paper describes the package and illustrates how it can\nbe used to model and analyse computerized-tracking data. A case study is also\nincluded to show the use of the package in empirical case studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:30:24 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Pastore", "Massimiliano", ""], ["Alto\u00e8", "Gianmarco", ""]]}, {"id": "1904.10182", "submitter": "Rituparna Sen", "authors": "Arnab Chakrabarti and Rituparna Sen", "title": "Copula estimation for nonsynchronous financial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula is a powerful tool to model multivariate data. We propose the\nmodelling of intraday financial returns of multiple assets through copula. The\nproblem originates due to the asynchronous nature of intraday financial data.\nWe propose a consistent estimator of the correlation coefficient in case of\nElliptical copula and show that the plug-in copula estimator is uniformly\nconvergent. For non-elliptical copulas, we capture the dependence through\nKendall's Tau. We demonstrate underestimation of the copula parameter and use a\nquadratic model to propose an improved estimator. In simulations, the proposed\nestimator reduces the bias significantly for a general class of copulas. We\napply the proposed methods to real data of several stock prices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 07:23:05 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 10:51:33 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chakrabarti", "Arnab", ""], ["Sen", "Rituparna", ""]]}, {"id": "1904.10199", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Ond\\v{r}ej Sokol and Vladim\\'ir Hol\\'y", "title": "How Many Customers Does a Retail Store Have?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge of the number of customers is the pillar of retail business\nanalytics. In our setting, we assume that a portion of customers is monitored\nand easily counted due to the loyalty program while the rest is not monitored.\nThe behavior of customers in both groups may significantly differ making the\nestimation of the number of unmonitored customers a non-trivial task. We\nidentify shopping patterns of several customer segments which allows us to\nestimate the distribution of customers without the loyalty card using the\nmaximum likelihood method. In a simulation study, we find that the proposed\napproach is quite precise even when the data sample is very small and its\nassumptions are violated to a certain degree. In an empirical study of a\ndrugstore chain, we validate and illustrate the proposed approach in practice.\nThe actual number of customers estimated by the proposed method is much higher\nthan the number suggested by the naive estimate assuming the constant customer\ndistribution. The proposed method can also be utilized to determine penetration\nof the loyalty program in the individual customer segments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 08:36:02 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 19:00:17 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sokol", "Ond\u0159ej", ""], ["Hol\u00fd", "Vladim\u00edr", ""]]}, {"id": "1904.10398", "submitter": "Rafael S. de Souza", "authors": "Rafael S. de Souza and Gary S. Berger", "title": "Can fallopian tube anatomy predict pregnancy and pregnancy outcomes\n  after tubal reversal surgery?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted this study to determine if postoperative fallopian tube anatomy\ncan predict the likelihood of pregnancy and pregnancy outcomes after tubal\nsterilisation reversal. We built a flexible, non-parametric, multivariate model\nvia generalised additive models to assess the effects of these tubal parameters\nobserved during tubal reparative surgery: tubal lengths; differences in tubal\nsegment location and diameters at the anastomosis sites; and, fibrosis of the\ntubal muscularis. Age and tubal length (in that order) are the primary features\ndetermining the likelihood of pregnancy. For pregnancy outcomes, age is the\nprimary predictor of miscarriage, but tubal length is the most influential\npredictor of the odds of birth and ectopic pregnancy. Segment location and\ndiameters contribute slightly to the odds of miscarriage and ectopic pregnancy,\nwhereas fibrosis has little apparent effect. This study is the first to show\nthat a statistical learning predictive model based on fallopian tube anatomy\ncan predict both pregnancy and pregnancy outcome probabilities after tubal\nreversal surgery.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:19:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["de Souza", "Rafael S.", ""], ["Berger", "Gary S.", ""]]}, {"id": "1904.10580", "submitter": "Shikun Ke", "authors": "Barry Ke and Angela Qiao", "title": "Who Gets the Job and How are They Paid? Machine Learning Application on\n  H-1B Case Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use machine learning techniques to explore the H-1B\napplication dataset disclosed by the Department of Labor (DOL), from 2008 to\n2018, in order to provide more stylized facts of the international workers in\nUS labor market. We train a LASSO Regression model to analyze the impact of\ndifferent features on the applicant's wage, and a Logistic Regression with\nL1-Penalty as a classifier to study the feature's impact on the likelihood of\nthe case being certified. Our analysis shows that working in the healthcare\nindustry, working in California, higher job level contribute to higher\nsalaries. In the meantime, lower job level, working in the education services\nindustry and nationality of Philippines are negatively correlated with the\nsalaries. In terms of application status, a Ph.D. degree, working in retail or\nfinance, majoring in computer science will give the applicants a better chance\nof being certified. Applicants with no or an associate degree, working in the\neducation services industry, or majoring in education are more likely to be\nrejected.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:20:03 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ke", "Barry", ""], ["Qiao", "Angela", ""]]}, {"id": "1904.10582", "submitter": "Eric Chi", "authors": "Halley L. Brantley and Joseph Guinness and Eric C. Chi", "title": "Baseline Drift Estimation for Air Quality Data Using Quantile Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating smoothly varying baseline trends in time\nseries data. This problem arises in a wide range of fields, including\nchemistry, macroeconomics, and medicine; however, our study is motivated by the\nanalysis of data from low cost air quality sensors. Our methods extend the\nquantile trend filtering framework to enable the estimation of multiple\nquantile trends simultaneously while ensuring that the quantiles do not cross.\nTo handle the computational challenge posed by very long time series, we\npropose a parallelizable alternating direction method of moments (ADMM)\nalgorithm. The ADMM algorthim enables the estimation of trends in a piecewise\nmanner, both reducing the computation time and extending the limits of the\nmethod to larger data sizes. We also address smoothing parameter selection and\npropose a modified criterion based on the extended Bayesian Information\nCriterion. Through simulation studies and our motivating application to low\ncost air quality sensor data, we demonstrate that our model provides better\nquantile trend estimates than existing methods and improves signal\nclassification of low-cost air quality sensor output.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:23:00 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Brantley", "Halley L.", ""], ["Guinness", "Joseph", ""], ["Chi", "Eric C.", ""]]}, {"id": "1904.10829", "submitter": "Jann Goschenhofer", "authors": "Jann Goschenhofer, Franz MJ Pfister, Kamer Ali Yuksel, Bernd Bischl,\n  Urban Fietzek, Janek Thomas", "title": "Wearable-based Parkinson's Disease Severity Monitoring using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One major challenge in the medication of Parkinson's disease is that the\nseverity of the disease, reflected in the patients' motor state, cannot be\nmeasured using accessible biomarkers. Therefore, we develop and examine a\nvariety of statistical models to detect the motor state of such patients based\non sensor data from a wearable device. We find that deep learning models\nconsistently outperform a classical machine learning model applied on\nhand-crafted features in this time series classification task. Furthermore, our\nresults suggest that treating this problem as a regression instead of an\nordinal regression or a classification task is most appropriate. For consistent\nmodel evaluation and training, we adopt the leave-one-subject-out validation\nscheme to the training of deep learning models. We also employ a\nclass-weighting scheme to successfully mitigate the problem of high multi-class\nimbalances in this domain. In addition, we propose a customized performance\nmeasure that reflects the requirements of the involved medical staff on the\nmodel. To solve the problem of limited availability of high quality training\ndata, we propose a transfer learning technique which helps to improve model\nperformance substantially. Our results suggest that deep learning techniques\noffer a high potential to autonomously detect motor states of patients with\nParkinson's disease.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 14:05:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Goschenhofer", "Jann", ""], ["Pfister", "Franz MJ", ""], ["Yuksel", "Kamer Ali", ""], ["Bischl", "Bernd", ""], ["Fietzek", "Urban", ""], ["Thomas", "Janek", ""]]}, {"id": "1904.10886", "submitter": "Behram Wali", "authors": "Behram Wali, Asad Khattak, David Greene, Jun Liu", "title": "Fuel Economy Gaps Within & Across Garages: A Bivariate Random Parameters\n  Seemingly Unrelated Regression Approach", "comments": "Fuel economy gap, two-vehicles, garage, My MPG, On-road & test cycle\n  estimates, Random parameters, Seemingly unrelated regression estimation", "journal-ref": "International Journal of Sustainable Transportation, 1-16 (2018)", "doi": "10.1080/15568318.2018.1466222", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key objective of this study is to investigate the interrelationship\nbetween fuel economy gaps and to quantify the differential effects of several\nfactors on fuel economy gaps of vehicles operated by the same garage. By using\na unique fuel economy database (fueleconomy.gov), users self-reported fuel\neconomy estimates and government fuel economy ratings are analyzed for more\nthan 7000 garages across the U.S. The empirical analysis, nonetheless, is\ncomplicated owing to the presence of important methodological concerns\nincluding potential interrelationship between vehicles within the same garage\nand unobserved heterogeneity. To address these concerns, bivariate seemingly\nunrelated fixed and random parameter models are presented. With government test\ncycle ratings tending to over-estimate the actual on-road fuel economy, a\nsignificant variation is observed in the fuel economy gaps for the two vehicles\nacross garages. A wide variety of factors such as driving style, fuel economy\ncalculation method, and several vehicle specific characteristics are\nconsidered. Drivers who drive for maximum gas mileage or drives with the\ntraffic flow have greater on-road fuel economy relative to the government\nofficial ratings. Contrarily, volatile drivers have smaller on-road fuel\neconomy relative to the official ratings. Compared to the previous findings,\nour analysis suggests that the relationship between fuel type and fuel economy\ngaps is complex and not unidirectional. Regarding several vehicle and\nmanufacturer related variables, the effects do not just significantly vary in\nmagnitude but also in the direction, underscoring the importance of accounting\nfor within-garage correlation and unobserved heterogeneity for making reliable\ninferences.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 20:30:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad", ""], ["Greene", "David", ""], ["Liu", "Jun", ""]]}, {"id": "1904.10890", "submitter": "Roel Henckaerts", "authors": "Roel Henckaerts, Marie-Pier C\\^ot\\'e, Katrien Antonio, Roel Verbelen", "title": "Boosting insights in insurance tariff plans with tree-based machine\n  learning methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pricing actuaries typically operate within the framework of generalized\nlinear models (GLMs). With the upswing of data analytics, our study puts focus\non machine learning methods to develop full tariff plans built from both the\nfrequency and severity of claims. We adapt the loss functions used in the\nalgorithms such that the specific characteristics of insurance data are\ncarefully incorporated: highly unbalanced count data with excess zeros and\nvarying exposure on the frequency side combined with scarce, but potentially\nlong-tailed data on the severity side. A key requirement is the need for\ntransparent and interpretable pricing models which are easily explainable to\nall stakeholders. We therefore focus on machine learning with decision trees:\nstarting from simple regression trees, we work towards more advanced ensembles\nsuch as random forests and boosted trees. We show how to choose the optimal\ntuning parameters for these models in an elaborate cross-validation scheme, we\npresent visualization tools to obtain insights from the resulting models and\nthe economic value of these new modeling approaches is evaluated. Boosted trees\noutperform the classical GLMs, allowing the insurer to form profitable\nportfolios and to guard against potential adverse risk selection.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:36:49 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:23:52 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 22:03:03 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Henckaerts", "Roel", ""], ["C\u00f4t\u00e9", "Marie-Pier", ""], ["Antonio", "Katrien", ""], ["Verbelen", "Roel", ""]]}, {"id": "1904.10891", "submitter": "Christian Ghiaus", "authors": "L. Raillon (CETHIL), Christian Ghiaus (CETHIL)", "title": "An efficient Bayesian experimental calibration of dynamic thermal models", "comments": null, "journal-ref": "Energy, Elsevier, 2018, 152, pp.818 - 833", "doi": "10.1016/j.energy.2018.03.168", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental calibration of dynamic thermal models is required for model\npredictive control and characterization of building energy performance. In\nthese applications, the uncertainty assessment of the parameter estimates is\ndecisive; this is why a Bayesian calibration procedure (selection, calibration\nand validation) is presented. The calibration is based on an improved\nMetropolis-Hastings algorithm suitable for linear and Gaussian state-space\nmodels. The procedure, illustrated on a real house experiment, shows that the\nalgorithm is more robust to initial conditions than a maximum likelihood\noptimization with a quasi-Newton algorithm. Furthermore, when the data are not\ninformative enough, the use of prior distributions helps to regularize the\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 06:55:08 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Raillon", "L.", "", "CETHIL"], ["Ghiaus", "Christian", "", "CETHIL"]]}, {"id": "1904.10918", "submitter": "Jeremiah Zhe Liu", "authors": "Jeremiah Zhe Liu, Jane Lee, Pi-i Debby Lin, Linda Valeri, David C.\n  Christiani, David C. Bellinger, Robert O. Wright, Maitreyi M. Mazumdar, Brent\n  A. Coull", "title": "A Cross-validated Ensemble Approach to Robust Hypothesis Testing of\n  Continuous Nonlinear Interactions: Application to Nutrition-Environment\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-environment and nutrition-environment studies often involve testing of\nhigh-dimensional interactions between two sets of variables, each having\npotentially complex nonlinear main effects on an outcome. Construction of a\nvalid and powerful hypothesis test for such an interaction is challenging, due\nto the difficulty in constructing an efficient and unbiased estimator for the\ncomplex, nonlinear main effects. In this work we address this problem by\nproposing a Cross-validated Ensemble of Kernels (CVEK) that learns the space of\nappropriate functions for the main effects using a cross-validated ensemble\napproach. With a carefully chosen library of base kernels, CVEK flexibly\nestimates the form of the main-effect functions from the data, and encourages\ntest power by guarding against over-fitting under the alternative. The method\nis motivated by a study on the interaction between metal exposures in utero and\nmaternal nutrition on children's neurodevelopment in rural Bangladesh. The\nproposed tests identified evidence of an interaction between minerals and\nvitamins intake and arsenic and manganese exposures. Results suggest that the\ndetrimental effects of these metals are most pronounced at low intake levels of\nthe nutrients, suggesting nutritional interventions in pregnant women could\nmitigate the adverse impacts of in utero metal exposures on children's\nneurodevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 16:50:19 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Liu", "Jeremiah Zhe", ""], ["Lee", "Jane", ""], ["Lin", "Pi-i Debby", ""], ["Valeri", "Linda", ""], ["Christiani", "David C.", ""], ["Bellinger", "David C.", ""], ["Wright", "Robert O.", ""], ["Mazumdar", "Maitreyi M.", ""], ["Coull", "Brent A.", ""]]}, {"id": "1904.10959", "submitter": "Samuel Asante Gyamerah", "authors": "Samuel Asante Gyamerah, Philip Ngare, Dennis Ikpe", "title": "Crop yield probability density forecasting via quantile random forest\n  and Epanechnikov Kernel function", "comments": "22 pages, 11 figures", "journal-ref": "Agricultural and Forest Meteorology, 280:107808 (2020)", "doi": "10.1016/j.ecolmodel.2014.01.030", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reliable and accurate forecasting model for crop yields is of crucial\nimportance for efficient decision-making process in the agricultural sector.\nHowever, due to weather extremes and uncertainties, most forecasting models for\ncrop yield are not reliable and accurate. For measuring the uncertainty and\nobtaining further information of future crop yields, a probability density\nforecasting model based on quantile random forest and Epanechnikov kernel\nfunction (QRF-SJ) is proposed. The nonlinear structure of random forest is\napplied to change the quantile regression model for building the probabilistic\nforecasting model. Epanechnikov kernel function and solve-the equation plug-in\napproach of Sheather and Jones are used in the kernel density estimation. A\ncase study using the annual crop yield of groundnut and millet in Ghana is\npresented to illustrate the efficiency and robustness of the proposed\ntechnique. The values of the prediction interval coverage probability and\nprediction interval normalized average width for the two crops show that the\nconstructed prediction intervals capture the observed yields with high coverage\nprobability. The probability density curves show that QRF-SJ method has a very\nhigh ability to forecast quality prediction intervals with a higher coverage\nprobability. The feature importance gave a score of the importance of each\nweather variable in building the quantile regression forest model. The farmer\nand other stakeholders are able to realize the specific weather variable that\naffect the yield of a selected crop through feature importance. The proposed\nmethod and its application on crop yield dataset are the first of its kind in\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:55:08 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 17:07:07 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gyamerah", "Samuel Asante", ""], ["Ngare", "Philip", ""], ["Ikpe", "Dennis", ""]]}, {"id": "1904.11006", "submitter": "Gwendolyn Eadie", "authors": "Gwendolyn Eadie, Daniela Huppenkothen, Aaron Springford, and Tyler\n  McCormick", "title": "Introducing Bayesian Analysis with $\\text{m&m's}^\\circledR$: an\n  active-learning exercise for undergraduates", "comments": "Accepted to the Journal of Statistics Education (in press); 15 pages,\n  7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT astro-ph.IM physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an active-learning strategy for undergraduates that applies\nBayesian analysis to candy-covered chocolate $\\text{m&m's}^\\circledR$. The\nexercise is best suited for small class sizes and tutorial settings, after\nstudents have been introduced to the concepts of Bayesian statistics. The\nexercise takes advantage of the non-uniform distribution of\n$\\text{m&m's}^\\circledR~$ colours, and the difference in distributions made at\ntwo different factories. In this paper, we provide the intended learning\noutcomes, lesson plan and step-by-step guide for instruction, and open-source\nteaching materials. We also suggest an extension to the exercise for the\ngraduate-level, which incorporates hierarchical Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:29:42 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Eadie", "Gwendolyn", ""], ["Huppenkothen", "Daniela", ""], ["Springford", "Aaron", ""], ["McCormick", "Tyler", ""]]}, {"id": "1904.11144", "submitter": "Jairo Fuquene", "authors": "Jairo F\\'uquene, Andryu Mendoza, Cesar Cristancho, Mariana Ospina", "title": "A Bayesian approach for small area population estimates using multiple\n  administrative records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area population estimates are useful for decision making in the private\nand public sectors. However, in small areas (i.e., those that are difficult to\nreach and with small population sizes) computing demographic quantities is\ncomplicated. Bayesian methods are an alternative for demographic population\nestimates which uses data from multiple administrative records. In this paper\nwe explore a Bayesian approach which is simple and flexible and represents an\nalternative procedure for base population estimates particularly powerful for\nintercensal periods. The applicability of the methodological procedure is\nillustrated using population pyramids in the municipality of Jamund\\'i in\nColombia.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:42:05 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 01:12:16 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 22:57:03 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["F\u00faquene", "Jairo", ""], ["Mendoza", "Andryu", ""], ["Cristancho", "Cesar", ""], ["Ospina", "Mariana", ""]]}, {"id": "1904.11156", "submitter": "Samuele Centorrino", "authors": "Raffaello Seri, Samuele Centorrino, Michele Bernasconi", "title": "Nonparametric Estimation and Inference in Economic and Psychological\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to provide some tools for nonparametric estimation\nand inference in psychological and economic experiments. We consider an\nexperimental framework in which each of $n$subjects provides $T$ responses to a\nvector of $T$ stimuli. We propose to estimate the unknown function $f$ linking\nstimuli to responses through a nonparametric sieve estimator. We give\nconditions for consistency when either $n$ or $T$ or both diverge. The rate of\nconvergence depends upon the error covariance structure, that is allowed to\ndiffer across subjects. With these results we derive the optimal divergence\nrate of the dimension of the sieve basis with both $n$ and $T$. We provide\nguidance about the optimal balance between the number of subjects and questions\nin a laboratory experiment and argue that a large $n$is often better than a\nlarge $T$. We derive conditions for asymptotic normality of functionals of the\nestimator of $T$ and apply them to obtain the asymptotic distribution of the\nWald test when the number of constraints under the null is finite and when it\ndiverges along with other asymptotic parameters. Lastly, we investigate the\nprevious properties when the conditional covariance matrix is replaced by an\nestimator.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 04:25:22 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 20:39:22 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 20:56:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Seri", "Raffaello", ""], ["Centorrino", "Samuele", ""], ["Bernasconi", "Michele", ""]]}, {"id": "1904.11258", "submitter": "Sumanta Kumar Das Dr", "authors": "Sumanta Kumar Das, Randhir Singh", "title": "Performance of Kriging Based Soft Classification on WiFS/IRS- 1D image\n  using Ground Hyperspectral Signatures", "comments": "5 pages,3 figures 3 tables", "journal-ref": "IEEE GEPSCIENCE AND REMOTE SENSING LETTERS, 2009, VOL 6, issue 3", "doi": "10.1109/LGRS.2008.2005851", "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard and soft classification techniques are the conventional ways of image\nclassification on satellite data. These classifiers have number of drawbacks.\nFirstly, these approaches are inappropriate for mixed pixels. Secondly, these\napproaches do not consider spatial variability. Kriging based soft classifier\n(KBSC) is a non-parametric geostatistical method. It exploits the spatial\nvariability of the classes within the image. This letter compares the\nperformance of KBSC with other conventional hard/soft classification\ntechniques. The satellite data used in this study is the Wide Field Sensor\n(WiFS) from the Indian Remote Sensing Satellite -1D (IRS-1D). The ground\nhyperspectral signatures acquired from the agricultural fields by a hand held\nspectroradiometer are used to detect subpixel targets from the satellite\nimages. Two measures of closeness have been used for accuracy assessment of the\nKBSC to that of the conventional classifications. The results prove that the\nKBSC is statistically more accurate than the other conventional techniques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:08:43 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Das", "Sumanta Kumar", ""], ["Singh", "Randhir", ""]]}, {"id": "1904.11280", "submitter": "Soumyabrata Dev", "authors": "Chidozie Shamrock Nwosu, Soumyabrata Dev, Peru Bhardwaj, Bharadwaj\n  Veeravalli, and Deepu John", "title": "Predicting Stroke from Electronic Health Records", "comments": "Published in Proc. 41st Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have identified various risk factors associated with the onset of\nstroke in an individual. Data mining techniques have been used to predict the\noccurrence of stroke based on these factors by using patients' medical records.\nHowever, there has been limited use of electronic health records to study the\ninter-dependency of different risk factors of stroke. In this paper, we perform\nan analysis of patients' electronic health records to identify the impact of\nrisk factors on stroke prediction. We also provide benchmark performance of the\nstate-of-art machine learning algorithms for predicting stroke using electronic\nhealth records.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:02:16 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Nwosu", "Chidozie Shamrock", ""], ["Dev", "Soumyabrata", ""], ["Bhardwaj", "Peru", ""], ["Veeravalli", "Bharadwaj", ""], ["John", "Deepu", ""]]}, {"id": "1904.11306", "submitter": "Jessi Cisewski-Kehe", "authors": "Jessi Cisewski-Kehe and Grant Weller and Chad Schafer", "title": "A Preferential Attachment Model for the Stellar Initial Mass Function", "comments": null, "journal-ref": "Electronic Journal of Statistics, 13(1), pp.1580-1607 (2019)", "doi": "10.1214/19-EJS1556", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate specification of a likelihood function is becoming increasingly\ndifficult in many inference problems in astronomy. As sample sizes resulting\nfrom astronomical surveys continue to grow, deficiencies in the likelihood\nfunction lead to larger biases in key parameter estimates. These deficiencies\nresult from the oversimplification of the physical processes that generated the\ndata, and from the failure to account for observational limitations.\nUnfortunately, realistic models often do not yield an analytical form for the\nlikelihood. The estimation of a stellar initial mass function (IMF) is an\nimportant example. The stellar IMF is the mass distribution of stars initially\nformed in a given cluster of stars, a population which is not directly\nobservable due to stellar evolution and other disruptions and observational\nlimitations of the cluster. There are several difficulties with specifying a\nlikelihood in this setting since the physical processes and observational\nchallenges result in measurable masses that cannot legitimately be considered\nindependent draws from an IMF. This work improves inference of the IMF by using\nan approximate Bayesian computation approach that both accounts for\nobservational and astrophysical effects and incorporates a physically-motivated\nmodel for star cluster formation. The methodology is illustrated via a\nsimulation study, demonstrating that the proposed approach can recover the true\nposterior in realistic situations, and applied to observations from\nastrophysical simulation data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:56:30 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cisewski-Kehe", "Jessi", ""], ["Weller", "Grant", ""], ["Schafer", "Chad", ""]]}, {"id": "1904.11430", "submitter": "Raiden Hasegawa", "authors": "Raiden B. Hasegawa, Dylan S. Small, and Daniel W Webster", "title": "Bracketing in the Comparative Interrupted Time-Series Design to Address\n  Concerns about History Interacting with Group: Evaluating Missouri Handgun\n  Purchaser Law", "comments": null, "journal-ref": "Epidemiology, Volume 30, Issue 3, p.371-379, 2019", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the comparative interrupted time series design (also called the method of\ndifference-in-differences), the change in outcome in a group exposed to\ntreatment in the periods before and after the exposure is compared to the\nchange in outcome in a control group not exposed to treatment in either period.\nThe standard difference-in-difference estimator for a comparative interrupted\ntime series design will be biased for estimating the causal effect of the\ntreatment if there is an interaction between history in the after period and\nthe groups; for example, there is a historical event besides the start of the\ntreatment in the after period that benefits the treated group more than the\ncontrol group. We present a bracketing method for bounding the effect of an\ninteraction between history and the groups that arises from a time-invariant\nunmeasured confounder having a different effect in the after period than the\nbefore period. The method is applied to a study of the effect of the repeal of\nMissouri's permit-to-purchase handgun law on its firearm homicide rate. We\nestimate that the effect of the permit-to-purchase repeal on Missouri's firearm\nhomicide rate is bracketed between 0.9 and 1.3 homicides per 100,000 people,\ncorresponding to a percentage increase of 17% to 27% (95% confidence interval:\n[0.6,1.7] or [11%,35%]). A placebo study provides additional support for the\nhypothesis that the repeal has a causal effect of increasing the rate of\nstate-wide firearm homicides.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:09:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hasegawa", "Raiden B.", ""], ["Small", "Dylan S.", ""], ["Webster", "Daniel W", ""]]}, {"id": "1904.11603", "submitter": "Federico Ferrari", "authors": "Federico Ferrari and David B Dunson", "title": "Bayesian Factor Analysis for Inference on Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by the problem of inference on interactions among\nchemical exposures impacting human health outcomes. Chemicals often co-occur in\nthe environment or in synthetic mixtures and as a result exposure levels can be\nhighly correlated. We propose a latent factor joint model, which includes\nshared factors in both the predictor and response components while assuming\nconditional independence. By including a quadratic regression in the latent\nvariables in the response component, we induce flexible dimension reduction in\ncharacterizing main effects and interactions. We propose a Bayesian approach to\ninference under this Factor analysis for INteractions (FIN) framework. Through\nappropriate modifications of the factor modeling structure, FIN can accommodate\nhigher order interactions and multivariate outcomes. We provide theory on\nposterior consistency and the impact of misspecifying the number of factors. We\nevaluate the performance using a simulation study and data from the National\nHealth and Nutrition Examination Survey (NHANES). Code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:50:54 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 09:02:13 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ferrari", "Federico", ""], ["Dunson", "David B", ""]]}, {"id": "1904.11678", "submitter": "Sumanta Kumar Das Dr", "authors": "Sumanta kumar das and R. S. Singh", "title": "Performance modeling of electro-optical devices for military target\n  acquisition", "comments": "9 pages,4 tables, 5 figures", "journal-ref": "Defence science journal, vol 57, no. 3, may 2007", "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate predictions of electro-optical imager performance are important for\ndefence decision-making. The predictions serve as a guide for system\ndevelopment and are used in war game, simulations that directly influence\nengagement tactics. In the present study, mathematical models have been\ndeveloped which involves detection of different military targets using their\nopto-electronics properties in different environmental conditions. The method\nfirst calculates the signal-to-noise ratio received by the observing sensors\nreflected from the target by quantifying the light energy in terms of photons,\nwhich is used for evaluating the detection probability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 05:48:21 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["das", "Sumanta kumar", ""], ["Singh", "R. S.", ""]]}, {"id": "1904.11857", "submitter": "Manie Tadayon", "authors": "Manie Tadayon, Greg Pottie", "title": "Predicting Student Performance in an Educational Game Using a Hidden\n  Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributions: Prior studies on education have mostly followed the model of\nthe cross sectional study, namely, examining the pretest and the posttest\nscores. This paper shows that students' knowledge throughout the intervention\ncan be estimated by time series analysis using a hidden Markov model.\nBackground: Analyzing time series and the interaction between the students and\nthe game data can result in valuable information that cannot be gained by only\ncross sectional studies of the exams. Research Questions: Can a hidden Markov\nmodel be used to analyze the educational games? Can a hidden Markov model be\nused to make a prediction of the students' performance? Methodology: The study\nwas conducted on (N=854) students who played the Save Patch game. Students were\ndivided into class 1 and class 2. Class 1 students are those who scored lower\nin the test than class 2 students. The analysis is done by choosing various\nfeatures of the game as the observations. Findings: The state trajectories can\npredict the students' performance accurately for both class 1 and class 2.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:55:50 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Tadayon", "Manie", ""], ["Pottie", "Greg", ""]]}, {"id": "1904.11907", "submitter": "Stephanie Hicks", "authors": "Stephanie C. Hicks, Roger D. Peng", "title": "Evaluating the Success of a Data Analysis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A fundamental problem in the practice and teaching of data science is how to\nevaluate the quality of a given data analysis, which is different than the\nevaluation of the science or question underlying the data analysis. Previously,\nwe defined a set of principles for describing data analyses that can be used to\ncreate a data analysis and to characterize the variation between data analyses.\nHere, we introduce a metric of quality evaluation that we call the success of a\ndata analysis, which is different than other potential metrics such as\ncompleteness, validity, or honesty. We define a successful data analysis as the\nmatching of principles between the analyst and the audience on which the\nanalysis is developed. In this paper, we propose a statistical model and\ngeneral framework for evaluating the success of a data analysis. We argue that\nthis framework can be used as a guide for practicing data scientists and\nstudents in data science courses for how to build a successful data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:48:56 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Hicks", "Stephanie C.", ""], ["Peng", "Roger D.", ""]]}, {"id": "1904.12064", "submitter": "Adam Sykulski Dr", "authors": "Jeffrey J. Early and Adam M. Sykulski", "title": "Smoothing and Interpolating Noisy GPS Data with Smoothing Splines", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1175/JTECH-D-19-0087.1", "report-no": null, "categories": "stat.ME physics.data-an stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive methodology is provided for smoothing noisy, irregularly\nsampled data with non-Gaussian noise using smoothing splines. We demonstrate\nhow the spline order and tension parameter can be chosen a priori from physical\nreasoning. We also show how to allow for non-Gaussian noise and outliers which\nare typical in GPS signals. We demonstrate the effectiveness of our methods on\nGPS trajectory data obtained from oceanographic floating instruments known as\ndrifters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:28:58 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 23:14:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Early", "Jeffrey J.", ""], ["Sykulski", "Adam M.", ""]]}, {"id": "1904.12243", "submitter": "Andriy Olenko", "authors": "Phil Broadbridge, Alexander D. Kolesnik, Nikolai Leonenko, Andriy\n  Olenko", "title": "Random spherical hyperbolic diffusion", "comments": "30 pages, 15 figures. Updated file. Some misprints are corrected", "journal-ref": null, "doi": "10.1007/s10955-019-02395-0", "report-no": null, "categories": "stat.AP astro-ph.IM math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper starts by giving a motivation for this research and justifying the\nconsidered stochastic diffusion models for cosmic microwave background\nradiation studies. Then it derives the exact solution in terms of a series\nexpansion to a hyperbolic diffusion equation on the unit sphere. The Cauchy\nproblem with random initial conditions is studied. All assumptions are stated\nin terms of the angular power spectrum of the initial conditions. An\napproximation to the solution is given and analysed by finitely truncating the\nseries expansion. The upper bounds for the convergence rates of the\napproximation errors are derived. Smoothness properties of the solution and its\napproximation are investigated. It is demonstrated that the sample H\\\"older\ncontinuity of these spherical fields is related to the decay of the angular\npower spectrum. Numerical studies of approximations to the solution and\napplications to cosmic microwave background data are presented to illustrate\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:15:04 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 12:08:46 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Broadbridge", "Phil", ""], ["Kolesnik", "Alexander D.", ""], ["Leonenko", "Nikolai", ""], ["Olenko", "Andriy", ""]]}, {"id": "1904.12417", "submitter": "Ander Wilson", "authors": "Ander Wilson, Hsiao-Hsien Leon Hsu, Yueh-Hsiu Mathilda Chiu, Robert O.\n  Wright, Rosalind J. Wright, Brent A. Coull", "title": "Kernel Machine and Distributed Lag Models for Assessing Windows of\n  Susceptibility to Environmental Mixtures in Children's Health Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposures to environmental chemicals during gestation can alter health status\nlater in life. Most studies of maternal exposure to chemicals during pregnancy\nfocus on a single chemical exposure observed at high temporal resolution.\nRecent research has turned to focus on exposure to mixtures of multiple\nchemicals, generally observed at a single time point. We consider statistical\nmethods for analyzing data on chemical mixtures that are observed at a high\ntemporal resolution. As motivation, we analyze the association between exposure\nto four ambient air pollutants observed weekly throughout gestation and birth\nweight in a Boston-area prospective birth cohort. To explore patterns in the\ndata, we first apply methods for analyzing data on (1) a single chemical\nobserved at high temporal resolution, and (2) a mixture measured at a single\npoint in time. We highlight the shortcomings of these approaches for\ntemporally-resolved data on exposure to chemical mixtures. Second, we propose a\nnovel method, a Bayesian kernel machine regression distributed lag model\n(BKMR-DLM), that simultaneously accounts for nonlinear associations and\ninteractions among time-varying measures of exposure to mixtures. BKMR-DLM uses\na functional weight for each exposure that parameterizes the window of\nsusceptibility corresponding to that exposure within a kernel machine framework\nthat captures non-linear and interaction effects of the multivariate exposure\non the outcome. In a simulation study, we show that the proposed method can\nbetter estimate the exposure-response function and, in high signal settings,\ncan identify critical windows in time during which exposure has an increased\nassociation with the outcome. Applying the proposed method to the Boston birth\ncohort data, we found evidence of a negative association between organic carbon\nand birth weight and that nitrate modifies the organic carbon, elemental\ncarbon...\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 01:33:08 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 14:21:04 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 22:34:26 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 04:48:19 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 03:03:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wilson", "Ander", ""], ["Hsu", "Hsiao-Hsien Leon", ""], ["Chiu", "Yueh-Hsiu Mathilda", ""], ["Wright", "Robert O.", ""], ["Wright", "Rosalind J.", ""], ["Coull", "Brent A.", ""]]}, {"id": "1904.12590", "submitter": "Abhishek Shah", "authors": "Abhishek Shah, Laurent Bertino, Francois Counillon, Mohamad El\n  Gharamti and Jiping Xie", "title": "Assimilation of semi-qualitative sea ice thickness data with the EnKF-SQ", "comments": "24 pages, 11 figures, research article", "journal-ref": null, "doi": "10.1080/16000870.2019.1697166", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A newly introduced stochastic data assimilation method, the Ensemble Kalman\nFilter Semi-Qualitative (EnKF-SQ) is applied to a realistic coupled ice-ocean\nmodel of the Arctic, the TOPAZ4 configuration, in a twin experiment framework.\nThe method is shown to add value to range-limited thin ice thickness\nmeasurements, as obtained from passive microwave remote sensing, with respect\nto more trivial solutions like neglecting the out-of-range values or\nassimilating climatology instead. Some known properties inherent to the EnKF-SQ\nare evaluated: the tendency to draw the solution closer to the thickness\nthreshold, the skewness of the resulting analysis ensemble and the potential\nappearance of outliers. The experiments show that none of these properties\nprove deleterious in light of the other sub-optimal characters of the sea ice\ndata assimilation system used here (non-linearities, non-Gaussian variables,\nlack of strong coupling). The EnKF-SQ has a single tuning parameter that is\nadjusted for best performance of the system at hand. The sensitivity tests\nreveal that the results do not depend critically on the choice of this tuning\nparameter. The EnKF-SQ makes overall a valid approach for assimilating\nsemi-qualitative observations into high-dimensional nonlinear systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:17:17 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Shah", "Abhishek", ""], ["Bertino", "Laurent", ""], ["Counillon", "Francois", ""], ["Gharamti", "Mohamad El", ""], ["Xie", "Jiping", ""]]}, {"id": "1904.12623", "submitter": "Semhar Michael", "authors": "Damon Bayer and Semhar Michael", "title": "Exploring the Daschle Collection using Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A U.S. Senator from South Dakota donated documents that were accumulated\nduring his service as a house representative and senator to be housed at the\nBridges library at South Dakota State University. This project investigated the\nutility of quantitative statistical methods to explore some portions of this\nvast document collection. The available scanned documents and emails from\nconstituents are analyzed using natural language processing methods including\nthe Latent Dirichlet Allocation (LDA) model. This model identified major topics\nbeing discussed in a given collection of documents. Important events and\npopular issues from the Senator Daschles career are reflected in the changing\ntopics from the model. These quantitative statistical methods provide a summary\nof the massive amount of text without requiring significant human effort or\ntime and can be applied to similar collections.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:31:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bayer", "Damon", ""], ["Michael", "Semhar", ""]]}, {"id": "1904.12651", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg, Sergej Sizov", "title": "Computational Approaches to Access Probabilistic Population Codes for\n  Higher Cognition an Decision-Making", "comments": "arXiv admin note: text overlap with arXiv:1804.10861", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research unveiled more and more evidence for the so-called\nBayesian Brain Paradigm, i.e. the human brain is interpreted as a probabilistic\ninference machine and Bayesian modelling approaches are hence used\nsuccessfully. One of the many theories is that of Probabilistic Population\nCodes (PPC). Although this model has so far only been considered as meaningful\nand useful for sensory perception as well as motor control, it has always been\nsuggested that this mechanism also underlies higher cognition and\ndecision-making. However, the adequacy of PPC for this regard cannot be\nconfirmed by means of neurological standard measurement procedures.\n  In this article we combine the parallel research branches of recommender\nsystems and predictive data mining with theoretical neuroscience. The nexus of\nboth fields is given by behavioural variability and resulting internal\ndistributions. We adopt latest experimental settings and measurement approaches\nfrom predictive data mining to obtain these internal distributions, to inform\nthe theoretical PPC approach and to deduce medical correlates which can indeed\nbe measured in vivo. This is a strong hint for the applicability of the PPC\napproach and the Bayesian Brain Paradigm for higher cognition and human\ndecision-making.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:32:02 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1904.12652", "submitter": "Azam Yazdani", "authors": "Azam Yazdani, Akram Yazdani, Sarah H. Elsea, Daniel J. Schaid, Michael\n  R. Kosorok, Gita Dangol, Ahmad Samiei", "title": "Genome analysis and pleiotropy assessment using causal networks with\n  loss of function mutation and metabolomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many genome-wide association studies have detected genomic\nregions associated with traits, yet understanding the functional causes of\nassociation often remains elusive. Utilizing systems approaches and focusing on\nintermediate molecular phenotypes might facilitate biologic understanding.\nResults: The availability of exome sequencing of two populations of\nAfrican-Americans and European-Americans from the Atherosclerosis Risk in\nCommunities study allowed us to investigate the effects of annotated\nloss-of-function (LoF) mutations on 122 serum metabolites. To assess the\nfindings, we built metabolomic causal networks for each population separately\nand utilized structural equation modeling. We then validated our findings with\na set of independent samples. By use of methods based on concepts of Mendelian\nrandomization of genetic variants, we showed that some of the affected\nmetabolites are risk predictors in the causal pathway of disease. For example,\nLoF mutations in the gene KIAA1755 were identified to elevate the levels of\neicosapentaenoate (p-value=5E-14), an essential fatty acid clinically\nidentified to increase essential hypertension. We showed that this gene is in\nthe pathway to triglycerides, where both triglycerides and essential\nhypertension are risk factors of metabolomic disorder and heart attack. We also\nidentified that the gene CLDN17, harboring loss-of-function mutations, had\npleiotropic actions on metabolites from amino acid and lipid pathways.\nConclusion: Using systems biology approaches for the analysis of metabolomics\nand genetic data, we integrated several biological processes, which lead to\nfindings that may functionally connect genetic variants with complex diseases.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:45:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yazdani", "Azam", ""], ["Yazdani", "Akram", ""], ["Elsea", "Sarah H.", ""], ["Schaid", "Daniel J.", ""], ["Kosorok", "Michael R.", ""], ["Dangol", "Gita", ""], ["Samiei", "Ahmad", ""]]}, {"id": "1904.12973", "submitter": "Gunnar R\\\"atsch", "authors": "Stefan G. Stark, Stephanie L. Hyland, Melanie F. Pradier, Kjong\n  Lehmann, Andreas Wicki, Fernando Perez Cruz, Julia E. Vogt, Gunnar R\\\"atsch", "title": "Unsupervised Extraction of Phenotypes from Cancer Clinical Notes for\n  Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent adoption of Electronic Health Records (EHRs) by health care\nproviders has introduced an important source of data that provides detailed and\nhighly specific insights into patient phenotypes over large cohorts. These\ndatasets, in combination with machine learning and statistical approaches,\ngenerate new opportunities for research and clinical care. However, many\nmethods require the patient representations to be in structured formats, while\nthe information in the EHR is often locked in unstructured texts designed for\nhuman readability. In this work, we develop the methodology to automatically\nextract clinical features from clinical narratives from large EHR corpora\nwithout the need for prior knowledge. We consider medical terms and sentences\nappearing in clinical narratives as atomic information units. We propose an\nefficient clustering strategy suitable for the analysis of large text corpora\nand to utilize the clusters to represent information about the patient\ncompactly. To demonstrate the utility of our approach, we perform an\nassociation study of clinical features with somatic mutation profiles from\n4,007 cancer patients and their tumors. We apply the proposed algorithm to a\ndataset consisting of about 65 thousand documents with a total of about 3.2\nmillion sentences. We identify 341 significant statistical associations between\nthe presence of somatic mutations and clinical features. We annotated these\nassociations according to their novelty, and report several known associations.\nWe also propose 32 testable hypotheses where the underlying biological\nmechanism does not appear to be known but plausible. These results illustrate\nthat the automated discovery of clinical features is possible and the joint\nanalysis of clinical and genetic datasets can generate appealing new\nhypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:15:22 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:13:56 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Stark", "Stefan G.", ""], ["Hyland", "Stephanie L.", ""], ["Pradier", "Melanie F.", ""], ["Lehmann", "Kjong", ""], ["Wicki", "Andreas", ""], ["Cruz", "Fernando Perez", ""], ["Vogt", "Julia E.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1904.12981", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Wentian Guo and Yuan Ji", "title": "PoD-TPI: Probability-of-Decision Toxicity Probability Interval Design to\n  Accelerate Phase I Trials", "comments": null, "journal-ref": null, "doi": "10.1007/s12561-019-09264-0", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohort-based enrollment can slow down dose-finding trials since the outcomes\nof the previous cohort must be fully evaluated before the next cohort can be\nenrolled. This results in frequent suspension of patient enrollment. The issue\nis exacerbated in recent immune-oncology trials where toxicity outcomes can\ntake a long time to observe. We propose a novel phase I design, the\nprobability-of-decision toxicity probability interval (PoD-TPI) design, to\naccelerate phase I trials. PoD-TPI enables dose assignment in real-time in the\npresence of pending toxicity outcomes. With uncertain outcomes, the dose\nassignment decisions are treated as a random variable, and we calculate the\nposterior distribution of the decisions. The posterior distribution reflects\nthe variability in the pending outcomes and allows a direct and intuitive\nevaluation of the confidence of all possible decisions. Optimal decisions are\ncalculated based on 0-1 loss, and extra safety rules are constructed to enforce\nsufficient protection from exposing patients to risky doses. A new and useful\nfeature of PoD-TPI is that it allows investigators and regulators to balance\nthe trade-off between enrollment speed and making risky decisions by tuning a\npair of intuitive design parameters. Through numerical studies, we evaluate the\noperating characteristics of PoD-TPI and demonstrate that PoD-TPI shortens\ntrial duration and maintains trial safety and efficiency compared to existing\ntime-to-event designs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:49:46 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 19:13:07 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhou", "Tianjian", ""], ["Guo", "Wentian", ""], ["Ji", "Yuan", ""]]}, {"id": "1904.13046", "submitter": "Yuguang Ipsen", "authors": "Yuguang F. Ipsen and Soudabeh Shemehsavar and Ross A. Maller", "title": "Models for Genetic Diversity Generated by Negative Binomial Point\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model based on a generalised Poisson-Dirichlet distribution for\nthe analysis of genetic diversity, and illustrate its use on microsatellite\ndata for the genus Dasyurus (the quoll, a marsupial carnivore listed as\nnear-threatened in Australia). Our class of distributions, termed\n$PD_\\alpha^{(r)}$, is constructed from a negative binomial point process,\ngeneralizing the usual one-parameter $PD_\\alpha$ model, which is constructed\nfrom a Poisson point process. Both models have at their heart a\nStable$(\\alpha)$ process, but in $PD_\\alpha^{(r)}$, an extra parameter $r>0$\nadds flexibility, analogous to the way the negative binomial distribution\nallows for \"overdispersion\" in the analysis of count data. A key result\nobtained is a generalised version of Ewens' sampling formula for\n$PD_\\alpha^{(r)}$. We outline the theoretical basis for the model, and, for the\nquolls data, estimate the parameters $\\alpha$ and r by least squares, showing\nhow the extra parameter r aids in the interpretability of the data by\ncomparison with the standard $PD_\\alpha$ model. The methods potentially have\nimplications for the management and conservation of threatened populations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 04:48:15 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ipsen", "Yuguang F.", ""], ["Shemehsavar", "Soudabeh", ""], ["Maller", "Ross A.", ""]]}, {"id": "1904.13083", "submitter": "Katharina Gruber", "authors": "Katharina Gruber, Claude Kloeckl, Peter Regner, Johann Baumgartner,\n  Johannes Schmidt", "title": "Assessing the Global Wind Atlas and local measurements for bias\n  correction of wind power generation simulated from MERRA-2 in Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NASAs MERRA-2 reanalysis is a widely used dataset in renewable energy\nresource modelling. The Global Wind Atlas (GWA) has been used to bias-correct\nMERRA-2 data before. There is, however, a lack of an analysis of the\nperformance of MERRA-2 with bias correction from GWA on different spatial\nlevels - and for regions outside of Europe, China or the United States. This\nstudy therefore evaluates different methods for wind power simulation on four\nspatial resolution levels from wind park to national level in Brazil. In\nparticular, spatial interpolation methods and spatial as well as spatiotemporal\nwind speed bias correction using local wind speed measurements and mean wind\nspeeds from the GWA are assessed. By validating the resulting timeseries\nagainst observed generation it is assessed at which spatial levels the\ndifferent methods improve results - and whether global information derived from\nthe GWA can compete with locally measured wind speed data as a source of bias\ncorrection. Results show that (i) bias correction with the GWA improves results\non state, sub-system, and national-level, but not on wind park level, that (ii)\nthe GWA improves results comparably to local measurements, and that (iii)\ncomplex spatial interpolation methods do not contribute in improving quality of\nthe simulation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:32:35 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 09:29:01 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Gruber", "Katharina", ""], ["Kloeckl", "Claude", ""], ["Regner", "Peter", ""], ["Baumgartner", "Johann", ""], ["Schmidt", "Johannes", ""]]}, {"id": "1904.13194", "submitter": "Gregor Zens", "authors": "Gregor Zens, Maximilian B\\\"ock", "title": "A Factor-Augmented Markov Switching (FAMS) Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the role of high-dimensional information sets in the\ncontext of Markov switching models with time varying transition probabilities.\nMarkov switching models are commonly employed in empirical macroeconomic\nresearch and policy work. However, the information used to model the switching\nprocess is usually limited drastically to ensure stability of the model.\nIncreasing the number of included variables to enlarge the information set\nmight even result in decreasing precision of the model. Moreover, it is often\nnot clear a priori which variables are actually relevant when it comes to\ninforming the switching behavior. Building strongly on recent contributions in\nthe field of factor analysis, we introduce a general type of Markov switching\nautoregressive models for non-linear time series analysis. Large numbers of\ntime series are allowed to inform the switching process through a factor\nstructure. This factor-augmented Markov switching (FAMS) model overcomes\nestimation issues that are likely to arise in previous assessments of the\nmodeling framework. More accurate estimates of the switching behavior as well\nas improved model fit result. The performance of the FAMS model is illustrated\nin a simulated data example as well as in an US business cycle application.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:44:05 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 18:18:04 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zens", "Gregor", ""], ["B\u00f6ck", "Maximilian", ""]]}]