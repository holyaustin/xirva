[{"id": "1404.0058", "submitter": "Raffi Sevlian", "authors": "Raffi Sevlian and Ram Rajagopal", "title": "Short Term Electricity Load Forecasting on Varying Levels of Aggregation", "comments": "Significant changes from previous version. Extension to full day\n  ahead forecasting, added appendix of methodologies and scaling equality (not\n  previous upper bound). Under review International Journal of Power and Energy\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple empirical scaling law that describes load forecasting\naccuracy at different levels of aggregation. The model is justified based on a\nsimple decomposition of individual consumption patterns. We show that for\ndifferent forecasting methods and horizons, aggregating more customers improves\nthe relative forecasting performance up to specific point. Beyond this point,\nno more improvement in relative performance can be obtained.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 22:46:34 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 01:21:04 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 19:08:29 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Sevlian", "Raffi", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1404.0122", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and G\\'abor J. Sz\\'ekely and Uri Ascher", "title": "Assessing stochastic algorithms for large scale nonlinear least squares\n  problems using extremal probabilities of linear combinations of gamma random\n  variables", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification. 3 (2015) 61-90", "doi": "10.1137/14096311X", "report-no": null, "categories": "math.NA cs.NA math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers stochastic algorithms for efficiently solving a class\nof large scale non-linear least squares (NLS) problems which frequently arise\nin applications. We propose eight variants of a practical randomized algorithm\nwhere the uncertainties in the major stochastic steps are quantified. Such\nstochastic steps involve approximating the NLS objective function using\nMonte-Carlo methods, and this is equivalent to the estimation of the trace of\ncorresponding symmetric positive semi-definite (SPSD) matrices. For the latter,\nwe prove tight necessary and sufficient conditions on the sample size (which\ntranslates to cost) to satisfy the prescribed probabilistic accuracy. We show\nthat these conditions are practically computable and yield small sample sizes.\nThey are then incorporated in our stochastic algorithm to quantify the\nuncertainty in each randomized step. The bounds we use are applications of more\ngeneral results regarding extremal tail probabilities of linear combinations of\ngamma distributed random variables. We derive and prove new results concerning\nthe maximal and minimal tail probabilities of such linear combinations, which\ncan be considered independently of the rest of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 04:25:42 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 02:32:28 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Sz\u00e9kely", "G\u00e1bor J.", ""], ["Ascher", "Uri", ""]]}, {"id": "1404.0200", "submitter": "Andreas Veit", "authors": "Andreas Veit, Christoph Goebel, Rohit Tidke, Christoph Doblander and\n  Hans-Arno Jacobsen", "title": "Household Electricity Demand Forecasting -- Benchmarking\n  State-of-the-Art Methods", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of renewable energy sources with variable output, such as\nsolar photovoltaic and wind power generation, calls for Smart Grids that\neffectively manage flexible loads and energy storage. The ability to forecast\nconsumption at different locations in distribution systems will be a key\ncapability of Smart Grids. The goal of this paper is to benchmark\nstate-of-the-art methods for forecasting electricity demand on the household\nlevel across different granularities and time scales in an explorative way,\nthereby revealing potential shortcomings and find promising directions for\nfuture research in this area. We apply a number of forecasting methods\nincluding ARIMA, neural networks, and exponential smoothening using several\nstrategies for training data selection, in particular day type and sliding\nwindow based strategies. We consider forecasting horizons ranging between 15\nminutes and 24 hours. Our evaluation is based on two data sets containing the\npower usage of individual appliances at second time granularity collected over\nthe course of several months. The results indicate that forecasting accuracy\nvaries significantly depending on the choice of forecasting methods/strategy\nand the parameter configuration. Measured by the Mean Absolute Percentage Error\n(MAPE), the considered state-of-the-art forecasting methods rarely beat\ncorresponding persistence forecasts. Overall, we observed MAPEs in the range\nbetween 5 and >100%. The average MAPE for the first data set was ~30%, while it\nwas ~85% for the other data set. These results show big room for improvement.\nBased on the identified trends and experiences from our experiments, we\ncontribute a detailed discussion of promising future research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 11:32:53 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Veit", "Andreas", ""], ["Goebel", "Christoph", ""], ["Tidke", "Rohit", ""], ["Doblander", "Christoph", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1404.0229", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Irene Matuonto, Mirella Pontello", "title": "A randomized most powerful test to detect a cheater's action. Applicaton\n  to identification of listeriosis in Lombardy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new randomized non-parametric test based on a sample\nof independent but not identically distributed variables; this test detects if\na cheater replaces one of the distributions of the sample with a\nconvex-dominating one. The presented test is the uniformely most powerful, in\nthe sense that it is the most powerful for any change of the cheater. We show\nthat this test may be applied when we have variables with distribution\nsatisfying the monotone likelihood ratio property and we need to check whether\na parameter of a variable has been changed. The application we present concerns\nthe detection of epidemics of listeriosis in Lombardy from 2005 to 2011.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 13:18:52 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 14:01:15 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Aletti", "Giacomo", ""], ["Matuonto", "Irene", ""], ["Pontello", "Mirella", ""]]}, {"id": "1404.0734", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Harris Jaffee, Michael Rosenblum", "title": "interAdapt -- An Interactive Tool for Designing and Evaluating\n  Randomized Trials with Adaptive Enrollment Criteria", "comments": "14 pages, 2 figures (software screenshots); v2 includes command line\n  function description", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interAdapt R package is designed to be used by statisticians and clinical\ninvestigators to plan randomized trials. It can be used to determine if certain\nadaptive designs offer tangible benefits compared to standard designs, in the\ncontext of investigators' specific trial goals and constraints. Specifically,\ninterAdapt compares the performance of trial designs with adaptive enrollment\ncriteria versus standard (non-adaptive) group sequential trial designs.\nPerformance is compared in terms of power, expected trial duration, and\nexpected sample size. Users can either work directly in the R console, or with\na user-friendly shiny application that requires no programming experience.\nSeveral added features are available when using the shiny application. For\nexample, the application allows users to immediately download the results of\nthe performance comparison as a csv-table, or as a printable, html-based\nreport.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 23:16:18 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 16:24:56 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Fisher", "Aaron", ""], ["Jaffee", "Harris", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1404.1371", "submitter": "Hai Shu", "authors": "Hai Shu, Bin Nan, Robert Koeppe", "title": "Multiple Testing for Neuroimaging via Hidden Markov Random Field", "comments": "A MATLAB package implementing the proposed FDR procedure is available\n  with this paper at the Biometrics website on Wiley Online Library", "journal-ref": "Biometrics, 71(3), pp.741-750 (2015)", "doi": "10.1111/biom.12329", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional voxel-level multiple testing procedures in neuroimaging, mostly\n$p$-value based, often ignore the spatial correlations among neighboring voxels\nand thus suffer from substantial loss of power. We extend the\nlocal-significance-index based procedure originally developed for the hidden\nMarkov chain models, which aims to minimize the false nondiscovery rate subject\nto a constraint on the false discovery rate, to three-dimensional neuroimaging\ndata using a hidden Markov random field model. A generalized\nexpectation-maximization algorithm for maximizing the penalized likelihood is\nproposed for estimating the model parameters. Extensive simulations show that\nthe proposed approach is more powerful than conventional false discovery rate\nprocedures. We apply the method to the comparison between mild cognitive\nimpairment, a disease status with increased risk of developing Alzheimer's or\nanother dementia, and normal controls in the FDG-PET imaging study of the\nAlzheimer's Disease Neuroimaging Initiative.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 20:00:05 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 05:23:39 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Shu", "Hai", ""], ["Nan", "Bin", ""], ["Koeppe", "Robert", ""]]}, {"id": "1404.1495", "submitter": "Alexander Kushpel", "authors": "Alexander Kushpel", "title": "Pricing of basket options II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximation of density functions which is\nimportant in the theory of pricing of basket options. Our method is well\nadopted to the multidimensional case. Observe that implementations of\npolynomial and spline approximation in this situation are connected with\ndifficulties of fundamental nature. A simple approximation formula for European\ncall options is presented. It is shown that this approximation formula has\nexponential rate of convergence.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 18:05:34 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kushpel", "Alexander", ""]]}, {"id": "1404.1710", "submitter": "Dimitris Fouskakis", "authors": "Dimitris Fouskakis, George Petrakos and Ioannis Vavouras", "title": "A Bayesian Hierarchical Model for Comparative Evaluation of Teaching\n  Quality Indicators in Higher Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem motivating the paper is the quantification of students'\npreferences regarding teaching/coursework quality, under certain numerical\nrestrictions, in order to build a model for identifying, assessing and\nmonitoring the major components of the overall academic quality. After\nreviewing the strengths and limitations of conjoint analysis and of the random\ncoefficient regression model used in similar problems in the past, we propose a\nBayesian beta regression model with a Dirichlet prior on the model\ncoefficients. This approach not only allows for the incorporation of\ninformative prior when it is available but also provides user friendly\ninterfaces and direct probability interpretations for all quantities.\nFurthermore, it is a natural way to implement the usual constraints for the\nmodel weights/coefficients. This model was applied to data collected in 2009\nand 2013 from undergraduate students in Panteion University, Athens, Greece and\nbesides the construction of an instrument for the assessment and monitoring of\nteaching quality, it gave some input for a preliminary discussion on the\nassociation of the differences in students preferences between the two time\nperiods with the current Greek economic and financial crisis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 09:44:35 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Petrakos", "George", ""], ["Vavouras", "Ioannis", ""]]}, {"id": "1404.2015", "submitter": "Kyungchul Song", "authors": "Denis Kojevnikov and Kyungchul Song", "title": "Econometric Inference on Large Bayesian Games with Heterogeneous Beliefs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Econometric models of strategic interactions among people or firms have\nreceived a great deal of attention in the literature. Less attention has been\npaid to the role of the underlying assumptions about the way agents form\nbeliefs about other agents. This paper focuses on a single large Bayesian game\nand develops a bootstrap inference method that relaxes the assumption of\nrational expectations and allows for the players to form beliefs differently\nfrom each other. By drawing on the main intuition of Kalai(2004), we introduce\nthe notion of a hindsight regret, which measures each player's ex post value of\nother players' type information, and obtain its belief-free bound. Using this\nbound, we derive testable implications and develop a bootstrap inference\nprocedure for the structural parameters. We demonstrate the finite sample\nperformance of the method through Monte Carlo simulations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 05:53:29 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 22:47:02 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kojevnikov", "Denis", ""], ["Song", "Kyungchul", ""]]}, {"id": "1404.2063", "submitter": "Pedro Lind", "authors": "Pedro G. Lind, Matthias W\\\"achter, Joachim Peinke", "title": "Reconstructing the intermittent dynamics of the torque in wind turbines", "comments": "8 pages, 6 figures, for Conference paper of TORQUE 2014 proceedings", "journal-ref": null, "doi": "10.1088/1742-6596/524/1/012179", "report-no": null, "categories": "physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a framework introduced in the late nineties to analyze load\nmeasurements in off-shore wind energy converters (WEC). The framework is\nborrowed from statistical physics and properly adapted to the analysis of\nmultivariate data comprising wind velocity, power production and torque\nmeasurements, taken at one single WEC. In particular, we assume that wind\nstatistics drives the fluctuations of the torque produced in the wind turbine\nand show how to extract an evolution equation of the Langevin type for the\ntorque driven by the wind velocity. It is known that the intermittent nature of\nthe atmosphere, i.e. of the wind field, is transferred to the power production\nof a wind energy converter and consequently to the shaft torque. We show that\nthe derived stochastic differential equation quantifies the dynamical coupling\nof the measured fluctuating properties as well as it reproduces the\nintermittency observed in the data. Finally, we discuss our approach in the\nlight of turbine monitoring, a particular important issue in off-shore wind\nfarms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 09:47:39 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Lind", "Pedro G.", ""], ["W\u00e4chter", "Matthias", ""], ["Peinke", "Joachim", ""]]}, {"id": "1404.2189", "submitter": "Julian Wolfson", "authors": "Sunayan Bandyopadhyay, Julian Wolfson, David M. Vock, Gabriela\n  Vazquez-Benitez, Gediminas Adomavicius, Mohamed Elidrisi, Paul E. Johnson,\n  and Patrick J. O'Connor", "title": "Data mining for censored time-to-event data: A Bayesian network model\n  for predicting cardiovascular risk from electronic health record data", "comments": "31 pages (including references), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for predicting the risk of cardiovascular events based on individual\npatient characteristics are important tools for managing patient care. Most\ncurrent and commonly used risk prediction models have been built from carefully\nselected epidemiological cohorts. However, the homogeneity and limited size of\nsuch cohorts restricts the predictive power and generalizability of these risk\nmodels to other populations. Electronic health data (EHD) from large health\ncare systems provide access to data on large, heterogeneous, and\ncontemporaneous patient populations. The unique features and challenges of EHD,\nincluding missing risk factor information, non-linear relationships between\nrisk factors and cardiovascular event outcomes, and differing effects from\ndifferent patient subgroups, demand novel machine learning approaches to risk\nmodel development. In this paper, we present a machine learning approach based\non Bayesian networks trained on EHD to predict the probability of having a\ncardiovascular event within five years. In such data, event status may be\nunknown for some individuals as the event time is right-censored due to\ndisenrollment and incomplete follow-up. Since many traditional data mining\nmethods are not well-suited for such data, we describe how to modify both\nmodelling and assessment techniques to account for censored observation times.\nWe show that our approach can lead to better predictive performance than the\nCox proportional hazards model (i.e., a regression-based approach commonly used\nfor censored, time-to-event data) or a Bayesian network with {\\em{ad hoc}}\napproaches to right-censoring. Our techniques are motivated by and illustrated\non data from a large U.S. Midwestern health care system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 15:51:10 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Bandyopadhyay", "Sunayan", ""], ["Wolfson", "Julian", ""], ["Vock", "David M.", ""], ["Vazquez-Benitez", "Gabriela", ""], ["Adomavicius", "Gediminas", ""], ["Elidrisi", "Mohamed", ""], ["Johnson", "Paul E.", ""], ["O'Connor", "Patrick J.", ""]]}, {"id": "1404.2405", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (- M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques, IMT), Paul Lema\\^itre (EDF R\\&D, INRIA Bordeaux -\n  Sud-Ouest)", "title": "A review on global sensitivity analysis methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter makes a review, in a complete methodological framework, of\nvarious global sensitivity analysis methods of model output. Numerous\nstatistical and probabilistic tools (regression, smoothing, tests, statistical\nlearning, Monte Carlo, \\ldots) aim at determining the model input variables\nwhich mostly contribute to an interest quantity depending on model output. This\nquantity can be for instance the variance of an output variable. Three kinds of\nmethods are distinguished: the screening (coarse sorting of the most\ninfluential inputs among a large number), the measures of importance\n(quantitative sensitivity indices) and the deep exploration of the model\nbehaviour (measuring the effects of inputs on their all variation range). A\nprogressive application methodology is illustrated on a scholar application. A\nsynthesis is given to place every method according to several axes, mainly the\ncost in number of model evaluations, the model complexity and the nature of\nbrought information.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 09:33:54 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Iooss", "Bertrand", "", "- M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques, IMT"], ["Lema\u00eetre", "Paul", "", "EDF R\\&D, INRIA Bordeaux -\n  Sud-Ouest"]]}, {"id": "1404.2462", "submitter": "Curtis Storlie", "authors": "Curtis Storlie, Blake Anderson, Scott Vander Wiel, Daniel Quist,\n  Curtis Hash, Nathan Brown", "title": "Stochastic identification of malware with dynamic traces", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS703 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 1-18", "doi": "10.1214/13-AOAS703", "report-no": "IMS-AOAS-AOAS703", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to malware classification is introduced based on analysis of\ninstruction traces that are collected dynamically from the program in question.\nThe method has been implemented online in a sandbox environment (i.e., a\nsecurity mechanism for separating running programs) at Los Alamos National\nLaboratory, and is intended for eventual host-based use, provided the issue of\nsampling the instructions executed by a given process without disruption to the\nuser can be satisfactorily addressed. The procedure represents an instruction\ntrace with a Markov chain structure in which the transition matrix, $\\mathbf\n{P}$, has rows modeled as Dirichlet vectors. The malware class (malicious or\nbenign) is modeled using a flexible spline logistic regression model with\nvariable selection on the elements of $\\mathbf {P}$, which are observed with\nerror. The utility of the method is illustrated on a sample of traces from\nmalware and nonmalware programs, and the results are compared to other leading\ndetection schemes (both signature and classification based). This article also\nhas supplementary materials available online.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 12:38:26 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Storlie", "Curtis", ""], ["Anderson", "Blake", ""], ["Wiel", "Scott Vander", ""], ["Quist", "Daniel", ""], ["Hash", "Curtis", ""], ["Brown", "Nathan", ""]]}, {"id": "1404.2477", "submitter": "Fan Yang", "authors": "Fan Yang, Scott A. Lorch, Dylan S. Small", "title": "Estimation of causal effects using instrumental variables with\n  nonignorable missing covariates: Application to effect of type of delivery\n  NICU on premature infants", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS699 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 48-73", "doi": "10.1214/13-AOAS699", "report-no": "IMS-AOAS-AOAS699", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how effective high-level NICUs (neonatal intensive care units\nthat have the capacity for sustained mechanical assisted ventilation and high\nvolume) are compared to low-level NICUs is important and valuable for both\nindividual mothers and for public policy decisions. The goal of this paper is\nto estimate the effect on mortality of premature babies being delivered in a\nhigh-level NICU vs. a low-level NICU through an observational study where there\nare unmeasured confounders as well as nonignorable missing covariates. We\nconsider the use of excess travel time as an instrumental variable (IV) to\ncontrol for unmeasured confounders. In order for an IV to be valid, we must\ncondition on confounders of the IV---outcome relationship, for example, month\nprenatal care started must be conditioned on for excess travel time to be a\nvalid IV. However, sometimes month prenatal care started is missing, and the\nmissingness may be nonignorable because it is related to the not fully measured\nmother's/infant's risk of complications. We develop a method to estimate the\ncausal effect of a treatment using an IV when there are nonignorable missing\ncovariates as in our data, where we allow the missingness to depend on the\nfully observed outcome as well as the partially observed compliance class,\nwhich is a proxy for the unmeasured risk of complications. A simulation study\nshows that under our nonignorable missingness assumption, the commonly used\nestimation methods, complete-case analysis and multiple imputation by chained\nequations assuming missingness at random, provide biased estimates, while our\nmethod provides approximately unbiased estimates. We apply our method to the\nNICU study and find evidence that high-level NICUs significantly reduce deaths\nfor babies of small gestational age, whereas for almost mature babies like 37\nweeks, the level of NICUs makes little difference. A sensitivity analysis is\nconducted to assess the sensitivity of our conclusions to key assumptions about\nthe missing covariates. The method we develop in this paper may be useful for\nmany observational studies facing similar issues of unmeasured confounders and\nnonignorable missing data as ours.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 13:21:47 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Yang", "Fan", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1404.2787", "submitter": "Andr\\'as L\\'aszl\\'o", "authors": "Andras Laszlo", "title": "Convergence and error propagation results on a linear iterative\n  unfolding method", "comments": "27 pages, 1 figure", "journal-ref": "SIAM/ASA Journal of Uncertainty Quantification 4 (2016) 1345", "doi": "10.1137/15M1035744", "report-no": null, "categories": "stat.AP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unfolding problems often arise in the context of statistical data analysis.\nSuch problematics occur when the probability distribution of a physical\nquantity is to be measured, but it is randomized (smeared) by some well\nunderstood process, such as a non-ideal detector response or a well described\nphysical phenomenon. In such case it is said that the original probability\ndistribution of interest is folded by a known response function. The\nreconstruction of the original probability distribution from the measured one\nis called unfolding. That technically involves evaluation of the non-bounded\ninverse of an integral operator over the space of L^1 functions, which is known\nto be an ill-posed problem. For the pertinent regularized operator inversion,\nwe propose a linear iterative formula and provide proof of convergence in a\nprobability theory context. Furthermore, we provide formulae for error\nestimates at finite iteration stopping order which are of utmost importance in\npractical applications: the approximation error, the propagated statistical\nerror, and the propagated systematic error can be quantified. The arguments are\nbased on the Riesz-Thorin theorem mapping the original L^1 problem to L^2\nspace, and subsequent application of ordinary L^2 spectral theory of operators.\nA library implementation in C of the algorithm along with corresponding error\npropagation is also provided. A numerical example also illustrates the method\nin operation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 12:58:39 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 20:18:12 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Laszlo", "Andras", ""]]}, {"id": "1404.2885", "submitter": "Tian-Shun Jiang", "authors": "Tian-Shun Jiang, Zachary Polizzi, Christopher Yuan", "title": "A Networks and Machine Learning Approach to Determine the Best College\n  Coaches of the 20th-21st Centuries", "comments": "18 pages, Submitted to the 2014 Mathematical Contest in Modeling\n  (MCM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to find the five best college sports coaches of past century\nfor three different sports. We decided to look at men's basketball, football,\nand baseball. We wanted to use an approach that could definitively determine\nteam skill from the games played, and then use a machine-learning algorithm to\ncalculate the correct coach skills for each team in a given year. We created a\nnetworks-based model to calculate team skill from historical game data. A\ndigraph was created for each year in each sport. Nodes represented teams, and\nedges represented a game played between two teams. The arrowhead pointed\ntowards the losing team. We calculated the team skill of each graph using a\nright-hand eigenvector centrality measure. This way, teams that beat good teams\nwill be ranked higher than teams that beat mediocre teams. The eigenvector\ncentrality rankings for most years were well correlated with tournament\nperformance and poll-based rankings. We assumed that the relationship between\ncoach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \\cdot P_s\n= T_s$. We then created a function to describe the probability that a given\nscore difference would occur based on player skill and coach skill. We\nmultiplied the probabilities of all edges in the network together to find the\nprobability that the correct network would occur with any given player skill\nand coach skill matrix. We was able to determine player skill as a function of\nteam skill and coach skill, eliminating the need to optimize two unknown\nmatrices. The top five coaches in each year were noted, and the top coach of\nall time was calculated by dividing the number of times that coach ranked in\nthe yearly top five by the years said coach had been active.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 22:04:53 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Jiang", "Tian-Shun", ""], ["Polizzi", "Zachary", ""], ["Yuan", "Christopher", ""]]}, {"id": "1404.2977", "submitter": "Frederic Pascal", "authors": "Joana Frontera-Pons and Frederic Pascal and Jean-Philippe Ovarlez", "title": "Adaptive non-Zero Mean Gaussian Detection and Application to\n  Hyperspectral Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical target detection schemes are usually obtained deriving the\nlikelihood ratio under Gaussian hypothesis and replacing the unknown background\nparameters by their estimates. In most applications, interference signals are\nassumed to be Gaussian with zero mean or with a known mean vector that can be\nremoved and with unknown covariance matrix. When mean vector is unknown, it has\nto be jointly estimated with the covariance matrix, as it is the case for\ninstance in hyperspectral imaging. In this paper, the adaptive versions of the\nclassical Matched Filter and the Normalized Matched Filter, as well as two\nversions of the Kelly detector are first derived and then are analyzed for the\ncase when the mean vector of the background is unknown. More precisely,\ntheoretical closed-form expressions for false-alarm regulation are derived and\nthe Constant False Alarm Rate property is pursued to allow the detector to be\nindependent of nuisance parameters. Finally, the theoretical contribution is\nvalidated through simulations and on real hyperspectral scenes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 01:42:39 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Frontera-Pons", "Joana", ""], ["Pascal", "Frederic", ""], ["Ovarlez", "Jean-Philippe", ""]]}, {"id": "1404.3008", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra and Asok K. Nanda", "title": "On Stochastic Comparison Between Two Coherent Systems", "comments": "Due to some crucial issues we want to withdraw this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic orders are very useful tool to compare the lifetimes of two\ncoherent systems. We show that, under certain conditions, a coherent system of\nused components performs better (worse) than a used coherent system with\nrespect to different stochastic orders. Some results on stochastic comparison\nbetween a coherent system of inactive components and an inactive coherent\nsystem are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 06:05:51 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 09:38:08 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Nanda", "Asok K.", ""]]}, {"id": "1404.3168", "submitter": "Mattia Ciollaro", "authors": "Mattia Ciollaro, Jessi Cisewski, Peter Freeman, Christopher Genovese,\n  Jing Lei, Ross O'Connell, Larry Wasserman", "title": "Functional Regression for Quasar Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lyman-alpha forest is a portion of the observed light spectrum of distant\ngalactic nuclei which allows us to probe remote regions of the Universe that\nare otherwise inaccessible. The observed Lyman-alpha forest of a quasar light\nspectrum can be modeled as a noisy realization of a smooth curve that is\naffected by a `damping effect' which occurs whenever the light emitted by the\nquasar travels through regions of the Universe with higher matter\nconcentration. To decode the information conveyed by the Lyman-alpha forest\nabout the matter distribution, we must be able to separate the smooth\n`continuum' from the noise and the contribution of the damping effect in the\nquasar light spectra. To predict the continuum in the Lyman-alpha forest, we\nuse a nonparametric functional regression model in which both the response and\nthe predictor variable (the smooth part of the damping-free portion of the\nspectrum) are function-valued random variables. We demonstrate that the\nproposed method accurately predicts the unobservable continuum in the\nLyman-alpha forest both on simulated spectra and real spectra. Also, we\nintroduce distribution-free prediction bands for the nonparametric functional\nregression model that have finite sample guarantees. These prediction bands,\ntogether with bootstrap-based confidence bands for the projection of the mean\ncontinuum on a fixed number of principal components, allow us to assess the\ndegree of uncertainty in the model predictions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 17:52:34 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Cisewski", "Jessi", ""], ["Freeman", "Peter", ""], ["Genovese", "Christopher", ""], ["Lei", "Jing", ""], ["O'Connell", "Ross", ""], ["Wasserman", "Larry", ""]]}, {"id": "1404.3533", "submitter": "Annamaria Guolo", "authors": "Annamaria Guolo, Cristiano Varin", "title": "Beta regression for time series analysis of bounded data, with\n  application to Canada Google${}^\\circledR$ Flu Trends", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS684 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 74-88", "doi": "10.1214/13-AOAS684", "report-no": "IMS-AOAS-AOAS684", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded time series consisting of rates or proportions are often encountered\nin applications. This manuscript proposes a practical approach to analyze\nbounded time series, through a beta regression model. The method allows the\ndirect interpretation of the regression parameters on the original response\nscale, while properly accounting for the heteroskedasticity typical of bounded\nvariables. The serial dependence is modeled by a Gaussian copula, with a\ncorrelation matrix corresponding to a stationary autoregressive and moving\naverage process. It is shown that inference, prediction, and control can be\ncarried out straightforwardly, with minor modifications to standard analysis of\nautoregressive and moving average models. The methodology is motivated by an\napplication to the influenza-like-illness incidence estimated by the\nGoogle${}^\\circledR$ Flu Trends project.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 10:48:25 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Guolo", "Annamaria", ""], ["Varin", "Cristiano", ""]]}, {"id": "1404.3560", "submitter": "Alberto Cassese", "authors": "Alberto Cassese, Michele Guindani, Mahlet G. Tadesse, Francesco\n  Falciani, Marina Vannucci", "title": "A hierarchical Bayesian model for inference of copy number variants and\n  their association to gene expression", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS705 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 148-175", "doi": "10.1214/13-AOAS705", "report-no": "IMS-AOAS-AOAS705", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of statistical models have been successfully developed for the\nanalysis of high-throughput data from a single source, but few methods are\navailable for integrating data from different sources. Here we focus on\nintegrating gene expression levels with comparative genomic hybridization (CGH)\narray measurements collected on the same subjects. We specify a measurement\nerror model that relates the gene expression levels to latent copy number\nstates which, in turn, are related to the observed surrogate CGH measurements\nvia a hidden Markov model. We employ selection priors that exploit the\ndependencies across adjacent copy number states and investigate MCMC stochastic\nsearch techniques for posterior inference. Our approach results in a unified\nmodeling framework for simultaneously inferring copy number variants (CNV) and\nidentifying their significant associations with mRNA transcripts abundance. We\nshow performance on simulated data and illustrate an application to data from a\ngenomic study on human cancer cell lines.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 12:49:00 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Cassese", "Alberto", ""], ["Guindani", "Michele", ""], ["Tadesse", "Mahlet G.", ""], ["Falciani", "Francesco", ""], ["Vannucci", "Marina", ""]]}, {"id": "1404.3584", "submitter": "Jos\\'{e} R. Zubizarreta", "authors": "Jos\\'e R. Zubizarreta, Ricardo D. Paredes, Paul R. Rosenbaum", "title": "Matching for balance, pairing for heterogeneity in an observational\n  study of the effectiveness of for-profit and not-for-profit high schools in\n  Chile", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS713 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 204-231", "doi": "10.1214/13-AOAS713", "report-no": "IMS-AOAS-AOAS713", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, the construction of a pair-matched sample selects treated and\ncontrol units and pairs them in a single step with a view to balancing observed\ncovariates $\\mathbf{x}$ and reducing the heterogeneity or dispersion of\ntreated-minus-control response differences, $Y$. In contrast, the method of\ncardinality matching developed here first selects the maximum number of units\nsubject to covariate balance constraints and, with a balanced sample for\n$\\mathbf{x}$ in hand, then separately pairs the units to minimize heterogeneity\nin $Y$. Reduced heterogeneity of pair differences in responses $Y$ is known to\nreduce sensitivity to unmeasured biases, so one might hope that cardinality\nmatching would succeed at both tasks, balancing $\\mathbf{x}$, stabilizing $Y$.\nWe use cardinality matching in an observational study of the effectiveness of\nfor-profit and not-for-profit private high schools in Chile - a controversial\nsubject in Chile - focusing on students who were in government run primary\nschools in 2004 but then switched to private high schools. By pairing to\nminimize heterogeneity in a cardinality match that has balanced covariates, a\nmeaningful reduction in sensitivity to unmeasured biases is obtained.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 14:01:42 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Zubizarreta", "Jos\u00e9 R.", ""], ["Paredes", "Ricardo D.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1404.3806", "submitter": "Morteza Amini", "authors": "Morteza Amini, Soudabeh Shemehsavar and Zhengqiang Pan", "title": "Optimal design for step-stress accelerated test with random discrete\n  stress elevating times based on gamma degradation process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a step-stress accelerated degradation test (SSADT) plan, in which\nthe stress level is elevated when the degradation value of a product crosses a\npre-specified value, was proposed. The times of stress level elevating are\nrandom and vary from product to product. In this paper we extend this model to\na more economic plan. The proposed extended model has two economical advantages\ncompared with the previous one. The first is that the times of stress level\nelevating in the new model are identical for all products, which enable us to\nuse only one chamber (oven) for testing all test units. The second is that, the\nnew method does not require continuous inspection and to elevate the stress\nlevel, it is not necessary for the experimenter to inspect the value of the\ndegradation continually. The new method decrease the cost of measurement and\nalso there is no need to use electronic sensors to detect the first passage\ntime of the degradation to the threshold value in the new method. We assume\nthat the degradation path follows a gamma process. The stress level is elevated\nas soon as the measurement of the degradation of one of the test units, at one\nof the specified times, exceeds the threshold value. Under the constraint that\nthe total experimental cost does not exceed a pre-specified budget, the optimal\nsettings including the optimal threshold value, sample size, measurement\nfrequency and termination time are obtained by minimizing the asymptotic\nvariance of an estimated quantile of the lifetime distribution of the product.\nA case study is presented to illustrate the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 03:23:54 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 04:28:28 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 12:01:00 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Amini", "Morteza", ""], ["Shemehsavar", "Soudabeh", ""], ["Pan", "Zhengqiang", ""]]}, {"id": "1404.3878", "submitter": "Nipun Batra", "authors": "Nipun Batra, Jack Kelly, Oliver Parson, Haimonti Dutta, William\n  Knottenbelt, Alex Rogers, Amarjeet Singh, Mani Srivastava", "title": "NILMTK: An Open Source Toolkit for Non-intrusive Load Monitoring", "comments": "To appear in the fifth International Conference on Future Energy\n  Systems (ACM e-Energy), Cambridge, UK. 2014", "journal-ref": null, "doi": "10.1145/2602044.2602051", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring, or energy disaggregation, aims to separate\nhousehold energy consumption data collected from a single point of measurement\ninto appliance-level consumption data. In recent years, the field has rapidly\nexpanded due to increased interest as national deployments of smart meters have\nbegun in many countries. However, empirically comparing disaggregation\nalgorithms is currently virtually impossible. This is due to the different data\nsets used, the lack of reference implementations of these algorithms and the\nvariety of accuracy metrics employed. To address this challenge, we present the\nNon-intrusive Load Monitoring Toolkit (NILMTK); an open source toolkit designed\nspecifically to enable the comparison of energy disaggregation algorithms in a\nreproducible manner. This work is the first research to compare multiple\ndisaggregation approaches across multiple publicly available data sets. Our\ntoolkit includes parsers for a range of existing data sets, a collection of\npreprocessing algorithms, a set of statistics for describing data sets, two\nreference benchmark disaggregation algorithms and a suite of accuracy metrics.\nWe demonstrate the range of reproducible analyses which are made possible by\nour toolkit, including the analysis of six publicly available data sets and the\nevaluation of both benchmark disaggregation algorithms across such data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 11:52:32 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Batra", "Nipun", ""], ["Kelly", "Jack", ""], ["Parson", "Oliver", ""], ["Dutta", "Haimonti", ""], ["Knottenbelt", "William", ""], ["Rogers", "Alex", ""], ["Singh", "Amarjeet", ""], ["Srivastava", "Mani", ""]]}, {"id": "1404.3989", "submitter": "Andrew Beam", "authors": "Andrew L. Beam, Alison Motsinger-Reif, Jon Doyle", "title": "Bayesian Neural Networks for Genetic Association Studies of Complex\n  Disease", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-014-0368-0", "report-no": null, "categories": "q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal genetic variants from large genetic association studies\nposes many difficult challenges. Assessing which genetic markers are involved\nin determining trait status is a computationally demanding task, especially in\nthe presence of gene-gene interactions. A non-parametric Bayesian approach in\nthe form of a Bayesian neural network is proposed for use in analyzing genetic\nassociation studies. Demonstrations on synthetic and real data reveal they are\nable to efficiently and accurately determine which variants are involved in\ndetermining case-control status. Using graphics processing units (GPUs) the\ntime needed to build these models is decreased by several orders of magnitude.\nIn comparison with commonly used approaches for detecting genetic interactions,\nBayesian neural networks perform very well across a broad spectrum of possible\ngenetic relationships while having the computational efficiency needed to\nhandle large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 17:11:53 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 00:44:21 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Beam", "Andrew L.", ""], ["Motsinger-Reif", "Alison", ""], ["Doyle", "Jon", ""]]}, {"id": "1404.4006", "submitter": "Tadilo E Bogale", "authors": "Tadilo Endeshaw Bogale, Luc Vandendorpe and Long Bao Le", "title": "Sensing Throughput Tradeoff for Cognitive Radio Networks with Noise\n  Variance Uncertainty", "comments": "Accepted in CROWNCOM, June 2014, Oulu, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes novel spectrum sensing algorithm, and examines the\nsensing throughput tradeoff for cognitive radio (CR) networks under noise\nvariance uncertainty. It is assumed that there are one white sub-band, and one\ntarget sub-band which is either white or non-white. Under this assumption,\nfirst we propose a novel generalized energy detector (GED) for examining the\ntarget sub-band by exploiting the noise information of the white sub-band,\nthen, we study the tradeoff between the sensing time and achievable throughput\nof the CR network. To study this tradeoff, we consider the sensing time\noptimization for maximizing the throughput of the CR network while\nappropriately protecting the primary network. The sensing time is optimized by\nutilizing the derived detection and false alarm probabilities of the GED. The\nproposed GED does not suffer from signal to noise ratio (SNR) wall (i.e.,\nrobust against noise variance uncertainty) and outperforms the existing signal\ndetectors. Moreover, the relationship between the proposed GED and conventional\nenergy detector (CED) is quantified analytically. We show that the optimal\nsensing times with perfect and imperfect noise variances are not the same. In\nparticular, when the frame duration is 2s, and SNR is -20dB, and each of the\nbandwidths of the white and target sub-bands is 6MHz, the optimal sensing times\nare 28.5ms and 50.6ms with perfect and imperfect noise variances, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 18:18:44 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Bogale", "Tadilo Endeshaw", ""], ["Vandendorpe", "Luc", ""], ["Le", "Long Bao", ""]]}, {"id": "1404.4009", "submitter": "Dennis Feehan", "authors": "Dennis M. Feehan and Matthew J. Salganik", "title": "Generalizing the Network Scale-Up Method: A New Estimator for the Size\n  of Hidden Populations", "comments": null, "journal-ref": "Sociological Methodology, August 2016 46: 153-186", "doi": "10.1177/0081175016665425", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network scale-up method enables researchers to estimate the size of\nhidden populations, such as drug injectors and sex workers, using sampled\nsocial network data. The basic scale-up estimator offers advantages over other\nsize estimation techniques, but it depends on problematic modeling assumptions.\nWe propose a new generalized scale-up estimator that can be used in settings\nwith non-random social mixing and imperfect awareness about membership in the\nhidden population. Further, the new estimator can be used when data are\ncollected via complex sample designs and from incomplete sampling frames.\nHowever, the generalized scale-up estimator also requires data from two\nsamples: one from the frame population and one from the hidden population. In\nsome situations these data from the hidden population can be collected by\nadding a small number of questions to already planned studies. For other\nsituations, we develop interpretable adjustment factors that can be applied to\nthe basic scale-up estimator. We conclude with practical recommendations for\nthe design and analysis of future studies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 18:25:38 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 03:29:31 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2015 16:58:36 GMT"}, {"version": "v4", "created": "Fri, 11 Nov 2016 08:14:32 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Feehan", "Dennis M.", ""], ["Salganik", "Matthew J.", ""]]}, {"id": "1404.4244", "submitter": "Darren Wraith", "authors": "Darren Wraith, Kerrie Mengersen, Clair Alston, Judith Rousseau, Tareq\n  Hussein", "title": "Using informative priors in the estimation of mixtures over time with\n  application to aerosol particle size distributions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS678 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 232-258", "doi": "10.1214/13-AOAS678", "report-no": "IMS-AOAS-AOAS678", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of using informative priors for estimation of mixtures at multiple\ntime points is examined. Several different informative priors and an\nindependent prior are compared using samples of actual and simulated aerosol\nparticle size distribution (PSD) data. Measurements of aerosol PSDs refer to\nthe concentration of aerosol particles in terms of their size, which is\ntypically multimodal in nature and collected at frequent time intervals. The\nuse of informative priors is found to better identify component parameters at\neach time point and more clearly establish patterns in the parameters over\ntime. Some caveats to this finding are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 13:44:40 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Wraith", "Darren", ""], ["Mengersen", "Kerrie", ""], ["Alston", "Clair", ""], ["Rousseau", "Judith", ""], ["Hussein", "Tareq", ""]]}, {"id": "1404.4601", "submitter": "Luo Xiao", "authors": "Luo Xiao, Bing He, Annemarie Koster, Paolo Caserotti, Brittney\n  Lange-Maia, Nancy W. Glynn, Tamara Harris and Ciprian M. Crainiceanu", "title": "Movement Prediction Using Accelerometers in a Human Population", "comments": "35 pages, 7 figures", "journal-ref": "Biometrics 2016, Vol. 72, No. 2, 513-524", "doi": "10.1111/biom.12382", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce statistical methods for predicting the types of human activity\nat sub-second resolution using triaxial accelerometry data. The major\ninnovation is that we use labeled activity data from some subjects to predict\nthe activity labels of other subjects. To achieve this, we normalize the data\nacross subjects by matching the standing up and lying down portions of triaxial\naccelerometry data. This is necessary to account for differences between the\nvariability in the position of the device relative to gravity, which are\ninduced by body shape and size as well as by the ambiguous definition of device\nplacement. We also normalize the data at the device level to ensure that the\nmagnitude of the signal at rest is similar across devices. After normalization\nwe use overlapping movelets (segments of triaxial accelerometry time series)\nextracted from some of the subjects to predict the movement type of the other\nsubjects. The problem was motivated by and is applied to a laboratory study of\n20 older participants who performed different activities while wearing\naccelerometers at the hip. Prediction results based on other people's labeled\ndictionaries of activity performed almost as well as those obtained using their\nown labeled dictionaries. These findings indicate that prediction of activity\ntypes for data collected during natural activities of daily living may actually\nbe possible.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 18:29:16 GMT"}, {"version": "v2", "created": "Wed, 30 Jul 2014 00:58:49 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Xiao", "Luo", ""], ["He", "Bing", ""], ["Koster", "Annemarie", ""], ["Caserotti", "Paolo", ""], ["Lange-Maia", "Brittney", ""], ["Glynn", "Nancy W.", ""], ["Harris", "Tamara", ""], ["Crainiceanu", "Ciprian M.", ""]]}, {"id": "1404.5358", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, G. Brooke Anderson, Roger Peng, Jeff Leek", "title": "A randomized trial in a massive online open course shows people don't\n  know what a statistically significant relationship looks like, but they can\n  learn", "comments": "7 pages, including 2 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplots are the most common way for statisticians, scientists, and the\npublic to visually detect relationships between measured variables. At the same\ntime, and despite widely publicized controversy, P-values remain the most\ncommonly used measure to statistically justify relationships identified between\nvariables. Here we measure the ability to detect statistically significant\nrelationships from scatterplots in a randomized trial of 2,039 students in a\nstatistics massive open online course (MOOC). Each subject was shown a random\nset of scatterplots and asked to visually determine if the underlying\nrelationships were statistically significant at the P < 0.05 level. Subjects\ncorrectly classified only 47.4% (95% CI: 45.1%-49.7%) of statistically\nsignificant relationships, and 74.6% (95% CI: 72.5%-76.6%) of non-significant\nrelationships. Adding visual aids such as a best fit line or scatterplot smooth\nincreased the probability a relationship was called significant, regardless of\nwhether the relationship was actually significant. Classification of\nstatistically significant relationships improved on repeat attempts of the\nsurvey, although classification of non-significant relationships did not. Our\nresults suggest: (1) that evidence-based data analysis can be used to identify\nweaknesses in theoretical procedures in the hands of average users, (2) data\nanalysts can be trained to improve detection of statistically significant\nresults with practice, but (3) data analysts have incorrect intuition about\nwhat statistically significant relationships look like, particularly for small\neffects. We have built a web tool for people to compare scatterplots with their\ncorresponding p-values which is available here:\nhttp://glimmer.rstudio.com/afisher/EDA/.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 00:29:48 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Fisher", "Aaron", ""], ["Anderson", "G. Brooke", ""], ["Peng", "Roger", ""], ["Leek", "Jeff", ""]]}, {"id": "1404.5598", "submitter": "Leonardo Bennun LB", "authors": "Henry Navarro and Leonardo Bennun", "title": "Descriptive examples of the limitations of Artificial Neural Networks\n  applied to the analysis of independent stochastic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show with a few descriptive examples the limitations of Artificial Neural\nNetworks when they are applied to the analysis of independent stochastic data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 19:24:23 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Navarro", "Henry", ""], ["Bennun", "Leonardo", ""]]}, {"id": "1404.5970", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Lars Holden, Geir Kjetil Sandve", "title": "Monte Carlo Null Models for Genomic Data", "comments": "Published at http://dx.doi.org/10.1214/14-STS484 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 59-71", "doi": "10.1214/14-STS484", "report-no": "IMS-STS-STS484", "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As increasingly complex hypothesis-testing scenarios are considered in many\nscientific fields, analytic derivation of null distributions is often out of\nreach. To the rescue comes Monte Carlo testing, which may appear deceptively\nsimple: as long as you can sample test statistics under the null hypothesis,\nthe $p$-value is just the proportion of sampled test statistics that exceed the\nobserved test statistic. Sampling test statistics is often simple once you have\na Monte Carlo null model for your data, and defining some form of randomization\nprocedure is also, in many cases, relatively straightforward. However, there\nmay be several possible choices of a randomization null model for the data and\nno clear-cut criteria for choosing among them. Obviously, different null models\nmay lead to very different $p$-values, and a very low $p$-value may thus occur\ndue to the inadequacy of the chosen null model. It is preferable to use\nassumptions about the underlying random data generation process to guide\nselection of a null model. In many cases, we may order the null models by\nincreasing preservation of the data characteristics, and we argue in this paper\nthat this ordering in most cases gives increasing $p$-values, that is, lower\nsignificance. We denote this as the null complexity principle. The principle\ngives a better understanding of the different null models and may guide in the\nchoice between the different models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 20:25:15 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 12:34:06 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 12:32:57 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Holden", "Lars", ""], ["Sandve", "Geir Kjetil", ""]]}, {"id": "1404.6193", "submitter": "Antonello Maruotti", "authors": "Valentina Raponi, Francesca Martella, Antonello Maruotti", "title": "A biclustering approach to university performances: an Italian case\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  University evaluation is a topic of increasing concern in Italy as well as in\nother countries. In empirical analysis, university activities and performances\nare generally measured by means of indicator variables, summarizing the\navailable information under different perspectives. In this paper, we argue\nthat the evaluation process is a complex issue that can not be addressed by a\nsimple descriptive approach and thus association between indicators and\nsimilarities among the observed universities should be accounted for.\nParticularly, we examine faculty-level data collected from different sources,\ncovering 55 Italian Economics faculties in the academic year 2009/2010. Making\nuse of a clustering framework, we introduce a biclustering model that accounts\nfor both homogeneity/heterogeneity among faculties and correlations between\nindicators. Our results show that there are two substantial different\nperformances between universities which can be strictly related to the nature\nof the institutions, namely the Private and Public profiles . Each of the two\ngroups has its own peculiar features and its own group-specific list of\npriorities, strengths and weaknesses. Thus, we suggest that caution should be\nused in interpreting standard university rankings as they generally do not\naccount for the complex structure of the data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 17:36:21 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Raponi", "Valentina", ""], ["Martella", "Francesca", ""], ["Maruotti", "Antonello", ""]]}, {"id": "1404.6423", "submitter": "Yen-Tsung Huang", "authors": "Yen-Tsung Huang, Tyler J. VanderWeele, Xihong Lin", "title": "Joint analysis of SNP and gene expression data in genetic association\n  studies of complex diseases", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS690 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 352-376", "doi": "10.1214/13-AOAS690", "report-no": "IMS-AOAS-AOAS690", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic association studies have been a popular approach for assessing the\nassociation between common Single Nucleotide Polymorphisms (SNPs) and complex\ndiseases. However, other genomic data involved in the mechanism from SNPs to\ndisease, for example, gene expressions, are usually neglected in these\nassociation studies. In this paper, we propose to exploit gene expression\ninformation to more powerfully test the association between SNPs and diseases\nby jointly modeling the relations among SNPs, gene expressions and diseases. We\npropose a variance component test for the total effect of SNPs and a gene\nexpression on disease risk. We cast the test within the causal mediation\nanalysis framework with the gene expression as a potential mediator. For eQTL\nSNPs, the use of gene expression information can enhance power to test for the\ntotal effect of a SNP-set, which is the combined direct and indirect effects of\nthe SNPs mediated through the gene expression, on disease risk. We show that\nthe test statistic under the null hypothesis follows a mixture of $\\chi^2$\ndistributions, which can be evaluated analytically or empirically using the\nresampling-based perturbation method. We construct tests for each of three\ndisease models that are determined by SNPs only, SNPs and gene expression, or\ninclude also their interactions. As the true disease model is unknown in\npractice, we further propose an omnibus test to accommodate different\nunderlying disease models. We evaluate the finite sample performance of the\nproposed methods using simulation studies, and show that our proposed test\nperforms well and the omnibus test can almost reach the optimal power where the\ndisease model is known and correctly specified. We apply our method to\nreanalyze the overall effect of the SNP-set and expression of the ORMDL3 gene\non the risk of asthma.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 14:10:13 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Huang", "Yen-Tsung", ""], ["VanderWeele", "Tyler J.", ""], ["Lin", "Xihong", ""]]}, {"id": "1404.6473", "submitter": "Lucas Mentch", "authors": "Lucas Mentch, Giles Hooker", "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests", "comments": "To appear in The Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 16:15:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 18:52:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1404.7175", "submitter": "Peng Ding", "authors": "Peng Ding, and Tyler J. VanderWeele", "title": "Generalized Cornfield conditions for the risk difference", "comments": null, "journal-ref": "Biometrika, 2014", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in causal inference with observational studies is the\nsensitivity of conclusions to unmeasured confounding. The classical Cornfield\ncondition allows us to assess whether an unmeasured binary confounder can\nexplain away the observed relative risk of the exposure on the outcome. It\nstates that for an unmeasured confounder to explain away an observed relative\nrisk, the association between the unmeasured confounder and the exposure, and\nalso that between the unmeasured confounder and the outcome, must both be\nlarger than the observed relative risk. In this paper, we extend the classical\nCornfield condition in three directions. First, we consider analogous\nconditions for the risk difference, and allow for a categorical, not just a\nbinary, unmeasured confounder. Second, we provide more stringent thresholds\nwhich the maximum of the above-mentioned associations must satisfy, rather than\nsimply weaker conditions that both must satisfy. Third, we show that all\nprevious results on Cornfield conditions hold under weaker assumptions than\npreviously used. We illustrate their potential applications by real examples,\nwhere our new conditions give more information than the classical ones.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 21:41:56 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Ding", "Peng", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1404.7197", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Bayesian Model Comparison in Genetic Association Analysis: Linear Mixed\n  Modeling and SNP Set Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of hypothesis testing and model comparison under a\nflexible Bayesian linear regression model whose formulation is closely\nconnected with the linear mixed effect model and the parametric models for SNP\nset analysis in genetic association studies. We derive a class of analytic\napproximate Bayes factors and illustrate their connections with a variety of\nfrequentist test statistics, including the Wald statistic and the variance\ncomponent score statistic. Taking advantage of Bayesian model averaging and\nhierarchical modeling, we demonstrate some distinct advantages and\nflexibilities in the approaches utilizing the derived Bayes factors in the\ncontext of genetic association studies. We demonstrate our proposed methods\nusing real or simulated numerical examples in applications of single SNP\nassociation testing, multi-locus fine-mapping and SNP set association testing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 00:26:57 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 15:36:14 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1404.7295", "submitter": "E. G. Hill", "authors": "E. G. Hill, E. H. Slate", "title": "A semi-parametric Bayesian model of inter- and intra-examiner agreement\n  for periodontal probing depth", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS688 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 331-351", "doi": "10.1214/13-AOAS688", "report-no": "IMS-AOAS-AOAS688", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodontal probing depth is a measure of periodontitis severity. We develop\na Bayesian hierarchical model linking true pocket depth to both observed and\nrecorded values of periodontal probing depth, while permitting correlation\namong measures obtained from the same mouth and between duplicate examiners'\nmeasures obtained at the same periodontal site. Periodontal site-specific\nexaminer effects are modeled as arising from a Dirichlet process mixture,\nfacilitating identification of classes of sites that are measured with similar\nbias. Using simulated data, we demonstrate the model's ability to recover\nexaminer site-specific bias and variance heterogeneity and to provide\ncluster-adjusted point and interval agreement estimates. We conclude with an\nanalysis of data from a probing depth calibration training exercise.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:08:34 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Hill", "E. G.", ""], ["Slate", "E. H.", ""]]}, {"id": "1404.7301", "submitter": "Matthew Reimherr", "authors": "Matthew Reimherr, Dan Nicolae", "title": "A functional data analysis approach for genetic association studies", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS692 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 406-429", "doi": "10.1214/13-AOAS692", "report-no": "IMS-AOAS-AOAS692", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method based on Functional Data Analysis (FDA) for detecting\nassociations between one or more scalar covariates and a longitudinal response,\nwhile correcting for other variables. Our methods exploit the temporal\nstructure of longitudinal data in ways that are otherwise difficult with a\nmultivariate approach. Our procedure, from an FDA perspective, is a departure\nfrom more established methods in two key aspects. First, the raw longitudinal\nphenotypes are assembled into functional trajectories prior to analysis.\nSecond, we explore an association test that is not directly based on principal\ncomponents. We instead focus on quantifying the reduction in $L^2$ variability\nas a means of detecting associations. Our procedure is motivated by\nlongitudinal genome wide association studies and, in particular, the childhood\nasthma management program (CAMP) which explores the long term effects of daily\nasthma treatments. We conduct a simulation study to better understand the\nadvantages (and/or disadvantages) of an FDA approach compared to a traditional\nmultivariate one. We then apply our methodology to data coming from CAMP. We\nfind a potentially new association with a SNP negatively affecting lung\nfunction. Furthermore, this SNP seems to have an interaction effect with one of\nthe treatments.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:33:13 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Reimherr", "Matthew", ""], ["Nicolae", "Dan", ""]]}, {"id": "1404.7339", "submitter": "Doyo G. Enki", "authors": "Doyo G. Enki, Angela Noufaily, C. Paddy Farrington", "title": "A time-varying shared frailty model with application to infectious\n  diseases", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS693 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 430-447", "doi": "10.1214/13-AOAS693", "report-no": "IMS-AOAS-AOAS693", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new parametric time-varying shared frailty model to represent\nchanges over time in population heterogeneity, for use with bivariate current\nstatus data. The model uses a power transformation of a time-invariant frailty\n$U$, and is particularly convenient when $U$ is a member of the generalized\ngamma family. This model avoids some shortcomings of a previously suggested\ntime-varying frailty model, notably time-dependent support. We describe some\nkey properties of the model, including its relative frailty variance function\nin different settings and how the model can be fitted to data. We describe\nseveral applications to shared frailty modeling of bivariate current status\ndata on infectious diseases, in which the frailty represents age-dependent\nheterogeneity in contact rates or susceptibility to infection.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 12:36:11 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Enki", "Doyo G.", ""], ["Noufaily", "Angela", ""], ["Farrington", "C. Paddy", ""]]}, {"id": "1404.7362", "submitter": "Jinzhu Jia", "authors": "Jinzhu Jia, Luke Miratrix, Bin Yu, Brian Gawalt, Laurent El Ghaoui,\n  Luke Barnesmoore, Sophie Clavier", "title": "Concise comparative summaries (CCS) of large text corpora with a human\n  experiment", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS698 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 499-529", "doi": "10.1214/13-AOAS698", "report-no": "IMS-AOAS-AOAS698", "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general framework for topic-specific summarization\nof large text corpora and illustrate how it can be used for the analysis of\nnews databases. Our framework, concise comparative summarization (CCS), is\nbuilt on sparse classification methods. CCS is a lightweight and flexible tool\nthat offers a compromise between simple word frequency based methods currently\nin wide use and more heavyweight, model-intensive methods such as latent\nDirichlet allocation (LDA). We argue that sparse methods have much to offer for\ntext analysis and hope CCS opens the door for a new branch of research in this\nimportant field. For a particular topic of interest (e.g., China or energy),\nCSS automatically labels documents as being either on- or off-topic (usually\nvia keyword search), and then uses sparse classification methods to predict\nthese labels with the high-dimensional counts of all the other words and\nphrases in the documents. The resulting small set of phrases found as\npredictive are then harvested as the summary. To validate our tool, we, using\nnews articles from the New York Times international section, designed and\nconducted a human survey to compare the different summarizers with human\nunderstanding. We demonstrate our approach with two case studies, a media\nanalysis of the framing of \"Egypt\" in the New York Times throughout the Arab\nSpring and an informal comparison of the New York Times' and Wall Street\nJournal's coverage of \"energy.\" Overall, we find that the Lasso with $L^2$\nnormalization can be effectively and usefully used to summarize large corpora,\nregardless of document size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 13:53:38 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Jia", "Jinzhu", ""], ["Miratrix", "Luke", ""], ["Yu", "Bin", ""], ["Gawalt", "Brian", ""], ["Ghaoui", "Laurent El", ""], ["Barnesmoore", "Luke", ""], ["Clavier", "Sophie", ""]]}, {"id": "1404.7397", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "A fully data-driven method for estimating the shape of a point cloud", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support $S$. Under the\nmild assumption that $S$ is $r$-convex, the smallest $r$-convex set which\ncontains the sample points is the natural estimator. The main problem for using\nthis estimator in practice is that $r$ is an unknown geometric characteristic\nof the set $S$. A stochastic algorithm is proposed for selecting it from the\ndata under the hypothesis that the sample is uniformly generated. The new\ndata-driven reconstruction of $S$ is able to achieve the same convergence rates\nas the convex hull for estimating convex sets, but under a much more flexible\nsmoothness shape condition. The practical performance of the estimator is\nillustrated through a real data example and a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 15:34:04 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 21:30:31 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1404.7534", "submitter": "Donatello Telesca", "authors": "Yafeng Zhang, Steve Horvath, Roel Ophoff, Donatello Telesca", "title": "Comparison of Clustering Methods for Time Course Genomic Data:\n  Applications to Aging Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time course microarray data provide insight about dynamic biological\nprocesses. While several clustering methods have been proposed for the analysis\nof these data structures, comparison and selection of appropriate clustering\nmethods are seldom discussed. We compared $3$ probabilistic based clustering\nmethods and $3$ distance based clustering methods for time course microarray\ndata. Among probabilistic methods, we considered: smoothing spline clustering\nalso known as model based functional data analysis (MFDA), functional\nclustering models for sparsely sampled data (FCM) and model-based clustering\n(MCLUST). Among distance based methods, we considered: weighted gene\nco-expression network analysis (WGCNA), clustering with dynamic time warping\ndistance (DTW) and clustering with autocorrelation based distance (ACF). We\nstudied these algorithms in both simulated settings and case study data. Our\ninvestigations showed that FCM performed very well when gene curves were short\nand sparse. DTW and WGCNA performed well when gene curves were medium or long\n($>=10$ observations). SSC performed very well when there were clusters of gene\ncurves similar to one another. Overall, ACF performed poorly in these\napplications. In terms of computation time, FCM, SSC and DTW were considerably\nslower than MCLUST and WGCNA. WGCNA outperformed MCLUST by generating more\naccurate and biological meaningful clustering results. WGCNA and MCLUST are the\nbest methods among the 6 methods compared, when performance and computation\ntime are both taken into account. WGCNA outperforms MCLUST, but MCLUST provides\nmodel based inference and uncertainty measure of clustering results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 20:58:48 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zhang", "Yafeng", ""], ["Horvath", "Steve", ""], ["Ophoff", "Roel", ""], ["Telesca", "Donatello", ""]]}, {"id": "1404.7625", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos", "title": "The R Package JMbayes for Fitting Joint Models for Longitudinal and\n  Time-to-Event Data using MCMC", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models for longitudinal and time-to-event data constitute an attractive\nmodeling framework that has received a lot of interest in the recent years.\nThis paper presents the capabilities of the R package JMbayes for fitting these\nmodels under a Bayesian approach using Markon chain Monte Carlo algorithms.\nJMbayes can fit a wide range of joint models, including among others joint\nmodels for continuous and categorical longitudinal responses, and provides\nseveral options for modeling the association structure between the two\noutcomes. In addition, this package can be used to derive dynamic predictions\nfor both outcomes, and offers several tools to validate these predictions in\nterms of discrimination and calibration. All these features are illustrated\nusing a real data example on patients with primary biliary cirrhosis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 08:19:50 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Rizopoulos", "Dimitris", ""]]}, {"id": "1404.7636", "submitter": "Kung-Sik Chan", "authors": "Kung-Sik Chan, Jinzheng Li, William Eichinger, Erwei Bai", "title": "Testing for shielding of special nuclear weapon materials", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS704 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 553-576", "doi": "10.1214/13-AOAS704", "report-no": "IMS-AOAS-AOAS704", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear-weapon-material detection via gamma-ray sensing is routinely applied,\nfor example, in monitoring cross-border traffic. Natural or deliberate\nshielding both attenuates and distorts the shape of the gamma-ray spectra of\nspecific radionuclides, thereby making such routine applications challenging.\nWe develop a Lagrange multiplier (LM) test for shielding. A strong advantage of\nthe LM test is that it only requires fitting a much simpler model that assumes\nno shielding. We show that, under the null hypothesis and some mild regularity\nconditions and as the detection time increases, LM test statistic for\n(composite) shielding is asymptotically Chi-square with the degree of freedom\nequal to the presumed number of shielding materials. We also derive the local\npower of the LM test. Extensive simulation studies suggest that the test is\nrobust to the number and nature of the intervening materials, which owes to the\nfact that common intervening materials have broadly similar attenuation\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 08:58:27 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Chan", "Kung-Sik", ""], ["Li", "Jinzheng", ""], ["Eichinger", "William", ""], ["Bai", "Erwei", ""]]}, {"id": "1404.7642", "submitter": "Fukang Zhu", "authors": "Fukang Zhu, Zongwu Cai, Liang Peng", "title": "Predictive regressions for macroeconomic data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS708 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 577-594", "doi": "10.1214/13-AOAS708", "report-no": "IMS-AOAS-AOAS708", "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have constantly asked whether stock returns can be predicted by\nsome macroeconomic data. However, it is known that macroeconomic data may\nexhibit nonstationarity and/or heavy tails, which complicates existing testing\nprocedures for predictability. In this paper we propose novel empirical\nlikelihood methods based on some weighted score equations to test whether the\nmonthly CRSP value-weighted index can be predicted by the log dividend-price\nratio or the log earnings-price ratio. The new methods work well both\ntheoretically and empirically regardless of the predicting variables being\nstationary or nonstationary or having an infinite variance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 09:15:11 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zhu", "Fukang", ""], ["Cai", "Zongwu", ""], ["Peng", "Liang", ""]]}, {"id": "1404.7653", "submitter": "Hajo Holzmann", "authors": "Hajo Holzmann, Matthias Eulert", "title": "The role of the information set for forecasting - with applications to\n  risk management", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS709 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 595-621", "doi": "10.1214/13-AOAS709", "report-no": "IMS-AOAS-AOAS709", "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions are issued on the basis of certain information. If the\nforecasting mechanisms are correctly specified, a larger amount of available\ninformation should lead to better forecasts. For point forecasts, we show how\nthe effect of increasing the information set can be quantified by using\nstrictly consistent scoring functions, where it results in smaller average\nscores. Further, we show that the classical Diebold-Mariano test, based on\nstrictly consistent scoring functions and asymptotically ideal forecasts, is a\nconsistent test for the effect of an increase in a sequence of information sets\non $h$-step point forecasts. For the value at risk (VaR), we show that the\naverage score, which corresponds to the average quantile risk, directly relates\nto the expected shortfall. Thus, increasing the information set will result in\nVaR forecasts which lead on average to smaller expected shortfalls. We\nillustrate our results in simulations and applications to stock returns for\nunconditional versus conditional risk management as well as univariate modeling\nof portfolio returns versus multivariate modeling of individual risk factors.\nThe role of the information set for evaluating probabilistic forecasts by using\nstrictly proper scoring rules is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 09:44:35 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Holzmann", "Hajo", ""], ["Eulert", "Matthias", ""]]}, {"id": "1404.7710", "submitter": "Soo-Heang Eo", "authors": "Soo-Heang Eo and Seung-Mo Hong and HyungJun Cho", "title": "Identification of Outlying Observations with Quantile Regression for\n  Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Outlying observations, which significantly deviate from other measurements,\nmay distort the conclusions of data analysis. Therefore, identifying outliers\nis one of the important problems that should be solved to obtain reliable\nresults. While there are many statistical outlier detection algorithms and\nsoftware programs for uncensored data, few are available for censored data. In\nthis article, we propose three outlier detection algorithms based on censored\nquantile regression, two of which are modified versions of existing algorithms\nfor uncensored or censored data, while the third is a newly developed algorithm\nto overcome the demerits of previous approaches. The performance of the three\nalgorithms was investigated in simulation studies. In addition, real data from\nSEER database, which contains a variety of data sets related to various\ncancers, is illustrated to show the usefulness of our methodology. The\nalgorithms are implemented into an R package OutlierDC which can be\nconveniently employed in the \\proglang{R} environment and freely obtained from\nCRAN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 13:06:53 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Eo", "Soo-Heang", ""], ["Hong", "Seung-Mo", ""], ["Cho", "HyungJun", ""]]}]