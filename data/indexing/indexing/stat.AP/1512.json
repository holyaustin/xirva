[{"id": "1512.00128", "submitter": "Andrew Gelman", "authors": "Andrew Gelman and Jonathan Auerbach", "title": "Age-aggregation bias in mortality trends", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": "10.1073/pnas.1523465113", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article in PNAS, Case and Deaton show a figure illustrating \"a\nmarked increase in the all-cause mortality of middle-aged white non-Hispanic\nmen and women in the United States between 1999 and 2013.\" The authors state\nthat their numbers \"are not age-adjusted within the 10-y 45-54 age group.\" They\ncalculated the mortality rate each year by dividing the total number of deaths\nfor the age group by the population of the age group.\n  We suspected an aggregation bias. After adjusting for changes in age\ncomposition, we find there is no longer a steady increase in mortality rates\nfor this age group. Instead there is an increasing trend from 1999-2005 and a\nconstant trend thereafter. Moreover, stratifying age-adjusted mortality rates\nby sex shows a marked increase only for women and not men, contrary to the\narticle's headline.\n  We stress that this does not change a key finding of the Case and Deaton\npaper: the comparison of non-Hispanic U.S. middle-aged whites to other\ncountries and other ethnic groups. These comparisons hold up after our age\nadjustment. While we do not believe that age-adjustment invalidates comparisons\nbetween countries, it does affect claims concerning the absolute increase in\nmortality among U.S. middle-aged white non-Hispanics. Breaking down the trends\nin this group by region of the country shows other interesting patterns: since\n1999 there has been an increase in death rates among women in the south. In\ncontrast, death rates for both sexes have been declining in the northeast, the\nregion where mortality rates were lowest to begin with. These graphs\ndemonstrate the value of this sort of data exploration, and we are grateful to\nCase and Deaton for focusing attention on these mortality trends.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 03:10:17 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Gelman", "Andrew", ""], ["Auerbach", "Jonathan", ""]]}, {"id": "1512.00205", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Nicolas Chopin, Vincent Cottet", "title": "Divide and conquer in ABC: Expectation-Progagation algorithms for\n  likelihood-free inference", "comments": "To appear in the forthcoming Handbook of Approximate Bayesian\n  Computation (ABC), edited by S. Sisson, L. Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABC algorithms are notoriously expensive in computing time, as they require\nsimulating many complete artificial datasets from the model. We advocate in\nthis paper a \"divide and conquer\" approach to ABC, where we split the\nlikelihood into n factors, and combine in some way n \"local\" ABC approximations\nof each factor. This has two advantages: (a) such an approach is typically much\nfaster than standard ABC and (b) it makes it possible to use local summary\nstatistics (i.e. summary statistics that depend only on the data-points that\ncorrespond to a single factor), rather than global summary statistics (that\ndepend on the complete dataset). This greatly alleviates the bias introduced by\nsummary statistics, and even removes it entirely in situations where local\nsummary statistics are simply the identity function.\n  We focus on EP (Expectation-Propagation), a convenient and powerful way to\ncombine n local approximations into a global approximation. Compared to the EP-\nABC approach of Barthelm\\'e and Chopin (2014), we present two variations, one\nbased on the parallel EP algorithm of Cseke and Heskes (2011), which has the\nadvantage of being implementable on a parallel architecture, and one version\nwhich bridges the gap between standard EP and parallel EP. We illustrate our\napproach with an expensive application of ABC, namely inference on spatial\nextremes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 10:16:46 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Chopin", "Nicolas", ""], ["Cottet", "Vincent", ""]]}, {"id": "1512.00319", "submitter": "Michael Messer", "authors": "Michael Messer, Kau\\^e M. Costa, Jochen Roeper and Gaby Schneider", "title": "Multi-scale detection of rate changes in spike trains with weak\n  dependencies", "comments": "The final publication is available at http://link.springer.com", "journal-ref": null, "doi": "10.1007/s10827-016-0635-3", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of neuronal spike trains by models of point\nprocesses often relies on the assumption of constant process parameters.\nHowever, it is a well-known problem that the parameters of empirical spike\ntrains can be highly variable, such as for example the firing rate. In order to\ntest the null hypothesis of a constant rate and to estimate the change points,\na Multiple Filter Test (MFT) and a corresponding algorithm (MFA) have been\nproposed that can be applied under the assumption of independent inter spike\nintervals (ISIs).\n  As empirical spike trains often show weak dependencies in the correlation\nstructure of ISIs, we extend the MFT here to point processes associated with\nshort range dependencies. By specifically estimating serial dependencies in the\ntest statistic, we show that the new MFT can be applied to a variety of\nempirical firing patterns, including positive and negative serial correlations\nas well as tonic and bursty firing. The new MFT is applied to a data set of\nempirical spike trains with serial correlations, and simulations show improved\nperformance against methods that assume independence. In case of positive\ncorrelations, our new MFT is necessary to reduce the number of false positives,\nwhich can be highly enhanced when falsely assuming independence. For the\nfrequent case of negative correlations, the new MFT shows an improved detection\nprobability of change points and thus, also a higher potential of signal\nextraction from noisy spike trains.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 16:14:46 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 20:51:11 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Messer", "Michael", ""], ["Costa", "Kau\u00ea M.", ""], ["Roeper", "Jochen", ""], ["Schneider", "Gaby", ""]]}, {"id": "1512.00336", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Dmitry Trushin", "title": "Gaussian and Robust Kronecker Product Covariance Estimation: Existence\n  and Uniqueness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gaussian and robust covariance estimation, assuming the true\ncovariance matrix to be a Kronecker product of two lower dimensional square\nmatrices. In both settings we define the estimators as solutions to the\nconstrained maximum likelihood programs. In the robust case, we consider\nTyler's estimator defined as the maximum likelihood estimator of a certain\ndistribution on a sphere. We develop tight sufficient conditions for the\nexistence and uniqueness of the estimates and show that in the Gaussian\nscenario with the unknown mean, $p/q+q/p + 2$ samples are almost surely enough\nto guarantee the existence and uniqueness, where $p$ and $q$ are the dimensions\nof the Kronecker product factors. In the robust case with the known mean, the\ncorresponding sufficient number of samples is $\\max[p/q, q/p] + 1$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 17:03:55 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 22:55:05 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Trushin", "Dmitry", ""]]}, {"id": "1512.00487", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio", "title": "The Joint Projected and Skew Normal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multivariate circular linear distribution suitable for\nmodeling direction and speed in (multiple) animal movement data. To properly\naccount for specific data features, such as heterogeneity and time dependence,\na hidden Markov model is used. Parameters are estimated under a Bayesian\nframework and we provide computational details to implement the Markov chain\nMonte Carlo algorithm.\n  The proposed model is applied to a dataset of six free-ranging Maremma\nSheepdogs. Its predictive performance, as well as the interpretability of the\nresults, are compared to those given by hidden Markov models built on all the\ncombinations of von Mises (circular), wrapped Cauchy (circular), gamma (linear)\nand Weibull (linear) distributions\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:24:46 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 18:59:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Mastrantonio", "Gianluca", ""]]}, {"id": "1512.00792", "submitter": "Rebecca Steorts", "authors": "Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, and\n  Rebecca C. Steorts", "title": "Microclustering: When the Cluster Sizes Grow Sublinearly with the Size\n  of the Data Set", "comments": "8 pages, 3 figures, NIPS Bayesian Nonparametrics: The Next Generation\n  Workshop Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some tasks, this\nassumption is undesirable. For example, when performing entity resolution, the\nsize of each cluster is often unrelated to the size of the data set.\nConsequently, each cluster contains a negligible fraction of the total number\nof data points. Such tasks therefore require models that yield clusters whose\nsizes grow sublinearly with the size of the data set. We address this\nrequirement by defining the \\emph{microclustering property} and introducing a\nnew model that exhibits this property. We compare this model to several\ncommonly used clustering models by checking model fit using real and simulated\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:08:48 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Miller", "Jeffrey", ""], ["Betancourt", "Brenda", ""], ["Zaidi", "Abbas", ""], ["Wallach", "Hanna", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1512.00810", "submitter": "Carsten Allefeld", "authors": "Carsten Allefeld, Kai G\\\"orgen, John-Dylan Haynes", "title": "Valid population inference for information-based imaging: From the\n  second-level $t$-test to prevalence inference", "comments": "manuscript accepted by NeuroImage, plus minor fixes and a note added\n  after publication", "journal-ref": "NeuroImage 141: 378-392, 2016", "doi": "10.1016/j.neuroimage.2016.07.040", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate pattern analysis of neuroimaging data, 'second-level'\ninference is often performed by entering classification accuracies into a\n$t$-test vs chance level across subjects. We argue that while the\nrandom-effects analysis implemented by the $t$-test does provide population\ninference if applied to activation differences, it fails to do so in the case\nof classification accuracy or other 'information-like' measures, because the\ntrue value of such measures can never be below chance level. This constraint\nchanges the meaning of the population-level null hypothesis being tested, which\nbecomes equivalent to the global null hypothesis that there is no effect in any\nsubject in the population. Consequently, rejecting it only allows to infer that\nthere are some subjects in which there is an information effect, but not that\nit generalizes, rendering it effectively equivalent to fixed-effects analysis.\nThis statement is supported by theoretical arguments as well as simulations. We\nreview possible alternative approaches to population inference for\ninformation-based imaging, converging on the idea that it should not target the\nmean, but the prevalence of the effect in the population. One method to do so,\n'permutation-based information prevalence inference using the minimum\nstatistic', is described in detail and applied to empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:59:46 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 19:46:45 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2016 17:00:26 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Allefeld", "Carsten", ""], ["G\u00f6rgen", "Kai", ""], ["Haynes", "John-Dylan", ""]]}, {"id": "1512.00899", "submitter": "Ying Yang", "authors": "Ying Yang, Michael J. Tarr and Robert E. Kass", "title": "Estimating Learning Effects: A Short-Time Fourier Transform Regression\n  Model for MEG Source Localization", "comments": "Author manuscript accepted to 4th NIPS Workshop on Machine Learning\n  and Interpretation in Neuroimaging (2014), (in press on Lecture Notes in\n  Computer Science, by Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalography (MEG) has a high temporal resolution well-suited for\nstudying perceptual learning. However, to identify where learning happens in\nthe brain, one needs to ap- ply source localization techniques to project MEG\nsensor data into brain space. Previous source localization methods, such as the\nshort-time Fourier transform (STFT) method by Gramfort et al.([Gramfort et al.,\n2013]) produced intriguing results, but they were not designed to incor- porate\ntrial-by-trial learning effects. Here we modify the approach in [Gramfort et\nal., 2013] to produce an STFT-based source localization method (STFT-R) that\nincludes an additional regression of the STFT components on covariates such as\nthe behavioral learning curve. We also exploit a hierarchical L 21 penalty to\ninduce structured sparsity of STFT components and to emphasize signals from\nregions of interest (ROIs) that are selected according to prior knowl- edge. In\nreconstructing the ROI source signals from simulated data, STFT-R achieved\nsmaller errors than a two-step method using the popular minimum-norm estimate\n(MNE), and in a real-world human learning experiment, STFT-R yielded more\ninterpretable results about what time-frequency components of the ROI signals\nwere correlated with learning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:02:58 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Yang", "Ying", ""], ["Tarr", "Michael J.", ""], ["Kass", "Robert E.", ""]]}, {"id": "1512.00905", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Philip B. Stark", "title": "Shape-constrained uncertainty quantification in unfolding steeply\n  falling elementary particle spectra", "comments": "44 pages, 9 figures, to appear in the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy physics unfolding problem is an important statistical inverse\nproblem in data analysis at the Large Hadron Collider (LHC) at CERN. The goal\nof unfolding is to make nonparametric inferences about a particle spectrum from\nmeasurements smeared by the finite resolution of the particle detectors.\nPrevious unfolding methods use ad hoc discretization and regularization,\nresulting in confidence intervals that can have significantly lower coverage\nthan their nominal level. Instead of regularizing using a roughness penalty or\nstopping iterative methods early, we impose physically motivated shape\nconstraints: positivity, monotonicity, and convexity. We quantify the\nuncertainty by constructing a nonparametric confidence set for the true\nspectrum, consisting of all those spectra that satisfy the shape constraints\nand that predict the observations within an appropriately calibrated level of\nfit. Projecting that set produces simultaneous confidence intervals for all\nfunctionals of the spectrum, including averages within bins. The confidence\nintervals have guaranteed conservative frequentist finite-sample coverage in\nthe important and challenging class of unfolding problems for steeply falling\nparticle spectra. We demonstrate the method using simulations that mimic\nunfolding the inclusive jet transverse momentum spectrum at the LHC. The\nshape-constrained intervals provide usefully tight conservative inferences,\nwhile the conventional methods suffer from severe undercoverage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:46:05 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 01:38:00 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 16:00:56 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 01:43:03 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Kuusela", "Mikael", ""], ["Stark", "Philip B.", ""]]}, {"id": "1512.01026", "submitter": "Francesco Finazzi", "authors": "F. Finazzi, A. Fass\\`o", "title": "A statistical approach to crowdsourced smartphone-based earthquake early\n  warning systems", "comments": null, "journal-ref": null, "doi": "10.1785/0120150354", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Earthquake Network research project implements a crowdsourced earthquake\nearly warning system based on smartphones. Smartphones, which are made\navailable by the global population, exploit the Internet connection to report a\nsignal to a central server every time a vibration is detected by the on-board\naccelerometer sensor. This paper introduces a statistical approach for the\ndetection of earthquakes from the data coming from the network of smartphones.\nThe approach allows to handle a dynamic network in which the number of active\nnodes constantly changes and where nodes are heterogeneous in terms of sensor\nsensibility and transmission delay. Additionally, the approach allows to keep\nthe probability of false alarm under control. The statistical approach is\napplied to the data collected by three subnetworks related to the cities of\nSantiago de Chile, Iquique (Chile) and Kathmandu (Nepal). The detection\ncapabilities of the approach are discussed in terms of earthquake magnitude and\ndetection delay. A simulation study is carried out in order to link the\nprobability of detection and the detection delay to the behaviour of the\nnetwork under an earthquake event.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 10:26:53 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Finazzi", "F.", ""], ["Fass\u00f2", "A.", ""]]}, {"id": "1512.01065", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer and Leonhard Held", "title": "Incorporating social contact data in spatio-temporal models for\n  infectious disease spread", "comments": "accepted manuscript; 14 pages, including 4 figures and 1 table", "journal-ref": "Biostatistics (2017); 18(2):338-351", "doi": "10.1093/biostatistics/kxw051", "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routine public health surveillance of notifiable infectious diseases gives\nrise to weekly counts of reported cases -- possibly stratified by region and/or\nage group. We investigate how an age-structured social contact matrix can be\nincorporated into a spatio-temporal endemic-epidemic model for infectious\ndisease counts. To illustrate the approach, we analyze the spread of norovirus\ngastroenteritis over 6 age groups within the 12 districts of Berlin, 2011-2015,\nusing contact data from the POLYMOD study. The proposed age-structured model\noutperforms alternative scenarios with homogeneous or no mixing between age\ngroups. An extended contact model suggests a power transformation of the\nsurvey-based contact matrix towards more within-group transmission.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 13:02:57 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 15:50:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Meyer", "Sebastian", ""], ["Held", "Leonhard", ""]]}, {"id": "1512.01255", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Moritz Grosse-Wentrup, Arthur Gretton", "title": "MERLiN: Mixture Effect Recovery in Linear Networks", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 10(7),\n  1254-1266, 2016", "doi": "10.1109/JSTSP.2016.2601144", "report-no": null, "categories": "stat.ME q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables, e.g. establishing whether a stimulus affects activity in a\ncertain brain region. The observed variables themselves often do not constitute\nmeaningful causal variables, however, and linear combinations need to be\nconsidered. In electroencephalographic studies, for example, one is not\ninterested in establishing cause-effect relationships between electrode signals\n(the observed variables), but rather between cortical signals (the causal\nvariables) which can be recovered as linear combinations of electrode signals.\n  We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of\ncausal inference algorithms that implement a novel means of constructing causal\nvariables from non-causal variables. We demonstrate through application to EEG\ndata how the basic MERLiN algorithm can be extended for application to\ndifferent (neuroimaging) data modalities. Given an observed linear mixture, the\nalgorithms can recover a causal variable that is a linear effect of another\ngiven variable. That is, MERLiN allows us to recover a cortical signal that is\naffected by activity in a certain brain region, while not being a direct effect\nof the stimulus. The Python/Matlab implementation for all presented algorithms\nis available on https://github.com/sweichwald/MERLiN\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:29:30 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 17:43:18 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 21:27:57 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Grosse-Wentrup", "Moritz", ""], ["Gretton", "Arthur", ""]]}, {"id": "1512.01423", "submitter": "Liang Wu", "authors": "Liang Wu, Yang Li and Xuezheng Chen", "title": "On the Scale free laws of Urban Facilities", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.physleta.2015.10.045", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement a double stochastic process as the mathematical model for the\nspatial point patterns of urban facilities. We find that the model with power\ncovariance function can produce the best fit not only to $K$ function (whose\nderivative gives the radial distribution $\\rho(t) = K'(t)/2\\pi t$) but also to\nadditional facts of spatial point patterns. These facts include the\nmean-variance relationship of number of events in a series of expanding bins,\nand other statistics beyond the first two orders, such as inter-event\ndistribution function $H(t)$ and nearest neighbor distribution functions $G(t)$\nand $F(t)$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 04:33:23 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wu", "Liang", ""], ["Li", "Yang", ""], ["Chen", "Xuezheng", ""]]}, {"id": "1512.01496", "submitter": "Radu Craiu", "authors": "Roberto Casarin, Radu V. Craiu and Fabrizio Leisen", "title": "Embarrassingly Parallel Sequential Markov-chain Monte Carlo for Large\n  Sets of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian computation crucially relies on Markov chain Monte Carlo (MCMC)\nalgorithms. In the case of massive data sets, running the Metropolis-Hastings\nsampler to draw from the posterior distribution becomes prohibitive due to the\nlarge number of likelihood terms that need to be calculated at each iteration.\nIn order to perform Bayesian inference for a large set of time series, we\nconsider an algorithm that combines 'divide and conquer\" ideas previously used\nto design MCMC algorithms for big data with a sequential MCMC strategy. The\nperformance of the method is illustrated using a large set of financial data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 17:56:41 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Casarin", "Roberto", ""], ["Craiu", "Radu V.", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1512.01524", "submitter": "Rebecca Barter", "authors": "Rebecca L Barter and Bin Yu", "title": "Superheat: An R package for creating beautiful and extendable heatmaps\n  for visualizing complex data", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technological advancements of the modern era have enabled the collection\nof huge amounts of data in science and beyond. Extracting useful information\nfrom such massive datasets is an ongoing challenge as traditional data\nvisualization tools typically do not scale well in high-dimensional settings.\nAn existing visualization technique that is particularly well suited to\nvisualizing large datasets is the heatmap. Although heatmaps are extremely\npopular in fields such as bioinformatics for visualizing large gene expression\ndatasets, they remain a severely underutilized visualization tool in modern\ndata analysis. In this paper we introduce superheat, a new R package that\nprovides an extremely flexible and customizable platform for visualizing large\ndatasets using extendable heatmaps. Superheat enhances the traditional heatmap\nby providing a platform to visualize a wide range of data types simultaneously,\nadding to the heatmap a response variable as a scatterplot, model results as\nboxplots, correlation information as barplots, text information, and more.\nSuperheat allows the user to explore their data to greater depths and to take\nadvantage of the heterogeneity present in the data to inform analysis\ndecisions. The goal of this paper is two-fold: (1) to demonstrate the potential\nof the heatmap as a default visualization method for a wide range of data types\nusing reproducible examples, and (2) to highlight the customizability and ease\nof implementation of the superheat package in R for creating beautiful and\nextendable heatmaps. The capabilities and fundamental applicability of the\nsuperheat package will be explored via three case studies, each based on\npublicly available data sources and accompanied by a file outlining the\nstep-by-step analytic pipeline (with code).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 19:56:48 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 22:59:05 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Barter", "Rebecca L", ""], ["Yu", "Bin", ""]]}, {"id": "1512.01569", "submitter": "Stefano M. Iacus", "authors": "Stefano Maria Iacus and Giuseppe Porro and Silvia Salini and Elena\n  Siletti", "title": "Social networks, happiness and health: from sentiment analysis to a\n  multidimensional indicator of subjective well-being", "comments": "26 pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies a novel technique of opinion analysis over social media\ndata with the aim of proposing a new indicator of perceived and subjective\nwell-being. This new index, namely SWBI, examines several dimension of\nindividual and social life. The indicator has been compared to some other\nexisting indexes of well-being and health conditions in Italy: the BES\n(Benessere Equo Sostenibile), the incidence rate of influenza and the abundance\nof PM10 in urban environments. SWBI is a daily measure available at province\nlevel. BES data, currently available only for 2013 and 2014, are annual and\navailable at regional level. Flu data are weekly and distributed as regional\ndata and PM10 are collected daily for different cities. Due to the fact that\nthe time scale and space granularity of the different indexes varies, we apply\na novel statistical technique to discover nowcasting features and the classical\nlatent analysis to study the relationships among them. A preliminary analysis\nsuggest that the environmental and health conditions anticipate several\ndimensions of the perception of well-being as measured by SWBI. Moreover, the\nset of indicators included in the BES represent a latent dimension of\nwell-being which shares similarities with the latent dimension represented by\nSWBI.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 09:13:30 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Iacus", "Stefano Maria", ""], ["Porro", "Giuseppe", ""], ["Salini", "Silvia", ""], ["Siletti", "Elena", ""]]}, {"id": "1512.01689", "submitter": "Jordan Rodu", "authors": "Jordan Rodu, Shane T. Jensen", "title": "Locating recombination hot spots in genomic sequences through the\n  singular value decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locating recombination hotspots in genomic data is an important but difficult\ntask. Current methods frequently rely on estimating complicated models at high\ncomputational cost. In this paper we develop an extremely fast, scalable method\nfor inferring recombination hot spots in a population of genomic sequences that\nis based on the singular value decomposition. Our method performs well in\nseveral synthetic data scenarios. We also apply our technique to a real data\ninvestigation of the evolution of drug therapy resistance in a population of\nHIV genomic sequences. Finally, we compare our method both on real and\nsimulated data to a state of the art algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 17:28:31 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rodu", "Jordan", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1512.02306", "submitter": "Barbara Engelhardt", "authors": "Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt", "title": "Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait\n  Association Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies have proven to be essential for understanding\nthe genetic basis of disease. However, many complex traits---personality\ntraits, facial features, disease subtyping---are inherently high-dimensional,\nimpeding simple approaches to association mapping. We developed a nonparametric\nBayesian reduced rank regression model for multi-SNP, multi-trait association\nmapping that does not require the rank of the linear subspace to be specified.\nWe show in simulations and real data that our model shares strength over SNPs\nand over correlated traits, improving statistical power to identify genetic\nassociations with an interpretable, SNP-supervised low-dimensional linear\nprojection of the high-dimensional phenotype. On the HapMap phase 3 gene\nexpression QTL study data, we identify pleiotropic expression QTLs that\nclassical univariate tests are underpowered to find and that two step\napproaches cannot recover. Our Python software, BERRRI, is publicly available\nat GitHub: https://github.com/ashlee1031/BERRRI.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 02:25:12 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Valente", "Ashlee", ""], ["Ginsburg", "Geoffrey", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1512.02452", "submitter": "Allan De Freitas", "authors": "Allan De Freitas, Fran\\c{c}ois Septier, Lyudmila Mihaylova", "title": "Sequential Markov Chain Monte Carlo for Bayesian Filtering with Massive\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in digital sensors, digital data storage and communications have\nresulted in systems being capable of accumulating large collections of data. In\nthe light of dealing with the challenges that massive data present, this work\nproposes solutions to inference and filtering problems within the Bayesian\nframework. Two novel Bayesian inference algorithms are developed for non-linear\nand non-Gaussian state space models, able to deal with large volumes of data\n(or observations). These are sequential Markov chain Monte Carlo (MCMC)\napproaches relying on two key ideas: 1) subsample the massive data and utilise\na smaller subset for filtering and inference, and 2) a divide and conquer type\napproach computing local filtering distributions each using a subset of the\nmeasurements. Simulation results highlight the accuracy and the large\ncomputational savings, that can reach 90% by the proposed algorithms when\ncompared with standard techniques.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 13:20:03 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["De Freitas", "Allan", ""], ["Septier", "Fran\u00e7ois", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1512.02602", "submitter": "Sergio Pequito", "authors": "Cassiano O. Becker, Sergio Pequito, George J. Pappas, Michael B.\n  Miller, Scott T. Grafton, Danielle S. Bassett and Victor M. Preciado", "title": "Accurately Predicting Functional Connectivity from Diffusion Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between the dynamics of neural processes and\nthe anatomical substrate of the brain is a central question in neuroscience. On\nthe one hand, modern neuroimaging technologies, such as diffusion tensor\nimaging, can be used to construct structural graphs representing the\narchitecture of white matter streamlines linking cortical and subcortical\nstructures. On the other hand, temporal patterns of neural activity can be used\nto construct functional graphs representing temporal correlations between brain\nregions. Although some studies provide evidence that whole-brain functional\nconnectivity is shaped by the underlying anatomy, the observed relationship\nbetween function and structure is weak, and the rules by which anatomy\nconstrains brain dynamics remain elusive. In this article, we introduce a\nmethodology to predict with high accuracy the functional connectivity of a\nsubject at rest from his or her structural graph. Using our methodology, we are\nable to systematically unveil the role of structural paths in the formation of\nfunctional correlations. Furthermore, in our empirical evaluations, we observe\nthat the eigen-modes of the predicted functional connectivity are aligned with\nactivity patterns associated with different cognitive systems. Our work offers\nthe potential to infer properties of brain dynamics in clinical or\ndevelopmental populations with low tolerance for functional neuroimaging.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 19:53:53 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 00:34:56 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 17:36:08 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Becker", "Cassiano O.", ""], ["Pequito", "Sergio", ""], ["Pappas", "George J.", ""], ["Miller", "Michael B.", ""], ["Grafton", "Scott T.", ""], ["Bassett", "Danielle S.", ""], ["Preciado", "Victor M.", ""]]}, {"id": "1512.02688", "submitter": "Adrien Ickowicz", "authors": "Adrien Ickowicz, Ross Sparks", "title": "Modelling Hospital length of stay using convolutive mixtures\n  distributions", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length of hospital stay (LOS) is an important indicator of the hospital\nactivity and management of health care. The skewness in the distribution of LOS\nposes problems in statistical modelling because it fails to adequately follow\nthe usual traditional distribution such as the log-normal distribution. The aim\nof this work is to model the variable LOS using the convolution of two\ndistributions; a technique well known in the signal processing community. The\nspecificity of that model is that the variable of interest is considered to be\nthe resulting sum of two random variables with different distributions. One of\nthe variables will feature the patient-related factors in terms their need to\nrecover from their admission condition, while the other models the hospital\nmanagement process such as the discharging process. Two estimation procedures\nare proposed. One is the classical maximum likelihood, while the other relates\nto the expectation maximisation algorithm. We will present some results\nobtained by applying this model to a set of real data from a group of hospitals\nin Victoria (Australia).\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 22:57:58 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Ickowicz", "Adrien", ""], ["Sparks", "Ross", ""]]}, {"id": "1512.02834", "submitter": "Hannes Matuschek", "authors": "Hannes Matuschek and Reinhold Kliegl", "title": "On the Ambiguity of Interaction and Nonlinear Main Effects in a Regime\n  of Dependent Covariates", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": "10.3758/s13428-017-0956-9", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large experimental datasets frequently reveals significant\ninteractions that are difficult to interpret within the theoretical framework\nguiding the research. Some of these interactions actually arise from the\npresence of unspecified nonlinear main effects and statistically dependent\ncovariates in the statistical model. Importantly, such nonlinear main effects\nmay be compatible (or, at least, not incompatible) with the current theoretical\nframework. In the present literature this issue has only been studied in terms\nof correlated (linearly dependent) covariates. Here we generalize to nonlinear\nmain effects (i.e., main effects of arbitrary shape) and dependent covariates.\nWe propose a novel nonparametric method to test for ambiguous interactions\nwhere present parametric methods fail. We illustrate the method with a set of\nsimulations and with reanalyses (a) of effects of parental education on their\nchildren's educational expectations and (b) of effects of word properties on\nfixation locations during reading of natural sentences, specifically of effects\nof length and morphological complexity of the word to be fixated next. The\nresolution of such ambiguities facilitates theoretical progress.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 12:34:14 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:25:35 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Matuschek", "Hannes", ""], ["Kliegl", "Reinhold", ""]]}, {"id": "1512.02865", "submitter": "Mariusz Tarnopolski", "authors": "Mariusz Tarnopolski", "title": "Testing the anisotropy in the angular distribution of $Fermi$/GBM\n  gamma-ray bursts", "comments": "14 pages, 5 figures; accepted in MNRAS", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 472,\n  Issue 4, p.4819-4831, 2017", "doi": "10.1093/mnras/stx2356", "report-no": null, "categories": "astro-ph.HE astro-ph.CO hep-ph physics.space-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gamma-ray bursts (GRBs) were confirmed to be of extragalactic origin due to\ntheir isotropic angular distribution, combined with the fact that they\nexhibited an intensity distribution that deviated strongly from the $-3/2$\npower law. This finding was later confirmed with the first redshift, equal to\nat least $z=0.835$, measured for GRB970508. Despite this result, the data from\n$CGRO$/BATSE and $Swift$/BAT indicate that long GRBs are indeed distributed\nisotropically, but the distribution of short GRBs is anisotropic. $Fermi$/GBM\nhas detected 1669 GRBs up to date, and their sky distribution is examined in\nthis paper. A number of statistical tests is applied: nearest neighbour\nanalysis, fractal dimension, dipole and quadrupole moments of the distribution\nfunction decomposed into spherical harmonics, binomial test, and the two point\nangular correlation function. Monte Carlo benchmark testing of each test is\nperformed in order to evaluate its reliability. It is found that short GRBs are\ndistributed anisotropically on the sky, and long ones have an isotropic\ndistribution. The probability that these results are not a chance occurence is\nequal to at least 99.98\\% and 30.68\\% for short and long GRBs, respectively.\nThe cosmological context of this finding and its relation to large-scale\nstructures is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 14:16:32 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 12:58:40 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 20:31:29 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Tarnopolski", "Mariusz", ""]]}, {"id": "1512.02896", "submitter": "Farid M. Naini", "authors": "Farid M. Naini, Jayakrishnan Unnikrishnan, Patrick Thiran, Martin\n  Vetterli", "title": "Where You Are Is Who You Are: User Identification by Matching Statistics", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2015.2498131", "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most users of online services have unique behavioral or usage patterns. These\nbehavioral patterns can be exploited to identify and track users by using only\nthe observed patterns in the behavior. We study the task of identifying users\nfrom statistics of their behavioral patterns. Specifically, we focus on the\nsetting in which we are given histograms of users' data collected during two\ndifferent experiments. We assume that, in the first dataset, the users'\nidentities are anonymized or hidden and that, in the second dataset, their\nidentities are known. We study the task of identifying the users by matching\nthe histograms of their data in the first dataset with the histograms from the\nsecond dataset. In recent works, the optimal algorithm for this user\nidentification task is introduced. In this paper, we evaluate the effectiveness\nof this method on three different types of datasets and in multiple scenarios.\nUsing datasets such as call data records, web browsing histories, and GPS\ntrajectories, we show that a large fraction of users can be easily identified\ngiven only histograms of their data; hence these histograms can act as users'\nfingerprints. We also verify that simultaneous identification of users achieves\nbetter performance compared to one-by-one user identification. We show that\nusing the optimal method for identification gives higher identification\naccuracy than heuristics-based approaches in practical scenarios. The accuracy\nobtained under this optimal method can thus be used to quantify the maximum\nlevel of user identification that is possible in such settings. We show that\nthe key factors affecting the accuracy of the optimal identification algorithm\nare the duration of the data collection, the number of users in the anonymized\ndataset, and the resolution of the dataset. We analyze the effectiveness of\nk-anonymization in resisting user identification attacks on these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:23:33 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Naini", "Farid M.", ""], ["Unnikrishnan", "Jayakrishnan", ""], ["Thiran", "Patrick", ""], ["Vetterli", "Martin", ""]]}, {"id": "1512.02914", "submitter": "Christopher Marcum", "authors": "Christopher Steven Marcum", "title": "Yet Another Statistical Analysis of Bob Ross Paintings", "comments": "This version based off of arXiv compliant pdflatex source (which\n  result in lossy data). Original R-code, Sweave source, and data files are\n  available upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a sample of clippings from paintings by the late\nartist Bob Ross. Previous work focused on the qualitative themes of his\npaintings (Hickey, 2014); here, we expand on that line of research by\nconsidering the colorspace and luminosity values as our data. Our results\ndemonstrate the subtle aesthetics of the average Ross painting, the common\nvariation shared by his paintings, and the structure of the relationships\nbetween each painting in our sample. We reveal, for the first time, renderings\nof the average paintings and introduce \"eigenross\" components to identify and\nevaluate shared variance. Additionally, all data and code are embedded in this\ndocument to encourage future research, and, in the spirit of Bob Ross, to teach\nothers how to do so.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:57:46 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Marcum", "Christopher Steven", ""]]}, {"id": "1512.02988", "submitter": "Iain Johnston", "authors": "Iain G. Johnston, Joerg P. Burgstaller, Vitezslav Havlicek, Thomas\n  Kolbe, Thomas Rulicke, Gottfried Brem, Jo Poulton, Nick S. Jones", "title": "Stochastic modelling, Bayesian inference, and new in vivo measurements\n  elucidate the debated mtDNA bottleneck mechanism", "comments": "Main text: 14 pages, 5 figures; Supplement: 17 pages, 4 figures;\n  Total: 31 pages, 9 figures", "journal-ref": "eLife 4 e07464 (2015)", "doi": "10.7554/eLife.07464", "report-no": null, "categories": "q-bio.QM q-bio.SC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dangerous damage to mitochondrial DNA (mtDNA) can be ameliorated during\nmammalian development through a highly debated mechanism called the mtDNA\nbottleneck. Uncertainty surrounding this process limits our ability to address\ninherited mtDNA diseases. We produce a new, physically motivated, generalisable\ntheoretical model for mtDNA populations during development, allowing the first\nstatistical comparison of proposed bottleneck mechanisms. Using approximate\nBayesian computation and mouse data, we find most statistical support for a\ncombination of binomial partitioning of mtDNAs at cell divisions and random\nmtDNA turnover, meaning that the debated exact magnitude of mtDNA copy number\ndepletion is flexible. New experimental measurements from a wild-derived mtDNA\npairing in mice confirm the theoretical predictions of this model. We\nanalytically solve a mathematical description of this mechanism, computing\nprobabilities of mtDNA disease onset, efficacy of clinical sampling strategies,\nand effects of potential dynamic interventions, thus developing a quantitative\nand experimentally-supported stochastic theory of the bottleneck.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 18:47:45 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Johnston", "Iain G.", ""], ["Burgstaller", "Joerg P.", ""], ["Havlicek", "Vitezslav", ""], ["Kolbe", "Thomas", ""], ["Rulicke", "Thomas", ""], ["Brem", "Gottfried", ""], ["Poulton", "Jo", ""], ["Jones", "Nick S.", ""]]}, {"id": "1512.03350", "submitter": "Michael Fop", "authors": "Michael Fop, Keith Smart, Thomas Brendan Murphy", "title": "Variable Selection for Latent Class Analysis with Application to Low\n  Back Pain Diagnosis", "comments": "Published in The Annals of Applied Statistics by the Institute of\n  Mathematical Statistics", "journal-ref": "The Annals of Applied Statistics 2017, Vol. 11, No. 4, 2085-2115", "doi": "10.1214/17-AOAS1061", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of most relevant clinical criteria related to low back\npain disorders may aid the evaluation of the nature of pain suffered in a way\nthat usefully informs patient assessment and treatment. Data concerning low\nback pain can be of categorical nature, in the form of a check-list in which\neach item denotes presence or absence of a clinical condition. Latent class\nanalysis is a model-based clustering method for multivariate categorical\nresponses, which can be applied to such data for a preliminary diagnosis of the\ntype of pain. In this work, we propose a variable selection method for latent\nclass analysis applied to the selection of the most useful variables in\ndetecting the group structure in the data. The method is based on the\ncomparison of two different models and allows the discarding of those variables\nwith no group information and those variables carrying the same information as\nthe already selected ones. We consider a swap-stepwise algorithm where at each\nstep the models are compared through an approximation to their Bayes factor.\nThe method is applied to the selection of the clinical criteria most useful for\nthe clustering of patients in different classes. It is shown to perform a\nparsimonious variable selection and to give a clustering performance comparable\nto the expert-based classification of patients into three classes of pain.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 18:03:54 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 18:38:52 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Fop", "Michael", ""], ["Smart", "Keith", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1512.03489", "submitter": "Mohammad Amin Rahimian", "authors": "Victor M. Preciado and M. Amin Rahimian", "title": "Moment-Based Spectral Analysis of Random Graphs with Given Expected\n  Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.PR physics.soc-ph stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the limiting spectral distribution of the adjacency\nmatrix of a random graph ensemble, proposed by Chung and Lu, in which a given\nexpected degree sequence $\\overline{w}_n^{^{T}} = (w^{(n)}_1,\\ldots,w^{(n)}_n)$\nis prescribed on the ensemble. Let $\\mathbf{a}_{i,j} =1$ if there is an edge\nbetween the nodes $\\{i,j\\}$ and zero otherwise, and consider the normalized\nrandom adjacency matrix of the graph ensemble: $\\mathbf{A}_n$ $=$ $\n[\\mathbf{a}_{i,j}/\\sqrt{n}]_{i,j=1}^{n}$. The empirical spectral distribution\nof $\\mathbf{A}_n$ denoted by $\\mathbf{F}_n(\\mathord{\\cdot})$ is the empirical\nmeasure putting a mass $1/n$ at each of the $n$ real eigenvalues of the\nsymmetric matrix $\\mathbf{A}_n$. Under some technical conditions on the\nexpected degree sequence, we show that with probability one,\n$\\mathbf{F}_n(\\mathord{\\cdot})$ converges weakly to a deterministic\ndistribution $F(\\mathord{\\cdot})$. Furthermore, we fully characterize this\ndistribution by providing explicit expressions for the moments of\n$F(\\mathord{\\cdot})$. We apply our results to well-known degree distributions,\nsuch as power-law and exponential. The asymptotic expressions of the spectral\nmoments in each case provide significant insights about the bulk behavior of\nthe eigenvalue spectrum.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 00:22:53 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:48:57 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 21:53:41 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Preciado", "Victor M.", ""], ["Rahimian", "M. Amin", ""]]}, {"id": "1512.03955", "submitter": "Rui Li", "authors": "Rui Li, Robert Perneczky, Alexander Drzezga, Stefan Kramer (for the\n  Alzheimer's Disease Neuroimaging Initiative)", "title": "Survival analysis, the infinite Gaussian mixture model, FDG-PET and\n  non-imaging data in the prediction of progression from mild cognitive\n  impairment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to discover interesting brain regions in [18F]\nfluorodeoxyglucose positron emission tomography (PET) scans, showing also the\nbenefits when PET scans are in combined use with non-imaging variables. The\ndiscriminative brain regions facilitate a better understanding of Alzheimer's\ndisease (AD) progression, and they can also be used for predicting conversion\nfrom mild cognitive impairment (MCI) to AD. A survival analysis(Cox regression)\nand infinite Gaussian mixture model (IGMM) are introduced to identify the\ninformative brain regions, which can be further used to make a prediction of\nconversion (in two years) from MCI to AD using only the baseline PET scan.\nFurther, the predictive accuracy can be enhanced when non-imaging variables are\nused together with identified informative brain voxels. The results suggest\nthat PET scan imaging data is more predictive than other non-imaging data,\nrevealing even better performance when both imaging and non-imaging data are\ncombined.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 20:01:57 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Li", "Rui", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Perneczky", "Robert", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Drzezga", "Alexander", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Kramer", "Stefan", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1512.03990", "submitter": "Mauricio Santillana", "authors": "Mauricio Santillana, Andre Nguyen, Tamara Louie, Anna Zink, Josh Gray,\n  Iyue Sung, John S. Brownstein", "title": "Cloud-based Electronic Health Records for Real-time, Region-specific\n  Influenza Surveillance", "comments": null, "journal-ref": null, "doi": "10.1038/srep25732", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time monitoring systems of influenza outbreaks help public\nhealth officials make informed decisions that may help save lives. We show that\ninformation extracted from cloud-based electronic health records databases, in\ncombination with machine learning techniques and historical epidemiological\ninformation, have the potential to accurately and reliably provide near\nreal-time regional predictions of flu outbreaks in the United States.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 02:51:36 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Santillana", "Mauricio", ""], ["Nguyen", "Andre", ""], ["Louie", "Tamara", ""], ["Zink", "Anna", ""], ["Gray", "Josh", ""], ["Sung", "Iyue", ""], ["Brownstein", "John S.", ""]]}, {"id": "1512.04387", "submitter": "Yura Perov N", "authors": "Yura N Perov, Tuan Anh Le, Frank Wood", "title": "Data-driven Sequential Monte Carlo in Probabilistic Programming", "comments": "Black Box Learning and Inference, NIPS 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC)\nalgorithms in existing probabilistic programming systems suboptimally use only\nmodel priors as proposal distributions. In this work, we describe an approach\nfor training a discriminative model, namely a neural network, in order to\napproximate the optimal proposal by using posterior estimates from previous\nruns of inference. We show an example that incorporates a data-driven proposal\nfor use in a non-parametric model in the Anglican probabilistic programming\nsystem. Our results show that data-driven proposals can significantly improve\ninference performance so that considerably fewer particles are necessary to\nperform a good posterior estimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:18:32 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:34:52 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Perov", "Yura N", ""], ["Le", "Tuan Anh", ""], ["Wood", "Frank", ""]]}, {"id": "1512.04416", "submitter": "Danilo Orlando", "authors": "A. De Maio, D. Orlando, C. Hao, and G. Foglia", "title": "Adaptive Detection of Point-like Targets in Spectrally Symmetric\n  Interference", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2539140", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address adaptive radar detection of targets embedded in ground clutter\ndominated environments characterized by a symmetrically structured power\nspectral density. At the design stage, we leverage on the spectrum symmetry for\nthe interference to come up with decision schemes capable of capitalizing the\na-priori information on the covariance structure. To this end, we prove that\nthe detection problem at hand can be formulated in terms of real variables and,\nthen, we apply design procedures relying on the GLRT, the Rao test, and the\nWald test. Specifically, the estimates of the unknown parameters under the\ntarget presence hypothesis are obtained through an iterative optimization\nalgorithm whose convergence and quality guarantee is thoroughly proved. The\nperformance analysis, both on simulated and on real radar data, confirms the\nsuperiority of the considered architectures over their conventional\ncounterparts which do not take advantage of the clutter spectral symmetry.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 09:27:30 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["De Maio", "A.", ""], ["Orlando", "D.", ""], ["Hao", "C.", ""], ["Foglia", "G.", ""]]}, {"id": "1512.04486", "submitter": "Stephen Burgess", "authors": "Stephen Burgess, Jack Bowden", "title": "Integrating summarized data from multiple genetic variants in Mendelian\n  randomization: bias and coverage properties of inverse-variance weighted\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mendelian randomization is the use of genetic variants as instrumental\nvariables to assess whether a risk factor is a cause of a disease outcome.\nIncreasingly, Mendelian randomization investigations are conducted on the basis\nof summarized data, rather than individual-level data. These summarized data\ncomprise the coefficients and standard errors from univariate regression models\nof the risk factor on each genetic variant, and of the outcome on each genetic\nvariant. A causal estimate can be derived from these associations for each\nindividual genetic variant, and a combined estimate can be obtained by\ninverse-variance weighted meta-analysis of these causal estimates. Various\nproposals have been made for how to calculate this inverse-variance weighted\nestimate. In this paper, we show that the inverse-variance weighted method as\noriginally proposed (equivalent to a two-stage least squares or allele score\nanalysis using individual-level data) can lead to over-rejection of the null,\nparticularly when there is heterogeneity between the causal estimates from\ndifferent genetic variants. Random-effects models should be routinely employed\nto allow for this possible heterogeneity. Additionally, over-rejection of the\nnull is observed when associations with the risk factor and the outcome are\nobtained in overlapping participants. The use of weights including second-order\nterms from the delta method is recommended in this case.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 17:12:33 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Burgess", "Stephen", ""], ["Bowden", "Jack", ""]]}, {"id": "1512.04636", "submitter": "Alexander Wong", "authors": "Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic\n  Resonance Imaging via a Stochastically Fully-Connected Joint Conditional\n  Random Field Model", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion weighted magnetic resonance imaging (DW-MR) is a powerful tool in\nimaging-based prostate cancer screening and detection. Endorectal coils are\ncommonly used in DW-MR imaging to improve the signal-to-noise ratio (SNR) of\nthe acquisition, at the expense of significant intensity inhomogeneities (bias\nfield) that worsens as we move away from the endorectal coil. The presence of\nbias field can have a significant negative impact on the accuracy of different\nimage analysis tasks, as well as prostate tumor localization, thus leading to\nincreased inter- and intra-observer variability. Retrospective bias correction\napproaches are introduced as a more efficient way of bias correction compared\nto the prospective methods such that they correct for both of the scanner and\nanatomy-related bias fields in MR imaging. Previously proposed retrospective\nbias field correction methods suffer from undesired noise amplification that\ncan reduce the quality of bias-corrected DW-MR image. Here, we propose a\nunified data reconstruction approach that enables joint compensation of bias\nfield as well as data noise in DW-MR imaging. The proposed noise-compensated,\nbias-corrected (NCBC) data reconstruction method takes advantage of a novel\nstochastically fully connected joint conditional random field (SFC-JCRF) model\nto mitigate the effects of data noise and bias field in the reconstructed MR\ndata. The proposed NCBC reconstruction method was tested on synthetic DW-MR\ndata, physical DW-phantom as well as real DW-MR data all acquired using\nendorectal MR coil. Both qualitative and quantitative analysis illustrated that\nthe proposed NCBC method can achieve improved image quality when compared to\nother tested bias correction methods. As such, the proposed NCBC method may\nhave potential as a useful retrospective approach for improving the consistency\nof image interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 03:44:28 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 16:47:37 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Boroomand", "Ameneh", ""], ["Shafiee", "Mohammad Javad", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.04811", "submitter": "Yae-lin Sheu Dr.", "authors": "Yae-lin Sheu, Liang-Yan Hsu, Pi-Tai Chou, and Hau-tieng Wu", "title": "Entropy-based Time-Varying Window Width Selection for Nonlinear type\n  Time-Frequency Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s41060-017-0053-2", "report-no": null, "categories": "physics.data-an math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a time-varying optimal window width (TVOWW) selection scheme to\noptimize the performance of several nonlinear-type time-frequency analyses,\nincluding the reassignment method, and the synchrosqueezing transform (SST) and\nits variations. A window rendering the most concentrated distribution in the\ntime-frequency representation (TFR) is regarded as the optimal window. The\nTVOWW selection scheme is particularly useful for signals that comprise\nfast-varying instantaneous frequencies and small spectral gaps. To demonstrate\nthe efficacy of the method, in addition to analyzing a synthetic signal, we\nstudy an atomic time-varying dipole moment driven by two-color mid-infrared\nlaser fields in attosecond physics.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 06:14:52 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 15:30:45 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 05:39:19 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sheu", "Yae-lin", ""], ["Hsu", "Liang-Yan", ""], ["Chou", "Pi-Tai", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1512.04922", "submitter": "Ramesh Johari", "authors": "Ramesh Johari, Leo Pekelis, David J. Walsh", "title": "Always Valid Inference: Bringing Sequential Analysis to A/B Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B tests are typically analyzed via frequentist p-values and confidence\nintervals; but these inferences are wholly unreliable if users endogenously\nchoose samples sizes by *continuously monitoring* their tests. We define\n*always valid* p-values and confidence intervals that let users try to take\nadvantage of data as fast as it becomes available, providing valid statistical\ninference whenever they make their decision. Always valid inference can be\ninterpreted as a natural interface for a sequential hypothesis test, which\nempowers users to implement a modified test tailored to them. In particular, we\nshow in an appropriate sense that the measures we develop tradeoff sample size\nand power efficiently, despite a lack of prior knowledge of the user's relative\npreference between these two goals. We also use always valid p-values to obtain\nmultiple hypothesis testing control in the sequential context. Our methodology\nhas been implemented in a large scale commercial A/B testing platform to\nanalyze hundreds of thousands of experiments to date.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 20:33:31 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 07:12:05 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 19:42:42 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Johari", "Ramesh", ""], ["Pekelis", "Leo", ""], ["Walsh", "David J.", ""]]}, {"id": "1512.05073", "submitter": "Anish Mukherjee", "authors": "Ayanendranath Basu, Smarajit Bose, Amita Pal, Anish Mukherjee,\n  Debasmita Das", "title": "A Novel Minimum Divergence Approach to Robust Speaker Identification", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel solution to the speaker identification problem is\nproposed through minimization of statistical divergences between the\nprobability distribution (g). of feature vectors from the test utterance and\nthe probability distributions of the feature vector corresponding to the\nspeaker classes. This approach is made more robust to the presence of outliers,\nthrough the use of suitably modified versions of the standard divergence\nmeasures. The relevant solutions to the minimum distance methods are referred\nto as the minimum rescaled modified distance estimators (MRMDEs). Three\nmeasures were considered - the likelihood disparity, the Hellinger distance and\nPearson's chi-square distance. The proposed approach is motivated by the\nobservation that, in the case of the likelihood disparity, when the empirical\ndistribution function is used to estimate g, it becomes equivalent to maximum\nlikelihood classification with Gaussian Mixture Models (GMMs) for speaker\nclasses, a highly effective approach used, for example, by Reynolds [22] based\non Mel Frequency Cepstral Coefficients (MFCCs) as features. Significant\nimprovement in classification accuracy is observed under this approach on the\nbenchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, with\nMFCC features, both in isolation and in combination with delta MFCC features.\nMoreover, the ubiquitous principal component transformation, by itself and in\nconjunction with the principle of classifier combination, is found to further\nenhance the performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 07:29:53 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Bose", "Smarajit", ""], ["Pal", "Amita", ""], ["Mukherjee", "Anish", ""], ["Das", "Debasmita", ""]]}, {"id": "1512.05170", "submitter": "Eleni Matechou Dr", "authors": "E. Matechou, G. Nicholls, B. J. T. Morgan, J. A. Collazo, J. E. Lyons", "title": "Bayesian analysis of Jolly-Seber type models; incorporating\n  heterogeneity in arrival and departure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of finite mixtures of continuous distributions in\nmodelling the process by which new individuals, that arrive in groups, become\npart of a wildlife population. We demonstrate this approach using a data set of\nmigrating semipalmated sandpipers (Calidris pussila) for which we extend\nexisting stopover models to allow for individuals to have different behaviour\nin terms of their stopover duration at the site. We demonstrate the use of\nreversible jump MCMC methods to derive posterior distributions for the model\nparameters and the models, simultaneously. The algorithm moves between models\nwith different numbers of arrival groups as well as between models with\ndifferent numbers of behavioural groups. The approach is shown to provide new\necological insights about the stopover behaviour of semipalmated sandpipers but\nis generally applicable to any population in which animals arrive in groups and\npotentially exhibit heterogeneity in terms of one or more other processes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 13:33:31 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Matechou", "E.", ""], ["Nicholls", "G.", ""], ["Morgan", "B. J. T.", ""], ["Collazo", "J. A.", ""], ["Lyons", "J. E.", ""]]}, {"id": "1512.05212", "submitter": "Yuanfang Chen", "authors": "Yuanfang Chen, Noel Crespi, Gyu Myoung Lee", "title": "Reality Mining with Mobile Big Data: Understanding the Impact of Network\n  Structure on Propagation Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information and epidemic propagation dynamics in complex networks is truly\nimportant to discover and control terrorist attack and disease spread. How to\ntrack, recognize and model such dynamics is a big challenge. With the\npopularity of intellectualization and the rapid development of Internet of\nThings (IoT), massive mobile data is automatically collected by millions of\nwireless devices (e.g., smart phone and tablet). In this article, as a typical\nuse case, the impact of network structure on epidemic propagation dynamics is\ninvestigated by using the mobile data collected from the smart phones carried\nby the volunteers of Ebola outbreak areas. On this basis, we propose a model to\nrecognize the dynamic structure of a network. Then, we introduce and discuss\nthe open issues and future work for developing the proposed recognition model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 15:33:19 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Chen", "Yuanfang", ""], ["Crespi", "Noel", ""], ["Lee", "Gyu Myoung", ""]]}, {"id": "1512.05307", "submitter": "Joy D'Andrea Ms.", "authors": "R. D. Wooten, K. Baah, J. D'Andrea", "title": "Implicit Regression: Detecting Constants and Inverse Relationships with\n  Bivariate Random Error", "comments": "11 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2011, Wooten introduced Non-Response Analysis the founding theory in\nImplicit Regression where Implicit Regression treats the variables implicitly\nas codependent variables and not as an explicit function with dependent or\nindependent variables as in standard regression. The motivation of this paper\nis to introduce methods of implicit regression to determine the constant nature\nof a variable or the interactive term, and address inverse relationship among\nmeasured variables with random error present in both directions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 20:00:19 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Wooten", "R. D.", ""], ["Baah", "K.", ""], ["D'Andrea", "J.", ""]]}, {"id": "1512.05419", "submitter": "Chao Zheng", "authors": "Chao Zheng, Davide Ferrari, Michael Zhang, and Paul Baird", "title": "Ranking genetic factors related to age-related maculardegeneration by\n  variable selection confidence sets", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of generalized linear models in case-control genetic\nstudies has helped identify many disease-associated risk factors typically\ndefined as DNA variants, or single nucleotide polymorphisms (SNPs). Up to now,\nmost literature has focused on selecting a unique best subset of SNPs based on\nsome statistical perspectives. In the presence of pronounced noise, however,\nmultiple biological paths are often found to be equally supported by a given\ndataset when dealing with complex genetic diseases. We address the ambiguity\nrelated to SNP selection by constructing a list of models called variable\nselection confidence set (VSCS), which contains the collection of all\nwell-supported SNP combinations at a user-specified confidence level. The VSCS\nextends the familiar notion of confidence intervals in the variable selection\nsetting and provides the practitioner with new tools aiding the variable\nselection activity beyond trusting a single model. Based on the VSCS, we\nconsider natural graphical and numerical statistics measuring the inclusion\nimportance of a SNP based on its frequency in the most parsimonious VSCS\nmodels. This work is motivated by available case-control genetic data on\nage-related macular degeneration, a widespread complex disease and leading\ncause of vision loss.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 00:16:02 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 12:52:02 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zheng", "Chao", ""], ["Ferrari", "Davide", ""], ["Zhang", "Michael", ""], ["Baird", "Paul", ""]]}, {"id": "1512.05446", "submitter": "Mohammad Jafari Jozani", "authors": "Mohammad Nourmohammadi, Mohammad Jafari Jozani and Brad Johnson", "title": "Parametric inference for proportional (reverse) hazard rate models with\n  nomination sampling", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\noindent Randomized nomination sampling (RNS) is a rank-based sampling\ntechnique which has been shown to be effective in several nonparametric studies\ninvolving environmental and ecological applications. In this paper, we\ninvestigate parametric inference using RNS design for estimating the unknown\nvector of parameters $\\boldsymbol{\\theta}$ in the proportional hazard rate and\nproportional reverse hazard rate models. We examine both maximum likelihood\n(ML) and method of moments (MM) methods and investigate the relative precision\nof our proposed RNS-based estimators compared with those based on simple random\nsampling (SRS). We introduce four types of RNS-based data as well as necessary\nEM algorithms for the ML estimation, and evaluate the performance of\ncorresponding estimators in estimating $\\boldsymbol{\\theta}$. We show that\nthere are always values of the design parameters on which RNS-based estimators\nare more efficient than those based on SRS. Inference based on imperfect\nranking is also explored and it is shown that the improvement holds even when\nthe ranking is imperfect. Theoretical results are augmented with numerical\nevaluations and a case study.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 02:51:06 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Nourmohammadi", "Mohammad", ""], ["Jozani", "Mohammad Jafari", ""], ["Johnson", "Brad", ""]]}, {"id": "1512.05538", "submitter": "Kangrui Wang", "authors": "Kangrui Wang and Dalia Chakrabarty", "title": "Bayesian Covariance Modelling of Large Tensor-Variate Data Sets $\\&$\n  Inverse Non-parametric Learning of the Unknown Model Parameter Vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for modelling the covariance structure of tensor-variate\ndata, with the ulterior aim of learning an unknown model parameter vector using\nsuch data. We express the high-dimensional observable as a function of this\nsought model parameter vector, and attempt to learn such a high-dimensional\nfunction given training data, by modelling it as a realisation from a\ntensor-variate Gaussian Process (GP). The likelihood of the unknowns given\ntraining data, is then tensor-normal. We choose vague priors on the unknown GP\nparameters (mean tensor and covariance matrices) and write the posterior\nprobability density of these unknowns given the data. We perform posterior\nsampling using Random-Walk Metropolis-Hastings. Thereafter we learn the\naforementioned unknown model parameter vector by performing posterior sampling\nin two different ways, given test and training data, using MCMC, to generate\n95$\\%$ HPD credible region on each unknown. We make an application of this\nmethod to the learning of the location of the Sun in the Milky Way disk.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 11:24:48 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1512.06123", "submitter": "Orit Peleg Orit Peleg", "authors": "O. Peleg and L. Mahadevan", "title": "Optimal switching strategies for stochastic geocentric/egocentric\n  navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cond-mat.soft stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals use a combination of egocentric navigation driven by the internal\nintegration of environmental cues, interspersed with geocentric course\ncorrection and reorientation, often with uncertainty in sensory acquisition of\ninformation, planning and execution. Inspired directly by observations of dung\nbeetle navigational strategies that show switching between geocentric and\negocentric strategies, we consider the question of optimal strategies for the\nnavigation of an agent along a preferred direction in the presence of multiple\nsources of noise. We address this using a model that takes the form of a\ncorrelated random walk at short time scales that is interspersed with\nreorientation events that yields a biased random walks at long time scales. We\nidentify optimal alternation schemes and characterize their robustness in the\ncontext of noisy sensory acquisition, and performance errors linked with\nvariations in environmental conditions and agent-environment interactions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 13:04:47 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Peleg", "O.", ""], ["Mahadevan", "L.", ""]]}, {"id": "1512.06141", "submitter": "Bruce Desmarais", "authors": "Alison Craig, Skyler J. Cranmer, Bruce A. Desmarais, Christopher J.\n  Clark, Vincent G. Moscardelli", "title": "The Role of Race, Ethnicity, and Gender in the Congressional\n  Cosponsorship Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research indicates that race, ethnicity, and gender influence\nlegislative behavior in important ways. The bulk of this research, however,\nfocuses on the way these characteristics shape an individual legislator's\nbehavior, making it less clear how they account for relationships between\nlegislators. We study the cosponsorship process in order to understand the race\nand gender based dynamics underlying the relational component of\nrepresentation. Using a temporal exponential random graph model, we examine the\nU.S. House cosponsorship network from 1981 through 2004. We find that Black and\nLatino members of Congress are at a comparative disadvantage as a result of\nrace-based assortative mixing in the cosponsorship process, yet this\ndisadvantage is mitigated by the electoral pressures that all members face.\nMembers representing districts with significant racial and ethnic minority\npopulations are more likely to support their minority colleagues. We also find\nthat women members do not appear to face a similar disadvantage as a result of\ntheir minority status. We argue that these race and gender dynamics in the\ncosponsorship network are the result of both the inherent tendency towards\nintra-group homophily in social networks and the electoral connection, which is\nmanifested here as members supporting minority colleagues to broaden their own\nelectoral base of support among minority constituencies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 21:26:56 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Craig", "Alison", ""], ["Cranmer", "Skyler J.", ""], ["Desmarais", "Bruce A.", ""], ["Clark", "Christopher J.", ""], ["Moscardelli", "Vincent G.", ""]]}, {"id": "1512.06217", "submitter": "Jingyi Guo", "authors": "Jingyi Guo and H{\\aa}vard Rue and Andrea Riebler", "title": "Bayesian bivariate meta-analysis of diagnostic test studies with\n  interpretable priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a bivariate meta-analysis the number of diagnostic studies involved is\noften very low so that frequentist methods may result in problems. Bayesian\ninference is attractive as informative priors that add small amount of\ninformation can stabilise the analysis without overwhelming the data. However,\nBayesian analysis is often computationally demanding and the selection of the\nprior for the covariance matrix of the bivariate structure is crucial with\nlittle data. The integrated nested Laplace approximations (INLA) method\nprovides an efficient solution to the computational issues by avoiding any\nsampling, but the important question of priors remain. We explore the penalised\ncomplexity (PC) prior framework for specifying informative priors for the\nvariance parameters and the correlation parameter. PC priors facilitate model\ninterpretation and hyperparameter specification as expert knowledge can be\nincorporated intuitively. We conduct a simulation study to compare the\nproperties and behaviour of differently defined PC priors to currently used\npriors in the field. The simulation study shows that the use of PC priors\nresults in more precise estimates when specified in a sensible neighbourhood\naround the truth. To investigate the usage of PC priors in practice we\nreanalyse a meta-analysis using the telomerase marker for the diagnosis of\nbladder cancer.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:00:01 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Guo", "Jingyi", ""], ["Rue", "H\u00e5vard", ""], ["Riebler", "Andrea", ""]]}, {"id": "1512.06220", "submitter": "Jingyi Guo", "authors": "Jingyi Guo and Andrea Riebler", "title": "meta4diag: Bayesian Bivariate Meta-analysis of Diagnostic Test Studies\n  for Routine Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the \\proglang{R} package \\pkg{meta4diag} for\nimplementing Bayesian bivariate meta-analyses of diagnostic test studies. Our\npackage \\pkg{meta4diag} is a purpose-built front end of the \\proglang{R}\npackage \\pkg{INLA}. While \\pkg{INLA} offers full Bayesian inference for the\nlarge set of latent Gaussian models using integrated nested Laplace\napproximations, \\pkg{meta4diag} extracts the features needed for bivariate\nmeta-analysis and presents them in an intuitive way. It allows the user a\nstraightforward model-specification and offers user-specific prior\ndistributions. Further, the newly proposed penalised complexity prior framework\nis supported, which builds on prior intuitions about the behaviours of the\nvariance and correlation parameters. Accurate posterior marginal distributions\nfor sensitivity and specificity as well as all hyperparameters, and covariates\nare directly obtained without Markov chain Monte Carlo sampling. Further,\nunivariate estimates of interest, such as odds ratios, as well as the SROC\ncurve and other common graphics are directly available for interpretation. An\ninteractive graphical user interface provides the user with the full\nfunctionality of the package without requiring any \\proglang{R} programming.\nThe package is available through CRAN\n\\url{https://cran.r-project.org/web/packages/meta4diag/} and its usage will be\nillustrated using three real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:11:14 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 13:55:05 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Guo", "Jingyi", ""], ["Riebler", "Andrea", ""]]}, {"id": "1512.06273", "submitter": "Sheldon Lin", "authors": "Andrei L. Badescu, X. Sheldon Lin and Dameng Tang", "title": "A Marked Cox Model for IBNR Claims: Model and Theory", "comments": "25 pages, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incurred but not reported (IBNR) loss reserving is an important issue for\nProperty & Casualty (P&C) insurers. The modeling of the claim arrival process,\nespecially its temporal dependence, has not been closely examined in many of\nthe current loss reserving models.\n  In this paper, we propose modeling the claim arrival process together with\nits reporting delays as a marked Cox process. Our model is versatile in\nmodeling temporal dependence, allowing also for natural interpretations. This\npaper focuses mainly on the theoretical aspects of the proposed model. We show\nthat the associated reported claim process and\n  IBNR claim process are both marked Cox processes with easily convertible\nintensity functions and marking distributions. The proposed model can also\naccount for fluctuations in the exposure. By an order statistics property, we\nshow that the corresponding discretely observed process preserves all the\ninformation about the claim arrival epochs. Finally, we derive closed-form\nexpressions for both the autocorrelation function (ACF) and the distributions\nof the numbers of reported claims and IBNR claims. Model estimation and its\napplications are considered in a subsequent paper, Badescu et al.(2015b)\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 18:18:38 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Badescu", "Andrei L.", ""], ["Lin", "X. Sheldon", ""], ["Tang", "Dameng", ""]]}, {"id": "1512.06469", "submitter": "Tuan Phan", "authors": "Prasanta Bhattacharya, Tuan Q. Phan, Xue Bai and Edoardo Airoldi", "title": "A Co-evolution Model of Network Structure and User Behavior in Online\n  Social Networks: The Case of Network-Driven Content Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of online social network sites (SNS), it has become\nimperative for platform owners and online marketers to investigate what drives\ncontent production on these platforms. However, previous research has found it\ndifficult to statistically model these factors from observational data due to\nthe inability to separately assess the effects of network formation and network\ninfluence. In this paper, we adopt and enhance an actor-oriented\ncontinuous-time model to jointly estimate the co-evolution of the users' social\nnetwork structure and their content production behavior using a Markov Chain\nMonte Carlo (MCMC)- based simulation approach. Specifically, we offer a method\nto analyze non-stationary and continuous behavior with network effects in the\npresence of observable and unobservable covariates, similar to what is observed\nin social media ecosystems. Leveraging a unique dataset from a large social\nnetwork site, we apply our model to data on university students across six\nmonths to find that: 1) users tend to connect with others that have similar\nposting behavior, 2) however, after doing so, users tend to diverge in posting\nbehavior, and 3) peer influences are sensitive to the strength of the posting\nbehavior. Further, our method provides researchers and practitioners with a\nstatistically rigorous approach to analyze network effects in observational\ndata. These results provide insights and recommendations for SNS platforms to\nsustain an active and viable community.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 01:58:09 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 04:48:34 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bhattacharya", "Prasanta", ""], ["Phan", "Tuan Q.", ""], ["Bai", "Xue", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1512.07208", "submitter": "Alexander Dubbs", "authors": "Alexander Dubbs", "title": "Statistics-Free Sports Prediction", "comments": "Didn't like it in hindsight", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a simple machine learning model, logistically-weighted regularized\nlinear least squares regression, in order to predict baseball, basketball,\nfootball, and hockey games. We do so using only the thirty-year record of which\nvisiting teams played which home teams, on what date, and what the final score\nwas. No real \"statistics\" are used. The method works best in basketball, likely\nbecause it is high-scoring and has long seasons. It works better in football\nand hockey than in baseball, but in baseball the predictions are closer to a\ntheoretical optimum. The football predictions, while good, can in principle be\nmade much better, and the hockey predictions can be made somewhat better. These\nfindings tells us that in basketball, most statistics are subsumed by the\nscores of the games, whereas in football, further study of game and player\nstatistics is necessary to predict games as well as can be done. Baseball and\nhockey lie somewhere in between.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:02:01 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 11:53:22 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 16:36:08 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Dubbs", "Alexander", ""]]}, {"id": "1512.07560", "submitter": "Malek Ben Salem", "authors": "Malek Ben Salem (DEMO-ENSMSE), Olivier Roustant (DEMO-ENSMSE), Fabrice\n  Gamboa (IMT), Lionel Tomaso", "title": "Universal Prediction Distribution for Surrogate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of surrogate models instead of computationally expensive simulation\ncodes is very convenient in engineering. Roughly speaking, there are two kinds\nof surrogate models: the deterministic and the probabilistic ones. These last\nare generally based on Gaussian assumptions. The main advantage of\nprobabilistic approach is that it provides a measure of uncertainty associated\nwith the surrogate model in the whole space. This uncertainty is an efficient\ntool to construct strategies for various problems such as prediction\nenhancement, optimization or inversion.In this paper, we propose a universal\nmethod to define a measure of uncertainty suitable for any surrogate model\neither deterministic or probabilistic. It relies on Cross-Validation (CV)\nsub-models predictions. This empirical distribution may be computed in much\nmore general frames than the Gaussian one. So that it is called the Universal\nPrediction distribution (UP distribution).It allows the definition of many\nsampling criteria. We give and study adaptive sampling techniques for global\nrefinement and an extension of the so-called Efficient Global Optimization\n(EGO) algorithm. We also discuss the use of the UP distribution for inversion\nproblems. The performances of these new algorithms are studied both on toys\nmodels and on an engineering design problem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 17:52:24 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Salem", "Malek Ben", "", "DEMO-ENSMSE"], ["Roustant", "Olivier", "", "DEMO-ENSMSE"], ["Gamboa", "Fabrice", "", "IMT"], ["Tomaso", "Lionel", ""]]}, {"id": "1512.07607", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Bailey K. Fosdick, Devin S. Johnson,\n  Josh M. London, John W. Durban", "title": "Dynamic social networks based on movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network modeling techniques provide a means for quantifying social structure\nin populations of individuals. Data used to define social connectivity are\noften expensive to collect and based on case-specific, ad hoc criteria.\nMoreover, in applications involving animal social networks, collection of these\ndata is often opportunistic and can be invasive. Frequently, the social network\nof interest for a given population is closely related to the way individuals\nmove. Thus telemetry data, which are minimally-invasive and relatively\ninexpensive to collect, present an alternative source of information. We\ndevelop a framework for using telemetry data to infer social relationships\namong animals. To achieve this, we propose a Bayesian hierarchical model with\nan underlying dynamic social network controlling movement of individuals via\ntwo mechanisms: an attractive effect, and an aligning effect. We demonstrate\nthe model and its ability to accurately identify complex social behavior in\nsimulation, and apply our model to telemetry data arising from killer whales.\nUsing auxiliary information about the study population, we investigate model\nvalidity and find the inferred dynamic social network is consistent with killer\nwhale ecology and expert knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 20:03:13 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 00:36:45 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Fosdick", "Bailey K.", ""], ["Johnson", "Devin S.", ""], ["London", "Josh M.", ""], ["Durban", "John W.", ""]]}, {"id": "1512.08097", "submitter": "Roberto Rivera", "authors": "Roberto Rivera", "title": "A Dynamic Linear Model to Forecast Hotel Registrations in Puerto Rico\n  Using Google Trends Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, studies have used search query volume (SQV) data to forecast a\ngiven process of interest. However, Google Trends SQV data comes from a\nperiodic sample of queries. As a result, Google Trends data is different every\nweek. We propose a Dynamic Linear Model that treats SQV data as a\nrepresentation of an unobservable process. We apply our model to forecast the\nnumber of hotel nonresident registrations in Puerto Rico using SQV data\ndownloaded in 11 different occasions. The model provides better inference on\nthe association between the number of hotel nonresident registrations and SQV\nthan using Google Trends data retrieved only on one occasion. Furthermore, our\nmodel results in more realistic prediction intervals of forecasts. However,\ncompared to simpler models we only find evidence of better performance for our\nmodel when making forecasts on a horizon of over 6 months.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 10:30:55 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 12:48:58 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Rivera", "Roberto", ""]]}, {"id": "1512.08191", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB)", "title": "Estimation of Kullback-Leibler losses for noisy recovery problems within\n  the exponential family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the question of estimating Kullback-Leibler losses rather than\nsquared losses in recovery problems where the noise is distributed within the\nexponential family. Inspired by Stein unbiased risk estimator (SURE), we\nexhibit conditions under which these losses can be unbiasedly estimated or\nestimated with a controlled bias. Simulations on parameter selection problems\nin applications to image denoising and variable selection with Gamma and\nPoisson noises illustrate the interest of Kullback-Leibler losses and the\nproposed estimators.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 09:46:32 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 11:13:05 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 12:37:29 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"]]}, {"id": "1512.08495", "submitter": "Robert Wolpert", "authors": "Robert L. Wolpert, Sarah E. Ogburn, Eliza S. Calder", "title": "The Longevity of Lava Dome Eruptions", "comments": "17 pages, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1002/2015JB012435", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the duration of past, on-going and future volcanic eruptions is\nan important scientific goal and a key societal need. We present a new\nmethodology for forecasting the duration of on-going and future lava dome\neruptions based on a database (DomeHaz) recently compiled by the authors. The\ndatabase includes duration and composition for 177 such eruptions, with\n\"eruption\" defined as the period encompassing individual episodes of dome\ngrowth along with associated quiescent periods during which extrusion pauses\nbut unrest continues. In a key finding we show that probability distributions\nfor dome eruption durations are both heavy-tailed and composition-dependent. We\nconstruct Objective Bayes statistical models featuring heavy-tailed Generalized\nPareto distributions with composition-specific parameters to make forecasts\nabout the durations of new and on-going eruptions that depend on both eruption\nduration-to-date and composition. Our Bayesian predictive distributions reflect\nboth uncertainty about model parameter values (epistemic uncertainty) and the\nnatural variability of the geologic processes (aleatoric uncertainty). The\nresults are illustrated by presenting likely trajectories for fourteen\ndome-building eruptions on-going in 2015. Full representation of the\nuncertainty is presented for two key eruptions, Soufri{\\'{e}}re Hills Volcano\nin Montserrat (10--139 years, median 35yr) and Sinabung, Indonesia (1--17\nyears, median 4yr). Uncertainties are high, but, importantly, quantifiable.\nThis work provides for the first time a quantitative and transferable method\nand rationale on which to base long-term planning decisions for lava dome\nforming volcanoes, with wide potential use and transferability to forecasts of\nother types of eruptions and other adverse events across the geohazard\nspectrum.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 20:11:58 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 17:02:41 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Wolpert", "Robert L.", ""], ["Ogburn", "Sarah E.", ""], ["Calder", "Eliza S.", ""]]}, {"id": "1512.08569", "submitter": "Roger Bilisoly", "authors": "Roger Bilisoly", "title": "Analyzing Walter Skeat's Forty-Five Parallel Extracts of William\n  Langland's Piers Plowman", "comments": "Presented at the Joint Statistical Meetings 2015 in Seattle,\n  Washington. 9 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walter Skeat published his critical edition of William Langland's 14th\ncentury alliterative poem, Piers Plowman, in 1886. In preparation for this he\nlocated forty-five manuscripts, and to compare dialects, he published excerpts\nfrom each of these. This paper does three statistical analyses using these\nexcerpts, each of which mimics a task he did in writing his critical edition.\nFirst, he combined multiple versions of a poetic line to create a best line,\nwhich is compared to the mean string that is computed by a generalization of\nthe arithmetic mean that uses edit distance. Second, he claims that a certain\nsubset of manuscripts varies little. This is quantified by computing a string\nvariance, which is closely related to the above generalization of the mean.\nThird, he claims that the manuscripts fall into three groups, which is a\nclustering problem that is addressed by using edit distance. The overall goal\nis to develop methodology that would be of use to a literary critic.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 00:39:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Bilisoly", "Roger", ""]]}, {"id": "1512.08731", "submitter": "Y. Samuel Wang", "authors": "Y. Samuel Wang, Ross Matsueda, Elena A. Erosheva", "title": "A Variational EM Method for Mixed Membership Models with Multivariate\n  Rank Data: an Analysis of Public Policy Preferences", "comments": "24 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider modeling ranked responses from a heterogeneous\npopulation. Specifically, we analyze data from the Eurobarometer 34.1 survey\nregarding public policy preferences towards drugs, alcohol and AIDS. Such\npolicy preferences are likely to exhibit substantial differences within as well\nas across European nations reflecting a wide variety of cultures, political\naffiliations, ideological perspectives and common practices. We use a mixed\nmembership model to account for multiple subgroups with differing preferences\nand to allow each individual to possess partial membership in more than one\nsubgroup. Previous methods for fitting mixed membership models to rank data in\na univariate setting have utilized an MCMC approach and do not estimate the\nrelative frequency of each subgroup. We propose a variational EM approach for\nfitting mixed membership models with multivariate rank data. Our method allows\nfor fast approximate inference and explicitly estimates the subgroup sizes.\nAnalyzing the Eurobarometer 34.1 data, we find interpretable subgroups which\ngenerally agree with the \"left vs right\" classification of political\nideologies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 17:13:48 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 16:24:10 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 17:08:09 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Matsueda", "Ross", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1512.08775", "submitter": "Whitney Huang", "authors": "Whitney K. Huang, Michael L. Stein, David J. McInerney, Shanshan Sun,\n  and Elisabeth J. Moyer", "title": "Estimating changes in temperature extremes from millennial scale climate\n  simulations using generalized extreme value (GEV) distributions", "comments": "33 pages, 22 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in extreme weather may produce some of the largest societal impacts\nof anthropogenic climate change. However, it is intrinsically difficult to\nestimate changes in extreme events from the short observational record. In this\nwork we use millennial runs from the CCSM3 in equilibrated pre-industrial and\npossible future conditions to examine both how extremes change in this model\nand how well these changes can be estimated as a function of run length. We\nestimate changes to distributions of future temperature extremes (annual minima\nand annual maxima) in the contiguous United States by fitting generalized\nextreme value (GEV) distributions. Using 1000-year pre-industrial and future\ntime series, we show that the magnitude of warm extremes largely shifts in\naccordance with mean shifts in summertime temperatures. In contrast, cold\nextremes warm more than mean shifts in wintertime temperatures, but changes in\nGEV location parameters are largely explainable by mean shifts combined with\nreduced wintertime temperature variability. In addition, changes in the spread\nand shape of the GEV distributions of cold extremes at inland locations can\nlead to discernible changes in tail behavior. We then examine uncertainties\nthat result from using shorter model runs. In principle, the GEV distribution\nprovides theoretical justification to predict infrequent events using time\nseries shorter than the recurrence frequency of those events. To investigate\nhow well this approach works in practice, we estimate 20-, 50-, and 100-year\nextreme events using segments of varying lengths. We find that even using GEV\ndistributions, time series that are of comparable or shorter length than the\nreturn period of interest can lead to very poor estimates. These results\nsuggest caution when attempting to use short observational time series or model\nruns to infer infrequent extremes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 20:22:23 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 15:17:38 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 18:45:32 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Huang", "Whitney K.", ""], ["Stein", "Michael L.", ""], ["McInerney", "David J.", ""], ["Sun", "Shanshan", ""], ["Moyer", "Elisabeth J.", ""]]}, {"id": "1512.08996", "submitter": "Mingyuan Zhou", "authors": "Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou", "title": "Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices", "comments": "Appeared in Artificial Intelligence and Statistics (AISTATS), May\n  2015. The ArXiv version fixes a typo in (8), the equation right above Section\n  3.2 in Page 4 of http://www.jmlr.org/proceedings/papers/v38/acharya15.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A gamma process dynamic Poisson factor analysis model is proposed to\nfactorize a dynamic count matrix, whose columns are sequentially observed count\nvectors. The model builds a novel Markov chain that sends the latent gamma\nrandom variables at time $(t-1)$ as the shape parameters of those at time $t$,\nwhich are linked to observed or latent counts under the Poisson likelihood. The\nsignificant challenge of inferring the gamma shape parameters is fully\naddressed, using unique data augmentation and marginalization techniques for\nthe negative binomial distribution. The same nonparametric Bayesian model also\napplies to the factorization of a dynamic binary matrix, via a\nBernoulli-Poisson link that connects a binary observation to a latent count,\nwith closed-form conditional posteriors for the latent counts and efficient\ncomputation for sparse observations. We apply the model to text and music\nanalysis, with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 16:28:55 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Acharya", "Ayan", ""], ["Ghosh", "Joydeep", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1512.09052", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer, Ingeborg Warnke, Wulf R\\\"ossler, Leonhard Held", "title": "Model-based testing for space-time interaction using point processes: An\n  application to psychiatric hospital admissions in an urban area", "comments": "21 pages including 4 figures and 5 tables; methods are implemented in\n  the R package surveillance (https://CRAN.R-project.org/package=surveillance)", "journal-ref": "Spatial and Spatio-temporal Epidemiology 17, 15-25 (2016)", "doi": "10.1016/j.sste.2016.03.002", "report-no": null, "categories": "stat.ME physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal interaction is inherent to cases of infectious diseases and\noccurrences of earthquakes, whereas the spread of other events, such as cancer\nor crime, is less evident. Statistical significance tests of space-time\nclustering usually assess the correlation between the spatial and temporal\n(transformed) distances of the events. Although appealing through simplicity,\nthese classical tests do not adjust for the underlying population nor can they\naccount for a distance decay of interaction. We propose to use the framework of\nan endemic-epidemic point process model to jointly estimate a background event\nrate explained by seasonal and areal characteristics, as well as a superposed\nepidemic component representing the hypothesis of interest. We illustrate this\nnew model-based test for space-time interaction by analysing psychiatric\ninpatient admissions in Zurich, Switzerland (2007-2012). Several socio-economic\nfactors were found to be associated with the admission rate, but there was no\nevidence of general clustering of the cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 18:32:58 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 12:53:42 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Meyer", "Sebastian", ""], ["Warnke", "Ingeborg", ""], ["R\u00f6ssler", "Wulf", ""], ["Held", "Leonhard", ""]]}]