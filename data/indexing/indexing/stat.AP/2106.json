[{"id": "2106.00072", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu, Alexander Bukharin, Liyan Xie, Shihao Yang, Pinar\n  Keskinocak, Yao Xie", "title": "Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Centers for Disease Control and Prevention (CDC) has worked\nwith other federal agencies to identify counties with increasing coronavirus\ndisease 2019 (COVID-19) incidence (hotspots) and offers support to local health\ndepartments to limit the spread of the disease. Understanding the\nspatio-temporal dynamics of hotspot events is of great importance to support\npolicy decisions and prevent large-scale outbreaks. This paper presents a\nspatio-temporal Bayesian framework for early detection of COVID-19 hotspots (at\nthe county level) in the United States. We assume both the observed number of\ncases and hotspots depend on a class of latent random variables, which encode\nthe underlying spatio-temporal dynamics of the transmission of COVID-19. Such\nlatent variables follow a zero-mean Gaussian process, whose covariance is\nspecified by a non-stationary kernel function. The most salient feature of our\nkernel function is that deep neural networks are introduced to enhance the\nmodel's representative power while still enjoying the interpretability of the\nkernel. We derive a sparse model and fit the model using a variational learning\nstrategy to circumvent the computational intractability for large data sets.\nOur model demonstrates better interpretability and superior hotspot-detection\nperformance compared to other baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:28:17 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhu", "Shixiang", ""], ["Bukharin", "Alexander", ""], ["Xie", "Liyan", ""], ["Yang", "Shihao", ""], ["Keskinocak", "Pinar", ""], ["Xie", "Yao", ""]]}, {"id": "2106.00283", "submitter": "Michelangelo Misuraca", "authors": "Raffaele Mattera, Michelangelo Misuraca, Germana Scepi, Maria Spano", "title": "A mixed-frequency approach for exchange rates predictions", "comments": null, "journal-ref": "Electron J Appl Stat Anal 14 (2021) 230-253", "doi": "10.1285/i20705948v14n1p230", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Selecting an appropriate statistical model to forecast exchange rates is\nstill today a relevant issue for policymakers and central bankers. The\nso-called Meese and Rogoff puzzle assesses that exchange rate fluctuations are\nunpredictable. In the literature, a lot of studies tried to solve the puzzle\nfinding alternative predictors and statistical models based on temporal\naggregation. In this paper, we propose an approach based on mixed frequency\nmodels to overcome the lack of information caused by temporal aggregation. We\nshow the effectiveness of our approach in comparison with other proposed\nmethods by performing CAD/USD exchange rate predictions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:33:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mattera", "Raffaele", ""], ["Misuraca", "Michelangelo", ""], ["Scepi", "Germana", ""], ["Spano", "Maria", ""]]}, {"id": "2106.00356", "submitter": "Joel Persson", "authors": "Amray Schwabe, Joel Persson and Stefan Feuerriegel", "title": "Predicting COVID-19 Spread from Large-Scale Mobility Data", "comments": "9 pages, 3 figures. Accepted for publication in KDD '21: 27th ACM\n  SIGKDD Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To manage the COVID-19 epidemic effectively, decision-makers in public health\nneed accurate forecasts of case numbers. A potential near real-time predictor\nof future case numbers is human mobility; however, research on the predictive\npower of mobility is lacking. To fill this gap, we introduce a novel model for\nepidemic forecasting based on mobility data, called mobility marked Hawkes\nmodel. The proposed model consists of three components: (1) A Hawkes process\ncaptures the transmission dynamics of infectious diseases. (2) A mark modulates\nthe rate of infections, thus accounting for how the reproduction number R\nvaries across space and time. The mark is modeled using a regularized Poisson\nregression based on mobility covariates. (3) A correction procedure\nincorporates new cases seeded by people traveling between regions. Our model\nwas evaluated on the COVID-19 epidemic in Switzerland. Specifically, we used\nmobility data from February through April 2020, amounting to approximately 1.5\nbillion trips. Trip counts were derived from large-scale telecommunication\ndata, i.e., cell phone pings from the Swisscom network, the largest\ntelecommunication provider in Switzerland. We compared our model against\nvarious state-of-the-art baselines in terms of out-of-sample root mean squared\nerror. We found that our model outperformed the baselines by 15.52%. The\nimprovement was consistently achieved across different forecast horizons\nbetween 5 and 21 days. In addition, we assessed the predictive power of\nconventional point of interest data, confirming that telecommunication data is\nsuperior. To the best of our knowledge, our work is the first to predict the\nspread of COVID-19 from telecommunication data. Altogether, our work\ncontributes to previous research by developing a scalable early warning system\nfor decision-makers in public health tasked with controlling the spread of\ninfectious diseases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:05:02 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Schwabe", "Amray", ""], ["Persson", "Joel", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2106.00413", "submitter": "Kristian Svendsen", "authors": "Mohsen Askar, Raphael Nozal Ca\\~nadas, Kristian Svendsen", "title": "An introduction to network analysis for studies of medication use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Network Analysis (NA) is a method that has been used in various\ndisciplines such as Social sciences and Ecology for decades. So far, NA has not\nbeen used extensively in studies of medication use. Only a handful of papers\nhave used NA in Drug Prescription Networks (DPN). We provide an introduction to\nNA terminology alongside a guide to creating and extracting results from the\nmedication networks. Objective: To introduce the readers to NA as a tool to\nstudy medication use by demonstrating how to apply different NA measures on 3\ngenerated medication networks. Methods: We used the Norwegian Prescription\nDatabase (NorPD) to create a network that describes the co-medication in\nelderly persons in Norway on January 1, 2013. We used the Norwegian Electronic\nPrescription Support System (FEST) to create another network of severe\ndrug-drug interactions (DDIs). Lastly, we created a network combining the two\nnetworks to show the actual use of drugs with severe DDIs. We used these\nnetworks to elucidate how to apply and interpret different network measures in\nmedication networks. Results: Interactive network graphs are made available\nonline, Stata and R syntaxes are provided. Various useful network measures for\nmedication networks were applied such as network topological features,\nmodularity analysis and centrality measures. Edge lists data used to generate\nthe networks are openly available for readers in an open data repository to\nexplore and use. Conclusion: We believe that NA can be a useful tool in\nmedication use studies. We have provided information and hopefully inspiration\nfor other researchers to use NA in their own projects. While network analyses\nare useful for exploring and discovering structures in medication use studies,\nit also has limitations. It can be challenging to interpret and it is not\nsuitable for hypothesis testing.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:53:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Askar", "Mohsen", ""], ["Ca\u00f1adas", "Raphael Nozal", ""], ["Svendsen", "Kristian", ""]]}, {"id": "2106.00457", "submitter": "Andrea Gilardi", "authors": "Andrea Gilardi, Riccardo Borgoni and Jorge Mateu", "title": "A non-separable first-order spatio-temporal intensity for events on\n  linear networks: an application to ambulance interventions", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The algorithms used for optimal management of ambulances require accurate\ndescription and prediction of the spatio-temporal evolution of emergency\ninterventions. In the last years, several authors have proposed sophisticated\nstatistical approaches to forecast the ambulance dispatches, typically\nmodelling the events as a point pattern occurring on a planar region.\nNevertheless, ambulance interventions can be more appropriately modelled as a\nrealisation of a point process occurring along a network of lines, such as a\nroad network. The constrained spatial domain raises specific challenges and\nunique methodological problems that cannot be ignored when developing a proper\nstatistical model. Hence, this paper proposes a spatiotemporal model to analyse\nthe ambulance interventions that occurred in the road network of Milan (Italy)\nfrom 2015 to 2017. We adopt a non-separable first-order intensity function with\nspatial and temporal terms. The temporal component is estimated\nsemi-parametrically using a Poisson regression model, while the spatial\ndimension is estimated nonparametrically using a network kernel function. A set\nof weights is included in the spatial term to capture space-time interactions,\ninducing non-separability in the intensity function. A series of maps and\ngraphical tests show that our approach successfully models the ambulance\ninterventions and captures the space-time patterns.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:07:42 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gilardi", "Andrea", ""], ["Borgoni", "Riccardo", ""], ["Mateu", "Jorge", ""]]}, {"id": "2106.00577", "submitter": "The Tien Mai", "authors": "The Tien Mai", "title": "Efficient adaptive MCMC implementation for Pseudo-Bayesian quantum\n  tomography", "comments": "ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the Pseudo-Bayesian approach to the problem of estimating density\nmatrix in quantum state tomography in this paper. Pseudo-Bayesian inference has\nbeen shown to offer a powerful paradign for quantum tomography with attractive\ntheoretical and empirical results. However, the computation of\n(Pseudo-)Bayesian estimators, due to sampling from complex and high-dimensional\ndistribution, pose significant challenges that hampers their usages in\npractical settings. To overcome this problem, we present an efficient adaptive\nMCMC sampling method for the Pseudo-Bayesian estimator. We show in simulations\nthat our approach is substantially faster than the previous implementation by\nat least two orders of magnitude which is significant for practical quantum\ntomography.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:39:17 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Mai", "The Tien", ""]]}, {"id": "2106.00612", "submitter": "Wei Yi", "authors": "Hang Xiao, Shixing Yang, and Wei Yi", "title": "Weak target detection with multi-bit quantization in colocated MIMO\n  radar", "comments": "6 pages, 3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the weak target detection problem with unknown parameter in\ncolocated multiple-input multiple-output (MIMO) radar. To cope with the sheer\namount of data for large-size systems, a multi-bit quantizer is utilized in the\nsampling process. As a low-complexity alternative to classic generalized\nlikelihood ratio test (GLRT) for quantized data, we propose the multi-bit\ndetector on Rao test with a closed-form test statistic, whose theoretical\nasymptotic distribution is provided to generalize the actual detection\nperformance. Besides, we refine the design of quantizer by optimized\nquantization thresholds, which are obtained resorting to the popular particle\nswarm optimization algorithmthe (PSOA). The simulation is conducted to\ndemonstrate the performance variations of detectors based on unquantized and\nquantized data. The numerical results corroborate our theoretical analyses and\nshow that the performance with 3-bit quantization approaches the case without\nquantization.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 07:37:09 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 02:17:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xiao", "Hang", ""], ["Yang", "Shixing", ""], ["Yi", "Wei", ""]]}, {"id": "2106.00630", "submitter": "Christian Rohrbeck", "authors": "Christian Rohrbeck and Daniel Cooley", "title": "Simulating flood event sets using extremal principal components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hazard event sets, which correspond to a collection of synthetic flood\nevents, are an important tool for practitioners to analyse and manage future\nflood risks. In this paper, we address the issue of generating hazard event\nsets for northern England and southern Scotland, a region which has been\nparticularly affected by flooding over the past years. We start by analysing\nextreme river flow across 45 gauges in the region using recently introduced\nideas from extreme value theory. This results in a set of extremal principal\ncomponents, with the first components describing the large-scale structure of\nthe observed flood events, and we find interesting connections to the region's\ntopography and climate. We then introduce a framework to approximate the\ndistribution of the extremal principal components which is dimension reducing\nin that it distinctly handles the large-scale and local extremal behavior.\nSynthetic flood events are subsequently generated efficiently by sampling from\nthe fitted distribution. Our approach for generating hazard event sets can be\neasily implemented by practitioners and our results indicate good agreement\nbetween the observed and simulated extreme river flow dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:46:21 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Rohrbeck", "Christian", ""], ["Cooley", "Daniel", ""]]}, {"id": "2106.00688", "submitter": "Dolev Bashi", "authors": "Dolev Bashi and Shay Zucker", "title": "Quantifying the Similarity of Planetary System Architectures", "comments": "8 pages, 5 figures, accepted for publication in A&A, usage examples\n  at: https://github.com/dolevbas/PASSta", "journal-ref": "A&A 651, A61 (2021)", "doi": "10.1051/0004-6361/202140699", "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The planetary systems detected so far already exhibit a wide diversity of\narchitectures, and various methods are proposed to study quantitatively this\ndiversity. Straightforward ways to quantify the difference between two systems\nand more generally, two sets of multiplanetary systems, are useful tools in the\nstudy of this diversity. In this work we present a novel approach, using a\nWeighted extension of the Energy Distance (WED) metric, to quantify the\ndifference between planetary systems on the logarithmic period-radius plane. We\ndemonstrate the use of this metric and its relation to previously introduced\ndescriptive measures to characterise the arrangements of Kepler planetary\nsystems. By applying exploratory machine learning tools, we attempt to find\nwhether there is some order that can be ascribed to the set of Kepler\nmultiplanet system architectures. Based on WED, the 'Sequencer', which is such\nan automatic tool, identifies a progression from small and compact planetary\nsystems to systems with distant giant planets. It is reassuring to see that a\nWED-based tool indeed identifies this progression. Next, we extend WED to\ndefine the Inter-Catalogue Energy Distance (ICED) - a distance metric between\nsets of multiplanetary systems. We have made the specific implementation\npresented in the paper available to the community through a public repository.\nWe suggest to use these metrics as complementary tools in attempting to compare\nbetween architectures of planetary system, and in general, catalogues of\nplanetary systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 18:00:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bashi", "Dolev", ""], ["Zucker", "Shay", ""]]}, {"id": "2106.00762", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Divya Venugopalan, Chun Lo, Shaunak Chatterjee", "title": "A/B Testing for Recommender Systems in a Two-sided Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sided marketplaces are standard business models of many online platforms\n(e.g., Amazon, Facebook, LinkedIn), wherein the platforms have consumers,\nbuyers or content viewers on one side and producers, sellers or\ncontent-creators on the other. Consumer side measurement of the impact of a\ntreatment variant can be done via simple online A/B testing. \\textit{Producer\nside measurement is more challenging because the producer experience depends on\nthe treatment assignment of the consumers}. Existing approaches for producer\nside measurement are either based on graph cluster-based randomization or on\ncertain treatment propagation assumptions. The former approach results in\nlow-powered experiments as the producer-consumer network density increases and\nthe latter approach lacks a strict notion of error control. In this paper, we\npropose (i) a quantification of the quality of a producer side experiment, and\n(ii) a new experiment design mechanism that generates high quality experiments\nbased on this quantification. Our approach, called UniCoRn ({Uni}fying\n{Co}unterfactual {R}a{n}kings), provides explicit control over the quality of\nthe experiment and its computation cost. Further, we prove that our experiment\ndesign is optimal. Our approach is agnostic to the density of the\nproducer-consumer network and does not rely on any treatment propagation\nassumption. Moreover, unlike the existing approaches, we do not need to know\nthe underlying network in advance, making this widely applicable to the\nindustrial setting where the underlying network is unknown and challenging to\npredict a priori due to its dynamic nature. We use simulations to thoroughly\nvalidate our approach and compare it against existing methods. We also\nimplement UniCoRn in an edge recommendation application that serves tens of\nmillions of members and billions of edge recommendations daily.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:11:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nandy", "Preetam", ""], ["Venugopalan", "Divya", ""], ["Lo", "Chun", ""], ["Chatterjee", "Shaunak", ""]]}, {"id": "2106.00787", "submitter": "Piyush Sharma", "authors": "Piyush K. Sharma and Adrienne Raglin", "title": "Image-Audio Encoding to Improve C2 Decision-Making in Multi-Domain\n  Environment", "comments": "Published in: The 25th International Command and Control Research and\n  Technology Symposium (ICCRTS - 2020)", "journal-ref": "https://internationalc2institute.org/2020-experimentation-analysis-assessment-and-metrics", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The military is investigating methods to improve communication and agility in\nits multi-domain operations (MDO). Nascent popularity of Internet of Things\n(IoT) has gained traction in public and government domains. Its usage in MDO\nmay revolutionize future battlefields and may enable strategic advantage. While\nthis technology offers leverage to military capabilities, it comes with\nchallenges where one is the uncertainty and associated risk. A key question is\nhow can these uncertainties be addressed. Recently published studies proposed\ninformation camouflage to transform information from one data domain to\nanother. As this is comparatively a new approach, we investigate challenges of\nsuch transformations and how these associated uncertainties can be detected and\naddressed, specifically unknown-unknowns to improve decision-making.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:39:25 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:56:22 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sharma", "Piyush K.", ""], ["Raglin", "Adrienne", ""]]}, {"id": "2106.00788", "submitter": "Jason Poulos", "authors": "Jason Poulos, Andrea Albanese, Andrea Mercatanti, Fan Li", "title": "Retrospective causal inference via matrix completion, with an evaluation\n  of the effect of European integration on cross-border employment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of retrospective counterfactual imputation in panel data\nsettings with later-treated and always-treated units, but no never-treated\nunits. We use the observed outcomes to impute the counterfactual outcomes of\nthe later-treated using a matrix completion estimator. We propose a novel\npropensity-score and elapsed-time weighting of the estimator's objective\nfunction to correct for differences in the observed covariate and unobserved\nfixed effects distributions, and elapsed time since treatment between groups.\nOur methodology is motivated by studying the effect of two milestones of\nEuropean integration -- the Free Movement of persons and the Schengen Agreement\n-- on the share of cross-border workers in sending border regions. We apply the\nproposed method to the European Labour Force Survey (ELFS) data and provide\nevidence that opening the border almost doubled the probability of working\nbeyond the border in Eastern European regions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:42:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Poulos", "Jason", ""], ["Albanese", "Andrea", ""], ["Mercatanti", "Andrea", ""], ["Li", "Fan", ""]]}, {"id": "2106.00876", "submitter": "Garrett A. Stevenson", "authors": "Garrett A. Stevenson, Jason Wilson, Brian M. Worthmann, Wlamir Xavier", "title": "Modeling Buried Object Brightness and Visibility for Ground Penetrating\n  Radar", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2021.3084100", "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Comparing the observed brightness of various buried objects is a\nstraightforward way to characterize the performance of a ground penetrating\nradar (GPR) system. However, a limitation arises. A simple comparison of buried\nobject brightness values does not disentangle the effects of the GPR system\nitself from the system's operating environment and the objects being observed.\nTherefore, with brightness values exhibiting an unknown synthesis of systemic,\nenvironmental, and object factors, GPR system analysis becomes a convoluted\naffair. In this work, we use an experimentally collected dataset of over 25,000\nobject observations from five different multi-static radar arrays to develop\nmodels of buried object brightness and control for these various effects. Our\nmodeling efforts provide a means for quantifying the relative brightness of GPR\nsystems, the objects they detect, and the physical properties of those objects\nwhich influence observed brightness. To evaluate the models' performance on new\nobject observations, we repeatedly simulate fitting them to half the dataset\nand predicting the observed brightness values of the unseen half. Additionally,\nwe introduce a method for estimating the probability that individual\nobservations constitute a visible object, which aids in failure analysis,\nperformance characterization, and dataset cleaning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:22:44 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Stevenson", "Garrett A.", ""], ["Wilson", "Jason", ""], ["Worthmann", "Brian M.", ""], ["Xavier", "Wlamir", ""]]}, {"id": "2106.00888", "submitter": "Lijun Sun Mr", "authors": "Kangli Zhu, Zhanhong Cheng, Jianjun Wu, Fuya Yuan, Lijun Sun", "title": "Quantifying out-of-station waiting time in oversaturated urban metro\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metro systems in megacities such as Beijing, Shenzhen and Guangzhou are under\ngreat passenger demand pressure. During peak hours, it is common to see\noversaturated conditions (i.e., passenger demand exceeds network capacity),\nwhich bring significant operational risks and safety issues. A popular control\nintervention is to restrict the entering rate during peak hours by setting up\nout-of-station queueing with crowd control barriers. The \\textit{out-of-station\nwaiting} can make up a substantial proportion of total travel time but is not\nwell-studied in the literature. Accurate quantification of out-of-station\nwaiting time is important to evaluating the social benefit and cost of service\nscheduling/optimization plans; however, out-of-station waiting time is\ndifficult to estimate because it is not a part of smart card transactions. In\nthis study, we propose an innovative method to estimate the out-of-station\nwaiting time by leveraging the information from a small group of transfer\npassengers -- those who transfer from nearby bus routes to the metro station.\nBased on the estimated transfer time for this small group, we first infer the\nout-of-station waiting time for all passengers by developing a Gaussian Process\nregression with a Student-$t$ likelihood and then use the estimated\nout-of-station waiting time to build queueing diagrams. We apply our method to\nthe Tiantongyuan North station of Beijing metro as a case study; our results\nshow that the maximum out-of-station waiting time can reach 15 minutes, and the\nmaximum queue length can be over 3000 passengers. Our results suggest that\nout-of-station waiting can cause significant travel costs and thus should be\nconsidered in analyzing transit performance, mode choice, and social benefits.\nTo the best of our knowledge, this paper is the first quantitative study for\nout-of-station waiting time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:58:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhu", "Kangli", ""], ["Cheng", "Zhanhong", ""], ["Wu", "Jianjun", ""], ["Yuan", "Fuya", ""], ["Sun", "Lijun", ""]]}, {"id": "2106.00911", "submitter": "Rosy Oh", "authors": "Jae Youn Ahn, Eric C.K. Cheung, Rosy Oh, Jae-Kyung Woo", "title": "Optimal relativities in a modified Bonus-Malus system with long memory\n  transition rules and frequency-severity dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the classical Bonus-Malus System (BMS) in automobile insurance, the\npremium for the next year is adjusted according to the policyholder's claim\nhistory (particularly frequency) in the previous year. Some variations of the\nclassical BMS have been considered by taking more of driver's claim experience\ninto account to better assess individual's risk. Nevertheless, we note that in\npractice it is common for a BMS to adopt transition rules according to the\nclaim history for the past multiple years in countries such as Belgium, Italy,\nKorea, and Singapore. In this paper, we revisit a modified BMS which was\nbriefly introduced in Lemaire (1995) and Pitrebois et al. (2003a).\nSpecifically, such a BMS extends the number of Bonus-Malus (BM) levels due to\nan additional component in the transition rules representing the number of\nconsecutive claim-free years. With the extended BM levels granting more\nreasonable bonus to careful drivers, this paper investigates the transition\nrules in a more rigorous manner, and provides the optimal BM relativities under\nvarious statistical model assumptions including the frequency random effect\nmodel and the dependent collective risk model. Also, numerical analysis of a\nreal data set is provided to compare the classical BMS and our proposed BMS.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:08:25 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 03:48:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ahn", "Jae Youn", ""], ["Cheung", "Eric C. K.", ""], ["Oh", "Rosy", ""], ["Woo", "Jae-Kyung", ""]]}, {"id": "2106.01125", "submitter": "Mohammed Es.Sebaiy", "authors": "Azzouz Dermoune, Mohammed Es.Sebaiy and Jabrane Moustaaid", "title": "Interpolation and linear prediction of data -- three kernel selection\n  criteria", "comments": "18 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpolation and prediction have been useful approaches in modeling data in\nmany areas of applications. The aim of this paper is the prediction of the next\nvalue of a time series (time series forecasting) using the techniques in\ninterpolation of the spatial data, for the tow approaches kernel interpolation\nand kriging. We are interested in finding some sufficient conditions for the\nkernels and provide a detailed analyse of the prediction using kernel\ninterpolation. Finally, we provide a natural idea to select a good kernel among\na given family of kernels using only the data. We illustrate our results by\napplication to the data set on the mean annual temperature of France and\nMorocco recorded for a period of 115 years (1901 to 2015).\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:48:22 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:52:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dermoune", "Azzouz", ""], ["Sebaiy", "Mohammed Es.", ""], ["Moustaaid", "Jabrane", ""]]}, {"id": "2106.01129", "submitter": "Aurora Torrente", "authors": "Javier Albert-Smet, Aurora Torrente, Juan Romo", "title": "Band Depth based initialization of $k$-Means for functional data\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $k$-Means algorithm is one of the most popular choices for clustering\ndata but is well-known to be sensitive to the initialization process. There is\na substantial number of methods that aim at finding optimal initial seeds for\n$k$-Means, though none of them are universally valid. This paper presents an\nextension to longitudinal data of one of such methods, the BRIk algorithm, that\nrelies on clustering a set of centroids derived from bootstrap replicates of\nthe data and on the use of the versatile Modified Band Depth. In our approach\nwe improve the BRIk method by adding a step where we fit appropriate B-splines\nto our observations and a resampling process that allows computational\nfeasibility and handling issues such as noise or missing data. Our results with\nsimulated and real data sets indicate that our $F$unctional Data $A$pproach to\nthe BRIK method (FABRIk) is more effective than previous proposals at providing\nseeds to initialize $k$-Means in terms of clustering recovery.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:51:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Albert-Smet", "Javier", ""], ["Torrente", "Aurora", ""], ["Romo", "Juan", ""]]}, {"id": "2106.01140", "submitter": "Georgy Meshcheryakov", "authors": "Georgy Meshcheryakov, Anna A. Igolkina, Maria G. Samsonova", "title": "semopy 2: A Structural Equation Modeling Package with Random Effects in\n  Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural Equation Modeling (SEM) is an umbrella term that includes numerous\nmultivariate statistical techniques that are employed throughout a plethora of\nresearch areas, ranging from social to natural sciences. Until recently, SEM\nsoftware was either commercial or restricted to niche languages, and the lack\nof SEM packages compatible with more mainstream programming languages was dire.\nTo combat that, we introduced a Python package semopy 1 that surpassed other\nstate-of-the-art software in terms of performance and estimation accuracy. Yet,\nit was lacking in functionality and its usage was burdened with unnecessary\nboilerplate code. Here, we introduce a complete overhaul of semopy that\nimproves upon the previous results and comes with lots of new capabilities.\nFurthermore, we propose a novel SEM model that combines in itself a notion of\nrandom effects from linear mixed models (LMMs) to model numerous phenomena,\nsuch as spatial data, time series or population stratification in genetics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:18:03 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 10:20:12 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 17:35:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Meshcheryakov", "Georgy", ""], ["Igolkina", "Anna A.", ""], ["Samsonova", "Maria G.", ""]]}, {"id": "2106.01421", "submitter": "Weitao Duan", "authors": "Weitao Duan, Shan Ba, Chunzhe Zhang", "title": "Online Experimentation with Surrogate Metrics: Guidelines and a Case\n  Study", "comments": null, "journal-ref": "Proceedings of the 14th ACM International Conference on Web Search\n  and Data Mining (2021)", "doi": "10.1145/3437963.3441737", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A/B tests have been widely adopted across industries as the golden rule that\nguides decision making. However, the long-term true north metrics we ultimately\nwant to drive through A/B test may take a long time to mature. In these\nsituations, a surrogate metric which predicts the long-term metric is often\nused instead to conclude whether the treatment is effective. However, because\nthe surrogate rarely predicts the true north perfectly, a regular A/B test\nbased on surrogate metrics tends to have high false positive rate and the\ntreatment variant deemed favorable from the test may not be the winning one. In\nthis paper, we discuss how to adjust the A/B testing comparison to ensure\nexperiment results are trustworthy. We also provide practical guidelines on the\nchoice of good surrogate metrics. To provide a concrete example of how to\nleverage surrogate metrics for fast decision making, we present a case study on\ndeveloping and evaluating the predicted confirmed hire surrogate metric in\nLinkedIn job marketplace.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:03:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Duan", "Weitao", ""], ["Ba", "Shan", ""], ["Zhang", "Chunzhe", ""]]}, {"id": "2106.01485", "submitter": "Chengliang Tang", "authors": "Chengliang Tang, Gan Yuan, Tian Zheng", "title": "Weakly Supervised Learning Creates a Fusion of Modeling Cultures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past two decades have witnessed the great success of the algorithmic\nmodeling framework advocated by Breiman et al. (2001). Nevertheless, the\nexcellent prediction performance of these black-box models rely heavily on the\navailability of strong supervision, i.e. a large set of accurate and exact\nground-truth labels. In practice, strong supervision can be unavailable or\nexpensive, which calls for modeling techniques under weak supervision. In this\ncomment, we summarize the key concepts in weakly supervised learning and\ndiscuss some recent developments in the field. Using algorithmic modeling alone\nunder a weak supervision might lead to unstable and misleading results. A\npromising direction would be integrating the data modeling culture into such a\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:52:05 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tang", "Chengliang", ""], ["Yuan", "Gan", ""], ["Zheng", "Tian", ""]]}, {"id": "2106.01552", "submitter": "Luyao Lin", "authors": "Luyao Lin, Derek Bingham, Floor Broekgaarden, Ilya Mandel", "title": "Uncertainty Quantification of a Computer Model for Binary Black Hole\n  Formation", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.HE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast and parallelizable method based on Gaussian Processes\n(GPs) is introduced to emulate computer models that simulate the formation of\nbinary black holes (BBHs) through the evolution of pairs of massive stars. Two\nobstacles that arise in this application are the a priori unknown conditions of\nBBH formation and the large scale of the simulation data. We address them by\nproposing a local emulator which combines a GP classifier and a GP regression\nmodel. The resulting emulator can also be utilized in planning future computer\nsimulations through a proposed criterion for sequential design. By propagating\nuncertainties of simulation input through the emulator, we are able to obtain\nthe distribution of BBH properties under the distribution of physical\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:35:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lin", "Luyao", ""], ["Bingham", "Derek", ""], ["Broekgaarden", "Floor", ""], ["Mandel", "Ilya", ""]]}, {"id": "2106.01590", "submitter": "Roberto Vega", "authors": "Roberto Vega, Leonardo Flores, Russell Greiner", "title": "SIMLR: Machine Learning inside the SIR model for COVID-19 Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate forecasts of the number of newly infected people during an epidemic\nare critical for making effective timely decisions. This paper addresses this\nchallenge using the SIMLR model, which incorporates machine learning (ML) into\nthe epidemiological SIR model. For each region, SIMLR tracks the changes in the\npolicies implemented at the government level, which it uses to estimate the\ntime-varying parameters of an SIR model for forecasting the number of new\ninfections 1- to 4-weeks in advance.It also forecasts the probability of\nchanges in those government policies at each of these future times, which is\nessential for the longer-range forecasts. We applied SIMLR to data from regions\nin Canada and in the United States,and show that its MAPE (mean average\npercentage error) performance is as good as SOTA forecasting models, with the\nadded advantage of being an interpretable model. We expect that this approach\nwill be useful not only for forecasting COVID-19 infections, but also in\npredicting the evolution of other infectious diseases.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 04:22:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Vega", "Roberto", ""], ["Flores", "Leonardo", ""], ["Greiner", "Russell", ""]]}, {"id": "2106.01694", "submitter": "Shengkun Zhu", "authors": "Shengkun Zhu", "title": "Analysis and Evaluation of the Inequality of the Spatial Distribution of\n  Medical Resources in Jinan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article will analyze the inequality and evaluation of the spatial\ndistribution of medical resources in Jinan. The research will be carried out\nfrom the following four aspects: analysis of existing medical resource\nallocation and distribution characteristics, medical resource accessibility\nanalysis, inequality evaluation and optimization layout analysis. The article\nwill use G2SFCA/M2SFCA Model, Spatial Clustering Analysis and HRAD.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:52:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Shengkun", ""]]}, {"id": "2106.01713", "submitter": "Bruno Sudret", "authors": "M. Moustapha, S. Marelli and B. Sudret", "title": "A generalized framework for active learning reliability: survey and\n  benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-002", "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning methods have recently surged in the literature due to their\nability to solve complex structural reliability problems within an affordable\ncomputational cost. These methods are designed by adaptively building an\ninexpensive surrogate of the original limit-state function. Examples of such\nsurrogates include Gaussian process models which have been adopted in many\ncontributions, the most popular ones being the efficient global reliability\nanalysis (EGRA) and the active Kriging Monte Carlo simulation (AK-MCS), two\nmilestone contributions in the field. In this paper, we first conduct a survey\nof the recent literature, showing that most of the proposed methods actually\nspan from modifying one or more aspects of the two aforementioned methods. We\nthen propose a generalized modular framework to build on-the-fly efficient\nactive learning strategies by combining the following four ingredients or\nmodules: surrogate model, reliability estimation algorithm, learning function\nand stopping criterion. Using this framework, we devise 39 strategies for the\nsolution of 20 reliability benchmark problems. The results of this extensive\nbenchmark are analyzed under various criteria leading to a synthesized set of\nrecommendations for practitioners. These may be refined with a priori knowledge\nabout the feature of the problem to solve, i.e., dimensionality and magnitude\nof the failure probability. This benchmark has eventually highlighted the\nimportance of using surrogates in conjunction with sophisticated reliability\nestimation algorithms as a way to enhance the efficiency of the latter.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:33:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Moustapha", "M.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "2106.01719", "submitter": "Kermorvant Claire Dr", "authors": "Claire Kermorvant, Benoit Liquet, Guy Litt, Kerrie Mengersen, Erin\n  Peterson, Rob Hyndman, Jeremy B. Jones Jr., and Catherine Leigh", "title": "Understanding links between water-quality variables and nitrate\n  concentration in freshwater streams using high-frequency sensor data", "comments": "4 figures, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real time monitoring using in situ sensors is becoming a common approach for\nmeasuring water quality within watersheds. High frequency measurements produce\nbig data sets that present opportunities to conduct new analyses for improved\nunderstanding of water quality dynamics and more effective management of rivers\nand streams. Of primary importance is enhancing knowledge of the relationships\nbetween nitrate, one of the most reactive forms of inorganic nitrogen in the\naquatic environment, and other water quality variables. We analysed high\nfrequency water quality data from in situ sensors deployed in three sites from\ndifferent watersheds and climate zones within the National Ecological\nObservatory Network, USA. We used generalised additive mixed models to explain\nthe nonlinear relationships at each site between nitrate concentration and\nconductivity, turbidity, dissolved oxygen, water temperature, and elevation.\nTemporal auto correlation was modelled with an auto regressive moving average\nmodel and we examined the relative importance of the explanatory variables.\nTotal deviance explained by the models was high for all sites. Although\nvariable importance and the smooth regression parameters differed among sites,\nthe models explaining the most variation in nitrate contained the same\nexplanatory variables. This study demonstrates that building a model for\nnitrate using the same set of explanatory water quality variables is\nachievable, even for sites with vastly different environmental and climatic\ncharacteristics. Applying such models will assist managers to select cost\neffective water quality variables to monitor when the goals are to gain a\nspatially and temporally in depth understanding of nitrate dynamics and adapt\nmanagement plans accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:42:46 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kermorvant", "Claire", ""], ["Liquet", "Benoit", ""], ["Litt", "Guy", ""], ["Mengersen", "Kerrie", ""], ["Peterson", "Erin", ""], ["Hyndman", "Rob", ""], ["Jones", "Jeremy B.", "Jr."], ["Leigh", "Catherine", ""]]}, {"id": "2106.01806", "submitter": "Dorcas Ofori-Boateng", "authors": "Dorcas Ofori-Boateng, Ignacio Segovia Dominguez, Murat Kantarcioglu,\n  Cuneyt G. Akcora, Yulia R. Gel", "title": "Topological Anomaly Detection in Dynamic Multilayer Blockchain Networks", "comments": "26 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.AT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent surge of criminal activities with\ncross-cryptocurrency trades, we introduce a new topological perspective to\nstructural anomaly detection in dynamic multilayer networks. We postulate that\nanomalies in the underlying blockchain transaction graph that are composed of\nmultiple layers are likely to also be manifested in anomalous patterns of the\nnetwork shape properties. As such, we invoke the machinery of clique persistent\nhomology on graphs to systematically and efficiently track evolution of the\nnetwork shape and, as a result, to detect changes in the underlying network\ntopology and geometry. We develop a new persistence summary for multilayer\nnetworks, called stacked persistence diagram, and prove its stability under\ninput data perturbations. We validate our new topological anomaly detection\nframework in application to dynamic multilayer networks from the Ethereum\nBlockchain and the Ripple Credit Network, and demonstrate that our stacked PD\napproach substantially outperforms state-of-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:58:04 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 18:05:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ofori-Boateng", "Dorcas", ""], ["Dominguez", "Ignacio Segovia", ""], ["Kantarcioglu", "Murat", ""], ["Akcora", "Cuneyt G.", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2106.01814", "submitter": "Christopher Barrie", "authors": "Roberto Cerina, Christopher Barrie, Neil Ketchley, and Aaron Zelin", "title": "Explaining Recruitment to Extremism: A Bayesian Contaminated Case\n  Control Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Who joins extremist movements? Answering this question poses considerable\nmethodological challenges. Survey techniques are practically infeasible and\nselective samples provide no counterfactual. Assigning recruits to contextual\nunits provides one solution, but is vulnerable to problems of ecological\ninference. In this article, we take inspiration from epidemiology and the\nprotest literature and elaborate a technique to combine survey and ecological\napproaches. The rare events, multilevel Bayesian contaminated case-control\ndesign we propose accounts for individual-level and contextual factors, as well\nas spatial autocorrelation in the incidence of recruitment. We validate our\napproach by matching a sample of Islamic State (ISIS) fighters from nine\nMuslim-majority countries with representative population surveys enumerated\nshortly before recruits joined the movement. We find that high status\nindividuals in their early twenties who had university education were more\nlikely to join ISIS. We find more mixed evidence for relative deprivation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:07:59 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:40:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cerina", "Roberto", ""], ["Barrie", "Christopher", ""], ["Ketchley", "Neil", ""], ["Zelin", "Aaron", ""]]}, {"id": "2106.01821", "submitter": "Stephen Walker", "authors": "Stephen G Walker", "title": "A New Measure of Overlap: An Alternative to the p--value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we present a new measure for the overlap of two density\nfunctions which provides motivation and interpretation currently lacking with\nbenchmark measures based on the proportion of similar response, also known as\nthe overlap coefficient. We use this new measure to present an alternative to\nthe $p$--value as a guide to the choice of treatment in a comparative trial;\nwhere a current treatment and a new treatment are undergoing investigation. We\nshow that it is possible to reject the null hypothesis; i.e. the new treatment\nis significantly different in response to the old treatment, while the proposed\nnew summary for the same experiment indicates that as low as one in ten\nindividuals subject to the new treatment behave differently to individuals on\nthe old one.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:16:39 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 12:00:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Walker", "Stephen G", ""]]}, {"id": "2106.01867", "submitter": "Oscar Garc\\'ia", "authors": "Oscar Garc\\'ia", "title": "Plasticity as a link between spatially explicit, distance-independent,\n  and whole-stand forest growth models", "comments": "9 pages, 8 figures. Submitted manuscript. Version 2 with minor\n  changes: expanded caption in Fig. 2, new Fig. 6, corrected Wikipedia\n  reference, fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models at various levels of resolution are commonly used, both for forest\nmanagement and in ecological research. They all have comparative advantages and\ndisadvantages, making desirable a better understanding of the relationships\nbetween the various approaches. It is found that accounting for crown and root\nplasticity creates more realistic links between spatial and non-spatial models\nthan simply ignoring spatial structure. The article reviews also the connection\nbetween distance-independent models and size distributions, and how\ndistributions evolve over time and relate to whole-stand descriptions. In\naddition, some ways in which stand-level knowledge feeds back into detailed\nindividual-tree formulations are demonstrated. The presentation intends to be\naccessible to non-specialists.\n  Study implications: Introducing plasticity improves the representation of\nphysio-ecological processes in spatial modelling. Plasticity explains in part\nthe practical success of distance-independent models. The nature of size\ndistributions and their relationship to individual-tree and whole-stand models\nare discussed. I point out limitations of various approaches and questions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 23:59:03 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:19:32 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Garc\u00eda", "Oscar", ""]]}, {"id": "2106.01921", "submitter": "James Long", "authors": "James P. Long and Min Jin Ha", "title": "Sample Selection Bias in Evaluation of Prediction Performance of Causal\n  Models", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal models are notoriously difficult to validate because they make\nuntestable assumptions regarding confounding. New scientific experiments offer\nthe possibility of evaluating causal models using prediction performance.\nPrediction performance measures are typically robust to violations in causal\nassumptions. However prediction performance does depend on the selection of\ntraining and test sets. In particular biased training sets can lead to\noptimistic assessments of model performance. In this work, we revisit the\nprediction performance of several recently proposed causal models tested on a\ngenetic perturbation data set of Kemmeren [Kemmeren et al., 2014]. We find that\nsample selection bias is likely a key driver of model performance. We propose\nusing a less-biased evaluation set for assessing prediction performance on\nKemmeren and compare models on this new set. In this setting, the causal model\ntested have similar performance to standard association based estimators such\nas Lasso. Finally we compare the performance of causal estimators in simulation\nstudies which reproduce the Kemmeren structure of genetic knockout experiments\nbut without any sample selection bias. These results provide an improved\nunderstanding of the performance of several causal models and offer guidance on\nhow future studies should use Kemmeren.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:15:30 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Long", "James P.", ""], ["Ha", "Min Jin", ""]]}, {"id": "2106.02035", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, and Manuel Hernandez", "title": "Home range estimation under a restricted sampling scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of animal movement has gained attention recently, and new\ncontinuous-time models and statistical methods have been developed. All of them\nare based on the assumption that this movement can be recorded over a long\nperiod of time, which is sometimes infeasible, for instance when the battery\nlife of the GPS is short. We prove that the estimation of its home range\nimproves if periods when the GPS is on are alternated with periods when the GPS\nis turned off. This is illustrated through a simulation study, and real life\ndata. We also provide estimators of the stationary distribution, level sets\n(which provides estimators of the core area) and the drift function.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:57:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Hernandez", "Manuel", ""]]}, {"id": "2106.02044", "submitter": "Piyush Sharma", "authors": "Piyush K. Sharma", "title": "Heterogeneous Noisy Short Signal Camouflage in Multi-Domain Environment\n  Decision-Making", "comments": "Published at:\n  http://www.ibai-publishing.org/journal/issue_massdata/2020_september/massdata_11_1_3_26.php.\n  arXiv admin note: substantial text overlap with arXiv:2106.01497", "journal-ref": "Transactions on Mass-Data Analysis of Images and Signals\n  P-ISSN1868-6451, E-ISSN 2509-9353, ISBN 978-3-942952-80-4 Volume 11 - Number\n  1 - September 2020 - Page 3-26", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data transmission between two or more digital devices in industry and\ngovernment demands secure and agile technology. Digital information\ndistribution often requires deployment of Internet of Things (IoT) devices and\nData Fusion techniques which have also gained popularity in both, civilian and\nmilitary environments, such as, emergence of Smart Cities and Internet of\nBattlefield Things (IoBT). This usually requires capturing and consolidating\ndata from multiple sources. Because datasets do not necessarily originate from\nidentical sensors, fused data typically results in a complex Big Data problem.\nDue to potentially sensitive nature of IoT datasets, Blockchain technology is\nused to facilitate secure sharing of IoT datasets, which allows digital\ninformation to be distributed, but not copied. However, blockchain has several\nlimitations related to complexity, scalability, and excessive energy\nconsumption. We propose an approach to hide information (sensor signal) by\ntransforming it to an image or an audio signal. In one of the latest attempts\nto the military modernization, we investigate sensor fusion approach by\ninvestigating the challenges of enabling an intelligent identification and\ndetection operation and demonstrates the feasibility of the proposed Deep\nLearning and Anomaly Detection models that can support future application for\nspecific hand gesture alert system from wearable devices.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:59:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sharma", "Piyush K.", ""]]}, {"id": "2106.02270", "submitter": "Tayo Fabusuyi", "authors": "Daniel Jordon, Robert Hampshire, Tayo Fabusuyi", "title": "Estimating parking occupancy using smart meter transaction data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The excessive search for parking, known as cruising, generates pollution and\ncongestion. Cities are looking for approaches that will reduce the negative\nimpact associated with searching for parking. However, adequately measuring the\nnumber of vehicles in search of parking is difficult and requires sensing\ntechnologies. In this paper, we develop an approach that eliminates the need\nfor sensing technology by using parking meter payment transactions to estimate\nparking occupancy and the number of cars searching for parking. The estimation\nscheme is based on Particle Markov Chain Monte Carlo. We validate the\nperformance of the Particle Markov Chain Monte Carlo approach using data\nsimulated from a GI/GI/s queue. We show that the approach generates\nasymptotically unbiased Bayesian estimates of the parking occupancy and\nunderlying model parameters such as arrival rates, average parking time, and\nthe payment compliance rate. Finally, we estimate parking occupancy and\ncruising using parking meter data from SFpark, a large scale parking experiment\nand subsequently, compare the Particle Markov Chain Monte Carlo parking\noccupancy estimates against the ground truth data from the parking sensors. Our\napproach is easily replicated and scalable given that it only requires using\ndata that cities already possess, namely historical parking payment\ntransactions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 05:36:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jordon", "Daniel", ""], ["Hampshire", "Robert", ""], ["Fabusuyi", "Tayo", ""]]}, {"id": "2106.02329", "submitter": "Xiuqin Xu", "authors": "Xiuqin Xu, Ying Chen", "title": "Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series\n  Forecasting with Regime Switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep switching state space model (DS$^3$M) for efficient\ninference and forecasting of nonlinear time series with irregularly switching\namong various regimes. The switching among regimes is captured by both discrete\nand continuous latent variables with recurrent neural networks. The model is\nestimated with variational inference using a reparameterization trick. We test\nthe approach on a variety of simulated and real datasets. In all cases, DS$^3$M\nachieves competitive performance compared to several state-of-the-art methods\n(e.g. GRU, SRNN, DSARF, SNLDS), with superior forecasting accuracy, convincing\ninterpretability of the discrete latent variables, and powerful representation\nof the continuous latent variables for different kinds of time series.\nSpecifically, the MAPE values increase by 0.09\\% to 15.71\\% against the\nsecond-best performing alternative models.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:25:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Xu", "Xiuqin", ""], ["Chen", "Ying", ""]]}, {"id": "2106.02521", "submitter": "Marc Chadeau-Hyam", "authors": "Barbara Bodinier, Sarah Filippi, Therese Haugdahl Nost, Julien Chiquet\n  and Marc Chadeau-Hyam", "title": "Automated calibration for stability selection in penalised regression\n  and graphical models: a multi-OMICs network application exploring the\n  molecular response to tobacco smoking", "comments": "Main paper 21 pages, SI: 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stability selection represents an attractive approach to identify sparse sets\nof features jointly associated with an outcome in high-dimensional contexts. We\nintroduce an automated calibration procedure via maximisation of an in-house\nstability score and accommodating a priori-known block structure (e.g.\nmulti-OMIC) data. It applies to (LASSO) penalised regression and graphical\nmodels. Simulations show our approach outperforms non-stability-based and\nstability selection approaches using the original calibration. Application to\nmulti-block graphical LASSO on real (epigenetic and transcriptomic) data from\nthe Norwegian Women and Cancer study reveals a central/credible and novel\ncross-OMIC role of the LRRN3 in the biological response to smoking.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:44:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bodinier", "Barbara", ""], ["Filippi", "Sarah", ""], ["Nost", "Therese Haugdahl", ""], ["Chiquet", "Julien", ""], ["Chadeau-Hyam", "Marc", ""]]}, {"id": "2106.02600", "submitter": "Song Wei", "authors": "Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran", "title": "Inferring Granger Causality from Irregularly Sampled Time Series", "comments": "33 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous, automated surveillance systems that incorporate machine learning\nmodels are becoming increasingly more common in healthcare environments. These\nmodels can capture temporally dependent changes across multiple patient\nvariables and can enhance a clinician's situational awareness by providing an\nearly warning alarm of an impending adverse event such as sepsis. However, most\ncommonly used methods, e.g., XGBoost, fail to provide an interpretable\nmechanism for understanding why a model produced a sepsis alarm at a given\ntime. The black-box nature of many models is a severe limitation as it prevents\nclinicians from independently corroborating those physiologic features that\nhave contributed to the sepsis alarm. To overcome this limitation, we propose a\ngeneralized linear model (GLM) approach to fit a Granger causal graph based on\nthe physiology of several major sepsis-associated derangements (SADs). We adopt\na recently developed stochastic monotone variational inequality-based estimator\ncoupled with forwarding feature selection to learn the graph structure from\nboth continuous and discrete-valued as well as regularly and irregularly\nsampled time series. Most importantly, we develop a non-asymptotic upper bound\non the estimation error for any monotone link function in the GLM. We conduct\nreal-data experiments and demonstrate that our proposed method can achieve\ncomparable performance to popular and powerful prediction methods such as\nXGBoost while simultaneously maintaining a high level of interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:59:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wei", "Song", ""], ["Xie", "Yao", ""], ["Josef", "Christopher S.", ""], ["Kamaleswaran", "Rishikesan", ""]]}, {"id": "2106.02673", "submitter": "Mengli Xiao", "authors": "Mengli Xiao, Haitao Chu, Stephen Cole, Yong Chen, Richard MacLehose,\n  David Richardson, Sander Greenland", "title": "Odds Ratios are far from \"portable\": A call to use realistic models for\n  effect variation in meta-analysis", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: Recently Doi et al. argued that risk ratios should be replaced\nwith odds ratios in clinical research. We disagreed, and empirically documented\nthe lack of portability of odds ratios, while Doi et al. defended their\nposition. In this response we highlight important errors in their position.\n  Study Design and Setting: We counter Doi et al.'s arguments by further\nexamining the correlations of odds ratios, and risk ratios, with baseline risks\nin 20,198 meta-analyses from the Cochrane Database of Systematic Reviews.\n  Results: Doi et al.'s claim that odds ratios are portable is invalid because\n1) their reasoning is circular: they assume a model under which the odds ratio\nis constant and show that under such a model the odds ratio is portable; 2) the\nmethod they advocate to convert odds ratios to risk ratios is biased; 3) their\nempirical example is readily-refuted by counter-examples of meta-analyses in\nwhich the risk ratio is portable but the odds ratio isn't; and 4) they fail to\nconsider the causal determinants of meta-analytic inclusion criteria: Doi et\nal. mistakenly claim that variation in odds ratios with different baseline\nrisks in meta-analyses is due to collider bias. Empirical comparison between\nthe correlations of odds ratios, and risk ratios, with baseline risks show that\nthe portability of odds ratios and risk ratios varies across settings.\n  Conclusion: The suggestion to replace risk ratios with odds ratios is based\non circular reasoning and a confusion of mathematical and empirical results. It\nis especially misleading for meta-analyses and clinical guidance. Neither the\nodds ratio nor the risk ratio is universally portable. To address this lack of\nportability, we reinforce our suggestion to report variation in effect measures\nconditioning on modifying factors such as baseline risk; understanding such\nvariation is essential to patient-centered practice.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:09:02 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:10:53 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xiao", "Mengli", ""], ["Chu", "Haitao", ""], ["Cole", "Stephen", ""], ["Chen", "Yong", ""], ["MacLehose", "Richard", ""], ["Richardson", "David", ""], ["Greenland", "Sander", ""]]}, {"id": "2106.02726", "submitter": "Mason A. Porter", "authors": "Elisa C. Baek, Ryan Hyon, Karina L\\'opez, Emily S. Finn, Mason A.\n  Porter, and Carolyn Parkinson", "title": "Popularity is linked to neural coordination: Neural evidence for an Anna\n  Karenina principle in social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People differ in how they attend to, interpret, and respond to their\nsurroundings. Convergent processing of the world may be one factor that\ncontributes to social connections between individuals. We used neuroimaging and\nnetwork analysis to investigate whether the most central individuals in their\ncommunities (as measured by in-degree centrality, a notion of popularity)\nprocess the world in a particularly normative way. More central individuals had\nexceptionally similar neural responses to their peers and especially to each\nother in brain regions associated with high-level interpretations and social\ncognition (e.g., in the default-mode network), whereas less-central individuals\nexhibited more idiosyncratic responses. Self-reported enjoyment of and interest\nin stimuli followed a similar pattern, but accounting for these data did not\nchange our main results. These findings suggest an \"Anna Karenina principle\" in\nsocial networks: Highly-central individuals process the world in exceptionally\nsimilar ways, whereas less-central individuals process the world in\nidiosyncratic ways.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:24:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Baek", "Elisa C.", ""], ["Hyon", "Ryan", ""], ["L\u00f3pez", "Karina", ""], ["Finn", "Emily S.", ""], ["Porter", "Mason A.", ""], ["Parkinson", "Carolyn", ""]]}, {"id": "2106.03023", "submitter": "Ioannis Papageorgiou", "authors": "Ioannis Papageorgiou, Ioannis Kontoyiannis", "title": "Hierarchical Bayesian Mixture Models for Time Series Using Context Trees\n  as State Space Partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general Bayesian framework is introduced for mixture modelling and\ninference with real-valued time series. At the top level, the state space is\npartitioned via the choice of a discrete context tree, so that the resulting\npartition depends on the values of some of the most recent samples. At the\nbottom level, a different model is associated with each region of the\npartition. This defines a very rich and flexible class of mixture models, for\nwhich we provide algorithms that allow for efficient, exact Bayesian inference.\nIn particular, we show that the maximum a posteriori probability (MAP) model\n(including the relevant MAP context tree partition) can be precisely\nidentified, along with its exact posterior probability. The utility of this\ngeneral framework is illustrated in detail when a different autoregressive (AR)\nmodel is used in each state-space region, resulting in a mixture-of-AR model\nclass. The performance of the associated algorithmic tools is demonstrated in\nthe problems of model selection and forecasting on both simulated and\nreal-world data, where they are found to provide results as good or better than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:46:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Papageorgiou", "Ioannis", ""], ["Kontoyiannis", "Ioannis", ""]]}, {"id": "2106.03032", "submitter": "Jongsu Kim", "authors": "Jongsu Kim and Changhoon Lee", "title": "Deep Particulate Matter Forecasting Model Using Correntropy-Induced Loss", "comments": "Submitted to Journal of Mechanical Science and Technology (In review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Forecasting the particulate matter (PM) concentration in South Korea has\nbecome urgently necessary owing to its strong negative impact on human life. In\nmost statistical or machine learning methods, independent and identically\ndistributed data, for example, a Gaussian distribution, are assumed; however,\ntime series such as air pollution and weather data do not meet this assumption.\nIn this study, the maximum correntropy criterion for regression (MCCR) loss is\nused in an analysis of the statistical characteristics of air pollution and\nweather data. Rigorous seasonality adjustment of the air pollution and weather\ndata was performed because of their complex seasonality patterns and the\nheavy-tailed distribution of data even after deseasonalization. The MCCR loss\nwas applied to multiple models including conventional statistical models and\nstate-of-the-art machine learning models. The results show that the MCCR loss\nis more appropriate than the conventional mean squared error loss for\nforecasting extreme values.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 05:17:24 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kim", "Jongsu", ""], ["Lee", "Changhoon", ""]]}, {"id": "2106.03130", "submitter": "Srikanta Sannigrahi", "authors": "Srikanta Sannigrahi, Arabinda Maiti, Francesco Pilla, Qi Zhang,\n  Somnath Bar, Saskia Keesstra, Artemi Cerda", "title": "Connection between forest fire emission and COVID-19 incidents in West\n  Coast regions of the United States", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forest fires impact on soil, water and biota resources has been widely\nresearched. Although forest fires profoundly impact the atmosphere and air\nquality across the ecosystems, much less research has been developed to examine\nits impact on the current pandemic. In-situ air pollution data were utilized to\nexamine the effects of the 2020 forest fire on atmosphere and coronavirus\n(COVID 19) casualties. The spatiotemporal concentrations of particulate matter\n(PM2.5 and PM10) and Nitrogen Dioxide (NO2) were collected from August 1 to\nOctober 30 for 2020 (fire year) and 2019 (reference year). Both spatial\n(Multiscale Geographically Weighted Regression) and non spatial (negative\nbinomial regression) regression analysis was performed to assess the adverse\neffects of fire emission on human health. The in situ data led measurements\nshowed that the maximum increases in PM2.5, PM10, and NO2 concentrations were\nclustered in the West Coastal fire-prone states during the August 1 to October\n30 period. The average concentration of particulate matter (PM2.5 and PM10) and\nNO2 were increased in all the fire states affected badly by forest fires. The\naverage PM2.5 concentration over the period was recorded as 7.9, 6.3, 5.5, and\n5.2 for California, Colorado, Oregon, and Washington in 2019, which was\nincreased up to 24.9, 13.4, 25, and 17 in 2020. Both spatial and non-spatial\nregression models exhibited a statistically significant association between\nfire emission and COVID 19 incidents. A total of 30 models were developed for\nanalyzing the spatial non-stationary and local association between the\npredictor and response factors. All these spatial models have demonstrated a\nstatistically significant association between fire emissions and COVID counts.\nMore thorough research is needed to better understand the complex association\nbetween forest fire and human health.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:08:48 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sannigrahi", "Srikanta", ""], ["Maiti", "Arabinda", ""], ["Pilla", "Francesco", ""], ["Zhang", "Qi", ""], ["Bar", "Somnath", ""], ["Keesstra", "Saskia", ""], ["Cerda", "Artemi", ""]]}, {"id": "2106.03263", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz and Herman Cherniaiev and Robert Pater", "title": "Estimating the number of entities with vacancies using administrative\n  and online data", "comments": "70 pages, 4 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we describe a study aimed at estimating job vacancy\nstatistics, in particular the number of entities with at least one vacancy. To\nachieve this goal, we propose an alternative approach to the methodology\nexploiting survey data, which is based solely on data from administrative\nregisters and online sources and relies on dual system estimation (DSE).\n  As these sources do not cover the whole reference population and the number\nof units appearing in all datasets is small, we have developed a DSE approach\nfor negatively dependent sources based on a recent work by Chatterjee and\nBhuyan (2020). To achieve the main goal we conducted a thorough data cleaning\nprocedure in order to remove out-of-scope units, identify entities from the\ntarget population, and link them by identifiers to minimize linkage errors. We\nverified the effectiveness and sensitivity of the proposed estimator in\nsimulation studies.\n  From a practical point of view, our results show that the current vacancy\nsurvey in Poland underestimates the number of entities with at least one\nvacancy by about 10-15%. The main reasons for this discrepancy are non-sampling\nerrors due to non-response and under-reporting, which is identified by\ncomparing survey data with administrative data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 22:19:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Cherniaiev", "Herman", ""], ["Pater", "Robert", ""]]}, {"id": "2106.03322", "submitter": "Edwin Ng", "authors": "Edwin Ng, Zhishi Wang, Athena Dai", "title": "Bayesian Time Varying Coefficient Model with Applications to Marketing\n  Mix Modeling", "comments": "7. figures 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both Bayesian and varying coefficient models are very useful tools in\npractice as they can be used to model parameter heterogeneity in a\ngeneralizable way. Motivated by the need of enhancing Marketing Mix Modeling at\nUber, we propose a Bayesian Time Varying Coefficient model, equipped with a\nhierarchical Bayesian structure. This model is different from other time\nvarying coefficient models in the sense that the coefficients are weighted over\na set of local latent variables following certain probabilistic distributions.\nStochastic Variational Inference is used to approximate the posteriors of\nlatent variables and dynamic coefficients. The proposed model also helps\naddress many challenges faced by traditional MMM approaches. We used\nsimulations as well as real world marketing datasets to demonstrate our model\nsuperior performance in terms of both accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:38:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:32:28 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ng", "Edwin", ""], ["Wang", "Zhishi", ""], ["Dai", "Athena", ""]]}, {"id": "2106.03691", "submitter": "Matteo Iacopini", "authors": "Michele Costola and Matteo Iacopini and Carlo R.M.A. Santagiustina", "title": "On the \"mementum\" of Meme Stocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.GN stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The meme stock phenomenon is yet to be explored. In this note, we provide\nevidence that these stocks display common stylized facts on the dynamics of\nprice, trading volume, and social media activity. Using a regime-switching\ncointegration model, we identify the meme stock \"mementum\" which exhibits a\ndifferent characterization with respect to other stocks with high volumes of\nactivity (persistent and not) on social media. Understanding these properties\nhelps the investors and market authorities in their decision.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:04:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Costola", "Michele", ""], ["Iacopini", "Matteo", ""], ["Santagiustina", "Carlo R. M. A.", ""]]}, {"id": "2106.03697", "submitter": "Jianhui Gao", "authors": "Jianhui Gao, Aliza Panjwani, Madeline Li, Osvaldo Espin-Garcia", "title": "Latent class growth analysis for ordinal response data in the Distress\n  Assessment and Response Tool: an evaluation of state-of-the-art\n  implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical psychology, longitudinal studies are often conducted to\nunderstand developmental trends over a period of time. Latent class growth\nanalysis is a standard and popular statistical approach to identify underlying\nsubpopulations with heterogenous trends. Several implementations, such as LCGA\n(Mplus), proc traj (SAS) and package lcmm (R) are specially designed to perform\nlatent growth analysis. Motivated by data collection of psychological\ninstruments over time in the largest cancer centre in Canada, we compare these\nimplementations using various simulated Edmonton Symptom Assessment System\nrevised (ESAS-r) scores, an ordinal outcome from 0 to 10, in a large data-set\nconsisting of more than 20,000 patients collected through the Distress\nAssessment and Response Tool (DART) from the Princess Margaret Cancer Center.\nWe have found that both Mplus and lcmm lead to high correct classification\nrate, but Proc Traj tends to over estimate the number of classes and fails to\nconverge. While Mplus is computationally more affordable than lcmm, it has a\nlimit of maximum 10 levels for ordinal data. We therefore suggest first\nanalyzing data on the original scale using lcmm. If computational time becomes\nan issue, then one can group the scores into categories and implement them in\nMplus.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:08:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gao", "Jianhui", ""], ["Panjwani", "Aliza", ""], ["Li", "Madeline", ""], ["Espin-Garcia", "Osvaldo", ""]]}, {"id": "2106.03742", "submitter": "Jeffrey N\\\"af", "authors": "Loris Michel, Jeffrey N\\\"af, Meta-Lina Spohn, Nicolai Meinshausen", "title": "Proper Scoring Rules for Missing Value Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the prevalence of missing data in modern statistical research, a broad\nrange of methods is available for any given imputation task. How does one\nchoose the `best' method in a given application? The standard approach is to\nselect some observations, set their status to missing, and compare prediction\naccuracy of the methods under consideration for these observations. Besides\nhaving to somewhat artificially mask additional observations, a shortcoming of\nthis approach is that the optimal imputation in this scheme chooses the\nconditional mean if predictive accuracy is measured with RMSE. In contrast, we\nwould like to rank highest methods that can sample from the true conditional\ndistribution. In this paper, we develop a principled and easy-to-use evaluation\nmethod for missing value imputation under the missing completely at random\n(MCAR) assumption. The approach is applicable for discrete and continuous data\nand works on incomplete data sets, without having to leave out additional\nobservations for evaluation. Moreover, it favors imputation methods that\nreproduce the original data distribution. We show empirically on a range of\ndata sets and imputation methods that our score consistently ranks true data\nhigh(est) and is able to avoid pitfalls usually associated with performance\nmeasures such as RMSE. Finally, we provide an R-package with an implementation\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:07:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""], ["Spohn", "Meta-Lina", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "2106.03755", "submitter": "Hankui Peng", "authors": "Hankui Peng, Angelica I. Aviles-Rivero, Carola-Bibiane Schonlieb", "title": "HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels serve as a powerful preprocessing tool in many computer vision\ntasks. By using superpixel representation, the number of image primitives can\nbe largely reduced by orders of magnitudes. The majority of superpixel methods\nuse handcrafted features, which usually do not translate well into strong\nadherence to object boundaries. A few recent superpixel methods have introduced\ndeep learning into the superpixel segmentation process. However, none of these\nmethods is able to produce superpixels in near real-time, which is crucial to\nthe applicability of a superpixel method in practice. In this work, we propose\na two-stage graph-based framework for superpixel segmentation. In the first\nstage, we introduce an efficient Deep Affinity Learning (DAL) network that\nlearns pairwise pixel affinities by aggregating multi-scale information. In the\nsecond stage, we propose a highly efficient superpixel method called\nHierarchical Entropy Rate Segmentation (HERS). Using the learned affinities\nfrom the first stage, HERS builds a hierarchical tree structure that can\nproduce any number of highly adaptive superpixels instantaneously. We\ndemonstrate, through visual and numerical experiments, the effectiveness and\nefficiency of our method compared to various state-of-the-art superpixel\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:20:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Peng", "Hankui", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Schonlieb", "Carola-Bibiane", ""]]}, {"id": "2106.03979", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Vijay R. Varma, Dmitri Volfson, Jacek Urbanek, Jeffrey\n  M. Hausdorff, Amber Watts and Vadim Zipunnikov", "title": "Scalar on time-by-distribution regression and its application for\n  modelling associations between daily-living physical activity and cognitive\n  functions in Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable data is a rich source of information that can provide deeper\nunderstanding of links between human behaviours and human health. Existing\nmodelling approaches use wearable data summarized at subject level via scalar\nsummaries using regression techniques, temporal (time-of-day) curves using\nfunctional data analysis (FDA), and distributions using distributional data\nanalysis (DDA). We propose to capture temporally local distributional\ninformation in wearable data using subject-specific time-by-distribution (TD)\ndata objects. Specifically, we propose scalar on time-by-distribution\nregression (SOTDR) to model associations between scalar response of interest\nsuch as health outcomes or disease status and TD predictors. We show that TD\ndata objects can be parsimoniously represented via a collection of time-varying\nL-moments that capture distributional changes over the time-of-day. The\nproposed method is applied to the accelerometry study of mild Alzheimer's\ndisease (AD). Mild AD is found to be significantly associated with reduced\nmaximal level of physical activity, particularly during morning hours. It is\nalso demonstrated that TD predictors attain much stronger associations with\nclinical cognitive scales of attention, verbal memory, and executive function\nwhen compared to predictors summarized via scalar total activity counts,\ntemporal functional curves, and quantile functions. Taken together, the present\nresults suggest that the SOTDR analysis provides novel insights into cognitive\nfunction and AD.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 21:42:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ghosal", "Rahul", ""], ["Varma", "Vijay R.", ""], ["Volfson", "Dmitri", ""], ["Urbanek", "Jacek", ""], ["Hausdorff", "Jeffrey M.", ""], ["Watts", "Amber", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "2106.04118", "submitter": "Matteo Sesia", "authors": "Shuangning Li, Matteo Sesia, Yaniv Romano, Emmanuel Cand\\`es, Chiara\n  Sabatti", "title": "Searching for consistent associations with a multi-environment knockoff\n  filter", "comments": "41 pages, 21 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a method based on model-X knockoffs to find conditional\nassociations that are consistent across diverse environments, controlling the\nfalse discovery rate. The motivation for this problem is that large data sets\nmay contain numerous associations that are statistically significant and yet\nmisleading, as they are induced by confounders or sampling imperfections.\nHowever, associations consistently replicated under different conditions may be\nmore interesting. In fact, consistency sometimes provably leads to valid causal\ninferences even if conditional associations do not. While the proposed method\nis flexible and can be deployed in a wide range of applications, this paper\nhighlights its relevance to genome-wide association studies, in which\nconsistency across populations with diverse ancestries mitigates confounding\ndue to unmeasured variants. The effectiveness of this approach is demonstrated\nby simulations and applications to the UK Biobank data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:04:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Shuangning", ""], ["Sesia", "Matteo", ""], ["Romano", "Yaniv", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "2106.04271", "submitter": "Tyler McCormick", "authors": "Mengjie Pan, Tyler H. McCormick, Bailey K. Fosdick", "title": "Inference for Network Regression Models with Community Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network regression models, where the outcome comprises the valued edge in a\nnetwork and the predictors are actor or dyad-level covariates, are used\nextensively in the social and biological sciences. Valid inference relies on\naccurately modeling the residual dependencies among the relations. Frequently\nhomogeneity assumptions are placed on the errors which are commonly incorrect\nand ignore critical, natural clustering of the actors. In this work, we present\na novel regression modeling framework that models the errors as resulting from\na community-based dependence structure and exploits the subsequent\nexchangeability properties of the error distribution to obtain parsimonious\nstandard errors for regression parameters.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:04:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pan", "Mengjie", ""], ["McCormick", "Tyler H.", ""], ["Fosdick", "Bailey K.", ""]]}, {"id": "2106.04285", "submitter": "Linda Nab", "authors": "Linda Nab, Rolf H.H. Groenwold", "title": "Sensitivity analysis for random measurement error using regression\n  calibration and simulation-extrapolation", "comments": "20 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sensitivity analysis for measurement error can be applied in the absence of\nvalidation data by means of regression calibration and\nsimulation-extrapolation. These have not been compared for this purpose. A\nsimulation study was conducted comparing the performance of regression\ncalibration and simulation-extrapolation in a multivariable model. The\nperformance of the two methods was evaluated in terms of bias, mean squared\nerror (MSE) and confidence interval coverage, for ranging reliability of the\nerror-prone measurement (0.2-0.9), sample size (125-1,000), number of\nreplicates (2-10), and R-squared (0.03-0.75). It was assumed that no validation\ndata were available about the error-free measures, while measurement error\nvariance was correctly estimated. In various scenarios, regression calibration\nwas unbiased while simulation-extrapolation was biased: median bias was 1.4%\n(interquartile range (IQR): 0.8;2%), and -12.8% (IQR: -13.2;-11.0%),\nrespectively. A small gain in efficiency was observed for\nsimulation-extrapolation (median MSE: 0.005, IQR: 0.004;0.006) versus\nregression calibration (median MSE: 0.006, IQR: 0.004;0.007). Confidence\ninterval coverage was at the nominal level of 95% for regression calibration,\nand smaller than 95% for simulation-extrapolation (median coverage: 92%, IQR:\n85;94%). In the absence of validation data, the use of regression calibration\nis recommended for sensitivity analysis for measurement error.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:23:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Nab", "Linda", ""], ["Groenwold", "Rolf H. H.", ""]]}, {"id": "2106.04461", "submitter": "Kasey Jones", "authors": "Kasey Jones, Emily Hadley, Sandy Preiss, Caroline Kery, Peter\n  Baumgartner, Marie Stoner, Sarah Rhea", "title": "North Carolina COVID-19 Agent-Based Model Framework for Hospitalization\n  Forecasting Overview, Design Concepts, and Details Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This Overview, Design Concepts, and Details Protocol (ODD) provides a\ndetailed description of an agent-based model (ABM) that was developed to\nsimulate hospitalizations during the COVID-19 pandemic. Using the descriptions\nof submodels, provided parameters, and the links to data sources, modelers will\nbe able to replicate the creation and results of this model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:43:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Jones", "Kasey", ""], ["Hadley", "Emily", ""], ["Preiss", "Sandy", ""], ["Kery", "Caroline", ""], ["Baumgartner", "Peter", ""], ["Stoner", "Marie", ""], ["Rhea", "Sarah", ""]]}, {"id": "2106.04510", "submitter": "R\\'emy Ben Messaoud", "authors": "Remy Ben Messaoud and Mario Chavez", "title": "Random Forest classifier for EEG-based seizure prediction", "comments": "all the python code used for this work can be found in this\n  repository: https://github.com/rbm1996/seizure_prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epileptic seizure prediction has gained considerable interest in the\ncomputational Epilepsy research community. This paper presents a Machine\nLearning based method for epileptic seizure prediction which outperforms\nstate-of-the art methods. We compute a probability for a given epoch, of being\npre-ictal against interictal using the Random Forest classifier and introduce\nnew concepts to enhance the robustness of the algorithm to false alarms. We\nassessed our method on 20 patients of the benchmark scalp EEG CHB-MIT dataset\nfor a seizure prediction horizon (SPH) of 5 minutes and a seizure occurrence\nperiod (SOP) of 30 minutes. Our approach achieves a sensitivity of 82.07 % and\na low false positive rate (FPR) of 0.0799 /h. We also tested our approach on\nintracranial EEG recordings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:46:35 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Messaoud", "Remy Ben", ""], ["Chavez", "Mario", ""]]}, {"id": "2106.04721", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan, Chanh Kieu, Dimitrios Sakellariou, Mahashweta\n  Patra", "title": "Hitting Time of Rapid Intensification Onset in Hurricane-like Vortices", "comments": "33 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting tropical cyclone (TC) rapid intensification (RI) is an important\nyet challenging task in current operational forecast due to our incomplete\nunderstanding of TC nonlinear processes. This study examines the variability of\nRI onset, including the probability of RI occurrence and the timing of RI\nonset, using a low-order stochastic model for TC development. Defining RI onset\ntime as the first hitting time in the model for a given subset in the TC-scale\nstate space, we quantify the probability of the occurrence of RI onset and the\ndistribution of the timing of RI onset for a range of initial conditions and\nmodel parameters. Based on asymptotic analysis for stochastic differential\nequations, our results show that RI onset occurs later, along with a larger\nvariance of RI onset timing, for weaker vortex initial condition and stronger\nnoise amplitude. In the small noise limit, RI onset probability approaches one\nand the RI onset timing has less uncertainty (i.e., a smaller variance),\nconsistent with observation of TC development under idealized environment. Our\ntheoretical results are verified against Monte-Carlo simulations and compared\nwith explicit results for a general 1-dimensional system, thus providing new\ninsights into the variability of RI onset and helping better quantify the\nuncertainties of RI variability for practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 22:31:14 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 11:45:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fan", "Wai-Tong Louis", ""], ["Kieu", "Chanh", ""], ["Sakellariou", "Dimitrios", ""], ["Patra", "Mahashweta", ""]]}, {"id": "2106.04809", "submitter": "Ranjan Maitra", "authors": "Geoffrey Z. Thompson and Bishoy Dawood and Tianyu Yu and Barbara K.\n  Lograsso and John D. Vanderkolk and Ranjan Maitra and William Q. Meeker and\n  Ashraf F. Bastawros", "title": "Fracture Mechanics-Based Quantitative Matching of Forensic Evidence\n  Fragments", "comments": "18 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.app-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractured metal fragments with rough and irregular surfaces are often found\nat crime scenes. Current forensic practice visually inspects the complex jagged\ntrajectory of fractured surfaces to recognize a ``match'' using comparative\nmicroscopy and physical pattern analysis. We developed a novel computational\nframework, utilizing the basic concepts of fracture mechanics and statistical\nanalysis to provide quantitative match analysis for match probability and error\nrates. The framework employs the statistics of fracture surfaces to become\nnon-self-affine with unique roughness characteristics at relevant microscopic\nlength scale, dictated by the intrinsic material resistance to fracture and its\nmicrostructure. At such a scale, which was found to be greater than two\ngrain-size or micro-feature-size, we establish that the material intrinsic\nproperties, microstructure, and exposure history to external forces on an\nevidence fragment have the premise of uniqueness, which quantitatively\ndescribes the microscopic features on the fracture surface for forensic\ncomparisons. The methodology utilizes 3D spectral analysis of overlapping\ntopological images of the fracture surface and classifies specimens with very\nhigh accuracy using statistical learning. Cross correlations of image-pairs in\ntwo frequency ranges are used to develop matrix variate statistical models for\nthe distributions among matching and non-matching pairs of images, and provides\na decision rule for identifying matches and determining error rates. A set of\nthirty eight different fracture surfaces of steel articles were correctly\nclassified. The framework lays the foundations for forensic applications with\nquantitative statistical comparison across a broad range of fractured materials\nwith diverse textures and mechanical properties.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:04:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Thompson", "Geoffrey Z.", ""], ["Dawood", "Bishoy", ""], ["Yu", "Tianyu", ""], ["Lograsso", "Barbara K.", ""], ["Vanderkolk", "John D.", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Bastawros", "Ashraf F.", ""]]}, {"id": "2106.05027", "submitter": "Keisuke Okamura", "authors": "Keisuke Okamura", "title": "Scientometric engineering: Revealing spatiotemporal citation dynamics\n  via open eprints", "comments": "1+25 pages, 6 figures for main text; 1+24 pages, 14 figures for\n  supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing speed and volume of knowledge production and\nconsumption, scholarly communication systems have been rapidly transformed into\ndigitised and networked open ecosystems, where preprint servers have played a\npivotal role. However, evidence is scarce regarding how this paradigm shift has\naffected the dynamics of collective attention on scientific knowledge. Herein,\nwe address this issue by investigating the citation dynamics of more than 1.5\nmillion eprints on arXiv, the most prominent and oldest eprint archive. The\ndiscipline-average citation history curves are estimated by applying a\nnonlinear regression model to the long-term citation data. The revealed\nspatiotemporal characteristics, including the growth and obsolescence patterns,\nare shown to vary across disciplines, reflecting the different publication and\ncitation practices. The results are used to develop a spatiotemporally\nnormalised citation index, called the $\\gamma$-index, with an approximately\nnormal distribution. It can be used to compare the citational impact of\nindividual papers across disciplines and time periods, providing a less biased\nmeasure of research impact than those widely used in the literature and in\npractice. Further, a stochastic model for the observed spatiotemporal citation\ndynamics is derived, reproducing both the Lognormal Law for the cumulative\ncitation distribution and the time trajectory of average citations in a unified\nformalism.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:38:44 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Okamura", "Keisuke", ""]]}, {"id": "2106.05067", "submitter": "Marco Mingione", "authors": "Marco Mingione and Pierfrancesco Alaimo Di Loro and Alessio Farcomeni\n  and Fabio Divino and Gianfranco Lovison and Giovanna Jona Lasinio and\n  Antonello Maruotti", "title": "Spatial modelling of COVID-19 incident cases using Richards' curve: an\n  application to the Italian regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce an extended generalised logistic growth model for discrete\noutcomes, in which a network structure can be specified to deal with spatial\ndependence and time dependence is dealt with using an Auto-Regressive approach.\nA major challenge concerns the specification of the network structure, crucial\nto consistently estimate the canonical parameters of the generalised logistic\ncurve, e.g. peak time and height. Parameters are estimated under the Bayesian\nframework, using the {\\texttt{ Stan}} probabilistic programming language. The\nproposed approach is motivated by the analysis of the first and second wave of\nCOVID-19 in Italy, i.e. from February 2020 to July 2020 and from July 2020 to\nDecember 2020, respectively. We analyse data at the regional level and,\ninterestingly enough, prove that substantial spatial and temporal dependence\noccurred in both waves, although strong restrictive measures were implemented\nduring the first wave. Accurate predictions are obtained, improving those of\nthe model where independence across regions is assumed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:38:24 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mingione", "Marco", ""], ["Di Loro", "Pierfrancesco Alaimo", ""], ["Farcomeni", "Alessio", ""], ["Divino", "Fabio", ""], ["Lovison", "Gianfranco", ""], ["Lasinio", "Giovanna Jona", ""], ["Maruotti", "Antonello", ""]]}, {"id": "2106.05092", "submitter": "David Degras", "authors": "David Degras, Chee-Ming Ting, Hernando Ombao", "title": "Markov-Switching State-Space Models with Applications to Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-space models (SSM) with Markov switching offer a powerful framework for\ndetecting multiple regimes in time series, analyzing mutual dependence and\ndynamics within regimes, and asserting transitions between regimes. These\nmodels however present considerable computational challenges due to the\nexponential number of possible regime sequences to account for. In addition,\nhigh dimensionality of time series can hinder likelihood-based inference. This\npaper proposes novel statistical methods for Markov-switching SSMs using\nmaximum likelihood estimation, Expectation-Maximization (EM), and parametric\nbootstrap. We develop solutions for initializing the EM algorithm, accelerating\nconvergence, and conducting inference that are ideally suited to massive\nspatio-temporal data such as brain signals. We evaluate these methods in\nsimulations and present applications to EEG studies of epilepsy and of motor\nimagery. All proposed methods are implemented in a MATLAB toolbox available at\nhttps://github.com/ddegras/switch-ssm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:12:27 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Degras", "David", ""], ["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""]]}, {"id": "2106.05174", "submitter": "Lorenz Gilch", "authors": "Lorenz A. Gilch", "title": "UEFA EURO 2020 Forecast via Nested Zero-Inflated Generalized Poisson\n  Regression", "comments": "13 pages, 11 tables. arXiv admin note: substantial text overlap with\n  arXiv:1905.03628", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is devoted to the forecast of the UEFA EURO 2020, Europe's\ncontinental football championship, taking place across Europe in June/July\n2021. We present the simulation results for this tournament, where the\nsimulations are based on a zero-inflated generalized Poisson regression model\nthat includes the Elo points of the participating teams and the location of the\nmatches as covariates and incorporates differences of team-specific skills. The\nproposed model allows predictions in terms of probabilities in order to\nquantify the chances for each team to reach a certain stage of the tournament.\nWe use Monte Carlo simulations for estimating the outcome of each single match\nof the tournament, from which we are able to simulate the whole tournament\nitself. The model is fitted on all football games of the participating teams\nsince 2014 weighted by date and importance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:17:24 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gilch", "Lorenz A.", ""]]}, {"id": "2106.05260", "submitter": "Jane Adams", "authors": "Jane L. Adams, Todd F. Deluca, Christopher M. Danforth, Peter S.\n  Dodds, Yuhang Zheng, Konstantinos Anastasakis, Boyoon Choi, Allison Min,\n  Michael M. Bessey", "title": "Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed\n  Data", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data scientists across disciplines are increasingly in need of exploratory\nanalysis tools for data sets with a high volume of features. We expand upon\ngraph mining approaches for exploratory analysis of high-dimensional data to\nintroduce Sirius, a visualization package for researchers to explore feature\nrelationships among mixed data types using mutual information and network\nbackbone sparsification. Visualizations of feature relationships aid data\nscientists in finding meaningful dependence among features, which can engender\nfurther analysis for feature selection, feature extraction, projection,\nidentification of proxy variables, or insight into temporal variation at the\nmacro scale. Graph mining approaches for feature analysis exist, such as\nassociation networks of binary features, or correlation networks of\nquantitative features, but mixed data types present a unique challenge for\ndeveloping comprehensive feature networks for exploratory analysis. Using an\ninformation theoretic approach, Sirius supports heterogeneous data sets\nconsisting of binary, continuous quantitative, and discrete categorical data\ntypes, and provides a user interface exploring feature pairs with high mutual\ninformation scores. We leverage a backbone sparsification approach from network\ntheory as a dimensionality reduction technique, which probabilistically trims\nedges according to the local network context. Sirius is an open source Python\npackage and Django web application for exploratory visualization, which can be\ndeployed in data analysis pipelines. The Sirius codebase and exemplary data\nsets can be found at: https://github.com/compstorylab/sirius\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:57:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Adams", "Jane L.", ""], ["Deluca", "Todd F.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter S.", ""], ["Zheng", "Yuhang", ""], ["Anastasakis", "Konstantinos", ""], ["Choi", "Boyoon", ""], ["Min", "Allison", ""], ["Bessey", "Michael M.", ""]]}, {"id": "2106.05328", "submitter": "Norman Fenton Prof", "authors": "Norman Fenton and Martin Neil", "title": "Calculating the Likelihood Ratio for Multiple Pieces of Evidence", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When presenting forensic evidence, such as a DNA match, experts often use the\nLikelihood ratio (LR) to explain the impact of evidence . The LR measures the\nprobative value of the evidence with respect to a single hypothesis such as\n'DNA comes from the suspect', and is defined as the probability of the evidence\nif the hypothesis is true divided by the probability of the evidence if the\nhypothesis is false. The LR is a valid measure of probative value because, by\nBayes Theorem, the higher the LR is, the more our belief in the probability the\nhypothesis is true increases after observing the evidence. The LR is popular\nbecause it measures the probative value of evidence without having to make any\nexplicit assumptions about the prior probability of the hypothesis. However,\nwhereas the LR can in principle be easily calculated for a distinct single\npiece of evidence that relates directly to a specific hypothesis, in most\nrealistic situations 'the evidence' is made up of multiple dependent components\nthat impact multiple different hypotheses. In such situations the LR cannot be\ncalculated . However, once the multiple pieces of evidence and hypotheses are\nmodelled as a causal Bayesian network (BN), any relevant LR can be\nautomatically derived using any BN software application.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:37:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Fenton", "Norman", ""], ["Neil", "Martin", ""]]}, {"id": "2106.05396", "submitter": "Naoufal Acharki", "authors": "Naoufal Acharki and Antoine Bertoncello and Josselin Garnier", "title": "Robust Prediction Interval estimation for Gaussian Processes by\n  Cross-Validation method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic regression models typically use the Maximum Likelihood\nEstimation or Cross-Validation to fit parameters. Unfortunately, these methods\nmay give advantage to the solutions that fit observations in average, but they\ndo not pay attention to the coverage and the width of Prediction Intervals. In\nthis paper, we address the question of adjusting and calibrating Prediction\nIntervals for Gaussian Processes Regression. First we determine the model's\nparameters by a standard Cross-Validation or Maximum Likelihood Estimation\nmethod then we adjust the parameters to assess the optimal type II Coverage\nProbability to a nominal level. We apply a relaxation method to choose\nparameters that minimize the Wasserstein distance between the Gaussian\ndistribution of the initial parameters (Cross-Validation or Maximum Likelihood\nEstimation) and the proposed Gaussian distribution among the set of parameters\nthat achieved the desired Coverage Probability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:19:08 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Acharki", "Naoufal", ""], ["Bertoncello", "Antoine", ""], ["Garnier", "Josselin", ""]]}, {"id": "2106.05617", "submitter": "Rituparna Sarkar", "authors": "Ximu Deng, Rituparna Sarkar, Elisabeth Labruyere, Jean-Christophe\n  Olivo-Marin and Anuj Srivastava", "title": "Dynamic Shape Modeling to Analyze Modes ofMigration During Cell Motility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops a generative statistical model for representing,\nmodeling, and comparing the morphological evolution of biological cells\nundergoing motility. It uses the elastic shape analysis to separate cell\nkinematics (overall location, rotation, speed, etc.) from its morphology and\nrepresents morphological changes using transported square-root vector fields\n(TSRVFs). This TSRVF representation, followed by a PCA-based dimension\nreduction, provides a convenient mathematical representation of a shape\nsequence in the form of a Euclidean time series. Fitting a vector\nauto-regressive (VAR) model to this TSRVF-PCA time series leads to statistical\nmodeling of the overall shape dynamics. We use the parameters of the fitted VAR\nmodel to characterize morphological evolution. We validate VAR models through\nmodel comparisons, synthesis, and sequence classifications. For classification,\nwe use the VAR parameters in conjunction with different classifiers: SVM,\nRandom Forest, and CNN, and obtain high classification rates. Extensive\nexperiments presented here demonstrate the success of the proposed pipeline.\nThese results are the first of the kind in classifying cell migration videos\nusing shape dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:51:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Deng", "Ximu", ""], ["Sarkar", "Rituparna", ""], ["Labruyere", "Elisabeth", ""], ["Olivo-Marin", "Jean-Christophe", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2106.05647", "submitter": "Stefano M. Iacus", "authors": "Michele Vespe, Stefano Maria Iacus, Carlos Santamaria, Francesco\n  Sermi, Spyridon Spyratos", "title": "On the Use of Data from Multiple Mobile Network Operators in Europe to\n  fight COVID-19", "comments": null, "journal-ref": "Data & Policy, 3, E9 (2021)", "doi": "10.1017/dap.2021.9", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid spread of COVID-19 infections on a global level has highlighted the\nneed for accurate, transparent and timely information regarding collective\nmobility patterns to inform de-escalation strategies as well as to provide\nforecasting capacity for re-escalation policies aiming at addressing further\nwaves of the virus. Such information can be extracted using aggregate\nanonymised data from innovative sources such as mobile positioning data. This\npaper presents lessons learnt and results of a unique Business-to-Government\n(B2G) initiative between several Mobile Network Operators in Europe and the\nEuropean Commission. Mobile positioning data have supported policy makers and\npractitioners with evidence and data-driven knowledge to understand and predict\nthe spread of the disease, the effectiveness of the containment measures, their\nsocio-economic impacts while feeding scenarios at EU scale and in a comparable\nway across countries. The challenges of this data sharing initiative are not\nlimited to data quality, harmonisation, and comparability across countries,\nhowever important they are. Equally essential aspects that need to be addressed\nfrom the onset are related to data privacy, security, fundamental rights and\ncommercial sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:39:21 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vespe", "Michele", ""], ["Iacus", "Stefano Maria", ""], ["Santamaria", "Carlos", ""], ["Sermi", "Francesco", ""], ["Spyratos", "Spyridon", ""]]}, {"id": "2106.05653", "submitter": "Tommaso Di Fonzo", "authors": "Tommaso Di Fonzo and Daniele Girolimetto", "title": "Forecast combination based forecast reconciliation: insights and\n  extensions", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent paper, while elucidating the links between forecast combination\nand cross-sectional forecast reconciliation, Hollyman et al. (2021) have\nproposed a forecast combination-based approach to the reconciliation of a\nsimple hierarchy. A new Level Conditional Coherent (LCC) point forecast\nreconciliation procedure was developed, and it was shown that the simple\naverage of a set of LCC, and bottom-up reconciled forecasts (called Combined\nConditional Coherent, CCC) results in good performance as compared to those\nobtained through the state-of-the-art cross-sectional reconciliation\nprocedures. In this paper, we build upon and extend this proposal along some\nnew directions. (1) We shed light on the nature and the mathematical derivation\nof the LCC reconciliation formula, showing that it is the result of an\nexogenously linearly constrained minimization of a quadratic loss function in\nthe differences between the target and the base forecasts with a diagonal\nassociated matrix. (2) Endogenous constraints may be considered as well,\nresulting in level conditional reconciled forecasts of all the involved series,\nwhere both the upper and the bottom time series are coherently revised. (3) As\nthe LCC procedure does not guarantee the non-negativity of the reconciled\nforecasts, we argue that - when non-negativity is a natural attribute of the\nvariables to be forecast - its interpretation as an unbiased top-down\nreconciliation procedure leaves room for some doubts. (4) The new procedures\nare used in a forecasting experiment on the classical Australian Tourism Demand\n(Visitor Nights) dataset. Due to the crucial role played by the (possibly\ndifferent) models used to compute the base forecasts, we re-interpret the CCC\nreconciliation of Hollyman et al. (2021) as a forecast pooling approach,\nshowing that accuracy improvement may be gained by adopting a simple forecast\naveraging strategy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:50:18 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Di Fonzo", "Tommaso", ""], ["Girolimetto", "Daniele", ""]]}, {"id": "2106.05799", "submitter": "Andreas Groll", "authors": "Andreas Groll, Lars Magnus Hvattum, Christophe Ley, Franziska Popp,\n  Gunther Schauberger, Hans Van Eetvelde, Achim Zeileis", "title": "Hybrid Machine Learning Forecasts for the UEFA EURO 2020", "comments": "Keywords: UEFA EURO 2020, Football, Machine Learning, Team abilities,\n  Sports tournaments. arXiv admin note: substantial text overlap with\n  arXiv:1906.01131, arXiv:1806.03208", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three state-of-the-art statistical ranking methods for forecasting football\nmatches are combined with several other predictors in a hybrid machine learning\nmodel. Namely an ability estimate for every team based on historic matches; an\nability estimate for every team based on bookmaker consensus; average\nplus-minus player ratings based on their individual performances in their home\nclubs and national teams; and further team covariates (e.g., market value, team\nstructure) and country-specific socio-economic factors (population, GDP). The\nproposed combined approach is used for learning the number of goals scored in\nthe matches from the four previous UEFA EUROs 2004-2016 and then applied to\ncurrent information to forecast the upcoming UEFA EURO 2020. Based on the\nresulting estimates, the tournament is simulated repeatedly and winning\nprobabilities are obtained for all teams. A random forest model favors the\ncurrent World Champion France with a winning probability of 14.8% before\nEngland (13.5%) and Spain (12.3%). Additionally, we provide survival\nprobabilities for all teams and at all tournament stages.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 21:56:49 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Groll", "Andreas", ""], ["Hvattum", "Lars Magnus", ""], ["Ley", "Christophe", ""], ["Popp", "Franziska", ""], ["Schauberger", "Gunther", ""], ["Van Eetvelde", "Hans", ""], ["Zeileis", "Achim", ""]]}, {"id": "2106.05818", "submitter": "Shiro Kuriwaki", "authors": "Valerie C. Bradley, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\n  Xiao-Li Meng, Seth Flaxman", "title": "Are We There Yet? Big Data Significantly Overestimates COVID-19\n  Vaccination in the US", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public health efforts to control the COVID-19 pandemic rely on accurate\nsurveys. However, estimates of vaccine uptake in the US from Delphi-Facebook,\nCensus Household Pulse, and Axios-Ipsos surveys exhibit the Big Data Paradox:\nthe larger the survey, the further its estimate from the benchmark provided by\nthe Centers for Disease Control and Prevention (CDC). In April 2021,\nDelphi-Facebook, the largest survey, overestimated vaccine uptake by 20\npercentage points. Discrepancies between estimates of vaccine willingness and\nhesitancy, which have no benchmarks, also grow over time and cannot be\nexplained through selection bias on traditional demographic variables alone.\nHowever, a recent framework on investigating Big Data quality (Meng, Annals of\nApplied Statistics, 2018) allows us to quantify contributing factors, and to\nprovide a data quality-driven scenario analysis for vaccine willingness and\nhesitancy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:34:14 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bradley", "Valerie C.", ""], ["Kuriwaki", "Shiro", ""], ["Isakov", "Michael", ""], ["Sejdinovic", "Dino", ""], ["Meng", "Xiao-Li", ""], ["Flaxman", "Seth", ""]]}, {"id": "2106.05820", "submitter": "Murray Pollock", "authors": "Paul A. Jenkins, Murray Pollock and Gareth O. Roberts", "title": "Bayesian semi-parametric inference for diffusion processes using splines", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-parametric method to simultaneously infer both the drift\nand volatility functions of a discretely observed scalar diffusion. We\nintroduce spline bases to represent these functions and develop a Markov chain\nMonte Carlo algorithm to infer, a posteriori, the coefficients of these\nfunctions in the spline basis. A key innovation is that we use spline bases to\nmodel transformed versions of the drift and volatility functions rather than\nthe functions themselves. The output of the algorithm is a posterior sample of\nplausible drift and volatility functions that are not constrained to any\nparticular parametric family. The flexibility of this approach provides\npractitioners a powerful investigative tool, allowing them to posit parametric\nmodels to better capture the underlying dynamics of their processes of\ninterest. We illustrate the versatility of our method by applying it to\nchallenging datasets from finance, paleoclimatology, and astrophysics. In view\nof the parametric diffusion models widely employed in the literature for those\nexamples, some of our results are surprising since they call into question some\naspects of these models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:37:38 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2106.05840", "submitter": "Mian Adnan", "authors": "Mian Arif Shams Adnan, H. M. Miraz Mahmud", "title": "A Bagging and Boosting Based Convexly Combined Optimum Mixture\n  Probabilistic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unlike previous studies on mixture distributions, a bagging and boosting\nbased convexly combined mixture probabilistic model has been suggested. This\nmodel is a result of iteratively searching for obtaining the optimum\nprobabilistic model that provides the maximum p value.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:20:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Adnan", "Mian Arif Shams", ""], ["Mahmud", "H. M. Miraz", ""]]}, {"id": "2106.05865", "submitter": "Davide Lauria", "authors": "Davide Lauria, Svetlozar T. Rachev, A. Alexandre Trindade", "title": "Global and Tail Dependence: A Differential Geometry Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Measures of tail dependence between random variables aim to numerically\nquantify the degree of association between their extreme realizations. Existing\ntail dependence coefficients (TDCs) are based on an asymptotic analysis of\nrelevant conditional probabilities, and do not provide a complete framework in\nwhich to compare extreme dependence between two random variables. In fact, for\nmany important classes of bivariate distributions, these coefficients take on\nnon-informative boundary values. We propose a new approach by first considering\nglobal measures based on the surface area of the conditional cumulative\nprobability in copula space, normalized with respect to departures from\nindependence and scaled by the difference between the two boundary copulas of\nco-monotonicity and counter-monotonicity. The measures could be approached by\ncumulating probability on either the lower left or upper right domain of the\ncopula space, and offer the novel perspective of being able to differentiate\nasymmetric dependence with respect to direction of conditioning. The resulting\nTDCs produce a smoother and more refined taxonomy of tail dependence. The\nempirical performance of the measures is examined in a simulated data context,\nand illustrated through a case study examining tail dependence between stock\nindices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:57:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lauria", "Davide", ""], ["Rachev", "Svetlozar T.", ""], ["Trindade", "A. Alexandre", ""]]}, {"id": "2106.06197", "submitter": "Marc Schneble", "authors": "Marc Schneble and G\\\"oran Kauermann", "title": "Statistical modeling of on-street parking lot occupancy in smart cities", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many studies suggest that searching for parking is associated with\nsignificant direct and indirect costs. Therefore, it is appealing to reduce the\ntime which car drivers spend on finding an available parking lot, especially in\nurban areas where the space for all road users is limited. The prediction of\non-street parking lot occupancy can provide drivers a guidance where clear\nparking lots are likely to be found. This field of research has gained more and\nmore attention in the last decade through the increasing availability of\nreal-time parking lot occupancy data. In this paper, we pursue a statistical\napproach for the prediction of parking lot occupancy, where we make use of time\nto event models and semi-Markov process theory. The latter involves the\nemployment of Laplace transformations as well as their inversion which is an\nambitious numerical task. We apply our methodology to data from the City of\nMelbourne in Australia. Our main result is that the semi-Markov model\noutperforms a Markov model in terms of both true negative rate and true\npositive rate while this is essentially achieved by respecting the current\nduration which a parking lot already sojourns in its initial state.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 06:55:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Schneble", "Marc", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2106.06268", "submitter": "Tiia-Maria Pasanen", "authors": "Tiia-Maria Pasanen and Miikka Voutilainen and Jouni Helske and Harri\n  H\\\"ogmander", "title": "A Bayesian spatio-temporal error correction analysis of markets during\n  the Finnish 1860s famine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian spatio-temporal error correction model and use it to\nanalyze grain market integration in Finland during the 1860s famine. Compared\nwith the existing error correction methodology, our approach allows\nsimultaneous modeling of multiple interdependent time series without cumbersome\nstatistical testing needed to predetermine the point of reference -- the market\nleader. Furthermore, introducing a spatio-temporal structure enables a flexible\nanalysis of the regional and intertemporal variations in the market mechanism.\nWe detected spatially asymmetric ``price ripples'' that spread out from the\nshock origin. We corroborated the existing literature on the speedier\nadjustment to emerging price differentials during the famine, but we detected\nthis principally in urban markets. This hastened return to long-run equilibrium\nmeans faster and longer travel of price shocks. This, in turn, implies\nprolonged out-of-equilibrium dynamics, proliferated influence of market shocks,\nand, importantly, a wider spread of famine conditions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:33:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pasanen", "Tiia-Maria", ""], ["Voutilainen", "Miikka", ""], ["Helske", "Jouni", ""], ["H\u00f6gmander", "Harri", ""]]}, {"id": "2106.06348", "submitter": "Saint-Clair Chabert-Liddell", "authors": "Saint-Clair Chabert-Liddell, Pierre Barbillon, Sophie Donnet", "title": "Impact of the mesoscale structure of a bipartite ecological interaction\n  network on its robustness through a probabilistic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The robustness of an ecological network quantifies the resilience of the\necosystem it represents to species loss. It corresponds to the proportion of\nspecies that are disconnected from the rest of the network when extinctions\noccur sequentially. Classically, the robustness is calculated for a given\nnetwork, from the simulation of a large number of extinction sequences. The\nlink between network structure and robustness remains an open question. Setting\na joint probabilistic model on the network and the extinction sequences allows\nto analyze this relation.\n  Bipartite stochastic block models have proven their ability to model\nbipartite networks e.g. plant-pollinator networks: species are divided into\nblocks and interaction probabilities are determined by the blocks of\nmembership. Analytical expressions of the expectation and variance of\nrobustness are obtained under this model, for different distributions of\nprimary extinction sequences. The impact of the network structure on the\nrobustness is analyzed through a set of properties and numerical illustrations.\nThe analysis of a collection of bipartite ecological networks allows us to\ncompare the empirical approach to our probabilistic approach, and illustrates\nthe relevance of the latter when it comes to computing the robustness of a\npartially observed or incompletely sampled network.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:42:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chabert-Liddell", "Saint-Clair", ""], ["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""]]}, {"id": "2106.06362", "submitter": "Tomi Kinnunen", "authors": "Tomi Kinnunen, Andreas Nautsch, Md Sahidullah, Nicholas Evans, Xin\n  Wang, Massimiliano Todisco, H\\'ector Delgado, Junichi Yamagishi, Kong Aik Lee", "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker\n  Verification and Voice Anti-Spoofing", "comments": "Accepted to Interspeech 2021. Example code available at\n  https://github.com/asvspoof-challenge/classifier-adjacency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Whether it be for results summarization, or the analysis of classifier\nfusion, some means to compare different classifiers can often provide\nilluminating insight into their behaviour, (dis)similarity or complementarity.\nWe propose a simple method to derive 2D representation from detection scores\nproduced by an arbitrary set of binary classifiers in response to a common\ndataset. Based upon rank correlations, our method facilitates a visual\ncomparison of classifiers with arbitrary scores and with close relation to\nreceiver operating characteristic (ROC) and detection error trade-off (DET)\nanalyses. While the approach is fully versatile and can be applied to any\ndetection task, we demonstrate the method using scores produced by automatic\nspeaker verification and voice anti-spoofing systems. The former are produced\nby a Gaussian mixture model system trained with VoxCeleb data whereas the\nlatter stem from submissions to the ASVspoof 2019 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:03:33 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kinnunen", "Tomi", ""], ["Nautsch", "Andreas", ""], ["Sahidullah", "Md", ""], ["Evans", "Nicholas", ""], ["Wang", "Xin", ""], ["Todisco", "Massimiliano", ""], ["Delgado", "H\u00e9ctor", ""], ["Yamagishi", "Junichi", ""], ["Lee", "Kong Aik", ""]]}, {"id": "2106.06591", "submitter": "Ira Herniter", "authors": "Joshua E. Gang, Wanqi Jia, Ira A. Herniter", "title": "Analysis of historical data leveraging the sandpile model of\n  self-organized criticality demonstrates the efficacy of prescribed burns in\n  reducing risk of destructive wildfires", "comments": "2 Figures, 3 Tables, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prescribed burns have been increasingly administered to forest management\nwith the assumption that they help reduce the risk of wildfires; however, this\nhypothesis has yet to be rigorously tested. We leverage the historical data of\nforest fires from multiple states to show that the sandpile model of\nself-organized criticality accurately represents the real-world incidence of\nfire by describing a negative linear relationship between the logarithm of fire\nsize and the logarithm of the fire incidence number of that size. We then\ninvestigate the association between the size of prescribed burn and the slope\nof the negative linear relationship which represents the relative risk of\ndestructive wildfires. The results demonstrate that increases in the area\nsubject to prescribed burning generally reduce the risk of destructive\nwildfires. This is consistent with the Florida data, which shows a trend in\nreduction of destructive wildfires as prescribed burns have been progressively\nintroduced to forest management. Our study justifies the application of the\nsandpile model to wildfire research and establishes a novel method of the\nanalysis of slope estimated from the sandpile model for facilitating the\ninvestigation of potential risk factors of destructive wildfires and the\ndevelopment of an optimal strategy for prescribed burning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:40:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gang", "Joshua E.", ""], ["Jia", "Wanqi", ""], ["Herniter", "Ira A.", ""]]}, {"id": "2106.06677", "submitter": "Shan Jiang", "authors": "Tigran Aslanyan, Shan Jiang", "title": "Examining Passenger Vehicle Miles Traveled and Carbon Emissions in the\n  Boston Metropolitan Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With spatial analytic, econometric, and visualization tools, this book\nchapter investigates greenhouse gas emissions for the on-road passenger vehicle\ntransport sector in the Boston metropolitan area in 2014. It compares\ngreenhouse gas emission estimations from both the production-based and\nconsumption-based perspectives with two large-scale administrative datasets:\nthe vehicle odometer readings from individual vehicle annual inspection, and\nthe road inventory data containing road segment level geospatial and traffic\ninformation. Based on spatial econometric models that examine socioeconomic and\nbuilt environment factors contributing to the vehicle miles traveled at the\ncensus tract level, it offers insights to help cities reduce VMT and carbon\nfootprint for passenger vehicle travel. Finally, it recommends a pathway for\ncities and towns in the Boston metropolitan area to curb VMT and mitigate\ncarbon emissions to achieve climate goals of carbon neutrality.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 03:28:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Aslanyan", "Tigran", ""], ["Jiang", "Shan", ""]]}, {"id": "2106.06691", "submitter": "Aaron Schein", "authors": "Aaron Schein, Anjali Nagulpally, Hanna Wallach, Patrick Flaherty", "title": "Doubly Non-Central Beta Matrix Factorization for DNA Methylation Data", "comments": "To appear in the Proceedings of the Conference on Uncertainty in\n  Artificial Intelligence (UAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new non-negative matrix factorization model for $(0,1)$\nbounded-support data based on the doubly non-central beta (DNCB) distribution,\na generalization of the beta distribution. The expressiveness of the DNCB\ndistribution is particularly useful for modeling DNA methylation datasets,\nwhich are typically highly dispersed and multi-modal; however, the model\nstructure is sufficiently general that it can be adapted to many other domains\nwhere latent representations of $(0,1)$ bounded-support data are of interest.\nAlthough the DNCB distribution lacks a closed-form conjugate prior, several\naugmentations let us derive an efficient posterior inference algorithm composed\nentirely of analytic updates. Our model improves out-of-sample predictive\nperformance on both real and synthetic DNA methylation datasets over\nstate-of-the-art methods in bioinformatics. In addition, our model yields\nmeaningful latent representations that accord with existing biological\nknowledge.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 05:36:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Schein", "Aaron", ""], ["Nagulpally", "Anjali", ""], ["Wallach", "Hanna", ""], ["Flaherty", "Patrick", ""]]}, {"id": "2106.06805", "submitter": "Rendani Mbuvha", "authors": "Rendani Mbuvha, Patience Zondo, Aluwani Mauda, Tshilidzi Marwala", "title": "Predicting Higher Education Throughput in South Africa Using a\n  Tree-Based Ensemble Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use gradient boosting machines and logistic regression to predict academic\nthroughput at a South African university. The results highlight the significant\ninfluence of socio-economic factors and field of study as predictors of\nthroughput. We further find that socio-economic factors become less of a\npredictor relative to the field of study as the time to completion increases.\nWe provide recommendations on interventions to counteract the identified\neffects, which include academic, psychosocial and financial support.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:54:19 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mbuvha", "Rendani", ""], ["Zondo", "Patience", ""], ["Mauda", "Aluwani", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "2106.06868", "submitter": "Laura Sof\\'ia Hoyos-G\\'omez", "authors": "Laura S. Hoyos-G\\'omez, Jose F. Ruiz-Mu\\~noz, Belizza J. Ruiz-Mendoza", "title": "Short-term forecasting of global solar irradiance with incomplete data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate mechanisms for forecasting solar irradiance and insolation provide\nimportant information for the planning of renewable energy and agriculture\nprojects as well as for environmental and socio-economical studies. This\nresearch introduces a pipeline for the one-day ahead forecasting of solar\nirradiance and insolation that only requires solar irradiance historical data\nfor training. Furthermore, our approach is able to deal with missing data since\nit includes a data imputation state. In the prediction stage, we consider four\ndata-driven approaches: Autoregressive Integrated Moving Average (ARIMA),\nSingle Layer Feed Forward Network (SL-FNN), Multiple Layer Feed Forward Network\n(FL-FNN), and Long Short-Term Memory (LSTM). The experiments are performed in a\nreal-world dataset collected with 12 Automatic Weather Stations (AWS) located\nin the Nari\\~no - Colombia. The results show that the neural network-based\nmodels outperform ARIMA in most cases. Furthermore, LSTM exhibits better\nperformance in cloudy environments (where more randomness is expected).\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 21:44:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hoyos-G\u00f3mez", "Laura S.", ""], ["Ruiz-Mu\u00f1oz", "Jose F.", ""], ["Ruiz-Mendoza", "Belizza J.", ""]]}, {"id": "2106.07096", "submitter": "Kenneth Harris", "authors": "Kenneth D. Harris", "title": "A test for partial correlation between repeatedly observed nonstationary\n  nonlinear timeseries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a family of statistical tests to measure partial correlation in\nvectorial timeseries. The test measures whether an observed timeseries Y can be\npredicted from a second series X, even after accounting for a third series Z\nwhich may correlate with X. It does not make any assumptions on the nature of\nthese timeseries, such as stationarity or linearity, but it does require that\nmultiple statistically independent recordings of the 3 series are available.\nIntuitively, the test works by asking if the series Y recorded on one\nexperiment can be better predicted from X recorded on the same experiment than\non a different experiment, after accounting for the prediction from Z recorded\non both experiments.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 21:35:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Harris", "Kenneth D.", ""]]}, {"id": "2106.07374", "submitter": "Ick Hoon Jin", "authors": "Yeseul Jeon and Dongjun Chung and Jina Park and Ick Hoon Jin", "title": "Network-based Trajectory Topic Interaction Map for Text Mining of\n  COVID-19 Biomedical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the emergence of the worldwide pandemic of COVID-19, relevant research\nhas been published at a dazzling pace, which makes it hard to follow the\nresearch in this area without dedicated efforts. It is practically impossible\nto implement this task manually due to the high volume of the relevant\nliterature. Text mining has been considered to be a powerful approach to\naddress this challenge, especially the topic modeling, a well-known\nunsupervised method that aims to reveal latent topics from the literature.\nHowever, in spite of its potential utility, the results generated from this\napproach are often investigated manually. Hence, its application to the\nCOVID-19 literature is not straightforward and expert knowledge is needed to\nmake meaningful interpretations. In order to address these challenges, we\npropose a novel analytical framework for estimating topic interactions and\neffective visualization for topic interpretation. Here we assumed that topics\nconstituting a paper can be positioned on an interaction map, which belongs to\na high-dimensional Euclidean space. Based on this assumption, after summarizing\ntopics with their topic-word distributions using the biterm topic model, we\nmapped these latent topics on networks to visualize relationships among the\ntopics. Moreover, in the proposed approach, the change of relationships among\ntopics can be traced using a trajectory plot generated with different levels of\nword richness. These results together provide deeply mined and intuitive\nrepresentation of relationships among topics related to a specific research\narea. The application of this proposed framework to the PubMed literature shows\nthat our approach facilitates understanding of the topics constituting the\nCOVID-19 knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:01:17 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 02:39:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jeon", "Yeseul", ""], ["Chung", "Dongjun", ""], ["Park", "Jina", ""], ["Jin", "Ick Hoon", ""]]}, {"id": "2106.07393", "submitter": "Ka Wong", "authors": "Ka Wong, Praveen Paritosh, Lora Aroyo", "title": "Cross-replication Reliability -- An Empirical Approach to Interpreting\n  Inter-rater Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new approach to interpreting IRR that is empirical and\ncontextualized. It is based upon benchmarking IRR against baseline measures in\na replication, one of which is a novel cross-replication reliability (xRR)\nmeasure based on Cohen's kappa. We call this approach the xRR framework. We\nopensource a replication dataset of 4 million human judgements of facial\nexpressions and analyze it with the proposed framework. We argue this framework\ncan be used to measure the quality of crowdsourced datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:15:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wong", "Ka", ""], ["Paritosh", "Praveen", ""], ["Aroyo", "Lora", ""]]}, {"id": "2106.07461", "submitter": "Gianluca Boo Dr", "authors": "Gianluca Boo, Edith Darin, Douglas R Leasure, Claire A Dooley, Heather\n  R Chamberlain, Attila N L\\'az\\'ar, Kevin Tschirhart, Cyrus Sinai, Nicole A\n  Hoff, Trevon Fuller, Kamy Musene, Arly Batumbo, Anne W Rimoin, Andrew J Tatem", "title": "High-resolution population estimation using household survey data and\n  building footprints", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The national census is an essential data source to support decision-making in\nmany areas of public interest. However, this data may become outdated during\nthe intercensal period, which can stretch up to several decades. We developed a\nBayesian hierarchical model leveraging recent household surveys with\nprobabilistic sampling designs and building footprints to produce up-to-date\npopulation estimates. We estimated population totals and age and sex breakdowns\nwith associated uncertainty measures within grid cells of approximately 100m in\nfive provinces of the Democratic Republic of the Congo, a country where the\nlast census was completed in 1984. The model exhibited a very good fit, with an\nR^2 value of 0.79 for out-of-sample predictions of population totals at the\nmicrocensus-cluster level and 1.00 for age and sex proportions at the province\nlevel. The results confirm the benefits of combining household surveys and\nbuilding footprints for high-resolution population estimation in countries with\noutdated censuses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:39:55 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Boo", "Gianluca", ""], ["Darin", "Edith", ""], ["Leasure", "Douglas R", ""], ["Dooley", "Claire A", ""], ["Chamberlain", "Heather R", ""], ["L\u00e1z\u00e1r", "Attila N", ""], ["Tschirhart", "Kevin", ""], ["Sinai", "Cyrus", ""], ["Hoff", "Nicole A", ""], ["Fuller", "Trevon", ""], ["Musene", "Kamy", ""], ["Batumbo", "Arly", ""], ["Rimoin", "Anne W", ""], ["Tatem", "Andrew J", ""]]}, {"id": "2106.07478", "submitter": "Inder Tecuapetla-G\\'omez", "authors": "Inder Tecuapetla-G\\'omez and Julia Trinidad Reyes", "title": "Modeling satellite-based open water fraction via flexible Beta\n  regression: An application to wetlands in the north-western Pacific coast of\n  Mexico", "comments": "5 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Carbon sequestration and water filtering are two examples of the several\necosystem services provided by wetlands. Open water mapping is an effective\nmeans to measure any wetland extension as these are comprised of many open\nwater bodies. An economical, though indirect, approach towards mapping open\nwater bodies is through applying geo-computational methods to satellite images.\nIn this work we propose the flexible Beta regression (FBR) model to predict\nopen water fraction from measurements of a water index. We focus on\nobservations derived from two MODIS images acquired during the dry season of\n2008 in Marismas Nacionales, a wetland located in the north-western Pacific\ncoast of Mexico. A Bayesian estimation procedure is presented to estimate the\nFBR model; in particular, we provide details of a nested Metropolis-Hastings\nand Gibbs sampling algorithm to carry out parameter estimation. Our results\nshow that the FBR model produces valid predictors of water fraction unlike the\nstandard model. Our work is complemented by software developed in the R\nlanguage and available through a GitHub repository.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:06:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tecuapetla-G\u00f3mez", "Inder", ""], ["Reyes", "Julia Trinidad", ""]]}, {"id": "2106.07623", "submitter": "Ciaran Evans", "authors": "Ciaran Evans, Zara Y. Weinberg, Manojkumar A. Puthenveedu, Max G'Sell", "title": "Inference with generalizable classifier predictions", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of making statistical inference about a\npopulation that can only be identified through classifier predictions. The\nproblem is motivated by scientific studies in which human labels of a\npopulation are replaced by a classifier. For downstream analysis of the\npopulation based on classifier predictions to be sound, the predictions must\ngeneralize equally across experimental conditions. In this paper, we formalize\nthe task of statistical inference using classifier predictions, and propose\nbootstrap procedures to allow inference with a generalizable classifier. We\ndemonstrate the performance of our methods through extensive simulations and a\ncase study with live cell imaging data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:28:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Evans", "Ciaran", ""], ["Weinberg", "Zara Y.", ""], ["Puthenveedu", "Manojkumar A.", ""], ["G'Sell", "Max", ""]]}, {"id": "2106.07626", "submitter": "Gabriel Calvo", "authors": "Gabriel Calvo, Carmen Armero, Virgilio G\\'omez-Rubio, Guido Mazzinari", "title": "Bayesian hierarchical nonlinear modelling of intra-abdominal volume\n  during pneumoperitoneum for laparoscopic surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laparoscopy is an operation carried out in the abdomen or pelvis through\nsmall incisions with external visual control by a camera. This technique needs\nthe abdomen to be insufflated with carbon dioxide to obtain a working space for\nsurgical instruments' manipulation. Identifying the critical point at which\ninsufflation should be limited is crucial to maximizing surgical working space\nand minimizing injurious effects. Bayesian nonlinear growth mixed-effects\nmodels are applied to data coming from a repeated measures design. This study\nallows to assess the relationship between the insufflation pressure and the\nintra--abdominal volume.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:31:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Calvo", "Gabriel", ""], ["Armero", "Carmen", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Mazzinari", "Guido", ""]]}, {"id": "2106.07797", "submitter": "Justin Krometis", "authors": "Justin A. Krometis, Hayden Ringer, Jared P. Whitehead, Nathan E.\n  Glatt-Holtz, Ronald A. Harris", "title": "Embracing Uncertainty in \"Small Data\" Problems: Estimating Earthquakes\n  from Historical Anecdotes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the Bayesian inversion process to make principled estimates of the\nmagnitude and location of a pre-instrumental earthquake in Eastern Indonesia in\nthe mid 19th century, by combining anecdotal historical accounts of the\nresultant tsunami with our modern understanding of the geology of the region.\nQuantifying the seismic record prior to modern instrumentation is critical to a\nmore thorough understanding of the current risks in Eastern Indonesia. In\nparticular, the occurrence of such a major earthquake in the 1850s provides\nevidence that this region is susceptible to future seismic hazards on the same\norder of magnitude. More importantly, the approach taken here gives evidence\nthat even \"small data\" that is limited in scope and extremely uncertain can\nstill be used to yield information on past seismic events, which is key to an\nincreased understanding of the current seismic state. Moreover, sensitivity\nbounds indicate that the results obtained here are robust despite the inherent\nuncertainty in the observations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:12:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Krometis", "Justin A.", ""], ["Ringer", "Hayden", ""], ["Whitehead", "Jared P.", ""], ["Glatt-Holtz", "Nathan E.", ""], ["Harris", "Ronald A.", ""]]}, {"id": "2106.07834", "submitter": "Grigorios Lavrentiadis", "authors": "Grigorios Lavrentiadis, Norman A. Abrahamson, Nicolas M. Kuehn", "title": "A Non-ergodic Effective Amplitude Ground-Motion Model for California", "comments": "34 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new non-ergodic ground-motion model (GMM) for effective amplitude spectral\n($EAS$) values for California is presented in this study. $EAS$, which is\ndefined in Goulet et al. (2018), is a smoothed rotation-independent Fourier\namplitude spectrum of the two horizontal components of an acceleration time\nhistory. The main motivation for developing a non-ergodic $EAS$ GMM, rather\nthan a spectral acceleration GMM, is that the scaling of $EAS$ does not depend\non spectral shape, and therefore, the more frequent small magnitude events can\nbe used in the estimation of the non-ergodic terms.\n  The model is developed using the California subset of the NGAWest2 dataset\nAncheta et al. (2013). The Bayless and Abrahamson (2019b) (BA18) ergodic $EAS$\nGMM was used as backbone to constrain the average source, path, and site\nscaling. The non-ergodic GMM is formulated as a Bayesian hierarchical model:\nthe non-ergodic source and site terms are modeled as spatially varying\ncoefficients following the approach of Landwehr et al. (2016), and the\nnon-ergodic path effects are captured by the cell-specific anelastic\nattenuation attenuation following the approach of Dawood and Rodriguez-Marek\n(2013). Close to stations and past events, the mean values of the non-ergodic\nterms deviate from zero to capture the systematic effects and their epistemic\nuncertainty is small. In areas with sparse data, the epistemic uncertainty of\nthe non-ergodic terms is large, as the systematic effects cannot be determined.\n  The non-ergodic total aleatory standard deviation is approximately $30$ to\n$40\\%$ smaller than the total aleatory standard deviation of BA18. This\nreduction in the aleatory variability has a significant impact on hazard\ncalculations at large return periods. The epistemic uncertainty of the ground\nmotion predictions is small in areas close to stations and past events.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 01:41:54 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 02:17:16 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lavrentiadis", "Grigorios", ""], ["Abrahamson", "Norman A.", ""], ["Kuehn", "Nicolas M.", ""]]}, {"id": "2106.07909", "submitter": "Gerg\\H{o} Pint\\'er", "authors": "Gerg\\H{o} Pint\\'er and Imre Felde", "title": "Evaluating the Effect of the Financial Status to the Mobility Customs", "comments": null, "journal-ref": null, "doi": "10.3390/ijgi10050328", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we explore the relationship between cellular phone data and\nhousing prices in Budapest, Hungary. We determine mobility indicators from one\nmonths of Call Detail Records (CDR) data, while the property price data are\nused to characterize the socioeconomic status at the Capital of Hungary. First,\nwe validated the proposed methodology by comparing the Home and Work locations\nestimation and the commuting patterns derived from the cellular network dataset\nwith reports of the national mini census. We investigated the statistical\nrelationships between mobile phone indicators, such as Radius of Gyration, the\ndistance between Home and Work locations or the Entropy of visited cells, and\nmeasures of economic status based on housing prices. Our findings show that the\nmobility correlates significantly with the socioeconomic status. We performed\nPrincipal Component Analysis (PCA) on combined vectors of mobility indicators\nin order to characterize the dependence of mobility habits on socioeconomic\nstatus. The results of the PCA investigation showed remarkable correlation of\nhousing prices and mobility customs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:47:05 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pint\u00e9r", "Gerg\u0151", ""], ["Felde", "Imre", ""]]}, {"id": "2106.08048", "submitter": "Boris Tseytlin Mr", "authors": "Boris Tseytlin and Ilya Makarov", "title": "Epidemic modelling of multiple virus strains: a case study of SARS-CoV-2\n  B.1.1.7 in Moscow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During a long-running pandemic a pathogen can mutate, producing new strains\nwith different epidemiological parameters. Existing approaches to epidemic\nmodelling only consider one virus strain. We have developed a modified SEIR\nmodel to simulate multiple virus strains within the same population. As a case\nstudy, we investigate the potential effects of SARS-CoV-2 strain B.1.1.7 on the\ncity of Moscow. Our analysis indicates a high risk of a new wave of infections\nin September-October 2021 with up to 35 000 daily infections at peak. We\nopen-source our code and data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:07:59 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:42:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tseytlin", "Boris", ""], ["Makarov", "Ilya", ""]]}, {"id": "2106.08077", "submitter": "Thiyanga Talagala Dr", "authors": "Jayani P. G. Lakshika, Thiyanga S. Talagala", "title": "Computer-aided Interpretable Features for Leaf Image Classification", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:11:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lakshika", "Jayani P. G.", ""], ["Talagala", "Thiyanga S.", ""]]}, {"id": "2106.08472", "submitter": "Bikramjit Das", "authors": "Bikramjit Das, Tiandong Wang and Gengling Dai", "title": "Asymptotic Behavior of Common Connections in Sparse Random Networks", "comments": "21 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random network models generated using sparse exchangeable graphs have\nprovided a mechanism to study a wide variety of complex real-life networks. In\nparticular, these models help with investigating power-law properties of degree\ndistributions, number of edges, and other relevant network metrics which\nsupport the scale-free structure of networks. Previous work on such graphs\nimposes a marginal assumption of univariate regular variation (e.g., power-law\ntail) on the bivariate generating graphex function. In this paper, we study\nsparse exchangeable graphs generated by graphex functions which are\nmultivariate regularly varying. We also focus on a different metric for our\nstudy: the distribution of the number of common vertices (connections) shared\nby a pair of vertices. The number being high for a fixed pair is an indicator\nof the original pair of vertices being connected. We find that the distribution\nof number of common connections are regularly varying as well, where the tail\nindices of regular variation are governed by the type of graphex function used.\nOur results are verified on simulated graphs by estimating the relevant tail\nindex parameters.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:43:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Das", "Bikramjit", ""], ["Wang", "Tiandong", ""], ["Dai", "Gengling", ""]]}, {"id": "2106.08511", "submitter": "Xu Han", "authors": "Lijia Wang, Xu Han and Xin Tong", "title": "Skilled Mutual Fund Selection: False Discovery Control under Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Selecting skilled mutual funds through the multiple testing framework has\nreceived increasing attention from finance researchers and statisticians. The\nintercept $\\alpha$ of Carhart four-factor model is commonly used to measure the\ntrue performance of mutual funds, and positive $\\alpha$'s are considered as\nskilled. We observe that the standardized OLS estimates of $\\alpha$'s across\nthe funds possess strong dependence and nonnormality structures, indicating\nthat the conventional multiple testing methods are inadequate for selecting the\nskilled funds. We start from a decision theoretic perspective, and propose an\noptimal testing procedure to minimize a combination of false discovery rate and\nfalse non-discovery rate. Our proposed testing procedure is constructed based\non the probability of each fund not being skilled conditional on the\ninformation across all of the funds in our study. To model the distribution of\nthe information used for the testing procedure, we consider a mixture model\nunder dependence and propose a new method called ``approximate empirical Bayes\"\nto fit the parameters. Empirical studies show that our selected skilled funds\nhave superior long-term and short-term performance, e.g., our selection\nstrongly outperforms the S\\&P 500 index during the same period.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:44:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Lijia", ""], ["Han", "Xu", ""], ["Tong", "Xin", ""]]}, {"id": "2106.08701", "submitter": "Olesya Mryglod", "authors": "O. Mryglod, S. Nazarovets and S. Kozmenko", "title": "Universal and specific features of Ukrainian economic research:\n  publication analysis based on Crossref data", "comments": "This is the version of the Article before peer-review has taken\n  place. The paper is submitted to \\textit{Scientometrics} journal, Manuscript\n  Number: SCIM-D-21-00282R1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our study is one of the first examples of multidimensional and longitudinal\ndisciplinary analysis at the national level based on Crossref data. We present\na large-scale quantitative analysis of Ukrainian economics. This study is not\nyet another example of research aimed at ranking of local journals, authors or\ninstitutions, but rather exploring general tendencies that can be compared to\nother countries or regions. We study different aspects of Ukrainian economics\noutput. In particular, the collaborative nature, geographic landscape and some\npeculiarities of citation statistics are investigated. We have found that\nUkrainian economics is characterized by a comparably small share of co-authored\npublications, however, it demonstrates the tendency towards more collaborative\noutput. Based on our analysis, we discuss specific and universal features of\nUkrainian economic research. The importance of supporting various initiatives\naimed at enriching open scholarly metadata is considered. A comprehensive and\nhigh-quality meta description of publications is probably the shortest path to\na better understanding of national trends, especially for non-English speaking\ncountries. The results of our analysis can be used to better understand\nUkrainian economic research and support research policy decisions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:07:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mryglod", "O.", ""], ["Nazarovets", "S.", ""], ["Kozmenko", "S.", ""]]}, {"id": "2106.08912", "submitter": "Andrew Finley Dr.", "authors": "Arne Nothdurft, Christoph Gollob, Ralf Kra{\\ss}nitzer, Gernot Erber,\n  Tim Ritter, Karl Stampfer, Andrew O. Finley", "title": "Estimating timber volume loss due to storm damage in Carinthia, Austria,\n  using ALS/TLS and spatial regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A spatial regression model framework is presented to predict growing stock\nvolume loss due to storm Adrian which caused heavy forest damage in the upper\nGail valley in Carinthia, Austria, in October 2018. Model parameters were\nestimated using growing stock volume measured with a terrestrial laser scanner\non 62 sample plots distributed across five sub-regions. Predictor variables\nwere derived from high resolution vegetation height measurements collected\nduring an airborne laser scanning campaign. Non-spatial and spatial candidate\nmodels were proposed and assessed based on fit to observed data and\nout-of-sample prediction. Spatial Gaussian processes associated model\nintercepts and regression coefficients were used to capture spatial dependence.\nResults show a spatially-varying coefficient model, which allowed the intercept\nand regression coefficients to vary spatially, yielded the best fit and\nprediction. Two approaches were considered for prediction over blowdown areas:\n1) an areal approach that viewed each blowdown as a single prediction unit\nindexed by its centroid; and 2) a block approach where each blowdown was\npartitioned into smaller prediction units to better align with sample plots'\nspatial support. Joint prediction was used to acknowledge spatial dependence\namong block units. Results demonstrated the block approach is preferable as it\nmitigated change-of-support issues encountered in the areal approach. Despite\nthe small sample size, predictions for 55% of the total 564 blowdown areas,\naccounting for 93% of the total loss, had a coefficient of variation less than\n25%. Key advantages of the proposed regression framework are the ability to\nquantify uncertainty in spatial covariance parameters, propagate parameter\nuncertainty through to prediction, and provide statistically valid prediction\npoint and interval estimates for individual blowdowns and collections of\nblowdowns.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:05:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nothdurft", "Arne", ""], ["Gollob", "Christoph", ""], ["Kra\u00dfnitzer", "Ralf", ""], ["Erber", "Gernot", ""], ["Ritter", "Tim", ""], ["Stampfer", "Karl", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2106.09105", "submitter": "Trevor Werho", "authors": "Trevor Werho, Junshan Zhang, Vijay Vittal, Yonghong Chen, Anupam\n  Thatte, Long Zhao", "title": "Scenario Generation of Wind Farm Power for Real-Time System Operation", "comments": "8 pages, 9 figures, submitted to IEEE transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work proposes a method of wind farm scenario generation to support\nreal-time optimization tools and presents key findings therein. This work draws\nupon work from the literature and presents an efficient and scalable method for\nproducing an adequate number of scenarios for a large fleet of wind farms while\ncapturing both spatial and temporal dependencies. The method makes\nprobabilistic forecasts using conditional heteroscedastic regression for each\nwind farm and time horizon. Past training data is transformed (using the\nprobabilistic forecasting models) into standard normal samples. A Gaussian\ncopula is estimated from the normalized samples and used in real-time to\nenforce proper spatial and temporal dependencies. The method is evaluated using\nhistorical data from MISO and performance within the MISO real-time look-ahead\nframework is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:18:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Werho", "Trevor", ""], ["Zhang", "Junshan", ""], ["Vittal", "Vijay", ""], ["Chen", "Yonghong", ""], ["Thatte", "Anupam", ""], ["Zhao", "Long", ""]]}, {"id": "2106.09114", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Bohan Wu", "title": "Semiparametric count data regression for self-reported mental health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"For how many days during the past 30 days was your mental health not good?\"\nThe responses to this question measure self-reported mental health and can be\nlinked to important covariates in the National Health and Nutrition Examination\nSurvey (NHANES). However, these count variables present major distributional\nchallenges: the data are overdispersed, zero-inflated, bounded by 30, and\nheaped in five- and seven-day increments. To meet these challenges, we design a\nsemiparametric estimation and inference framework for count data regression.\nThe data-generating process is defined by simultaneously transforming and\nrounding (STAR) a latent Gaussian regression model. The transformation is\nestimated nonparametrically and the rounding operator ensures the correct\nsupport for the discrete and bounded data. Maximum likelihood estimators are\ncomputed using an EM algorithm that is compatible with any continuous data\nmodel estimable by least squares. STAR regression includes asymptotic\nhypothesis testing and confidence intervals, variable selection via information\ncriteria, and customized diagnostics. Simulation studies validate the utility\nof this framework. STAR is deployed to study the factors associated with\nself-reported mental health and demonstrates substantial improvements in\ngoodness-of-fit compared to existing count data regression models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:38:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Wu", "Bohan", ""]]}, {"id": "2106.09379", "submitter": "Helmi Shat", "authors": "Helmi Shat", "title": "Optimal Design of Stress Levels in Accelerated Degradation Testing for\n  Multivariate Linear Degradation Models", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.09446", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, more attention has been paid prominently to accelerated\ndegradation testing in order to characterize accurate estimation of reliability\nproperties for systems that are designed to work properly for years of even\ndecades. %In this regard, degradation data from particular testing levels of\nthe stress variable(s) are extrapolated with an appropriate statistical model\nto obtain estimates of lifetime quantiles at normal use levels. In this paper\nwe propose optimal experimental designs for repeated measures accelerated\ndegradation tests with competing failure modes that correspond to multiple\nresponse components. The observation time points are assumed to be fixed and\nknown in advance. The marginal degradation paths are expressed using linear\nmixed effects models. The optimal design is obtained by minimizing the\nasymptotic variance of the estimator of some quantile of the failure time\ndistribution at the normal use conditions. Numerical examples are introduced to\nensure the robustness of the proposed optimal designs and compare their\nefficiency with standard experimental designs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:52:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Shat", "Helmi", ""]]}, {"id": "2106.09489", "submitter": "Bruno Andreotti", "authors": "Florian Poydenot, Ismael Abdourahamane, Elsa Caplain, Samuel Der,\n  Jacques Haiech, Antoine Jallon, Ines Khoutami, Amir Loucif, Emil Marinov,\n  Bruno Andreotti", "title": "Risk assessment for long and short range airborne transmission of\n  SARS-CoV-2, indoors and outdoors, using carbon dioxide measurements", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantitative analysis of viral transmission risk in public places such as\nschools, offices, university lecture halls, hospitals, museums, theaters or\nshopping malls makes it possible to identify the effective levers for a\nproactive policy of health security and to evaluate the reduction in\ntransmission thus obtained. The contribution to the epidemic propagation of\nSARS-CoV-2 in such public spaces can be reduced in the short term to a level\ncompatible with an epidemic decline, i.e. with an overall epidemic reproduction\nrate below one. Here, we revisit the quantitative assessment of indoor and\noutdoor transmission risk. We show that the long range aerosol transmission is\ncontrolled by the flow rate of fresh air and by the mask filtering quality, and\nis quantitatively related to the CO2 concentration, regardless the room volume\nand the number of people. The short range airborne transmission is investigated\nexperimentally using dedicated dispersion experiments performed in two French\nshopping malls. Exhaled aerosols are dispersed by turbulent draughts in a cone,\nleading to a concentration inversely proportional to the squared distance and\nto the flow velocity. We show that the average infection dose, called the viral\nquantum, can be consistently determined from epidemiological and biological\nexperimental data.\n  Practical implications. The results provide a rational design of sanitary\npolicies to prevent the dominant routes of viral transmission by reinforced\nventilation, air purification, mechanical dispersion by fans and incentives for\ncorrect wearing of quality masks (surgical mask, possibly covered by a fabric\nmask, or non-medical FFP2 masks). Combined, such measures significantly reduce\nthe airborne transmission risk of SARS-CoV-2, with a quantitative assessment.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:50:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Poydenot", "Florian", ""], ["Abdourahamane", "Ismael", ""], ["Caplain", "Elsa", ""], ["Der", "Samuel", ""], ["Haiech", "Jacques", ""], ["Jallon", "Antoine", ""], ["Khoutami", "Ines", ""], ["Loucif", "Amir", ""], ["Marinov", "Emil", ""], ["Andreotti", "Bruno", ""]]}, {"id": "2106.09512", "submitter": "Sebastian Lerch", "authors": "Benedikt Schulz and Sebastian Lerch", "title": "Machine learning methods for postprocessing ensemble forecasts of wind\n  gusts: A systematic comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Postprocessing ensemble weather predictions to correct systematic errors has\nbecome a standard practice in research and operations. However, only few recent\nstudies have focused on ensemble postprocessing of wind gust forecasts, despite\nits importance for severe weather warnings. Here, we provide a comprehensive\nreview and systematic comparison of eight statistical and machine learning\nmethods for probabilistic wind gust forecasting via ensemble postprocessing,\nthat can be divided in three groups: State of the art postprocessing techniques\nfrom statistics (ensemble model output statistics (EMOS), member-by-member\npostprocessing, isotonic distributional regression), established machine\nlearning methods (gradient-boosting extended EMOS, quantile regression forests)\nand neural network-based approaches (distributional regression network,\nBernstein quantile network, histogram estimation network). The methods are\nsystematically compared using six years of data from a high-resolution,\nconvection-permitting ensemble prediction system that was run operationally at\nthe German weather service, and hourly observations at 175 surface weather\nstations in Germany. While all postprocessing methods yield calibrated\nforecasts and are able to correct the systematic errors of the raw ensemble\npredictions, incorporating information from additional meteorological predictor\nvariables beyond wind gusts leads to significant improvements in forecast\nskill. In particular, we propose a flexible framework of locally adaptive\nneural networks with different probabilistic forecast types as output, which\nnot only significantly outperform all benchmark postprocessing methods but also\nlearn physically consistent relations associated with the diurnal cycle,\nespecially the evening transition of the planetary boundary layer.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:03:29 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Schulz", "Benedikt", ""], ["Lerch", "Sebastian", ""]]}, {"id": "2106.09541", "submitter": "Soumaya Elkantassi", "authors": "Soumaya Elkantassi and Anthony Davison", "title": "Satellite conjunction assessment: Statistical space oddity?", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite conjunctions involving \"near misses\" of space objects are becoming\nincreasingly likely. One approach to risk analysis for them involves the\ncomputation of the collision probability, but this has been regarded as having\nsome counter-intuitive properties, and its meaning as a probability is unclear.\nWe formulate an alternative approach based on a simple statistical model that\nallows highly accurate inference on the miss distance between the two objects,\nshow that this provides a close approximation to a default Bayesian approach,\nillustrate the method with a case study, and give Monte Carlo results to show\nits excellent performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:25:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Elkantassi", "Soumaya", ""], ["Davison", "Anthony", ""]]}, {"id": "2106.09579", "submitter": "Richard Glennie", "authors": "Richard Glennie, Len Thomas, Todd Speakman, Lance Garrison, Ryan\n  Takeshita, Lori Schwacke", "title": "Estimating spatially-varying density and time-varying demographics with\n  open population spatial capture-recapture: a photo-ID case study on\n  bottlenose dolphins in Barataria Bay, Louisana, USA", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  1. From long-term, spatial capture-recapture (SCR) surveys we infer a\npopulation's dynamics over time and distribution over space. It is becoming\nmore computationally feasible to fit these open population SCR (openSCR) models\nto large datasets and include complex model components, e.g., spatially-varying\ndensity surfaces and time-varying population dynamics. Yet, there is limited\nknowledge on how these methods perform.\n  2. As a case study, we analyze a multi-year, photo-ID survey on bottlenose\ndolphins (Tursiops truncatus) in Barataria Bay, Louisana, USA. This population\nhas been monitored due to the impacts of the nearby Deepwater Horizon oil spill\nin 2010. Over 2000 capture histories have been collected between 2010 and 2019.\nOur aim is to identify the challenges in applying openSCR methods to real data\nand to describe a workflow for other analysts using these methods.\n  3. We show that inference on survival, recruitment, and density over time\nsince the oil spill provides insight into increased mortality after the spill,\npossible redistribution of the population thereafter, and continued population\ndecline. Issues in the application are highlighted throughout: possible model\nmisspecification, sensitivity of parameters to model selection, and difficulty\nin interpreting results due to model assumptions and irregular surveying in\ntime and space. For each issue, we present practical solutions including\nassessing goodness-of-fit, model-averaging, and clarifying the difference\nbetween quantitative results and their qualitative interpretation.\n  4. Overall, this case study serves as a practical template other analysts can\nfollow and extend; it also highlights the need for further research on the\napplicability of these methods as we demand richer inference from them.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:02:41 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Glennie", "Richard", ""], ["Thomas", "Len", ""], ["Speakman", "Todd", ""], ["Garrison", "Lance", ""], ["Takeshita", "Ryan", ""], ["Schwacke", "Lori", ""]]}, {"id": "2106.09586", "submitter": "Susan Martonosi", "authors": "Banafsheh Behzad, Bhavana Bheem, Daniela Elizondo, Deyana Marsh, Susan\n  Martonosi", "title": "Prevalence and Propagation of Fake News", "comments": "45 pages, 22 figures. Submitted for peer review on 7 May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, scholars have raised concerns on the effects that unreliable\nnews, or \"fake news,\" has on our political sphere, and our democracy as a\nwhole. For example, the propagation of fake news on social media is widely\nbelieved to have influenced the outcome of national elections, including the\n2016 U.S. Presidential Election, and the 2020 COVID-19 pandemic. What drives\nthe propagation of fake news on an individual level, and which interventions\ncould effectively reduce the propagation rate? Our model disentangles bias from\ntruthfulness of an article and examines the relationship between these two\nparameters and a reader's own beliefs. Using the model, we create policy\nrecommendations for both social media platforms and individual social media\nusers to reduce the spread of untruthful or highly biased news. We recommend\nthat platforms sponsor unbiased truthful news, focus fact-checking efforts on\nmild to moderately biased news, recommend friend suggestions across the\npolitical spectrum, and provide users with reports about the political\nalignment of their feed. We recommend that individual social media users fact\ncheck news that strongly aligns with their political bias and read articles of\nopposing political bias.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:13:08 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Behzad", "Banafsheh", ""], ["Bheem", "Bhavana", ""], ["Elizondo", "Daniela", ""], ["Marsh", "Deyana", ""], ["Martonosi", "Susan", ""]]}, {"id": "2106.09597", "submitter": "David John", "authors": "David N. John, Livia Stohrer, Claudia Schillings, Michael Schick,\n  Vincent Heuveline", "title": "Hierarchical surrogate-based Approximate Bayesian Computation for an\n  electric motor test bench", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Inferring parameter distributions of complex industrial systems from noisy\ntime series data requires methods to deal with the uncertainty of the\nunderlying data and the used simulation model. Bayesian inference is well\nsuited for these uncertain inverse problems. Standard methods used to identify\nuncertain parameters are Markov Chain Monte Carlo (MCMC) methods with explicit\nevaluation of a likelihood function. However, if the likelihood is very\ncomplex, such that its evaluation is computationally expensive, or even unknown\nin its explicit form, Approximate Bayesian Computation (ABC) methods provide a\npromising alternative. In this work both methods are first applied to\nartificially generated data and second on a real world problem, by using data\nof an electric motor test bench. We show that both methods are able to infer\nthe distribution of varying parameters with a Bayesian hierarchical approach.\nBut the proposed ABC method is computationally much more efficient in order to\nachieve results with similar accuracy. We suggest to use summary statistics in\norder to reduce the dimension of the data which significantly increases the\nefficiency of the algorithm. Further the simulation model is replaced by a\nPolynomial Chaos Expansion (PCE) surrogate to speed up model evaluations. We\nproof consistency for the proposed surrogate-based ABC method with summary\nstatistics under mild conditions on the (approximated) forward model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:29:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["John", "David N.", ""], ["Stohrer", "Livia", ""], ["Schillings", "Claudia", ""], ["Schick", "Michael", ""], ["Heuveline", "Vincent", ""]]}, {"id": "2106.09632", "submitter": "Xu Han", "authors": "Xu Han, Sanat Sarkar, Shiyu Zhang", "title": "Large-Scale Multiple Testing for Matrix-Valued Data under Double\n  Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-dimensional inference based on matrix-valued data has drawn increasing\nattention in modern statistical research, yet not much progress has been made\nin large-scale multiple testing specifically designed for analysing such data\nsets. Motivated by this, we consider in this article an electroencephalography\n(EEG) experiment that produces matrix-valued data and presents a scope of\ndeveloping novel matrix-valued data based multiple testing methods controlling\nfalse discoveries for hypotheses that are of importance in such an experiment.\nThe row-column cross-dependency of observations appearing in a matrix form,\nreferred to as double-dependency, is one of the main challenges in the\ndevelopment of such methods. We address it by assuming matrix normal\ndistribution for the observations at each of the independent matrix\ndata-points. This allows us to fully capture the underlying double-dependency\ninformed through the row- and column-covariance matrices and develop methods\nthat are potentially more powerful than the corresponding one (e.g., Fan and\nHan (2017)) obtained by vectorizing each data point and thus ignoring the\ndouble-dependency. We propose two methods to approximate the false discovery\nproportion with statistical accuracy. While one of these methods is a general\napproach under double-dependency, the other one provides more computational\nefficiency for higher dimensionality. Extensive numerical studies illustrate\nthe superior performance of the proposed methods over the principal factor\napproximation method of Fan and Han (2017). The proposed methods have been\nfurther applied to the aforementioned EEG data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:18:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Han", "Xu", ""], ["Sarkar", "Sanat", ""], ["Zhang", "Shiyu", ""]]}, {"id": "2106.09702", "submitter": "Tyler McCormick", "authors": "Shane Lubold and Bolun Liu and Tyler H. McCormick", "title": "Spectral goodness-of-fit tests for complete and partial network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networks describe the, often complex, relationships between individual\nactors. In this work, we address the question of how to determine whether a\nparametric model, such as a stochastic block model or latent space model, fits\na dataset well and will extrapolate to similar data. We use recent results in\nrandom matrix theory to derive a general goodness-of-fit test for dyadic data.\nWe show that our method, when applied to a specific model of interest, provides\nan straightforward, computationally fast way of selecting parameters in a\nnumber of commonly used network models. For example, we show how to select the\ndimension of the latent space in latent space models. Unlike other network\ngoodness-of-fit methods, our general approach does not require simulating from\na candidate parametric model, which can be cumbersome with large graphs, and\neliminates the need to choose a particular set of statistics on the graph for\ncomparison. It also allows us to perform goodness-of-fit tests on partial\nnetwork data, such as Aggregated Relational Data. We show with simulations that\nour method performs well in many situations of interest. We analyze several\nempirically relevant networks and show that our method leads to improved\ncommunity detection algorithms. R code to implement our method is available on\nGithub.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:56:30 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lubold", "Shane", ""], ["Liu", "Bolun", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "2106.09905", "submitter": "Michael Biehler", "authors": "Michael Biehler, Zhen Zhong, Jianjun Shi", "title": "SAGE: Stealthy Attack GEneration for Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPS) have been increasingly attacked by hackers.\nRecent studies have shown that CPS are especially vulnerable to insider\nattacks, in which case the attacker has full knowledge of the systems\nconfiguration. To better prevent such types of attacks, we need to understand\nhow insider attacks are generated. Typically, there are three critical aspects\nfor a successful insider attack: (i) Maximize damage, (ii) Avoid detection and\n(iii) Minimize the attack cost. In this paper we propose a Stealthy Attack\nGEneration (SAGE) framework by formulizing a novel optimization problem\nconsidering these three objectives and the physical constraints of the CPS. By\nadding small worst-case perturbations to the system, the SAGE attack can\ngenerate significant damage, while remaining undetected by the systems\nmonitoring algorithms. The proposed methodology is evaluated on several anomaly\ndetection algorithms. The results show that SAGE attacks can cause severe\ndamage while staying undetected and keeping the cost of an attack low. Our\nmethod can be accessed in the supplementary material of this paper to aid\nresearcher and practitioners in the design and development of resilient CPS and\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 04:05:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Biehler", "Michael", ""], ["Zhong", "Zhen", ""], ["Shi", "Jianjun", ""]]}, {"id": "2106.09985", "submitter": "Yoann Altmann", "authors": "Zeng Li and Yoann Altmann and Jie Chen and Stephen Mclaughlin and\n  Susanto Rahardja", "title": "Sparse Linear Spectral Unmixing of Hyperspectral images using\n  Expectation-Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a novel Bayesian approach for hyperspectral image\nunmixing. The observed pixels are modeled by a linear combination of material\nsignatures weighted by their corresponding abundances. A spike-and-slab\nabundance prior is adopted to promote sparse mixtures and an Ising prior model\nis used to capture spatial correlation of the mixture support across pixels. We\napproximate the posterior distribution of the abundances using the\nexpectation-propagation (EP) method. We show that it can significantly reduce\nthe computational complexity of the unmixing stage and meanwhile provide\nuncertainty measures, compared to expensive Monte Carlo strategies\ntraditionally considered for uncertainty quantification. Moreover, many\nvariational parameters within each EP factor can be updated in a parallel\nmanner, which enables mapping of efficient algorithmic architectures based on\ngraphics processing units (GPU). Under the same approximate Bayesian framework,\nwe then extend the proposed algorithm to semi-supervised unmixing, whereby the\nabundances are viewed as latent variables and the expectation-maximization (EM)\nalgorithm is used to refine the endmember matrix. Experimental results on\nsynthetic data and real hyperspectral data illustrate the benefits of the\nproposed framework over state-of-art linear unmixing methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:14:41 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Zeng", ""], ["Altmann", "Yoann", ""], ["Chen", "Jie", ""], ["Mclaughlin", "Stephen", ""], ["Rahardja", "Susanto", ""]]}, {"id": "2106.10057", "submitter": "Alexander Wolfgang Jung", "authors": "Alexander W. Jung and Moritz Gerstung", "title": "Bayesian Cox Regression for Population-scale Inference in Electronic\n  Health Records", "comments": "35 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox model is an indispensable tool for time-to-event analysis,\nparticularly in biomedical research. However, medicine is undergoing a profound\ntransformation, generating data at an unprecedented scale, which opens new\nfrontiers to study and understand diseases. With the wealth of data collected,\nnew challenges for statistical inference arise, as datasets are often high\ndimensional, exhibit an increasing number of measurements at irregularly spaced\ntime points, and are simply too large to fit in memory. Many current\nimplementations for time-to-event analysis are ill-suited for these problems as\ninference is computationally demanding and requires access to the full data at\nonce. Here we propose a Bayesian version for the counting process\nrepresentation of Cox's partial likelihood for efficient inference on\nlarge-scale datasets with millions of data points and thousands of\ntime-dependent covariates. Through the combination of stochastic variational\ninference and a reweighting of the log-likelihood, we obtain an approximation\nfor the posterior distribution that factorizes over subsamples of the data,\nenabling the analysis in big data settings. Crucially, the method produces\nviable uncertainty estimates for large-scale and high-dimensional datasets. We\nshow the utility of our method through a simulation study and an application to\nmyocardial infarction in the UK Biobank.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:13:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jung", "Alexander W.", ""], ["Gerstung", "Moritz", ""]]}, {"id": "2106.10203", "submitter": "Ekaterina Krymova", "authors": "Ekaterina Krymova, Benjam\\'in B\\'ejar, Dorina Thanou, Tao Sun, Elisa\n  Manetti, Gavin Lee, Kristen Namigai, Christine Choirat, Antoine Flahault,\n  Guillaume Obozinski", "title": "Trend estimation and short-term forecasting of COVID-19 cases and deaths\n  worldwide", "comments": "15 pages including 5 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the beginning of the COVID-19 pandemic, many dashboards have emerged as\nuseful tools to monitor the evolution of the pandemic, inform the public, and\nassist governments in decision making. Our goal is to develop a globally\napplicable method, integrated in a twice daily updated dashboard that provides\nan estimate of the trend in the evolution of the number of cases and deaths\nfrom reported data of more than 200 countries and territories, as well as a\nseven-day forecast. One of the significant difficulties to manage a quickly\npropagating epidemic is that the details of the dynamic needed to forecast its\nevolution are obscured by the delays in the identification of cases and deaths\nand by irregular reporting. Our forecasting methodology substantially relies on\nestimating the underlying trend in the observed time series using robust\nseasonal trend decomposition techniques. This allows us to obtain forecasts\nwith simple, yet effective extrapolation methods in linear or log scale. We\npresent the results of an assessment of our forecasting methodology and discuss\nits application to the production of global and regional risk maps.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:13:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Krymova", "Ekaterina", ""], ["B\u00e9jar", "Benjam\u00edn", ""], ["Thanou", "Dorina", ""], ["Sun", "Tao", ""], ["Manetti", "Elisa", ""], ["Lee", "Gavin", ""], ["Namigai", "Kristen", ""], ["Choirat", "Christine", ""], ["Flahault", "Antoine", ""], ["Obozinski", "Guillaume", ""]]}, {"id": "2106.10236", "submitter": "Anand Deo", "authors": "Anand Deo, Karthyek Murthy", "title": "Efficient Black-Box Importance Sampling for VaR and CVaR Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG math.PR stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers Importance Sampling (IS) for the estimation of tail\nrisks of a loss defined in terms of a sophisticated object such as a machine\nlearning feature map or a mixed integer linear optimisation formulation.\nAssuming only black-box access to the loss and the distribution of the\nunderlying random vector, the paper presents an efficient IS algorithm for\nestimating the Value at Risk and Conditional Value at Risk. The key challenge\nin any IS procedure, namely, identifying an appropriate change-of-measure, is\nautomated with a self-structuring IS transformation that learns and replicates\nthe concentration properties of the conditional excess from less rare samples.\nThe resulting estimators enjoy asymptotically optimal variance reduction when\nviewed in the logarithmic scale. Simulation experiments highlight the efficacy\nand practicality of the proposed scheme\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:29:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Deo", "Anand", ""], ["Murthy", "Karthyek", ""]]}, {"id": "2106.10338", "submitter": "Thomas Varley", "authors": "Thomas F. Varley", "title": "Intersectional synergies: untangling irreducible effects of intersecting\n  identities via information decomposition", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The idea of intersectionality has become a frequent topic of discussion both\nin academic sociology, as well as among popular movements for social justice\nsuch as Black Lives Matter, intersectional feminism, and LGBT rights.\nIntersectionality proposes that an individual's experience of society has\naspects that are irreducible to the sum of one's various identities considered\nindividually, but are \"greater than the sum of their parts.\" In this work, we\nshow that the effects of intersectional identities can be statistically\nobserved in empirical data using information theory. We show that, when\nconsidering the predictive relationship between various identities categories\nsuch as race, sex, and income (as a proxy for class) on outcomes such as health\nand wellness, robust statistical synergies appear. These synergies show that\nthere are joint-effects of identities on outcomes that are irreducible to any\nidentity considered individually and only appear when specific categories are\nconsidered together (for example, there is a large, synergistic effect of race\nand sex considered jointly on income irreducible to either race or sex). We\nthen show using synthetic data that the current gold-standard method of\nassessing intersectionalities in data (linear regression with multiplicative\ninteraction coefficients) fails to disambiguate between truly synergistic,\ngreater-than-the-sum-of-their-parts interactions, and redundant interactions.\nWe explore the significance of these two distinct types of interactions in the\ncontext of making inferences about intersectional relationships in data and the\nimportance of being able to reliably differentiate the two. Finally, we\nconclude that information theory, as a model-free framework sensitive to\nnonlinearities and synergies in data, is a natural method by which to explore\nthe space of higher-order social dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:05:16 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:09:21 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Varley", "Thomas F.", ""]]}, {"id": "2106.10364", "submitter": "Chelsea Krantsevich", "authors": "Chelsea Krantsevich, P. Richard Hahn, Yi Zheng and Charles Katz", "title": "Bayesian decision theory for tree-based adaptive screening tests with an\n  application to youth delinquency", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Crime prevention strategies based on early intervention depend on accurate\nrisk assessment instruments for identifying high risk youth. It is important in\nthis context that the instruments be convenient to administer, which means, in\nparticular, that they must be reasonably brief; adaptive screening tests are\nuseful for this purpose. Although item response theory (IRT) bears a long and\nrich history in producing reliable adaptive tests, adaptive tests constructed\nusing classification and regression trees are becoming a popular alternative to\nthe traditional IRT approach for item selection. On the upside, unlike IRT,\ntree-based questionnaires require no real-time parameter estimation during\nadministration. On the downside, while item response theory provides robust\ncriteria for terminating the exam, the stopping criterion for a tree-based\nadaptive test (the maximum tree depth) is unclear. We present a Bayesian\ndecision theory approach for characterizing the trade-offs of administering\ntree-based questionnaires of different lengths. This formalism involves\nspecifying 1) a utility function measuring the goodness of the assessment; 2) a\ntarget population over which this utility should be maximized; 3) an action\nspace comprised of different-length assessments, populated via a tree-fitting\nalgorithm. Using this framework, we provide uncertainty estimates for the\ntrade-offs of shortening the exam, allowing practitioners to determine an\noptimal exam length in a principled way. The method is demonstrated through an\napplication to youth delinquency risk assessment in Honduras.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:40:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Krantsevich", "Chelsea", ""], ["Hahn", "P. Richard", ""], ["Zheng", "Yi", ""], ["Katz", "Charles", ""]]}, {"id": "2106.10387", "submitter": "Ning Ning", "authors": "Ning Ning and Edward L. Ionides", "title": "Systemic Infinitesimal Over-dispersion on General Stochastic Graphical\n  Models", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models of interacting populations have crucial roles in scientific\nfields such as epidemiology and ecology, yet the standard approach to extending\nan ordinary differential equation model to a Markov chain does not have\nsufficient flexibility in the mean-variance relationship to match data (e.g.\n\\cite{bjornstad2001noisy}). A previous theory on time-homogeneous dynamics over\na single arrow by \\cite{breto2011compound} showed how gamma white noise could\nbe used to construct certain over-dispersed Markov chains, leading to widely\nused models (e.g. \\cite{breto2009time,he2010plug}). In this paper, we define\nsystemic infinitesimal over-dispersion, developing theory and methodology for\ngeneral time-inhomogeneous stochastic graphical models. Our approach, based on\nDirichlet noise, leads to a new class of Markov models over general direct\ngraphs. It is compatible with modern likelihood-based inference methodologies\n(e.g. \\cite{ionides2006inference,ionides2015inference,king2008inapparent}) and\ntherefore we can assess how well the new models fit data. We demonstrate our\nmethodology on a widely analyzed measles dataset, adding Dirichlet noise to a\nclassical SEIR (Susceptible-Exposed-Infected-Recovered) model. We find that the\nproposed methodology has higher log-likelihood than the gamma white noise\napproach, and the resulting parameter estimations provide new insights into the\nover-dispersion of this biological system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 23:29:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ning", "Ning", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2106.10571", "submitter": "Harrison Quick", "authors": "Guangzi Song, Loni Philip Tabb, and Harrison Quick", "title": "Geographic and Racial Disparities in the Incidence of Low Birthweight in\n  Pennsylvania", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Babies born with low and very low birthweights -- i.e., birthweights below\n2,500 and 1,500 grams, respectively -- have an increased risk of complications\ncompared to other babies, and the proportion of babies with a low birthweight\nis a common metric used when evaluating public health in a population. While\nmany factors increase the risk of a baby having a low birthweight, many can be\nlinked to the mother's socioeconomic status, which in turn contributes to large\nracial disparities in the incidence of low weight births. Here, we employ\nBayesian statistical models to analyze the proportion of babies with low\nbirthweight in Pennsylvania counties by race/ethnicity. Due to the small number\nof births -- and low weight births -- in many Pennsylvania counties when\nstratified by race/ethnicity, our methods must walk a fine line. On one hand,\nleveraging spatial structure can help improve the precision of our estimates.\nOn the other hand, we must be cautious to avoid letting the model overwhelm the\ninformation in the data and produce spurious conclusions. As such, we first\ndevelop a framework by which we can measure (and control) the informativeness\nof our spatial model. After demonstrating the properties of our framework via\nsimulation, we analyze the low birthweight data from Pennsylvania and examine\nthe extent to which the commonly used conditional autoregressive model can lead\nto oversmoothing. We then reanalyze the data using our proposed framework and\nhighlight its ability to detect (or not detect) evidence of racial disparities\nin the incidence of low birthweight.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 20:59:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Song", "Guangzi", ""], ["Tabb", "Loni Philip", ""], ["Quick", "Harrison", ""]]}, {"id": "2106.10610", "submitter": "Anna Trella", "authors": "Anna L. Trella, Peniel N. Argaw, Michelle M. Li, James A. Hay", "title": "Discrepancies in Epidemiological Modeling of Aggregated Heterogeneous\n  Data", "comments": "Accepted to IJCAI 2021 Workshop on Artificial Intelligence for Social\n  Good", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within epidemiological modeling, the majority of analyses assume a single\nepidemic process for generating ground-truth data. However, this assumed data\ngeneration process can be unrealistic, since data sources for epidemics are\noften aggregated across geographic regions and communities. As a result,\nstate-of-the-art models for estimating epidemiological parameters,\ne.g.~transmission rates, can be inappropriate when faced with complex systems.\nOur work empirically demonstrates some limitations of applying epidemiological\nmodels to aggregated datasets. We generate three complex outbreak scenarios by\ncombining incidence curves from multiple epidemics that are independently\nsimulated via SEIR models with different sets of parameters. Using these\nscenarios, we assess the robustness of a state-of-the-art Bayesian inference\nmethod that estimates the epidemic trajectory from viral load surveillance\ndata. We evaluate two data-generating models within this Bayesian inference\nframework: a simple exponential growth model and a highly flexible Gaussian\nprocess prior model. Our results show that both models generate accurate\ntransmission rate estimates for the combined incidence curve at the cost of\ngenerating biased estimates for each underlying epidemic, reflecting highly\nheterogeneous underlying population dynamics. The exponential growth model,\nwhile interpretable, is unable to capture the complexity of the underlying\nepidemics. With sufficient surveillance data, the Gaussian process prior model\ncaptures the shape of complex trajectories, but is imprecise for periods of low\ndata coverage. Thus, our results highlight the potential pitfalls of neglecting\ncomplexity and heterogeneity in the data generation process, which can mask\nunderlying location- and population-specific epidemic dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 03:41:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Trella", "Anna L.", ""], ["Argaw", "Peniel N.", ""], ["Li", "Michelle M.", ""], ["Hay", "James A.", ""]]}, {"id": "2106.10621", "submitter": "Dong Li", "authors": "Dong Li and Ruoming Jin and Jing Gao and Zhi Liu", "title": "On Sampling Top-K Recommendation Evaluation", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403262", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Rendle has warned that the use of sampling-based top-$k$ metrics\nmight not suffice. This throws a number of recent studies on deep\nlearning-based recommendation algorithms, and classic non-deep-learning\nalgorithms using such a metric, into jeopardy. In this work, we thoroughly\ninvestigate the relationship between the sampling and global top-$K$ Hit-Ratio\n(HR, or Recall), originally proposed by Koren[2] and extensively used by\nothers. By formulating the problem of aligning sampling top-$k$ ($SHR@k$) and\nglobal top-$K$ ($HR@K$) Hit-Ratios through a mapping function $f$, so that\n$SHR@k\\approx HR@f(k)$, we demonstrate both theoretically and experimentally\nthat the sampling top-$k$ Hit-Ratio provides an accurate approximation of its\nglobal (exact) counterpart, and can consistently predict the correct winners\n(the same as indicate by their corresponding global Hit-Ratios).\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 04:48:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Dong", ""], ["Jin", "Ruoming", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""]]}, {"id": "2106.10624", "submitter": "Zheng Chen", "authors": "Jingjing Lyu, Yawen Hou, Zheng Chen", "title": "Combined tests based on restricted mean time lost for competing risks\n  data", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "SBR-20-016", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks data are common in medical studies, and the sub-distribution\nhazard (SDH) ratio is considered an appropriate measure. However, because the\nlimitations of hazard itself are not easy to interpret clinically and because\nthe SDH ratio is valid only under the proportional SDH assumption, this article\nintroduced an alternative index under competing risks, named restricted mean\ntime lost (RMTL). Several test procedures were also constructed based on RMTL.\nFirst, we introduced the definition and estimation of RMTL based on\nAalen-Johansen cumulative incidence functions. Then, we considered several\ncombined tests based on the SDH and the RMTL difference (RMTLd). The\nstatistical properties of the methods are evaluated using simulations and are\napplied to two examples. The type I errors of combined tests are close to the\nnominal level. All combined tests show acceptable power in all situations. In\nconclusion, RMTL can meaningfully summarize treatment effects for clinical\ndecision making, and three combined tests have robust power under various\nconditions, which can be considered for statistical inference in real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:06:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lyu", "Jingjing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.10625", "submitter": "Zheng Chen", "authors": "Zijing Yang, Hongji Wu, Yawen Hou, Hao Yuan, Zheng Chen", "title": "Dynamic prediction and analysis based on restricted mean survival time\n  in survival analysis with nonproportional hazards", "comments": "25 pages, 5 figures", "journal-ref": "Computer Methods and Programs in Biomedicine. 2021, 207: 106155", "doi": "10.1016/j.cmpb.2021.106155", "report-no": "207:106155", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of clinical diagnosis and treatment, the restricted mean\nsurvival time (RMST), which reflects the life expectancy of patients up to a\nspecified time, can be used as an appropriate outcome measure. However, the\nRMST only calculates the mean survival time of patients within a period of time\nafter the start of follow-up and may not accurately portray the change in a\npatient's life expectancy over time. The life expectancy can be adjusted for\nthe time the patient has already survived and defined as the conditional\nrestricted mean survival time (cRMST). A dynamic RMST model based on the cRMST\ncan be established by incorporating time-dependent covariates and covariates\nwith time-varying effects. We analysed data from a study of primary biliary\ncirrhosis (PBC) to illustrate the use of the dynamic RMST model. The predictive\nperformance was evaluated using the C-index and the prediction error. The\nproposed dynamic RMST model, which can explore the dynamic effects of\nprognostic factors on survival time, has better predictive performance than the\nRMST model. Three PBC patient examples were used to illustrate how the\npredicted cRMST changed at different prediction times during follow-up. The use\nof the dynamic RMST model based on the cRMST allows for optimization of\nevidence-based decision-making by updating personalized dynamic life expectancy\nfor patients.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:07:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Zijing", ""], ["Wu", "Hongji", ""], ["Hou", "Yawen", ""], ["Yuan", "Hao", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.10694", "submitter": "Xiaolei Chu", "authors": "Xiaolei Chu, Hung Nguyen Sinh, Wei Cui, Lin Zhao, Yaojun Ge", "title": "Life-cycle assessment for flutter probability of a long-span suspension\n  bridge based on field monitoring data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Assessment of structural safety status is of paramount importance for\nexisting bridges, where accurate evaluation of flutter probability is essential\nfor long-span bridges. In current engineering practice, at the design stage,\nflutter critical wind speed is usually estimated by the wind tunnel test, which\nis sensitive to modal frequencies and damping ratios. After construction,\nstructural properties of existing structures will change with time due to\nvarious factors, such as structural deteriorations and periodic environments.\nThe structural dynamic properties, such as modal frequencies and damping\nratios, cannot be considered as the same values as the initial ones, and the\ndeteriorations should be included when estimating the life-cycle flutter\nprobability. This paper proposes an evaluation framework to assess the\nlife-cycle flutter probability of long-span bridges considering the\ndeteriorations of structural properties, based on field monitoring data. The\nBayesian approach is employed for modal identification of a suspension bridge\nwith the main span of 1650 m, and the field monitoring data during 2010-2015 is\nanalyzed to determine the deterioration functions of modal frequencies and\ndamping ratios, as well as their inter-seasonal fluctuations. According to the\nhistorical trend, the long-term structural properties can be predicted, and the\nprobability distributions of flutter critical wind speed for each year in the\nlong term are calculated. Consequently, the life-cycle flutter probability is\nestimated, based on the predicted modal frequencies and damping ratios.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 13:35:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chu", "Xiaolei", ""], ["Sinh", "Hung Nguyen", ""], ["Cui", "Wei", ""], ["Zhao", "Lin", ""], ["Ge", "Yaojun", ""]]}, {"id": "2106.10721", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Tae Yoon Lee, Paul Gustafson", "title": "Uncertainty and Value of Perfect Information in Risk Prediction Modeling", "comments": "20 pages, 1 table, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Predicted probabilities from a risk prediction model are\ninevitably uncertain. This uncertainty has mostly been studied from a\nstatistical perspective. We apply Value of Information methodology to evaluate\nthe decision-theoretic implications of prediction uncertainty.\n  Methods: Adopting a Bayesian perspective, we extend the definition of the\nExpected Value of Perfect Information (EVPI) from decision analysis to net\nbenefit calculations in risk prediction. EVPI is the expected gain in net\nbenefit by using the correct predictions as opposed to predictions from a\nproposed model. We suggest bootstrap methods for sampling from the posterior\ndistribution of predictions for EVPI calculation using Monte Carlo simulations.\nIn a case study, we used subsets of data of various sizes from a clinical trial\nfor predicting mortality after myocardial infarction to show how EVPI can be\ninterpreted and how it changes with sample size.\n  Results: With a sample size of 1,000, EVPI was 0 at threshold values larger\nthan 0.6, indicating there is no point in procuring more development data for\nsuch thresholds. At thresholds of 0.4-0.6, the proposed model was not net\nbeneficial, but EVPI was positive, indicating that obtaining more development\ndata might be justified. Across all thresholds, the gain in net benefit by\nusing the correct model was 24% higher than the gain by using the proposed\nmodel. EVPI declined with larger samples and was generally low with sample\nsizes of 4,000 or greater. We summarize an algorithm for incorporating EVPI\ncalculations into the commonly used bootstrap method for optimism correction.\n  Conclusion: Value of Information methods can be applied to explore\ndecision-theoretic consequences of uncertainty in risk prediction, and can\ncomplement inferential methods when developing or validating risk prediction\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 16:19:44 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 01:07:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Lee", "Tae Yoon", ""], ["Gustafson", "Paul", ""]]}, {"id": "2106.10758", "submitter": "Jonathan Dong", "authors": "Jonathan Dong, Dante Maestre, Clara Conrad-Billroth, and Thomas\n  Juffmann", "title": "Fundamental bounds on the precision of iSCAT, COBRI and dark-field\n  microscopy for 3D localization and mass photometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interferometric imaging is an emerging technique for particle tracking and\nmass photometry. Mass or position are estimated from weak signals, coherently\nscattered from nanoparticles or single molecules, and interfered with a\nco-propagating reference. In this work, we perform a statistical analysis and\nderive lower bounds on the measurement precision of the parameters of interest\nfrom shot-noise limited images. This is done by computing the classical\nCram\\'er-Rao bound for localization and mass estimation, using a precise\nvectorial model of interferometric imaging techniques. We then derive\nfundamental bounds valid for any imaging system, based on the quantum\nCram\\'er-Rao formalism. This approach enables a rigorous and quantitative\ncomparison of common techniques such as interferometric scattering microscopy\n(iSCAT), Coherent Brightfield microscopy (COBRI), and dark-field microscopy. In\nparticular, we demonstrate that the light collection geometry in iSCAT greatly\nincreases the axial position sensitivity, and that the Quantum Cram\\'er-Rao\nbound for mass estimation yields a minimum relative estimation error of\n$\\sigma_m/m=1/(2\\sqrt{N})$, where $N$ is the number of collected scattered\nphotons.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 21:45:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dong", "Jonathan", ""], ["Maestre", "Dante", ""], ["Conrad-Billroth", "Clara", ""], ["Juffmann", "Thomas", ""]]}, {"id": "2106.10941", "submitter": "Karthik Bharath", "authors": "Shariq Mohammed, Sebastian Kurtek, Karthik Bharath, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "Tumor Radiogenomics with Bayesian Layered Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a statistical framework to integrate radiological magnetic\nresonance imaging (MRI) and genomic data to identify the underlying\nradiogenomic associations in lower grade gliomas (LGG). We devise a novel\nimaging phenotype by dividing the tumor region into concentric spherical layers\nthat mimics the tumor evolution process. MRI data within each layer is\nrepresented by voxel--intensity-based probability density functions which\ncapture the complete information about tumor heterogeneity. Under a\nRiemannian-geometric framework these densities are mapped to a vector of\nprincipal component scores which act as imaging phenotypes. Subsequently, we\nbuild Bayesian variable selection models for each layer with the imaging\nphenotypes as the response and the genomic markers as predictors. Our novel\nhierarchical prior formulation incorporates the interior-to-exterior structure\nof the layers, and the correlation between the genomic markers. We employ a\ncomputationally-efficient Expectation--Maximization-based strategy for\nestimation. Simulation studies demonstrate the superior performance of our\napproach compared to other approaches. With a focus on the cancer driver genes\nin LGG, we discuss some biologically relevant findings. Genes implicated with\nsurvival and oncogenesis are identified as being associated with the spherical\nlayers, which could potentially serve as early-stage diagnostic markers for\ndisease monitoring, prior to routine invasive approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:24:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mohammed", "Shariq", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2106.11234", "submitter": "Elis Ailer", "authors": "Elisabeth Ailer, Christian L. M\\\"uller, Niki Kilbertus", "title": "A causal view on compositional data", "comments": "Code available on https://github.com/EAiler/comp-iv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific datasets are compositional in nature. Important examples\ninclude species abundances in ecology, rock compositions in geology, topic\ncompositions in large-scale text corpora, and sequencing count data in\nmolecular biology. Here, we provide a causal view on compositional data in an\ninstrumental variable setting where the composition acts as the cause.\nThroughout, we pay particular attention to the interpretation of compositional\ncauses from the viewpoint of interventions and crisply articulate potential\npitfalls for practitioners. Focusing on modern high-dimensional microbiome\nsequencing data as a timely illustrative use case, our analysis first reveals\nthat popular one-dimensional information-theoretic summary statistics, such as\ndiversity and richness, may be insufficient for drawing causal conclusions from\necological data. Instead, we advocate for multivariate alternatives using\nstatistical data transformations and regression techniques that take the\nspecial structure of the compositional sample space into account. In a\ncomparative analysis on synthetic and semi-synthetic data we show the\nadvantages and limitations of our proposal. We posit that our framework may\nprovide a useful starting point for cause-effect estimation in the context of\ncompositional data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:29:41 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ailer", "Elisabeth", ""], ["M\u00fcller", "Christian L.", ""], ["Kilbertus", "Niki", ""]]}, {"id": "2106.11448", "submitter": "Gabriel Franco", "authors": "Gabriel Franco, Camila P. E. de Souza, Nancy L. Garcia", "title": "Aggregated functional data model applied on clustering and\n  disaggregation of UK electrical load profiles", "comments": "46 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding electrical energy demand at the consumer level plays an\nimportant role in planning the distribution of electrical networks and offering\nof off-peak tariffs, but observing individual consumption patterns is still\nexpensive. On the other hand, aggregated load curves are normally available at\nthe substation level. The proposed methodology separates substation aggregated\nloads into estimated mean consumption curves, called typical curves, including\ninformation given by explanatory variables. In addition, a model-based\nclustering approach for substations is proposed based on the similarity of\ntheir consumers typical curves and covariance structures. The methodology is\napplied to a real substation load monitoring dataset from the United Kingdom\nand tested in eight simulated scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 23:36:20 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 18:22:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Franco", "Gabriel", ""], ["de Souza", "Camila P. E.", ""], ["Garcia", "Nancy L.", ""]]}, {"id": "2106.11545", "submitter": "Michael LuValle", "authors": "M. LuValle", "title": "Predictive multiview embedding", "comments": "12 pages 19 references, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiview embedding is a way to model strange attractors that takes advantage\nof the way measurements are often made in real chaotic systems, using\nmultidimensional measurements to make up for a lack of long timeseries.\nPredictive multiview embedding adapts this approach to the problem of\npredicting new values, and provides a natural framework for combining multiple\nsources of information such as natural measurements and computer model runs for\npotentially improved prediction. Here, using 18 month ahead prediction of\nmonthly averages, we show how predictive multiview embedding can be combined\nwith simple statistical approaches to explore predictability of four climate\nvariables by a GCM, build prediction bounds, explore the local manifold\nstructure of the attractor, and show that even though the GCM does not predict\na particular variable well, a hybrid model combining information from the GCM\nand empirical data predicts that variable significantly better than the purely\nempirical model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:46:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["LuValle", "M.", ""]]}, {"id": "2106.11578", "submitter": "Yu Du", "authors": "Yu Du", "title": "Online Ordering Platform City Distribution Based on Genetic Algorithm", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the rising of the takeaway ordering platform, the M platform has taken\nthe lead in the industry with its high-quality service. The increasing order\nvolume leads the competition between platforms to reduce the distribution cost,\nwhich increases rapidly because of the unreasonable distribution route. By\nanalyzing platform distribution's current situation, we study the vehicle\nrouting problem of urban distribution on the M platform and minimize the\ndistribution cost. Considering the constraints of the customer's expected\ndelivery time and vehicle condition, we combine the different arrival times of\nthe vehicle routing problem model using three soft time windows and solve the\nproblem using a genetic algorithm (GA). The results show that our model and\nalgorithm can design the vehicle path superior to the original model in terms\nof distribution cost and delivery time, thus providing decision support for the\nM platform to save distribution cost in urban distribution in the future.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 07:24:46 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Du", "Yu", ""]]}, {"id": "2106.11686", "submitter": "Jose Storopoli", "authors": "Jose Storopoli, Andre Luis Marques Ferreira dos Santos, Alessandra\n  Cristina Guedes Pellini, Breck Baldwin", "title": "Simulation-Driven COVID-19 Epidemiological Modeling with Social Media", "comments": "15 pages, 6 figures, 3 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern Bayesian approaches and workflows emphasize in how simulation is\nimportant in the context of model developing. Simulation can help researchers\nunderstand how the model behaves in a controlled setting and can be used to\nstress the model in different ways before it is exposed to any real data. This\nimproved understanding could be beneficial in epidemiological models, specially\nwhen dealing with COVID-19. Unfortunately, few researchers perform any\nsimulations. We present a simulation algorithm that implements a simple\nagent-based model for disease transmission that works with a standard\ncompartment epidemiological model for COVID-19. Our algorithm can be applied in\ndifferent parameterizations to reflect several plausible epidemic scenarios.\nAdditionally, we also model how social media information in the form of daily\nsymptom mentions can be incorporate into COVID-19 epidemiological models. We\ntest our social media COVID-19 model with two experiments. The first using\nsimulated data from our agent-based simulation algorithm and the second with\nreal data using a machine learning tweet classifier to identify tweets that\nmention symptoms from noise. Our results shows how a COVID-19 model can be (1)\nused to incorporate social media data and (2) assessed and evaluated with\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 11:40:00 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Storopoli", "Jose", ""], ["Santos", "Andre Luis Marques Ferreira dos", ""], ["Pellini", "Alessandra Cristina Guedes", ""], ["Baldwin", "Breck", ""]]}, {"id": "2106.11793", "submitter": "Yitao Yang", "authors": "Yitao Yang, Bin Jia, Xiao-Yong Yan, Jiangtao Li, Zhenzhen Yang, Ziyou\n  Gao", "title": "Identifying intercity freight trip ends of heavy trucks from GPS data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The intercity freight trips of heavy trucks are important data for\ntransportation system planning and urban agglomeration management. In recent\ndecades, the extraction of freight trips from GPS data has gradually become the\nmain alternative to traditional surveys. Identifying the trip ends (origin and\ndestination, OD) is the first task in trip extraction. In previous trip end\nidentification methods, some key parameters, such as speed and time thresholds,\nhave mostly been defined on the basis of empirical knowledge, which inevitably\nlacks universality. Here, we propose a data-driven trip end identification\nmethod. First, we define a speed threshold by analyzing the speed distribution\nof heavy trucks and identify all truck stops from raw GPS data. Second, we\ndefine minimum and maximum time thresholds by analyzing the distribution of the\ndwell times of heavy trucks at stop location and classify truck stops into\nthree types based on these time thresholds. Third, we use highway network GIS\ndata and freight-related points-of-interest (POIs) data to identify valid trip\nends from among the three types of truck stops. In this step, we detect POI\nboundaries to determine whether a heavy truck is stopping at a freight-related\nlocation. We further analyze the spatiotemporal characteristics of intercity\nfreight trips of heavy trucks and discuss their potential applications in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:04:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yang", "Yitao", ""], ["Jia", "Bin", ""], ["Yan", "Xiao-Yong", ""], ["Li", "Jiangtao", ""], ["Yang", "Zhenzhen", ""], ["Gao", "Ziyou", ""]]}, {"id": "2106.11917", "submitter": "Zhihao Jiang", "authors": "Haochen Yang and Jicheng Gu and Zhihao Jiang", "title": "Model-based Pre-clinical Trials for Medical Devices Using Statistical\n  Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials are considered as the golden standard for medical device\nvalidation. However, many sacrifices have to be made during the design and\nconduction of the trials due to cost considerations and partial information,\nwhich may compromise the significance of the trial results. In this paper, we\nproposed a model-based pre-clinical trial framework using statistical model\nchecking. Physiological models represent disease mechanism, which enable\nautomated adjudication of simulation results. Sampling of the patient\nparameters and hypothesis testing are performed by statistical model checker.\nThe framework enables a broader range of hypothesis to be tested with\nguaranteed statistical significance, which are useful complements to the\nclinical trials. We demonstrated our framework with a pre-clinical trial on\nimplantable cardioverter defibrillators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:35:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yang", "Haochen", ""], ["Gu", "Jicheng", ""], ["Jiang", "Zhihao", ""]]}, {"id": "2106.11918", "submitter": "Gilles Gasso", "authors": "Komi Midzodzi P\\'ekp\\'e, Djamel Zitouni, Gilles Gasso, Wajdi Dhifli,\n  Benjamin C. Guinhouya", "title": "From SIR to SEAIRD: a novel data-driven modeling approach based on the\n  Grey-box System Theory to predict the dynamics of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common compartmental modeling for COVID-19 is based on a priori knowledge and\nnumerous assumptions. Additionally, they do not systematically incorporate\nasymptomatic cases. Our study aimed at providing a framework for data-driven\napproaches, by leveraging the strengths of the grey-box system theory or\ngrey-box identification, known for its robustness in problem solving under\npartial, incomplete, or uncertain data. Empirical data on confirmed cases and\ndeaths, extracted from an open source repository were used to develop the\nSEAIRD compartment model. Adjustments were made to fit current knowledge on the\nCOVID-19 behavior. The model was implemented and solved using an Ordinary\nDifferential Equation solver and an optimization tool. A cross-validation\ntechnique was applied, and the coefficient of determination $R^2$ was computed\nin order to evaluate the goodness-of-fit of the model. %to the data. Key\nepidemiological parameters were finally estimated and we provided the rationale\nfor the construction of SEAIRD model. When applied to Brazil's cases, SEAIRD\nproduced an excellent agreement to the data, with an %coefficient of\ndetermination $R^2$ $\\geq 90\\%$. The probability of COVID-19 transmission was\ngenerally high ($\\geq 95\\%$). On the basis of a 20-day modeling data, the\nincidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed\npersons in Brazil and France. Within the same time frame, the fatality rate of\nCOVID-19 was the highest in France (16.4\\%) followed by Brazil (6.9\\%), and the\nlowest in Russia ($\\leq 1\\%$). SEAIRD represents an asset for modeling\ninfectious diseases in their dynamical stable phase, especially for new viruses\nwhen pathophysiology knowledge is very limited.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 21:25:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["P\u00e9kp\u00e9", "Komi Midzodzi", ""], ["Zitouni", "Djamel", ""], ["Gasso", "Gilles", ""], ["Dhifli", "Wajdi", ""], ["Guinhouya", "Benjamin C.", ""]]}, {"id": "2106.12047", "submitter": "Benjamin Sch\\\"afer", "authors": "Benjamin Sch\\\"afer, Catherine M. Heppell, Hefin Rhys, Christian Beck", "title": "Fluctuations of water quality time series in rivers follow\n  superstatistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.bio-ph physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Superstatistics is a general method from nonequilibrium statistical physics\nwhich has been applied to a variety of complex systems, ranging from\nhydrodynamic turbulence to traffic delays and air pollution dynamics. Here, we\ninvestigate water quality time series (such as dissolved oxygen concentrations\nand electrical conductivity) as measured in rivers, and provide evidence that\nthey exhibit superstatistical behaviour. Our main example are time series as\nrecorded in the river Chess in South East England. Specifically, we use\nseasonal detrending and empirical mode decomposition (EMD) to separate trends\nfrom fluctuations for the measured data. With either detrending method, we\nobserve heavy-tailed fluctuation distributions, which are well described by a\nlog-normal superstatistics for dissolved oxygen. Contrarily, we find a double\npeaked non-standard superstatistics for the electrical conductivity data, which\nwe model using two combined $\\chi^2$-distributions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:06:54 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sch\u00e4fer", "Benjamin", ""], ["Heppell", "Catherine M.", ""], ["Rhys", "Hefin", ""], ["Beck", "Christian", ""]]}, {"id": "2106.12064", "submitter": "Joseph Patrick Meagher Dr.", "authors": "Joe Meagher and Nial Friel", "title": "Assessing epidemic curves for evidence of superspreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The expected number of secondary infections arising from each index case, the\nreproduction number, or $R$ number is a vital summary statistic for\nunderstanding and managing epidemic diseases. There are many methods for\nestimating $R$; however, few of these explicitly model heterogeneous disease\nreproduction, which gives rise to superspreading within the population. Here we\npropose a parsimonious discrete-time branching process model for epidemic\ncurves that incorporates heterogeneous individual reproduction numbers. Our\nBayesian approach to inference illustrates that this heterogeneity results in\nless certainty on estimates of the time-varying cohort reproduction number\n$R_t$. Leave-future-out cross-validation evaluates the predictive performance\nof the proposed model, allowing us to assess epidemic curves for evidence of\nsuperspreading. We apply these methods to a COVID-19 epidemic curve for the\nRepublic of Ireland and find some support for heterogeneous disease\nreproduction. We conclude that the 10\\% most infectious index cases account for\napproximately 40-80\\% of the expected secondary infections. Our analysis\nhighlights the difficulties in identifying heterogeneous disease reproduction\nfrom epidemic curves and that heterogeneity is a vital consideration when\nestimating $R_t$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:21:14 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Meagher", "Joe", ""], ["Friel", "Nial", ""]]}, {"id": "2106.12160", "submitter": "Simin Ma", "authors": "Simin Ma, Shihao Yang", "title": "COVID-19 Forecasts Using Internet Search Information in the United\n  States", "comments": "16 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 ravaging through the globe, accurate forecasts of the disease\nspread is crucial for situational awareness, resource allocation, and public\nhealth decision-making. Alternative to the traditional disease surveillance\ndata collected by the United States (US) Centers for Disease Control and\nPrevention (CDC), big data from Internet such as online search volumes has been\npreviously shown to contain valuable information for tracking infectious\ndisease dynamics. In this study, we evaluate the feasibility of using Internet\nsearch volume of relevant queries to track and predict COVID-19 pandemic. We\nfound strong association between COVID-19 death trend and the search volume of\nsymptom-related queries such as \"loss of taste\". Then, we further develop an\ninfluenza-tracking model to predict future 2-week COVID-19 deaths on the US\nnational level, by combining search volume information with COVID-19 time\nseries information. Encouraged by the 45% error reduction on national level\ncomparing to the baseline time series model, we additionally build state-level\nCOVID-19 deaths models, leveraging the cross-state cross-resolution spatial\ntemporal framework that pools information from search volume and COVID-19\nreports across states, regions and the nation. These variants of ARGOX are then\naggregated in a winner-takes-all ensemble fashion to produce the final\nstate-level 2-week forecasts. Numerical experiments demonstrate that our method\nsteadily outperforms time series baseline models, and achieves the\nstate-of-the-art performance among the publicly available benchmark models.\nOverall, we show that disease dynamics and relevant public search behaviors\nco-evolve during the COVID-19 pandemic, and capturing their dependencies while\nleveraging historical cases/deaths as well as spatial-temporal cross-region\ninformation will enable stable and accurate US national and state-level\nforecasts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:00:05 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ma", "Simin", ""], ["Yang", "Shihao", ""]]}, {"id": "2106.12180", "submitter": "Xueheng Shi", "authors": "Xueheng Shi, Claudie Beaulieu, Rebecca Killick, Robert Lund", "title": "Changepoint Detection: An Analysis of the Central England Temperature\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a statistical analysis of structural changes in the\nCentral England temperature series. This series contains one of the longest\nsurface temperature records available and a changepoint analysis of it reveals\nseveral interesting aspects. Regression functions with structural breaks,\nincluding mean and trend shifts, are fitted to the series and compared via two\ncommonly used multiple changepoint penalized likelihood criteria. In the end,\nthe optimal model is judged to be one containing three location and trend\nshifts, with a transition to a rapidly warming regime circa 1989. The\nvariability of the series is not found to be significantly changing, and shift\nfeatures are judged to be more plausible than short- or long-memory\nautocorrelations. The analysis serves as a walk-through tutorial of different\nchangepoint techniques, illustrating what can statistically be inferred from\ndifferent models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:10:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shi", "Xueheng", ""], ["Beaulieu", "Claudie", ""], ["Killick", "Rebecca", ""], ["Lund", "Robert", ""]]}, {"id": "2106.12360", "submitter": "M\\'elodie Monod", "authors": "M\\'elodie Monod, Alexandra Blenkinsop, Andrea Brizzi, Yu Chen,\n  Vidoushee Jogarah, Yuanrong Wang, Seth Flaxman, Samir Bhatt and Oliver\n  Ratmann", "title": "Regularised B-splines projected Gaussian Process priors to estimate the\n  age profile of COVID-19 deaths before and after vaccine roll-out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has caused severe public health consequences in the\nUnited States. The United States began a vaccination campaign at the end of\n2020 targeting primarily elderly residents before extending access to younger\nindividuals. With both COVID-19 infection fatality ratios and vaccine uptake\nbeing heterogeneous across ages, an important consideration is whether the age\ncontribution to deaths shifted over time towards younger age groups. In this\nstudy, we use a Bayesian non-parametric spatial approach to estimate the\nage-specific contribution to COVID-19 attributable deaths over time. The\nproposed spatial approach is a low-rank Gaussian Process projected by\nregularised B-splines. Simulation analyses and benchmark results show that the\nspatial approach performs better than a standard B-splines approach and\nequivalently well as a standard Gaussian Process, for considerably lower\nruntimes. We find that COVID-19 has been especially deadly in the United\nStates. The mortality rates among individuals aged 85+ ranged from 1\\% to 5\\%\nacross the US states. Since the beginning of the vaccination campaign, the\nnumber of weekly deaths reduced in every US state with a faster decrease among\nindividuals aged 75+ than individuals aged 0-74. Simultaneously to this\nreduction, the contribution of individuals age 75+ to deaths decreased, with\nimportant disparities in the timing and rapidity of this decrease across the\ncountry.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 12:53:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Monod", "M\u00e9lodie", ""], ["Blenkinsop", "Alexandra", ""], ["Brizzi", "Andrea", ""], ["Chen", "Yu", ""], ["Jogarah", "Vidoushee", ""], ["Wang", "Yuanrong", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""], ["Ratmann", "Oliver", ""]]}, {"id": "2106.12425", "submitter": "B{\\aa}rd St{\\o}ve", "authors": "Anders D. Sleire, B{\\aa}rd St{\\o}ve, H{\\aa}kon Otneim, Geir Drage\n  Berentsen, Dag Tj{\\o}stheim, Sverre Hauso Haugen", "title": "Portfolio Allocation under Asymmetric Dependence in Asset Returns using\n  Local Gaussian Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that there are asymmetric dependence structures between\nfinancial returns. In this paper we use a new nonparametric measure of local\ndependence, the local Gaussian correlation, to improve portfolio allocation. We\nextend the classical mean-variance framework, and show that the portfolio\noptimization is straightforward using our new approach, only relying on a\ntuning parameter (the bandwidth). The new method is shown to outperform the\nequally weighted (1/N) portfolio and the classical Markowitz portfolio for\nmonthly asset returns data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:29:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sleire", "Anders D.", ""], ["St\u00f8ve", "B\u00e5rd", ""], ["Otneim", "H\u00e5kon", ""], ["Berentsen", "Geir Drage", ""], ["Tj\u00f8stheim", "Dag", ""], ["Haugen", "Sverre Hauso", ""]]}, {"id": "2106.12768", "submitter": "Jingru Zhang", "authors": "Jingru Zhang, Kathleen R. Merikangas, Hongzhe Li and Haochang Shou", "title": "Two-sample tests for repeated measurements of histogram objects with\n  applications to wearable device data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated observations have become increasingly common in biomedical research\nand longitudinal studies. For instance, wearable sensor devices are deployed to\ncontinuously track physiological and biological signals from each individual\nover multiple days. It remains of great interest to appropriately evaluate how\nthe daily distribution of biosignals might differ across disease groups and\ndemographics. Hence these data could be formulated as multivariate complex\nobject data such as probability densities, histograms, and observations on a\ntree. Traditional statistical methods would often fail to apply as they are\nsampled from an arbitrary non-Euclidean metric space. In this paper, we propose\nnovel non-parametric graph-based two-sample tests for object data with repeated\nmeasures. A set of test statistics are proposed to capture various possible\nalternatives. We derive their asymptotic null distributions under the\npermutation null. These tests exhibit substantial power improvements over the\nexisting methods while controlling the type I errors under finite samples as\nshown through simulation studies. The proposed tests are demonstrated to\nprovide additional insights on the location, inter- and intra-individual\nvariability of the daily physical activity distributions in a sample of studies\nfor mood disorders.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:01:39 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhang", "Jingru", ""], ["Merikangas", "Kathleen R.", ""], ["Li", "Hongzhe", ""], ["Shou", "Haochang", ""]]}, {"id": "2106.12827", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz, Dagmara Nikulin, Marcin Szymkowiak, Kamil Wilak", "title": "The gig economy in Poland: evidence based on mobile big data", "comments": "44 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we address the question of how to measure the size and\ncharacteristics of the platform economy. We propose a~different, to sample\nsurveys, approach based on smartphone data, which are passively collected\nthrough programmatic systems as part of online marketing. In particular, in our\nstudy we focus on two types of services: food delivery (Bolt Courier, Takeaway,\nGlover, Wolt and transport services (Bolt Driver, Free Now, iTaxi and Uber).\nOur results show that the platform economy in Poland is growing. In particular,\nwith respect to food delivery and transportation services performed by means of\napplications, we observed a growing trend between January 2018 and December\n2020. Taking into account the demographic structure of apps users, our results\nconfirm findings from past studies: the majority of platform workers are young\nmen but the age structure of app users is different for each of the two\ncategories of services. Another surprising finding is that foreigners do not\naccount for the majority of gig workers in Poland. When the number of platform\nworkers is compared with corresponding working populations, the estimated share\nof active app users accounts for about 0.5-2% of working populations in 9\nlargest Polish cities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:30:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Nikulin", "Dagmara", ""], ["Szymkowiak", "Marcin", ""], ["Wilak", "Kamil", ""]]}, {"id": "2106.12885", "submitter": "Zhan Zhao", "authors": "Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Identifying Hidden Visits from Sparse Call Detail Record Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a large body of literature on trip inference using call detail record\n(CDR) data, a fundamental understanding of their limitations is lacking. In\nparticular, because of the sparse nature of CDR data, users may travel to a\nlocation without being revealed in the data, which we refer to as a \"hidden\nvisit\". The existence of hidden visits hinders our ability to extract reliable\ninformation about human mobility and travel behavior from CDR data. In this\nstudy, we propose a data fusion approach to obtain labeled data for statistical\ninference of hidden visits. In the absence of complementary data, this can be\naccomplished by extracting labeled observations from more granular cellular\ndata access records, and extracting features from voice call and text messaging\nrecords. The proposed approach is demonstrated using a real-world CDR dataset\nof 3 million users from a large Chinese city. Logistic regression, support\nvector machine, random forest, and gradient boosting are used to infer whether\na hidden visit exists during a displacement observed from CDR data. The test\nresults show significant improvement over the naive no-hidden-visit rule, which\nis an implicit assumption adopted by most existing studies. Based on the\nproposed model, we estimate that over 10% of the displacements extracted from\nCDR data involve hidden visits. The proposed data fusion method offers a\nsystematic statistical approach to inferring individual mobility patterns based\non telecommunication records.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:41:47 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhao", "Zhan", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2106.12888", "submitter": "Tushar Sarkar", "authors": "Tushar Sarkar, Umang Patel, Rupali Patil", "title": "COVID-19 cases prediction using regression and novel SSM model for\n  non-converged countries", "comments": null, "journal-ref": "J. Appl. Sci. Eng. Technol. Educ., vol. 3, no. 1, pp. 74-81, Feb.\n  2021", "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anticipating the quantity of new associated or affirmed cases with novel\ncoronavirus ailment 2019 (COVID-19) is critical in the counteraction and\ncontrol of the COVID-19 flare-up. The new associated cases with COVID-19\ninformation were gathered from 20 January 2020 to 21 July 2020. We filtered out\nthe countries which are converging and used those for training the network. We\nutilized the SARIMAX, Linear regression model to anticipate new suspected\nCOVID-19 cases for the countries which did not converge yet. We predict the\ncurve of non-converged countries with the help of proposed Statistical SARIMAX\nmodel (SSM). We present new information investigation-based forecast results\nthat can assist governments with planning their future activities and help\nclinical administrations to be more ready for what's to come. Our framework can\nforesee peak corona cases with an R-Squared value of 0.986 utilizing linear\nregression and fall of this pandemic at various levels for countries like\nIndia, US, and Brazil. We found that considering more countries for training\ndegrades the prediction process as constraints vary from nation to nation.\nThus, we expect that the outcomes referenced in this work will help individuals\nto better understand the possibilities of this pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:02:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sarkar", "Tushar", ""], ["Patel", "Umang", ""], ["Patil", "Rupali", ""]]}, {"id": "2106.12987", "submitter": "Dhagash Mehta", "authors": "Vipul Satone, Dhruv Desai, Dhagash Mehta", "title": "Fund2Vec: Mutual Funds Similarity using Graph Learning", "comments": "2 column format, 8 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying similar mutual funds with respect to the underlying portfolios\nhas found many applications in financial services ranging from fund recommender\nsystems, competitors analysis, portfolio analytics, marketing and sales, etc.\nThe traditional methods are either qualitative, and hence prone to biases and\noften not reproducible, or, are known not to capture all the nuances\n(non-linearities) among the portfolios from the raw data. We propose a\nradically new approach to identify similar funds based on the weighted\nbipartite network representation of funds and their underlying assets data\nusing a sophisticated machine learning method called Node2Vec which learns an\nembedded low-dimensional representation of the network. We call the embedding\n\\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network\nrepresentation of the funds-assets network in its original form that identifies\nstructural similarity among portfolios as opposed to merely portfolio overlaps.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:35:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Satone", "Vipul", ""], ["Desai", "Dhruv", ""], ["Mehta", "Dhagash", ""]]}, {"id": "2106.12991", "submitter": "Yulei Qin", "authors": "Yulei Qin, Yun Gu, Hanxiao Zhang, Jie Yang, Lihui Wang, Feng Yao,\n  Yue-Min Zhu", "title": "Relationship between pulmonary nodule malignancy and surrounding\n  pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI\n  dataset", "comments": "33 pages, 3 figures, Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To investigate whether the pleurae, airways and vessels surrounding a nodule\non non-contrast computed tomography (CT) can discriminate benign and malignant\npulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available\nCT database, was exploited for study. A total of 1556 nodules from 694 patients\nwere involved in statistical analysis, where nodules with average scorings <3\nand >3 were respectively denoted as benign and malignant. Besides, 339 nodules\nfrom 113 patients with diagnosis ground-truth were independently evaluated.\nComputer algorithms were developed to segment pulmonary structures and quantify\nthe distances to pleural surface, airways and vessels, as well as the counting\nnumber and normalized volume of airways and vessels near a nodule. Odds ratio\n(OR) and Chi-square (\\chi^2) testing were performed to demonstrate the\ncorrelation between features of surrounding structures and nodule malignancy. A\nnon-parametric receiver operating characteristic (ROC) analysis was conducted\nin logistic regression to evaluate discrimination ability of each structure.\nFor benign and malignant groups, the average distances from nodules to pleural\nsurface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and\n(1.42, 1.07) mm. The correlation between nodules and the counting number of\nairways and vessels that contact or project towards nodules are respectively\n(OR=22.96, \\chi^2=105.04) and (OR=7.06, \\chi^2=290.11). The correlation between\nnodules and the volume of airways and vessels are (OR=9.19, \\chi^2=159.02) and\n(OR=2.29, \\chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and\nvessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that\nmalignant nodules are often surrounded by more pulmonary structures compared\nwith benign ones, suggesting that features of these structures could be viewed\nas lung cancer biomarkers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:05:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qin", "Yulei", ""], ["Gu", "Yun", ""], ["Zhang", "Hanxiao", ""], ["Yang", "Jie", ""], ["Wang", "Lihui", ""], ["Yao", "Feng", ""], ["Zhu", "Yue-Min", ""]]}, {"id": "2106.13011", "submitter": "Tarmo Nurmi", "authors": "Sallamari Sallmen, Tarmo Nurmi, Mikko Kivel\\\"a", "title": "Graphlets in multilayer networks", "comments": "58 pages, 45 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing various networked data as multiplex networks, networks of\nnetworks and other multilayer networks can reveal completely new types of\nstructures in these system. We introduce a general and principled graphlet\nframework for multilayer networks which allows one to break any multilayer\nnetwork into small multilayered building blocks. These multilayer graphlets can\nbe either analyzed themselves or used to do tasks such as comparing different\nsystems. The method is flexible in terms of multilayer isomorphism,\nautomorphism orbit definition, and the type of multilayer network. We\nillustrate our method for multiplex networks and show how it can be used to\ndistinguish networks produced with multiple models from each other in an\nunsupervised way. In addition, we include an automatic way of generating the\nhundreds of dependency equations between the orbit counts needed to remove\nredundant orbit counts. The framework introduced here allows one to analyze\nmultilayer networks with versatile semantics, and these methods can thus be\nused to analyze the structural building blocks of myriad multilayer networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:45:12 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sallmen", "Sallamari", ""], ["Nurmi", "Tarmo", ""], ["Kivel\u00e4", "Mikko", ""]]}, {"id": "2106.13077", "submitter": "Rapha\\\"el de Fondeville", "authors": "Rapha\\\"el de Fondeville and Matthieu Wilhelm", "title": "Optimal sequential sampling design for environmental extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sihl river, located near the city of Zurich in Switzerland, is under\ncontinuous and tight surveillance as it flows directly under the city's main\nrailway station. To issue early warnings and conduct accurate risk\nquantification, a dense network of monitoring stations is necessary inside the\nriver basin. However, as of 2021 only three automatic stations are operated in\nthis region, naturally raising the question: how to extend this network for\noptimal monitoring of extreme rainfall events?\n  So far, existing methodologies for station network design have mostly focused\non maximizing interpolation accuracy or minimizing the uncertainty of some\nmodel's parameters estimates. In this work, we propose new principles inspired\nfrom extreme value theory for optimal monitoring of extreme events. For\nstationary processes, we study the theoretical properties of the induced\nsampling design that yields non-trivial point patterns resulting from a\ncompromise between a boundary effect and the maximization of inter-location\ndistances. For general applications, we propose a theoretically justified\nfunctional peak-over-threshold model and provide an algorithm for sequential\nstation selection. We then issue recommendations for possible extensions of the\nSihl river monitoring network, by efficiently leveraging both station and radar\nmeasurements available in this region.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:03:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["de Fondeville", "Rapha\u00ebl", ""], ["Wilhelm", "Matthieu", ""]]}, {"id": "2106.13110", "submitter": "Daniela Castro-Camilo", "authors": "Daniela Castro-Camilo, Rapha\\\"el Huser, H{\\aa}vard Rue", "title": "Practical strategies for GEV-based regression models for extremes", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalised extreme value (GEV) distribution is a three parameter family\nthat describes the asymptotic behaviour of properly renormalised maxima of a\nsequence of independent and identically distributed random variables. If the\nshape parameter $\\xi$ is zero, the GEV distribution has unbounded support,\nwhereas if $\\xi$ is positive, the limiting distribution is heavy-tailed with\ninfinite upper endpoint but finite lower endpoint. In practical applications,\nwe assume that the GEV family is a reasonable approximation for the\ndistribution of maxima over blocks, and we fit it accordingly. This implies\nthat GEV properties, such as finite lower endpoint in the case $\\xi>0$, are\ninherited by the finite-sample maxima, which might not have bounded support.\nThis is particularly problematic when predicting extreme observations based on\nmultiple and interacting covariates. To tackle this usually overlooked issue,\nwe propose a blended GEV distribution, which smoothly combines the left tail of\na Gumbel distribution (GEV with $\\xi=0$) with the right tail of a Fr\\'echet\ndistribution (GEV with $\\xi>0$) and, therefore, has unbounded support. Using a\nBayesian framework, we reparametrise the GEV distribution to offer a more\nnatural interpretation of the (possibly covariate-dependent) model parameters.\nIndependent priors over the new location and spread parameters induce a joint\nprior distribution for the original location and scale parameters. We introduce\nthe concept of property-preserving penalised complexity (P$^3$C) priors and\napply it to the shape parameter to preserve first and second moments. We\nillustrate our methods with an application to NO$_2$ pollution levels in\nCalifornia, which reveals the robustness of the bGEV distribution, as well as\nthe suitability of the new parametrisation and the P$^3$C prior framework.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:44:09 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Castro-Camilo", "Daniela", ""], ["Huser", "Rapha\u00ebl", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2106.13295", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin", "title": "Stochastic modeling of scientific impact", "comments": "Accepted for publication by Europhysics Letters", "journal-ref": "EPL, 134 (2021) 48004", "doi": "10.1209/0295-5075/134/48004", "report-no": null, "categories": "physics.soc-ph cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has found that select scientists have a disproportional share\nof highly cited papers. Researchers reasoned that this could not have happened\nif success in science was random and introduced a hidden parameter Q, or\ntalent, to explain this finding. So, the talented high-Q scientists have many\nhigh impact papers. Here I show that an upgrade of an old random citation\ncopying model could also explain this finding. In the new model the probability\nof citation copying is not the same for all papers but is proportional to the\nlogarithm of the total number of citations to all papers of its author.\nNumerical simulations of the model give results similar to the empirical\nfindings of the Q-factor article.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 19:54:43 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Simkin", "M. V.", ""]]}, {"id": "2106.13390", "submitter": "Zheng Chen", "authors": "Hongji Wu, Hao Yuan, Zijing Yang, Yawen Hou, Zheng Chen", "title": "Implementation of an alternative method for assessing competing risks:\n  restricted mean time lost", "comments": "American Journal of Epidemiology, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical and epidemiological studies, hazard ratios are often applied to\ncompare treatment effects between two groups for survival data. For competing\nrisks data, the corresponding quantities of interest are cause-specific hazard\nratios (CHRs) and subdistribution hazard ratios (SHRs). However, they all have\nsome limitations related to model assumptions and clinical interpretation.\nTherefore, we introduce restricted mean time lost (RMTL) as an alternative that\nis easy to interpret in a competing risks framework. We propose a hypothetical\ntest and sample size estimator based on the difference in RMTL (RMTLd). The\nsimulation results show that the RMTLd test has robust statistical performance\n(both type I error and power). Meanwhile, the RMTLd-based sample size can\napproximately achieve the predefined power level. The results of two example\nanalyses also verify the performance of the RMTLd test. From the perspectives\nof clinical interpretation, application conditions and statistical performance,\nwe recommend that the RMTLd be reported with the HR when analyzing competing\nrisks data and that the RMTLd even be regarded as the primary outcome when the\nproportional hazard assumption fails.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:13:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wu", "Hongji", ""], ["Yuan", "Hao", ""], ["Yang", "Zijing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.13535", "submitter": "Sophie Mathieu", "authors": "Sophie Mathieu, Laure Lef\\`evre, Rainer von Sachs, V\\'eronique\n  Delouille, Christian Ritter, Fr\\'ed\\'eric Clette", "title": "Nonparametric monitoring of sunspot number observations: a case study", "comments": "27 pages (without appendices), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solar activity is an important driver of long-term climate trends and must be\naccounted for in climate models. Unfortunately, direct measurements of this\nquantity over long periods do not exist. The only observation related to solar\nactivity whose records reach back to the seventeenth century are sunspots.\nSurprisingly, determining the number of sunspots consistently over time has\nremained until today a challenging statistical problem. It arises from the need\nof consolidating data from multiple observing stations around the world in a\ncontext of low signal-to-noise ratios, non-stationarity, missing data,\nnon-standard distributions and many kinds of errors. The data from some\nstations experience therefore severe and various deviations over time. In this\npaper, we propose the first systematic and thorough statistical approach for\nmonitoring these complex and important series. It consists of three steps\nessential for successful treatment of the data: smoothing on multiple\ntimescales, monitoring using block bootstrap calibrated CUSUM charts and\nclassifying of out-of-control situations by support vector techniques. This\napproach allows us to detect a wide range of anomalies (such as sudden jumps or\nmore progressive drifts), unseen in previous analyses. It helps us to identify\nthe causes of major deviations, which are often observer or equipment related.\nTheir detection and identification will contribute to improve future\nobservations. Their elimination or correction in past data will lead to a more\nprecise reconstruction of the world reference index for solar activity: the\nInternational Sunspot Number.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:59:28 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mathieu", "Sophie", ""], ["Lef\u00e8vre", "Laure", ""], ["von Sachs", "Rainer", ""], ["Delouille", "V\u00e9ronique", ""], ["Ritter", "Christian", ""], ["Clette", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2106.13540", "submitter": "Helmi Shat", "authors": "Helmi Shat and Norbert Gaffke", "title": "Optimal Accelerated Degradation Testing Based on Bivariate Gamma Process\n  with Dependent Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerated degradation testing (ADT) is one of the major approaches in\nreliability engineering which allows accurate estimation of reliability\ncharacteristics of highly reliable systems within a relatively short time. The\ntesting data are extrapolated through a physically reasonable statistical model\nto obtain estimates of lifetime quantiles at normal use conditions. The Gamma\nprocess is a natural model for degradation, which exhibits a monotone and\nstrictly increasing degradation path. In this work, optimal experimental\ndesigns are derived for ADT with two response components. We consider the\nsituations of independent as well as dependent marginal responses where the\nobservational times are assumed to be fixed and known. The marginal degradation\npaths are assumed to follow a Gamma process where a copula function is utilized\nto express the dependence between both components. For the case of independent\nresponse components the optimal design minimizes the asymptotic variance of an\nestimated quantile of the failure time distribution at the normal use\nconditions. For the case of dependent response components the $D$-criterion is\nadopted to derive $D$-optimal designs. Further, $D$- and $c$-optimal designs\nare developed when the copula-based models are reduced to bivariate binary\noutcomes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:20:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Shat", "Helmi", ""], ["Gaffke", "Norbert", ""]]}, {"id": "2106.13576", "submitter": "Hector Rodriguez-Deniz", "authors": "Hector Rodriguez-Deniz and Mattias Villani", "title": "Robust Real-Time Delay Predictions in a Network of High-Frequency Urban\n  Buses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing transport users and operators with accurate forecasts on travel\ntimes is challenging due to a highly stochastic traffic environment. Public\ntransport users are particularly sensitive to unexpected waiting times, which\nnegatively affect their perception on the system's reliability. In this paper\nwe develop a robust model for real-time bus travel time prediction that depart\nfrom Gaussian assumptions by using Student-$t$ errors. The proposed approach\nuses spatiotemporal characteristics from the route and previous bus trips to\nmodel short-term effects, and date/time variables and Gaussian processes for\nlong-run forecasts. The model allows for flexible modeling of mean, variance\nand kurtosis spaces. We propose algorithms for Bayesian inference and for\ncomputing probabilistic forecast distributions. Experiments are performed using\ndata from high-frequency buses in Stockholm, Sweden. Results show that\nStudent-$t$ models outperform Gaussian ones in terms of log-posterior\npredictive power to forecast bus delays at specific stops, which reveals the\nimportance of accounting for predictive uncertainty in model selection.\nEstimated Student-$t$ regressions capture typical temporal variability between\nwithin-day hours and different weekdays. Strong spatiotemporal effects are\ndetected for incoming buses from immediately previous stops, which is in line\nwith many recently developed models. We finally show how Bayesian inference\nnaturally allows for predictive uncertainty quantification, e.g. by returning\nthe predictive probability that the delay of an incoming bus exceeds a given\nthreshold.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 12:00:49 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Rodriguez-Deniz", "Hector", ""], ["Villani", "Mattias", ""]]}, {"id": "2106.13639", "submitter": "Farid Mohammadi", "authors": "Farid Mohammadi, Elissa Eggenweiler, Bernd Flemisch, Sergey\n  Oladyshkin, Iryna Rybak, Martin Schneider, Kilian Weishaupt", "title": "Uncertainty-aware Validation Benchmarks for Coupling Free Flow and\n  Porous-Medium Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correct choice of interface conditions and useful model parameters for\ncoupled free-flow and porous-medium systems is vital for physically consistent\nmodeling and accurate numerical simulations of applications. We consider the\nStokes--Darcy problem with different models for the porous-medium compartment\nand corresponding coupling strategies: the standard averaged model based on\nDarcy's law with classical or generalized interface conditions, as well as the\npore-network model. We study the coupled flow problems' behaviors considering a\nbenchmark case where a pore-scale resolved model provides the reference\nsolution and quantify the uncertainties in the models' parameters and the\nreference data. To achieve this, we apply a statistical framework that\nincorporates a probabilistic modeling technique using a fully Bayesian\napproach. A Bayesian perspective on a validation task yields an optimal\nbias-variance trade-off against the reference data. It provides an integrative\nmetric for model validation that incorporates parameter and conceptual\nuncertainty. Additionally, a model reduction technique, namely Bayesian Sparse\nPolynomial Chaos Expansion, is employed to accelerate the calibration and\nvalidation processes for computationally demanding Stokes--Darcy models with\ndifferent coupling strategies. We perform uncertainty-aware validation,\ndemonstrate each model's predictive capabilities, and make a model comparison\nusing a Bayesian validation metric.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:44:30 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mohammadi", "Farid", ""], ["Eggenweiler", "Elissa", ""], ["Flemisch", "Bernd", ""], ["Oladyshkin", "Sergey", ""], ["Rybak", "Iryna", ""], ["Schneider", "Martin", ""], ["Weishaupt", "Kilian", ""]]}, {"id": "2106.13703", "submitter": "Alec Farid", "authors": "Alec Farid, Sushant Veer, Anirudha Majumdar", "title": "Task-Driven Out-of-Distribution Detection with Statistical Guarantees\n  for Robot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect\nwhen a robot is operating in environments that are drawn from a different\ndistribution than the environments used to train the robot. We leverage\nProbably Approximately Correct (PAC)-Bayes theory in order to train a policy\nwith a guaranteed bound on performance on the training distribution. Our key\nidea for OOD detection then relies on the following intuition: violation of the\nperformance bound on test environments provides evidence that the robot is\noperating OOD. We formalize this via statistical techniques based on p-values\nand concentration inequalities. The resulting approach (i) provides guaranteed\nconfidence bounds on OOD detection, and (ii) is task-driven and sensitive only\nto changes that impact the robot's performance. We demonstrate our approach on\na simulated example of grasping objects with unfamiliar poses or shapes. We\nalso present both simulation and hardware experiments for a drone performing\nvision-based obstacle avoidance in unfamiliar environments (including wind\ndisturbances and different obstacle densities). Our examples demonstrate that\nwe can perform task-driven OOD detection within just a handful of trials.\nComparisons with baselines also demonstrate the advantages of our approach in\nterms of providing statistical guarantees and being insensitive to\ntask-irrelevant distribution shifts.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:41:25 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Farid", "Alec", ""], ["Veer", "Sushant", ""], ["Majumdar", "Anirudha", ""]]}, {"id": "2106.13790", "submitter": "Som Dhulipala", "authors": "S. L. N. Dhulipala, M. D. Shields, B. W. Spencer, C. Bolisetti, A. E.\n  Slaughter, V. M. Laboure, P. Chakroborty", "title": "Active Learning with Multifidelity Modeling for Efficient Rare Event\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While multifidelity modeling provides a cost-effective way to conduct\nuncertainty quantification with computationally expensive models, much greater\nefficiency can be achieved by adaptively deciding the number of required\nhigh-fidelity (HF) simulations, depending on the type and complexity of the\nproblem and the desired accuracy in the results. We propose a framework for\nactive learning with multifidelity modeling emphasizing the efficient\nestimation of rare events. Our framework works by fusing a low-fidelity (LF)\nprediction with an HF-inferred correction, filtering the corrected LF\nprediction to decide whether to call the high-fidelity model, and for enhanced\nsubsequent accuracy, adapting the correction for the LF prediction after every\nHF model call. The framework does not make any assumptions as to the LF model\ntype or its correlations with the HF model. In addition, for improved\nrobustness when estimating smaller failure probabilities, we propose using\ndynamic active learning functions that decide when to call the HF model. We\ndemonstrate our framework using several academic case studies and two finite\nelement (FE) model case studies: estimating Navier-Stokes velocities using the\nStokes approximation and estimating stresses in a transversely isotropic model\nsubjected to displacements via a coarsely meshed isotropic model. Across these\ncase studies, not only did the proposed framework estimate the failure\nprobabilities accurately, but compared with either Monte Carlo or a standard\nvariance reduction method, it also required only a small fraction of the calls\nto the HF model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:44:28 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dhulipala", "S. L. N.", ""], ["Shields", "M. D.", ""], ["Spencer", "B. W.", ""], ["Bolisetti", "C.", ""], ["Slaughter", "A. E.", ""], ["Laboure", "V. M.", ""], ["Chakroborty", "P.", ""]]}, {"id": "2106.13827", "submitter": "Giacomo De Nicola", "authors": "Giacomo De Nicola, G\\\"oran Kauermann, Michael H\\\"ohle", "title": "On assessing excess mortality in Germany during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is associated with a very high number of\ncasualties in the general population. Assessing the exact magnitude of this\nnumber is a non-trivial problem, as relying only on officially reported\nCOVID-19 associated fatalities runs the risk of incurring in several kinds of\nbiases. One of the ways to approach the issue is to compare overall mortality\nduring the pandemic with expected mortality computed using the observed\nmortality figures of previous years. In this paper, we build on existing\nmethodology and propose two ways to compute expected as well as excess\nmortality, namely at the weekly and at the yearly level. Particular focus is\nput on the role of age, which plays a central part in both COVID-19-associated\nand overall mortality. We illustrate our methods by making use of\nage-stratified mortality data from the years 2016 to 2020 in Germany to compute\nage group-specific excess mortality during the COVID-19 pandemic in 2020.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:08:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["De Nicola", "Giacomo", ""], ["Kauermann", "G\u00f6ran", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "2106.13874", "submitter": "Steven Ellis", "authors": "Steven P. Ellis", "title": "Statistical Methods for the meta-analysis paper by Itzhaky et al", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the statistical methods used in Itzhaky et al\n(\"Systematic Review and Meta-analysis: Twenty-six Years of Randomized Clinical\nTrials of Psychosocial Interventions to Reduce Suicide Risk in Adolescents\").\nThat paper is a meta-analysis of randomized controlled clinical trials testing\nmethods for preventing suicidal behavior and/or ideation in youth. Particularly\non the behavior side the meta-data are challenging to analyze.\n  This paper has two parts. The first is an informal discussion of the\nstatistical methods used. The second gives detailed mathematical derivations of\nsome formulas and methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:23:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ellis", "Steven P.", ""]]}, {"id": "2106.14005", "submitter": "Angelos Alexopoulos Dr", "authors": "Angelos Alexopoulos, Petros Dellaportas, Stanley Gyoshev, Christos\n  Kotsogiannis, Sofia C. Olhede, Trifon Pavkov", "title": "Detecting anomalies in heterogeneous population-scale VAT networks", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection in network science is the method to determine aberrant\nedges, nodes, subgraphs or other network events. Heterogeneous networks\ntypically contain information going beyond the observed network itself. Value\nAdded Tax (VAT, a tax on goods and services) networks, defined from pairwise\ninteractions of VAT registered taxpayers, are analysed at a population-scale\nrequiring scalable algorithms. By adopting a quantitative understanding of the\nnature of VAT-anomalies, we define a method that identifies them utilising\ninformation from micro-scale, meso-scale and global-scale patterns that can be\ninterpreted, and efficiently implemented, as population-scale network analysis.\nThe proposed method is automatable, and implementable in real time, enabling\nrevenue authorities to prevent large losses of tax revenues through performing\nearly identification of fraud within the VAT system.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 11:54:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Dellaportas", "Petros", ""], ["Gyoshev", "Stanley", ""], ["Kotsogiannis", "Christos", ""], ["Olhede", "Sofia C.", ""], ["Pavkov", "Trifon", ""]]}, {"id": "2106.14045", "submitter": "Ning Ning", "authors": "Ning Ning and Jinwen Qiu", "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models\n  in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:28:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ning", "Ning", ""], ["Qiu", "Jinwen", ""]]}, {"id": "2106.14063", "submitter": "Walter K Kremers", "authors": "Walter K Kremers", "title": "A general, simple, robust method to account for measurement error when\n  analyzing data with an internal validation subsample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: Measurement errors in terms of quantification or classification\nfrequently occur in epidemiologic data and can strongly impact inference.\nMeasurement errors may occur when ascertaining, recording or extracting data.\nAlthough the effects of measurement errors can be severe and are well\ndescribed, simple straight forward general analytic solutions are not readily\navailable for statistical analysis and measurement error is frequently not\nacknowledged or accounted for. Generally, to account for measurement error\nrequires some data where we can observe the variables once with and once\nwithout error, to establish the relationship between the two. Methods: Here we\ndescribe a general method accounting for measurement error in outcome and/or\npredictor variables for the parametric regression setting when there is a\nvalidation subsample where variables are measured once with and once without\nerror. The method does not describe and thus does not depend on the particular\nrelation between the variables measured with and without error, and is\ngenerally robust to the type of measurement error, for example nondifferential,\ndifferential or Berkson errors. Results: Simulation studies show how the method\nreduces bias compared to models based upon variables measured with error alone\nand reduces variances compared to models based upon the variables measured\nwithout error in the validation subsample alone. Conclusion: The proposed\nestimator has favorable properties in terms of bias and variance, is easily\nderived empirically, and is robust to different types of measurement error.\nThis method should be a valuable tool in the analysis of data with measurement\nerror.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:52:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kremers", "Walter K", ""]]}, {"id": "2106.14083", "submitter": "Michele Guindani", "authors": "Wei Zhang, Ivor Cribben, sonia Petrone, Michele Guindani", "title": "Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic\n  Effective Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in functional magnetic resonance imaging (fMRI)\ninvestigate how some brain regions directly influence the activity of other\nregions of the brain {\\it dynamically} throughout the course of an experiment,\nnamely dynamic effective connectivity. Time-varying vector autoregressive\n(TV-VAR) models have been employed to draw inferencesfor this purpose, but they\nare very computationally intensive, since the number of parameters to be\nestimated increases quadratically with the number of time series. In this\npaper, we propose a computationally efficient Bayesian time-varying VAR\napproach for modeling high-dimensional time series. The proposed framework\nemploys a tensor decomposition for the VAR coefficient matrices at different\nlags. Dynamically varying connectivity patterns are captured by assuming that\nat any given time only a subset of components in the tensor decomposition is\nactive. Latent binary time series select the active components at each time via\na convenient Ising prior specification. The proposed prior structure encourages\nsparsity in the tensor structure and allows to ascertain model complexity\nthrough the posterior distribution. More specifically, sparsity-inducing priors\nare employed to allow for global-local shrinkage of the coefficients, to\ndetermine automatically the rank of the tensor decomposition and to guide the\nselection of the lags of the auto-regression. We show the performances of our\nmodel formulation via simulation studies and data from a real fMRI study\ninvolving a book reading experiment.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:14:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Wei", ""], ["Cribben", "Ivor", ""], ["Petrone", "sonia", ""], ["Guindani", "Michele", ""]]}, {"id": "2106.14109", "submitter": "Han Fu", "authors": "Han Fu (1), Shahrul Mt-Isa (2), Richard Baumgartner (3), William\n  Malbecq (2) ((1) The Ohio State University, (2) MSD, (3) Merck)", "title": "Parmsurv: a SAS Macro for Flexible Parametric Survival Analysis with\n  Long-Term Predictions", "comments": "15 pages, 1 figure, 10 tables, accepted by The Clinical Data Science\n  Conference - PHUSE US Connect 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic evaluations often require predictions of survival rates\nbeyond the follow-up period. Parametric survival models can be more convenient\nfor economic modelling than the Cox model. The generalized gamma (GG) and\ngeneralized F (GF) distributions are extensive families that contain almost all\ncommonly used distributions with various hazard shapes and arbitrary\ncomplexity. In this study, we present a new SAS macro for implementing a wide\nvariety of flexible parametric models including the GG and GF distributions and\ntheir special cases, as well as the Gompertz distribution. Proper custom\ndistributions are also supported. Different from existing SAS procedures, this\nmacro not only supports regression on the location parameter but also on\nancillary parameters, which greatly increases model flexibility. In addition,\nthe SAS macro supports weighted regression, stratified regression and robust\ninference. This study demonstrates with several examples how the SAS macro can\nbe used for flexible survival modeling and extrapolation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 23:24:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fu", "Han", "", "The Ohio State University"], ["Mt-Isa", "Shahrul", "", "MSD"], ["Baumgartner", "Richard", "", "Merck"], ["Malbecq", "William", "", "MSD"]]}, {"id": "2106.14145", "submitter": "Duncan Clark", "authors": "Duncan A. Clark and Mark S. Handcock", "title": "An Approach to Causal Inference over Stochastic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Claiming causal inferences in network settings necessitates careful\nconsideration of the often complex dependency between outcomes for actors. Of\nparticular importance are treatment spillover or outcome interference effects.\nWe consider causal inference when the actors are connected via an underlying\nnetwork structure. Our key contribution is a model for causality when the\nunderlying network is unobserved and the actor covariates evolve stochastically\nover time. We develop a joint model for the relational and covariate generating\nprocess that avoids restrictive separability assumptions and deterministic\nnetwork assumptions that do not hold in the majority of social network settings\nof interest. Our framework utilizes the highly general class of\nExponential-family Random Network models (ERNM) of which Markov Random Fields\n(MRF) and Exponential-family Random Graph models (ERGM) are special cases. We\npresent potential outcome based inference within a Bayesian framework, and\npropose a simple modification to the exchange algorithm to allow for sampling\nfrom ERNM posteriors. We present results of a simulation study demonstrating\nthe validity of the approach. Finally, we demonstrate the value of the\nframework in a case-study of smoking over time in the context of adolescent\nfriendship networks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:09:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Clark", "Duncan A.", ""], ["Handcock", "Mark S.", ""]]}, {"id": "2106.14258", "submitter": "Jianhao Zhang", "authors": "Jianhao Zhang and Yoonkyung Lee", "title": "Sparse Logistic Tensor Decomposition for Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor data are increasingly available in many application domains. We\ndevelop several tensor decomposition methods for binary tensor data. Different\nfrom classical tensor decompositions for continuous-valued data with squared\nerror loss, we formulate logistic tensor decompositions for binary data with a\nBernoulli likelihood. To enhance the interpretability of estimated factors and\nimprove their stability further, we propose sparse formulations of logistic\ntensor decomposition by considering $\\ell_{1}$-norm and $\\ell_{0}$-norm\nregularized likelihood. To handle the resulting optimization problems, we\ndevelop computational algorithms which combine the strengths of tensor power\nmethod and majorization-minimization (MM) algorithm. Through simulation\nstudies, we demonstrate the utility of our methods in analysis of binary tensor\ndata. To illustrate the effectiveness of the proposed methods, we analyze a\ndataset concerning nations and their political relations and perform\nco-clustering of estimated factors to find associations between the nations and\npolitical relations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:22:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Jianhao", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "2106.14345", "submitter": "Jean-Louis Foulley JLF", "authors": "Jean-Louis Foulley", "title": "More on verification of probability forecasts for football outcomes:\n  score decompositions, reliability, and discrimination analyses", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecast of football outcomes in terms of Home Win, Draw and Away Win relies\nlargely on ex ante probability elicitation of these events and ex post\nverification of them via computation of probability scoring rules (Brier,\nRanked Probability, Logarithmic, Zero-One scores). Usually, appraisal of the\nquality of forecasting procedures is restricted to reporting mean score values.\nThe purpose of this article is to propose additional tools of verification,\nsuch as score decompositions into several components of special interest.\nGraphical and numerical diagnoses of reliability and discrimination and kindred\nstatistical methods are presented using different techniques of binning (fixed\nthresholds, quantiles, logistic and iso regression). These procedures are\nillustrated on probability forecasts for the outcomes of the UEFA Champions\nLeague (C1) at the end of the group stage based on typical Poisson regression\nmodels with reasonably good results in terms of reliability as compared to\nthose obtained from bookmaker odds and whatever the technique used. Links with\nresearch in machine learning and different areas of application (meteorology,\nmedicine) are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:21:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Foulley", "Jean-Louis", ""]]}, {"id": "2106.14364", "submitter": "Janie Coulombe", "authors": "Janie Coulombe, Erica E. M. Moodie, Robert W. Platt, Christel Renoux", "title": "Estimation of the marginal effect of antidepressants on body mass index\n  under confounding and endogenous covariate-driven monitoring times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In studying the marginal effect of antidepressants on body mass index using\nelectronic health records data, we face several challenges. Patients'\ncharacteristics can affect the exposure (confounding) as well as the timing of\nroutine visits (measurement process), and those characteristics may be altered\nfollowing a visit which can create dependencies between the monitoring and body\nmass index when viewed as a stochastic or random processes in time. This may\nresult in a form of selection bias that distorts the estimation of the marginal\neffect of the antidepressant. Inverse intensity of visit weights have been\nproposed to adjust for these imbalances, however no approaches have addressed\ncomplex settings where the covariate and the monitoring processes affect each\nother in time so as to induce endogeneity, a situation likely to occur in\nelectronic health records. We review how selection bias due to\noutcome-dependent follow-up times may arise and propose a new cumulated weight\nthat models a complete monitoring path so as to address the above-mentioned\nchallenges and produce a reliable estimate of the impact of antidepressants on\nbody mass index. More specifically, we do so using data from the Clinical\nPractice Research Datalink in the United Kingdom, comparing the marginal effect\nof two commonly used antidepressants, citalopram and fluoxetine, on body mass\nindex. The results are compared to those obtained with simpler methods that do\nnot account for the extent of the dependence due to an endogenous covariate\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:50:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Coulombe", "Janie", ""], ["Moodie", "Erica E. M.", ""], ["Platt", "Robert W.", ""], ["Renoux", "Christel", ""]]}, {"id": "2106.14381", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg and Mitchell Kracman", "title": "Armoured Fighting Vehicle Team Performance Prediction against Missile\n  Attacks with Directed Energy Weapons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent study has introduced a procedure to quantify the survivability of a\nteam of armoured fighting vehicles when it is subjected to a single missile\nattack. In particular this study investigated the concept of collaborative\nactive protection systems, focusing on the case where vehicle defence is\nprovided by high power radio frequency directed energy weapons. The purpose of\nthe current paper is to demonstrate how this analysis can be extended to\naccount for more than one missile threat. This is achieved by introducing a\njump stochastic process whose states represent the number of missiles defeated\nat a given time instant. Analysis proceeds through consideration of the sojourn\ntimes of this stochastic process, and it is shown how consideration of these\njump times can be related to transition probabilities of the auxiliary\nstochastic process. The latter probabilities are then related to the\nprobabilities of detection and disruption of missile threats. The sum of these\nsojourn times can then be used to quantify the survivability of the team at any\ngiven time instant. Due to the fact that there is much interest in the\napplication of high energy lasers in the context of this paper, the numerical\nexamples will thus focus on such directed energy weapons for armoured fighting\nvehicle team defence.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 03:29:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Weinberg", "Graham V.", ""], ["Kracman", "Mitchell", ""]]}, {"id": "2106.14384", "submitter": "Yihuang Kang", "authors": "Yihuang Kang, Yi-Wen Chiu, Ming-Yen Lin, Fang-yi Su, Sheng-Tai Huang", "title": "Towards Model-informed Precision Dosing with Expert-in-the-loop Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) and its applications have been transforming our lives\nbut it is also creating issues related to the development of fair, accountable,\ntransparent, and ethical Artificial Intelligence. As the ML models are not\nfully comprehensible yet, it is obvious that we still need humans to be part of\nalgorithmic decision-making processes. In this paper, we consider a ML\nframework that may accelerate model learning and improve its interpretability\nby incorporating human experts into the model learning loop. We propose a novel\nhuman-in-the-loop ML framework aimed at dealing with learning problems that the\ncost of data annotation is high and the lack of appropriate data to model the\nassociation between the target tasks and the input features. With an\napplication to precision dosing, our experimental results show that the\napproach can learn interpretable rules from data and may potentially lower\nexperts' workload by replacing data annotation with rule representation\nediting. The approach may also help remove algorithmic bias by introducing\nexperts' feedback into the iterative model learning process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 03:45:09 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 03:11:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kang", "Yihuang", ""], ["Chiu", "Yi-Wen", ""], ["Lin", "Ming-Yen", ""], ["Su", "Fang-yi", ""], ["Huang", "Sheng-Tai", ""]]}, {"id": "2106.14436", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor and Ricardo Andrade-Pacheco and Hugh Sturrock and\n  Busiku Hamainza and Kafula Silumbe and John Miller and Thomas P. Eisele and\n  Francois Rerolle and Hannah Slater and Adam Bennett", "title": "Malaria Risk Mapping Using Routine Health System Incidence Data in\n  Zambia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements to Zambia's malaria surveillance system allow better monitoring\nof incidence and targetting of responses at refined spatial scales. As\ntransmission decreases, understanding heterogeneity in risk at fine spatial\nscales becomes increasingly important. However, there are challenges in using\nhealth system data for high-resolution risk mapping: health facilities have\nundefined and overlapping catchment areas, and report on an inconsistent basis.\nWe propose a novel inferential framework for risk mapping of malaria incidence\ndata based on formal down-scaling of confirmed case data reported through the\nhealth system in Zambia. We combine data from large community intervention\ntrials in 2011-2016 and model health facility catchments based upon\ntreatment-seeking behaviours; our model for monthly incidence is an aggregated\nlog-Gaussian Cox process, which allows us to predict incidence at fine scale.\nWe predicted monthly malaria incidence at 5km$^2$ resolution nationally:\nwhereas 4.8 million malaria cases were reported through the health system in\n2016, we estimated that the number of cases occurring at the community level\nwas closer to 10 million. As Zambia continues to scale up community-based\nreporting of malaria incidence, these outputs provide realistic estimates of\ncommunity-level malaria burden as well as high resolution risk maps for\ntargeting interventions at the sub-catchment level.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:40:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Taylor", "Benjamin M.", ""], ["Andrade-Pacheco", "Ricardo", ""], ["Sturrock", "Hugh", ""], ["Hamainza", "Busiku", ""], ["Silumbe", "Kafula", ""], ["Miller", "John", ""], ["Eisele", "Thomas P.", ""], ["Rerolle", "Francois", ""], ["Slater", "Hannah", ""], ["Bennett", "Adam", ""]]}, {"id": "2106.14562", "submitter": "Pascal Fries", "authors": "Pascal Fries (1 and 2) and Eric Maris (2) ((1) Ernst Str\\\"ungmann\n  Institute for Neuroscience in Cooperation with Max Planck Society, Frankfurt,\n  Germany, (2) Donders Institute for Brain, Cognition and Behaviour, Radboud\n  University, Nijmegen, Netherlands)", "title": "What to do if N is two?", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of in-vivo neurophysiology currently uses statistical standards\nthat are based on tradition rather than formal analysis. Typically, data from\ntwo (or few) animals are pooled for one statistical test, or a significant test\nin a first animal is replicated in one (or few) further animals. The use of\nmore than one animal is widely believed to allow an inference on the\npopulation. Here, we explain that a useful inference on the population would\nrequire larger numbers and a different statistical approach. The field should\nconsider to perform studies at that standard, potentially through coordinated\nmulti-center efforts, for selected questions of exceptional importance. Yet,\nfor many questions, this is ethically and/or economically not justifiable. We\nexplain why in those studies with two (or few) animals, any useful inference is\nlimited to the sample of investigated animals, irrespective of whether it is\nbased on few animals, two animals or a single animal.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:32:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fries", "Pascal", "", "1 and 2"], ["Maris", "Eric", ""]]}, {"id": "2106.14641", "submitter": "Kamil Oster", "authors": "Kamil Oster, Stefan G\\\"uttel, Jonathan L. Shapiro, Lu Chen, Megan\n  Jobson", "title": "Pre-treatment of outliers and anomalies in plant data: Methodology and\n  case study of a Vacuum Distillation Unit", "comments": "33 pages, 20 figures, submitted to the Journal of Process Control\n  (ref: JPROCONT-D-21-00332)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data pre-treatment plays a significant role in improving data quality, thus\nallowing extraction of accurate information from raw data. One of the data\npre-treatment techniques commonly used is outliers detection. The so-called\n3${\\sigma}$ method is a common practice to identify the outliers. As shown in\nthe manuscript, it does not identify all outliers, resulting in possible\ndistortion of the overall statistics of the data. This problem can have a\nsignificant impact on further data analysis and can lead to reduction in the\naccuracy of predictive models. There is a plethora of various techniques for\noutliers detection, however, aside from theoretical work, they all require case\nstudy work. Two types of outliers were considered: short-term (erroneous data,\nnoise) and long-term outliers (e.g. malfunctioning for longer periods). The\ndata used were taken from the vacuum distillation unit (VDU) of an Asian\nrefinery and included 40 physical sensors (temperature, pressure and flow\nrate). We used a modified method for 3${\\sigma}$ thresholds to identify the\nshort-term outliers, i.e. ensors data are divided into chunks determined by\nchange points and 3${\\sigma}$ thresholds are calculated within each chunk\nrepresenting near-normal distribution. We have shown that piecewise 3${\\sigma}$\nmethod offers a better approach to short-term outliers detection than\n3${\\sigma}$ method applied to the entire time series. Nevertheless, this does\nnot perform well for long-term outliers (which can represent another state in\nthe data). In this case, we used principal component analysis (PCA) with\nHotelling's $T^2$ statistics to identify the long-term outliers. The results\nobtained with PCA were subject to DBSCAN clustering method. The outliers (which\nwere visually obvious and correctly detected by the PCA method) were also\ncorrectly identified by DBSCAN which supported the consistency and accuracy of\nthe PCA method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:17:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Oster", "Kamil", ""], ["G\u00fcttel", "Stefan", ""], ["Shapiro", "Jonathan L.", ""], ["Chen", "Lu", ""], ["Jobson", "Megan", ""]]}, {"id": "2106.14667", "submitter": "Maryam Akbari-Moghaddam", "authors": "Maryam Akbari-Moghaddam, Na Li, Douglas G. Down, Donald M. Arnold,\n  Jeannie Callum, Philippe B\\'egin, Nancy M. Heddle", "title": "Data-driven Fair Resource Allocation For Novel Emerging Epidemics: A\n  COVID-19 Convalescent Plasma Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epidemics are a serious public health threat, and the resources for\nmitigating their effects are typically limited. Decision-makers face challenges\nin forecasting the demand for these resources as prior information about the\ndisease is often not available, the behaviour of the disease can periodically\nchange (either naturally or as a result of public health policies) and differs\nby geographical region. In this work, we discuss a model that is suitable for\nshort-term real-time supply and demand forecasting during emerging outbreaks\nwithout having to rely on demographic information. We propose a data-driven\nmixed-integer programming (MIP) resource allocation model that assigns\navailable resources to maximize a notion of fairness among the\nresource-demanding entities. Numerical results from applying our MIP model to a\nCOVID-19 Convalescent Plasma (CCP) case study suggest that our approach can\nhelp balance the supply and demand of limited products such as CCP and minimize\nthe unmet demand ratios of the demand entities.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 00:00:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Akbari-Moghaddam", "Maryam", ""], ["Li", "Na", ""], ["Down", "Douglas G.", ""], ["Arnold", "Donald M.", ""], ["Callum", "Jeannie", ""], ["B\u00e9gin", "Philippe", ""], ["Heddle", "Nancy M.", ""]]}, {"id": "2106.14841", "submitter": "Fotis Kopsaftopoulos", "authors": "Ahmad Amer and Fotis Kopsaftopoulos", "title": "Gaussian Process Regression for Active Sensing Probabilistic Structural\n  Health Monitoring: Experimental Assessment Across Multiple Damage and Loading\n  Scenarios", "comments": "47 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the near future, Structural Health Monitoring (SHM) technologies will be\ncapable of overcoming the drawbacks in the current maintenance and life-cycle\nmanagement paradigms, namely: cost, increased downtime, less-than-optimal\nsafety management paradigm and the limited applicability of fully-autonomous\noperations. In the context of SHM, one of the most challenging tasks is damage\nquantification. Current methods face accuracy and/or robustness issues when it\ncomes to varying operating and environmental conditions. In addition, the\ndamage/no-damage paradigm of current frameworks does not offer much information\nto maintainers on the ground for proper decision-making. In this study, a novel\nstructural damage quantification framework is proposed based on widely-used\nDamage Indices (DIs) and Gaussian Process Regression Models (GPRMs). The\nnovelty lies in calculating the probability of an incoming test DI point\noriginating from a specific state, which allows for probability-educated\ndecision-making. This framework is applied to three test cases: a Carbon\nFiber-Reinforced Plastic (CFRP) coupon with attached weights as simulated\ndamage, an aluminum coupon with a notch, and an aluminum coupon with attached\nweights as simulated damage under varying loading states. The state prediction\nmethod presented herein is applied to single-state quantification in the first\ntwo test cases, as well as the third one assuming the loading state is known.\nFinally, the proposed method is applied to the third test case assuming neither\nthe damage size nor the load is known in order to predict both simultaneously\nfrom incoming DI test points. In applying this framework, two forms of GPRMs\n(standard and variational heteroscedastic) are used in order to critically\nassess their performances with respect to the three test cases.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:38:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Amer", "Ahmad", ""], ["Kopsaftopoulos", "Fotis", ""]]}, {"id": "2106.15051", "submitter": "Zhuoqun Wang", "authors": "Zhuoqun Wang, Jialiang Mao, and Li Ma", "title": "Logistic-tree normal model for microbiome compositions", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic model, called the \"logistic-tree normal\" (LTN),\nfor microbiome compositional data. The LTN marries two popular classes of\nmodels -- the logistic-normal (LN) and the Dirichlet-tree (DT) -- and inherits\nthe key benefits of both. LN models are flexible in characterizing rich\ncovariance structure among taxa but can be computationally prohibitive in face\nof high dimensionality (i.e., when the number of taxa is large) due to its lack\nof conjugacy to the multinomial sampling model. On the other hand, DT avoids\nthis issue by decomposing the multinomial sampling model into a collection of\nbinomials, one at each split of the phylogenetic tree of the taxa, and adopting\na conjugate beta model for each binomial probability, but at the same time the\nDT incurs restrictive covariance among the taxa. In contrast, the LTN model\ndecomposes the multinomial model into binomials as the DT does, but it jointly\nmodels the corresponding binomial probabilities using a (multivariate) LN\ndistribution instead of betas. It therefore allows rich covariance structures\nas the LN models, while the decomposition of the multinomial likelihood allows\nconjugacy to be restored through the P\\'olya-Gamma augmentation. Accordingly,\nBayesian inference on the LTN model can readily proceed by Gibbs sampling.\nMoreover, the multivariate Gaussian aspect of the model allows common\ntechniques for effective inference on high-dimensional data -- such as those\nbased on sparsity and low-rank assumptions in the covariance structure -- to be\nreadily incorporated. Depending on the goal of the analysis, the LTN model can\nbe used either as a standalone model or embedded into more sophisticated\nmodels. We demonstrate its use in estimating taxa covariance and in\nmixed-effects modeling. Finally, we carry out a case study using an LTN-based\nmixed-effects model to analyze a longitudinal dataset from the DIABIMMUNE\nproject.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:42:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Zhuoqun", ""], ["Mao", "Jialiang", ""], ["Ma", "Li", ""]]}, {"id": "2106.15074", "submitter": "Ye Wang", "authors": "Ye Wang", "title": "Causal Inference under Temporal and Spatial Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social events and policies generate spillover effects in both time and\nspace. Their occurrence influences not only the outcomes of interest in the\nfuture, but also these outcomes in nearby areas. In this paper, we propose a\ndesign-based approach to estimate the direct and indirect/spillover treatment\neffects of any event or policy under the assumption of sequential ignorability,\nwhen both temporal and spatial interference are allowed to present. The\nproposed estimators are shown to be consistent and asymptotically Normal if the\ndegree of interference dependence does not grow too fast relative to the sample\nsize. The conventional difference-in-differences (DID) or two-way fixed effects\nmodel, nevertheless, leads to biased estimates in this scenario. We apply the\nmethod to examine the impact of Hong Kong's Umbrella Movement on the result of\nthe ensuing election and how an institutional reform affects real estate\nassessment in New York State.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:47:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Ye", ""]]}, {"id": "2106.15076", "submitter": "Ye Wang", "authors": "Dana Burde, Joel Middleton, Cyrus Samii and Ye Wang", "title": "How to Account for Alternatives When Comparing Effects: Revisiting\n  'Bringing Education to Afghan Girls'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a \"principal strata\" approach to decompose treatment effects\nand interpret why a schooling intervention that yielded exceptional initial\neffects yielded substantially smaller effects in a replication years later. The\nspecific application is a set of 2008 and 2015 replications of an intervention\naiming to increase primary education for girls in rural Afghanistan. The\nintervention offers a new schooling option, and as such, its effects depend on\nhow individuals use alternatives that already exist. The principal strata\napproach accounts variation in use patterns when comparing effects across the\nreplications. Our findings show that even though the share of girls for whom\nthe intervention would be valuable dropped considerably in 2015 as compared to\n2008, the intervention was even more efficaciousness for those who continued to\nbenefit from it.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:53:53 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Burde", "Dana", ""], ["Middleton", "Joel", ""], ["Samii", "Cyrus", ""], ["Wang", "Ye", ""]]}, {"id": "2106.15081", "submitter": "Ye Wang", "authors": "Peter M. Aronow, Cyrus Samii, Jonathan Sullivan, and Ye Wang", "title": "Inference in Spatial Experiments with Interference using the\n  SpatialEffect Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents methods for analyzing spatial experiments when complex\nspillovers, displacement effects, and other types of \"interference\" are\npresent. We present a robust, design-based approach to analyzing effects in\nsuch settings. The design-based approach derives inferential properties for\ncausal effect estimators from known features of the experimental design, in a\nmanner analogous to inference in sample surveys. The methods presented here\ntarget a quantity of interest called the \"average marginalized response,\" which\nis equal to the average effect of activating a treatment at an intervention\npoint that is a given distance away, averaging ambient effects emanating from\nother intervention points. We provide a step-by-step tutorial based on the\nSpatialEffect package for R. We apply the methods to a randomized experiment on\npayments for community forest conservation in Uganda, showing how our methods\nreveal possibly substantial spatial spillovers that more conventional analyses\ncannot detect.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:09:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Sullivan", "Jonathan", ""], ["Wang", "Ye", ""]]}, {"id": "2106.15168", "submitter": "Ion Gabriel Ion", "authors": "Ion Gabriel Ion, Melvin Liebsch, Abele Simona, Dimitrios Loukrezis,\n  Carlo Petrone, Stephan Russenschuck, Herbert De Gersem, Sebastian Sch\\\"ops", "title": "Local field reconstruction from rotating coil measurements in particle\n  accelerator magnets", "comments": null, "journal-ref": null, "doi": "10.1016/j.nima.2021.165580", "report-no": null, "categories": "physics.acc-ph stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper a general approach to reconstruct three dimensional field\nsolutions in particle accelerator magnets from distributed magnetic\nmeasurements is presented. To exploit the locality of the measurement operation\na special discretization of the Laplace equation is used. Extracting the\ncoefficients of the field representations yields an inverse problem which is\nsolved by Bayesian inversion. This allows not only to pave the way for\nuncertainty quantification, but also to derive a suitable regularization. The\napproach is applied to rotating coil measurements and can be extended to any\nother measurement procedure.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:29:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ion", "Ion Gabriel", ""], ["Liebsch", "Melvin", ""], ["Simona", "Abele", ""], ["Loukrezis", "Dimitrios", ""], ["Petrone", "Carlo", ""], ["Russenschuck", "Stephan", ""], ["De Gersem", "Herbert", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "2106.15285", "submitter": "Sam Royston", "authors": "Sam Royston, Ben Greenberg, Omeed Tavasoli, Courtenay Cotton", "title": "Anomaly Detection and Automated Labeling for Voter Registration File\n  Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Voter eligibility in United States elections is determined by a patchwork of\nstate databases containing information about which citizens are eligible to\nvote. Administrators at the state and local level are faced with the\nexceedingly difficult task of ensuring that each of their jurisdictions is\nproperly managed, while also monitoring for improper modifications to the\ndatabase. Monitoring changes to Voter Registration Files (VRFs) is crucial,\ngiven that a malicious actor wishing to disrupt the democratic process in the\nUS would be well-advised to manipulate the contents of these files in order to\nachieve their goals. In 2020, we saw election officials perform admirably when\nfaced with administering one of the most contentious elections in US history,\nbut much work remains to secure and monitor the election systems Americans rely\non. Using data created by comparing snapshots taken of VRFs over time, we\npresent a set of methods that make use of machine learning to ease the burden\non analysts and administrators in protecting voter rolls. We first evaluate the\neffectiveness of multiple unsupervised anomaly detection methods in detecting\nVRF modifications by modeling anomalous changes as sparse additive noise. In\nthis setting we determine that statistical models comparing administrative\ndistricts within a short time span and non-negative matrix factorization are\nmost effective for surfacing anomalous events for review. These methods were\ndeployed during 2019-2020 in our organization's monitoring system and were used\nin collaboration with the office of the Iowa Secretary of State. Additionally,\nwe propose a newly deployed model which uses historical and demographic\nmetadata to label the likely root cause of database modifications. We hope to\nuse this model to predict which modifications have known causes and therefore\nbetter identify potentially anomalous modifications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:48:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Royston", "Sam", ""], ["Greenberg", "Ben", ""], ["Tavasoli", "Omeed", ""], ["Cotton", "Courtenay", ""]]}, {"id": "2106.15298", "submitter": "Yangxin Fan", "authors": "Yuan Wang and Yangxin Fan", "title": "US Fatal Police Shooting Analysis and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We believe that \"all men are created equal\". With the rise of the police\nshootings reported by media, more people in the U.S. think that police use\nexcessive force during law enforcement, especially to a specific group of\npeople. We want to apply multidimensional statistical analysis to reveal more\nfacts than the monotone mainstream media. Our paper has three parts. First, we\nproposed a new method to quantify fatal police shooting news reporting\ndeviation of mainstream media, which includes CNN, FOX, ABC, and NBC. Second,\nwe analyzed the most comprehensive US fatal police shooting dataset from\nWashington Post. We used FP-growth to reveal the frequent patterns and DBSCAN\nclustering to find fatal shooting hotspots. We brought multi-attributes (social\neconomics, demographics, political tendency, education, gun ownership rate,\npolice training hours, etc.) to reveal connections under the iceberg. We found\nthat the police shooting rate of a state depends on many variables. The top\nfour most relevant attributes were state joined year, state land area, gun\nownership rate, and violent crime rate. Third, we proposed four regression\nmodels to predict police shooting rates at the state level. The best model\nKstar could predict the fatal police shooting rate with about 88.53%\ncorrelation coefficient. We also proposed classification models, including\nGradient Boosting Machine, Multi-class Classifier, Logistic Regression, and\nNaive Bayes Classifier, to predict the race of fatal police shooting victims.\nOur classification models show no significant evidence to conclude that racial\ndiscrimination happened during fatal police shootings recorded by the WP\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:39:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Yuan", ""], ["Fan", "Yangxin", ""]]}, {"id": "2106.15354", "submitter": "Qihuang Zhang", "authors": "Qihuang Zhang, Grace Y. Yi, Li-Pang Chen and Wenqing He", "title": "Text mining and sentiment analysis of COVID-19 tweets", "comments": "20 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human severe acute respiratory syndrome coronavirus 2 (SARS-Cov-2),\ncausing the COVID-19 disease, has continued to spread all over the world. It\nmenacingly affects not only public health and global economics but also mental\nhealth and mood. While the impact of the COVID-19 pandemic has been widely\nstudied, relatively fewer discussions about the sentimental reaction of the\npopulation have been available. In this article, we scrape COVID-19 related\ntweets on the microblogging platform, Twitter, and examine the tweets from\nFeb~24, 2020 to Oct~14, 2020 in four Canadian cities (Toronto, Montreal,\nVancouver, and Calgary) and four U.S. cities (New York, Los Angeles, Chicago,\nand Seattle). Applying the Vader and NRC approaches, we evaluate the sentiment\nintensity scores and visualize the information over different periods of the\npandemic. Sentiment scores for the tweets concerning three anti-epidemic\nmeasures, masks, vaccine, and lockdown, are computed for comparisons. The\nresults of four Canadian cities are compared with four cities in the United\nStates. We study the causal relationships between the infected cases, the tweet\nactivities, and the sentiment scores of COVID-19 related tweets, by integrating\nthe echo state network method with convergent cross-mapping. Our analysis shows\nthat public sentiments regarding COVID-19 vary in different time periods and\nlocations. In general, people have a positive mood about COVID-19 and masks,\nbut negative in the topics of vaccine and lockdown. The causal inference shows\nthat the sentiment influences people's activities on Twitter, which is also\ncorrelated to the daily number of infections.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 13:24:27 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Qihuang", ""], ["Yi", "Grace Y.", ""], ["Chen", "Li-Pang", ""], ["He", "Wenqing", ""]]}, {"id": "2106.15381", "submitter": "Spencer Thomas", "authors": "Spencer A. Thomas", "title": "Place of Occurrence of COVID-19 Deaths in the UK: Modelling and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analysed publicly available data on place of occurrence of COVID-19 deaths\nfrom national statistical agencies in the UK between March 9 2020 and February\n28 2021. We introduce a modified Weibull model that describes the deaths due to\nCOVID-19 at a national and place of occurrence level. We observe similar trends\nin the UK where deaths due to COVID-19 first peak in Homes, followed by\nHospitals and Care Homes 1-2 weeks later in the first and second waves. This is\nin line with the infectious period of the disease, indicating a possible\ntransmission vehicle between the settings. Our results show that the first wave\nis characterised by fast growth and a slow reduction after the peak in deaths\ndue to COVID-19. The second and third waves have the converse property, with\nslow growth and a rapid decrease from the peak. This difference may result from\nbehavioural changes in the population (social distancing, masks, etc). Finally,\nwe introduce a double logistic model to describe the dynamic proportion of\nCOVID-19 deaths occurring in each setting. This analysis reveals that the\nproportion of COVID-19 deaths occurring in Care Homes increases from the start\nof the pandemic and past the peak in total number of COVID-19 deaths in the\nfirst wave. After the catastrophic impact in the first wave, the proportion of\nCOVID-19 deaths occurring in Care Homes gradually decreased from is maximum\nafter the first wave indicating residence were better protected in the second\nand third waves compared to the first.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:10:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Thomas", "Spencer A.", ""]]}, {"id": "2106.15528", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "Damping effect in innovation processes: case studies from Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the innovation process, that is the underlying mechanisms\nthrough which novelties emerge, diffuse and trigger further novelties is\nundoubtedly of fundamental importance in many areas (biology, linguistics,\nsocial science and others). The models introduced so far satisfy the Heaps'\nlaw, regarding the rate at which novelties appear, and the Zipf's law, that\nstates a power law behavior for the frequency distribution of the elements.\nHowever, there are empirical cases far from showing a pure power law behavior\nand such a deviation is present for elements with high frequencies. We explain\nthis phenomenon by means of a suitable \"damping\" effect in the probability of a\nrepetition of an old element. While the proposed model is extremely general and\nmay be also employed in other contexts, it has been tested on some Twitter data\nsets and demonstrated great performances with respect to Heaps' law and, above\nall, with respect to the fitting of the frequency-rank plots for low and high\nfrequencies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:57:59 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "2106.15698", "submitter": "Sergio Consoli", "authors": "Sergio Consoli and Luca Tiozzo Pezzoli and Elisa Tosetti", "title": "Emotions in Macroeconomic News and their Impact on the European Bond\n  Market", "comments": "Journal of International Money and Finance (to appear); 39 pages; 14\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG physics.soc-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how emotions extracted from macroeconomic news can be used to explain\nand forecast future behaviour of sovereign bond yield spreads in Italy and\nSpain. We use a big, open-source, database known as Global Database of Events,\nLanguage and Tone to construct emotion indicators of bond market affective\nstates. We find that negative emotions extracted from news improve the\nforecasting power of government yield spread models during distressed periods\neven after controlling for the number of negative words present in the text. In\naddition, stronger negative emotions, such as panic, reveal useful information\nfor predicting changes in spread at the short-term horizon, while milder\nemotions, such as distress, are useful at longer time horizons. Emotions\ngenerated by the Italian political turmoil propagate to the Spanish news\naffecting this neighbourhood market.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:15:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Consoli", "Sergio", ""], ["Pezzoli", "Luca Tiozzo", ""], ["Tosetti", "Elisa", ""]]}, {"id": "2106.15730", "submitter": "Indranil Sahoo", "authors": "Indranil Sahoo and Arnab Hazra", "title": "Contamination mapping in Bangladesh using a multivariate spatial\n  Bayesian model for left-censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arsenic (As) and other toxic elements contamination of groundwater in\nBangladesh poses a major threat to millions of people on a daily basis.\nUnderstanding complex relationships between arsenic and other elements can\nprovide useful insights for mitigating arsenic poisoning in drinking water and\nrequires multivariate modeling of the elements. However, environmental\nmonitoring of such contaminants often involves a substantial proportion of\nleft-censored observations falling below a minimum detection limit (MDL). This\nproblem motivates us to propose a multivariate spatial Bayesian model for\nleft-censored data for investigating the abundance of arsenic in Bangladesh\ngroundwater and for creating spatial maps of the contaminants. Inference about\nthe model parameters is drawn using an adaptive Markov Chain Monte Carlo (MCMC)\nsampling. The computation time for the proposed model is of the same order as a\nmultivariate Gaussian process model that does not impute the censored values.\nThe proposed method is applied to the arsenic contamination dataset made\navailable by the Bangladesh Water Development Board (BWDB). Spatial maps of\narsenic, barium (Ba), and calcium (Ca) concentrations in groundwater are\nprepared using the posterior predictive means calculated on a fine lattice over\nBangladesh. Our results indicate that Chittagong and Dhaka divisions suffer\nfrom excessive concentrations of arsenic and only the divisions of Rajshahi and\nRangpur have safe drinking water based on recommendations by the World Health\nOrganization (WHO).\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 21:35:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sahoo", "Indranil", ""], ["Hazra", "Arnab", ""]]}, {"id": "2106.15737", "submitter": "Laura Balzer PhD", "authors": "Laura B. Balzer, Mark van der Laan, James Ayieko, Moses Kamya, Gabriel\n  Chamie, Joshua Schwab, Diane V. Havlir, Maya L. Petersen", "title": "Two-Stage TMLE to Reduce Bias and Improve Efficiency in Cluster\n  Randomized Trials", "comments": "32 pages (16.5 pgs of main text); 1 figure; 3 main tables; 3 supp\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cluster randomized trials (CRTs) randomly assign an intervention to groups of\nindividuals (e.g., clinics or communities), and measure outcomes on individuals\nin those groups. While offering many advantages, this experimental design\nintroduces challenges that are only partially addressed by existing analytic\napproaches. First, outcomes are often missing for some individuals within\nclusters. Failing to appropriately adjust for differential outcome measurement\ncan result in biased estimates and inference. Second, CRTs often randomize\nlimited numbers of clusters, resulting in chance imbalances on baseline outcome\npredictors between arms. Failing to adaptively adjust for these imbalances and\nother predictive covariates can result in efficiency losses. To address these\nmethodological gaps, we propose and evaluate a novel two-stage targeted minimum\nloss-based estimator (TMLE) to adjust for baseline covariates in a manner that\noptimizes precision, after controlling for baseline and post-baseline causes of\nmissing outcomes. Finite sample simulations illustrate that our approach can\nnearly eliminate bias due to differential outcome measurement, while other\ncommon CRT estimators yield misleading results and inferences. Application to\nreal data from the SEARCH community randomized trial demonstrates the gains in\nefficiency afforded through adaptive adjustment for cluster-level covariates,\nafter controlling for missingness on individual-level outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 21:47:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Balzer", "Laura B.", ""], ["van der Laan", "Mark", ""], ["Ayieko", "James", ""], ["Kamya", "Moses", ""], ["Chamie", "Gabriel", ""], ["Schwab", "Joshua", ""], ["Havlir", "Diane V.", ""], ["Petersen", "Maya L.", ""]]}, {"id": "2106.15767", "submitter": "Xian Li", "authors": "Xian Li", "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:14:59 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Xian", ""]]}, {"id": "2106.15988", "submitter": "Stratis Tsirtsis", "authors": "Stratis Tsirtsis, Abir De, Lars Lorch, Manuel Gomez-Rodriguez", "title": "Group Testing under Superspreading Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing is recommended for all close contacts of confirmed COVID-19 patients.\nHowever, existing group testing methods are oblivious to the circumstances of\ncontagion provided by contact tracing. Here, we build upon a well-known\nsemi-adaptive pool testing method, Dorfman's method with imperfect tests, and\nderive a simple group testing method based on dynamic programming that is\nspecifically designed to use the information provided by contact tracing.\nExperiments using a variety of reproduction numbers and dispersion levels,\nincluding those estimated in the context of the COVID-19 pandemic, show that\nthe pools found using our method result in a significantly lower number of\ntests than those found using standard Dorfman's method, especially when the\nnumber of contacts of an infected individual is small. Moreover, our results\nshow that our method can be more beneficial when the secondary infections are\nhighly overdispersed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:27:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tsirtsis", "Stratis", ""], ["De", "Abir", ""], ["Lorch", "Lars", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "2106.15997", "submitter": "Yuming Zhang", "authors": "Yuming Zhang, Davide A. Cucci, Roberto Molinari, St\\'ephane Guerrier", "title": "Scale-wise Variance Minimization for Optimal Virtual Signals: An\n  Approach for Redundant Gyroscopes", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increased use of low-cost gyroscopes within inertial sensors for\nnavigation purposes, among others, has brought to the development of a\nconsiderable amount of research in improving their measurement precision. Aside\nfrom developing methods that allow to model and account for the deterministic\nand stochastic components that contribute to the measurement errors of these\ndevices, an approach that has been put forward in recent years is to make use\nof arrays of such sensors in order to combine their measurements thereby\nreducing the impact of individual sensor noise. Nevertheless combining these\nmeasurements is not straightforward given the complex stochastic nature of\nthese errors and, although some solutions have been suggested, these are\nlimited to certain specific settings which do not allow to achieve solutions in\nmore general and common circumstances. Hence, in this work we put forward a\nnon-parametric method that makes use of the wavelet cross-covariance at\ndifferent scales to combine the measurements coming from an array of gyroscopes\nin order to deliver an optimal measurement signal without needing any\nassumption on the processes underlying the individual error signals. We also\nstudy an appropriate non-parametric approach for the estimation of the\nasymptotic covariance matrix of the wavelet cross-covariance estimator which\nhas important applications beyond the scope of this work. The theoretical\nproperties of the proposed approach are studied and are supported by\nsimulations and real applications, indicating that this method represents an\nappropriate and general tool for the construction of optimal virtual signals\nthat are particularly relevant for arrays of gyroscopes. Moreover, our results\ncan support the creation of optimal signals for other types of inertial sensors\nother than gyroscopes as well as for redundant measurements in other domains\nother than navigation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:41:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhang", "Yuming", ""], ["Cucci", "Davide A.", ""], ["Molinari", "Roberto", ""], ["Guerrier", "St\u00e9phane", ""]]}, {"id": "2106.16022", "submitter": "Jamil Ownuk", "authors": "Jamil Ownuk and Ahmad Nezakati and Hossein Baghishani", "title": "Developing flexible classes of distributions to account for both\n  skewness and bimodality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop two novel approaches for constructing skewed and bimodal flexible\ndistributions that can effectively generalize classical symmetric\ndistributions. We illustrate the application of introduced techniques by\nextending normal, student-t, and Laplace distributions. We also study the\nproperties of the newly constructed distributions. The method of maximum\nlikelihood is proposed for estimating the model parameters. Furthermore, the\napplication of new distributions is represented using real-life data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:39:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ownuk", "Jamil", ""], ["Nezakati", "Ahmad", ""], ["Baghishani", "Hossein", ""]]}, {"id": "2106.16070", "submitter": "M\\'arton Karsai", "authors": "Gergely \\'Odor, Domonkos Czifra, J\\'ulia Komj\\'athy, L\\'aszl\\'o\n  Lov\\'asz and M\\'arton Karsai", "title": "Switchover phenomenon induced by epidemic seeding on geometric networks", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.app-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is a fundamental question in disease modelling how the initial seeding of\nan epidemic, spreading over a network, determines its final outcome. Research\nin this topic has primarily concentrated on finding the seed configuration\nwhich infects the most individuals. Although these optimal configurations give\ninsight into how the initial state affects the outcome of an epidemic, they are\nunlikely to occur in real life. In this paper we identify two important seeding\nscenarios, both motivated by historical data, that reveal a new complex\nphenomenon. In one scenario, the seeds are concentrated on the central nodes of\na network, while in the second, they are spread uniformly in the population.\nComparing the final size of the epidemic started from these two initial\nconditions through data-driven and synthetic simulations on real and modelled\ngeometric metapopulation networks, we find evidence for a switchover\nphenomenon: When the basic reproduction number $R_0$ is close to its critical\nvalue, more individuals become infected in the first seeding scenario, but for\nlarger values of $R_0$, the second scenario is more dangerous. We find that the\nswitchover phenomenon is amplified by the geometric nature of the underlying\nnetwork, and confirm our results via mathematically rigorous proofs, by mapping\nthe network epidemic processes to bond percolation. Our results expand on the\nprevious finding that in case of a single seed, the first scenario is always\nmore dangerous, and further our understanding why the sizes of consecutive\nwaves can differ even if their epidemic characters are similar.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:57:59 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["\u00d3dor", "Gergely", ""], ["Czifra", "Domonkos", ""], ["Komj\u00e1thy", "J\u00falia", ""], ["Lov\u00e1sz", "L\u00e1szl\u00f3", ""], ["Karsai", "M\u00e1rton", ""]]}, {"id": "2106.16124", "submitter": "Emmett Kendall", "authors": "Emmett Kendall, Brenden Beck, Joseph Antonelli", "title": "Robust inference for geographic regression discontinuity designs:\n  assessing the impact of police precincts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variation in policing outcomes attributable to differential policing\npractices in New York City (NYC) using geographic regression discontinuity\ndesigns. By focusing on small geographic windows near police precinct\nboundaries we can estimate local average treatment effects of precincts on\narrest rates. The standard geographic regression discontinuity design relies on\ncontinuity assumptions of the potential outcome surface or a local\nrandomization assumption within a window around the boundary. While these\nassumptions are often thought to be more realistic than other assumptions used\nto infer causality from observational data, they can easily be violated in\nrealistic applications. We develop a novel and robust approach to testing\nwhether there are differences in policing outcomes that are caused by\ndifferences in police precincts across NYC. In particular, our test is robust\nto violations of the assumptions traditionally made in geographic regression\ndiscontinuity designs and is valid under much weaker assumptions. We use a\nunique form of resampling to identify new geographic boundaries that are known\nto have no treatment effect, which provides a valid estimate of our estimator's\nnull distribution even under violations of standard assumptions. We find that\nthis procedure gives substantially different results in the analysis of NYC\narrest rates than those that rely on standard assumptions, thereby providing\nmore robust estimates of the nature of the effect of police precincts on arrest\nrates in NYC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:20:46 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kendall", "Emmett", ""], ["Beck", "Brenden", ""], ["Antonelli", "Joseph", ""]]}]