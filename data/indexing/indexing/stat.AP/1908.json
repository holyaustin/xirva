[{"id": "1908.00123", "submitter": "Marcel M{\\l}y\\'nczak", "authors": "Marcel M{\\l}y\\'nczak", "title": "Temporal orders and causal vector for physiological data analysis", "comments": "Accepted for the 42nd IEEE Annual International Conference of the\n  IEEE Engineering in Medicine and Biology Society, EMBC'20; 4 pages, 3\n  figures, 5 tables, 14 references", "journal-ref": "Proceedings of IEEE 2020", "doi": "10.1109/EMBC44109.2020.9176842", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to the global parameter- and time-series-based approaches,\nphysiological analyses should constitute a local temporal one, particularly\nwhen analyzing data within protocol segments. Hence, we introduce the R package\nimplementing the estimation of temporal orders with a causal vector (CV). It\nmay use linear modeling or time series distance. The algorithm was tested on\ncardiorespiratory data comprising tidal volume and tachogram curves, obtained\nfrom elite athletes (supine and standing, in static conditions) and a control\ngroup (different rates and depths of breathing, while supine). We checked the\nrelation between CV and body position or breathing style. The rate of breathing\nhad a greater impact on the CV than does the depth. The tachogram curve\npreceded the tidal volume relatively more when breathing was slower. Clinical\nrelevance - The method can assess (1) relationships between two signals, having\none of the two time-shifted in a given range, (2) curves of most optimal\ninter-signal shift, and (c) their stability across time; also for other\nphysiological studies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 22:24:23 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 12:03:13 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["M\u0142y\u0144czak", "Marcel", ""]]}, {"id": "1908.00141", "submitter": "Sven Serneels", "authors": "Sven Serneels", "title": "Projection pursuit based generalized betas accounting for higher order\n  co-moment effects in financial market analysis", "comments": null, "journal-ref": "In: JSM Proceedings, Business and Economic Statistics Section,\n  2019. Alexandria, VA: American Statistical Association. 3009-3035", "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betas are possibly the most frequently applied tool to analyze how securities\nrelate to the market. While in very widespread use, betas only express dynamics\nderived from second moment statistics. Financial returns data often deviate\nfrom normal assumptions in the sense that they have significant third and\nfourth order moments and contain outliers. This paper targets to introduce a\nway to calculate generalized betas that also account for higher order moment\neffects, while maintaining the conceptual simplicity and interpretability of\nbetas. Thereunto, the co-moment analysis projection index (CAPI) is introduced.\nWhen applied as a projection index in the projection pursuit (PP) framework,\ngeneralized betas are obtained as the directions optimizing the CAPI objective.\nA version of CAPI based on trimmed means is introduced as well, which is more\nstable in the presence of outliers. Simulation results underpin the statistical\nproperties of all projections and a small, yet highly illustrative example is\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 23:27:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Serneels", "Sven", ""]]}, {"id": "1908.00160", "submitter": "Arman Adibi", "authors": "Mohammad Mahdi Naghsh, Maryam Masjedi, Arman Adibi, and Petre Stoica", "title": "Max-Min Fairness Design for MIMO Interference Channels: a\n  Minorization-Maximization Approach", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2929470", "report-no": null, "categories": "eess.SP cs.SI cs.SY eess.SY math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of linear precoder (beamformer) design in a\nmultiple-input multiple-output interference channel (MIMO-IC). The aim is to\ndesign the transmit covariance matrices in order to achieve max-min utility\nfairness for all users. The corresponding optimization problem is non-convex\nand NP-hard in general. We devise an efficient algorithm based on the\nminorization-maximization (MM) technique to obtain quality solutions to this\nproblem. The proposed method solves a second-order cone convex program (SOCP)\nat each iteration. We prove that the devised method converges to stationary\npoints of the problem. We also extend our algorithm to the case where there are\nuncertainties in the noise covariance matrices or channel state information\n(CSI). Simulation results show the effectiveness of the proposed method\ncompared with its main competitor.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 00:55:31 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 15:29:30 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Naghsh", "Mohammad Mahdi", ""], ["Masjedi", "Maryam", ""], ["Adibi", "Arman", ""], ["Stoica", "Petre", ""]]}, {"id": "1908.00255", "submitter": "Akarsh Asoka", "authors": "Akarsh Asoka (1) and Vimal Mishra (1)", "title": "Groundwater pumping to increase food production causes persistent\n  groundwater drought in India", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid groundwater depletion in India is a sustainability challenge. However,\nthe crucial role of climate and groundwater pumping on persisting groundwater\ndrought remains unrecognized. Using the data from Gravity recovery climate\nexperiment (GRACE) satellites and more than 5000 observational wells, here we\nshow that the increase in precipitation in northwest India (NWI) no longer\nhelps to recover from groundwater drought that started after 2012. Groundwater\nstorage anomaly (GWSA) from the GRACE well observations is strongly linked with\naccumulated precipitation for 153, 105, and 18 months for NWI, northcentral\n(NCI), and south India (SI). Precipitation and GWSA have decoupled in NWI after\n2012 indicating the higher influence of groundwater pumping for crop production\nthan climate. The relative contribution of vegetation growth (R2=0.26) on GWSA\nis higher than precipitation (R2=0.02) for 2002-2016 in NWI than in NCI and SI.\nOur findings highlight the urgent need of reducing groundwater pumping in\nIndia.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:11:56 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Asoka", "Akarsh", ""], ["Mishra", "Vimal", ""]]}, {"id": "1908.00431", "submitter": "Zachary Mullen", "authors": "Zachary Mullen, Ashton Wiens, Eric Vance, Henry Lovejoy", "title": "Mapping the uncertainty of 19th century West African slave origins using\n  a Markov decision process model", "comments": "Still in progress, especially citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of modern computers has added an increased emphasis on channeling\ncomputational power and statistical methods into digital humanities. Including\nincreased statistical rigor in history poses unique challenges due to the\ninherent uncertainties of word-of-mouth and poorly recorded data. African\ngenealogies form an important such example, both in terms of individual\nancestries and broader historical context in the absence of written records.\nOur project aims to bridge the lack of accurate maps of Africa during the\ntrans-Atlantic slave trade with the personalized question of where within\nAfrica an individual slave may have hailed. We approach this question with a\ntwo part mathematical model informed by two primary sets of data. We begin with\na conflict intensity surface which can generate capture locations of\ntheoretical slaves, and accompany this with a Markov decision process which\nmodels the transport of these slaves through existing cities to the coastal\nareas. Ultimately, we can use this two-step approach of providing capture\nlocations to a historical trade network in a simulative fashion to generate and\nvisualize the conditional probability of a slave coming from a certain spatial\nregion given they were sold at a certain port. This is a data-driven visual\nanswer to the research question of where the slaves departing these ports\noriginated.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:28:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Mullen", "Zachary", ""], ["Wiens", "Ashton", ""], ["Vance", "Eric", ""], ["Lovejoy", "Henry", ""]]}, {"id": "1908.00520", "submitter": "Youjin Lee", "authors": "Youjin Lee and Elizabeth L. Ogburn", "title": "Network Dependence Can Lead to Spurious Associations and Invalid\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers across the health and social sciences generally assume that\nobservations are independent, even while relying on convenience samples that\ndraw subjects from one or a small number of communities, schools, hospitals,\netc. A paradigmatic example of this is the Framingham Heart Study (FHS). Many\nof the limitations of such samples are well-known, but the issue of statistical\ndependence due to social network ties has not previously been addressed. We\nshow that, along with anticonservative variance estimation, this can result in\nspurious associations due to network dependence. Using a statistical test that\nwe adapted from one developed for spatial autocorrelation, we test for network\ndependence in several of the thousands of influential papers that have been\npublished using FHS data. Results suggest that some of the many decades of\nresearch on coronary heart disease, other health outcomes, and peer influence\nusing FHS data may suffer from spurious associations, error-prone point\nestimates, and anticonservative inference due to unacknowledged network\ndependence. These issues are not unique to the FHS; as researchers in\npsychology, medicine, and beyond grapple with replication failures, this\nunacknowledged source of invalid statistical inference should be part of the\nconversation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:36:45 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:07:58 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lee", "Youjin", ""], ["Ogburn", "Elizabeth L.", ""]]}, {"id": "1908.00523", "submitter": "Ting Li", "authors": "Ting Li, Xianshi Yu, Bing-Yi Jing", "title": "Measuring the Clustering Strength of a Network via the Normalized\n  Clustering Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel statistic of networks, the normalized\nclustering coefficient, which is a modified version of the clustering\ncoefficient that is robust to network size, network density and degree\nheterogeneity under different network generative models. In particular, under\nthe degree corrected block model (DCBM), the \"in-out-ratio\" could be inferred\nfrom the normalized clustering coefficient. Asymptotic properties of the\nproposed indicator are studied under three popular network generative models.\nThe normalized clustering coefficient can also be used for networks clustering,\nnetwork sampling as well as dynamic network analysis. Simulations and real data\nanalysis are carried out to demonstrate these applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 17:28:54 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Li", "Ting", ""], ["Yu", "Xianshi", ""], ["Jing", "Bing-Yi", ""]]}, {"id": "1908.00624", "submitter": "Peetak Mitra", "authors": "Peetak Mitra, Suhrid Deshmukh", "title": "Modeling PKT at a global level: A machine learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-accepted that the ability to go from one place to another, or\nmobility, contributes significantly to one's wellbeing. The need for mobility\nis universal, but the demand for mobility shows a great variation on a country\nbasis. This particular study looks at what are some of the most important\nfactors on a global level that can help in predicting the\npassengerkilometers-travelled or passenger-miles-travelled (PKT/PMT) on a\ncountry by country basis. This particular work tries to quantify the impact of\nsome of the key variables like Gross Domestic Product (GDP), population growth,\nemployment rate, number of households, age demographics within the population\nand macroeconomic variables on the total vehicle-based travel within each\ncountry. A panel-based regression model is developed to identify the effect of\nsome of the key macroeconomic variables on the countries' PKT growth.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 13:33:57 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Mitra", "Peetak", ""], ["Deshmukh", "Suhrid", ""]]}, {"id": "1908.00650", "submitter": "Siamak Zamani Dadaneh", "authors": "Siamak Zamani Dadaneh, Paul de Figueiredo, Sing-Hoi Sze, Mingyuan Zhou\n  and Xiaoning Qian", "title": "Bayesian Gamma-Negative Binomial Modeling of Single-Cell RNA Sequencing\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Single-cell RNA sequencing (scRNA-seq) is a powerful profiling\ntechnique at the single-cell resolution. Appropriate analysis of scRNA-seq data\ncan characterize molecular heterogeneity and shed light into the underlying\ncellular process to better understand development and disease mechanisms. The\nunique analytic challenge is to appropriately model highly over-dispersed\nscRNA-seq count data with prevalent dropouts (zero counts), making\nzero-inflated dimensionality reduction techniques popular for scRNA-seq data\nanalyses. Employing zero-inflated distributions, however, may place extra\nemphasis on zero counts, leading to potential bias when identifying the latent\nstructure of the data. Results: In this paper, we propose a fully generative\nhierarchical gamma-negative binomial (hGNB) model of scRNA-seq data, obviating\nthe need for explicitly modeling zero inflation. At the same time, hGNB can\nnaturally account for covariate effects at both the gene and cell levels to\nidentify complex latent representations of scRNA-seq data, without the need for\ncommonly adopted pre-processing steps such as normalization. Efficient Bayesian\nmodel inference is derived by exploiting conditional conjugacy via novel data\naugmentation techniques. Conclusion: Experimental results on both simulated\ndata and several real-world scRNA-seq datasets suggest that hGNB is a powerful\ntool for cell cluster discovery as well as cell lineage inference.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 22:45:56 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Dadaneh", "Siamak Zamani", ""], ["de Figueiredo", "Paul", ""], ["Sze", "Sing-Hoi", ""], ["Zhou", "Mingyuan", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1908.00654", "submitter": "Yuqing Xu", "authors": "Yuqing Xu, Meijing Wu, Weili He, Qiming Liao, Yabing Mai", "title": "Teasing out the overall survival benefit with adjustment for treatment\n  switching to other therapies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In oncology clinical trials, characterizing the long-term overall survival\n(OS) benefit for an experimental drug or treatment regimen (experimental group)\nis often unobservable if some patients in the control group switch to drugs in\nthe experimental group and/or other cancer treatments after disease\nprogression. A key question often raised by payers and reimbursement agencies\nis how to estimate the true benefit of the experimental drug group on overall\nsurvival that would have been estimated if there were no treatment switches.\nSeveral commonly used statistical methods are available to estimate overall\nsurvival benefit while adjusting for treatment switching, ranging from naive\nexclusion or censoring approaches to more advanced methods including inverse\nprobability of censoring weighting (IPCW), iterative parameter estimation (IPE)\nalgorithm or rank-preserving structural failure time models (RPSFTM). However,\nmany clinical trials now have patients switching to different treatment\nregimens other than the test drugs, and the existing methods cannot handle more\ncomplicated scenarios. To address this challenge, we propose two additional\nmethods: stratified RPSFTM and random-forest-based prediction. A simulation\nstudy is conducted to assess the properties of the existing methods along with\nthe two newly proposed approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:01:50 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Xu", "Yuqing", ""], ["Wu", "Meijing", ""], ["He", "Weili", ""], ["Liao", "Qiming", ""], ["Mai", "Yabing", ""]]}, {"id": "1908.00659", "submitter": "Xin Li", "authors": "Xin Li, Ondrej Dyck, Stephen Jesse, Andrew R. Lupini, Sergei V.\n  Kalinin, and Mark P. Oxley", "title": "Structure retrieval from 4D-STEM: statistical analysis of potential\n  pitfalls in high-dimensional data", "comments": null, "journal-ref": "Phys. Rev. E 100, 023308, 2019", "doi": "10.1103/PhysRevE.100.023308", "report-no": null, "categories": "stat.AP eess.IV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four-dimensional scanning transmission electron microscopy (4D-STEM) is one\nof the most rapidly growing modes of electron microscopy imaging. The advent of\nfast pixelated cameras and the associated data infrastructure have greatly\naccelerated this process. Yet conversion of the 4D datasets into physically\nmeaningful structure images in real-space remains an open issue. In this work,\nwe demonstrate that, it is possible to systematically create filters that will\naffect the apparent resolution or even qualitative features of the real-space\nstructure image, reconstructing artificially generated patterns. As initial\nefforts, we explore statistical model selection algorithms, aiming for\nrobustness and reliability of estimated filters. This statistical model\nselection analysis demonstrates the need for regularization and\ncross-validation of inversion methods to robustly recover structure from\nhigh-dimensional diffraction datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:14:54 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 15:39:41 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Li", "Xin", ""], ["Dyck", "Ondrej", ""], ["Jesse", "Stephen", ""], ["Lupini", "Andrew R.", ""], ["Kalinin", "Sergei V.", ""], ["Oxley", "Mark P.", ""]]}, {"id": "1908.00663", "submitter": "Sida Peng", "authors": "Sida Peng", "title": "Heterogeneous Endogenous Effects in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method to identify leaders and followers in a\nnetwork. Prior works use spatial autoregression models (SARs) which implicitly\nassume that each individual in the network has the same peer effects on others.\nMechanically, they conclude the key player in the network to be the one with\nthe highest centrality. However, when some individuals are more influential\nthan others, centrality may fail to be a good measure. I develop a model that\nallows for individual-specific endogenous effects and propose a two-stage LASSO\nprocedure to identify influential individuals in a network. Under an assumption\nof sparsity: only a subset of individuals (which can increase with sample size\nn) is influential, I show that my 2SLSS estimator for individual-specific\nendogenous effects is consistent and achieves asymptotic normality. I also\ndevelop robust inference including uniformly valid confidence intervals. These\nresults also carry through to scenarios where the influential individuals are\nnot sparse. I extend the analysis to allow for multiple types of connections\n(multiple networks), and I show how to use the sparse group LASSO to detect\nwhich of the multiple connection types is more influential. Simulation evidence\nshows that my estimator has good finite sample performance. I further apply my\nmethod to the data in Banerjee et al. (2013) and my proposed procedure is able\nto identify leaders and effective networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 00:05:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Peng", "Sida", ""]]}, {"id": "1908.00795", "submitter": "Robin Upham", "authors": "Robin E. Upham, Lee Whittaker and Michael L. Brown", "title": "Exact joint likelihood of pseudo-$C_\\ell$ estimates from correlated\n  Gaussian cosmological fields", "comments": "17 pages, 7 figures. Updated to match accepted version", "journal-ref": null, "doi": "10.1093/mnras/stz3225", "report-no": null, "categories": "astro-ph.CO astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the exact joint likelihood of pseudo-$C_\\ell$ power spectrum\nestimates measured from an arbitrary number of Gaussian cosmological fields.\nOur method is applicable to both spin-0 fields and spin-2 fields, including a\nmixture of the two, and is relevant to Cosmic Microwave Background, weak\nlensing and galaxy clustering analyses. We show that Gaussian cosmological\nfields are mixed by a mask in such a way that retains their Gaussianity,\nwithout making any assumptions about the mask geometry. We then show that each\nauto- or cross-pseudo-$C_\\ell$ estimator can be written as a quadratic form,\nand apply the known joint distribution of quadratic forms to obtain the exact\njoint likelihood of a set of pseudo-$C_\\ell$ estimates in the presence of an\narbitrary mask. Considering the polarisation of the Cosmic Microwave Background\nas an example, we show using simulations that our likelihood recovers the full,\nexact multivariate distribution of $EE$, $BB$ and $EB$ pseudo-$C_\\ell$ power\nspectra. Our method provides a route to robust cosmological constraints from\nfuture Cosmic Microwave Background and large-scale structure surveys in an era\nof ever-increasing statistical precision.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 10:45:07 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 15:30:14 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Upham", "Robin E.", ""], ["Whittaker", "Lee", ""], ["Brown", "Michael L.", ""]]}, {"id": "1908.00823", "submitter": "Hendrik van der Wurp", "authors": "Hendrik van der Wurp, Andreas Groll, Thomas Kneib, Giampiero Marra and\n  Rosalba Radice", "title": "Generalised Joint Regression for Count Data with a Focus on Modelling\n  Football Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a versatile joint regression framework for count responses. The\nmethod is implemented in the R add-on package GJRM and allows for modelling\nlinear and non-linear dependence through the use of several copulae. Moreover,\nthe parameters of the marginal distributions of the count responses and of the\ncopula can be specified as flexible functions of covariates. Motivated by a\nfootball application, we also discuss an extension which forces the regression\ncoefficients of the marginal (linear) predictors to be equal via a suitable\npenalisation. Model fitting is based on a trust region algorithm which\nestimates simultaneously all the parameters of the joint models. We investigate\nthe proposal's empirical performance in two simulation studies, the first one\ndesigned for arbitrary count data, the other one reflecting football-specific\nsettings. Finally, the method is applied to FIFA World Cup data, showing its\ncompetitiveness to the standard approach with regard to predictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 12:29:45 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 12:36:22 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["van der Wurp", "Hendrik", ""], ["Groll", "Andreas", ""], ["Kneib", "Thomas", ""], ["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""]]}, {"id": "1908.00939", "submitter": "Bradley Lowery", "authors": "Bradley Lowery, Abigail Slater, and Kaison Thies", "title": "Functional Ratings in Sports", "comments": "12 pages, 8 Figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new model for ranking sports teams. Our model\nuses all scoring data from all games to produce a functional rating by the\nmethod of least squares. The functional rating can be interpreted as a teams\naverage point differential adjusted for strength of schedule. Using two team's\nfunctional ratings we can predict the expected point differential at any time\nin the game. We looked at three variations of our model accounting for\nhome-court advantage in different ways. We use the 2018-2019 NCAA Division 1\nmen's college basketball season to test the models and determined that\nhome-court advantage is statistically important but does not differ between\nteams.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:15:40 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Lowery", "Bradley", ""], ["Slater", "Abigail", ""], ["Thies", "Kaison", ""]]}, {"id": "1908.00960", "submitter": "Marcel M{\\l}y\\'nczak", "authors": "Marcel M{\\l}y\\'nczak, Tulio A. Valdez, Wojciech Kukwa", "title": "Comparing sleep studies in terms of the Apnea-Hypopnea Index using\n  dedicated Shiny web application", "comments": "In review in Biomedical Signal Processing and Control; 17 pages, 6\n  figures, 3 tables, 36 references", "journal-ref": "Biomedical Signal Processing and Control 2021;\n  https://www.sciencedirect.com/science/article/abs/pii/S1746809421002111?via%3Dihub", "doi": "10.1016/j.bspc.2021.102614", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apnea-Hypopnea Index (AHI) is one of the most-used parameters from the\nsleep study that allows assessing both the severity of obstructive sleep apnea\nand the reliability of new devices and methods. However, in many cases, it is\ncompared with a reference only via a correlation coefficient, or this value is\nat least the most emphasized. In this paper, we discuss the limitations of such\nan approach and list several alternative quantitative and qualitative\ntechniques, along with their interpretations. We propose the assessment of\nclinical significance along with the statistical one. Qualitative analysis can\nbe used for this purpose, or we suggest using the ranking function which\nenables consideration of various AHI values with different weights. It can be\nreliable for both adult-related and pediatric sleep studies. The dedicated\nShiny web application, written in R, was developed to enable quick analysis for\nboth physicians and statisticians.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 17:15:10 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 21:50:54 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 07:08:35 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 10:26:08 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["M\u0142y\u0144czak", "Marcel", ""], ["Valdez", "Tulio A.", ""], ["Kukwa", "Wojciech", ""]]}, {"id": "1908.01231", "submitter": "Michael LuValle", "authors": "Michael LuValle", "title": "Asymptotically consistent prediction of extremes in chaotic systems:1\n  stationary case", "comments": "4 pages, 1 figure, 7 references Plus 2 more references further\n  explanation of the example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world chaotic systems, the interest is typically in determining\nwhen the system will behave in an extreme manner. Flooding and drought, extreme\nheatwaves, large earthquakes, and large drops in the stock market are examples\nof the extreme behaviors of interest. For clarity, in this paper we confine\nourselves to the case where the chaotic system to be predicted is stationary so\ntheory for asymptotic consistency can be easily illuminated. We will start with\na simple case, where the attractor of the chaotic system is of known dimension\nso the answer is clear from prior work. Some extension will be made to\nstationary chaotic system with higher dimension where a number of empirical\nresults will be described and a theoretical framework proposed to help explain\nthem.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 20:37:39 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 03:49:41 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["LuValle", "Michael", ""]]}, {"id": "1908.01272", "submitter": "Kyungchul Song", "authors": "Elena Krasnokutskaya, Kyungchul Song, Xun Tang", "title": "Estimating Unobserved Individual Heterogeneity Using Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for studying environments with unobserved individual\nheterogeneity. Based on model-implied pairwise inequalities, the method\nclassifies individuals in the sample into groups defined by discrete unobserved\nheterogeneity with unknown support. We establish conditions under which the\ngroups are identified and consistently estimated through our method. We show\nthat the method performs well in finite samples through Monte Carlo simulation.\nWe then apply the method to estimate a model of lowest-price procurement\nauctions with unobserved bidder heterogeneity, using data from the California\nhighway procurement market.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 04:58:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 21:19:56 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 16:04:53 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Krasnokutskaya", "Elena", ""], ["Song", "Kyungchul", ""], ["Tang", "Xun", ""]]}, {"id": "1908.01406", "submitter": "David Ritzwoller", "authors": "David M. Ritzwoller and Joseph P. Romano", "title": "Uncertainty in the Hot Hand Fallacy: Detecting Streaky Alternatives to\n  Random Bernoulli Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of permutation tests of the randomness of a collection of\nBernoulli sequences and their application to analyses of the human tendency to\nperceive streaks of consecutive successes as overly representative of positive\ndependence - the hot hand fallacy. In particular, we study permutation tests of\nthe null hypothesis of randomness (i.e., that trials are i.i.d.) based on test\nstatistics that compare the proportion of successes that directly follow k\nconsecutive successes with either the overall proportion of successes or the\nproportion of successes that directly follow k consecutive failures. We\ncharacterize the asymptotic distributions of these test statistics and their\npermutation distributions under randomness, under a set of general stationary\nprocesses, and under a class of Markov chain alternatives, which allow us to\nderive their local asymptotic power. The results are applied to evaluate the\nempirical support for the hot hand fallacy provided by four controlled\nbasketball shooting experiments. We establish that substantially larger data\nsets are required to derive an informative measurement of the deviation from\nrandomness in basketball shooting. In one experiment, for which we were able to\nobtain data, multiple testing procedures reveal that one shooter exhibits a\nshooting pattern significantly inconsistent with randomness - supplying strong\nevidence that basketball shooting is not random for all shooters all of the\ntime. However, we find that the evidence against randomness in this experiment\nis limited to this shooter. Our results provide a mathematical and statistical\nfoundation for the design and validation of experiments that directly compare\ndeviations from randomness with human beliefs about deviations from randomness,\nand thereby constitute a direct test of the hot hand fallacy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 21:49:14 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 17:07:06 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 20:28:37 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 21:46:51 GMT"}, {"version": "v5", "created": "Fri, 8 Jan 2021 21:38:17 GMT"}, {"version": "v6", "created": "Fri, 2 Apr 2021 18:02:37 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ritzwoller", "David M.", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1908.01408", "submitter": "Cedric Neumann", "authors": "Cedric Neumann", "title": "Defence Against the Modern Arts: the Curse of Statistics -- FRStat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several decades, legal and scientific scholars have argued that\nconclusions from forensic examinations should be supported by statistical data\nand reported within a probabilistic framework. Multiple models have been\nproposed to quantify the probative value of forensic evidence. Unfortunately,\nseveral of these models rely on ad-hoc strategies that are not scientifically\nsound. The opacity of the technical jargon that is used to present these models\nand their results and the complexity of the techniques involved make it very\ndifficult for the untrained user to separate the wheat from the chaff. This\nseries of paper is intended to help forensic scientists and lawyers recognise\nissues in tools proposed to interpret the results of forensic examinations.\nThis paper focuses on the tool proposed by the Latent Print Branch of the U.S.\nDefense Forensic Science Center (DFSC) and called FRStat. In this paper, I\nexplore the compatibility of the results outputted by FRStat with the language\nused by the DFCS to report the conclusions of their fingerprint examinations,\nas well as the appropriateness of the statistical modelling underpinning the\ntool and the validation of its performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 21:57:01 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Neumann", "Cedric", ""]]}, {"id": "1908.01444", "submitter": "Rong Huang", "authors": "Rong Huang, Ronghui Xu, Parambir S. Dulai", "title": "Sensitivity Analysis of Treatment Effect to Unmeasured Confounding in\n  Observational Studies with Survival and Competing Risks Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No unmeasured confounding is often assumed in estimating treatment effects in\nobservational data when using approaches such as propensity scores and inverse\nprobability weighting. However, in many such studies due to the limitation of\nthe databases, collected confounders are not exhaustive, and it is crucial to\nexamine the extent to which the resulting estimate is sensitive to the\nunmeasured confounders. We consider this problem for survival and competing\nrisks data. Due to the complexity of models for such data, we adapt the\nsimulated potential confounders approach of Carnegie et al. (2016), which\nprovides a general tool for sensitivity analysis due to unmeasured confounding.\nMore specifically, we specify one sensitivity parameter to quantify the\nassociation between an unmeasured confounder and the treatment assignment, and\nanother set of parameters to quantify the association between the confounder\nand the time-to-event outcomes. By varying the magnitudes of the sensitivity\nparameters, we estimate the treatment effect of interest using the stochastic\nEM and the EM algorithms. We demonstrate the performance of our methods on\nsimulated data, and apply them to a comparative effectiveness study in\ninflammatory bowel disease (IBD).\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:10:05 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Huang", "Rong", ""], ["Xu", "Ronghui", ""], ["Dulai", "Parambir S.", ""]]}, {"id": "1908.01446", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Forecasting age distribution of death counts: An application to annuity\n  pricing", "comments": "30 pages, 4 figures, To appear in Annals of Actuarial Science", "journal-ref": "Annals of Actuarial Science, 2020, 14(1), 150-169", "doi": "10.1017/S1748499519000101", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a compositional data analysis approach to forecasting the age\ndistribution of death counts. Using the age-specific period life-table death\ncounts in Australia obtained from the Human Mortality Database, the\ncompositional data analysis approach produces more accurate one- to\n20-step-ahead point and interval forecasts than Lee-Carter method,\nHyndman-Ullah method, and two na\\\"{i}ve random walk methods. The improved\nforecast accuracy of period life-table death counts is of great interest to\ndemographers for estimating survival probabilities and life expectancy, and to\nactuaries for determining temporary annuity prices for various ages and\nmaturities. Although we focus on temporary annuity prices, we consider\nlong-term contracts which make the annuity almost lifetime, in particular when\nthe age at entry is sufficiently high.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:32:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "1908.01488", "submitter": "Charlotte Recapet", "authors": "Charlotte R\\'ecapet (ECOBIOP), Mathilde Arriv\\'e (IBMP), Blandine\n  Doligez, Pierre Bize", "title": "Antioxidant capacity is repeatable across years but does not\n  consistently correlate with a marker of peroxidation in a free-living\n  passerine bird", "comments": null, "journal-ref": "Journal of Comparative Physiology B, Springer Verlag, 2019, 189\n  (2), pp.283-298", "doi": "10.1007/s00360-019-01211-1", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oxidative stress occurs when reactive oxygen species (ROS) exceed antioxidant\ndefences, which can have deleterious effects on cell function, health and\nsurvival. Therefore, organisms are expected to finely regulate pro-oxidant and\nantioxidant processes. ROS are mainly produced through aerobic metabolism and\nvary in response to changes in energetic requirements, whereas antioxidants may\nbe enhanced, depleted or show no changes in response to changes in ROS levels.\nWe investigated the repeatability, within-individual variation and correlation\nacross different conditions of two plasmatic markers of the oxidative balance\nin 1108 samples from 635 free-living adult collared flycatchers (Ficedula\nalbicollis). We sought to manipulate energy constraints by increasing wing load\nin 2012 and 2013 and by providing additional food in 2014. We then tested the\nrelative importance of within- and between-individual variation on reactive\noxygen metabolites (ROMs), a marker of lipid and protein peroxidation, and on\nnon-enzymatic antioxidant defences (OXY test). We also investigated whether the\nexperimental treatments modified the correlation between markers. Antioxidant\ndefences were repeatable (range of repeatability estimates = 0.128--0.581),\nwhereas ROMs were not (0--0.061). Antioxidants varied neither between\nincubation and nestling feeding nor between sexes. ROMs increased from\nincubation to nestling feeding in females and were higher in females than\nmales. Antioxidant defences and ROM concentration were globally positively\ncorrelated, but the correlation varied between experimental conditions and\nbetween years. Hence, the management of oxidative balance in wild animals\nappears flexible under variable environmental conditions, an observation which\nshould be confirmed over a wider range of markers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 06:50:51 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["R\u00e9capet", "Charlotte", "", "ECOBIOP"], ["Arriv\u00e9", "Mathilde", "", "IBMP"], ["Doligez", "Blandine", ""], ["Bize", "Pierre", ""]]}, {"id": "1908.01555", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Alex Gibberd, Sandipan Roy, Matt Nunes, Romy\n  Lorenz, Robert Leech, Takeshi Ogawa, Motoaki Kawanabe, Aapo Hyvarinen", "title": "Interpretable brain age prediction using linear latent variable models\n  of functional connectivity", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0232296", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging-driven prediction of brain age, defined as the predicted\nbiological age of a subject using only brain imaging data, is an exciting\navenue of research. In this work we seek to build models of brain age based on\nfunctional connectivity while prioritizing model interpretability and\nunderstanding. This way, the models serve to both provide accurate estimates of\nbrain age as well as allow us to investigate changes in functional connectivity\nwhich occur during the ageing process. The methods proposed in this work\nconsist of a two-step procedure: first, linear latent variable models, such as\nPCA and its extensions, are employed to learn reproducible functional\nconnectivity networks present across a cohort of subjects. The activity within\neach network is subsequently employed as a feature in a linear regression model\nto predict brain age. The proposed framework is employed on the data from the\nCamCAN repository and the inferred brain age models are further demonstrated to\ngeneralize using data from two open-access repositories: the Human Connectome\nProject and the ATR Wide-Age-Range.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 10:42:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Gibberd", "Alex", ""], ["Roy", "Sandipan", ""], ["Nunes", "Matt", ""], ["Lorenz", "Romy", ""], ["Leech", "Robert", ""], ["Ogawa", "Takeshi", ""], ["Kawanabe", "Motoaki", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1908.01583", "submitter": "Nina Lazarevic", "authors": "Nina Lazarevic, Luke D. Knibbs, Peter D. Sly, Adrian G. Barnett", "title": "Performance of variable and function selection methods for estimating\n  the non-linear health effects of correlated chemical mixtures: a simulation\n  study", "comments": null, "journal-ref": "Stat Med (2020) 39:3947-3967", "doi": "10.1002/sim.8701", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods for identifying harmful chemicals in a correlated mixture\noften assume linearity in exposure-response relationships. Non-monotonic\nrelationships are increasingly recognised (e.g., for endocrine-disrupting\nchemicals); however, the impact of non-monotonicity on exposure selection has\nnot been evaluated. In a simulation study, we assessed the performance of\nBayesian kernel machine regression (BKMR), Bayesian additive regression trees\n(BART), Bayesian structured additive regression with spike-slab priors\n(BSTARSS), and lasso penalised regression. We used data on exposure to 12\nphthalates and phenols in pregnant women from the U.S. National Health and\nNutrition Examination Survey to simulate realistic exposure data using a\nmultivariate copula. We simulated datasets of size N = 250 and compared methods\nacross 32 scenarios, varying by model size and sparsity, signal-to-noise ratio,\ncorrelation structure, and exposure-response relationship shapes. We compared\nmethods in terms of their sensitivity, specificity, and estimation accuracy. In\nmost scenarios, BKMR and BSTARSS achieved moderate to high specificity\n(0.56--0.91 and 0.57--0.96, respectively) and sensitivity (0.49--0.98 and\n0.25--0.97, respectively). BART achieved high specificity ($\\geq$ 0.96), but\nlow to moderate sensitivity (0.13--0.66). Lasso was highly sensitive\n(0.75--0.99), except for symmetric inverse-U-shaped relationships ($\\leq$ 0.2).\nPerformance was affected by the signal-to-noise ratio, but not substantially by\nthe correlation structure. Penalised regression methods that assume linearity,\nsuch as lasso, may not be suitable for studies of environmental chemicals\nhypothesised to have non-monotonic relationships with outcomes. Instead, BKMR\nand BSTARSS are attractive methods for flexibly estimating the shapes of\nexposure-response relationships and selecting among correlated exposures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 12:27:01 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Lazarevic", "Nina", ""], ["Knibbs", "Luke D.", ""], ["Sly", "Peter D.", ""], ["Barnett", "Adrian G.", ""]]}, {"id": "1908.01675", "submitter": "Thomas McAndrew PhD", "authors": "Thomas McAndrew, Nicholas G. Reich", "title": "Adaptively stacking ensembles for influenza forecasting with incomplete\n  data", "comments": "V0.2 added small paragraph on BMA and acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seasonal influenza infects between 10 and 50 million people in the United\nStates every year, overburdening hospitals during weeks of peak incidence.\nNamed by the CDC as an important tool to fight the damaging effects of these\nepidemics, accurate forecasts of influenza and influenza-like illness (ILI)\nforewarn public health officials about when, and where, seasonal influenza\noutbreaks will hit hardest. Multi-model ensemble forecasts---weighted\ncombinations of component models---have shown positive results in forecasting.\nEnsemble forecasts of influenza outbreaks have been static, training on all\npast ILI data at the beginning of a season, generating a set of optimal weights\nfor each model in the ensemble, and keeping the weights constant. We propose an\nadaptive ensemble forecast that (i) changes model weights week-by-week\nthroughout the influenza season, (ii) only needs the current influenza season's\ndata to make predictions, and (iii) by introducing a prior distribution,\nshrinks weights toward the reference equal weighting approach and adjusts for\nobserved ILI percentages that are subject to future revisions. We investigate\nthe prior's ability to impact adaptive ensemble performance and, after finding\nan optimal prior via a cross-validation approach, compare our adaptive\nensemble's performance to equal-weighted and static ensembles. Applied to\nforecasts of short-term ILI incidence at the regional and national level in the\nUS, our adaptive model outperforms a naive equal-weighted ensemble, and has\nsimilar or better performance to the static ensemble, which requires multiple\nyears of training data. Adaptive ensembles are able to quickly train and\nforecast during epidemics, and provide a practical tool to public health\nofficials looking for forecasts that can conform to unique features of a\nspecific season.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:02:57 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 19:01:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["McAndrew", "Thomas", ""], ["Reich", "Nicholas G.", ""]]}, {"id": "1908.01688", "submitter": "Ari Ercole", "authors": "Katharina Kohler, Ari Ercole", "title": "Characterising complex healthcare systems using network science: The\n  small world of emergency surgery", "comments": "14 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hospitals are complex systems and optimising their function is critical to\nthe provision of high quality, cost effective healthcare. Nevertheless, metrics\nof performance have to date focused on the performance of individual elements\nrather than the system as a whole. Manipulation of individual elements of a\ncomplex system without an integrative understanding of its function is\nundesirable and may lead to counter-intuitive outcomes and a holistic metric of\nhospital function might help design more efficient services. We aimed to\ncharacterise the system of peri-operative care for emergency surgical\nadmissions in our tertiary care hospital using network analysis. We used\nretrospective electronic health record data to construct a weighted directional\nnetwork of the system. For this we selected all unplanned admissions during a\n3.5 year period involving a surgical intervention during the inpatient stay and\nobtained a set of 16,500 individual inpatient episodes. We then constructed and\nanalysed the structure of this network using established methods from network\nscience such as degree distribution, betweenness centrality and small-world\ncharacteristics. The analysis showed the service to be a complex system with\nscale-free, small-world network properties. This finding has implications for\nthe structure and resilience of the service as such networks, whilst being\nrobust in general, may be vulnerable to outages at specific key nodes. We also\nidentified such potential hubs and bottlenecks in the system based on a variety\nof network measures. It is hoped that such a holistic, system-wide description\nof a hospital service may provide better metrics for hospital strain and serve\nto help planners engineer systems that are as robust as possible to external\nshocks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:20:18 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kohler", "Katharina", ""], ["Ercole", "Ari", ""]]}, {"id": "1908.01923", "submitter": "Vivek Srikrishnan", "authors": "Vivek Srikrishnan, Yawen Guan, Richard S. J. Tol, Klaus Keller", "title": "Highest CO$_2$ Emissions Scenarios Are Extreme Given Observations and\n  Expert Judgements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic projections of baseline future carbon emissions are important\nfor sound climate risk management. Deep uncertainty surrounds many drivers of\nprojected emissions. For example, there is no consensus about estimates of\nfossil fuel resources. We use a simple integrated assessment model to make\nprobabilistic projections of carbon emissions through 2100. We find that, given\ncurrent mitigation policies and in the absence of negative emissions\ntechnologies, more moderate scenarios used by the Intergovernmental Panel on\nClimate Change are more likely than the extreme high or low scenarios. This\nfinding is robust to a variety of assumptions about fossil fuel resource\nconstraints and decarbonization rates. However, the most likely range for\ncumulative CO$_2$ emissions from 2018--2100 across our varying assumptions is\n700--1800 GtC. Much more aggressive mitigation will be required to reliably\nachieve the $2^\\circ$C Paris Agreement target.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:25:54 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 14:49:19 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Srikrishnan", "Vivek", ""], ["Guan", "Yawen", ""], ["Tol", "Richard S. J.", ""], ["Keller", "Klaus", ""]]}, {"id": "1908.02034", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "Second-order Control of Complex Systems with Correlated Synthetic Data", "comments": "18 pages, 5 figures", "journal-ref": "Complex Adaptive Systems Modeling, 7, 4 (2019)", "doi": "10.1186/s40294-019-0065-y", "report-no": null, "categories": "stat.AP cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of synthetic data is an essential tool to study complex\nsystems, allowing for example to test models of these in precisely controlled\nsettings, or to parametrize simulation models when data is missing. This paper\nfocuses on the generation of synthetic data with an emphasis on correlation\nstructure. We introduce a new methodology to generate such correlated synthetic\ndata. It is implemented in the field of socio-spatial systems, more precisely\nby coupling an urban growth model with a transportation network generation\nmodel. We also show the genericity of the method with an application on\nfinancial time-series. The simulation results show that the generation of\ncorrelated synthetic data for such systems is indeed feasible within a broad\nrange of correlations, and suggest applications of such synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 09:18:17 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 18:34:42 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Raimbault", "Juste", ""]]}, {"id": "1908.02101", "submitter": "Bruno Scalzo Dees", "authors": "Bruno Scalzo Dees", "title": "Analysing Global Fixed Income Markets with Tensors", "comments": "9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.EM eess.SP q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global fixed income returns span across multiple maturities and economies,\nthat is, they naturally reside on multi-dimensional data structures referred to\nas tensors. In contrast to standard \"flat-view\" multivariate models that are\nagnostic to data structure and only describe linear pairwise relationships, we\nintroduce a tensor-valued approach to model the global risks shared by multiple\ninterest rate curves. In this way, the estimated risk factors can be\nanalytically decomposed into maturity-domain and country-domain constituents,\nwhich allows the investor to devise rigorous and tractable global portfolio\nmanagement and hedging strategies tailored to each risk domain. An empirical\nanalysis confirms the existence of global risk factors shared by eight\ndeveloped economies, and demonstrates their ability to compactly describe the\nglobal macroeconomic environment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:16:16 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 15:07:44 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 12:33:18 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 14:30:56 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Dees", "Bruno Scalzo", ""]]}, {"id": "1908.02334", "submitter": "Emily Diller", "authors": "Emily E Diller, Sha Cao, Beth Ey, Robert Lober, Jason G Parker", "title": "Predicted disease compositions of human gliomas estimated from\n  multiparametric MRI can predict endothelial proliferation, tumor grade, and\n  overall survival", "comments": "13 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV physics.med-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background and Purpose: Biopsy is the main determinants of glioma clinical\nmanagement, but require invasive sampling that fail to detect relevant features\nbecause of tumor heterogeneity. The purpose of this study was to evaluate the\naccuracy of a voxel-wise, multiparametric MRI radiomic method to predict\nfeatures and develop a minimally invasive method to objectively assess\nneoplasms.\n  Methods: Multiparametric MRI were registered to T1-weighted gadolinium\ncontrast-enhanced data using a 12 degree-of-freedom affine model. The\nretrospectively collected MRI data included T1-weighted, T1-weighted gadolinium\ncontrast-enhanced, T2-weighted, fluid attenuated inversion recovery, and\nmulti-b-value diffusion-weighted acquired at 1.5T or 3.0T. Clinical experts\nprovided voxel-wise annotations for five disease states on a subset of patients\nto establish a training feature vector of 611,930 observations. Then, a\nk-nearest-neighbor (k-NN) classifier was trained using a 25% hold-out design.\nThe trained k-NN model was applied to 13,018,171 observations from seventeen\nhistologically confirmed glioma patients. Linear regression tested overall\nsurvival (OS) relationship to predicted disease compositions (PDC) and\ndiagnostic age (alpha = 0.05). Canonical discriminant analysis tested if PDC\nand diagnostic age could differentiate clinical, genetic, and microscopic\nfactors (alpha = 0.05).\n  Results: The model predicted voxel annotation class with a Dice similarity\ncoefficient of 94.34% +/- 2.98. Linear combinations of PDCs and diagnostic age\npredicted OS (p = 0.008), grade (p = 0.014), and endothelia proliferation (p =\n0.003); but fell short predicting gene mutations for TP53BP1 and IDH1.\n  Conclusions: This voxel-wise, multi-parametric MRI radiomic strategy holds\npotential as a non-invasive decision-making aid for clinicians managing\npatients with glioma.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 19:10:32 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Diller", "Emily E", ""], ["Cao", "Sha", ""], ["Ey", "Beth", ""], ["Lober", "Robert", ""], ["Parker", "Jason G", ""]]}, {"id": "1908.02344", "submitter": "Hossein Baghishani", "authors": "Mahsa Nadifar, Hossein Baghishani, Afshin Fallah, and Havard Rue", "title": "Statistical modeling of groundwater quality assessment in Iran using a\n  flexible Poisson likelihood", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing water quality and recognizing its associated risks to human health\nand the broader environment is undoubtedly essential. Groundwater is widely\nused to supply water for drinking, industry, and agriculture purposes. The\ngroundwater quality measurements vary for different climates and various human\nbehaviors, and consequently, their spatial variability can be substantial. In\nthis paper, we aim to analyze a groundwater dataset from the Golestan province,\nIran, for November 2003 to November 2013. Our target response variable to\nmonitor the quality of groundwater is the number of counts that the quality of\nwater is good for a drink. Hence, we are facing spatial count data. Due to the\nubiquity of over or underdispersion in count data, we propose a Bayesian\nhierarchical modeling approach based on the renewal theory that relates\nnonexponential waiting times between events and the distribution of the counts,\nrelaxing the assumption of equidispersion at the cost of an additional\nparameter. Particularly, we extend the methodology for the analysis of spatial\ncount data based on the gamma distribution assumption for waiting times. The\nmodel can be formulated as a latent Gaussian model, and therefore, we can carry\nout the fast computation by using the integrated nested Laplace approximation\nmethod. The analysis of the groundwater dataset and a simulation study show a\nsignificant improvement over both Poisson and negative binomial models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 19:49:13 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Nadifar", "Mahsa", ""], ["Baghishani", "Hossein", ""], ["Fallah", "Afshin", ""], ["Rue", "Havard", ""]]}, {"id": "1908.02423", "submitter": "Dani Chu", "authors": "Dani Chu, Matthew Reyers, James Thomson, Lucas Wu", "title": "Route Identification in the National Football League", "comments": "Work developed from the NFL Big Data Bowl for submission in the JQAS\n  special issue on NFL tracking data", "journal-ref": null, "doi": "10.1515/jqas-2019-0047", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking data in the NFL is a sequence of spatial-temporal measurements that\nvary in length depending on the duration of the play. In this paper, we\ndemonstrate how model-based curve clustering of observed player trajectories\ncan be used to identify the routes run by eligible receivers on offensive\npassing plays. We use a Bernstein polynomial basis function to represent\ncluster centers, and the Expectation Maximization algorithm to learn the route\nlabels for each of the 34,698 routes run on the 6,963 passing plays in the data\nset. We go on to suggest ideas for new potential receiver metrics that account\nfor receiver deployment. The resulting route labels can also be paired with\nfilm to enable streamlined queries of game film.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 02:36:54 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chu", "Dani", ""], ["Reyers", "Matthew", ""], ["Thomson", "James", ""], ["Wu", "Lucas", ""]]}, {"id": "1908.02586", "submitter": "Susan Fennell", "authors": "Susan C. Fennell, James P. Gleeson, Michael Quayle, Kevin Durrheim,\n  Kevin Burke", "title": "A modelling methodology for social interaction experiments", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of temporal network data arising from online interactive social\nexperiments is not possible with standard statistical methods because the\nassumptions of these models, such as independence of observations, are not\nsatisfied. In this paper, we outline a modelling methodology for such\nexperiments where, as an example, we analyse data collected using the Virtual\nInteraction Application (VIAPPL) --- a software platform for conducting\nexperiments that reveal how social norms and identities emerge through social\ninteraction. We apply our model to show that ingroup favouritism and\nreciprocity are present in the experiments, and to quantify the strengthening\nof these behaviours over time. Our method enables us to identify participants\nwhose behaviour is markedly different from the norm. We use the method to\nprovide a visualisation of the data that highlights the level of ingroup\nfavouritism, the strong reciprocal relationships, and the different behaviour\nof participants in the game. While our methodology was developed with VIAPPL in\nmind, its usage extends to any type of social interaction data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 12:37:10 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Fennell", "Susan C.", ""], ["Gleeson", "James P.", ""], ["Quayle", "Michael", ""], ["Durrheim", "Kevin", ""], ["Burke", "Kevin", ""]]}, {"id": "1908.02806", "submitter": "Toryn Schafer", "authors": "Toryn L.J. Schafer, Christopher K. Wikle, Jay A. VonBank, Bart M.\n  Ballard, Mitch D. Weegman", "title": "A Bayesian Markov model with P\\'olya-Gamma sampling for estimating\n  individual behavior transition probabilities from accelerometer\n  classifications", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-020-00399-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of accelerometers in wildlife tracking provides a fine-scale data\nsource for understanding animal behavior and decision-making. Current methods\nin movement ecology focus on behavior as a driver of movement mechanisms. Our\nMarkov model is a flexible and efficient method for inference related to\neffects on behavior that considers dependence between current and past\nbehaviors. We applied this model to behavior data from six greater\nwhite-fronted geese (Anser albifrons frontalis) during spring migration in\nmid-continent North America and considered likely drivers of behavior,\nincluding habitat, weather and time of day effects. We modeled the transitions\nbetween flying, feeding, stationary and walking behavior states using a\nfirst-order Bayesian Markov model. We introduced P\\'olya-Gamma latent variables\nfor automatic sampling of the covariate coefficients from the posterior\ndistribution and we calculated the odds ratios from the posterior samples. Our\nmodel provides a unifying framework for including both acceleration and Global\nPositioning System data. We found significant differences in behavioral\ntransition rates among habitat types, diurnal behavior and behavioral changes\ndue to weather. Our model provides straightforward inference of behavioral time\nallocation across used habitats, which is not amenable in activity budget or\nresource selection frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 19:33:00 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 20:14:38 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 19:52:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Schafer", "Toryn L. J.", ""], ["Wikle", "Christopher K.", ""], ["VonBank", "Jay A.", ""], ["Ballard", "Bart M.", ""], ["Weegman", "Mitch D.", ""]]}, {"id": "1908.02891", "submitter": "Feng Li", "authors": "Xiaoqian Wang, Yanfei Kang, Fotios Petropoulos, Feng Li", "title": "The uncertainty estimation of feature-based forecast combinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting is an indispensable element of operational research (OR) and an\nimportant aid to planning. The accurate estimation of the forecast uncertainty\nfacilitates several operations management activities, predominantly in\nsupporting decisions in inventory and supply chain management and effectively\nsetting safety stocks. In this paper, we introduce a feature-based framework,\nwhich links the relationship between time series features and the interval\nforecasting performance into providing reliable interval forecasts. We propose\nan optimal threshold ratio searching algorithm and a new weight determination\nmechanism for selecting an appropriate subset of models and assigning\ncombination weights for each time series tailored to the observed features. We\nevaluate our approach using a large set of time series from the M4 competition.\nOur experiments show that our approach significantly outperforms a wide range\nof benchmark models, both in terms of point forecasts as well as prediction\nintervals.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 00:52:55 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 13:11:39 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:17:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Xiaoqian", ""], ["Kang", "Yanfei", ""], ["Petropoulos", "Fotios", ""], ["Li", "Feng", ""]]}, {"id": "1908.02934", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri and Andrew Duncan and Duncan Simpson and George Thorne\n  and Geoffrey Parks", "title": "Spatial Flow-Field Approximation Using Few Thermodynamic Measurements\n  Part II: Uncertainty Assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this second part of our two-part paper, we provide a detailed, frequentist\nframework for propagating uncertainties within our multivariate linear least\nsquares model. This permits us to quantify the impact of uncertainties in\nthermodynamic measurements---arising from calibrations and the data acquisition\nsystem---and the correlations therein, along with uncertainties in probe\npositions. We show how the former has a much larger effect (relatively) than\nuncertainties in probe placement.\n  We use this non-deterministic framework to demonstrate why the well-worn\nmetric for assessing spatial sampling uncertainty falls short of providing an\naccurate characterization of the effect of a few spatial measurements. In other\nwords, it does not accurately describe the uncertainty associated with sampling\na non-uniform pattern with a few circumferentially scattered rakes. To this\nend, we argue that our data-centric framework can offer a more rigorous\ncharacterization of this uncertainty. Our paper proposes two new uncertainty\nmetrics: one for characterizing spatial sampling uncertainty and another for\ncapturing the impact of measurement imprecision in individual probes. These\nmetrics are rigorously derived in our paper and their ease in computation\npermits them to be widely adopted by the turbomachinery community for carrying\nout uncertainty assessments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 05:08:46 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Seshadri", "Pranay", ""], ["Duncan", "Andrew", ""], ["Simpson", "Duncan", ""], ["Thorne", "George", ""], ["Parks", "Geoffrey", ""]]}, {"id": "1908.02954", "submitter": "Giulia Cereda", "authors": "Giulia Cereda and Richard D. Gill", "title": "A nonparametric Bayesian approach to the rare type match problem", "comments": "arXiv admin note: text overlap with arXiv:1506.08444", "journal-ref": null, "doi": "10.3390/e22040439", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"rare type match problem\" is the situation in which the suspect's DNA\nprofile, matching the DNA profile of the crime stain, is not in the database of\nreference. The evaluation of this match in the light of the two competing\nhypotheses (the crime stain has been left by the suspect or by another person)\nis based on the calculation of the likelihood ratio and depends on the\npopulation proportions of the DNA profiles, that are unknown. We propose a\nBayesian nonparametric method that uses a two-parameter Poisson Dirichlet\ndistribution as a prior over the ranked population proportions, and discards\nthe information about the names of the different DNA profiles. This fits very\nwell the data coming from European Y-STR DNA profiles, and the calculation of\nthe likelihood ratio becomes quite simple thanks to a justified Empirical Bayes\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 07:19:37 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 09:21:30 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 15:25:28 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Cereda", "Giulia", ""], ["Gill", "Richard D.", ""]]}, {"id": "1908.03107", "submitter": "Anna Kiriliouk", "authors": "Anna Kiriliouk and Philippe Naveau", "title": "Climate extreme event attribution using multivariate\n  peaks-over-thresholds modeling and counterfactual theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical climate models are complex and combine a large number of physical\nprocesses. They are key tools in quantifying the relative contribution of\npotential anthropogenic causes (e.g., the current increase in greenhouse gases)\non high impact atmospheric variables like heavy rainfall. These so-called\nclimate extreme event attribution problems are particularly challenging in a\nmultivariate context, that is, when the atmospheric variables are measured on a\npossibly high-dimensional grid.\n  In this paper, we leverage two statistical theories to assess causality in\nthe context of multivariate extreme event attribution. As we consider an event\nto be extreme when at least one of the components of the vector of interest is\nlarge, extreme-value theory justifies, in an asymptotical sense, a multivariate\ngeneralized Pareto distribution to model joint extremes. Under this class of\ndistributions, we derive and study probabilities of necessary and sufficient\ncausation as defined by the counterfactual theory of Pearl. To increase causal\nevidence, we propose a dimension reduction strategy based on the optimal linear\nprojection that maximizes such causation probabilities. Our approach is tested\non simulated examples and applied to weekly winter maxima precipitation outputs\nof the French CNRM from the recent CMIP6 experiment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:06:06 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 08:01:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Naveau", "Philippe", ""]]}, {"id": "1908.03390", "submitter": "Martin Bladt", "authors": "Martin Bladt, Hansjoerg Albrecher, Jan Beirlant", "title": "Combined Tail Estimation Using Censored Data and Expert Information", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tail estimation in Pareto-like settings for datasets with a high\npercentage of randomly right-censored data, and where some expert information\non the tail index is available for the censored observations. This setting\narises for instance naturally for liability insurance claims, where actuarial\nexperts build reserves based on the specificity of each open claim, which can\nbe used to improve the estimation based on the already available data points\nfrom closed claims. Through an entropy-perturbed likelihood we derive an\nexplicit estimator and establish a close analogy with Bayesian methods.\nEmbedded in an extreme value approach, asymptotic normality of the estimator is\nshown, and when the expert is clair-voyant, a simple combination formula can be\ndeduced, bridging the classical statistical approach with the expert\ninformation. Following the aforementioned combination formula, a combination of\nquantile estimators can be naturally defined. In a simulation study, the\nestimator is shown to often outperform the Hill estimator for censored\nobservations and recent Bayesian solutions, some of which require more\ninformation than usually available. Finally we perform a case study on a motor\nthird-party liability insurance claim dataset, where Hill-type and quantile\nplots incorporate ultimate values into the estimation procedure in an intuitive\nmanner.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 09:59:52 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 14:53:05 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Bladt", "Martin", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "1908.03431", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri and Duncan Simpson and George Thorne and Andrew Duncan\n  and Geoffrey Parks", "title": "Spatial Flow-Field Approximation Using Few Thermodynamic Measurements\n  Part I: Formulation and Area Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our investigation raises an important question that is of relevance to the\nwider turbomachinery community: how do we estimate the spatial average of a\nflow quantity given finite (and sparse) measurements? This paper seeks to\nadvance efforts to answer this question rigorously. In this paper, we develop a\nregularized multivariate linear regression framework for studying engine\ntemperature measurements. As part of this investigation, we study the\ntemperature measurements obtained from the same axial plane across five\ndifferent engines yielding a total of 82 data-sets. The five different engines\nhave similar architectures and therefore similar temperature spatial harmonics\nare expected. Our problem is to estimate the spatial field in engine\ntemperature given a few measurements obtained from thermocouples positioned on\na set of rakes. Our motivation for doing so is to understand key engine\ntemperature modes that cannot be captured in a rig or in computational\nsimulations, as the cause of these modes may not be replicated in these simpler\nenvironments. To this end, we develop a multivariate linear least squares model\nwith Tikhonov regularization to estimate the 2D temperature spatial field. Our\nmodel uses a Fourier expansion in the circumferential direction and a quadratic\npolynomial expansion in the radial direction. One important component of our\nmodeling framework is the selection of model parameters, i.e. the harmonics in\nthe circumferential direction. A training-testing paradigm is proposed and\napplied to quantify the harmonics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 04:58:10 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Seshadri", "Pranay", ""], ["Simpson", "Duncan", ""], ["Thorne", "George", ""], ["Duncan", "Andrew", ""], ["Parks", "Geoffrey", ""]]}, {"id": "1908.03435", "submitter": "Alexander Kott", "authors": "Alexander Kott, Philip Perconti, Nandi Leslie", "title": "Discovering a Regularity: the Case of An 800-year Law of Advances in\n  Small-Arms Technologies", "comments": "under review, Technology Analysis and Strategic Management journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering a broad family of technologies where a measure of performance\n(MoP) is difficult or impossible to formulate, we seek an alternative measure\nthat exhibits a regular pattern of evolution over time, similar to how a MoP\nmay follow a Moore's law. In an empirical case study, we explore an approach to\nidentifying such a composite measure called a Figure of Regularity (FoR). We\nuse the proposed approach to identify a novel FoR for diverse classes of small\narms - bows, crossbows, harquebuses, muskets, rifles, repeaters, and assault\nrifles - and show that this FoR agrees well with the empirical data. We\nidentify a previously unreported regular trend in the FoR of an exceptionally\nlong duration - from approximately 1200 CE to the present - and discuss how\nresearch managers can analyze long-term trends in conjunction with a portfolio\nof research directions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 12:31:44 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kott", "Alexander", ""], ["Perconti", "Philip", ""], ["Leslie", "Nandi", ""]]}, {"id": "1908.03493", "submitter": "Xin Ma", "authors": "Xin Ma, Wenqing Wu, Yuanyuan Zhang", "title": "Improved GM(1,1) model based on Simpson formula and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical GM(1,1) model is an efficient tool to {make accurate forecasts}\nwith limited samples. But the accuracy of the GM(1,1) model still needs to be\nimproved. This paper proposes a novel discrete GM(1,1) model, named ${\\rm\nGM_{SD}}$(1,1) model, of which the background value is reconstructed using\nSimpson formula. The expression of the specific time response function is\ndeduced, and the relationship between our model} and the continuous GM(1,1)\nmodel with Simpson formula called ${\\rm GM_{SC} }$(1,1) model is systematically\ndiscussed. The proposed model is proved to be unbiased to simulate the\nhomogeneous exponent sequence. Further, some numerical examples are given to\nvalidate the accuracy of the new ${\\rm GM_{SD}}$(1,1) model. Finally, this\nmodel is used to predict the Gross Domestic Product and the freightage of\nLanzhou, and the results illustrate the ${\\rm GM_{SD}}$(1,1) model provides\naccurate prediction.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 15:16:22 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:30:58 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ma", "Xin", ""], ["Wu", "Wenqing", ""], ["Zhang", "Yuanyuan", ""]]}, {"id": "1908.03531", "submitter": "Guillaume Basse", "authors": "Guillaume Basse, Yi Ding, Panos Toulis", "title": "Minimax designs for causal effects in temporal experiments with\n  treatment habituation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments are the gold standard for estimating the causal\neffects of an intervention. In the simplest setting, each experimental unit is\nrandomly assigned to receive treatment or control, and then the outcomes in\neach treatment arm are compared. In many settings, however, randomized\nexperiments need to be executed over several time periods such that treatment\nassignment happens at each time period. In such temporal experiments, it has\nbeen observed that the effects of an intervention on a given unit may be large\nwhen the unit is first exposed to it, but then it often attenuates, or even\nvanishes, after repeated exposures. This phenomenon is typically due to units'\nhabituation to the intervention, or some other general form of learning, such\nas when users gradually start to ignore repeated mails sent by a promotional\ncampaign. This paper proposes randomized designs for estimating causal effects\nin temporal experiments when habituation is present. We show that our designs\nare minimax optimal in a large class of practical designs. Our analysis is\nbased on the randomization framework of causal inference, and imposes no\nparametric modeling assumptions on the outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 16:40:05 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 09:52:07 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Basse", "Guillaume", ""], ["Ding", "Yi", ""], ["Toulis", "Panos", ""]]}, {"id": "1908.03652", "submitter": "Michael Johnson", "authors": "Michael Johnson, Jiongyi Cao, and Hyunseung Kang", "title": "Detecting Heterogeneous Treatment Effect with Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing interest in estimating heterogeneity in causal effects\nin randomized and observational studies. However, little research has been\nconducted to understand heterogeneity in an instrumental variables study. In\nthis work, we present a method to estimate heterogeneous causal effects using\nan instrumental variable approach. The method has two parts. The first part\nuses subject-matter knowledge and interpretable machine learning techniques,\nsuch as classification and regression trees, to discover potential effect\nmodifiers. The second part uses closed testing to test for the statistical\nsignificance of the effect modifiers while strongly controlling familywise\nerror rate. We conducted this method on the Oregon Health Insurance Experiment,\nestimating the effect of Medicaid on the number of days an individual's health\ndoes not impede their usual activities, and found evidence of heterogeneity in\nolder men who prefer English and don't self-identify as Asian and younger\nindividuals who have at most a high school diploma or GED and prefer English.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 22:52:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 17:05:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Johnson", "Michael", ""], ["Cao", "Jiongyi", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1908.03926", "submitter": "Zhigang Yao", "authors": "Zhigang Yao, Zengyan Fan, Masahito Hayashi, William F. Eddy", "title": "Quantifying Time-Varying Sources in Magnetoencephalography -- A Discrete\n  Approach", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "2019", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of brain source from the most advanced brain\nimaging technique, Magnetoencephalography (MEG), which measures the magnetic\nfields outside the human head produced by the electrical activity inside the\nbrain. Common time-varying source localization methods assume the source\ncurrent with a time-varying structure and solve the MEG inverse problem by\nmainly estimating the source moment parameters. These methods use the fact that\nthe magnetic fields linearly depend on the moment parameters of the source, and\nwork well under the linear dynamic system. However, magnetic fields are known\nto be non-linearly related to the location parameters of the source. The\nexisting work on estimating the time-varying unknown location parameters is\nlimited. We are motivated to investigate the source distribution for the\nlocation parameters based on a dynamic framework, where the posterior\ndistribution of the source is computed in a closed form discretely. The new\nframework allows us not only to directly approximate the posterior distribution\nof the source current, where sequential sampling methods may suffer from slow\nconvergence due to the large volume of measurement, but also to quantify the\nsource distribution at any time point from the entire set of measurements\nreflecting the distribution of the source, rather than using only the\nmeasurements up to the time point of interest. Both a dynamic procedure and a\nswitch procedure are proposed for the new discrete approach, balancing\nestimation accuracy and computational efficiency when multiple sources are\npresent. In both simulation and real data, we illustrate that the new method is\nable to provide comprehensive insight into the time evolution of the sources at\ndifferent stages of the MEG and EEG experiment.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 15:46:23 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yao", "Zhigang", ""], ["Fan", "Zengyan", ""], ["Hayashi", "Masahito", ""], ["Eddy", "William F.", ""]]}, {"id": "1908.04000", "submitter": "Priyanga Dilini Talagala", "authors": "Priyanga Dilini Talagala and Rob J. Hyndman and Kate Smith-Miles", "title": "Anomaly Detection in High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HDoutliers algorithm is a powerful unsupervised algorithm for detecting\nanomalies in high-dimensional data, with a strong theoretical foundation.\nHowever, it suffers from some limitations that significantly hinder its\nperformance level, under certain circumstances. In this article, we propose an\nalgorithm that addresses these limitations. We define an anomaly as an\nobservation that deviates markedly from the majority with a large distance gap.\nAn approach based on extreme value theory is used for the anomalous threshold\ncalculation. Using various synthetic and real datasets, we demonstrate the wide\napplicability and usefulness of our algorithm, which we call the stray\nalgorithm. We also demonstrate how this algorithm can assist in detecting\nanomalies present in other data structures using feature engineering. We show\nthe situations where the stray algorithm outperforms the HDoutliers algorithm\nboth in accuracy and computational time. This framework is implemented in the\nopen source R package stray.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 04:48:03 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Talagala", "Priyanga Dilini", ""], ["Hyndman", "Rob J.", ""], ["Smith-Miles", "Kate", ""]]}, {"id": "1908.04510", "submitter": "Bikramjit Das", "authors": "Bikramjit Das and Souvik Ghosh", "title": "Growth of Common Friends in a Preferential Attachment Model", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of common friends (or connections) in a graph is a commonly used\nmeasure of proximity between two nodes. Such measures are used in link\nprediction algorithms and recommendation systems in large online social\nnetworks. We obtain the rate of growth of the number of common friends in a\nlinear preferential attachment model. We apply our result to develop an\nestimate for the number of common friends. We also observe a phase transition\nin the limiting behavior of the number of common friends; depending on the\nrange of the parameters of the model, the growth is either power-law, or,\nlogarithmic, or static with the size of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:28:46 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Das", "Bikramjit", ""], ["Ghosh", "Souvik", ""]]}, {"id": "1908.04597", "submitter": "Tom Lefebvre", "authors": "Wannes De Groote, Tom Lefebvre, Georges Tod, Nele De Geeter, Bruno\n  Depraetere, Suzanne Van Poppel, Guillaume Crevecoeur", "title": "Inverse Parametric Uncertain Identification using Polynomial Chaos and\n  high-order Moment Matching benchmarked on a Wet Friction Clutch", "comments": "37 pages, 20 figures", "journal-ref": null, "doi": "10.1016/j.mechatronics.2019.102320", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A numerically efficient inverse method for parametric model uncertainty\nidentification using maximum likelihood estimation is presented. The goal is to\nidentify a probability model for a fixed number of model parameters based on a\nset of experiments. To perform maximum likelihood estimation, the output\nprobability density function is required. Forward propagation of input\nuncertainty is established combining Polynomial Chaos and moment matching.\nHigh-order moments of the output distribution are estimated using the\ngeneralized Polynomial Chaos framework. Next, a maximum entropy parametric\ndistribution is matched with the estimated moments. This method is numerically\nvery attractive due to reduced forward sampling and deterministic nature of the\npropagation strategy. The methodology is applied on a wet clutch system for\nwhich certain model variables are considered as stochastic. The number of\nrequired model simulations to achieve the same accuracy as the brute force\nmethodologies is decreased by one order of magnitude. The probability model\nidentified with the high order estimates resulted into a true log-likelihood\nincrease of about 4% since the accuracy of the estimated output probability\ndensity function could be improved up to 47%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 12:04:17 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["De Groote", "Wannes", ""], ["Lefebvre", "Tom", ""], ["Tod", "Georges", ""], ["De Geeter", "Nele", ""], ["Depraetere", "Bruno", ""], ["Van Poppel", "Suzanne", ""], ["Crevecoeur", "Guillaume", ""]]}, {"id": "1908.04695", "submitter": "Lam Yau", "authors": "Ekkehard Glimm, Lillian Yau, and Heike Woehling", "title": "Blinded sample size re-estimation in equivalence testing", "comments": "25 pages, 7 figures", "journal-ref": "Statistics in Biopharmaceutical Research 2021", "doi": "10.1080/19466315.2020.1845232", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates type I error violations that occur when blinded\nsample size reviews are applied in equivalence testing. We give a derivation\nwhich explains why such violations are more pronounced in equivalence testing\nthan in the case of superiority testing. In addition, the amount of type I\nerror inflation is quantified by simulation as well as by some theoretical\nconsiderations. Non-negligible type I error violations arise when blinded\ninterim re-assessments of sample sizes are performed particularly if sample\nsizes are small, but within the range of what is practically relevant.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:10:31 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Glimm", "Ekkehard", ""], ["Yau", "Lillian", ""], ["Woehling", "Heike", ""]]}, {"id": "1908.04748", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Optimal Estimation of Generalized Average Treatment Effects using Kernel\n  Optimal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal inference, a variety of causal effect estimands have been studied,\nincluding the sample, uncensored, target, conditional, optimal subpopulation,\nand optimal weighted average treatment effects. Ad-hoc methods have been\ndeveloped for each estimand based on inverse probability weighting (IPW) and on\noutcome regression modeling, but these may be sensitive to model\nmisspecification, practical violations of positivity, or both. The contribution\nof this paper is twofold. First, we formulate the generalized average treatment\neffect (GATE) to unify these causal estimands as well as their IPW estimates.\nSecond, we develop a method based on Kernel Optimal Matching (KOM) to optimally\nestimate GATE and to find the GATE most easily estimable by KOM, which we term\nthe Kernel Optimal Weighted Average Treatment Effect. KOM provides uniform\ncontrol on the conditional mean squared error of a weighted estimator over a\nclass of models while simultaneously controlling for precision. We study its\ntheoretical properties and evaluate its comparative performance in a simulation\nstudy. We illustrate the use of KOM for GATE estimation in two case studies:\ncomparing spine surgical interventions and studying the effect of peer support\non people living with HIV.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 17:09:02 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:21:44 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1908.04875", "submitter": "Casey Fleeter", "authors": "Casey M. Fleeter, Gianluca Geraci, Daniele E. Schiavazzi, Andrew M.\n  Kahn, Alison L. Marsden", "title": "Multilevel and multifidelity uncertainty quantification for\n  cardiovascular hemodynamics", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113030", "report-no": null, "categories": "q-bio.QM physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches for uncertainty quantification in cardiovascular modeling\npose challenges due to the large number of uncertain inputs and the significant\ncomputational cost of realistic three-dimensional simulations. We propose an\nefficient uncertainty quantification framework utilizing a multilevel\nmultifidelity Monte Carlo estimator to improve the accuracy of hemodynamic\nquantities of interest while maintaining reasonable computational cost. This is\nachieved by leveraging three cardiovascular model fidelities, each with varying\nspatial resolution to rigorously quantify the variability in hemodynamic\noutputs. We employ two low-fidelity models to construct several different\nestimators. Our goal is to investigate and compare the efficiency of estimators\nbuilt from combinations of these low-fidelity and high-fidelity models. We\ndemonstrate this framework on healthy and diseased models of aortic and\ncoronary anatomy, including uncertainties in material property and boundary\ncondition parameters. We seek to demonstrate that for this application it is\npossible to accelerate the convergence of the estimators by utilizing a MLMF\nparadigm. Therefore, we compare our approach to Monte Carlo and multilevel\nMonte Carlo estimators based only on three-dimensional simulations. We\ndemonstrate significant reduction in total computational cost with the MLMF\nestimators. We also examine the differing properties of the MLMF estimators in\nhealthy versus diseased models, as well as global versus local quantities of\ninterest. As expected, global quantities and healthy models show larger\nreductions than local quantities and diseased model, as the latter rely more\nheavily on the highest fidelity model evaluations. In all cases, our workflow\ncoupling Dakota's MLMF estimators with the SimVascular cardiovascular modeling\nframework makes uncertainty quantification feasible for constrained\ncomputational budgets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 22:10:47 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 23:44:26 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fleeter", "Casey M.", ""], ["Geraci", "Gianluca", ""], ["Schiavazzi", "Daniele E.", ""], ["Kahn", "Andrew M.", ""], ["Marsden", "Alison L.", ""]]}, {"id": "1908.04947", "submitter": "Philip Stark", "authors": "Wojciech Jamroga and Peter B. Roenne and Peter Y. A. Ryan and Philip\n  B. Stark", "title": "Risk-Limiting Tallies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many voter-verifiable, coercion-resistant schemes have been proposed, but\neven the most carefully designed systems necessarily leak information via the\nannounced result. In corner cases, this may be problematic. For example, if all\nthe votes go to one candidate then all vote privacy evaporates. The mere\npossibility of candidates getting no or few votes could have implications for\nsecurity in practice: if a coercer demands that a voter cast a vote for such an\nunpopular candidate, then the voter may feel obliged to obey, even if she is\nconfident that the voting system satisfies the standard coercion resistance\ndefinitions. With complex ballots, there may also be a danger of \"Italian\"\nstyle (aka \"signature\") attacks: the coercer demands the voter cast a ballot\nwith a specific, identifying pattern. Here we propose an approach to tallying\nend-to-end verifiable schemes that avoids revealing all the votes but still\nachieves whatever confidence level in the announced result is desired. Now a\ncoerced voter can claim that the required vote must be amongst those that\nremained shrouded. Our approach is based on the well-established notion of\nRisk-Limiting Audits, but here applied to the tally rather than to the audit.\nWe show that this approach counters coercion threats arising in extreme tallies\nand \"Italian\" attacks. We illustrate our approach by applying it to the Selene\nscheme, and we extend the approach to Risk-Limiting Verification, where not all\nvote trackers are revealed, thereby enhancing the coercion mitigation\nproperties of Selene.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 04:23:10 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Jamroga", "Wojciech", ""], ["Roenne", "Peter B.", ""], ["Ryan", "Peter Y. A.", ""], ["Stark", "Philip B.", ""]]}, {"id": "1908.05091", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, James M.S. Wason", "title": "Borrowing of information across patient subgroups in a basket trial\n  based on distributional discrepancy", "comments": "A submitted paper with 15 pages, 2 figures and 2 tables", "journal-ref": "Biostatistics 2020", "doi": "10.1093/biostatistics/kxaa019", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basket trials have emerged as a new class of efficient approaches in oncology\nto evaluate a new treatment in several patient subgroups simultaneously. In\nthis paper, we extend the key ideas to disease areas outside of oncology,\ndeveloping a robust Bayesian methodology for randomised, placebo-controlled\nbasket trials with a continuous endpoint to enable borrowing of information\nacross subtrials with similar treatment effects. After adjusting for\ncovariates, information from a complementary subtrial can be represented into a\ncommensurate prior for the parameter that underpins the subtrial under\nconsideration. We propose using distributional discrepancy to characterise the\ncommensurability between subtrials for appropriate borrowing of information\nthrough a spike-and-slab prior, which is placed on the prior precision factor.\nWhen the basket trial has at least three subtrials, commensurate priors for\npoint-to-point borrowing are combined into a marginal predictive prior,\naccording to the weights transformed from the pairwise discrepancy measures. In\nthis way, only information from subtrial(s) with the most commensurate\ntreatment effect is leveraged. The marginal predictive prior is updated to a\nrobust posterior by the contemporary subtrial data to inform decision making.\nOperating characteristics of the proposed methodology are evaluated through\nsimulations motivated by a real basket trial in chronic diseases. The proposed\nmethodology has advantages compared to other selected Bayesian analysis models,\nfor (i) identifying the most commensurate source of information, and (ii)\ngauging the degree of borrowing from specific subtrials. Numerical results also\nsuggest that our methodology can improve the precision of estimates and,\npotentially, the statistical power for hypothesis testing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:17:22 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:52:43 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 11:14:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zheng", "Haiyan", ""], ["Wason", "James M. S.", ""]]}, {"id": "1908.05338", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, Marc Modat, M.\n  Jorge Cardoso, S\\'ebastien Ourselin, Lauge S{\\o}rensen", "title": "Robust parametric modeling of Alzheimer's disease progression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative characterization of disease progression using longitudinal data\ncan provide long-term predictions for the pathological stages of individuals.\nThis work studies the robust modeling of Alzheimer's disease progression using\nparametric methods. The proposed method linearly maps the individual's age to a\ndisease progression score (DPS) and jointly fits constrained generalized\nlogistic functions to the longitudinal dynamics of biomarkers as functions of\nthe DPS using M-estimation. Robustness of the estimates is quantified using\nbootstrapping via Monte Carlo resampling, and the estimated inflection points\nof the fitted functions are used to temporally order the modeled biomarkers in\nthe disease course. Kernel density estimation is applied to the obtained DPSs\nfor clinical status classification using a Bayesian classifier. Different\nM-estimators and logistic functions, including a novel type proposed in this\nstudy, called modified Stannard, are evaluated on the data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) for robust modeling of volumetric MRI\nand PET biomarkers, CSF measurements, as well as cognitive tests. The results\nshow that the modified Stannard function fitted using the logistic loss\nachieves the best modeling performance with an average normalized MAE of 0.991\nacross all biomarkers and bootstraps. Applied to the ADNI test set, this model\nachieves a multiclass AUC of 0.934 in clinical status classification. The\nobtained results for the proposed model outperform almost all state-of-the-art\nresults in predicting biomarker values and classifying clinical status.\nFinally, the experiments show that the proposed model, trained using abundant\nADNI data, generalizes well to data from the National Alzheimer's Coordinating\nCenter (NACC) with an average normalized MAE of 1.182 and a multiclass AUC of\n0.929.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:26:21 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 16:58:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 14:38:05 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Modat", "Marc", ""], ["Cardoso", "M. Jorge", ""], ["Ourselin", "S\u00e9bastien", ""], ["S\u00f8rensen", "Lauge", ""]]}, {"id": "1908.05339", "submitter": "Hyunji Moon", "authors": "Hyunji Moon, Bomi Song, Hyeonseop Lee", "title": "Mixed pooling of seasonality for time series forecasting: An application\n  to pallet transport data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple seasonal patterns play a key role in time series forecasting,\nespecially for business time series where seasonal effects are often dramatic.\nPrevious approaches including Fourier decomposition, exponential smoothing, and\nseasonal autoregressive integrated moving average (SARIMA) models do not\nreflect the distinct characteristics of each period in seasonal patterns. We\npropose a mixed hierarchical seasonality (MHS) model. Intermediate parameters\nfor each seasonal period are first estimated, and a mixture of intermediate\nparameters is taken. This results in a model that automatically learns the\nrelative importance of each seasonality and addresses the interactions between\nthem. The model is implemented with Stan, a probabilistic language, and was\ncompared with three existing models on a real-world dataset of pallet transport\nfrom a logistic network. Our new model achieved considerable improvements in\nterms of out of sample prediction error (MAPE) and predictive density (ELPD)\ncompared to complete pooling, Fourier decomposition, and SARIMA model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:29:41 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 14:10:51 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Moon", "Hyunji", ""], ["Song", "Bomi", ""], ["Lee", "Hyeonseop", ""]]}, {"id": "1908.05340", "submitter": "Joshua Keller", "authors": "Joshua P. Keller, Joanne Katz, Amid K. Pokhrel, Michael N. Bates,\n  James Tielsch, Scott L. Zeger", "title": "A hierarchical model for estimating exposure-response curves from\n  multiple studies", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cookstove replacement trials have found mixed results on their impact on\nrespiratory health. The limited range of concentrations and small sample sizes\nof individual studies are important factors that may be limiting their\nstatistical power. We present a hierarchical approach to modeling exposure\nconcentrations and pooling data from multiple studies in order to estimate a\ncommon exposure-response curve. The exposure concentration model accommodates\ntemporally sparse, clustered longitudinal observations. The exposure-response\ncurve model provides a flexible, semi-parametric estimate of the\nexposure-response relationship while accommodating heterogeneous clustered\ndata. We apply this model to data from three studies of cookstoves and\nrespiratory infections in children in Nepal, which represent three study types:\ncrossover trial, parallel trial, and case-control study. We find evidence of\nincreased odds of disease for particulate matter concentrations between 50 and\n200 $\\mu$g/m$^3$ and a flattening of the exposure-response curve for higher\nexposure concentrations. The model we present can incorporate additional\nstudies and be applied to other settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:37:15 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Keller", "Joshua P.", ""], ["Katz", "Joanne", ""], ["Pokhrel", "Amid K.", ""], ["Bates", "Michael N.", ""], ["Tielsch", "James", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1908.05372", "submitter": "Zhenyu Zhao", "authors": "Zhenyu Zhao and Totte Harinen", "title": "Uplift Modeling for Multiple Treatments with Cost Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift modeling is an emerging machine learning approach for estimating the\ntreatment effect at an individual or subgroup level. It can be used for\noptimizing the performance of interventions such as marketing campaigns and\nproduct designs. Uplift modeling can be used to estimate which users are likely\nto benefit from a treatment and then prioritize delivering or promoting the\npreferred experience to those users. An important but so far neglected use case\nfor uplift modeling is an experiment with multiple treatment groups that have\ndifferent costs, such as for example when different communication channels and\npromotion types are tested simultaneously. In this paper, we extend standard\nuplift models to support multiple treatment groups with different costs. We\nevaluate the performance of the proposed models using both synthetic and real\ndata. We also describe a production implementation of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 23:35:25 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 03:55:31 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 17:49:57 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhao", "Zhenyu", ""], ["Harinen", "Totte", ""]]}, {"id": "1908.05428", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "With Malice Towards None: Assessing Uncertainty via Equalized Coverage", "comments": "14 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important factor to guarantee a fair use of data-driven recommendation\nsystems is that we should be able to communicate their uncertainty to decision\nmakers. This can be accomplished by constructing prediction intervals, which\nprovide an intuitive measure of the limits of predictive performance. To\nsupport equitable treatment, we force the construction of such intervals to be\nunbiased in the sense that their coverage must be equal across all protected\ngroups of interest. We present an operational methodology that achieves this\ngoal by offering rigorous distribution-free coverage guarantees holding in\nfinite samples. Our methodology, equalized coverage, is flexible as it can be\nviewed as a wrapper around any predictive algorithm. We test the applicability\nof the proposed framework on real data, demonstrating that equalized coverage\nconstructs unbiased prediction intervals, unlike competitive methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 05:50:27 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Romano", "Yaniv", ""], ["Barber", "Rina Foygel", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1908.05627", "submitter": "Lu Wang", "authors": "Lu Wang and Zhengwu Zhang", "title": "Learning Signal Subgraphs from Longitudinal Brain Networks with\n  Symmetric Bilinear Logistic Regression", "comments": "34 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neuroimaging technologies, combined with state-of-the-art data\nprocessing pipelines, have made it possible to collect longitudinal\nobservations of an individual's brain connectome at different ages. It is of\nsubstantial scientific interest to study how brain connectivity varies over\ntime in relation to human cognitive traits. In brain connectomics, the\nstructural brain network for an individual corresponds to a set of\ninterconnections among brain regions. We propose a symmetric bilinear logistic\nregression to learn a set of small subgraphs relevant to a binary outcome from\nlongitudinal brain networks as well as estimating the time effects of the\nsubgraphs. We enforce the extracted signal subgraphs to have clique structure\nwhich has appealing interpretations as they can be related to neurological\ncircuits. The time effect of each signal subgraph reflects how its predictive\neffect on the outcome varies over time, which may improve our understanding of\ninteractions between the aging of brain structure and neurological disorders.\nApplication of this method on longitudinal brain connectomics and cognitive\ncapacity data shows interesting discovery of relevant interconnections among a\nsmall set of brain regions in frontal and temporal lobes with better predictive\nperformance than competitors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 16:36:02 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wang", "Lu", ""], ["Zhang", "Zhengwu", ""]]}, {"id": "1908.05711", "submitter": "Maia Lesosky", "authors": "Maia Lesosky, Janet Raboud, Tracy Glass, Elaine Abrams, Landon Myer", "title": "A simulation model for longitudinal HIV viral load and\n  mother-to-child-transmission in pregnant and postpartum women", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript describes the model specification, including input and output\nmeasures, dependencies, and structure for VLSiM-PPW, a Monte Carlo type model\nfor HIV adherence, viral load and vertical transmission in pregnant and\nbreastfeeding women.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 06:52:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lesosky", "Maia", ""], ["Raboud", "Janet", ""], ["Glass", "Tracy", ""], ["Abrams", "Elaine", ""], ["Myer", "Landon", ""]]}, {"id": "1908.05745", "submitter": "Guanyu Hu", "authors": "Jieying Jiao, Guanyu Hu, Jun Yan", "title": "A Bayesian marked spatial point processes model for basketball shot\n  chart", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports (2020)", "doi": "10.1515/jqas-2019-0106", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success rate of a basketball shot may be higher at locations where a\nplayer makes more shots. For a marked spatial point process, this means that\nthe mark and the intensity are associated. We propose a Bayesian joint model\nfor the mark and the intensity of marked point processes, where the intensity\nis incorporated in the mark model as a covariate. Inferences are done with a\nMarkov chain Monte Carlo algorithm. Two Bayesian model comparison criteria, the\nDeviance Information Criterion and the Logarithm of the Pseudo-Marginal\nLikelihood, were used to assess the model. The performances of the proposed\nmethods were examined in extensive simulation studies. The proposed methods\nwere applied to the shot charts of four players (Curry, Harden, Durant, and\nJames) in the 2017--2018 regular season of the National Basketball Association\nto analyze their shot intensity in the field and the field goal percentage in\ndetail. Application to the top 50 most frequent shooters in the season suggests\nthat the field goal percentage and the shot intensity are positively associated\nfor a majority of the players. The fitted parameters were used as inputs in a\nsecondary analysis to cluster the players into different groups.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:16:49 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 17:21:23 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 20:00:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jiao", "Jieying", ""], ["Hu", "Guanyu", ""], ["Yan", "Jun", ""]]}, {"id": "1908.05752", "submitter": "Andrii Babii", "authors": "Andrii Babii and Rohit Kumar", "title": "Isotonic Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation and inference for the isotonic regression\nat the boundary point, an object that is particularly interesting and required\nin the analysis of monotone regression discontinuity designs. We show that the\nisotonic regression is inconsistent in this setting and derive the asymptotic\ndistributions of boundary corrected estimators. Interestingly, the boundary\ncorrected estimators can be bootstrapped without subsampling or additional\nnonparametric smoothing which is not the case for the interior point. The Monte\nCarlo experiments indicate that shape restrictions can improve dramatically the\nfinite-sample performance of unrestricted estimators. Lastly, we apply the\nisotonic regression discontinuity designs to estimate the causal effect of\nincumbency in the U.S. House elections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:44:44 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 20:57:46 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 20:23:32 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 21:55:47 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 17:22:30 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2020 19:53:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""], ["Kumar", "Rohit", ""]]}, {"id": "1908.05873", "submitter": "Fan Yin", "authors": "Fan Yin, Nolan Edward Phillips, Carter T. Butts", "title": "Selection of Exponential-Family Random Graph Models via Held-Out\n  Predictive Evaluation (HOPE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models for networks with complex dependencies pose particular\nchallenges for model selection and evaluation. In particular, many\nwell-established statistical tools for selecting between models assume\nconditional independence of observations and/or conventional asymptotics, and\ntheir theoretical foundations are not always applicable in a network modeling\ncontext. While simulation-based approaches to model adequacy assessment are now\nwidely used, there remains a need for procedures that quantify a model's\nperformance in a manner suitable for selecting among competing models. Here, we\npropose to address this issue by developing a predictive evaluation strategy\nfor exponential family random graph models that is analogous to\ncross-validation. Our approach builds on the held-out predictive evaluation\n(HOPE) scheme introduced by Wang et al. (2016) to assess imputation\nperformance. We systematically hold out parts of the observed network to:\nevaluate how well the model is able to predict the held-out data; identify\nwhere the model performs poorly based on which data are held-out, indicating\ne.g. potential weaknesses; and calculate general summaries of predictive\nperformance that can be used for model selection. As such, HOPE can assist\nresearchers in improving models by indicating where a model performs poorly,\nand by quantitatively comparing predictive performance across competing models.\nThe proposed method is applied to model selection problem of two well-known\ndata sets, and the results are compared to those obtained via nominal AIC and\nBIC scores.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 07:40:56 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 04:29:35 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yin", "Fan", ""], ["Phillips", "Nolan Edward", ""], ["Butts", "Carter T.", ""]]}, {"id": "1908.05896", "submitter": "Ruby Chanchal", "authors": "Ruby Chanchal, Vaishali Gupta, Amit Kumar Misra", "title": "Stochastic Comparisons of Series and Parallel Systems with Topp-Leone\n  Generated Family of Distributions", "comments": "2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we stochastically compare the series and parallel systems\nhaving Topp Leone generated family of distributions. We consider that the\nlifetimes of the components of the systems have either the different shape\nparameters when the scale parameters are fixed or the different scale\nparameters when the shape parameters are fixed and established some ordering\nresults With the help of vector majorization technique.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:03:34 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chanchal", "Ruby", ""], ["Gupta", "Vaishali", ""], ["Misra", "Amit Kumar", ""]]}, {"id": "1908.06075", "submitter": "Xueying Tang", "authors": "Xueying Tang, Zhi Wang, Jingchen Liu, and Zhiliang Ying", "title": "An Exploratory Analysis of the Latent Structure of Process Data via\n  Action Sequence Autoencoder", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations have become a popular tool of assessing complex skills\nsuch as problem-solving skills. Log files of computer-based items record the\nentire human-computer interactive processes for each respondent. The response\nprocesses are very diverse, noisy, and of nonstandard formats. Few generic\nmethods have been developed for exploiting the information contained in process\ndata. In this article, we propose a method to extract latent variables from\nprocess data. The method utilizes a sequence-to-sequence autoencoder to\ncompress response processes into standard numerical vectors. It does not\nrequire prior knowledge of the specific items and human-computers interaction\npatterns. The proposed method is applied to both simulated and real process\ndata to demonstrate that the resulting latent variables extract useful\ninformation from the response processes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:55:20 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Tang", "Xueying", ""], ["Wang", "Zhi", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1908.06081", "submitter": "Michael Thrun PhD", "authors": "Michael C. Thrun, Tino Gehlert, Alfred Ultsch", "title": "Analyzing the Fine Structure of Distributions", "comments": "66 pages, 81 figures, accepted in PLOS ONE", "journal-ref": null, "doi": "10.1371/journal.pone.0238835", "report-no": "PONE-D-19-19081R4", "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One aim of data mining is the identification of interesting structures in\ndata. For better analytical results, the basic properties of an empirical\ndistribution, such as skewness and eventual clipping, i.e. hard limits in value\nranges, need to be assessed. Of particular interest is the question of whether\nthe data originate from one process or contain subsets related to different\nstates of the data producing process. Data visualization tools should deliver a\nclear picture of the univariate probability density distribution (PDF) for each\nfeature. Visualization tools for PDFs typically use kernel density estimates\nand include both the classical histogram, as well as the modern tools like\nridgeline plots, bean plots and violin plots. If density estimation parameters\nremain in a default setting, conventional methods pose several problems when\nvisualizing the PDF of uniform, multimodal, skewed distributions and\ndistributions with clipped data, For that reason, a new visualization tool\ncalled the mirrored density plot (MD plot), which is specifically designed to\ndiscover interesting structures in continuous features, is proposed. The MD\nplot does not require adjusting any parameters of density estimation, which is\nwhat may make the use of this plot compelling particularly to non-experts. The\nvisualization tools in question are evaluated against statistical tests with\nregard to typical challenges of explorative distribution analysis. The results\nof the evaluation are presented using bimodal Gaussian, skewed distributions\nand several features with already published PDFs. In an exploratory data\nanalysis of 12 features describing quarterly financial statements, when\nstatistical testing poses a great difficulty, only the MD plots can identify\nthe structure of their PDFs. In sum, the MD plot outperforms the above\nmentioned methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 19:24:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 07:01:09 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 08:59:10 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Thrun", "Michael C.", ""], ["Gehlert", "Tino", ""], ["Ultsch", "Alfred", ""]]}, {"id": "1908.06325", "submitter": "Michael Pfarrhofer", "authors": "Michael Pfarrhofer", "title": "Measuring international uncertainty using global vector autoregressions\n  with drifting parameters", "comments": "JEL: C11, C55, E32, E66, G15; Keywords: Bayesian state-space\n  modeling, hierarchical priors, factor stochastic volatility, stochastic\n  volatility in mean", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the time-varying impacts of international\nmacroeconomic uncertainty shocks. We use a global vector autoregressive\nspecification with drifting coefficients and factor stochastic volatility in\nthe errors to model six economies jointly. The measure of uncertainty is\nconstructed endogenously by estimating a scalar driving the innovation\nvariances of the latent factors, which is also included in the mean of the\nprocess. To achieve regularization, we use Bayesian techniques for estimation,\nand introduce a set of hierarchical global-local priors. The adopted priors\ncenter the model on a constant parameter specification with homoscedastic\nerrors, but allow for time-variation if suggested by likelihood information.\nMoreover, we assume coefficients across economies to be similar, but provide\nsufficient flexibility via the hierarchical prior for country-specific\nidiosyncrasies. The results point towards pronounced real and financial effects\nof uncertainty shocks in all countries, with differences across economies and\nover time.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 17:47:19 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 14:04:33 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Pfarrhofer", "Michael", ""]]}, {"id": "1908.06340", "submitter": "Christian R\\\"over", "authors": "Stefan Andreas, Christian R\\\"over, Judith Heinz, Sebastian Straube,\n  Henrik Watz, Tim Friede", "title": "Decline of COPD exacerbations in clinical trials over two decades -- a\n  systematic review and meta-regression", "comments": "11 pages, 4 figures", "journal-ref": "Respiratory Research 20:186, 2019", "doi": "10.1186/s12931-019-1163-2", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BACKGROUND: An important goal of chronic obstructive pulmonary disease (COPD)\ntreatment is to reduce the frequency of exacerbations. Some observations\nsuggest a decline in exacerbation rates in clinical trials over time. A more\nsystematic understanding would help to improve the design and interpretation of\nCOPD trials.\n  METHODS: We performed a systematic review and meta-regression of the placebo\ngroups in published randomized controlled trials reporting exacerbations as an\noutcome. A Bayesian negative binomial model was developed to accommodate\nresults that are reported in different formats; results are reported with\ncredible intervals (CI) and posterior tail probabilities ($p_B$).\n  RESULTS: Of 1114 studies identified by our search, 55 were ultimately\nincluded. Exacerbation rates decreased by 6.7% (95% CI (4.4, 9.0); $p_B$ <\n0.001) per year, or 50% (95% CI (36, 61)) per decade. Adjusting for available\nstudy and baseline characteristics such as forced expiratory volume in 1 s\n(FEV1) did not alter the observed trend considerably. Two subsets of studies,\none using a true placebo group and the other allowing inhaled corticosteroids\nin the \"placebo\" group, also yielded consistent results.\n  CONCLUSIONS: In conclusion, this meta-regression indicates that the rate of\nCOPD exacerbations decreased over the past two decades to a clinically relevant\nextent independent of important prognostic factors. This suggests that care is\nneeded in the design of new trials or when comparing results from older trials\nwith more recent ones. Also a considerable effect of adjunct therapy on COPD\nexacerbations can be assumed.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 20:32:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Andreas", "Stefan", ""], ["R\u00f6ver", "Christian", ""], ["Heinz", "Judith", ""], ["Straube", "Sebastian", ""], ["Watz", "Henrik", ""], ["Friede", "Tim", ""]]}, {"id": "1908.06370", "submitter": "Omid Sedehi", "authors": "Omid Sedehi, Lambros S. Katafygiotis, Costas Papadimitriou", "title": "Hierarchical Bayesian Operational Modal Analysis: Theory and\n  Computations", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 140, June 2020,\n  106663", "doi": "10.1016/j.ymssp.2020.106663", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hierarchical Bayesian modeling framework for the\nuncertainty quantification in modal identification of linear dynamical systems\nusing multiple vibration data sets. This novel framework integrates the\nstate-of-the-art Bayesian formulations into a hierarchical setting aiming to\ncapture both the identification precision and the ensemble variability prompted\ndue to modeling errors. Such cutting-edge developments have been absent from\nthe modal identification literature, sustained as a long-standing problem at\nthe research spotlight. Central to this framework is a Gaussian hyper\nprobability model, whose mean and covariance matrix are unknown encapsulating\nthe uncertainty of the modal parameters. Detailed computation of this\nhierarchical model is addressed under two major algorithms using Markov chain\nMonte Carlo (MCMC) sampling and Laplace asymptotic approximation methods. Since\nfor a small number of data sets the hyper covariance matrix is often\nunidentifiable, a practical remedy is suggested through the eigenbasis\ntransformation of the covariance matrix, which effectively reduces the number\nof unknown hyper-parameters. It is also proved that under some conditions the\nmaximum a posteriori (MAP) estimation of the hyper mean and covariance coincide\nwith the ensemble mean and covariance computed using the MAP estimations\ncorresponding to multiple data sets. This interesting finding addresses\nrelevant concerns related to the outcome of the mainstream Bayesian methods in\ncapturing the stochastic variability from dissimilar data sets. Finally, the\ndynamical response of a prototype structure tested on a shaking table subjected\nto Gaussian white noise base excitation and the ambient vibration measurement\nof a cable footbridge are employed to demonstrate the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 03:56:49 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 06:00:29 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 11:26:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sedehi", "Omid", ""], ["Katafygiotis", "Lambros S.", ""], ["Papadimitriou", "Costas", ""]]}, {"id": "1908.06687", "submitter": "Lucie Biard", "authors": "Lucie Biard, Anne Bergeron, Sylvie Chevret", "title": "Bayesian models for survival data of clinical trials: Comparison of\n  implementations using R software", "comments": "21 pages, 8 figures (5 as supplementary material), 5 tables (4 as\n  supplementary material); Corrected typos in arXiv abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To provide guidance for the use of the main functions available in\nR for performing post hoc Bayesian analysis of a randomized clinical trial with\na survival endpoint using proportional hazard models. Study Design and Setting:\nData derived from the ALLOZITHRO trial, conducted with 465 patients after\nallograft to prevent pulmonary complications and allocated between azithromycin\nand placebo; airflow decline-free survival at 2 years after randomization was\nthe main endpoint. Results: Despite heterogeneity in modeling assumptions, in\nparticular for the baseline hazard (parametric or nonparametric), and in\nestimation methods, Bayesian posterior mean hazard ratio (HR) estimates of\nazithromycin effect were close to those obtained by the maximum likelihood\napproach. Conclusion: Bayesian models can be implemented using various R\npackages, providing results in close agreement with the maximum likelihood\nestimates. These models provide probabilistic statements that could not be\nobtained otherwise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:43:54 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 07:52:19 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 16:48:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Biard", "Lucie", ""], ["Bergeron", "Anne", ""], ["Chevret", "Sylvie", ""]]}, {"id": "1908.06731", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz and Greta Bia{\\l}kowska and Krzysztof\n  Marcinkowski and Magdalena Ma\\'slak and Piotr Opiela and Robert Pater and\n  Katarzyna Zadroga", "title": "Enhancing the Demand for Labour survey by including skills from online\n  job advertisements using model-assisted calibration", "comments": null, "journal-ref": "2021", "doi": "10.18148/srm/2021.v15i2.7670", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article we describe an enhancement to the Demand for Labour (DL)\nsurvey conducted by Statistics Poland, which involves the inclusion of skills\nobtained from online job advertisements. The main goal is to provide estimates\nof the demand for skills (competences), which is missing in the DL survey. To\nachieve this, we apply a data integration approach combining traditional\ncalibration with the LASSO-assisted approach to correct representation error in\nthe online data. Faced with the lack of access to unit-level data from the DL\nsurvey, we use estimated population totals and propose a~bootstrap approach\nthat accounts for the uncertainty of totals reported by Statistics Poland. We\nshow that the calibration estimator assisted with LASSO outperforms traditional\ncalibration in terms of standard errors and reduces representation bias in\nskills observed in online job ads. Our empirical results show that online data\nsignificantly overestimate interpersonal, managerial and self-organization\nskills while underestimating technical and physical skills. This is mainly due\nto the under-representation of occupations categorised as Craft and Related\nTrades Workers and Plant and Machine Operators and Assemblers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:43:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Bia\u0142kowska", "Greta", ""], ["Marcinkowski", "Krzysztof", ""], ["Ma\u015blak", "Magdalena", ""], ["Opiela", "Piotr", ""], ["Pater", "Robert", ""], ["Zadroga", "Katarzyna", ""]]}, {"id": "1908.06746", "submitter": "Mohsen Shahhosseini", "authors": "Mohsen Shahhosseini, Rafael A. Martinez-Feria, Guiping Hu, Sotirios V.\n  Archontoulis", "title": "Maize Yield and Nitrate Loss Prediction with Machine Learning Algorithms", "comments": null, "journal-ref": "Environmental Research Letters 14(12) (2019) 124026", "doi": "10.1088/1748-9326/ab5268", "report-no": null, "categories": "q-bio.OT cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-season prediction of crop production outcomes such as grain yields and N\nlosses can provide insights to stakeholders when making decisions. Simulation\nmodels can assist in scenario planning, but their use is limited because of\ndata requirements and long run times. Thus, there is a need for more\ncomputationally expedient approaches to scale up predictions. We evaluated the\npotential of five machine learning (ML) algorithms as meta-models for a\ncropping systems simulator (APSIM) to inform future decision-support tool\ndevelopment. We asked: 1) How well do ML meta-models predict maize yield and N\nlosses using pre-season information? 2) How many data are needed to train ML\nalgorithms to achieve acceptable predictions?; 3) Which input data variables\nare most important for accurate prediction?; and 4) Do ensembles of ML\nmeta-models improve prediction? The simulated dataset included more than 3\nmillion genotype, environment and management scenarios. Random forests most\naccurately predicted maize yield and N loss at planting time, with a RRMSE of\n14% and 55%, respectively. ML meta-models reasonably reproduced simulated maize\nyields but not N loss. They also differed in their sensitivities to the size of\nthe training dataset. Across all ML models, yield prediction error decreased by\n10-40% as the training dataset increased from 0.5 to 1.8 million data points,\nwhereas N loss prediction error showed no consistent pattern. ML models also\ndiffered in their sensitivities to input variables. Averaged across all ML\nmodels, weather conditions, soil properties, management information and initial\nconditions were roughly equally important when predicting yields. Modest\nprediction improvements resulted from ML ensembles. These results can help\naccelerate progress in coupling simulation models and ML toward developing\ndynamic decision support tools for pre-season management.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:43:24 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 18:23:37 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 17:48:56 GMT"}, {"version": "v4", "created": "Thu, 19 Sep 2019 15:12:00 GMT"}, {"version": "v5", "created": "Fri, 6 Nov 2020 18:18:11 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Shahhosseini", "Mohsen", ""], ["Martinez-Feria", "Rafael A.", ""], ["Hu", "Guiping", ""], ["Archontoulis", "Sotirios V.", ""]]}, {"id": "1908.06822", "submitter": "Md Mahsin", "authors": "Md Mahsin, Rob Deardon, Patrick Brown", "title": "Geographically-dependent individual-level models for infectious diseases\n  transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious disease models can be of great use for understanding the\nunderlying mechanisms that influence the spread of diseases and predicting\nfuture disease progression. Modeling has been increasingly used to evaluate the\npotential impact of different control measures and to guide public health\npolicy decisions. In recent years, there has been rapid progress in developing\nspatio-temporal modeling of infectious diseases and an example of such recent\ndevelopments is the discrete time individual-level models (ILMs). These models\nare well developed and provide a common framework for modeling many disease\nsystems, however, they assume the probability of disease transmission between\ntwo individuals depends only on their spatial separation and not on their\nspatial locations. In cases where spatial location itself is important for\nunderstanding the spread of emerging infectious diseases and identifying their\ncauses, it would be beneficial to incorporate the effect of spatial location in\nthe model. In this study, we thus generalize the ILMs to a new class of\ngeographically-dependent ILMs (GD-ILMs), to allow for the evaluation of the\neffect of spatially varying risk factors (e.g., education, environmental), as\nwell as unobserved spatial structure, upon the transmission of infectious\ndisease. Specifically, we consider a conditional autoregressive model to\ncapture the effects of unobserved spatially structured latent covariates or\nmeasurement error. This results in flexible infectious disease models that can\nbe used for formulating etiological hypotheses and identifying geographical\nregions of unusually high risk to formulate preventive action. The reliability\nof these models are investigated on a combination of simulated epidemic data\nand Alberta seasonal influenza outbreak data (2009). This new class of models\nis fitted to data within a Bayesian statistical framework using MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 23:02:51 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mahsin", "Md", ""], ["Deardon", "Rob", ""], ["Brown", "Patrick", ""]]}, {"id": "1908.07031", "submitter": "Weipeng Huang", "authors": "Weipeng Huang, Guangyuan Piao, Raul Moreno, Neil J. Hurley", "title": "Partially Observable Markov Decision Process Modelling for Assessing\n  Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering has been shown to be valuable in many scenarios.\nDespite its usefulness to many situations, there is no agreed methodology on\nhow to properly evaluate the hierarchies produced from different techniques,\nparticularly in the case where ground-truth labels are unavailable. This\nmotivates us to propose a framework for assessing the quality of hierarchical\nclustering allocations which covers the case of no ground-truth information.\nThis measurement is useful, e.g., to assess the hierarchical structures used by\nonline retailer websites to display their product catalogues. Our framework is\none of the few attempts for the hierarchy evaluation from a decision-theoretic\nperspective. We model the process as a bot searching stochastically for items\nin the hierarchy and establish a measure representing the degree to which the\nhierarchy supports this search. We employ Partially Observable Markov Decision\nProcesses (POMDP) to model the uncertainty, the decision making, and the\ncognitive return for searchers in such a scenario.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 19:13:27 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 13:56:23 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 17:20:36 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 01:31:59 GMT"}, {"version": "v5", "created": "Tue, 13 Oct 2020 20:35:41 GMT"}, {"version": "v6", "created": "Wed, 4 Nov 2020 13:28:16 GMT"}, {"version": "v7", "created": "Tue, 8 Dec 2020 09:07:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Huang", "Weipeng", ""], ["Piao", "Guangyuan", ""], ["Moreno", "Raul", ""], ["Hurley", "Neil J.", ""]]}, {"id": "1908.07084", "submitter": "Wei Vivian Li", "authors": "Wei Vivian Li and Jingyi Jessica Li", "title": "Issues arising from benchmarking single-cell RNA sequencing imputation\n  methods", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On June 25th, 2018, Huang et al. published a computational method SAVER on\nNature Methods for imputing dropout gene expression levels in single cell RNA\nsequencing (scRNA-seq) data. Huang et al. performed a set of comprehensive\nbenchmarking analyses, including comparison with the data from RNA fluorescence\nin situ hybridization, to demonstrate that SAVER outperformed two existing\nscRNA-seq imputation methods, scImpute and MAGIC. However, their computational\nanalyses were based on semi-synthetic data that the authors had generated\nfollowing the Poisson-Gamma model used in the SAVER method. We have therefore\nre-examined Huang et al.'s study. We find that the semi-synthetic data have\nvery different properties from those of real scRNA-seq data and that the cell\nclusters used for benchmarking are inconsistent with the cell types labeled by\nbiologists. We show that a reanalysis based on real scRNA-seq data and grounded\non biological knowledge of cell types leads to different results and\nconclusions from those of Huang et al.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 21:50:09 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Li", "Wei Vivian", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "1908.07100", "submitter": "Benjamin Campbell", "authors": "Benjamin Campbell", "title": "Alliances and Conflict, or Conflict and Alliances? Appraising the Causal\n  Effect of Alliances on Conflict", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deterrent effect of military alliances is well documented and widely\naccepted. However, such work has typically assumed that alliances are\nexogenous. This is problematic as alliances may simultaneously influence the\nprobability of conflict and be influenced by the probability of conflict.\nFailing to account for such endogeneity produces overly simplistic theories of\nalliance politics and barriers to identifying the causal effect of alliances on\nconflict. In this manuscript, I propose a solution to this theoretical and\nempirical modeling challenge. Synthesizing theories of alliance formation and\nthe alliance-conflict relationship, I innovate an endogenous theory of\nalliances and conflict. I then test this theory using innovative generalized\njoint regression models that allow me to endogenize alliance formation on the\ncausal path to conflict. Once doing so, I ultimately find that alliances\nneither deter nor provoke aggression. This has significant implications for our\nunderstanding of interstate conflict and alliance politics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 23:03:32 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Campbell", "Benjamin", ""]]}, {"id": "1908.07112", "submitter": "Satrajit Roychoudhury", "authors": "Satrajit Roychoudhury, Keaven M Anderson, Jiabu Ye, and Pralay\n  Mukhopadhyay", "title": "Robust Design and Analysis of Clinical Trials With Non-proportional\n  Hazards: A Straw Man Guidance from a Cross-pharma Working Group", "comments": null, "journal-ref": "Statistics in Biopharmaceutical Research 2021", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss of power and clear description of treatment differences are key issues\nin designing and analyzing a clinical trial where non-proportional hazard is a\npossibility. A log-rank test may be very inefficient and interpretation of the\nhazard ratio estimated using Cox regression is potentially problematic. In this\ncase, the current ICH E9 (R1) addendum would suggest designing a trial with a\nclinically relevant estimand, e.g., expected life gain. This approach considers\nappropriate analysis methods for supporting the chosen estimand. However, such\nan approach is case specific and may suffer lack of power for important choices\nof the underlying alternate hypothesis distribution. On the other hand, there\nmay be a desire to have robust power under different deviations from\nproportional hazards. Also, we would contend that no single number adequately\ndescribes treatment effect under non-proportional hazards scenarios. The\ncross-pharma working group has proposed a combination test to provide robust\npower under a variety of alternative hypotheses. These can be specified for\nprimary analysis at the design stage and methods appropriately accounting for\ncombination test correlations are efficient for a variety of scenarios. We have\nprovided design and analysis considerations based on a combination test under\ndifferent non-proportional hazard types and present a straw man proposal for\npractitioners. The proposals are illustrated with real life example and\nsimulation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 00:01:29 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 20:05:11 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 04:07:46 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 06:02:12 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Roychoudhury", "Satrajit", ""], ["Anderson", "Keaven M", ""], ["Ye", "Jiabu", ""], ["Mukhopadhyay", "Pralay", ""]]}, {"id": "1908.07151", "submitter": "Collin A. Politsch", "authors": "Collin A. Politsch, Jessi Cisewski-Kehe, Rupert A. C. Croft, Larry\n  Wasserman", "title": "Trend Filtering -- I. A Modern Statistical Tool for Time-Domain\n  Astronomy and Astronomical Spectroscopy", "comments": "Part 1 of 2; 15 pages, 3 figures", "journal-ref": null, "doi": "10.1093/mnras/staa106", "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.EP astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of denoising a one-dimensional signal possessing varying degrees\nof smoothness is ubiquitous in time-domain astronomy and astronomical\nspectroscopy. For example, in the time domain, an astronomical object may\nexhibit a smoothly varying intensity that is occasionally interrupted by abrupt\ndips or spikes. Likewise, in the spectroscopic setting, a noiseless spectrum\ntypically contains intervals of relative smoothness mixed with localized higher\nfrequency components such as emission peaks and absorption lines. In this work,\nwe present trend filtering, a modern nonparametric statistical tool that yields\nsignificant improvements in this broad problem space of denoising $spatially$\n$heterogeneous$ signals. When the underlying signal is spatially heterogeneous,\ntrend filtering is superior to any statistical estimator that is a linear\ncombination of the observed data---including kernel smoothers, LOESS, smoothing\nsplines, Gaussian process regression, and many other popular methods.\nFurthermore, the trend filtering estimate can be computed with practical and\nscalable efficiency via a specialized convex optimization algorithm, e.g.\nhandling sample sizes of $n\\gtrsim10^7$ within a few minutes. In a companion\npaper, we explicitly demonstrate the broad utility of trend filtering to\nobservational astronomy by carrying out a diverse set of spectroscopic and\ntime-domain analyses.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 03:52:32 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 17:53:20 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 16:31:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Politsch", "Collin A.", ""], ["Cisewski-Kehe", "Jessi", ""], ["Croft", "Rupert A. C.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1908.07193", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo, Ricardo Silva, Soong M Kang, Arthur Gretton", "title": "Counterfactual Distribution Regression for Structured Inference", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems in which a system receives external \\emph{perturbations}\nfrom time to time. For instance, the system can be a train network in which\nparticular lines are repeatedly disrupted without warning, having an effect on\npassenger behavior. The goal is to predict changes in the behavior of the\nsystem at particular points of interest, such as passenger traffic around\nstations at the affected rails. We assume that the data available provides\nrecords of the system functioning at its \"natural regime\" (e.g., the train\nnetwork without disruptions) and data on cases where perturbations took place.\nThe inference problem is how information concerning perturbations, with\nparticular covariates such as location and time, can be generalized to predict\nthe effect of novel perturbations. We approach this problem from the point of\nview of a mapping from the counterfactual distribution of the system behavior\nwithout disruptions to the distribution of the disrupted system. A variant on\n\\emph{distribution regression} is developed for this setup.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:13:01 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Colombo", "Nicolo", ""], ["Silva", "Ricardo", ""], ["Kang", "Soong M", ""], ["Gretton", "Arthur", ""]]}, {"id": "1908.07265", "submitter": "Satrajit Roychoudhury", "authors": "Satrajit Roychoudhury and Beat Neuenschwander", "title": "Bayesian leveraging of historical control data for a clinical trial with\n  time-to-event endpoint", "comments": null, "journal-ref": "Statistics in Medicine 2020", "doi": "10.1002/sim.8456", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent 21st Century Cures Act propagates innovations to accelerate the\ndiscovery, development, and delivery of 21st century cures. It includes the\nbroader application of Bayesian statistics and the use of evidence from\nclinical expertise. An example of the latter is the use of trial-external (or\nhistorical) data, which promises more efficient or ethical trial designs. We\npropose a Bayesian meta-analytic approach to leveraging historical data for\ntime-to-event endpoints, which are common in oncology and cardiovascular\ndiseases. The approach is based on a robust hierarchical model for piecewise\nexponential data. It allows for various degrees of between trial-heterogeneity\nand for leveraging individual as well as aggregate data. An ovarian carcinoma\ntrial and a non-small-cell cancer trial illustrate methodological and practical\naspects of leveraging historical data for the analysis and design of\ntime-to-event trials.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 10:34:24 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 01:54:24 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Roychoudhury", "Satrajit", ""], ["Neuenschwander", "Beat", ""]]}, {"id": "1908.07279", "submitter": "Mostafa Mansour Mr.", "authors": "Mostafa Mansour, Oleg Stepanov", "title": "Indoor Navigation Using Information From A Map And A Rangefinder", "comments": "23rd Saint Petersburg International Conference on Integrated\n  Navigation Systems, ICINS 2016 - Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of indoor navigation of mobile objects, using a map and\nmeasurements of distances to the walls is considered. A nonlinear filtering\nproblem aimed at calculating the optimal, in the root-mean-square sense, of the\nsought parameters is formulated in the context of the Bayesian approach. The\nalgorithm for its solution based on the point-mass method is described. The\nsimulation results illustrating the advantages of the proposed problem\nstatement and the resultant algorithm are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 11:31:46 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Mansour", "Mostafa", ""], ["Stepanov", "Oleg", ""]]}, {"id": "1908.07372", "submitter": "Santosh Kumar Radha", "authors": "Santosh Kumar Radha", "title": "Stochastic differential theory of cricket", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.PR stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new formalism for analyzing the progression of cricket game using\nStochastic differential equation (SDE) is introduced. This theory enables a\nquantitative way of representing every team using three key variables which\nhave physical meaning associated with them. This is in contrast with the\ntraditional system of rating/ranking teams based on combination of different\nstatical cumulants. Further more, using this formalism, a new method to\ncalculate the winning probability as a progression of number of balls is given.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:59:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Radha", "Santosh Kumar", ""]]}, {"id": "1908.07384", "submitter": "Robert Eyre", "authors": "Robert Eyre, Flavia De Luca, Filippo Simini", "title": "Social media usage reveals how regions recover after natural disaster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of nowcasting and forecasting the effect of natural disasters\n(e.g. earthquakes, floods, hurricanes) on assets, people and society is of\nprimary importance for assessing the ability of such systems to recover from\nextreme events. Traditional disaster recovery estimates, such as surveys and\ninterviews, are usually costly, time consuming and do not scale. Here we\npresent a methodology to indirectly estimate the post-emergency recovery status\n('downtime') of small businesses in urban areas looking at their online posting\nactivity on social media. Analysing the time series of posts before and after\nan event, we quantify the downtime of small businesses for three natural\ndisasters occurred in Nepal, Puerto Rico and Mexico. A convenient and reliable\nmethod for nowcasting the post-emergency recovery status of economic activities\ncould help local governments and decision makers to better target their\ninterventions and distribute the available resources more effectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 14:21:20 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Eyre", "Robert", ""], ["De Luca", "Flavia", ""], ["Simini", "Filippo", ""]]}, {"id": "1908.07390", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento and Vera Costa", "title": "An Overview of Statistical Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of statistical software in academia and enterprises has been evolving\nover the last years. More often than not, students, professors, workers, and\nusers, in general, have all had, at some point, exposure to statistical\nsoftware. Sometimes, difficulties are felt when dealing with such type of\nsoftware. Very few persons have theoretical knowledge to clearly understand\nsoftware configurations or settings, and sometimes even the presented results.\nVery often, the users are required by academies or enterprises to present\nreports, without the time to explore or understand the results or tasks\nrequired to do an optimal preparation of data or software settings. In this\nwork, we present a statistical overview of some theoretical concepts, to\nprovide fast access to some concepts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:54:50 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Costa", "Vera", ""]]}, {"id": "1908.07409", "submitter": "Ritwik Bhaduri", "authors": "Ritwik Bhaduri, Soham Bonnerjee, Subhrajyoty Roy", "title": "Onset detection: A new approach to QBH system", "comments": "30 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query by Humming (QBH) is a system to provide a user with the song(s) which\nthe user hums to the system. Current QBH method requires the extraction of\nonset and pitch information in order to track similarity with various versions\nof different songs. However, we here focus on detecting precise onsets only and\nuse them to build a QBH system which is better than existing methods in terms\nof speed and memory and empirically in terms of accuracy. We also provide\nstatistical analogy for onset detection functions and provide a measure of\nerror in our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 14:44:32 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 18:40:49 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bhaduri", "Ritwik", ""], ["Bonnerjee", "Soham", ""], ["Roy", "Subhrajyoty", ""]]}, {"id": "1908.07514", "submitter": "Federico Zertuche", "authors": "Federico Zertuche and Abigail Meza-Pe\\~naloza", "title": "A Parametric Bootstrap for the Mean Measure of Divergence", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than $50$ years the {\\it Mean Measure of Divergence} (MMD) has been\none of the most prominent tools used in anthropology for the study of\nnon-metric traits. However, one of the problems, in anthropology including\npalaeoanthropology (more often there), is the lack of big enough samples or the\nexistence of samples without sufficiently measured traits. Since 1969, with the\nadvent of bootstrapping techniques, this issue has been tackled successfully in\nmany different ways. Here, we present a parametric bootstrap technique based on\nthe fact that the transformed $ \\theta $, obtained from the Anscombe\ntransformation to stabilize the variance, nearly follows a normal distribution\nwith zero mean and variance $ \\sigma^2 = 1 / (N + 1/2) $, where $ N $ is the\nsize of the measured trait. When the probabilistic distribution is known,\nparametric procedures offer more powerful results than non-parametric ones. We\nprofit from knowing the probabilistic distribution of $ \\theta $ to develop a\nparametric bootstrapping method. We explain it carefully with mathematical\nsupport. We give examples, both with artificial data and with real ones. Our\nresults show that this parametric bootstrap procedure is a powerful tool to\nstudy samples with scarcity of data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:18:11 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zertuche", "Federico", ""], ["Meza-Pe\u00f1aloza", "Abigail", ""]]}, {"id": "1908.07632", "submitter": "Kelly Moran", "authors": "Kelly R. Moran, Elizabeth L. Turner, David Dunson, and Amy H. Herring", "title": "Bayesian Hierarchical Factor Regression Models to Infer Cause of Death\n  From Verbal Autopsy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low-resource settings where vital registration of death is not routine it\nis often of critical interest to determine and study the cause of death (COD)\nfor individuals and the cause-specific mortality fraction (CSMF) for\npopulations. Post-mortem autopsies, considered the gold standard for COD\nassignment, are often difficult or impossible to implement due to deaths\noccurring outside the hospital, expense, and/or cultural norms. For this\nreason, Verbal Autopsies (VAs) are commonly conducted, consisting of a\nquestionnaire administered to next of kin recording demographic information,\nknown medical conditions, symptoms, and other factors for the decedent. This\narticle proposes a novel class of hierarchical factor regression models that\navoid restrictive assumptions of standard methods, allow both the mean and\ncovariance to vary with COD category, and can include covariate information on\nthe decedent, region, or events surrounding death. Taking a Bayesian approach\nto inference, this work develops an MCMC algorithm and validates the FActor\nRegression for Verbal Autopsy (FARVA) model in simulation experiments. An\napplication of FARVA to real VA data shows improved goodness-of-fit and better\npredictive performance in inferring COD and CSMF over competing methods. Code\nand a user manual are made available at https://github.com/kelrenmor/farva.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:16:16 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 23:16:55 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Moran", "Kelly R.", ""], ["Turner", "Elizabeth L.", ""], ["Dunson", "David", ""], ["Herring", "Amy H.", ""]]}, {"id": "1908.07639", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky, Matthew R. Williams", "title": "Risk-Efficient Bayesian Data Synthesis for Privacy Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies utilize models to synthesize respondent-level data for\nrelease to the public for privacy protection. In this work, we efficiently\ninduce privacy protection into any Bayesian synthesis model by employing a\npseudo likelihood that exponentiates each likelihood contribution by an\nobservation record-indexed weight in [0, 1], defined to be inversely\nproportional to the identification risk for that record. We start with the\nmarginal probability of identification risk for a record, which is composed as\nthe probability that the identity of the record may be disclosed. Our\napplication to the Consumer Expenditure Surveys (CE) of the U.S. Bureau of\nLabor Statistics demonstrates that the marginally risk-adjusted synthesizer\nprovides an overall improved privacy protection; however, the identification\nrisks actually increase for some moderate-risk records after risk-adjusted\npseudo posterior estimation synthesis due to increased isolation after\nweighting; a phenomenon we label \"whack-a-mole\". We proceed to construct a\nweight for each record from a collection of pairwise identification risk\nprobabilities with other records, where each pairwise probability measures the\njoint probability of re-identification of the pair of records, which mitigates\nthe whack-a-mole issue and produces a more efficient set of synthetic data with\nlower risk and higher utility for the CE data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:42:40 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 21:31:33 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 16:02:40 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 18:01:55 GMT"}, {"version": "v5", "created": "Mon, 2 Nov 2020 20:33:23 GMT"}, {"version": "v6", "created": "Mon, 8 Feb 2021 23:03:48 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "1908.07751", "submitter": "Satrajit Roychoudhury", "authors": "Satrajit Roychoudhury, Nicolas Scheuer, and Beat Neuenschwander", "title": "Beyond p-values: a phase II dual-criterion design with statistical\n  significance and clinical relevance", "comments": null, "journal-ref": "Published in Clinical Trials 2018", "doi": "10.1177/1740774518770661", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Well-designed phase II trials must have acceptable error rates\nrelative to a pre-specified success criterion, usually a statistically\nsignificant p-value. Such standard designs may not always suffice from a\nclinical perspective because clinical relevance may call for more. For example,\nproof-of-concept in phase II often requires not only statistical significance\nbut also a sufficiently large effect estimate.\n  Purpose: We propose dual-criterion designs to complement statistical\nsignificance with clinical relevance, discuss their methodology, and illustrate\ntheir implementation in phase II.\n  Methods: Clinical relevance requires the effect estimate to pass a clinically\nmotivated threshold (the decision value). In contrast to standard designs, the\nrequired effect estimate is an explicit design input whereas study power is\nimplicit. The sample size for a dual-criterion design needs careful\nconsiderations of the study's operating characteristics (type-I error, power).\n  Results: Dual-criterion designs are discussed for a randomized controlled and\na single-arm phase II trial, including decision criteria, sample size\ncalculations, decisions under various data scenarios, and operating\ncharacteristics. The designs facilitate GO/NO-GO decisions due to their\ncomplementary statistical-clinical criterion.\n  Conclusion: To improve evidence-based decision-making, a formal yet\ntransparent quantitative framework is important. Dual-criterion designs offer\nan appealing statistical-clinical compromise, which may be preferable to\nstandard designs if evidence against the null hypothesis alone does not suffice\nfor an efficacy claim.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:44:52 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Roychoudhury", "Satrajit", ""], ["Scheuer", "Nicolas", ""], ["Neuenschwander", "Beat", ""]]}, {"id": "1908.07805", "submitter": "Hanna Meyer", "authors": "Hanna Meyer, Christoph Reudenbach, Stephan W\\\"ollauer, Thomas Nauss", "title": "Importance of spatial predictor variable selection in machine learning\n  applications -- Moving from data reproduction to spatial prediction", "comments": "under review in Ecological Modelling", "journal-ref": "Ecological Modelling, 411, 2019, 108815", "doi": "10.1016/j.ecolmodel.2019.108815", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning algorithms find frequent application in spatial prediction\nof biotic and abiotic environmental variables. However, the characteristics of\nspatial data, especially spatial autocorrelation, are widely ignored. We\nhypothesize that this is problematic and results in models that can reproduce\ntraining data but are unable to make spatial predictions beyond the locations\nof the training samples. We assume that not only spatial validation strategies\nbut also spatial variable selection is essential for reliable spatial\npredictions. We introduce two case studies that use remote sensing to predict\nland cover and the leaf area index for the \"Marburg Open Forest\", an open\nresearch and education site of Marburg University, Germany. We use the machine\nlearning algorithm Random Forests to train models using non-spatial and spatial\ncross-validation strategies to understand how spatial variable selection\naffects the predictions. Our findings confirm that spatial cross-validation is\nessential in preventing overoptimistic model performance. We further show that\nhighly autocorrelated predictors (such as geolocation variables, e.g. latitude,\nlongitude) can lead to considerable overfitting and result in models that can\nreproduce the training data but fail in making spatial predictions. The problem\nbecomes apparent in the visual assessment of the spatial predictions that show\nclear artefacts that can be traced back to a misinterpretation of the spatially\nautocorrelated predictors by the algorithm. Spatial variable selection could\nautomatically detect and remove such variables that lead to overfitting,\nresulting in reliable spatial prediction patterns and improved statistical\nspatial model performance. We conclude that in addition to spatial validation,\na spatial variable selection must be considered in spatial predictions of\necological data to produce reliable predictions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:47:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Meyer", "Hanna", ""], ["Reudenbach", "Christoph", ""], ["W\u00f6llauer", "Stephan", ""], ["Nauss", "Thomas", ""]]}, {"id": "1908.07963", "submitter": "Keefe Murphy", "authors": "Keefe Murphy, Thomas Brendan Murphy, Raffaella Piccarreta, Isobel\n  Claire Gormley", "title": "Clustering Longitudinal Life-Course Sequences Using Mixtures of\n  Exponential-Distance Models", "comments": "Published in Journal of the Royal Statistical Society: Series A\n  (Statistics in Society)", "journal-ref": null, "doi": "10.1111/rssa.12712", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence analysis is an increasingly popular approach for analysing life\ncourses represented by ordered collections of activities experienced by\nsubjects over time. Here, we analyse a survey data set containing information\non the career trajectories of a cohort of Northern Irish youths tracked between\nthe ages of 16 and 22. We propose a novel, model-based clustering approach\nsuited to the analysis of such data from a holistic perspective, with the aims\nof estimating the number of typical career trajectories, identifying the\nrelevant features of these patterns, and assessing the extent to which such\npatterns are shaped by background characteristics.\n  Several criteria exist for measuring pairwise dissimilarities among\ncategorical sequences. Typically, dissimilarity matrices are employed as input\nto heuristic clustering algorithms. The family of methods we develop instead\nclusters sequences directly using mixtures of exponential-distance models.\nBasing the models on weighted variants of the Hamming distance metric permits\nclosed-form expressions for parameter estimation. Simultaneously allowing the\ncomponent membership probabilities to depend on fixed covariates and\naccommodating sampling weights in the clustering process yields new insights on\nthe Northern Irish data. In particular, we find that school examination\nperformance is the single most important predictor of cluster membership.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:15:43 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 14:34:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 17:19:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Murphy", "Keefe", ""], ["Murphy", "Thomas Brendan", ""], ["Piccarreta", "Raffaella", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1908.07979", "submitter": "Baolin Wu", "authors": "Yun Bai, Zengri Wang, Theodore Lystig, Baolin Wu", "title": "Efficient and powerful equivalency test on combined mean and variance\n  with application to diagnostic device comparison studies", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical device comparison studies, equivalency test is commonly used to\ndemonstrate two measurement methods agree up to a pre-specified performance\ngoal based on the paired repeated measures. Such equivalency test often\ninvolves controlling the absolute differences that depend on both the mean and\nvariance parameters, and poses some challenges for statistical analysis. For\nexample, for the oximetry comparison study that motivates our research, FDA has\nclear guidelines approving an investigational pulse oximeter in comparison to a\nstandard oximeter via testing the root mean squares (RMS), a composite measure\nof both mean and variance parameters. For the hypothesis testing of this\ncomposite measure, existing methods have been either exploratory or relying on\nthe large-sample normal approximation with conservative and unsatisfactory\nperformance. We develop a novel generalized pivotal test to rigorously and\naccurately test the system equivalency based on RMS. The proposed method has\nwell-controlled type I error and favorable performance in our extensive\nnumerical studies. When analyzing data from an oximetry comparison study,\naiming to demonstrate performance equivalency between an FDA-cleared oximetry\nsystem and an investigational system, our proposed method resulted in a highly\nsignificant test result strongly supporting the system equivalency. We also\nprovide efficient R programs for the proposed method in a publicly available R\npackage. Considering that many practical equivalency studies of diagnostic\ndevices are of small to medium sizes, our proposed method and software timely\nbridge an existing gap in the field.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:38:52 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Bai", "Yun", ""], ["Wang", "Zengri", ""], ["Lystig", "Theodore", ""], ["Wu", "Baolin", ""]]}, {"id": "1908.08093", "submitter": "Yongli Han", "authors": "Yongli Han, Paul S. Albert, Christine D. Berg, Nicolas Wentzensen,\n  Hormuzd A. Katki, Danping Liu", "title": "Statistical approaches using longitudinal biomarkers for disease early\n  detection: A comparison of methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of clinical outcomes such as cancer may be predicted based on\nlongitudinal biomarker measurements. Tracking longitudinal biomarkers as a way\nto identify early disease onset may help to reduce mortality from diseases like\novarian cancer that are more treatable if detected early. Two general\nframeworks for disease risk prediction, the shared random effects model (SREM)\nand the pattern mixture model (PMM) could be used to assess longitudinal\nbiomarkers on disease early detection. In this paper, we studied the predictive\nperformances of SREM and PMM on disease early detection through an application\nto ovarian cancer, where early detection using the risk of ovarian cancer\nalgorithm (ROCA) has been evaluated. Comparisons of the above three methods\nwere performed via the analyses of the ovarian cancer data from the Prostate,\nLung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial and extensive\nsimulation studies. The time-dependent receiving operating characteristic (ROC)\ncurve and its area (AUC) were used to evaluate the prediction accuracy. The\nout-of-sample predictive performance was calculated using leave-one-out\ncross-validation (LOOCV), aiming to minimize the problem of model over-fitting.\nA careful analysis of the use of the biomarker cancer antigen 125 for ovarian\ncancer early detection showed improved performance of PMM as compared with SREM\nand ROCA. More generally, simulation studies showed that PMM outperforms ROCA\nunless biomarkers are taken at very frequent screening settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 19:30:00 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Han", "Yongli", ""], ["Albert", "Paul S.", ""], ["Berg", "Christine D.", ""], ["Wentzensen", "Nicolas", ""], ["Katki", "Hormuzd A.", ""], ["Liu", "Danping", ""]]}, {"id": "1908.08116", "submitter": "Tirthankar Dasgupta", "authors": "Brian Libgober and Tirthankar Dasgupta", "title": "An Email Experiment to Identify the Effect of Racial Discrimination on\n  Access to Lawyers: A Statistical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conducting an experiment to study the prevalence\nof racial bias against individuals seeking legal assistance, in particular\nwhether lawyers use clues about a potential client's race in deciding whether\nto reply to e-mail requests for representations. The problem of discriminating\nbetween potential linear and non-linear effects of a racial signal is\nformulated as a statistical inference problem, whose objective is to infer a\nparameter determining the shape of a specific function. Various complexities\nassociated with the design and analysis of this experiment are handled by\napplying a novel combination of rigorous, semi-rigorous and rudimentary\nstatistical techniques. The actual experiment was attempted with a population\nof lawyers in Florida, but could not be performed with the desired sample size\ndue to resource limitations. Nonetheless, it provides a nice demonstration of\nthe proposed steps involved in conducting such a study.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 21:07:03 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:39:27 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Libgober", "Brian", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1908.08127", "submitter": "Mina Lee", "authors": "Mina Lee, Joseph Y. J. Chow, Gyugeun Yoon, Brian Yueshuai He", "title": "Forecasting e-scooter substitution of direct and access trips by mode\n  and distance", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.trd.2021.102892", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An e-scooter trip model is estimated from four U.S. cities: Portland, Austin,\nChicago and New York City. A log-log regression model is estimated for\ne-scooter trips based on user age, population, land area, and the number of\nscooters. The model predicts 75K daily e-scooter trips in Manhattan for a\ndeployment of 2000 scooters, which translates to 77 million USD in annual\nrevenue. We propose a novel nonlinear, multifactor model to break down the\nnumber of daily trips by the alternative modes of transportation that they\nwould likely substitute based on statistical similarity. The model parameters\nreveal a relationship with direct trips of bike, walk, carpool, automobile and\ntaxi as well as access/egress trips with public transit in Manhattan. Our model\nestimates that e-scooters could replace 32% of carpool; 13% of bike; and 7.2%\nof taxi trips. The distance structure of revenue from access/egress trips is\nfound to differ from that of other substituted trips.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 21:58:25 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 23:20:06 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 03:24:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lee", "Mina", ""], ["Chow", "Joseph Y. J.", ""], ["Yoon", "Gyugeun", ""], ["He", "Brian Yueshuai", ""]]}, {"id": "1908.08144", "submitter": "Philip Stark", "authors": "Philip B. Stark and Ran Xie", "title": "Testing Cannot Tell Whether Ballot-Marking Devices Alter Election\n  Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like all computerized systems, ballot-marking devices (BMDs) can be hacked,\nmisprogrammed, and misconfigured. Several approaches to testing BMDs have been\nproposed. In _logic and accuracy_ (_L&A_) tests, trusted agents input known\ntest patterns into the BMD and check whether the printout matches. In\n_parallel_ or _live_ testing, agents use the BMDs on election day, emulating\nvoters. In _passive_ testing, agents monitor the rate at which voters \"spoil\"\nballots and request another opportunity to mark a ballot: an anomalously high\nrate might result from BMD malfunctions. In practice, none of these methods can\nprotect against outcome-altering problems. L&A testing is ineffective in part\nbecause BMDs \"know\" the time and date of the test and the election. Neither L&A\nnor parallel testing can probe even a small fraction of the possible voting\ntransactions that could comprise enough votes to change outcomes. Under mild\nassumptions, to develop a model of voter interactions with BMDs accurate enough\nto ensure that parallel tests could reliably detect changes to 5% of the votes\n(which could change margins by 10% or more) would require monitoring the\nbehavior of more than a million voters in each jurisdiction in minute\ndetail---but the median turnout by jurisdiction in the U.S. is under 3000\nvoters. Given an accurate model of voter behavior, the number of tests required\nis still larger than the turnout in a typical U.S. jurisdiction. Under\noptimistic assumptions, passive testing that has a 99% chance of detecting a 1%\nchange to the margin with a 1% false alarm rate is impossible in jurisdictions\nwith fewer than about 1 million voters, even if the \"normal\" spoiled ballot\nrate were known exactly and did not vary from election to election and place to\nplace.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 23:39:04 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 22:46:29 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Stark", "Philip B.", ""], ["Xie", "Ran", ""]]}, {"id": "1908.08320", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid", "title": "Spatial and Spatiotemporal GARCH Models -- A Unified Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-series analyses, particularly for finance, generalized autoregressive\nconditional heteroscedasticity (GARCH) models are widely applied statistical\ntools for modelling volatility clusters (i.e., periods of increased or\ndecreased risk). In contrast, it has not been considered to be of critical\nimportance until now to model spatial dependence in the conditional second\nmoments. Only a few models have been proposed for modelling local clusters of\nincreased risks. In this paper, we introduce novel spatial GARCH and\nexponential GARCH processes in a unified spatial and spatiotemporal GARCH-type\nmodel, which also covers all previously proposed spatial ARCH models as well as\ntime-series GARCH models. For this common modelling framework, estimators are\nderived based on nonlinear least squares and on the maximum-likelihood\napproach. In addition to the theoretical contributions of this paper, we\nsuggest a model selection strategy that is verified by a series of Monte Carlo\nsimulation studies. Eventually, the use of the unified model is demonstrated by\nan empirical example that focuses on real estate prices from 1995 to 2014\nacross the ZIP-Code areas of Berlin. A spatial autoregressive model is applied\nto the data to illustrate how locally varying model uncertainties can be\ncaptured by the spatial GARCH-type models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:29:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:33:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1908.08570", "submitter": "Ayush Maheshwari", "authors": "Ayush Maheshwari, Kamal Kumar Murari, T. Jayaraman", "title": "Peak Electricity Demand and Global Warming in the Industrial and\n  Residential areas of Pune : An Extreme Value Approach", "comments": "20 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial and residential activities respond distinctly to electricity\ndemand on temperature. Due to increasing temperature trend on account of global\nwarming, its impact on peak electricity demand is a proxy for effective\nmanagement of electricity infrastructure. Few studies explore the relationship\nbetween electricity demand and temperature changes in industrial areas in India\nmainly due to the limitation of data. The precise role of industrial and\nresidential activities response to the temperature is not explored in\nsub-tropical humid climate of India. Here, we show the temperature sensitivity\nof industrial and residential areas in the city of Pune, Maharashtra by keeping\nother influencing variables on electricity demand as constant. The study seeks\nto estimate the behaviour of peak electricity demand with the apparent\ntemperature (AT) using the Extreme Value Theory. Our analysis shows that\nindustrial activities are not much influenced by the temperature whereas\nresidential activities show around 1.5-2% change in average electricity demand\nwith 1 degree rise in AT. Further, we show that peak electricity demand in\nresidential areas, performed using stationary and non-stationary GEV models,\nare significantly influenced by the rise in temperature. The study shows that\nwith the improvement in data collection, better planning for the future\ndevelopment, accounting for the climate change effects, will enhance the\neffectiveness of electricity distribution system. The study is limited to the\ngeographical area of Pune. However, the methods are useful in estimating the\npeak power load attributed to climate change to other geographical regions\nlocated in subtropical and humid climate.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 19:17:34 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Maheshwari", "Ayush", ""], ["Murari", "Kamal Kumar", ""], ["Jayaraman", "T.", ""]]}, {"id": "1908.08670", "submitter": "Ningning Xia", "authors": "Moming Wang, Ningning Xia, You Zhou", "title": "On the estimation of high-dimensional integrated covariance matrix based\n  on high-frequency data with multiple transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the mechanism of recording, the presence of multiple transactions at\neach recording time becomes a common feature for high-frequency data in\nfinancial market. Using random matrix theory, this paper considers the\nestimation of integrated covariance (ICV) matrices of high-dimensional\ndiffusion processes based on multiple high-frequency observations. We start by\nstudying the estimator, the time-variation adjusted realized covariance (TVA)\nmatrix, proposed in Zheng and Li (2011) without microstructure noise. We show\nthat in the high-dimensional case, for a class C of diffusion processes, the\nlimiting spectral distribution (LSD) of averaged TVA depends not only on that\nof ICV, but also on the numbers of multiple transactions at each recording\ntime. However, in practice, the observed prices are always contaminated by the\nmarket microstructure noise. Thus the limiting behavior of pre-averaging\naveraged TVA matrices is studied based on the noisy multiple observations. We\nshow that for processes in class C, the pre-averaging averaged TVA has\ndesirable properties that it eliminates the effects of microstructure noise and\nmultiple transactions, and its LSD depends solely on that of the ICV matrix.\nFurther, three types of nonlinear shrinkage estimators of ICV are proposed\nbased on high-frequency noisy multiple observations. Simulation studies support\nour theoretical results and show the finite sample performance of the proposed\nestimators. At last, the high-frequency portfolio strategies are evaluated\nunder these estimators in real data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 05:30:05 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 09:20:47 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Moming", ""], ["Xia", "Ningning", ""], ["Zhou", "You", ""]]}, {"id": "1908.08819", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Yuxuan Xia, Karl Granstr\\\"om, Lennart\n  Svensson, Jason L. Williams", "title": "Gaussian implementation of the multi-Bernoulli mixture filter", "comments": "Matlab code of the MBM and PMBM filters is provided in\n  https://github.com/Agarciafernandez/MTT . Additional information on MTT\n  including PMBM and MBM filters can be found in the online course\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "Proceedings of the 22nd International Conference on Information\n  Fusion, 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Gaussian implementation of the multi-Bernoulli\nmixture (MBM) filter. The MBM filter provides the filtering (multi-target)\ndensity for the standard dynamic and radar measurement models when the birth\nmodel is multi-Bernoulli or multi-Bernoulli mixture. Under linear/Gaussian\nmodels, the single target densities of the MBM mixture admit Gaussian\nclosed-form expressions. Murty's algorithm is used to select the global\nhypotheses with highest weights. The MBM filter is compared with other\nalgorithms in the literature via numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 13:31:49 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Xia", "Yuxuan", ""], ["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""], ["Williams", "Jason L.", ""]]}, {"id": "1908.08970", "submitter": "Bruce Cox", "authors": "Zachary T. Hornberger, Bruce A. Cox, and Brian J. Lunday", "title": "Optimal Heterogeneous Asset Location Modeling for Expected\n  Spatiotemporal Search and Rescue Demands using Historic Event Data", "comments": "Preprint version, submitted to IISE Transactions on 17 April 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States Coast Guard is charged with the coordination of all search\nand rescue missions in maritime regions within the United States purview. Given\nthe size of the Pacific Ocean and the limited resources available to respond to\nsearch and rescue missions in this region, the service seeks to posture its\naligned fleet of maritime and aeronautical assets to reduce the expected\nresponse time for such missions. Leveraging historic event records for the\nregion of interest, we propose and demonstrate a two-stage solution approach.\nIn the first stage, we develop and apply a stochastic zonal distribution model\nto evaluate spatiotemporal trends for emergency event rates and corresponding\nresponse strategies to inform the probabilistic modeling of future rescue\nevents respective locations, frequencies, and demands for support. In the\nsecond stage, the results from the aforementioned analysis enable the\nparameterization and solution of a integer linear programming formulation to\nidentify the best locations at which to station limited heterogeneous search\nand rescue assets. Considering both the 50th and 75th percentile levels of\nforecast event and asset demand distributions using 7.5 years of historical\nevent data, our models identify asset location strategies that respectively\nyield a 9.6 percent and 17.6 percent increase in coverage over current asset\nbasing when allowing locations among current homeports and airports, as well as\nrespective 67.3 percent and 57.4 percent increases in coverage when considering\na larger set of feasible basing locations.\n  Keywords: search and rescue, spatiotemporal forecasting, location-allocation\nmodeling, p-median location problem, multi-objective optimization\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 18:31:31 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hornberger", "Zachary T.", ""], ["Cox", "Bruce A.", ""], ["Lunday", "Brian J.", ""]]}, {"id": "1908.08980", "submitter": "Edward Wheatcroft", "authors": "Edward Wheatcroft", "title": "Evaluating probabilistic forecasts of football matches: The case against\n  the Ranked Probability Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scoring rule is a function of a probabilistic forecast and a corresponding\noutcome that is used to evaluate forecast performance. A wide range of scoring\nrules have been defined over time and there is some debate as to which are the\nmost appropriate for evaluating the performance of forecasts of sporting\nevents. This paper focuses on forecasts of the outcomes of football matches.\nThe ranked probability score (RPS) is often recommended since it is `sensitive\nto distance', that is it takes into account the ordering in the outcomes (a\nhome win is `closer' to a draw than it is to an away win, for example). In this\npaper, this reasoning is disputed on the basis that it adds nothing in terms of\nthe actual aims of using scoring rules. A related property of scoring rules is\nlocality. A scoring rule is local if it only takes the probability placed on\nthe outcome into consideration. Two simulation experiments are carried out in\nthe context of football matches to compare the performance of the RPS, which is\nnon-local and sensitive to distance, the Brier score, which is non-local and\ninsensitive to distance, and the ignorance score, which is local and\ninsensitive to distance. The ignorance score is found to outperform both the\nRPS and the Brier score, casting doubt on the value of non-locality and\nsensitivity to distance as properties of scoring rules in this context.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 19:04:03 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wheatcroft", "Edward", ""]]}, {"id": "1908.09029", "submitter": "Bryan Graham", "authors": "Bryan S. Graham", "title": "Dyadic Regression", "comments": "20 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic data, where outcomes reflecting pairwise interaction among sampled\nunits are of primary interest, arise frequently in social science research.\nRegression analyses with such data feature prominently in many research\nliteratures (e.g., gravity models of trade). The dependence structure\nassociated with dyadic data raises special estimation and, especially,\ninference issues. This chapter reviews currently available methods for\n(parametric) dyadic regression analysis and presents guidelines for empirical\nresearchers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 20:31:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Graham", "Bryan S.", ""]]}, {"id": "1908.09038", "submitter": "Tom Velez PhD", "authors": "Tom Velez, Tony Wang, Ioannis Koutroulis, James Chamberlain, Amit\n  Uppal, Seife Yohannes, Tim Tschampel, Emilia Apostolova", "title": "Identification of Pediatric Sepsis Subphenotypes for Enhanced Machine\n  Learning Predictive Performance: A Latent Profile Analysis", "comments": "Keywords: Pediatric Sepsis, Mortality, Latent Profile Analysis,\n  Machine Learning, Subphenotypes 15 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: While machine learning (ML) models are rapidly emerging as\npromising screening tools in critical care medicine, the identification of\nhomogeneous subphenotypes within populations with heterogeneous conditions such\nas pediatric sepsis may facilitate attainment of high-predictive performance of\nthese prognostic algorithms. This study is aimed to identify subphenotypes of\npediatric sepsis and demonstrate the potential value of partitioned\ndata/subtyping-based training. Methods: This was a retrospective study of\nclinical data extracted from medical records of 6,446 pediatric patients that\nwere admitted at a major hospital system in the DC area. Vitals and labs\nassociated with patients meeting the diagnostic criteria for sepsis were used\nto perform latent profile analysis. Modern ML algorithms were used to explore\nthe predictive performance benefits of reduced training data heterogeneity via\nlabel profiling. Results: In total 134 (2.1%) patients met the diagnostic\ncriteria for sepsis in this cohort and latent profile analysis identified four\nprofiles/subphenotypes of pediatric sepsis. Profiles 1 and 3 had the lowest\nmortality and included pediatric patients from different age groups. Profile 2\nwere characterized by respiratory dysfunction; profile 4 by neurological\ndysfunction and highest mortality rate (22.2%). Machine learning experiments\ncomparing the predictive performance of models derived without training data\nprofiling against profile targeted models suggest statistically significant\nimproved performance of prediction can be obtained. For example, area under ROC\ncurve (AUC) obtained to predict profile 4 with 24-hour data (AUC = .998, p <\n.0001) compared favorably with the AUC obtained from the model considering all\nprofiles as a single homogeneous group (AUC = .918) with 24-hour data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 20:59:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Velez", "Tom", ""], ["Wang", "Tony", ""], ["Koutroulis", "Ioannis", ""], ["Chamberlain", "James", ""], ["Uppal", "Amit", ""], ["Yohannes", "Seife", ""], ["Tschampel", "Tim", ""], ["Apostolova", "Emilia", ""]]}, {"id": "1908.09059", "submitter": "Yiqun Chen", "authors": "Yiqun Chen, Wenjing Zheng, Lillian B. Brown, Gabriel Chamie, Dalsone\n  Kwarisiima, Jane Kabami, Tamara D. Clark, Norton Sang, James Ayieko, Edwin D.\n  Charlebois, Vivek Jain, Laura Balzer, Moses R Kamya, Diane Havlir, Maya\n  Petersen, and the SEARCH Collaboration", "title": "Semi-Supervised Record Linkage for Construction of Large-Scale\n  Sociocentric Networks in Resource-limited Settings: An application to the\n  SEARCH Study in Rural Uganda and Kenya", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel semi-supervised algorithmic approach to creating\nlarge scale sociocentric networks in rural East Africa. We describe the\nconstruction of 32 large-scale sociocentric social networks in rural\nSub-Saharan Africa. Networks were constructed by applying a semi-supervised\nrecord-linkage algorithm to data from census-enumerated residents of the 32\ncommunities included in the SEARCH study (NCT01864603), a community-cluster\nrandomized HIV prevention trial in Uganda and Kenya. Contacts were solicited\nusing a five question name generator in the domains of emotional support, food\nsharing, free time, health issues and money issues. The fully constructed\nnetworks include 170; 028 nodes and 362; 965 edges aggregated across\ncommunities (ranging from 4449 to 6829 nodes and from 2349 to 31,779 edges per\ncommunity). Our algorithm matched on average 30% of named contacts in Kenyan\ncommunities and 50% of named contacts in Ugandan communities to residents named\nin census enumeration. Assortative mixing measures for eight different\ncovariates reveal that residents in the network have a very strong tendency to\nassociate with others who are similar to them in age, sex, and especially\nvillage. The networks in the SEARCH Study will provide a platform for improved\nunderstanding of health outcomes in rural East Africa. The network construction\nalgorithm we present may facilitate future social network research in\nresource-limited settings.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 00:39:08 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chen", "Yiqun", ""], ["Zheng", "Wenjing", ""], ["Brown", "Lillian B.", ""], ["Chamie", "Gabriel", ""], ["Kwarisiima", "Dalsone", ""], ["Kabami", "Jane", ""], ["Clark", "Tamara D.", ""], ["Sang", "Norton", ""], ["Ayieko", "James", ""], ["Charlebois", "Edwin D.", ""], ["Jain", "Vivek", ""], ["Balzer", "Laura", ""], ["Kamya", "Moses R", ""], ["Havlir", "Diane", ""], ["Petersen", "Maya", ""], ["Collaboration", "the SEARCH", ""]]}, {"id": "1908.09071", "submitter": "Yishu Xue", "authors": "Yishu Xue, Elizabeth D. Schifano, Guanyu Hu", "title": "Geographically Weighted Cox Regression for Prostate Cancer Survival Data\n  in Louisiana", "comments": null, "journal-ref": null, "doi": "10.1111/gean.12223", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Cox proportional hazard model is one of the most popular tools in\nanalyzing time-to-event data in public health studies. When outcomes observed\nin clinical data from different regions yield a varying pattern correlated with\nlocation, it is often of great interest to investigate spatially varying\neffects of covariates. In this paper, we propose a geographically weighted Cox\nregression model for sparse spatial survival data. In addition, a stochastic\nneighborhood weighting scheme is introduced at the county level. Theoretical\nproperties of the proposed geographically weighted estimators are examined in\ndetail. A model selection scheme based on the Takeuchi's model robust\ninformation criteria (TIC) is discussed. Extensive simulation studies are\ncarried out to examine the empirical performance of the proposed methods. We\nfurther apply the proposed methodology to analyze real data on prostate cancer\nfrom the Surveillance, Epidemiology, and End Results cancer registry for the\nstate of Louisiana.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 02:04:57 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Xue", "Yishu", ""], ["Schifano", "Elizabeth D.", ""], ["Hu", "Guanyu", ""]]}, {"id": "1908.09112", "submitter": "Daniel Andrade", "authors": "Daniel Andrade and Kenji Fukumizu", "title": "Disjunct Support Spike and Slab Priors for Variable Selection in\n  Regression under Quasi-sparseness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparseness of the regression coefficient vector is often a desirable\nproperty, since, among other benefits, sparseness improves interpretability. In\npractice, many true regression coefficients might be negligibly small, but\nnon-zero, which we refer to as quasi-sparseness. Spike-and-slab priors as\nintroduced in (Chipman et al., 2001) can be tuned to ignore very small\nregression coefficients, and, as a consequence provide a trade-off between\nprediction accuracy and interpretability. However, spike-and-slab priors with\nfull support lead to inconsistent Bayes factors, in the sense that the Bayes\nfactors of any two models are bounded in probability. This is clearly an\nundesirable property for Bayesian hypotheses testing, where we wish that\nincreasing sample sizes lead to increasing Bayes factors favoring the true\nmodel. The moment matching priors as in (Johnson and Rossell, 2012) can resolve\nthis issue, but are unsuitable for the quasi-sparse setting due to their full\nsupport outside the exact value 0. As a remedy, we suggest disjunct support\nspike and slab priors, for which we prove consistent Bayes factors in the\nquasi-sparse setting, and show experimentally fast growing Bayes factors\nfavoring the true model. Several experiments on simulated and real data confirm\nthe usefulness of our proposed method to identify models with high effect size,\nwhile leading to better control over false positives than hard-thresholding.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 09:23:59 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 09:42:59 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Andrade", "Daniel", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1908.09195", "submitter": "Samuel I. Berchuck", "authors": "Samuel I. Berchuck, Felipe A. Medeiros and Sayan Mukherjee", "title": "Scalable Modeling of Spatiotemporal Data using the Variational\n  Autoencoder: an Application in Glaucoma", "comments": "This is a preprint of an article submitted for publication in the\n  Annals of Applied Statistics. The article contains 26 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As big spatial data becomes increasingly prevalent, classical spatiotemporal\n(ST) methods often do not scale well. While methods have been developed to\naccount for high-dimensional spatial objects, the setting where there are\nexceedingly large samples of spatial observations has had less attention. The\nvariational autoencoder (VAE), an unsupervised generative model based on deep\nlearning and approximate Bayesian inference, fills this void using a latent\nvariable specification that is inferred jointly across the large number of\nsamples. In this manuscript, we compare the performance of the VAE with a more\nclassical ST method when analyzing longitudinal visual fields from a large\ncohort of patients in a prospective glaucoma study. Through simulation and a\ncase study, we demonstrate that the VAE is a scalable method for analyzing ST\ndata, when the goal is to obtain accurate predictions. R code to implement the\nVAE can be found on GitHub: https://github.com/berchuck/vaeST.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 20:02:41 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Berchuck", "Samuel I.", ""], ["Medeiros", "Felipe A.", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1908.09230", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and Sarah E. Robertson and Lucia C. Petito and Miguel\n  A. Hern\\'an and Jon A. Steingrimsson", "title": "Efficient and robust methods for causally interpretable meta-analysis:\n  transporting inferences from multiple randomized trials to a target\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for causally interpretable meta-analyses that combine\ninformation from multiple randomized trials to estimate potential\n(counterfactual) outcome means and average treatment effects in a target\npopulation. We consider identifiability conditions, derive implications of the\nconditions for the law of the observed data, and obtain identification results\nfor transporting causal inferences from a collection of independent randomized\ntrials to a new target population in which experimental data may not be\navailable. We propose an estimator for the potential (counterfactual) outcome\nmean in the target population under each treatment studied in the trials. The\nestimator uses covariate, treatment, and outcome data from the collection of\ntrials, but only covariate data from the target population sample. We show that\nit is doubly robust, in the sense that it is consistent and asymptotically\nnormal when at least one of the models it relies on is correctly specified. We\nstudy the finite sample properties of the estimator in simulation studies and\ndemonstrate its implementation using data from a multi-center randomized trial.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 23:25:44 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:39:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 16:54:18 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 00:53:32 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robertson", "Sarah E.", ""], ["Petito", "Lucia C.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Steingrimsson", "Jon A.", ""]]}, {"id": "1908.09377", "submitter": "Hannah Director", "authors": "Hannah M. Director, Adrian E. Raftery, Cecilia M. Bitz", "title": "Probabilistic Forecasting of the Arctic Sea Ice Edge with Contour\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sea ice, or frozen ocean water, freezes and melts every year in the Arctic.\nForecasts of where sea ice will be located weeks to months in advance have\nbecome more important as the amount of sea ice declines due to climate change,\nfor maritime planning and other uses. Typical sea ice forecasts are made with\nensemble models, physics-based models of sea ice and the surrounding ocean and\natmosphere. This paper introduces Mixture Contour Forecasting, a method to\nforecast sea ice probabilistically using a mixture of two distributions, one\nbased on post-processed output from ensembles and the other on observed sea ice\npatterns in recent years. At short lead times, these forecasts are better\ncalibrated than unadjusted dynamic ensemble forecasts and other statistical\nreference forecasts. To produce these forecasts, a statistical technique is\nintroduced that directly models the sea ice edge contour, the boundary around\nthe region that is ice-covered. Mixture Contour Forecasting and reference\nmethods are evaluated for monthly sea ice forecasts for 2008-2016 at lead times\nranging from 0.5-6.5 months using one of the European Centre for Medium-Range\nWeather Forecasts ensembles.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 19:20:47 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:59:28 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 22:58:45 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Director", "Hannah M.", ""], ["Raftery", "Adrian E.", ""], ["Bitz", "Cecilia M.", ""]]}, {"id": "1908.09431", "submitter": "Weijian Liu", "authors": "Weijian Liu, Jun Liu, Yongchan Gao, Guoshi Wang, Yong-Liang Wang", "title": "Multichannel signal detection in interference and noise when signal\n  mismatch happens", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2019.107268", "report-no": null, "categories": "eess.SP stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of detecting a multichannel signal in\ninterference and noise when signal mismatch happens. We first propose two\nselective detectors, since their strong selectivity is preferred in some\nsituations. However, these two detectors would not be suitable candidates if a\nrobust detector is needed. To overcome this shortcoming, we then devise a\ntunable detector, which is parametrized by a non-negative scaling factor,\nreferred to as the tunable parameter. By adjusting the tunable parameter, the\nproposed detector can smoothly change its capability in rejecting or robustly\ndetecting a mismatch signal. Moreover, one selective detector and the tunable\ndetector with an appropriate tunable parameter can provide nearly the same\ndetection performance as existing detectors in the absence of signal mismatch.\nWe obtain analytical expressions for the probabilities of detection (PDs) and\nprobabilities of false alarm (PFAs) of the three proposed detectors, which are\nverified by Monte Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:46:19 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liu", "Weijian", ""], ["Liu", "Jun", ""], ["Gao", "Yongchan", ""], ["Wang", "Guoshi", ""], ["Wang", "Yong-Liang", ""]]}, {"id": "1908.09440", "submitter": "Mason A. Porter", "authors": "Jane Carlen, Jaume de Dios Pont, Cassidy Mentus, Shyr-Shea Chang,\n  Stephanie Wang, Mason A. Porter", "title": "Role Detection in Bicycle-Sharing Networks Using Multilayer Stochastic\n  Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST nlin.AO physics.soc-ph stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban spatial networks are complex systems with interdependent roles of\nneighborhoods and methods of transportation between them. In this paper, we\nclassify docking stations in bicycle-sharing networks to gain insight into the\nspatial delineations of three major United States cities from human mobility\ndynamics. We propose novel time-dependent stochastic block models, with\ndegree-heterogeneous blocks and either mixed or discrete block membership,\nwhich (1) detect the roles served by bicycle-sharing docking stations and (2)\ndescribe the traffic within and between blocks of stations over the course of a\nday. Our models produce concise descriptions of daily bicycle-sharing usage\npatterns in urban environments. They successfully uncover work and home\ndistricts, and they also reveal dynamics of such districts that are particular\nto each city. When we look for more than two roles, we uncover blocks with\nexpected uses, such as leisure activity, as well as previously unknown\nstructures. Our time-dependent SBMs also reveal how the functional roles of\nbicycle-sharing stations are influenced by surrounding public transportation\ninfrastructure. Our work has direct application to the design and maintenance\nof bicycle-sharing systems, and it can be applied more broadly to community\ndetection in temporal and multilayer networks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 02:35:25 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Carlen", "Jane", ""], ["Pont", "Jaume de Dios", ""], ["Mentus", "Cassidy", ""], ["Chang", "Shyr-Shea", ""], ["Wang", "Stephanie", ""], ["Porter", "Mason A.", ""]]}, {"id": "1908.09712", "submitter": "Louis Falissard", "authors": "Louis Falissard, Claire Morgand, Sylvie Roussel, Claire Imbaud, Walid\n  Ghosn, Karim Bounebache, Gr\\'egoire Rey", "title": "A deep artificial neural network based model for underlying cause of\n  death prediction from death certificates", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underlying cause of death coding from death certificates is a process that is\nnowadays undertaken mostly by humans with a potential assistance from expert\nsystems such as the Iris software. It is as a consequence an expensive process\nthat can in addition suffer from geospatial discrepancies, thus severely\nimpairing the comparability of death statistics at the international level. The\nrecent advances in artificial intelligence, specifically the raise of deep\nlearning methods, has enabled computers to make efficient decisions on a number\nof complex problem that were typically considered as out of reach without human\nassistance. They however require a considerable amount of data to learn from,\nwhich is typically their main limiting factor. However, the C\\'epiDc stores an\nexhaustive database of death certificate at the French national scale,\namounting to several millions training example available for the machine\nlearning practitioner. This article presents a deep learning based tool for\nautomated coding of the underlying cause of death from the data contained in\ndeath certificates with 97.8% accuracy, a substantial achievement compared to\nthe Iris software and its 75% accuracy assessed on the same test examples. Such\nan improvement opens a whole field of new applications, from nosologist-level\nbatch automated coding to international and temporal harmonization of cause of\ndeath statistics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:46:36 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Falissard", "Louis", ""], ["Morgand", "Claire", ""], ["Roussel", "Sylvie", ""], ["Imbaud", "Claire", ""], ["Ghosn", "Walid", ""], ["Bounebache", "Karim", ""], ["Rey", "Gr\u00e9goire", ""]]}, {"id": "1908.09729", "submitter": "Yili Hong", "authors": "Yueyao Wang and I-Chen Lee and Lu Lu and Yili Hong", "title": "Statistical Analysis of Modern Reliability Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional reliability analysis has been using time to event data,\ndegradation data, and recurrent event data, while the associated covariates\ntend to be simple and constant over time. Over the past years, we have\nwitnessed the rapid development of sensor and wireless technology, which\nenables us to track how the product has been used and under which environmental\nconditions it has been used. Nowadays, we are able to collect richer\ninformation on covariates which provides opportunities for better reliability\npredictions. In this chapter, we first review recent development on statistical\nmethods for reliability analysis. We then focus on introducing several specific\nmethods that were developed for different types of reliability data with\ncovariate information. Illustrations of those methods are also provided using\nexamples from industry. Test planning is also an important part of reliability\nanalysis. In addition to data analysis, we also provide a briefly review on\nrecent developments of test planning and then focus on illustrating the\nsequential Bayesian design with an example of fatigue testing for polymer\ncomposites. The paper is concluded with some discussions and remarks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 15:23:13 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Yueyao", ""], ["Lee", "I-Chen", ""], ["Lu", "Lu", ""], ["Hong", "Yili", ""]]}, {"id": "1908.09828", "submitter": "Xianan Huang", "authors": "Xianan Huang, Boqi Li, Huei Peng, Joshua A. Auld, Vadim O. Sokolov", "title": "Eco-Mobility-on-Demand Fleet Control with Ride-Sharing", "comments": "arXiv admin note: text overlap with arXiv:1801.08602", "journal-ref": null, "doi": "10.1109/TITS.2020.3032473", "report-no": null, "categories": "stat.AP cs.RO eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared Mobility-on-Demand using automated vehicles can reduce energy\nconsumption and cost for future mobility. However, its full potential in energy\nsaving has not been fully explored. An algorithm to minimize fleet fuel\nconsumption while satisfying customers travel time constraints is developed in\nthis paper. Numerical simulations with realistic travel demand and route choice\nare performed, showing that if fuel consumption is not considered, the MOD\nservice can increase fleet fuel consumption due to increased empty vehicle\nmileage. With fuel consumption as part of the cost function, we can reduce\ntotal fuel consumption by 7 percent while maintaining a high level of mobility\nservice.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 18:45:42 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 19:32:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huang", "Xianan", ""], ["Li", "Boqi", ""], ["Peng", "Huei", ""], ["Auld", "Joshua A.", ""], ["Sokolov", "Vadim O.", ""]]}, {"id": "1908.09830", "submitter": "Adrian Dobra", "authors": "Zhihang Dong, Yen-Chi Chen and Adrian Dobra", "title": "A statistical framework for measuring the temporal stability of human\n  mobility patterns", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing popularity of human mobility studies that collect GPS\nlocation data, the problem of determining the minimum required length of GPS\nmonitoring has not been addressed in the current statistical literature. In\nthis paper we tackle this problem by laying out a theoretical framework for\nassessing the temporal stability of human mobility based on GPS location data.\nWe define several measures of the temporal dynamics of human spatiotemporal\ntrajectories based on the average velocity process, and on activity\ndistributions in a spatial observation window. We demonstrate the use of our\nmethods with data that comprise the GPS locations of 185 individuals over the\ncourse of 18 months. Our empirical results suggest that GPS monitoring should\nbe performed over periods of time that are significantly longer than what has\nbeen previously suggested. Furthermore, we argue that GPS study designs should\ntake into account demographic groups.\n  KEYWORDS: Density estimation; global positioning systems (GPS); human\nmobility; spatiotemporal trajectories; temporal dynamics\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 18:35:01 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Dong", "Zhihang", ""], ["Chen", "Yen-Chi", ""], ["Dobra", "Adrian", ""]]}, {"id": "1908.09881", "submitter": "Tyler McCormick", "authors": "Emily Breza, Arun G. Chandrasekhar, Tyler H. McCormick, Mengjie Pan", "title": "Consistently estimating graph statistics using Aggregated Relational\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated Relational Data, known as ARD, capture information about a social\nnetwork by asking about the number of connections between a person and a group\nwith a particular characteristic, rather than asking about connections between\neach pair of individuals directly. Breza et al. (Forthcoming) and McCormick and\nZheng (2015) relate ARD questions, consisting of survey items of the form \"How\nmany people with characteristic X do you know?\" to parametric statistical\nmodels for complete graphs. In this paper, we propose criteria for consistent\nestimation of individual and graph level statistics from ARD data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:04:05 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Breza", "Emily", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""], ["Pan", "Mengjie", ""]]}, {"id": "1908.09968", "submitter": "Anna Booth", "authors": "Anna Tudehope Booth, Jacqui Macdonald, George Youssef", "title": "Contextual Stress and Maternal Sensitivity: A Meta-Analytic Review of\n  Stress Associations with the Maternal Behavior Q-Sort in Observational\n  Studies", "comments": null, "journal-ref": null, "doi": "10.1016/j.dr.2018.02.002", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maternal sensitivity is a modifiable determinant of infant attachment\nsecurity and a precursor to optimal child development. Contextual stressors\nundermine sensitivity, but research was yet to be synthesized. We aimed to\nidentify i) types of stress associations analyzed in studies of maternal\nsensitivity and ii) the strength of effects of various stress factors. A\nsystematic search identified all studies that used the Maternal Behavior Q-Sort\n(MBQS) to code sensitivity in dyadic observations and that reported a\ncoefficient for MBQS associations with contextual stress. Identified stressors\ncohered around three spheres: sociodemography (maternal education, family\nincome, composite SES, maternal age and cohabitation status); parenting stress\n(perceived maternal stress related to parenting); and mental health\n(specifically maternal internalizing symptoms). Seven meta-analyses (combined\nns range 223-1239) of a subset of 30 effects from 20 articles, and a\nmulti-level meta-analysis (N=1324) assessed aggregated correlations with\nsensitivity. Significant mean effects emerged in expected directions, whereby\nall stress indicators were negatively associated with sensitivity. Small\neffects were found for associations with parenting stress (r=-0.13) and mental\nhealth indicators (r=-0.12). Generally moderate effects were found for\nassociations with socio-demographic indicators (range r=-0.12 to r=0.32).\nEmerging findings support the proposition that in various contexts of stress,\nmaternal sensitivity to infant needs can be undermined. Implications and\nresearch directions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 00:48:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Booth", "Anna Tudehope", ""], ["Macdonald", "Jacqui", ""], ["Youssef", "George", ""]]}, {"id": "1908.10104", "submitter": "Clement Atzberger", "authors": "Chrisgone Adede, Robert Oboko, Peter W. Wagacha and Clement Atzberger", "title": "Model ensembles of artificial neural networks and support vector\n  regression for improved accuracy in the prediction of vegetation conditions", "comments": "27 pages, 13 figure, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing need for highly predictive and stable models for the\nprediction of drought as an aid to better planning for drought response. This\npaper presents the performance of both homogenous and heterogenous model\nensembles in the prediction of drought severity using the study case techniques\nof artificial neural networks (ANN) and support vector regression (SVR). For\neach of the homogenous and heterogenous model ensembles, the study investigates\nthe performance of three model ensembling approaches: linear averaging\n(non-weighted), ranked weighted averaging and model stacking using artificial\nneural networks. Using the approach of 'over-produce then select', the study\nused 17 years of data on 16 selected variables for predictive drought\nmonitoring to build 244 individual ANN and SVR models from which 111 models\nwere selected for the building of the model ensembles. The results indicate\nmarginal superiority of heterogenous to homogenous model ensembles. Model\nstacking is shown to realize models that are superior in performance in the\nprediction of future vegetation conditions as compared to the linear averaging\nand weighted averaging approaches. The best performance from the heterogenous\nstacked model ensembles recorded an R2 of 0.94 in the prediction of future\nvegetation conditions as compared to an R2 of 0.83 and R2 of 0.78 for both ANN\nand SVR respectively in the traditional champion model approaches to the\nrealization of predictive models. We conclude that despite the computational\nresource intensiveness of the model ensembling approach to drought prediction,\nthe returns in terms of model performance is worth the investment, especially\nin the context of the recent exponential increase in computational power.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 09:33:46 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Adede", "Chrisgone", ""], ["Oboko", "Robert", ""], ["Wagacha", "Peter W.", ""], ["Atzberger", "Clement", ""]]}, {"id": "1908.10226", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga, Tristan Bertin, Theresa M. Hardy, David J. Albers,\n  No\\'emie Elhadad", "title": "Multi-Task Gaussian Processes and Dilated Convolutional Networks for\n  Reconstruction of Reproductive Hormonal Dynamics", "comments": "Accepted and presented in Machine Learning for Healthcare 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end statistical framework for personalized, accurate,\nand minimally invasive modeling of female reproductive hormonal patterns.\nReconstructing and forecasting the evolution of hormonal dynamics is a\nchallenging task, but a critical one to improve general understanding of the\nmenstrual cycle and personalized detection of potential health issues. Our goal\nis to infer and forecast individual hormone daily levels over time, while\naccommodating pragmatic and minimally invasive measurement settings. To that\nend, our approach combines the power of probabilistic generative models (i.e.,\nmulti-task Gaussian processes) with the flexibility of neural networks (i.e., a\ndilated convolutional architecture) to learn complex temporal mappings. To\nattain accurate hormone level reconstruction with as little data as possible,\nwe propose a sampling mechanism for optimal reconstruction accuracy with\nlimited sampling budget. Our results show the validity of our proposed hormonal\ndynamic modeling framework, as it provides accurate predictive performance\nacross different realistic sampling budgets and outperforms baselines methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 14:21:31 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Bertin", "Tristan", ""], ["Hardy", "Theresa M.", ""], ["Albers", "David J.", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "1908.10282", "submitter": "Qiwen Sun", "authors": "Serge Richard and Qiwen Sun", "title": "Analysis on MathSciNet database: some preliminary results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate some investigations on MathSciNet database. For\nmany mathematicians this website is used on a regular basis, but surprisingly\nexcept for the information provided by MathSciNet itself, there exist almost no\nindependent investigations or independent statistics on this database. This\ncurrent research has been triggered by a rumor: do international collaborations\nincrease the number of citations of an academic work in mathematics? We use\nMathSciNet for providing some information about this rumor, and more generally\npave the way for further investigations on or with MathSciNet. Keywords:\nMathSciNet, tree-based methods, international collaborations\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:40:34 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Richard", "Serge", ""], ["Sun", "Qiwen", ""]]}, {"id": "1908.10315", "submitter": "Ming Jin", "authors": "Ming Jin, Javad Lavaei, Somayeh Sojoudi, Ross Baldick", "title": "Boundary Defense against Cyber Threat for Power System Operation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The operation of power grids is becoming increasingly data-centric. While the\nabundance of data could improve the efficiency of the system, it poses major\nreliability challenges. In particular, state estimation aims to learn the\nbehavior of the network from data but an undetected attack on this problem\ncould lead to a large-scale blackout. Nevertheless, understanding vulnerability\nof state estimation against cyber attacks has been hindered by the lack of\ntools studying the topological and data-analytic aspects of the network.\nAlgorithmic robustness is of critical need to extract reliable information from\nabundant but untrusted grid data. We propose a robust state estimation\nframework that leverages network sparsity and data abundance. For a large-scale\npower grid, we quantify, analyze, and visualize the regions of the network\nprone to cyber attacks. We also propose an optimization-based graphical\nboundary defense mechanism to identify the border of the geographical area\nwhose data has been manipulated. The proposed method does not allow a local\nattack to have a global effect on the data analysis of the entire network,\nwhich enhances the situational awareness of the grid especially in the face of\nadversity. The developed mathematical framework reveals key geometric and\nalgebraic factors that can affect algorithmic robustness and is used to study\nthe vulnerability of the U.S. power grid in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 17:40:57 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Jin", "Ming", ""], ["Lavaei", "Javad", ""], ["Sojoudi", "Somayeh", ""], ["Baldick", "Ross", ""]]}, {"id": "1908.10341", "submitter": "Marco Broccardo", "authors": "Ziqi Wang, Marco Broccardo", "title": "A novel active learning-based Gaussian process metamodelling strategy\n  for estimating the full probability distribution in forward UQ analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an active learning-based Gaussian process (AL-GP)\nmetamodelling method to estimate the cumulative as well as complementary\ncumulative distribution function (CDF/CCDF) for forward uncertainty\nquantification (UQ) problems. Within the field of UQ, previous studies focused\non developing AL-GP approaches for reliability (rare event probability)\nanalysis of expensive black-box solvers. A naive iteration of these algorithms\nwith respect to different CDF/CCDF threshold values would yield a discretized\nCDF/CCDF. However, this approach inevitably leads to a trade-off between\naccuracy and computational efficiency since both depend (in opposite way) on\nthe selected discretization. In this study, a specialized error measure and a\nlearning function are developed such that the resulting AL-GP method is able to\nefficiently estimate the CDF/CCDF for a specified range of interest without an\nexplicit dependency on discretization. Particularly, the proposed AL-GP method\nis able to simultaneously provide accurate CDF and CCDF estimation in their\nmedian-low probability regions. Three numerical examples are introduced to test\nand verify the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:25:01 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Ziqi", ""], ["Broccardo", "Marco", ""]]}, {"id": "1908.10448", "submitter": "Lin Liu", "authors": "Lin Liu, Zach Shahn, James M. Robins, Andrea Rotnitzky", "title": "Efficient estimation of optimal regimes under a no direct effect\n  assumption", "comments": "In press in the Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new estimators of an optimal joint testing and treatment regime\nunder the no direct effect (NDE) assumption that a given laboratory,\ndiagnostic, or screening test has no effect on a patient's clinical outcomes\nexcept through the effect of the test results on the choice of treatment. We\nmodel the optimal joint strategy using an optimal regime structural nested mean\nmodel (opt-SNMM). The proposed estimators are more efficient than previous\nestimators of the parameters of an opt-SNMM because they efficiently leverage\nthe `no direct effect (NDE) of testing' assumption. Our methods will be of\nimportance to decision scientists who either perform cost-benefit analyses or\nare tasked with the estimation of the `value of information' supplied by an\nexpensive diagnostic test (such as an MRI to screen for lung cancer).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 20:09:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 09:03:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Lin", ""], ["Shahn", "Zach", ""], ["Robins", "James M.", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "1908.10465", "submitter": "Yuwen Yang", "authors": "Yuwen Yang, Jayant Rajgopal", "title": "Outreach Strategies for Vaccine Distribution: A Multi-Period Stochastic\n  Modeling Approach", "comments": null, "journal-ref": "Operations Research Forum 2, 24 (2021)", "doi": "10.1007/s43069-021-00064-1", "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vaccination has been proven to be the most effective method to prevent\ninfectious diseases. However, in many low and middle-income countries with\ngeographically dispersed and nomadic populations, last-mile vaccine delivery\ncan be extremely complex. Because newborns in remote population centers often\ndo not have direct access to clinics and hospitals, they face significant risk\nfrom diseases and infections. An approach known as outreach is typically\nutilized to raise immunization rates in these situations. A set of these remote\nlocations is chosen, and over an appropriate planning period, teams of\nclinicians and support personnel are sent from a depot to set up mobile clinics\nat these locations to vaccinate people there and in the immediate surrounding\narea. In this paper, we model the problem of optimally designing outreach\nefforts as a mixed integer program that is a combination of a set covering\nproblem and a vehicle routing problem. In addition, because elements relevant\nto outreach (such as populations and road conditions) are often unstable and\nunpredictable, we address uncertainty and determine the worst-case solutions.\nThis is done using a multi-period stochastic modeling approach that considers\nupdated model parameter estimates and revised plans for subsequent planning\nperiods. We also conduct numerical experiments to provide insights on how\ndemographic characteristics affect outreach planning and where outreach\nplanners should focus their attention when gathering data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 21:06:38 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 22:05:03 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yang", "Yuwen", ""], ["Rajgopal", "Jayant", ""]]}, {"id": "1908.10502", "submitter": "Jose Jimenez", "authors": "Jos\\'e L. Jim\\'enez", "title": "Quantifying treatment differences in confirmatory trials under\n  non-proportional hazards", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2020.1815673", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional hazards are a common assumption when designing confirmatory\nclinical trials in oncology. With the emergence of immunotherapy and novel\ntargeted therapies, departure from the proportional hazard assumption is not\nrare in nowadays clinical research. Under non-proportional hazards, the hazard\nratio does not have a straightforward clinical interpretation, and the log-rank\ntest is no longer the most powerful statistical test even though it is still\nvalid. Nevertheless, the log-rank test and the hazard ratio are still the\nprimary analysis tools, and traditional approaches such as sample size increase\nare still proposed to account for the impact of non-proportional hazards. The\nweighed log-rank test and the test based on the restricted mean survival time\n(RMST) are receiving a lot of attention as a potential alternative to the\nlog-rank test. We conduct a simulation study comparing the performance and\noperating characteristics of the log-rank test, the weighted log-rank test and\nthe test based on the RMST, including a treatment effect estimation, under\ndifferent non-proportional hazards patterns. Results show that, under\nnon-proportional hazards, the hazard ratio and weighted hazard ratio have no\nstraightforward clinical interpretation whereas the RMST ratio can be\ninterpreted regardless of the proportional hazards assumption. In terms of\npower, the RMST achieves a similar performance when compared to the log-rank\ntest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 00:33:38 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 17:38:57 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 16:38:18 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jim\u00e9nez", "Jos\u00e9 L.", ""]]}, {"id": "1908.10613", "submitter": "Tat-Thang Vo", "authors": "Tat-Thang Vo, Raphael Porcher and Stijn Vansteelandt", "title": "Rethinking meta-analysis: assessing case-mix heterogeneity when\n  combining treatment effects across patient populations", "comments": "27 pages, 6 tables, 2 figures. Currently under peer-review at the\n  journal of Research Synthesis Methods", "journal-ref": null, "doi": "10.1002/jrsm.1382", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-mix heterogeneity across studies complicates meta-analyses. As a result\nof this, treatments that are equally effective on patient subgroups may appear\nto have different effectiveness on patient populations with different case mix.\nIt is therefore important that meta-analyses be explicit for what patient\npopulation they describe the treatment effect. To achieve this, we develop\nalternative approaches for meta-analysis of randomized clinical trials, which\nuse individual patient data (IPD) from all trials to infer the treatment effect\nfor the patient population in a given trial, based on direct standardization\nusing either outcome regression (OCR) or inverse probability weighting (IPW).\nAccompanying random-effect meta-analysis models are developed. The new\napproaches enable disentangling heterogeneity due to case-mix inconsistency\nfrom that due to beyond case-mix reasons.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 09:50:16 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 14:46:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Vo", "Tat-Thang", ""], ["Porcher", "Raphael", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1908.10616", "submitter": "Nadhir Ben Rached", "authors": "Nadhir Ben Rached and Daniel MacKinlay and Zdravko Botev and Raul\n  Tempone and Mohamed-Slim Alouini", "title": "A Universal Splitting Estimator for the Performance Evaluation of\n  Wireless Communications Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified rare-event estimator for the performance evaluation of\nwireless communication systems. The estimator is derived from the well-known\nmultilevel splitting algorithm. In its original form, the splitting algorithm\ncannot be applied to the simulation and estimation of time-independent\nproblems, because splitting requires an underlying continuous-time Markov\nprocess whose trajectories can be split. We tackle this problem by embedding\nthe static problem of interest within a continuous-time Markov process, so that\nthe target time-independent distribution becomes the distribution of the Markov\nprocess at a given time instant. The main feature of the proposed multilevel\nsplitting algorithm is its large scope of applicability. For illustration, we\nshow how the same algorithm can be applied to the problem of estimating the\ncumulative distribution function (CDF) of sums of random variables (RVs), the\nCDF of partial sums of ordered RVs, the CDF of ratios of RVs, and the CDF of\nweighted sums of Poisson RVs. We investigate the computational efficiency of\nthe proposed estimator via a number of simulation studies and find that it\ncompares favorably with existing estimators.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 09:54:11 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Rached", "Nadhir Ben", ""], ["MacKinlay", "Daniel", ""], ["Botev", "Zdravko", ""], ["Tempone", "Raul", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1908.10636", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Mat\\'u\\v{s} Maciak, Ostap Okhrin and Michal Pe\\v{s}ta", "title": "Infinitely Stochastic Micro Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting costs is now a front burner in empirical economics. We propose an\nunconventional tool for stochastic prediction of future expenses based on the\nindividual (micro) developments of recorded events. Consider a firm,\nenterprise, institution, or state, which possesses knowledge about particular\nhistorical events. For each event, there is a series of several related\nsubevents: payments or losses spread over time, which all leads to an\ninfinitely stochastic process at the end. Nevertheless, the issue is that some\nalready occurred events do not have to be necessarily reported. The aim lies in\nforecasting future subevent flows coming from already reported, occurred but\nnot reported, and yet not occurred events. Our methodology is illustrated on\nquantitative risk assessment, however, it can be applied to other areas such as\nstartups, epidemics, war damages, advertising and commercials, digital\npayments, or drug prescription as manifested in the paper. As a theoretical\ncontribution, inference for infinitely stochastic processes is developed. In\nparticular, a non-homogeneous Poisson process with non-homogeneous Poisson\nprocesses as marks is used, which includes for instance the Cox process as a\nspecial case.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 10:54:18 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 08:35:41 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Maciak", "Mat\u00fa\u0161", ""], ["Okhrin", "Ostap", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1908.10852", "submitter": "Gabriel Oliveira Mr", "authors": "Gabriel Martins de Oliveira and Andre Luiz Cunha", "title": "A calibration method structured on Bayesian Inference of the HCM\n  speed-flow relationship for freeways and multilane highways and a temporal\n  analysis of traffic behavior", "comments": "13 pages; 43 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a calibration method for the speed-flow model of the HCM\n2016 for freeways and multilane highways allied to temporal analysis of traffic\nstream. The proposed method was developed using a sample of more than one\nmillion observations collected by 23 traffic sensors on four highways in the\nstate of S{\\~a}o Paulo. The method is structured on Bayesian inference and\nprovided for each model parameters a probability distribution function. The\nfree-flow speed and capacity presented a probability density function that\napproximates a Normal distribution. The segment in which the speed of traffic\nstream remain constant with the increase of the traffic flow is lower than\ndescribed in HCM 2016, being in some cases close to zero. Along with the\nproposed calibration method an analysis of temporal variation is performed\nwhich shows a significant variation in traffic behavior for different periods.\nThe free-flow speed, capacity and breakpoint distributions obtained through\nmonthly and annual calibration were considered equal by means of\nKolmogorov-Smirnov test, different for the model calibration coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:47:23 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["de Oliveira", "Gabriel Martins", ""], ["Cunha", "Andre Luiz", ""]]}, {"id": "1908.10928", "submitter": "David Edward Williams", "authors": "David E Williams", "title": "Low Cost Sensor Networks; How Do We Know the Data are Reliable?", "comments": "14 pages", "journal-ref": null, "doi": "10.1021/acssensors.9b01455", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Plausibility of data from networks of low-cost measurement devices is a\ngrowing and important contentious issue. Informal networks of low-cost devices\nhave particularly come to prominence for air quality monitoring. The\ncontentious point is the believability of data without regular on-site\ncalibration since that is a specialist task and the costs very quickly become\nvery much larger than the cost of installation in the first place. This article\nsuggests that approaches to the problem that involve appropriate use of\nindependent information have the potential to resolve the contention. Ideas are\nillustrated particularly with reference to low-cost sensor networks for air\nquality measurement.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 20:08:41 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Williams", "David E", ""]]}, {"id": "1908.10965", "submitter": "Christine Peterson", "authors": "Christine B. Peterson, Nathan Osborne, Francesco C. Stingo, Pierrick\n  Bourgeat, James D. Doecke, and Marina Vannucci", "title": "Bayesian Modeling of Multiple Structural Connectivity Networks During\n  the Progression of Alzheimer's Disease", "comments": "Accepted to Biometrics January 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common neurodegenerative disease. The aim of\nthis study is to infer structural changes in brain connectivity resulting from\ndisease progression using cortical thickness measurements from a cohort of\nparticipants who were either healthy control, or with mild cognitive\nimpairment, or Alzheimer's disease patients. For this purpose, we develop a\nnovel approach for inference of multiple networks with related edge values\nacross groups. Specifically, we infer a Gaussian graphical model for each group\nwithin a joint framework, where we rely on Bayesian hierarchical priors to link\nthe precision matrix entries across groups. Our proposal differs from existing\napproaches in that it flexibly learns which groups have the most similar edge\nvalues, and accounts for the strength of connection (rather than only edge\npresence or absence) when sharing information across groups. Our results\nidentify key alterations in structural connectivity which may reflect\ndisruptions to the healthy brain, such as decreased connectivity within the\noccipital lobe with increasing disease severity. We also illustrate the\nproposed method through simulations, where we demonstrate its performance in\nstructure learning and precision matrix estimation with respect to alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 22:09:17 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 15:45:28 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Peterson", "Christine B.", ""], ["Osborne", "Nathan", ""], ["Stingo", "Francesco C.", ""], ["Bourgeat", "Pierrick", ""], ["Doecke", "James D.", ""], ["Vannucci", "Marina", ""]]}, {"id": "1908.11056", "submitter": "Tao Wen", "authors": "Guanjie Zheng, Mengqi Liu, Tao Wen, Hongjian Wang, Huaxiu Yao, Susan\n  L. Brantley, Zhenhui Li", "title": "Targeted Source Detection for Environmental Data", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the face of growing needs for water and energy, a fundamental\nunderstanding of the environmental impacts of human activities becomes critical\nfor managing water and energy resources, remedying water pollution, and making\nregulatory policy wisely. Among activities that impact the environment, oil and\ngas production, wastewater transport, and urbanization are included. In\naddition to the occurrence of anthropogenic contamination, the presence of some\ncontaminants (e.g., methane, salt, and sulfate) of natural origin is not\nuncommon. Therefore, scientists sometimes find it difficult to identify the\nsources of contaminants in the coupled natural and human systems. In this\npaper, we propose a technique to simultaneously conduct source detection and\nprediction, which outperforms other approaches in the interdisciplinary case\nstudy of the identification of potential groundwater contamination within a\nregion of high-density shale gas development.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 05:15:53 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zheng", "Guanjie", ""], ["Liu", "Mengqi", ""], ["Wen", "Tao", ""], ["Wang", "Hongjian", ""], ["Yao", "Huaxiu", ""], ["Brantley", "Susan L.", ""], ["Li", "Zhenhui", ""]]}, {"id": "1908.11099", "submitter": "Daniel Kosiorowski", "authors": "Kosiorowski Daniel, Jerzy P. Rydlewski", "title": "Centrality-oriented Causality -- A Study of EU Agricultural Subsidies\n  and Digital Developement in Poland", "comments": null, "journal-ref": "Operations Research and Decisions, 2020, vol. 30, no. 3, pp. 47-63", "doi": "10.37190/ord200303", "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results of a convincing causal statistical inference related to\nsocio-economic phenomena are treated as especially desired background for\nconducting various socio-economic programs or government interventions.\nUnfortunately, quite often real socio-economic issues do not fulfill\nrestrictive assumptions of procedures of causal analysis proposed in the\nliterature. This paper indicates certain empirical challenges and conceptual\nopportunities related to applications of procedures of data depth concept into\na process of causal inference as to socio-economic phenomena. We show, how to\napply a statistical functional depths in order to indicate factual and\ncounterfactual distributions commonly used within procedures of causal\ninference. Thus a modification of Rubin causality concept is proposed, i.e., a\ncentrality-oriented causality concept. The presented framework is especially\nuseful in a context of conducting causal inference basing on official\nstatistics, i.e., basing on already existing databases. Methodological\nconsiderations related to extremal depth, modified band depth, Fraiman-Muniz\ndepth, and multivariate Wilcoxon sum rank statistic are illustrated by means of\nexample related to a study of an impact of EU direct agricultural subsidies on\na digital development in Poland in a period of 2012-2019.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:36:09 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 07:46:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Daniel", "Kosiorowski", ""], ["Rydlewski", "Jerzy P.", ""]]}, {"id": "1908.11249", "submitter": "Julia Mortera", "authors": "Francesco Dotto and Julia Mortera and Laura Baldasarri and Vincenzo\n  Pascali", "title": "Analysis of a DNA mixture case involving Romani reference populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an Italian criminal case that shows how statistical methods\ncan be used to extract information from a series of mixed DNA profiles. The\ncase involves several different individuals and a set of different DNA traces.\nThe case possibly involves persons of interest of a small, inbred population of\nRomani origin. First, a brief description of the case is provided. Secondly, we\nintroduce some heuristic tools that can be used to evaluate the data and\nbriefly outline the statistical model used for analysing DNA mixtures. Finally,\nwe illustrate some of the findings on the case and discuss further directions\nof research. The results show how the use of different population database\nallele frequencies for analysing the DNA mixtures can lead to very different\nresults, some seemingly inculpatory and some seemingly exculpatory. We also\nillustrate the results obtained from combining the evidence from different\nsamples.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:15:52 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Dotto", "Francesco", ""], ["Mortera", "Julia", ""], ["Baldasarri", "Laura", ""], ["Pascali", "Vincenzo", ""]]}, {"id": "1908.11334", "submitter": "Matthew Wheeler", "authors": "Matthew W. Wheeler, Joost Westerhout, Joe L. Baumert, and Benjamin C.\n  Remington", "title": "Bayesian Stacked Parametric Survival with Frailty Components and\n  Interval Censored Failure Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand effects of exposure to food allergens, food challenge\nstudies are designed to slowly increase the dose of an allergen delivered to\nallergic individuals until an objective reaction occurs. These dose-to-failure\nstudies are used to determine acceptable intake levels and are analyzed using\nparametric failure time models. Though these models can provide estimates of\nthe survival curve, their parametric form may misrepresent the survival\nfunction for doses of interest, and different models that describe the data\nsimilarly may produce different dose-to-failure estimates. Motivated by\npredictive inference, we developed a Bayesian approach to combine survival\nestimates based upon posterior predictive stacking, where the weights are\nformed to maximize posterior predictive accuracy. The approach allows for the\ninclusion of flexible models, and, in our case, allows us to include random\neffects to account for frailty components entering the model through\nstudy-to-study heterogeneity. The methodology is investigated in simulation,\nand is used to estimate allergic population eliciting doses for multiple food\nallergens.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:34:05 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Wheeler", "Matthew W.", ""], ["Westerhout", "Joost", ""], ["Baumert", "Joe L.", ""], ["Remington", "Benjamin C.", ""]]}, {"id": "1908.11490", "submitter": "Oliver Stevenson", "authors": "Oliver George Stevenson and Brendon James Brewer", "title": "Finding your feet: A Gaussian process model for estimating the abilities\n  of batsmen in Test cricket", "comments": "27 pages, 9 figures", "journal-ref": "J R Stat Soc Series C, 70: 481-506", "doi": "10.1111/rssc.12470", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sport of cricket, player batting ability is traditionally measured\nusing the batting average. However, the batting average fails to measure both\nshort-term changes in ability that occur during an innings, and long-term\nchanges that occur between innings, due to the likes of age and experience in\nvarious match conditions. We derive and fit a Bayesian parametric model that\nemploys a Gaussian process to measure and predict how the batting abilities of\ncricket players vary and fluctuate over the course of entire playing careers.\nThe results allow us to better quantify and predict player batting ability,\ncompared with both traditional cricket statistics, such as the batting average,\nand more complex models, such as the official International Cricket Council\nratings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 00:14:09 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 04:35:04 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Stevenson", "Oliver George", ""], ["Brewer", "Brendon James", ""]]}, {"id": "1908.11500", "submitter": "Yanfei Kang", "authors": "Thiyanga S. Talagala, Feng Li, Yanfei Kang", "title": "FFORMPP: Feature-based forecast model performance prediction", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel meta-learning algorithm for time series\nforecast model performance prediction. We model the forecast error as a\nfunction of time series features calculated from the historical time series\nwith an efficient Bayesian multivariate surface regression approach. The\nminimum predicted forecast error is then used to identify an individual model\nor a combination of models to produce the final forecasts. It is well-known\nthat the performance of most meta-learning models depends on the\nrepresentativeness of the reference dataset used for training. In such\ncircumstances, we augment the reference dataset with a feature-based time\nseries simulation approach, namely GRATIS, in generating a rich and\nrepresentative time series collection. The proposed framework is tested using\nthe M4 competition data and is compared against commonly used forecasting\napproaches. Our approach provides comparable performances to other model\nselection/combination approaches but at a lower computational cost and a higher\ndegree of interpretability, which is important for supporting decisions. We\nalso provide useful insights regarding which forecasting models are expected to\nwork better for particular types of time series, the intrinsic mechanisms of\nthe meta-learners and how the forecasting performances are affected by various\nfactors.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 01:32:34 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 07:40:52 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 02:37:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Talagala", "Thiyanga S.", ""], ["Li", "Feng", ""], ["Kang", "Yanfei", ""]]}, {"id": "1908.11601", "submitter": "Harjit Hullait", "authors": "Harjit Hullait, David S. Leslie, Nicos G. Pavlidis and Steve King", "title": "Robust Function-on-Function Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional linear regression is a widely used approach to model functional\nresponses with respect to functional inputs. However, classical functional\nlinear regression models can be severely affected by outliers. We therefore\nintroduce a Fisher-consistent robust functional linear regression model that is\nable to effectively fit data in the presence of outliers. The model is built\nusing robust functional principal component and least squares regression\nestimators. The performance of the functional linear regression model depends\non the number of principal components used. We therefore introduce a consistent\nrobust model selection procedure to choose the number of principal components.\nOur robust functional linear regression model can be used alongside an outlier\ndetection procedure to effectively identify abnormal functional responses. A\nsimulation study shows our method is able to effectively capture the regression\nbehaviour in the presence of outliers, and is able to find the outliers with\nhigh accuracy. We demonstrate the usefulness of our method on jet engine sensor\ndata. We identify outliers that would not be found if the functional responses\nwere modelled independently of the functional input, or using non-robust\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:04:02 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Hullait", "Harjit", ""], ["Leslie", "David S.", ""], ["Pavlidis", "Nicos G.", ""], ["King", "Steve", ""]]}, {"id": "1908.11604", "submitter": "Stefan Sperlich", "authors": "Stefan Sperlich and Jose-Ramon Uriarte", "title": "The economics of minority language use: theory and empirical evidence\n  for a language game model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language and cultural diversity is a fundamental aspect of the present world.\nWe study three modern multilingual societies -- the Basque Country, Ireland and\nWales -- which are endowed with two, linguistically distant, official\nlanguages: $A$, spoken by all individuals, and $B$, spoken by a bilingual\nminority. In the three cases it is observed a decay in the use of minoritarian\n$B$, a sign of diversity loss. However, for the \"Council of Europe\" the key\nfactor to avoid the shift of $B$ is its use in all domains. Thus, we\ninvestigate the language choices of the bilinguals by means of an evolutionary\ngame theoretic model. We show that the language population dynamics has reached\nan evolutionary stable equilibrium where a fraction of bilinguals have shifted\nto speak $A$. Thus, this equilibrium captures the decline in the use of $B$. To\ntest the theory we build empirical models that predict the use of $B$ for each\nproportion of bilinguals. We show that model-based predictions fit very well\nthe observed use of Basque, Irish, and Welsh.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:13:17 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Sperlich", "Stefan", ""], ["Uriarte", "Jose-Ramon", ""]]}, {"id": "1908.11736", "submitter": "Jean-Philippe Montillet Dr.", "authors": "J.P. Montillet, X. He, K. Yu", "title": "Application of Levy Processes in Modelling (Geodetic) Time Series With\n  Mixed Spectra", "comments": "24 pages, 3 figures, 4 Tables - Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various models have been developed, including the fractional\nBrownian motion (fBm), to analyse the stochastic properties of geodetic time\nseries, together with the extraction of geophysical signals. The noise spectrum\nof these time series is generally modeled as a mixed spectrum, with a sum of\nwhite and coloured noise. Here, we are interested in modelling the residual\ntime series, after deterministically subtracting geophysical signals from the\nobservations. This residual time series is then assumed to be a sum of three\nrandom variables (r.v.), with the last r.v. belonging to the family of Levy\nprocesses. This stochastic term models the remaining residual signals and other\ncorrelated processes. Via simulations and real time series, we identify three\nclasses of Levy processes: Gaussian, fractional and stable. In the first case,\nresiduals are predominantly constituted of short-memory processes. Fractional\nLevy process can be an alternative model to the fBm in the presence of\nlong-term correlations and self-similarity property. Stable process is\ncharacterized by a large variance, which can be satisfied in the case of\nheavy-tailed distributions. The application to geodetic time series imply\npotential anxiety in the functional model selection where missing geophysical\ninformation can generate such residual time series.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 14:27:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Montillet", "J. P.", ""], ["He", "X.", ""], ["Yu", "K.", ""]]}]