[{"id": "1301.0264", "submitter": "Claudia Beleites", "authors": "Claudia Beleites, Reiner Salzer, and Valter Sergo", "title": "Validation of Soft Classification Models using Partial Class\n  Memberships: An Extended Concept of Sensitivity & Co. applied to the Grading\n  of Astrocytoma Tissues", "comments": "The manuscript is accepted for publication in Chemometrics and\n  Intelligent Laboratory Systems. Supplementary figures and tables are at the\n  end of the pdf", "journal-ref": "Chemometrics and Intelligent Laboratory Systems, 122 (2013), 12 -\n  22", "doi": "10.1016/j.chemolab.2012.12.003", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use partial class memberships in soft classification to model uncertain\nlabelling and mixtures of classes. Partial class memberships are not restricted\nto predictions, but may also occur in reference labels (ground truth, gold\nstandard diagnosis) for training and validation data.\n  Classifier performance is usually expressed as fractions of the confusion\nmatrix, such as sensitivity, specificity, negative and positive predictive\nvalues. We extend this concept to soft classification and discuss the bias and\nvariance properties of the extended performance measures. Ambiguity in\nreference labels translates to differences between best-case, expected and\nworst-case performance. We show a second set of measures comparing expected and\nideal performance which is closely related to regression performance, namely\nthe root mean squared error RMSE and the mean absolute error MAE.\n  All calculations apply to classical crisp classification as well as to soft\nclassification (partial class memberships and/or one-class classifiers). The\nproposed performance measures allow to test classifiers with actual borderline\ncases. In addition, hardening of e.g. posterior probabilities into class labels\nis not necessary, avoiding the corresponding information loss and increase in\nvariance.\n  We implement the proposed performance measures in the R package\n\"softclassval\", which is available from CRAN and at\nhttp://softclassval.r-forge.r-project.org.\n  Our reasoning as well as the importance of partial memberships for\nchemometric classification is illustrated by a real-word application:\nastrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for finding\nsurgical excision borders. As borderline cases are the actual target of the\nanalytical technique, samples which are diagnosed to be borderline cases must\nbe included in the validation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2013 17:03:58 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 15:44:47 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Beleites", "Claudia", ""], ["Salzer", "Reiner", ""], ["Sergo", "Valter", ""]]}, {"id": "1301.0413", "submitter": "Ernie Esser", "authors": "Ernie Esser, Yifei Lou, Jack Xin", "title": "A Method for Finding Structured Sparse Solutions to Non-negative Least\n  Squares Problems with Applications", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demixing problems in many areas such as hyperspectral imaging and\ndifferential optical absorption spectroscopy (DOAS) often require finding\nsparse nonnegative linear combinations of dictionary elements that match\nobserved data. We show how aspects of these problems, such as misalignment of\nDOAS references and uncertainty in hyperspectral endmembers, can be modeled by\nexpanding the dictionary with grouped elements and imposing a structured\nsparsity assumption that the combinations within each group should be sparse or\neven 1-sparse. If the dictionary is highly coherent, it is difficult to obtain\ngood solutions using convex or greedy methods, such as non-negative least\nsquares (NNLS) or orthogonal matching pursuit. We use penalties related to the\nHoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsity\npenalties to be added to the objective in NNLS-type models. For solving the\nresulting nonconvex models, we propose a scaled gradient projection algorithm\nthat requires solving a sequence of strongly convex quadratic programs. We\ndiscuss its close connections to convex splitting methods and difference of\nconvex programming. We also present promising numerical results for example\nDOAS analysis and hyperspectral demixing problems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 10:09:41 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Esser", "Ernie", ""], ["Lou", "Yifei", ""], ["Xin", "Jack", ""]]}, {"id": "1301.0633", "submitter": "Sarod Yatawatta", "authors": "Sanaz Kazemi, Sarod Yatawatta, Saleem Zaroubi", "title": "Clustered Calibration: An Improvement to Radio Interferometric Direction\n  Dependent Self-Calibration", "comments": "18 pages, 21 figures, Accepted 2013 January 2. Abstract abridged", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 430,\n  Issue 2, p.1457-1472, 2013", "doi": "10.1093/mnras/stt018", "report-no": null, "categories": "astro-ph.IM cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new generation of radio synthesis arrays, such as LOFAR and SKA, have\nbeen designed to surpass existing arrays in terms of sensitivity, angular\nresolution and frequency coverage. This evolution has led to the development of\nadvanced calibration techniques that ensure the delivery of accurate results at\nthe lowest possible computational cost. However, the performance of such\ncalibration techniques is still limited by the compact, bright sources in the\nsky, used as calibrators. It is important to have a bright enough source that\nis well distinguished from the background noise level in order to achieve\nsatisfactory results in calibration. We present \"clustered calibration\" as a\nmodification to traditional radio interferometric calibration, in order to\naccommodate faint sources that are almost below the background noise level into\nthe calibration process. The main idea is to employ the information of the\nbright sources' measured signals as an aid to calibrate fainter sources that\nare nearby the bright sources. In the case where we do not have bright enough\nsources, a source cluster could act as a bright source that can be\ndistinguished from background noise. We construct a number of source clusters\nassuming that the signals of the sources belonging to a single cluster are\ncorrupted by almost the same errors, and each cluster is calibrated as a single\nsource, using the combined coherencies of its sources simultaneously. This\nupgrades the power of an individual faint source by the effective power of its\ncluster. We give performance analysis of clustered calibration to show the\nsuperiority of this approach compared to the traditional unclustered\ncalibration. We also provide analytical criteria to choose the optimum number\nof clusters for a given observation in an efficient manner.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 21:13:39 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Kazemi", "Sanaz", ""], ["Yatawatta", "Sarod", ""], ["Zaroubi", "Saleem", ""]]}, {"id": "1301.0891", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "Concerns on Monotonic Imbalance Bounding Matching Methods", "comments": null, "journal-ref": "On line supplement for the JASA paper by Iacus, King and Porro\n  (2011)", "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns are expressed for the Monotonic Imbalance Bounding (MIB) property\n(Iacus et al. 2011) and for MIB matching because i) the definition of the MIB\nproperty leads to inconsistencies and the nature of the imbalance measure is\nnot clearly defined, ii) MIB property does not generalize Equal Percent Bias\nReducing (EPBR) property, iii) MIB matching does not provide statistical\ninformation available with EPBR matching.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 11:01:48 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "1301.0974", "submitter": "Yuan Yao", "authors": "Yuan Yao, Raymond Z. Cui, Gregory R. Bowman, Daniel Silva, Jian Sun,\n  Xuhui Huang", "title": "Hierarchical Nystrom Methods for Constructing Markov State Models for\n  Conformational Dynamics", "comments": null, "journal-ref": null, "doi": "10.1063/1.4802007", "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov state models (MSMs) have become a popular approach for investigating\nthe conformational dynamics of proteins and other biomolecules. MSMs are\ntypically built from numerous molecular dynamics simulations by dividing the\nsampled configurations into a large number of microstates based on geometric\ncriteria. The resulting microstate model can then be coarse-grained into a more\nunderstandable macro state model by lumping together rapidly mixing microstates\ninto larger, metastable aggregates. However, finite sampling often results in\nthe creation of many poorly sampled microstates. During coarse-graining, these\nstates are mistakenly identified as being kinetically important because\ntransitions to/from them appear to be slow. In this paper we propose a\nformalism based on an algebraic principle for matrix approximation, i.e. the\nNystrom method, to deal with such poorly sampled microstates. Our scheme builds\na hierarchy of microstates from high to low populations and progressively\napplies spectral clustering on sets of microstates within each level of the\nhierarchy. It helps spectral clustering identify metastable aggregates with\nhighly populated microstates rather than being distracted by lowly populated\nstates. We demonstrate the ability of this algorithm to discover the major\nmetastable states on two model systems, the alanine dipeptide and TrpZip2.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 04:33:55 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Yao", "Yuan", ""], ["Cui", "Raymond Z.", ""], ["Bowman", "Gregory R.", ""], ["Silva", "Daniel", ""], ["Sun", "Jian", ""], ["Huang", "Xuhui", ""]]}, {"id": "1301.1034", "submitter": "Richard Clymo", "authors": "R. S. Clymo (School of Biological and Chemical Sciences, Queen Mary\n  University of London)", "title": "How many of the digits in a mean of 12.3456789012 are worth reporting?", "comments": "5 pages, 1 Table, 2 Figures. New simpler index unifies Table and\n  Figures. Now published. This arXiv-ed version has small amendments to the\n  published version", "journal-ref": "BMC Research Notes 2019 12 (148)", "doi": "10.1186/s13104-019-4175-6", "report-no": null, "categories": "q-bio.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBJECTIVE. A computer program tells me that a mean value is 12.3456789012,\nbut how many of these digits are significant (the rest being random junk)?\nShould I report: 12.3?, 12.3456?, or even 10 (if only the first digit is\nsignificant)? There are several rules-of-thumb but, surprisingly (given that\nthe problem is so common in science), none seem to be evidence-based. RESULTS.\nHere I show how the significance of a digit in a particular decade of a mean\ndepends on the standard error of the mean (SEM). I define an index, DM that can\nbe plotted in graphs. From these a simple evidence-based rule for the number of\nsignificant digits (\"sigdigs\") is distilled: the last sigdig in the mean is in\nthe same decade as the first or second non-zero digit in the SEM. As example,\nfor mean 34.63 (SEM 25.62), with n = 17, the reported value should be 35 (SEM\n26). Digits beyond these contain little or no useful information, and should\nnot be reported lest they damage your credibility.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 18:00:45 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2013 11:21:39 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 11:36:02 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 17:41:51 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Clymo", "R. S.", "", "School of Biological and Chemical Sciences, Queen Mary\n  University of London"]]}, {"id": "1301.1178", "submitter": "Simone Riggi", "authors": "S. Riggi, S. Ingrassia", "title": "Modeling high energy cosmic rays mass composition data via mixtures of\n  multivariate skew-t distributions", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multivariate skew-t distributions for modeling composition data\nof high energy cosmic rays. The model has been validated with simulated data\nfor different primary nuclei and hadronic models focusing on the depth of\nmaximum Xmax and number of muons N{\\mu} observables. Further, we consider\nmixtures of multivariate skew-t distributions for cosmic ray mass composition\ndetermination and event-by-event classification. With respect to other\napproaches in the field, it is based on analytical calculations and allows to\nincorporate different sets of constraints provided by the present hadronic\nmodels. We present some applications to simulated data sets generated with\ndifferent nuclear abundances assumptions. As it does not fully rely on the\nhadronic model predictions, the method is particularly suited to the current\nexperimental scenario in which evidences of discrepancies of the measured data\nwith respect to the models have been reported for some shower observables, such\nas the number of muons at ground level.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 13:07:39 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Riggi", "S.", ""], ["Ingrassia", "S.", ""]]}, {"id": "1301.1191", "submitter": "Tilmann Gneiting", "authors": "Tilmann Gneiting", "title": "Section on the special year for mathematics of planet earth (MPE 2013)", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS606 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1349-1351", "doi": "10.1214/12-AOAS606", "report-no": "IMS-AOAS-AOAS606", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dozens of research centers, foundations, international organizations and\nscientific societies, including the Institute of Mathematical Statistics, have\njoined forces to celebrate 2013 as a special year for the Mathematics of Planet\nEarth. In its five-year history, the Annals of Applied Statistics has been\npublishing cutting edge research in this area, including geophysical,\nbiological and socio-economic aspects of planet Earth, with the special section\non statistics in the atmospheric sciences edited by Fuentes, Guttorp and Stein\n(2008) and the discussion paper by McShane and Wyner (2011) on paleoclimate\nreconstructions [Stein (2011)] having been highlights.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 13:37:50 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Gneiting", "Tilmann", ""]]}, {"id": "1301.1428", "submitter": "Daniel Cooley", "authors": "Daniel Cooley, Richard A. Davis, Philippe Naveau", "title": "Approximating the conditional density given large observed values via a\n  multivariate extremes framework, with application to environmental data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS554 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1406-1429", "doi": "10.1214/12-AOAS554", "report-no": "IMS-AOAS-AOAS554", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phenomena such as air pollution levels are of greatest interest when\nobservations are large, but standard prediction methods are not specifically\ndesigned for large observations. We propose a method, rooted in extreme value\ntheory, which approximates the conditional distribution of an unobserved\ncomponent of a random vector given large observed values. Specifically, for\n$\\mathbf{Z}=(Z_1,...,Z_d)^T$ and $\\mathbf{Z}_{-d}=(Z_1,...,Z_{d-1})^T$, the\nmethod approximates the conditional distribution of\n$[Z_d|\\mathbf{Z}_{-d}=\\mathbf{z}_{-d}]$ when $|\\mathbf{z}_{-d}|>r_*$. The\napproach is based on the assumption that $\\mathbf{Z}$ is a multivariate\nregularly varying random vector of dimension $d$. The conditional distribution\napproximation relies on knowledge of the angular measure of $\\mathbf{Z}$, which\nprovides explicit structure for dependence in the distribution's tail. As the\nmethod produces a predictive distribution rather than just a point predictor,\none can answer any question posed about the quantity being predicted, and, in\nparticular, one can assess how well the extreme behavior is represented. Using\na fitted model for the angular measure, we apply our method to nitrogen dioxide\nmeasurements in metropolitan Washington DC. We obtain a predictive distribution\nfor the air pollutant at a location given the air pollutant's measurements at\nfour nearby locations and given that the norm of the vector of the observed\nmeasurements is large.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 07:00:30 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Cooley", "Daniel", ""], ["Davis", "Richard A.", ""], ["Naveau", "Philippe", ""]]}, {"id": "1301.1444", "submitter": "Julia Mortera", "authors": "Julia Mortera, Paola Vicard, Cecilia Vergari", "title": "Object-oriented Bayesian networks for a decision support system for\n  antitrust enforcement", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS625 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 714-738", "doi": "10.1214/12-AOAS625", "report-no": "IMS-AOAS-AOAS625", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an economic decision problem where the actors are two firms and the\nAntitrust Authority whose main task is to monitor and prevent firms' potential\nanti-competitive behaviour and its effect on the market. The Antitrust\nAuthority's decision process is modelled using a Bayesian network where both\nthe relational structure and the parameters of the model are estimated from a\ndata set provided by the Authority itself. A number of economic variables that\ninfluence this decision process are also included in the model. We analyse how\nmonitoring by the Antitrust Authority affects firms' strategies about\ncooperation. Firms' strategies are modelled as a repeated prisoner's dilemma\nusing object-oriented Bayesian networks. We show how the integration of firms'\ndecision process and external market information can be modelled in this way.\nVarious decision scenarios and strategies are illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 08:54:26 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 10:03:42 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Mortera", "Julia", ""], ["Vicard", "Paola", ""], ["Vergari", "Cecilia", ""]]}, {"id": "1301.1446", "submitter": "Giovanna Jona-Lasinio", "authors": "Giovanna Jona-Lasinio, Alan Gelfand, Mattia Jona-Lasinio", "title": "Spatial analysis of wave direction data using wrapped Gaussian processes", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS576 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1478-1498", "doi": "10.1214/12-AOAS576", "report-no": "IMS-AOAS-AOAS576", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional data arise in various contexts such as oceanography (wave\ndirections) and meteorology (wind directions), as well as with measurements on\na periodic scale (weekdays, hours, etc.). Our contribution is to introduce a\nmodel-based approach to handle periodic data in the case of measurements taken\nat spatial locations, anticipating structured dependence between these\nmeasurements. We formulate a wrapped Gaussian spatial process model for this\nsetting, induced from a customary linear Gaussian process. We build a\nhierarchical model to handle this situation and show that the fitting of such a\nmodel is possible using standard Markov chain Monte Carlo methods. Our approach\nenables spatial interpolation (and can accommodate measurement error). We\nillustrate with a set of wave direction data from the Adriatic coast of Italy,\ngenerated through a complex computer model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 09:02:47 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Jona-Lasinio", "Giovanna", ""], ["Gelfand", "Alan", ""], ["Jona-Lasinio", "Mattia", ""]]}, {"id": "1301.1463", "submitter": "Trond Reitan", "authors": "Trond Reitan, Tore Schweder, Jorijntje Henderiks", "title": "Phenotypic evolution studied by layered stochastic differential\n  equations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS559 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1531-1551", "doi": "10.1214/12-AOAS559", "report-no": "IMS-AOAS-AOAS559", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series of cell size evolution in unicellular marine algae (division\nHaptophyta; Coccolithus lineage), covering 57 million years, are studied by a\nsystem of linear stochastic differential equations of hierarchical structure.\nThe data consists of size measurements of fossilized calcite platelets\n(coccoliths) that cover the living cell, found in deep-sea sediment cores from\nsix sites in the world oceans and dated to irregular points in time. To\naccommodate biological theory of populations tracking their fitness optima, and\nto allow potentially interpretable correlations in time and space, the model\nframework allows for an upper layer of partially observed site-specific\npopulation means, a layer of site-specific theoretical fitness optima and a\nbottom layer representing environmental and ecological processes. While the\nmodeled process has many components, it is Gaussian and analytically tractable.\nA total of 710 model specifications within this framework are compared and\ninference is drawn with respect to model structure, evolutionary speed and the\neffect of global temperature.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 09:55:22 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Reitan", "Trond", ""], ["Schweder", "Tore", ""], ["Henderiks", "Jorijntje", ""]]}, {"id": "1301.1525", "submitter": "Andrew W. Baggaley", "authors": "Andrew W. Baggaley, Richard J. Boys, Andrew Golightly, Graeme R.\n  Sarson, Anvar Shukurov", "title": "Inference for population dynamics in the Neolithic period", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS579 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1352-1376", "doi": "10.1214/12-AOAS579", "report-no": "IMS-AOAS-AOAS579", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation for the spread of the Neolithic incipient\nfarming across Europe using radiocarbon dates. We model the arrival time of\nfarming at radiocarbon-dated, early Neolithic sites by a numerical solution to\nan advancing wavefront. We allow for (technical) uncertainty in the radiocarbon\ndata, lack-of-fit of the deterministic model and use a Gaussian process to\nsmooth spatial deviations from the model. Inference for the parameters in the\nwavefront model is complicated by the computational cost required to produce a\nsingle numerical solution. We therefore employ Gaussian process emulators for\nthe arrival time of the advancing wavefront at each radiocarbon-dated site. We\nvalidate our model using predictive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 13:39:22 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Baggaley", "Andrew W.", ""], ["Boys", "Richard J.", ""], ["Golightly", "Andrew", ""], ["Sarson", "Graeme R.", ""], ["Shukurov", "Anvar", ""]]}, {"id": "1301.1527", "submitter": "Panu Er\\\"{a}st\\\"{o}", "authors": "Panu Er\\\"ast\\\"o, Lasse Holmstr\\\"om, Atte Korhola, Jan Weckstr\\\"om", "title": "Finding a consensus on credible features among several paleoclimate\n  reconstructions", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS540 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1377-1405", "doi": "10.1214/12-AOAS540", "report-no": "IMS-AOAS-AOAS540", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to merge several paleoclimate time series into one that\nexhibits a consensus on the features of the individual times series. The\npaleoclimate time series can be noisy, nonuniformly sampled and the dates at\nwhich the paleoclimate is reconstructed can have errors. Bayesian inference is\nused to model the various sources of uncertainty and smoothing of the posterior\ndistribution of the consensus is used to capture its credible features in\ndifferent time scales. The technique is demonstrated by analyzing a collection\nof six Holocene temperature reconstructions from Finnish Lapland based on\nvarious biological proxies. Although the paper focuses on paleoclimate time\nseries, the proposed method can be applied in other contexts where one seeks to\ninfer features that are jointly supported by an ensemble of irregularly sampled\nnoisy time series.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 13:45:08 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Er\u00e4st\u00f6", "Panu", ""], ["Holmstr\u00f6m", "Lasse", ""], ["Korhola", "Atte", ""], ["Weckstr\u00f6m", "Jan", ""]]}, {"id": "1301.1530", "submitter": "Brian J. Reich", "authors": "Brian J. Reich, Benjamin A. Shaby", "title": "A hierarchical max-stable spatial model for extreme precipitation", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS591 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1430-1451", "doi": "10.1214/12-AOAS591", "report-no": "IMS-AOAS-AOAS591", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme environmental phenomena such as major precipitation events manifestly\nexhibit spatial dependence. Max-stable processes are a class of\nasymptotically-justified models that are capable of representing spatial\ndependence among extreme values. While these models satisfy modeling\nrequirements, they are limited in their utility because their corresponding\njoint likelihoods are unknown for more than a trivial number of spatial\nlocations, preventing, in particular, Bayesian analyses. In this paper, we\npropose a new random effects model to account for spatial dependence. We show\nthat our specification of the random effect distribution leads to a max-stable\nprocess that has the popular Gaussian extreme value process (GEVP) as a\nlimiting case. The proposed model is used to analyze the yearly maximum\nprecipitation from a regional climate model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 13:50:46 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Reich", "Brian J.", ""], ["Shaby", "Benjamin A.", ""]]}, {"id": "1301.1760", "submitter": "Robert McKilliam", "authors": "Robby McKilliam, Andre Pollok, Bill Cowley, I. Vaughan L. Clarkson and\n  Barry Quinn", "title": "Carrier phase and amplitude estimation for phase shift keying using\n  pilots and data", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2332976", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider least squares estimators of carrier phase and amplitude from a\nnoisy communications signal that contains both pilot signals, known to the\nreceiver, and data signals, unknown to the receiver. We focus on signaling\nconstellations that have symbols evenly distributed on the complex unit circle,\ni.e., M-ary phase shift keying. We show, under reasonably mild conditions on\nthe distribution of the noise, that the least squares estimator of carrier\nphase is strongly consistent and asymptotically normally distributed. However,\nthe amplitude estimator is not consistent, but converges to a positive real\nnumber that is a function of the true carrier amplitude, the noise distribution\nand the size of the constellation. Our theoretical results can also be applied\nto the case where no pilot symbols exist, i.e., noncoherent detection. The\nresults of Monte Carlo simulations are provided and these agree with the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 06:48:47 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["McKilliam", "Robby", ""], ["Pollok", "Andre", ""], ["Cowley", "Bill", ""], ["Clarkson", "I. Vaughan L.", ""], ["Quinn", "Barry", ""]]}, {"id": "1301.1799", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann and Richard Williams", "title": "How to calculate the practical significance of citation impact\n  differences? An empirical example from evaluative institutional bibliometrics\n  using adjusted predictions and marginal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluative bibliometrics is concerned with comparing research units by using\nstatistical procedures. According to Williams (2012) an empirical study should\nbe concerned with the substantive and practical significance of the findings as\nwell as the sign and statistical significance of effects. In this study we will\nexplain what adjusted predictions and marginal effects are and how useful they\nare for institutional evaluative bibliometrics. As an illustration, we will\ncalculate a regression model using publications (and citation data) produced by\nfour universities in German-speaking countries from 1980 to 2010. We will show\nhow these predictions and effects can be estimated and plotted, and how this\nmakes it far easier to get a practical feel for the substantive meaning of\nresults in evaluative bibliometric studies. We will focus particularly on\nAverage Adjusted Predictions (AAPs), Average Marginal Effects (AMEs), Adjusted\nPredictions at Representative Values (APRVs) and Marginal Effects at\nRepresentative Values (MERVs).\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 10:07:27 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Bornmann", "Lutz", ""], ["Williams", "Richard", ""]]}, {"id": "1301.1817", "submitter": "Janine B. Illian", "authors": "Janine B. Illian, Sigrunn H. S{\\o}rbye, H{\\aa}vard Rue", "title": "A toolbox for fitting complex spatial point process models using\n  integrated nested Laplace approximation (INLA)", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS530 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1499-1530", "doi": "10.1214/11-AOAS530", "report-no": "IMS-AOAS-AOAS530", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops methodology that provides a toolbox for routinely fitting\ncomplex models to realistic spatial point pattern data. We consider models that\nare based on log-Gaussian Cox processes and include local interaction in these\nby considering constructed covariates. This enables us to use integrated nested\nLaplace approximation and to considerably speed up the inferential task. In\naddition, methods for model comparison and model assessment facilitate the\nmodelling process. The performance of the approach is assessed in a simulation\nstudy. To demonstrate the versatility of the approach, models are fitted to two\nrather different examples, a large rainforest data set with covariates and a\npoint pattern with multiple marks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 11:40:13 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Illian", "Janine B.", ""], ["S\u00f8rbye", "Sigrunn H.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1301.1893", "submitter": "Milan \\v{Z}ukovi\\v{c}", "authors": "Milan \\v{Z}ukovi\\v{c}", "title": "Dynamics of episodic transient correlations in currency exchange rate\n  returns and their predictability", "comments": "19 pages, 8 figures", "journal-ref": "Central European Journal of Physics 10 (3) 615 (2012)", "doi": "10.2478/s11534-011-0120-6", "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamics of the linear and non-linear serial dependencies in\nfinancial time series in a rolling window framework. In particular, we focus on\nthe detection of episodes of statistically significant two- and three-point\ncorrelations in the returns of several leading currency exchange rates that\ncould offer some potential for their predictability. We employ a rolling window\napproach in order to capture the correlation dynamics for different window\nlengths and analyze the distributions of periods with statistically significant\ncorrelations. We find that for sufficiently large window lengths these\ndistributions fit well to power-law behavior. We also measure the\npredictability itself by a hit rate, i.e. the rate of consistency between the\nsigns of the actual returns and their predictions, obtained from a simple\ncorrelation-based predictor. It is found that during these relatively brief\nperiods the returns are predictable to a certain degree and the predictability\ndepends on the selection of the window length.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 10:51:03 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["\u017dukovi\u010d", "Milan", ""]]}, {"id": "1301.2093", "submitter": "Marco Scutari", "authors": "Marco Scutari and Ian Mackay and David J. Balding", "title": "Improving the Efficiency of Genomic Selection", "comments": "17 pages, 5 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 2013,\n  12(4), 517-527", "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two approaches to increase the efficiency of phenotypic\nprediction from genome-wide markers, which is a key step for genomic selection\n(GS) in plant and animal breeding. The first approach is feature selection\nbased on Markov blankets, which provide a theoretically-sound framework for\nidentifying non-informative markers. Fitting GS models using only the\ninformative markers results in simpler models, which may allow cost savings\nfrom reduced genotyping. We show that this is accompanied by no loss, and\npossibly a small gain, in predictive power for four GS models: partial least\nsquares (PLS), ridge regression, LASSO and elastic net. The second approach is\nthe choice of kinship coefficients for genomic best linear unbiased prediction\n(GBLUP). We compare kinships based on different combinations of centring and\nscaling of marker genotypes, and a newly proposed kinship measure that adjusts\nfor linkage disequilibrium (LD).\n  We illustrate the use of both approaches and examine their performances using\nthree real-world data sets from plant and animal genetics. We find that elastic\nnet with feature selection and GBLUP using LD-adjusted kinships performed\nsimilarly well, and were the best-performing methods in our study.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 11:38:59 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 19:08:45 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Scutari", "Marco", ""], ["Mackay", "Ian", ""], ["Balding", "David J.", ""]]}, {"id": "1301.2261", "submitter": "Tianjiao Chu", "authors": "Tianjiao Chu, Richard Scheines, Peter L. Spirtes", "title": "Semi-Instrumental Variables: A Test for Instrument Admissibility", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-83-90", "categories": "stat.ME cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a causal graphical model, an instrument for a variable X and its effect Y\nis a random variable that is a cause of X and independent of all the causes of\nY except X. (Pearl (1995), Spirtes et al (2000)). Instrumental variables can be\nused to estimate how the distribution of an effect will respond to a\nmanipulation of its causes, even in the presence of unmeasured common causes\n(confounders). In typical instrumental variable estimation, instruments are\nchosen based on domain knowledge. There is currently no statistical test for\nvalidating a variable as an instrument. In this paper, we introduce the concept\nof semi-instrument, which generalizes the concept of instrument. We show that\nin the framework of additive models, under certain conditions, we can test\nwhether a variable is semi-instrumental. Moreover, adding some distribution\nassumptions, we can test whether two semi-instruments are instrumental. We give\nalgorithms to estimate the p-value that a random variable is semi-instrumental,\nand the p-value that two semi-instruments are both instrumental. These\nalgorithms can be used to test the experts' choice of instruments, or to\nidentify instruments automatically.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:57 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Chu", "Tianjiao", ""], ["Scheines", "Richard", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1301.2264", "submitter": "Gary A. Davis", "authors": "Gary A. Davis", "title": "Using Bayesian Networks to Identify the Causal Effect of Speeding in\n  Individual Vehicle/Pedestrian Collisions", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-105-111", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On roads showing significant violations of posted speed limits, one measure\nof the safety effect of speeding is the difference between the road's actual\naccident count and the count that would have occurred if the posted speed limit\nhad been strictly obeyed. An estimate of this accident reduction can be had by\ncomputing the probability that speeding was a necessary condition for each of\nset of accidents. This is an instance of assessing individual probabilities of\ncausation, which is generally not possible absent prior knowledge of causal\nstructure. For traffic accidents such prior knowledge is often available and\nthis paper illustrates how, for a commonly occurring class of\nvehicle/pedestrian accidents, approaches to uncertainty and causal analyses\nappearing in the accident reconstruction literature can be unified using\nBayesian networks. Measured skidmarks, pedestrian throw distances, and\npedestrian injury severity are treated as evidence, and using the Gibbs\nSampling routine BUGS, the posterior probability distribution over exogenous\nvariables, such as the vehicle's initial speed, location, and driver reaction\ntime, is computed. This posterior distribution is then used to compute the\n\"probability of necessity\" for speeding.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:23:09 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Davis", "Gary A.", ""]]}, {"id": "1301.2399", "submitter": "Jeng-Min Chiou", "authors": "Jeng-Min Chiou", "title": "Dynamical functional prediction and classification, with application to\n  traffic flow prediction", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS595 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1588-1614", "doi": "10.1214/12-AOAS595", "report-no": "IMS-AOAS-AOAS595", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for accurate traffic flow prediction in transportation\nmanagement, we propose a functional data method to analyze traffic flow\npatterns and predict future traffic flow. In this study we approach the problem\nby sampling traffic flow trajectories from a mixture of stochastic processes.\nThe proposed functional mixture prediction approach combines functional\nprediction with probabilistic functional classification to take distinct\ntraffic flow patterns into account. The probabilistic classification procedure,\nwhich incorporates functional clustering and discrimination, hinges on subspace\nprojection. The proposed methods not only assist in predicting traffic flow\ntrajectories, but also identify distinct patterns in daily traffic flow of\ntypical temporal trends and variabilities. The proposed methodology is widely\napplicable in analysis and prediction of longitudinally recorded functional\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 07:10:25 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Chiou", "Jeng-Min", ""]]}, {"id": "1301.2405", "submitter": "Gelila Tilahun", "authors": "Gelila Tilahun, Andrey Feuerverger, Michael Gervers", "title": "Dating medieval English charters", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS566 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1615-1640", "doi": "10.1214/12-AOAS566", "report-no": "IMS-AOAS-AOAS566", "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deeds, or charters, dealing with property rights, provide a continuous\ndocumentation which can be used by historians to study the evolution of social,\neconomic and political changes. This study is concerned with charters (written\nin Latin) dating from the tenth through early fourteenth centuries in England.\nOf these, at least one million were left undated, largely due to administrative\nchanges introduced by William the Conqueror in 1066. Correctly dating such\ncharters is of vital importance in the study of English medieval history. This\npaper is concerned with computer-automated statistical methods for dating such\ndocument collections, with the goal of reducing the considerable efforts\nrequired to date them manually and of improving the accuracy of assigned dates.\nProposed methods are based on such data as the variation over time of word and\nphrase usage, and on measures of distance between documents. The extensive (and\ndated) Documents of Early England Data Set (DEEDS) maintained at the University\nof Toronto was used for this purpose.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 07:46:27 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Tilahun", "Gelila", ""], ["Feuerverger", "Andrey", ""], ["Gervers", "Michael", ""]]}, {"id": "1301.2410", "submitter": "Dimitris Kugiumtzis", "authors": "Ioannis Vlachos and Dimitris Kugiumtzis", "title": "Backward-in-Time Selection of the Order of Dynamic Regression Prediction\n  Model", "comments": "42 pages, 3 figures, accepted for publication in the Journal of\n  Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the optimal structure of dynamic regression models used in\nmultivariate time series prediction and propose a scheme to form the lagged\nvariable structure called Backward-in-Time Selection (BTS) that takes into\naccount feedback and multi-collinearity, often present in multivariate time\nseries. We compare BTS to other known methods, also in conjunction with\nregularization techniques used for the estimation of model parameters, namely\nprincipal components, partial least squares and ridge regression estimation.\nThe predictive efficiency of the different models is assessed by means of Monte\nCarlo simulations for different settings of feedback and multi-collinearity.\nThe results show that BTS has consistently good prediction performance while\nother popular methods have varying and often inferior performance. The\nprediction performance of BTS was also found the best when tested on human\nelectroencephalograms of an epileptic seizure, and to the prediction of returns\nof indices of world financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 08:04:51 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Vlachos", "Ioannis", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1301.2411", "submitter": "Candemir \\c{C}\\.{i}\\u{g}\\c{s}ar", "authors": "Candemir \\c{C}\\.i\\u{g}\\c{s}ar, Jerald F. Lawless", "title": "Assessing transient carryover effects in recurrent event processes, with\n  application to chronic health conditions", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS560 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1641-1663", "doi": "10.1214/12-AOAS560", "report-no": "IMS-AOAS-AOAS560", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some settings involving recurrent events, the occurrence of one event may\nproduce a temporary increase in the event intensity; we refer to this\nphenomenon as a transient carryover effect. This paper provides models and\ntests for carryover effect. Motivation for our work comes from events\nassociated with chronic health conditions, and we consider two studies\ninvolving asthma attacks in children in some detail. We consider how carryover\neffects can be modeled and assessed, and note some difficulties in the context\nof heterogeneous groups of individuals. We give a simple intuitive test for no\ncarryover effect and examine its properties. In addition, we demonstrate the\nneed for detailed modeling in trying to deconstruct the dynamics of recurrent\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 08:10:36 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["\u00c7i\u011f\u015far", "Candemir", ""], ["Lawless", "Jerald F.", ""]]}, {"id": "1301.2420", "submitter": "Yunting Sun", "authors": "Yunting Sun, Nancy R. Zhang, Art B. Owen", "title": "Multiple hypothesis testing adjusted for latent variables, with an\n  application to the AGEMAP gene expression data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS561 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1664-1688", "doi": "10.1214/12-AOAS561", "report-no": "IMS-AOAS-AOAS561", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high throughput settings we inspect a great many candidate variables\n(e.g., genes) searching for associations with a primary variable (e.g., a\nphenotype). High throughput hypothesis testing can be made difficult by the\npresence of systemic effects and other latent variables. It is well known that\nthose variables alter the level of tests and induce correlations between tests.\nThey also change the relative ordering of significance levels among hypotheses.\nPoor rankings lead to wasteful and ineffective follow-up studies. The problem\nbecomes acute for latent variables that are correlated with the primary\nvariable. We propose a two-stage analysis to counter the effects of latent\nvariables on the ranking of hypotheses. Our method, called LEAPP, statistically\nisolates the latent variables from the primary one. In simulations, it gives\nbetter ordering of hypotheses than competing methods such as SVA and\nEIGENSTRAT. For an illustration, we turn to data from the AGEMAP study relating\ngene expression to age for 16 tissues in the mouse. LEAPP generates rankings\nwith greater consistency across tissues than the rankings attained by the other\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 08:53:15 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Sun", "Yunting", ""], ["Zhang", "Nancy R.", ""], ["Owen", "Art B.", ""]]}, {"id": "1301.2429", "submitter": "Hao Wang", "authors": "Hao Wang, Saul Shiffman, Sandra D. Griffith, Daniel F. Heitjan", "title": "Truth and memory: Linking instantaneous and retrospective self-reported\n  cigarette consumption", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS557 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1689-1706", "doi": "10.1214/12-AOAS557", "report-no": "IMS-AOAS-AOAS557", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of smoking behavior commonly use the time-line follow-back (TLFB)\nmethod, or periodic retrospective recall, to gather data on daily cigarette\nconsumption. TLFB is considered adequate for identifying periods of abstinence\nand lapse but not for measurement of daily cigarette consumption, thanks to\nsubstantial recall and digit preference biases. With the development of the\nhand-held electronic diary (ED), it has become possible to collect cigarette\nconsumption data using ecological momentary assessment (EMA), or the\ninstantaneous recording of each cigarette as it is smoked. EMA data, because\nthey do not rely on retrospective recall, are thought to more accurately\nmeasure cigarette consumption. In this article we present an analysis of\nconsumption data collected simultaneously by both methods from 236 active\nsmokers in the pre-quit phase of a smoking cessation study. We define a\nstatistical model that describes the genesis of the TLFB records as a two-stage\nprocess of mis-remembering and rounding, including fixed and random effects at\neach stage. We use Bayesian methods to estimate the model, and we evaluate its\nadequacy by studying histograms of imputed values of the latent remembered\ncigarette count. Our analysis suggests that both mis-remembering and heaping\ncontribute substantially to the distortion of self-reported cigarette counts.\nHigher nicotine dependence, white ethnicity and male sex are associated with\ngreater remembered smoking given the EMA count. The model is potentially useful\nin other applications where it is desirable to understand the process by which\nsubjects remember and report true observations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 09:26:53 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Wang", "Hao", ""], ["Shiffman", "Saul", ""], ["Griffith", "Sandra D.", ""], ["Heitjan", "Daniel F.", ""]]}, {"id": "1301.2435", "submitter": "Trina Patel", "authors": "Trina Patel, Donatello Telesca, Saji George, Andr\\'e E. Nel", "title": "Toxicity profiling of engineered nanomaterials via multivariate\n  dose-response surface modeling", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS563 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1707-1729", "doi": "10.1214/12-AOAS563", "report-no": "IMS-AOAS-AOAS563", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New generation in vitro high-throughput screening (HTS) assays for the\nassessment of engineered nanomaterials provide an opportunity to learn how\nthese particles interact at the cellular level, particularly in relation to\ninjury pathways. These types of assays are often characterized by small sample\nsizes, high measurement error and high dimensionality, as multiple cytotoxicity\noutcomes are measured across an array of doses and durations of exposure. In\nthis paper we propose a probability model for the toxicity profiling of\nengineered nanomaterials. A hierarchical structure is used to account for the\nmultivariate nature of the data by modeling dependence between outcomes and\nthereby combining information across cytotoxicity pathways. In this framework\nwe are able to provide a flexible surface-response model that provides\ninference and generalizations of various classical risk assessment parameters.\nWe discuss applications of this model to data on eight nanoparticles evaluated\nin relation to four cytotoxicity parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 09:49:27 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Patel", "Trina", ""], ["Telesca", "Donatello", ""], ["George", "Saji", ""], ["Nel", "Andr\u00e9 E.", ""]]}, {"id": "1301.2459", "submitter": "S. N. Lahiri", "authors": "S. N. Lahiri, C. Spiegelman, J. Appiah, L. Rilett", "title": "Gap bootstrap methods for massive data sets with an application to\n  transportation engineering", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS587 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1552-1587", "doi": "10.1214/12-AOAS587", "report-no": "IMS-AOAS-AOAS587", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe two bootstrap methods for massive data sets. Naive\napplications of common resampling methodology are often impractical for massive\ndata sets due to computational burden and due to complex patterns of\ninhomogeneity. In contrast, the proposed methods exploit certain structural\nproperties of a large class of massive data sets to break up the original\nproblem into a set of simpler subproblems, solve each subproblem separately\nwhere the data exhibit approximate uniformity and where computational\ncomplexity can be reduced to a manageable level, and then combine the results\nthrough certain analytical considerations. The validity of the proposed methods\nis proved and their finite sample properties are studied through a moderately\nlarge simulation study. The methodology is illustrated with a real data example\nfrom Transportation Engineering, which motivated the development of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 11:31:06 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Lahiri", "S. N.", ""], ["Spiegelman", "C.", ""], ["Appiah", "J.", ""], ["Rilett", "L.", ""]]}, {"id": "1301.2467", "submitter": "Qunhua Li", "authors": "Qunhua Li, Jimmy K. Eng, Matthew Stephens", "title": "A likelihood-based scoring method for peptide identification using mass\n  spectrometry", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS568 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1775-1794", "doi": "10.1214/12-AOAS568", "report-no": "IMS-AOAS-AOAS568", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry provides a high-throughput approach to identify proteins in\nbiological samples. A key step in the analysis of mass spectrometry data is to\nidentify the peptide sequence that, most probably, gave rise to each observed\nspectrum. This is often tackled using a database search: each observed spectrum\nis compared against a large number of theoretical \"expected\" spectra predicted\nfrom candidate peptide sequences in a database, and the best match is\nidentified using some heuristic scoring criterion. Here we provide a more\nprincipled, likelihood-based, scoring criterion for this problem. Specifically,\nwe introduce a probabilistic model that allows one to assess, for each\ntheoretical spectrum, the probability that it would produce the observed\nspectrum. This probabilistic model takes account of peak locations and\nintensities, in both observed and theoretical spectra, which enables\nincorporation of detailed knowledge of chemical plausibility in peptide\nidentification. Besides placing peptide scoring on a sounder theoretical\nfooting, the likelihood-based score also has important practical benefits: it\nprovides natural measures for assessing the uncertainty of each identification,\nand in comparisons on benchmark data it produced more accurate peptide\nidentifications than other methods, including SEQUEST. Although we focus here\non peptide identification, our scoring rule could easily be integrated into any\ndownstream analyses that require peptide-spectrum match scores.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 12:03:30 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Li", "Qunhua", ""], ["Eng", "Jimmy K.", ""], ["Stephens", "Matthew", ""]]}, {"id": "1301.2473", "submitter": "Tyler H. McCormick", "authors": "Tyler H. McCormick, Tian Zheng", "title": "Latent demographic profile estimation in hard-to-reach groups", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS569 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1795-1813", "doi": "10.1214/12-AOAS569", "report-no": "IMS-AOAS-AOAS569", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sampling frame in most social science surveys excludes members of certain\ngroups, known as hard-to-reach groups. These groups, or subpopulations, may be\ndifficult to access (the homeless, e.g.), camouflaged by stigma (individuals\nwith HIV/AIDS), or both (commercial sex workers). Even basic demographic\ninformation about these groups is typically unknown, especially in many\ndeveloping nations. We present statistical models which leverage social network\nstructure to estimate demographic characteristics of these subpopulations using\nAggregated relational data (ARD), or questions of the form \"How many X's do you\nknow?\" Unlike other network-based techniques for reaching these groups, ARD\nrequire no special sampling strategy and are easily incorporated into standard\nsurveys. ARD also do not require respondents to reveal their own group\nmembership. We propose a Bayesian hierarchical model for estimating the\ndemographic characteristics of hard-to-reach groups, or latent demographic\nprofiles, using ARD. We propose two estimation techniques. First, we propose a\nMarkov-chain Monte Carlo algorithm for existing data or cases where the full\nposterior distribution is of interest. For cases when new data can be\ncollected, we propose guidelines and, based on these guidelines, propose a\nsimple estimate motivated by a missing data approach. Using data from McCarty\net al. [Human Organization 60 (2001) 28-39], we estimate the age and gender\nprofiles of six hard-to-reach groups, such as individuals who have HIV, women\nwho were raped, and homeless persons. We also evaluate our simple estimates\nusing simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 12:20:52 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["McCormick", "Tyler H.", ""], ["Zheng", "Tian", ""]]}, {"id": "1301.2490", "submitter": "Juned Siddique", "authors": "Juned Siddique, Ofer Harel, Catherine M. Crespi", "title": "Addressing missing data mechanism uncertainty using multiple-model\n  multiple imputation: Application to a longitudinal clinical trial", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS555 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1814-1837", "doi": "10.1214/12-AOAS555", "report-no": "IMS-AOAS-AOAS555", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for generating multiple imputations for continuous\ndata when the missing data mechanism is unknown. Imputations are generated from\nmore than one imputation model in order to incorporate uncertainty regarding\nthe missing data mechanism. Parameter estimates based on the different\nimputation models are combined using rules for nested multiple imputation.\nThrough the use of simulation, we investigate the impact of missing data\nmechanism uncertainty on post-imputation inferences and show that incorporating\nthis uncertainty can increase the coverage of parameter estimates. We apply our\nmethod to a longitudinal clinical trial of low-income women with depression\nwhere nonignorably missing data were a concern. We show that different\nassumptions regarding the missing data mechanism can have a substantial impact\non inferences. Our method provides a simple approach for formalizing subjective\nnotions regarding nonresponse so that they can be easily stated, communicated\nand compared.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 13:17:25 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Siddique", "Juned", ""], ["Harel", "Ofer", ""], ["Crespi", "Catherine M.", ""]]}, {"id": "1301.2503", "submitter": "Shan Ba", "authors": "Shan Ba, V. Roshan Joseph", "title": "Composite Gaussian process models for emulating expensive functions", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS570 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1838-1860", "doi": "10.1214/12-AOAS570", "report-no": "IMS-AOAS-AOAS570", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of nonstationary Gaussian process model is developed for\napproximating computationally expensive functions. The new model is a composite\nof two Gaussian processes, where the first one captures the smooth global trend\nand the second one models local details. The new predictor also incorporates a\nflexible variance model, which makes it more capable of approximating surfaces\nwith varying volatility. Compared to the commonly used stationary Gaussian\nprocess model, the new predictor is numerically more stable and can more\naccurately approximate complex surfaces when the experimental design is sparse.\nIn addition, the new model can also improve the prediction intervals by\nquantifying the change of local variability associated with the response.\nAdvantages of the new predictor are demonstrated using several examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 14:05:15 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Ba", "Shan", ""], ["Joseph", "V. Roshan", ""]]}, {"id": "1301.2654", "submitter": "Anish Sugathan", "authors": "Anish Sugathan, Deepak Malghan, S. Chandrashekar, Deepak K. Sinha", "title": "Estimation of firm-level productivity changes in the Indian power\n  sector: Disentangling unobserved heterogeneity by a transformed fixed-effect\n  stochastic frontier model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measure firm-level productivity changes in the Indian electricity sector\nduring a period that witnessed several pro-market regulatory changes. Using\ninformation collected from multiple sources we construct a unique panel of\ngenerating firms and transmission and distribution utilities spanning the years\n2000 to 2009. We employ a recently developed improvement in the Stochastic\nFrontier panel method that allows controlling for time-invariant unobserved\nheterogeneity. Using the method we jointly estimate inefficiency and exogenous\ndeterminants of inefficiency. We estimate a flexible translog production model\nand compute decomposition of productivity into components of changes in\ntechnology, efficiency, scale and price effect. During this period, especially\npost Electricity Act 2003, we observed a general decline in firm-level\nproductivity at the mean rate of -1.6% per year. A positive and large technical\nchange is observed in the sector at the rate of 8% per year, attributable\npossibly to newer capacity addition. Except for smaller gas based generators,\ninefficiency is observed to be increasing at the mean rate of 3.1% per year in\nthe sector. Consistent with extant findings we also document no significant\nimpact of un-bundling on firm-level efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 07:41:25 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Sugathan", "Anish", ""], ["Malghan", "Deepak", ""], ["Chandrashekar", "S.", ""], ["Sinha", "Deepak K.", ""]]}, {"id": "1301.2706", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin and V.P. Roychowdhury", "title": "A mathematical theory of fame", "comments": "Journal of Statistical Physics, Published on line 12 January 2013.\n  arXiv admin note: substantial text overlap with arXiv:0906.3558,\n  arXiv:cond-mat/0310049", "journal-ref": "Journal of Statistical Physics: Volume 151, Issue 1 (2013), Page\n  319-328", "doi": "10.1007/s10955-012-0677-5", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study empirically how the fame of WWI fighter-pilot aces, measured in\nnumbers of web pages mentioning them, is related to their achievement, measured\nin numbers of opponent aircraft destroyed. We find that on the average fame\ngrows exponentially with achievement; the correlation coefficient between\nachievement and the logarithm of fame is 0.72. The number of people with a\nparticular level of achievement decreases exponentially with the level, leading\nto a power-law distribution of fame. We propose a stochastic model that can\nexplain the exponential growth of fame with achievement. Next, we hypothesize\nthat the same functional relation between achievement and fame that we found\nfor the aces holds for other professions. This allows us to estimate\nachievement for professions where an unquestionable and universally accepted\nmeasure of achievement does not exist. We apply the method to Nobel Prize\nwinners in Physics. For example, we obtain that Paul Dirac, who is a hundred\ntimes less famous than Einstein contributed to physics only two times less. We\ncompare our results with Landau's ranking.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 18:44:24 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Simkin", "M. V.", ""], ["Roychowdhury", "V. P.", ""]]}, {"id": "1301.2728", "submitter": "Hayafumi Watanabe", "authors": "Misako Takayasu, Hayafumi Watanabe, Hideki Takayasu", "title": "Generalised central limit theorems for growth rate distribution of\n  complex systems", "comments": null, "journal-ref": null, "doi": "10.1007/s10955-014-0956-4", "report-no": null, "categories": "physics.soc-ph physics.data-an q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a solvable model of randomly growing systems consisting of many\nindependent subunits. Scaling relations and growth rate distributions in the\nlimit of infinite subunits are analysed theoretically. Various types of scaling\nproperties and distributions reported for growth rates of complex systems in a\nvariety of fields can be derived from this basic physical model. Statistical\ndata of growth rates for about 1 million business firms are analysed as a\nreal-world example of randomly growing systems. Not only are the scaling\nrelations consistent with the theoretical solution, but the entire functional\nform of the growth rate distribution is fitted with a theoretical distribution\nthat has a power-law tail.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 22:57:32 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 12:56:17 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2013 12:16:04 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2014 07:52:50 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Takayasu", "Misako", ""], ["Watanabe", "Hayafumi", ""], ["Takayasu", "Hideki", ""]]}, {"id": "1301.2738", "submitter": "Li Chen", "authors": "Li Chen, Carey E. Priebe, Daniel L. Sussman, Douglas C. Comer, Will P.\n  Megarry, and James C. Tilton", "title": "Enhanced Archaeological Predictive Modelling in Space Archaeology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and preserving archaeological sites before they are destroyed is\na very important issue. In this paper, we develop a greatly improved\narchaeological predictive model $APM_{enhanced}$ that predicts where\narchaeological sites will be found. This approach is applied to remotely-sensed\nmultispectral bands and a single topographical band obtained from advanced\nremote sensing technologies such as satellites and Airborne Laser Scanning\n(ALS). Our $APM_{enhanced}$ is composed of band transformation, image analysis,\nfeature extraction and classification. We evaluate our methodology on the\nsensor bands over Ft.Irwin, CA, USA. A nested bi-loop cross-validation and\nreceiver operating characteristics curves are used to assess the performance of\nour algorithm. We first validate our method on the east swath of Ft.Irwin and\nthen test on a separate dataset from the west swath of Ft.Irwin. A convex\ncombination of two methodologies: $APM_{conventional}$, which has been used\namong archaeologists for many years, and our $APM_{enhanced}$, is demonstrated\nto yield superior classification performance compared to either alone at low\nfalse negative rates. We compare the performance of our methodology on\ndifferent band combinations, chosen based on the archaeological importance for\nthese sensor bands. We also compare the two types of $APM$s in the aspects of\ninput data, output values, practicality and transferability.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2013 01:09:08 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Chen", "Li", ""], ["Priebe", "Carey E.", ""], ["Sussman", "Daniel L.", ""], ["Comer", "Douglas C.", ""], ["Megarry", "Will P.", ""], ["Tilton", "James C.", ""]]}, {"id": "1301.2871", "submitter": "Hai Liu", "authors": "Hai Liu, Wanzhu Tu", "title": "A semiparametric regression model for paired longitudinal outcomes with\n  application in childhood blood pressure development", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS567 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1861-1882", "doi": "10.1214/12-AOAS567", "report-no": "IMS-AOAS-AOAS567", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research examines the simultaneous influences of height and weight on\nlongitudinally measured systolic and diastolic blood pressure in children.\nPrevious studies have shown that both height and weight are positively\nassociated with blood pressure. In children, however, the concurrent increases\nof height and weight have made it all but impossible to discern the effect of\nheight from that of weight. To better understand these influences, we propose\nto examine the joint effect of height and weight on blood pressure. Bivariate\nthin plate spline surfaces are used to accommodate the potentially nonlinear\neffects as well as the interaction between height and weight. Moreover, we\nconsider a joint model for paired blood pressure measures, that is, systolic\nand diastolic blood pressure, to account for the underlying correlation between\nthe two measures within the same individual. The bivariate spline surfaces are\nallowed to vary across different groups of interest. We have developed related\nmodel fitting and inference procedures. The proposed method is used to analyze\ndata from a real clinical investigation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 07:21:18 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Liu", "Hai", ""], ["Tu", "Wanzhu", ""]]}, {"id": "1301.2878", "submitter": "M. Filippone", "authors": "M. Filippone, A. F. Marquand, C. R. V. Blain, S. C. R. Williams, J.\n  Mour\\~ao-Miranda, M. Girolami", "title": "Probabilistic prediction of neurological disorders with a statistical\n  assessment of neuroimaging data modalities", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS562 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1883-1905", "doi": "10.1214/12-AOAS562", "report-no": "IMS-AOAS-AOAS562", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many neurological disorders, prediction of disease state is an important\nclinical aim. Neuroimaging provides detailed information about brain structure\nand function from which such predictions may be statistically derived. A\nmultinomial logit model with Gaussian process priors is proposed to: (i)\npredict disease state based on whole-brain neuroimaging data and (ii) analyze\nthe relative informativeness of different image modalities and brain regions.\nAdvanced Markov chain Monte Carlo methods are employed to perform posterior\ninference over the model. This paper reports a statistical assessment of\nmultiple neuroimaging modalities applied to the discrimination of three\nParkinsonian neurological disorders from one another and healthy controls,\nshowing promising predictive performance of disease states when compared to\nnonprobabilistic classifiers based on multiple modalities. The statistical\nanalysis also quantifies the relative importance of different neuroimaging\nmeasures and brain regions in discriminating between these diseases and\nsuggests that for prediction there is little benefit in acquiring multiple\nneuroimaging sequences. Finally, the predictive capability of different brain\nregions is found to be in accordance with the regional pathology of the\ndiseases as reported in the clinical literature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 07:57:40 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Filippone", "M.", ""], ["Marquand", "A. F.", ""], ["Blain", "C. R. V.", ""], ["Williams", "S. C. R.", ""], ["Mour\u00e3o-Miranda", "J.", ""], ["Girolami", "M.", ""]]}, {"id": "1301.2894", "submitter": "John A. D. Aston", "authors": "John A. D. Aston, Claudia Kirch", "title": "Evaluating stationarity via change-point alternatives with applications\n  to fMRI data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS565 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1906-1948", "doi": "10.1214/12-AOAS565", "report-no": "IMS-AOAS-AOAS565", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is now a well-established\ntechnique for studying the brain. However, in many situations, such as when\ndata are acquired in a resting state, it is difficult to know whether the data\nare truly stationary or if level shifts have occurred. To this end,\nchange-point detection in sequences of functional data is examined where the\nfunctional observations are dependent and where the distributions of\nchange-points from multiple subjects are required. Of particular interest is\nthe case where the change-point is an epidemic change---a change occurs and\nthen the observations return to baseline at a later time. The case where the\ncovariance can be decomposed as a tensor product is considered with particular\nattention to the power analysis for detection. This is of interest in the\napplication to fMRI, where the estimation of a full covariance structure for\nthe three-dimensional image is not computationally feasible. Using the\ndeveloped methods, a large study of resting state fMRI data is conducted to\ndetermine whether the subjects undertaking the resting scan have\nnonstationarities present in their time courses. It is found that a sizeable\nproportion of the subjects studied are not stationary. The change-point\ndistribution for those subjects is empirically determined, as well as its\ntheoretical properties examined.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 09:25:32 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Aston", "John A. D.", ""], ["Kirch", "Claudia", ""]]}, {"id": "1301.2936", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "Bayesian inference and the parametric bootstrap", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS571 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1971-1997", "doi": "10.1214/12-AOAS571", "report-no": "IMS-AOAS-AOAS571", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric bootstrap can be used for the efficient computation of Bayes\nposterior distributions. Importance sampling formulas take on an easy form\nrelating to the deviance in exponential families and are particularly simple\nstarting from Jeffreys invariant prior. Because of the i.i.d. nature of\nbootstrap sampling, familiar formulas describe the computational accuracy of\nthe Bayes estimates. Besides computational methods, the theory provides a\nconnection between Bayesian and frequentist analysis. Efficient algorithms for\nthe frequentist accuracy of Bayesian inferences are developed and demonstrated\nin a model selection example.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 12:06:44 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "1301.2954", "submitter": "Guido Masarotto", "authors": "Guido Masarotto, Cristiano Varin", "title": "The ranking lasso and its application to sport tournaments", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS581 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1949-1970", "doi": "10.1214/12-AOAS581", "report-no": "IMS-AOAS-AOAS581", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking a vector of alternatives on the basis of a series of paired\ncomparisons is a relevant topic in many instances. A popular example is ranking\ncontestants in sport tournaments. To this purpose, paired comparison models\nsuch as the Bradley-Terry model are often used. This paper suggests fitting\npaired comparison models with a lasso-type procedure that forces contestants\nwith similar abilities to be classified into the same group. Benefits of the\nproposed method are easier interpretation of rankings and a significant\nimprovement of the quality of predictions with respect to the standard maximum\nlikelihood fitting. Numerical aspects of the proposed method are discussed in\ndetail. The methodology is illustrated through ranking of the teams of the\nNational Football League 2010-2011 and the American College Hockey Men's\nDivision I 2009-2010.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 12:42:13 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Masarotto", "Guido", ""], ["Varin", "Cristiano", ""]]}, {"id": "1301.2975", "submitter": "Theodore  Kypraios", "authors": "Simon R. White, Theodore Kypraios, Simon P. Preston", "title": "Fast Approximate Bayesian Computation for discretely observed Markov\n  models using a factorised posterior distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications involve inference for complicated\nstochastic models for which the likelihood function is difficult or even\nimpossible to calculate, and hence conventional likelihood-based inferential\nechniques cannot be used. In such settings, Bayesian inference can be performed\nusing Approximate Bayesian Computation (ABC). However, in spite of many recent\ndevelopments to ABC methodology, in many applications the computational cost of\nABC necessitates the choice of summary statistics and tolerances that can\npotentially severely bias the estimate of the posterior.\n  We propose a new \"piecewise\" ABC approach suitable for discretely observed\nMarkov models that involves writing the posterior density of the parameters as\na product of factors, each a function of only a subset of the data, and then\nusing ABC within each factor. The approach has the advantage of side-stepping\nthe need to choose a summary statistic and it enables a stringent tolerance to\nbe set, making the posterior \"less approximate\". We investigate two methods for\nestimating the posterior density based on ABC samples for each of the factors:\nthe first is to use a Gaussian approximation for each factor, and the second is\nto use a kernel density estimate. Both methods have their merits. The Gaussian\napproximation is simple, fast, and probably adequate for many applications. On\nthe other hand, using instead a kernel density estimate has the benefit of\nconsistently estimating the true ABC posterior as the number of ABC samples\ntends to infinity. We illustrate the piecewise ABC approach for three examples;\nin each case, the approach enables \"exact matching\" between simulations and\ndata and offers fast and accurate inference.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 13:53:07 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 16:04:42 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["White", "Simon R.", ""], ["Kypraios", "Theodore", ""], ["Preston", "Simon P.", ""]]}, {"id": "1301.3027", "submitter": "Alexander Blocker", "authors": "Alexander W Blocker and Pavlos Protopapas", "title": "Semi-parametric Robust Event Detection for Massive Time-Domain Databases", "comments": "16 pages, 5 figures. A shorter version of this work appeared in\n  Statistical Challenges in Modern Astronomy V, Springer-Verlag, 177-189.\n  Implementations of the core algorithms of this paper in C and R are available\n  as the rowavedt package via https://www.github.com/awblocker/rowavedt/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection and analysis of events within massive collections of\ntime-series has become an extremely important task for time-domain astronomy.\nIn particular, many scientific investigations (e.g. the analysis of\nmicrolensing and other transients) begin with the detection of isolated events\nin irregularly-sampled series with both non-linear trends and non-Gaussian\nnoise. We outline a semi-parametric, robust, parallel method for identifying\nvariability and isolated events at multiple scales in the presence of the above\ncomplications. This approach harnesses the power of Bayesian modeling while\nmaintaining much of the speed and scalability of more ad-hoc machine learning\napproaches. We also contrast this work with event detection methods from other\nfields, highlighting the unique challenges posed by astronomical surveys.\nFinally, we present results from the application of this method to 87.2 million\nEROS-2 sources, where we have obtained a greater than 100-fold reduction in\ncandidates for certain types of phenomena while creating high-quality features\nfor subsequent analyses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 15:58:38 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2013 18:35:35 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Blocker", "Alexander W", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1301.3110", "submitter": "Jeffrey Shaman", "authors": "Jeffrey Shaman, Alicia Karspeck and Marc Lipsitch", "title": "Week 1 Influenza Forecast for the 2012-2013 U.S. Season", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is part of a series of weekly influenza forecasts made during the\n2012-2013 influenza season. Here we present results of forecasts initiated\nfollowing assimilation of observations for Week 1 (i.e. the forecast begins\nJanuary 6, 2013) for municipalities in the United States. These forecasts were\nperformed on January 11, 2013. Results from forecasts initiated the six\nprevious weeks (Weeks 47-52) are also presented. The accuracy of these\npredictions will not be known for certain until the conclusion of the current\ninfluenza season; however, at the moment a number of the forecasted peaks\nappear to be inaccurate. This inaccuracy may be due to the virulence of\ninfluenza this season, which appears to be sending more influenza-infected\npersons to seek medical attention and inflates ILI levels (and possibly the\nproportion testing influenza positive) relative to years with milder flu\nstrains. New forecasts that adjust, or scale, for this difference and match the\ntwo focus cities that appear to have already peaked are identified. These new\nforecasts will be used, in addition to the previously scaled forms, to make\ninfluenza predictions for the remainder of the season.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 20:18:47 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 01:57:53 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Shaman", "Jeffrey", ""], ["Karspeck", "Alicia", ""], ["Lipsitch", "Marc", ""]]}, {"id": "1301.3523", "submitter": "Dapo Omidiran", "authors": "Dapo Omidiran", "title": "Penalized Regression Models for the NBA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the National Basketball Association (NBA), teams must make choices about\nwhich players to acquire, how much to pay them, and other decisions that are\nfundamentally dependent on player effectiveness. Thus, there is great interest\nin quantitatively understanding the impact of each player. In this paper we\ndevelop a new penalized regression model for the NBA, use cross-validation to\nselect its tuning parameters, and then use it to produce ratings of player\nability. We then apply the model to the 2010-2011 NBA season to predict the\noutcome of games. We compare the performance of our procedure to other known\nregression techniques for this problem, and demonstrate empirically that our\nmodel produces substantially better predictions. We evaluate the performance of\nour procedure against the Las Vegas gambling lines, and show that with a\nsufficiently large number of games to train on our model outperforms those\nlines. Finally, we demonstrate how the technique developed in this paper can be\nused to quantitively identify \"overrated\" players who are less impactful than\ncommon wisdom might suggest.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 22:50:02 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Omidiran", "Dapo", ""]]}, {"id": "1301.3601", "submitter": "Carlos Morais de Lima", "authors": "Carlos H. M. de Lima, Mehdi Bennis and Matti Latva-aho", "title": "Statistical Analysis of Self-Organizing Networks with Biased Cell\n  Association and Interference Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we assess the viability of heterogeneous networks composed of\nlegacy macrocells which are underlaid with self-organizing picocells. Aiming to\nimprove coverage, cell-edge throughput and overall system capacity,\nself-organizing solutions, such as range expansion bias, almost blank subframe\nand distributed antenna systems are considered. Herein, stochastic geometry is\nused to model network deployments, while higher-order statistics through the\ncumulants concept is utilized to characterize the probability distribution of\nthe received power and aggregate interference at the user of interest. A\ncompre- hensive analytical framework is introduced to evaluate the performance\nof such self-organizing networks in terms of outage probability and average\nchannel capacity with respect to the tagged receiver. To conduct our studies,\nwe consider a shadowed fading channel model incorporating log-normal shadowing\nand Nakagami-m fading. Results show that the analytical framework matches well\nwith numerical results obtained from Monte Carlo simulations. We also observed\nthat by simply using almost blank subframes the aggregate interference at the\ntagged receiver is reduced by about 12dB. Although more elaborated interference\ncontrol techniques such as, downlink bitmap and distributed antennas systems\nbecome needed, when the density of picocells in the underlaid tier gets high.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 06:54:59 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["de Lima", "Carlos H. M.", ""], ["Bennis", "Mehdi", ""], ["Latva-aho", "Matti", ""]]}, {"id": "1301.3718", "submitter": "Jeffrey Leek Jeffrey Leek", "authors": "Leah R. Jager, Jeffrey T. Leek", "title": "Empirical estimates suggest most published medical research is true", "comments": "11 pages, 4 figures, Correspondance to J. Leek", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of published medical research is critical both for scientists,\nphysicians and patients who rely on these results. But the fundamental belief\nin the medical literature was called into serious question by a paper\nsuggesting most published medical research is false. Here we adapt estimation\nmethods from the genomics community to the problem of estimating the rate of\nfalse positives in the medical literature using reported P-values as the data.\nWe then collect P-values from the abstracts of all 77,430 papers published in\nThe Lancet, The Journal of the American Medical Association, The New England\nJournal of Medicine, The British Medical Journal, and The American Journal of\nEpidemiology between 2000 and 2010. We estimate that the overall rate of false\npositives among reported results is 14% (s.d. 1%), contrary to previous claims.\nWe also find there is not a significant increase in the estimated rate of\nreported false positive results over time (0.5% more FP per year, P = 0.18) or\nwith respect to journal submissions (0.1% more FP per 100 submissions, P =\n0.48). Statistical analysis must allow for false positives in order to make\nclaims on the basis of noisy data. But our analysis suggests that the medical\nliterature remains a reliable record of scientific progress.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:19:46 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Jager", "Leah R.", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1301.3759", "submitter": "Isabella Gollini", "authors": "Isabella Gollini and Thomas Brendan Murphy", "title": "Joint Modelling of Multiple Network Views", "comments": "Main paper and Supplementary material: 37 (27 + 10) pages, 20 (16 +\n  4) figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models (LSM) for network data were introduced by Hoff et al.\n(2002) under the basic assumption that each node of the network has an unknown\nposition in a D-dimensional Euclidean latent space: generally the smaller the\ndistance between two nodes in the latent space, the greater their probability\nof being connected. In this paper we propose a variational Bayes approach to\nestimate the intractable posterior of the LSM.\n  In many cases, different network views on the same set of nodes are\navailable. It can therefore be useful to build a model able to jointly\nsummarise the information given by all the network views. For this purpose, we\nintroduce the latent space joint model (LSJM) that merges the information given\nby multiple network views assuming that the probability of a node being\nconnected with other nodes in each network view is explained by a unique latent\nvariable. This model is demonstrated on the analysis of two datasets: the\nexcerpt of 50 girls from `Teenage Friends and Lifestyle Study' data at three\ntime points and the Saccharomyces cerevisiae genetic and physical\nprotein-protein interactions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 17:27:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 13:15:59 GMT"}, {"version": "v3", "created": "Thu, 25 Sep 2014 09:32:40 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Gollini", "Isabella", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1301.3825", "submitter": "Grzegorz Michalski PhD", "authors": "Grzegorz Michalski, Aleksander Mercik", "title": "Polish and Silesian Non-Profit Organizations Liquidity Strategies", "comments": null, "journal-ref": "Statistika 2011 48(4), 45-61", "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.RM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kind of realized mission inflows the sensitivity to risk. Among other\nfactors, the risk results from decision about liquid assets investment level\nand liquid assets financing. The higher the risk exposure, the higher the level\nof liquid assets. If the specific risk exposure is smaller, the more aggressive\ncould be the net liquid assets strategy. The organization choosing between\nvarious solutions in liquid assets needs to decide what level of risk is\nacceptable for her owners (or donors) and / or capital suppliers. The paper\nshows how, in authors opinion, decisions, about liquid assets management\nstrategy inflow the risk of the organizations and its economical results during\nrealization of main mission. Comparison of theoretical model with empirical\ndata for over 450 Silesian nonprofit organization results suggests that\nnonprofit organization managing teams choose more risky aggressive liquid\nassets solutions than for-profit firms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 20:50:23 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Michalski", "Grzegorz", ""], ["Mercik", "Aleksander", ""]]}, {"id": "1301.3902", "submitter": "David M Williamson", "authors": "David M. Williamson, Russell Almond, Robert Mislevy", "title": "Model Criticism of Bayesian Networks with Latent Variables", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-634-643", "categories": "cs.AI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Bayesian networks (BNs) to cognitive assessment and\nintelligent tutoring systems poses new challenges for model construction. When\ncognitive task analyses suggest constructing a BN with several latent\nvariables, empirical model criticism of the latent structure becomes both\ncritical and complex. This paper introduces a methodology for criticizing\nmodels both globally (a BN in its entirety) and locally (observable nodes), and\nexplores its value in identifying several kinds of misfit: node errors, edge\nerrors, state errors, and prior probability errors in the latent structure. The\nresults suggest the indices have potential for detecting model misfit and\nassisting in locating problematic components of the model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:53:20 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Williamson", "David M.", ""], ["Almond", "Russell", ""], ["Mislevy", "Robert", ""]]}, {"id": "1301.3947", "submitter": "Hilary Parker", "authors": "Hilary S. Parker, H\\'ector Corrada Bravo and Jeffrey T. Leek", "title": "Removing batch effects for prediction problems with frozen surrogate\n  variable analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch effects are responsible for the failure of promising genomic prognos-\ntic signatures, major ambiguities in published genomic results, and retractions\nof widely-publicized findings. Batch effect corrections have been developed to\nre- move these artifacts, but they are designed to be used in population\nstudies. But genomic technologies are beginning to be used in clinical\napplications where sam- ples are analyzed one at a time for diagnostic,\nprognostic, and predictive applica- tions. There are currently no batch\ncorrection methods that have been developed specifically for prediction. In\nthis paper, we propose an new method called frozen surrogate variable analysis\n(fSVA) that borrows strength from a training set for individual sample batch\ncorrection. We show that fSVA improves prediction ac- curacy in simulations and\nin public genomic studies. fSVA is available as part of the sva Bioconductor\npackage.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:16:02 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Parker", "Hilary S.", ""], ["Bravo", "H\u00e9ctor Corrada", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1301.3949", "submitter": "Marianna Pensky", "authors": "Daniela De Canditiis, Marianna Pensky and Patrick J. Wolfe", "title": "De-noising procedures for frame operators", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper provides a comprehensive study of de-noising properties of\nframes and, in particular, tight frames, which constitute one of the most\npopular tools in contemporary signal processing. The objective of the paper is\nto bridge the existing gap between mathematical and statistical theories on one\nhand and engineering practice on the other and explore how one can take\nadvantage of a specific structure of a frame in contrast to an arbitrary\ncollection of vectors or an orthonormal basis. For both the general and the\ntight frames, the paper presents a set of practically implementable de-noising\ntechniques which take frame induced correlation structures into account. These\nresults are supplemented by an examination of the case when the frame is\nconstructed as a collection of orthonormal bases. In particular,\nrecommendations are given for aggregation of the estimators at the stage of\nframe coefficients. The paper is concluded by a finite sample simulation study\nwhich confirms that taking frame structure and frame induced correlations into\naccount indeed improves de-noising precision.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:42:42 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["De Canditiis", "Daniela", ""], ["Pensky", "Marianna", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1301.4114", "submitter": "Fran\\c{c}ois Bachoc", "authors": "Fran\\c{c}ois Bachoc, Guillaume Bois, Josselin Garnier, Jean-Marc\n  Martinez", "title": "Calibration and improved prediction of computer models by universal\n  Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the use of experimental data for calibrating a computer\nmodel and improving its predictions of the underlying physical system. A global\nstatistical approach is proposed in which the bias between the computer model\nand the physical system is modeled as a realization of a Gaussian process. The\napplication of classical statistical inference to this statistical model yields\na rigorous method for calibrating the computer model and for adding to its\npredictions a statistical correction based on experimental data. This\nstatistical correction can substantially improve the calibrated computer model\nfor predicting the physical system on new experimental conditions. Furthermore,\na quantification of the uncertainty of this prediction is provided. Physical\nexpertise on the calibration parameters can also be taken into account in a\nBayesian framework. Finally, the method is applied to the thermal-hydraulic\ncode FLICA 4, in a single phase friction model framework. It allows to improve\nthe predictions of the thermal-hydraulic code FLICA 4 significantly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 15:12:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 08:18:20 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Bois", "Guillaume", ""], ["Garnier", "Josselin", ""], ["Martinez", "Jean-Marc", ""]]}, {"id": "1301.4144", "submitter": "Dimitris Vavoulis", "authors": "Dimitrios V. Vavoulis and Julian Gough", "title": "Non-parametric Bayesian modelling of digital gene expression data", "comments": null, "journal-ref": "J Comput Sci Syst Biol 7:001-009 (2013)", "doi": "10.4172/jcsb.1000131", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing technologies provide a revolutionary tool for\ngenerating gene expression data. Starting with a fixed RNA sample, they\nconstruct a library of millions of differentially abundant short sequence tags\nor \"reads\", which constitute a fundamentally discrete measure of the level of\ngene expression. A common limitation in experiments using these technologies is\nthe low number or even absence of biological replicates, which complicates the\nstatistical analysis of digital gene expression data. Analysis of this type of\ndata has often been based on modified tests originally devised for analysing\nmicroarrays; both these and even de novo methods for the analysis of RNA-seq\ndata are plagued by the common problem of low replication. We propose a novel,\nnon-parametric Bayesian approach for the analysis of digital gene expression\ndata. We begin with a hierarchical model for modelling over-dispersed count\ndata and a blocked Gibbs sampling algorithm for inferring the posterior\ndistribution of model parameters conditional on these counts. The algorithm\ncompensates for the problem of low numbers of biological replicates by\nclustering together genes with tag counts that are likely sampled from a common\ndistribution and using this augmented sample for estimating the parameters of\nthis distribution. The number of clusters is not decided a priori, but it is\ninferred along with the remaining model parameters. We demonstrate the ability\nof this approach to model biological data with high fidelity by applying the\nalgorithm on a public dataset obtained from cancerous and non-cancerous neural\ntissues.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 16:08:00 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Vavoulis", "Dimitrios V.", ""], ["Gough", "Julian", ""]]}, {"id": "1301.4167", "submitter": "Ekkehard  Glimm", "authors": "Ekkehard Glimm and J\\\"urgen L\\\"auter", "title": "Some Notes on Blinded Sample Size Re-Estimation", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note investigates a number of scenarios in which unadjusted testing\nfollowing a blinded sample size re-estimation leads to type I error violations.\nFor superiority testing, this occurs in certain small-sample borderline cases.\nWe discuss a number of alternative approaches that keep the type I error rate.\nThe paper also gives a reason why the type I error inflation in the superiority\ncontext might have been missed in previous publications and investigates why it\nis more marked in case of non-inferiority testing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 17:35:00 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Glimm", "Ekkehard", ""], ["L\u00e4uter", "J\u00fcrgen", ""]]}, {"id": "1301.4674", "submitter": "Elvan Ceyhan", "authors": "E. Ceyhan, T. Nishino, J. Alexopolous, R. D. Todd, K. N. Botteron, M.\n  I. Miller, J. T. Ratnanather", "title": "Censoring Distances Based on Labeled Cortical Distance Maps in Cortical\n  Morphometry", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-12-1", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape differences are manifested in cortical structures due to\nneuropsychiatric disorders. Such differences can be measured by labeled\ncortical distance mapping (LCDM) which characterizes the morphometry of the\nlaminar cortical mantle of cortical structures. LCDM data consist of signed\ndistances of gray matter (GM) voxels with respect to GM/white matter (WM)\nsurface. Volumes and descriptive measures (such as means and variances) for\neach subject and the pooled distances provide the morphometric differences\nbetween diagnostic groups, but they do not reveal all the morphometric\ninformation contained in LCDM distances. To extract more information from LCDM\ndata, censoring of the distances is introduced. For censoring of LCDM\ndistances, the range of LCDM distances is partitioned at a fixed increment\nsize; and at each censoring step, and distances not exceeding the censoring\ndistance are kept. Censored LCDM distances inherit the advantages of the pooled\ndistances. Furthermore, the analysis of censored distances provides information\nabout the location of morphometric differences which cannot be obtained from\nthe pooled distances. However, at each step, the censored distances aggregate,\nwhich might confound the results. The influence of data aggregation is\ninvestigated with an extensive Monte Carlo simulation analysis and it is\ndemonstrated that this influence is negligible. As an illustrative example, GM\nof ventral medial prefrontal cortices (VMPFCs) of subjects with major\ndepressive disorder (MDD), subjects at high risk (HR) of MDD, and healthy\ncontrol (Ctrl) subjects are used. A significant reduction in laminar thickness\nof the VMPFC and perhaps shrinkage in MDD and HR subjects is observed when\ncompared to Ctrl subjects. The methodology is also applicable to LCDM-based\nmorphometric measures of other cortical structures affected by disease.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2013 19:28:06 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Ceyhan", "E.", ""], ["Nishino", "T.", ""], ["Alexopolous", "J.", ""], ["Todd", "R. D.", ""], ["Botteron", "K. N.", ""], ["Miller", "M. I.", ""], ["Ratnanather", "J. T.", ""]]}, {"id": "1301.4797", "submitter": "Julie Josse", "authors": "Vincent Audigier, Fran\\c{c}ois Husson, Julie Josse", "title": "A principal components method to impute missing values for mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to impute missing values in mixed datasets. It is\nbased on a principal components method, the factorial analysis for mixed data,\nwhich balances the influence of all the variables that are continuous and\ncategorical in the construction of the dimensions of variability. Because the\nimputation uses the principal axes and components, the prediction of the\nmissing values are based on the similarity between individuals and on the\nrelationships between variables. The quality of the imputation is assessed\nthrough a simulation study and real datasets. The method is compared to a\nrecent method (Stekhoven and B\\\"uhlmann, 2011) based on random forests and\nshows better performances especially for the imputation of categorical\nvariables and when there are highly linear relationships between continuous\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 09:36:49 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 16:23:15 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Audigier", "Vincent", ""], ["Husson", "Fran\u00e7ois", ""], ["Josse", "Julie", ""]]}, {"id": "1301.4913", "submitter": "Fran\\c{c}ois Giraud", "authors": "Fran\\c{c}ois Giraud, Pierre Minvielle and Pierre Del Moral", "title": "Advanced Interacting Sequential Monte Carlo Sampling for Inverse\n  Scattering", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/29/9/095014", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following electromagnetism (EM) inverse problem is addressed. It consists\nin estimating local radioelectric properties of materials recovering an object\nfrom global EM scattering measurements, at various incidences and wave\nfrequencies. This large scale ill-posed inverse problem is explored by an\nintensive exploitation of an efficient 2D Maxwell solver, distributed on high\nperformance computing machines. Applied to a large training data set, a\nstatistical analysis reduces the problem to a simpler probabilistic metamodel,\non which Bayesian inference can be performed. Considering the radioelectric\nproperties as a hidden dynamic stochastic process, that evolves in function of\nthe frequency, it is shown how advanced Markov Chain Monte Carlo methods,\ncalled Sequential Monte Carlo (SMC) or interacting particles, can take benefit\nof the structure and provide local EM property estimates.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 16:22:40 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Giraud", "Fran\u00e7ois", ""], ["Minvielle", "Pierre", ""], ["Del Moral", "Pierre", ""]]}, {"id": "1301.4944", "submitter": "Julio Stern", "authors": "Marcelo S. Lauretto, Barbara B. C. Silva and Pablo M. Andrade", "title": "Evaluation of a Supervised Learning Approach for Stock Market Operations", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining methods have been widely applied in financial markets, with the\npurpose of providing suitable tools for prices forecasting and automatic\ntrading. Particularly, learning methods aim to identify patterns in time series\nand, based on such patterns, to recommend buy/sell operations. The objective of\nthis work is to evaluate the performance of Random Forests, a supervised\nlearning method based on ensembles of decision trees, for decision support in\nstock markets. Preliminary results indicate good rates of successful operations\nand good rates of return per operation, providing a strong motivation for\nfurther research in this topic.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 18:17:05 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Lauretto", "Marcelo S.", ""], ["Silva", "Barbara B. C.", ""], ["Andrade", "Pablo M.", ""]]}, {"id": "1301.5007", "submitter": "Francois Roueff", "authors": "Ban Zheng (LTCI, FiQuant), Fran\\c{c}ois Roueff (LTCI), Fr\\'ed\\'eric\n  Abergel (FiQuant, MAS)", "title": "Ergodicity and scaling limit of a constrained multivariate Hawkes\n  process", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multivariate Hawkes process with constraints on its\nconditional density. It is a multivariate point process with conditional\nintensity similar to that of a multivariate Hawkes process but certain events\nare forbidden with respect to boundary conditions on a multidimensional\nconstraint variable, whose evolution is driven by the point process. We study\nthis process in the special case where the fertility function is exponential so\nthat the process is entirely described by an underlying Markov chain, which\nincludes the constraint variable. Some conditions on the parameters are\nestablished to ensure the ergodicity of the chain. Moreover, scaling limits are\nderived for the integrated point process. This study is primarily motivated by\nthe stochastic modelling of a limit order book for high frequency financial\ndata analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 15:15:31 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 19:42:15 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Zheng", "Ban", "", "LTCI, FiQuant"], ["Roueff", "Fran\u00e7ois", "", "LTCI"], ["Abergel", "Fr\u00e9d\u00e9ric", "", "FiQuant, MAS"]]}, {"id": "1301.5129", "submitter": "Audrone Virbickaite", "authors": "Audrone Virbickaite, M. Concepci\\'on Aus\\'in and Pedro Galeano", "title": "A Bayesian Non-Parametric Approach to Asymmetric Dynamic Conditional\n  Correlation Model With Application to Portfolio Selection", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.csda.2014.12.005", "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian non-parametric approach for modeling the distribution\nof multiple returns. In particular, we use an asymmetric dynamic conditional\ncorrelation (ADCC) model to estimate the time-varying correlations of financial\nreturns where the individual volatilities are driven by GJR-GARCH models. The\nADCC-GJR-GARCH model takes into consideration the asymmetries in individual\nassets' volatilities, as well as in the correlations. The errors are modeled\nusing a Dirichlet location-scale mixture of multivariate Gaussian distributions\nallowing for a great flexibility in the return distribution in terms of\nskewness and kurtosis. Model estimation and prediction are developed using MCMC\nmethods based on slice sampling techniques. We carry out a simulation study to\nillustrate the flexibility of the proposed approach. We find that the proposed\nDPM model is able to adapt to several frequently used distribution models and\nalso accurately estimates the posterior distribution of the volatilities of the\nreturns, without assuming any underlying distribution. Finally, we present a\nfinancial application using Apple and NASDAQ Industrial index data to solve a\nportfolio allocation problem. We find that imposing a restrictive parametric\ndistribution can result into underestimation of the portfolio variance, whereas\nDPM model is able to overcome this problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 10:28:08 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2014 16:43:28 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Virbickaite", "Audrone", ""], ["Aus\u00edn", "M. Concepci\u00f3n", ""], ["Galeano", "Pedro", ""]]}, {"id": "1301.5259", "submitter": "Marianna Bolla CSc", "authors": "Marianna Bolla", "title": "SVD, discrepancy, and regular structure of contingency tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will use the factors obtained by correspondence analysis to find\nbiclustering of a contingency table such that the row-column cluster pairs are\nregular, i.e., they have small discrepancy. In our main theorem, the constant\nof the so-called volume-regularity is related to the SVD of the normalized\ncontingency table. Our result is applicable to two-way cuts when both the rows\nand columns are divided into the same number of clusters, thus extending partly\nthe result of Butler estimating the discrepancy of a contingency table by the\nsecond largest singular value of the normalized table (one-cluster, rectangular\ncase), and partly a former result of the author for estimating the constant of\nvolume-regularity by the structural eigenvalues and the distances of the\ncorresponding eigen-subspaces of the normalized modularity matrix of an\nedge-weighted graph (several clusters, symmetric case).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 18:03:43 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Bolla", "Marianna", ""]]}, {"id": "1301.5390", "submitter": "Christopher Paciorek", "authors": "Mariel M. Finucane, Christopher J. Paciorek, Gretchen A. Stevens, and\n  Majid Ezzati", "title": "Semiparametric Bayesian Density Estimation with Disparate Data Sources:\n  A Meta-Analysis of Global Childhood Undernutrition", "comments": "41 total pages, 6 figures, 1 table", "journal-ref": "Journal of the American Statistical Association (2015) 110:\n  889-901", "doi": "10.1080/01621459.2014.937487", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undernutrition, resulting in restricted growth, and quantified here using\nheight-for-age z-scores, is an important contributor to childhood morbidity and\nmortality. Since all levels of mild, moderate and severe undernutrition are of\nclinical and public health importance, it is of interest to estimate the shape\nof the z-scores' distributions.\n  We present a finite normal mixture model that uses data on 4.3 million\nchildren to make annual country-specific estimates of these distributions for\nunder-5-year-old children in the world's 141 low- and middle-income countries\nbetween 1985 and 2011. We incorporate both individual-level data when\navailable, as well as aggregated summary statistics from studies whose\nindividual-level data could not be obtained. We place a hierarchical Bayesian\nprobit stick-breaking model on the mixture weights. The model allows for\nnonlinear changes in time, and it borrows strength in time, in covariates, and\nwithin and across regional country clusters to make estimates where data are\nuncertain, sparse, or missing.\n  This work addresses three important problems that often arise in the fields\nof public health surveillance and global health monitoring. First, data are\nalways incomplete. Second, different data sources commonly use different\nreporting metrics. Last, distributions, and especially their tails, are often\nof substantive interest.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 03:13:54 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 16:22:37 GMT"}, {"version": "v3", "created": "Sun, 29 Jun 2014 00:48:35 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Finucane", "Mariel M.", ""], ["Paciorek", "Christopher J.", ""], ["Stevens", "Gretchen A.", ""], ["Ezzati", "Majid", ""]]}, {"id": "1301.5510", "submitter": "Chris Wallace", "authors": "Chris Wallace", "title": "Statistical testing of shared genetic control for potentially related\n  traits", "comments": "Supplementary information attached", "journal-ref": "Genetic Epidemiology 37 (2013) 802-813", "doi": "10.1002/gepi.21765", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of data from genome-wide single nucleotide polymorphism (SNP)\nassociation studies of different traits should allow researchers to disentangle\nthe genetics of potentially related traits within individually associated\nregions. Formal statistical colocalisation testing of individual regions, which\nrequires selection of a set of SNPs summarizing the association in a region. We\nshow that the SNP selection method greatly affects type 1 error rates, with\npublished studies having used methods expected to result in substantially\ninflated type 1 error rates. We show that either avoiding variable selection\nand instead testing the most informative principal components or integrating\nover variable selection using Bayesian model averaging can lead to correct\ncontrol of type 1 error rates. Application to data from Graves' disease and\nHashimoto's thyroiditis reveals a common genetic signature across seven regions\nshared between the diseases, and indicates that in five of six regions\nassociated with Graves' disease and not Hashimoto's thyroiditis, this more\nlikely reflects genuine absence of association with the latter rather than lack\nof power. Our examination, by simulation, of the performance of colocalisation\ntests and associated software will foster more widespread adoption of formal\ncolocalisation testing. Given the increasing availability of large expression\nand genetic association data sets from disease-relevant tissue and purified\ncell populations, coupled with identification of regulatory sequences by\nprojects such as ENCODE, colocalisation analysis has the potential to reveal\nboth shared genetic signatures of related traits and causal disease genes and\ntissues.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 14:19:35 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 14:44:20 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Wallace", "Chris", ""]]}, {"id": "1301.5701", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, George V. Moustakides, and Xiaodong Wang", "title": "Sequential and Decentralized Estimation of Linear Regression Parameters\n  in Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT math.OC math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential estimation of a vector of linear regression coefficients is\nconsidered under both centralized and decentralized setups. In sequential\nestimation, the number of observations used for estimation is determined by the\nobserved samples, hence is random, as opposed to fixed-sample-size estimation.\nSpecifically, after receiving a new sample, if a target accuracy level is\nreached, we stop and estimate using the samples collected so far; otherwise we\ncontinue to receive another sample. It is known that finding an optimum\nsequential estimator, which minimizes the average sample number for a given\ntarget accuracy level, is an intractable problem with a general stopping rule\nthat depends on the complete observation history. By properly restricting the\nsearch space to stopping rules that depend on a specific subset of the complete\nobservation history, we derive the optimum sequential estimator in the\ncentralized case via optimal stopping theory. However, finding the optimum\nstopping rule in this case requires numerical computations that {\\em\nquadratically} scales with the number of parameters to be estimated. For the\ndecentralized setup with stringent energy constraints, under an alternative\nproblem formulation that is conditional on the observed regressors, we first\nderive a simple optimum scheme whose computational complexity is {\\em constant}\nwith respect to the number of parameters. Then, following this simple optimum\nscheme we propose a decentralized sequential estimator whose computational\ncomplexity and energy consumption scales {\\em linearly} with the number of\nparameters. Specifically, in the proposed decentralized scheme a\nclose-to-optimum average stopping time performance is achieved by infrequently\ntransmitting a single pulse with very short duration.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 05:10:13 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 11:15:30 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2014 22:03:28 GMT"}, {"version": "v4", "created": "Wed, 17 Dec 2014 09:11:05 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Moustakides", "George V.", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1301.5874", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB), Gabriel Peyr\\'e (CEREMADE), Jalal\n  Fadili (GREYC)", "title": "Stein COnsistent Risk Estimator (SCORE) for hard thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we construct a risk estimator for hard thresholding which can\nbe used as a basis to solve the difficult task of automatically selecting the\nthreshold. As hard thresholding is not even continuous, Stein's lemma cannot be\nused to get an unbiased estimator of degrees of freedom, hence of the risk. We\nprove that under a mild condition, our estimator of the degrees of freedom,\nalthough biased, is consistent. Numerical evidence shows that our estimator\noutperforms another biased risk estimator.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 19:29:49 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"], ["Fadili", "Jalal", "", "GREYC"]]}, {"id": "1301.5899", "submitter": "Stephen Tennenbaum", "authors": "Stephen Tennenbaum, Caroline Freitag, and Svetlana Roudenko", "title": "Modeling the Influence of Environment and Intervention on Cholera in\n  Haiti", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple model with two infective classes in order to model the\ncholera epidemic in Haiti. We include the impact of environmental events\n(rainfall, temperature and tidal range) on the epidemic in the Artibonite and\nOuest regions by introducing terms in the transmission rate that vary with\nenvironmental conditions. We fit the model on weekly data from the beginning of\nthe epidemic until December 2013, including the vaccination programs that were\nrecently undertaken in the Ouest and Artibonite regions. We then modified these\nprojections excluding vaccination to assess the programs' effectiveness. Using\nreal-time daily rainfall, we found lag times between precipitation events and\nnew cases that range from 3.4 to 8.4 weeks in Artibonite and 5.1 to 7.4 in\nOuest. In addition, it appears that, in the Ouest region, tidal influences play\na significant role in the dynamics of the disease. Intervention efforts of all\ntypes have reduced case numbers in both regions; however, persistent outbreaks\ncontinue. In Ouest, where the population at risk seems particularly besieged\nand the overall population is larger, vaccination efforts seem to be taking\nhold more slowly than in Artibonite, where a smaller core population was\nvaccinated. The models including the vaccination programs predicted that a year\nand six months later, the mean number of cases in Artibonite would be reduced\nby about two thousand cases, and in Ouest by twenty four hundred cases below\nthat predicted by the models without vaccination. We also found that\nvaccination is best when done in the early spring, and as early as possible in\nthe epidemic. Comparing vaccination between the first spring and the second,\nthere is a drop of about 40% in the case reduction due to the vaccine and about\n10% per year after that.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 17:00:27 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2013 04:12:06 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2013 03:03:40 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2013 17:21:17 GMT"}, {"version": "v5", "created": "Tue, 16 Sep 2014 21:16:46 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Tennenbaum", "Stephen", ""], ["Freitag", "Caroline", ""], ["Roudenko", "Svetlana", ""]]}, {"id": "1301.5948", "submitter": "Michael Inouye", "authors": "Gad Abraham, Jason A. Tye-Din, Oneil G. Bhalala, Adam Kowalczyk,\n  Justin Zobel, and Michael Inouye", "title": "Accurate and robust genomic prediction of celiac disease using\n  statistical learning", "comments": "Main text: 26 pages, 7 figures; Supplementary: 18 pages including\n  genomic prediction model, available at\n  http://dx.doi.org/10.6084/m9.figshare.154193 (alternative link:\n  http://figshare.com/articles/Accurate_and_robust_genomic_prediction_of_celiac_disease_using_statistical_learning/154193)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical application of genomic-based risk stratification to clinical\ndiagnosis is appealing yet performance varies widely depending on the disease\nand genomic risk score (GRS) method. Celiac disease (CD), a common\nimmune-mediated illness, is strongly genetically determined and requires\nspecific HLA haplotypes. HLA testing can exclude diagnosis but has low\nspecificity, providing little information suitable for clinical risk\nstratification. Using six European CD cohorts, we provide a proof-of-concept\nthat statistical learning approaches which simultaneously model all SNPs can\ngenerate robust and highly accurate predictive models based on genome-wide SNP\nprofiles. The high predictive capacity replicated both in cross-validation\nwithin each cohort (AUC of 0.87-0.89) and in independent replication across\ncohorts (AUC of 0.86-0.9), despite differences in ethnicity. The models\nexplained 30-35% of disease variance and up to $\\sim43\\%$ of heritability. The\nGRS's utility was assessed in different clinically relevant settings.\nComparable to HLA typing, the GRS can be used to identify individuals without\nCD with $\\geq99.6\\%$ negative predictive value however, unlike HLA typing,\npatients can also be stratified into categories of higher-risk for CD who would\nbenefit from more invasive and costly definitive testing. The GRS is flexible\nand its performance can be adapted to the clinical situation by adjusting the\nthreshold cut-off. Despite explaining a minority of disease heritability, our\nfindings indicate a predictive GRS provides clinically relevant information to\nimprove upon current diagnostic pathways for CD, and support further studies\nevaluating the clinical utility of this approach in CD and other complex\ndiseases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 02:03:05 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 22:27:02 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2013 23:22:25 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Abraham", "Gad", ""], ["Tye-Din", "Jason A.", ""], ["Bhalala", "Oneil G.", ""], ["Kowalczyk", "Adam", ""], ["Zobel", "Justin", ""], ["Inouye", "Michael", ""]]}, {"id": "1301.6255", "submitter": "Ramanan Subramanian", "authors": "Ramanan Subramanian, Badri Vellambi, and Ingmar Land", "title": "Information Loss due to Finite Block Length in a Gaussian Line Network:\n  An Improved Bound", "comments": "7 pages, 4 figures. To be submitted to ISIT 2013 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bound on the maximum information transmission rate through a cascade of\nGaussian links is presented. The network model consists of a source node\nattempting to send a message drawn from a finite alphabet to a sink, through a\ncascade of Additive White Gaussian Noise links each having an input power\nconstraint. Intermediate nodes are allowed to perform arbitrary\nencoding/decoding operations, but the block length and the encoding rate are\nfixed. The bound presented in this paper is fundamental and depends only on the\ndesign parameters namely, the network size, block length, transmission rate,\nand signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 13:51:03 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Subramanian", "Ramanan", ""], ["Vellambi", "Badri", ""], ["Land", "Ingmar", ""]]}, {"id": "1301.6307", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Xiaodong Wang", "title": "Sequential Distributed Detection in Energy-Constrained Wireless Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2320458", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed sequential distributed detector based on\nlevel-triggered sampling operates as simple as the decision fusion techniques\nand at the same time performs as well as the data fusion techniques. Hence, it\nis well suited for resource-constrained wireless sensor networks. However, in\npractical cases where sensors observe discrete-time signals, the random\novershoot above or below the sampling thresholds considerably degrades the\nperformance of the considered detector. We propose, for systems with stringent\nenergy constraints, a novel approach to tackle this problem by encoding the\novershoot into the time delay between the sampling time and the transmission\ntime. Specifically, each sensor computes the local log-likelihood ratio (LLR)\nand samples it using level-triggered sampling. Then, it transmits a single\npulse to the fusion center (FC) after a transmission delay that is proportional\nto the overshoot, as in pulse position modulation (PPM). The FC, upon receiving\na bit decodes the corresponding overshoot and recovers the transmitted LLR\nvalue. It then updates the approximate global LLR and compares it with two\nthreshold to either make a decision or to continue the sequential process. We\nanalyze the asymptotic average detection delay performance of the proposed\nscheme. We then apply the proposed sequential scheme to target detection in\nwireless sensor networks under the four Swerling fluctuating target models. It\nis seen that the proposed sequential distributed detector offers significant\nperformance advantage over conventional decision fusion techniques.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2013 02:59:40 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2013 18:19:20 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2014 17:09:12 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1301.6635", "submitter": "Jeffrey W. Miller", "authors": "Jeffrey W. Miller, Matthew T. Harrison", "title": "Exact sampling and counting for fixed-margin matrices", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1131 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: text overlap with\n  arXiv:1104.0323", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1569-1592", "doi": "10.1214/13-AOS1131", "report-no": "IMS-AOS-AOS1131", "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uniform distribution on matrices with specified row and column sums is\noften a natural choice of null model when testing for structure in two-way\ntables (binary or nonnegative integer). Due to the difficulty of sampling from\nthis distribution, many approximate methods have been developed. We will show\nthat by exploiting certain symmetries, exact sampling and counting is in fact\npossible in many nontrivial real-world cases. We illustrate with real datasets\nincluding ecological co-occurrence matrices and contingency tables.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 18:33:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2013 11:08:27 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Harrison", "Matthew T.", ""]]}, {"id": "1301.6722", "submitter": "Robert Mislevy", "authors": "Robert Mislevy, Russell Almond, Duanli Yan, Linda S. Steinberg", "title": "Bayes Nets in Educational Assessment: Where Do the Numbers Come From?", "comments": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1999-PG-437-446", "categories": "cs.AI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As observations and student models become complex, educational assessments\nthat exploit advances in technology and cognitive psychology can outstrip\nfamiliar testing models and analytic methods. Within the Portal conceptual\nframework for assessment design, Bayesian inference networks (BINs) record\nbeliefs about students' knowledge and skills, in light of what they say and do.\nJoining evidence model BIN fragments- which contain observable variables and\npointers to student model variables - to the student model allows one to update\nbelief about knowledge and skills as observations arrive. Markov Chain Monte\nCarlo (MCMC) techniques can estimate the required conditional probabilities\nfrom empirical data, supplemented by expert judgment or substantive theory.\nDetails for the special cases of item response theory (IRT) and multivariate\nlatent class modeling are given, with a numerical example of the latter.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 15:59:50 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Mislevy", "Robert", ""], ["Almond", "Russell", ""], ["Yan", "Duanli", ""], ["Steinberg", "Linda S.", ""]]}, {"id": "1301.6737", "submitter": "David A. Schum", "authors": "David A. Schum", "title": "Inference Networks and the Evaluation of Evidence: Alternative Analyses", "comments": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1999-PG-575-584", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference networks have a variety of important uses and are constructed by\npersons having quite different standpoints. Discussed in this paper are three\ndifferent but complementary methods for generating and analyzing probabilistic\ninference networks. The first method, though over eighty years old, is very\nuseful for knowledge representation in the task of constructing probabilistic\narguments. It is also useful as a heuristic device in generating new forms of\nevidence. The other two methods are formally equivalent ways for combining\nprobabilities in the analysis of inference networks. The use of these three\nmethods is illustrated in an analysis of a mass of evidence in a celebrated\nAmerican law case.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 16:00:48 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Schum", "David A.", ""]]}, {"id": "1301.7192", "submitter": "Michael Schreiber", "authors": "Michael Schreiber", "title": "Empirical Evidence for the Relevance of Fractional Scoring in the\n  Calculation of Percentile Rank Scores", "comments": "10 pages, 4 tables, accepted for publication in Journal of American\n  Society for Information Science and Technology", "journal-ref": "Journal of the American Society for Information Science and\n  Technology, 64(4), 861-867 (2013)", "doi": "10.1002/asi.22774", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Fractional scoring has been proposed to avoid inconsistencies in the\nattribution of publications to percentile rank classes. Uncertainties and\nambiguities in the evaluation of percentile ranks can be demonstrated most\neasily with small datasets. But for larger datasets an often large number of\npapers with the same citation count leads to the same uncertainties and\nambiguities which can be avoided by fractional scoring. This is demonstrated\nfor four different empirical datasets with several thousand publications each\nwhich are assigned to 6 percentile rank classes. Only by utilizing fractional\nscoring the total score of all papers exactly reproduces the theoretical value\nin each case.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 10:49:39 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Schreiber", "Michael", ""]]}, {"id": "1301.7400", "submitter": "Charles F. Manski", "authors": "Charles F. Manski", "title": "Treatment Choice in Heterogeneous Populations Using Experiments without\n  Covariate Data (Invited Paper)", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-379-385", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I examine the problem of treatment choice when a planner observes (i)\ncovariates that describe each member of a population of interest and (ii) the\noutcomes of an experiment in which subjects randomly drawn from this population\nare randomly assigned to treatment groups within which all subjects receive the\nsame treatment. Covariate data for the subjects of the experiment are not\navailable. The optimal treatment rule is to divide the population into\nsubpopulations whose members share the same covariate value, and then to choose\nfor each subpopulation a treatment that maximizes its mean outcome. However the\nplanner cannot implement this rule. I draw on my work on nonparametric analysis\nof treatment response to address the planner's problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:05:49 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Manski", "Charles F.", ""]]}, {"id": "1301.7628", "submitter": "Pablo Dorta-Gonz\\'alez", "authors": "Pablo Dorta-Gonz\\'alez and Mar\\'ia Isabel Dorta-Gonz\\'alez", "title": "The student evaluation of teaching and the competence of students as\n  evaluators", "comments": "20 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the college student satisfaction survey is considered in the promotion\nand recognition of instructors, a usual complaint is related to the impact that\nbiased ratings have on the arithmetic mean (used as a measure of teaching\neffectiveness). This is especially significant when the number of students\nresponding to the survey is small. In this work a new methodology, considering\nstudent to student perceptions, is presented. Two different estimators of\nstudent rating credibility, based on centrality properties of the student\nsocial network, are proposed. This method is established on the idea that in\nthe case of on-site higher education, students often know which others are\ncompetent in rating the teaching and learning process.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 14:37:20 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Dorta-Gonz\u00e1lez", "Pablo", ""], ["Dorta-Gonz\u00e1lez", "Mar\u00eda Isabel", ""]]}, {"id": "1301.7720", "submitter": "Lucas Tcheuko Mr", "authors": "Lucas Tcheuko and Frank Samuelson", "title": "Properties of the Nonparametric Maximum Likelihood {ROC} Model with a\n  Monotonic Likelihood Ratio", "comments": "Journal of Mathematical Psycholoy 2013", "journal-ref": null, "doi": null, "report-no": "3017962665", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expect that some observers in perceptual signal detection experiments,\nsuch as radiologists, will make rational decisions, and therefore ratings from\nthose observers are expected to form a convex ROC curve. However, measured and\npublished curves are often not convex. This article examines the\nconvexity-constrained nonparametric maximum likelihood estimator of the ROC\ncurve given by Lloyd (2002). Like Lloyd we use the Pool Adjacent Violator\nAlgorithm (PAVA) to construct the estimate of the convex curve. We present a\ndirect proof that this estimate is a convex hull of the empirical ROC curve.\nThe estimate is simple to construct by hand, and follows the suggestions by\nPesce, et~al.~(2010).\n  We examine the properties of this constrained nonparametric maximum\nlikelihood estimator (NPMLE) under a large number of experimental conditions.\nIn particular we examine the behavior of the area under the curve which is\noften used as summary metric of diagnostic performance. This constrained ROC\nestimator gives an area under the curve (AUC) estimate that is biased high with\nrespect to the usual empirical AUC estimate, but may be less biased with\nrespect to the underlying continuous true AUC value. The constrained ROC\nestimator has lower variance than the usual empirical one. Unlike previous\nauthors who used complex bootstrapping to estimate the variance of the\nconstrained NPMLE we demonstrate that standard unbiased estimators of variance\nwork well to estimate the variance of the NPMLE AUC.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 19:12:30 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Tcheuko", "Lucas", ""], ["Samuelson", "Frank", ""]]}]