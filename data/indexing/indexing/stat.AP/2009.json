[{"id": "2009.00001", "submitter": "Victoria Lin", "authors": "Victoria Lin, Jeffrey M. Girard, Michael A. Sayette, Louis-Philippe\n  Morency", "title": "Toward Multimodal Modeling of Emotional Expressiveness", "comments": "V. Lin and J.M. Girard contributed equally to this research. This\n  paper was accepted to ICMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional expressiveness captures the extent to which a person tends to\noutwardly display their emotions through behavior. Due to the close\nrelationship between emotional expressiveness and behavioral health, as well as\nthe crucial role that it plays in social interaction, the ability to\nautomatically predict emotional expressiveness stands to spur advances in\nscience, medicine, and industry. In this paper, we explore three related\nresearch questions. First, how well can emotional expressiveness be predicted\nfrom visual, linguistic, and multimodal behavioral signals? Second, which\nbehavioral modalities are uniquely important to the prediction of emotional\nexpressiveness? Third, which behavioral signals are reliably related to\nemotional expressiveness? To answer these questions, we add highly reliable\ntranscripts and human ratings of perceived emotional expressiveness to an\nexisting video database and use this data to train, validate, and test\npredictive models. Our best model shows promising predictive performance on\nthis dataset (RMSE=0.65, R^2=0.45, r=0.74). Multimodal models tend to perform\nbest overall, and models trained on the linguistic modality tend to outperform\nmodels trained on the visual modality. Finally, examination of our\ninterpretable models' coefficients reveals a number of visual and linguistic\nbehavioral signals--such as facial action unit intensity, overall word count,\nand use of words related to social processes--that reliably predict emotional\nexpressiveness.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:17:26 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lin", "Victoria", ""], ["Girard", "Jeffrey M.", ""], ["Sayette", "Michael A.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2009.00065", "submitter": "Elizabeth Handorf", "authors": "Elizabeth Handorf, Yinuo Yin, Michael Slifker, Shannon Lynch", "title": "Variable selection in social-environmental data: Sparse regression and\n  tree ensemble machine learning approaches", "comments": "22 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Social-environmental data obtained from the U.S. Census is an\nimportant resource for understanding health disparities, but rarely is the full\ndataset utilized for analysis. A barrier to incorporating the full data is a\nlack of solid recommendations for variable selection, with researchers often\nhand-selecting a few variables. Thus, we evaluated the ability of empirical\nmachine learning approaches to identify social-environmental factors having a\ntrue association with a health outcome.\n  Materials and Methods: We compared several popular machine learning methods,\nincluding penalized regressions (e.g. lasso, elastic net), and tree ensemble\nmethods. Via simulation, we assessed the methods' ability to identify census\nvariables truly associated with binary and continuous outcomes while minimizing\nfalse positive results (10 true associations, 1,000 total variables). We\napplied the most promising method to the full census data (p=14,663 variables)\nlinked to prostate cancer registry data (n=76,186 cases) to identify\nsocial-environmental factors associated with advanced prostate cancer.\n  Results: In simulations, we found that elastic net identified many\ntrue-positive variables, while lasso provided good control of false positives.\nUsing a combined measure of accuracy, hierarchical clustering based on\nSpearman's correlation with sparse group lasso regression performed the best\noverall. Bayesian Adaptive Regression Trees outperformed other tree ensemble\nmethods, but not the sparse group lasso. In the full dataset, the sparse group\nlasso successfully identified a subset of variables, three of which replicated\nearlier findings.\n  Discussion: This analysis demonstrated the potential of empirical machine\nlearning approaches to identify a small subset of census variables having a\ntrue association with the outcome, and that replicate across empiric methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:04:34 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Handorf", "Elizabeth", ""], ["Yin", "Yinuo", ""], ["Slifker", "Michael", ""], ["Lynch", "Shannon", ""]]}, {"id": "2009.00148", "submitter": "Jinglong Zhao", "authors": "Iavor Bojinov, David Simchi-Levi, Jinglong Zhao", "title": "Design and Analysis of Switchback Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In switchback experiments, a firm sequentially exposes an experimental unit\nto a random treatment, measures its response, and repeats the procedure for\nseveral periods to determine which treatment leads to the best outcome.\nAlthough practitioners have widely adopted this experimental design technique,\nthe development of its theoretical properties and the derivation of optimal\ndesign procedures have been, to the best of our knowledge, elusive. In this\npaper, we address these limitations by establishing the necessary results to\nensure that practitioners can apply this powerful class of experiments with\nminimal assumptions. Our main result is the derivation of the optimal design of\nswitchback experiments under a range of different assumptions on the order of\ncarryover effect - that is, the length of time a treatment persists in\nimpacting the outcome. We cast the experimental design problem as a minimax\ndiscrete robust optimization problem, identify the worst-case adversarial\nstrategy, establish structural results for the optimal design, and finally\nsolve the problem via a continuous relaxation. For the optimal design, we\nderive two approaches for performing inference after running the experiment.\nThe first provides exact randomization based $p$-values and the second uses a\nfinite population central limit theorem to conduct conservative hypothesis\ntests and build confidence intervals. We further provide theoretical results\nfor our inferential procedures when the order of the carryover effect is\nmisspecified. For firms that possess the capability to run multiple switchback\nexperiments, we also provide a data-driven strategy to identify the likely\norder of carryover effect. To study the empirical properties of our results, we\nconduct extensive simulations. We conclude the paper by providing some\npractical suggestions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:40:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:00:49 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bojinov", "Iavor", ""], ["Simchi-Levi", "David", ""], ["Zhao", "Jinglong", ""]]}, {"id": "2009.00149", "submitter": "Timo Bolkart", "authors": "Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael\n  Black, Timo Bolkart", "title": "GIF: Generative Interpretable Faces", "comments": "International Conference on 3D Vision (3DV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic visualization and animation of expressive human faces have\nbeen a long standing challenge. 3D face modeling methods provide parametric\ncontrol but generates unrealistic images, on the other hand, generative 2D\nmodels like GANs (Generative Adversarial Networks) output photo-realistic face\nimages, but lack explicit control. Recent methods gain partial control, either\nby attempting to disentangle different factors in an unsupervised manner, or by\nadding control post hoc to a pre-trained model. Unconditional GANs, however,\nmay entangle factors that are hard to undo later. We condition our generative\nmodel on pre-defined control parameters to encourage disentanglement in the\ngeneration process. Specifically, we condition StyleGAN2 on FLAME, a generative\n3D face model. While conditioning on FLAME parameters yields unsatisfactory\nresults, we find that conditioning on rendered FLAME geometry and photometric\ndetails works well. This gives us a generative 2D face model named GIF\n(Generative Interpretable Faces) that offers FLAME's parametric control. Here,\ninterpretable refers to the semantic meaning of different parameters. Given\nFLAME parameters for shape, pose, expressions, parameters for appearance,\nlighting, and an additional style vector, GIF outputs photo-realistic face\nimages. We perform an AMT based perceptual study to quantitatively and\nqualitatively evaluate how well GIF follows its conditioning. The code, data,\nand trained model are publicly available for research purposes at\nhttp://gif.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:40:26 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 13:37:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ghosh", "Partha", ""], ["Gupta", "Pravir Singh", ""], ["Uziel", "Roy", ""], ["Ranjan", "Anurag", ""], ["Black", "Michael", ""], ["Bolkart", "Timo", ""]]}, {"id": "2009.00401", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe", "title": "Time-Varying Parameters as Ridge Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying parameters (TVPs) models are frequently used in economics to\nmodel structural change. I show that they are in fact ridge regressions.\nInstantly, this makes computations, tuning, and implementation much easier than\nin the state-space paradigm. Among other things, solving the equivalent dual\nridge problem is computationally very fast even in high dimensions, and the\ncrucial \"amount of time variation\" is tuned by cross-validation. Evolving\nvolatility is dealt with using a two-step ridge regression. I consider\nextensions that incorporate sparsity (the algorithm selects which parameters\nvary and which do not) and reduced-rank restrictions (variation is tied to a\nfactor model). To demonstrate the usefulness of the approach, I use it to study\nthe evolution of monetary policy in Canada. The application requires the\nestimation of about 4600 TVPs, a task well within the reach of the new method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:07:04 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 20:07:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Coulombe", "Philippe Goulet", ""]]}, {"id": "2009.00503", "submitter": "Sara Algeri", "authors": "Sara Algeri", "title": "Informative Goodness-of-Fit for Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces an informative goodness-of-fit (iGOF) approach to\nstudy multivariate distributions. When the null model is rejected, iGOF allows\nus to identify the underlying sources of mismodeling and naturally equips\npractitioners with additional insights on the nature of the deviations from the\ntrue distribution. The informative character of the procedure is achieved by\nexploiting smooth tests and random fields theory to facilitate the analysis of\nmultivariate data. Simulation studies show that iGOF enjoys high power for\ndifferent types of alternatives. The methods presented here directly address\nthe problem of background mismodeling arising in physics and astronomy. It is\nin these areas that the motivation of this work is rooted.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:04:30 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 15:13:17 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 16:19:56 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 21:38:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Algeri", "Sara", ""]]}, {"id": "2009.00536", "submitter": "Emmanuel Kidando", "authors": "Emmanuel Kidando, Angela E. Kitali, Boniphace Kutela, Thobias Sando", "title": "A Comparative Study of Parametric Regression Models to Detect Breakpoint\n  in Traffic Fundamental Diagram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A speed threshold is a crucial parameter in breakdown and capacity\ndistribution analysis as it defines the boundary between free-flow and\ncongested regimes. However, literature on approaches to establishing the\nbreakpoint value for detecting breakdown events is limited. Most of existing\nstudies rely on the use of either visual observation or predefined thresholds.\nThese approaches may not be reliable considering the variations associated with\nfield data. Thus, this study compared the performance of two data-driven\nmethods, that is, logistic function (LGF) and two-regime models, used to\nestablish the breakpoint from traffic flow variables. The two models were\ncalibrated using urban freeway traffic data. The models'performance results\nrevealed that with less computation efforts, the LGF has slightly better\nprediction accuracy than the two-regime model. Although the two-regime model\nhad relatively lower performance, it can be useful in identifying the\ntransitional state.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:10:42 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Kidando", "Emmanuel", ""], ["Kitali", "Angela E.", ""], ["Kutela", "Boniphace", ""], ["Sando", "Thobias", ""]]}, {"id": "2009.00558", "submitter": "Jens Braband", "authors": "Hendrik Sch\\\"abe and Jens Braband", "title": "Application of the Cox Regression Model for Analysis of Railway Safety\n  Performance", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of in-service safety performance is an important task, not\nonly in railways. For example it is important to identify deviations early, in\nparticular possible deterioration of safety performance, so that corrective\nactions can be applied early. On the other hand the assessment should be fair\nand objective and rely on sound and proven statistical methods. A popular means\nfor this task is trend analysis. This paper defines a model for trend analysis\nand compares different approaches, e. g. classical and Bayes approaches, on\nreal data. The examples show that in particular for small sample sizes, e. g.\nwhen railway operators shall be assessed, the Bayesian prior may influence the\nresults significantly.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:43:07 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sch\u00e4be", "Hendrik", ""], ["Braband", "Jens", ""]]}, {"id": "2009.00729", "submitter": "Sina Khatami", "authors": "Sina Khatami, Timothy John Peterson, Murray C Peel, Andrew Western", "title": "Evaluating Catchment Models as Multiple Working Hypotheses: on the Role\n  of Error Metrics, Parameter Sampling, Model Structure, and Data Information\n  Content", "comments": null, "journal-ref": null, "doi": "10.1002/essoar.10504066.1", "report-no": null, "categories": "stat.ME physics.data-an physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To evaluate models as hypotheses, we developed the method of Flux Mapping to\nconstruct a hypothesis space based on dominant runoff generating mechanisms.\nAcceptable model runs, defined as total simulated flow with similar (and\nminimal) model error, are mapped to the hypothesis space given their simulated\nrunoff components. In each modeling case, the hypothesis space is the result of\nan interplay of factors: model structure and parameterization, chosen error\nmetric, and data information content. The aim of this study is to disentangle\nthe role of each factor in model evaluation. We used two model structures\n(SACRAMENTO and SIMHYD), two parameter sampling approaches (Latin Hypercube\nSampling of the parameter space and guided-search of the solution space), three\nwidely used error metrics (Nash-Sutcliffe Efficiency - NSE, Kling-Gupta\nEfficiency skill score - KGEss, and Willmott refined Index of Agreement - WIA),\nand hydrological data from a large sample of Australian catchments. First, we\ncharacterized how the three error metrics behave under different error types\nand magnitudes independent of any modeling. We then conducted a series of\ncontrolled experiments to unpack the role of each factor in runoff generation\nhypotheses. We show that KGEss is a more reliable metric compared to NSE and\nWIA for model evaluation. We further demonstrate that only changing the error\nmetric -- while other factors remain constant -- can change the model solution\nspace and hence vary model performance, parameter sampling sufficiency, and or\nthe flux map. We show how unreliable error metrics and insufficient parameter\nsampling impair model-based inferences, particularly runoff generation\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 22:24:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Khatami", "Sina", ""], ["Peterson", "Timothy John", ""], ["Peel", "Murray C", ""], ["Western", "Andrew", ""]]}, {"id": "2009.00785", "submitter": "Elizabeth Handorf", "authors": "Elizabeth A. Handorf, Marc Smaldone, Sujana Movva, Nandita Mitra", "title": "Analysis of survival data with non-proportional hazards: A comparison of\n  propensity score weighted methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common ways researchers compare survival outcomes across\ntreatments when confounding is present is using Cox regression. This model is\nlimited by its underlying assumption of proportional hazards; in some cases,\nsubstantial violations may occur. Here we present and compare approaches which\nattempt to address this issue, including Cox models with time-varying hazard\nratios; parametric accelerated failure time models; Kaplan-Meier curves; and\npseudo-observations. To adjust for differences between treatment groups, we use\nInverse Probability of Treatment Weighting based on the propensity score. We\nexamine clinically meaningful outcome measures that can be computed and\ndirectly compared across each method, namely, survival probability at time T,\nmedian survival, and restricted mean survival. We conduct simulation studies\nunder a range of scenarios, and determine the biases, coverages, and standard\nerrors of the Average Treatment Effects for each method. We then apply these\napproaches to two published observational studies of survival after cancer\ntreatment. The first examines chemotherapy in sarcoma, where survival is very\nsimilar initially, but after two years the chemotherapy group shows a benefit.\nThe other study is a comparison of surgical techniques for kidney cancer, where\nsurvival differences are attenuated over time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:08:49 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:46:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Handorf", "Elizabeth A.", ""], ["Smaldone", "Marc", ""], ["Movva", "Sujana", ""], ["Mitra", "Nandita", ""]]}, {"id": "2009.01099", "submitter": "Le Bao", "authors": "Zhou Lan and Le Bao", "title": "A Joint Spatial Conditional Auto-Regressive Model for Estimating HIV\n  Prevalence Rates Among Key Populations", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ending the HIV/AIDS pandemic is among the Sustainable Development Goals for\nthe next decade. In order to overcome the gap between the need for care and the\navailable resources, better understanding of HIV epidemics is needed to guide\npolicy decisions, especially for key populations that are at higher risk for\nHIV infection. Accurate HIV epidemic estimates for key populations have been\ndifficult to obtain because their HIV surveillance data is very limited. In\nthis paper, we propose a so-called joint spatial conditional auto-regressive\nmodel for estimating HIV prevalence rates among key populations. Our model\nborrows information from both neighboring locations and dependent populations.\nAs illustrated in the real data analysis, it provides more accurate estimates\nthan independently fitting the sub-epidemic for each key population. In\naddition, we provide a study to reveal the conditions that our proposal gives a\nbetter prediction. The study combines both theoretical investigation and\nnumerical study, revealing strength and limitations of our proposal.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:20:10 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Lan", "Zhou", ""], ["Bao", "Le", ""]]}, {"id": "2009.01119", "submitter": "Tilo Wiklund", "authors": "Juozas Vaicenavicius and Tilo Wiklund and Aust\\.e Grigait\\.e and\n  Antanas Kalkauskas and Ignas Vysniauskas and Steven Keen", "title": "Self-driving car safety quantification via component-level analysis", "comments": "Various minor linguistic, typographical, and notational improvements.\n  To appear in the SAE International Journal of Connected and Automated\n  Vehicles, 4(1):2021, doi:10.4271/12-04-01-0004", "journal-ref": "SAE Intl. J CAV 4(1):35-45, 2021", "doi": "10.4271/12-04-01-0004", "report-no": null, "categories": "stat.AP cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a rigorous modular statistical approach for arguing\nsafety or its insufficiency of an autonomous vehicle through a concrete\nillustrative example. The methodology relies on making appropriate quantitative\nstudies of the performance of constituent components. We explain the importance\nof sufficient and necessary conditions at the component level for the overall\nsafety of the vehicle as well as the cost-saving benefits of the approach. A\nsimple concrete automated braking example studied illustrates how separate\nperception system and operational design domain statistical analyses can be\nused to prove or disprove safety at the vehicle level.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:51:19 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 11:27:11 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 12:08:48 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 14:52:46 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Vaicenavicius", "Juozas", ""], ["Wiklund", "Tilo", ""], ["Grigait\u0117", "Aust\u0117", ""], ["Kalkauskas", "Antanas", ""], ["Vysniauskas", "Ignas", ""], ["Keen", "Steven", ""]]}, {"id": "2009.01147", "submitter": "Arnald Puy", "authors": "Arnald Puy, William Becker, Samuele Lo Piano, Andrea Saltelli", "title": "A comprehensive comparison of total-order estimators for global\n  sensitivity analysis", "comments": "Previous versions of this manuscript were titled \"The Battle of\n  Total-Order Sensitivity Indices\". The version with the title \"A comprehensive\n  comparison of total-order estimators for global sensitivity analysis\" has\n  been accepted for publication in the International Journal for Uncertainty\n  Quantification. Please kindly cite the journal version", "journal-ref": null, "doi": "10.1615/Int.J.UncertaintyQuantification.2021038133", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sensitivity analysis helps identify which model inputs convey the most\nuncertainty to the model output. One of the most authoritative measures in\nglobal sensitivity analysis is the Sobol' total-order index, which can be\ncomputed with several different estimators. Although previous comparisons\nexist, it is hard to know which estimator performs best since the results are\ncontingent on the benchmark setting defined by the analyst (the sampling\nmethod, the distribution of the model inputs, the number of model runs, the\ntest function or model and its dimensionality, the weight of higher order\neffects or the performance measure selected). Here we compare several\ntotal-order estimators in an eight-dimension hypercube where these benchmark\nparameters are treated as random parameters. This arrangement significantly\nrelaxes the dependency of the results on the benchmark design. We observe that\nthe most accurate estimators are Razavi and Gupta's, Jansen's or Janon/Monod's\nfor factor prioritization, and Jansen's, Janon/Monod's or Azzini and Rosati's\nfor approaching the \"true\" total-order indices. The rest lag considerably\nbehind. Our work helps analysts navigate the myriad of total-order formulae by\nreducing the uncertainty in the selection of the most appropriate estimator.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:48:11 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 12:14:33 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 10:35:37 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 17:26:02 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Puy", "Arnald", ""], ["Becker", "William", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "2009.01296", "submitter": "Manjunath B G", "authors": "Barry C. Arnold and B.G. Manjunath", "title": "Statistical Inference for distributions with one Poisson conditional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It will be recalled that the classical bivariate normal distributions have\nnormal marginals and normal conditionals. It is natural to ask whether a\nsimilar phenomenon can be encountered involving Poisson marginals and\nconditionals. Reference to Arnold, Castillo and Sarabia's (1999) book on\nconditionally specified models will confirm that Poisson marginals will be\nencountered, together with both conditionals being of the Poisson form, only in\nthe case in which the variables are independent. Instead, in the present\narticle we will be focusing on bivariate distributions with one marginal and\nthe other family of conditionals being of the Poisson form. Such distributions\nare called Pseudo-Poisson distributions. We discuss distributional features of\nsuch models, explore inferential aspects and include an example of applications\nof the Pseudo-Poisson model to sets of over-dispersed data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:49:54 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Arnold", "Barry C.", ""], ["Manjunath", "B. G.", ""]]}, {"id": "2009.01595", "submitter": "Xiuqin Xu", "authors": "Xiuqin Xu, Ying Chen, Yannig Goude, Qiwei Yao", "title": "Probabilistic Forecasting for Daily Electricity Loads and Quantiles for\n  Curve-to-Curve Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting of electricity load curves is of fundamental\nimportance for effective scheduling and decision making in the increasingly\nvolatile and competitive energy markets. We propose a novel approach to\nconstruct probabilistic predictors for curves (PPC), which leads to a natural\nand new definition of quantiles in the context of curve-to-curve linear\nregression. There are three types of PPC: a predictive set, a predictive band\nand a predictive quantile, all of which are defined at a pre-specified nominal\nprobability level. In the simulation study, the PPC achieve promising coverage\nprobabilities under a variety of data generating mechanisms. When applying to\none day ahead forecasting for the French daily electricity load curves, PPC\noutperform several state-of-the-art predictive methods in terms of forecasting\naccuracy, coverage rate and average length of the predictive bands. The\npredictive quantile curves provide insightful information which is highly\nrelevant to hedging risks in electricity supply management.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:00:20 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 02:57:17 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Xu", "Xiuqin", ""], ["Chen", "Ying", ""], ["Goude", "Yannig", ""], ["Yao", "Qiwei", ""]]}, {"id": "2009.02099", "submitter": "Yudi Pawitan", "authors": "Yudi Pawitan", "title": "Defending the P-value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attacks on the P-value are nothing new, but the recent attacks are\nincreasingly more serious. They come from more mainstream sources, with\nwidening targets such as a call to retire the significance testing altogether.\nWhile well meaning, I believe these attacks are nevertheless misdirected:\nBlaming the P-value for the naturally tentative trial-and-error process of\nscientific discoveries, and presuming that banning the P-value would make the\nprocess cleaner and less error-prone. However tentative, the skeptical\nscientists still have to form unambiguous opinions, proximately to move forward\nin their investigations and ultimately to present results to the wider\ncommunity. With obvious reasons, they constantly need to balance between the\nfalse-positive and false-negative errors. How would banning the P-value or\nsignificance tests help in this balancing act? It seems trite to say that this\nbalance will always depend on the relative costs or the trade-off between the\nerrors. These costs are highly context specific, varying by area of\napplications or by stage of investigation. A calibrated but tunable knob, such\nas that given by the P-value, is needed for controlling this balance. This\npaper presents detailed arguments in support of the P-value.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 10:29:07 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Pawitan", "Yudi", ""]]}, {"id": "2009.02136", "submitter": "Kelly Van Lancker", "authors": "Kelly Van Lancker, An Vandebosch and Stijn Vansteelandt", "title": "Efficient, Doubly Robust Estimation of the Effect of Dose Switching for\n  Switchers in a Randomised Clinical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a clinical trial conducted by Janssen Pharmaceuticals in which a\nflexible dosing regimen is compared to placebo, we evaluate how switchers in\nthe treatment arm (i.e., patients who were switched to the higher dose) would\nhave fared had they been kept on the low dose. This in order to understand\nwhether flexible dosing is potentially beneficial for them. Simply comparing\nthese patients' responses with those of patients who stayed on the low dose is\nunsatisfactory because the latter patients are usually in a better health\ncondition. Because the available information in the considered trial is too\nscarce to enable a reliable adjustment, we will instead transport data from a\nfixed dosing trial that has been conducted concurrently on the same target,\nalbeit not in an identical patient population. In particular, we will propose\nan estimator which relies on an outcome model and a propensity score model for\nthe association between study and patient characteristics. The proposed\nestimator is asymptotically unbiased if at least one of both models is\ncorrectly specified, and efficient (under the model defined by the restrictions\non the propensity score) when both models are correctly specified. We show that\nthe proposed method for using results from an external study is generically\napplicable in studies where a classical confounding adjustment is not possible\ndue to positivity violation (e.g., studies where switching takes place in a\ndeterministic manner). Monte Carlo simulations and application to the\nmotivating study demonstrate adequate performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 12:26:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Van Lancker", "Kelly", ""], ["Vandebosch", "An", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2009.02152", "submitter": "Xiaoqi Zhang", "authors": "Xiaoqi Zhang, Zheng Ji, Yanqiao Zheng, Xinyue Ye, Dong Li", "title": "Evaluating the effect of city lock-down on controlling COVID-19\n  propagation through deep learning and network science models", "comments": "27 pages, 9 figures", "journal-ref": "[J]. Cities, 2020: 102869", "doi": "10.1016/j.cities.2020.102869", "report-no": null, "categories": "q-bio.PE physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The special epistemic characteristics of the COVID-19, such as the long\nincubation period and the infection through asymptomatic cases, put severe\nchallenge to the containment of its outbreak. By the end of March 2020, China\nhas successfully controlled the within-spreading of COVID-19 at a high cost of\nlocking down most of its major cities, including the epicenter, Wuhan. Since\nthe low accuracy of outbreak data before the mid of Feb. 2020 forms a major\ntechnical concern on those studies based on statistic inference from the early\noutbreak. We apply the supervised learning techniques to identify and train\nNP-Net-SIR model which turns out robust under poor data quality condition. By\nthe trained model parameters, we analyze the connection between population flow\nand the cross-regional infection connection strength, based on which a set of\ncounterfactual analysis is carried out to study the necessity of lock-down and\nsubstitutability between lock-down and the other containment measures. Our\nfindings support the existence of non-lock-down-typed measures that can reach\nthe same containment consequence as the lock-down, and provide useful guideline\nfor the design of a more flexible containment strategy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 12:39:12 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Zhang", "Xiaoqi", ""], ["Ji", "Zheng", ""], ["Zheng", "Yanqiao", ""], ["Ye", "Xinyue", ""], ["Li", "Dong", ""]]}, {"id": "2009.02198", "submitter": "Marc-Oliver Pohle", "authors": "Uwe Hassler and Marc-Oliver Pohle", "title": "Unlucky Number 13? Manipulating Evidence Subject to Snooping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questionable research practices like HARKing or p-hacking have generated\nconsiderable recent interest throughout and beyond the scientific community. We\nsubsume such practices involving secret data snooping that influences\nsubsequent statistical inference under the term MESSing (manipulating evidence\nsubject to snooping) and discuss, illustrate and quantify the possibly dramatic\neffects of several forms of MESSing using an empirical and a simple theoretical\nexample. The empirical example uses numbers from the most popular German\nlottery, which seem to suggest that 13 is an unlucky number.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:55:37 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hassler", "Uwe", ""], ["Pohle", "Marc-Oliver", ""]]}, {"id": "2009.02307", "submitter": "Lorenzo Cappello", "authors": "Lorenzo Cappello and Julia A. Palacios", "title": "Adaptive preferential sampling in phylodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal molecular data of rapidly evolving viruses and pathogens provide\ninformation about disease spread and complement traditional surveillance\napproaches based on case count data. The coalescent is used to model the\ngenealogy that represents the sample ancestral relationships. The basic\nassumption is that coalescent events occur at a rate inversely proportional to\nthe effective population size $N_{e}(t)$, a time-varying measure of genetic\ndiversity. When the sampling process (collection of samples over time) depends\non $N_{e}(t)$, the coalescent and the sampling processes can be jointly modeled\nto improve estimation of $N_{e}(t)$. Failing to do so can lead to bias due to\nmodel misspecification. However, the way that the sampling process depends on\nthe effective population size may vary over time. We introduce an approach\nwhere the sampling process is modeled as an inhomogeneous Poisson process with\nrate equal to the product of $N_{e}(t)$ and a time-varying coefficient, making\nminimal assumptions on their functional shapes via Markov random field priors.\nWe provide scalable algorithms for inference, show the model performance\nvis-a-vis alternative methods in a simulation study, and apply our model to\nSARS-CoV-2 sequences from Los Angeles and Santa Clara counties. The methodology\nis implemented and available in the R package adapref.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 17:14:50 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Cappello", "Lorenzo", ""], ["Palacios", "Julia A.", ""]]}, {"id": "2009.02362", "submitter": "Nirvik Sinha", "authors": "Nirvik Sinha", "title": "Bootstrap p-values reduce type 1 error of the robust rank-order test of\n  difference in medians", "comments": "22 pages, 1 table, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The robust rank-order test (Fligner and Policello, 1981) was designed as an\nimprovement of the non-parametric Wilcoxon-Mann-Whitney U-test to be more\nappropriate when the samples being compared have unequal variance. However, it\ntends to be excessively liberal when the samples are asymmetric. This is likely\nbecause the test statistic is assumed to have a standard normal distribution\nfor sample sizes > 12. This work proposes an on-the-fly method to obtain the\ndistribution of the test statistic from which the critical/p-value may be\ncomputed directly. The method of likelihood maximization is used to estimate\nthe parameters of the parent distributions of the samples being compared. Using\nthese estimated populations, the null distribution of the test statistic is\nobtained by the Monte-Carlo method. Simulations are performed to compare the\nproposed method with that of standard normal approximation of the test\nstatistic. For small sample sizes (<= 20), the Monte-Carlo method outperforms\nthe normal approximation method. This is especially true for low values of\nsignificance levels (< 5%). Additionally, when the smaller sample has the\nlarger standard deviation, the Monte-Carlo method outperforms the normal\napproximation method even for large sample sizes (= 40/60). The two methods do\nnot differ in power. Finally, a Monte-Carlo sample size of 10^4 is found to be\nsufficient to obtain the aforementioned relative improvements in performance.\nThus, the results of this study pave the way for development of a toolbox to\nperform the robust rank-order test in a distribution-free manner.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:59:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sinha", "Nirvik", ""]]}, {"id": "2009.02372", "submitter": "Le Bao", "authors": "Jacob Parsons, Xiaoyue Niu, Le Bao", "title": "Evaluating the relative contribution of data sources in a Bayesian\n  analysis with the application of estimating the size of hard to reach\n  populations", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using multiple data sources in an analysis, it is important to\nunderstand the influence of each data source on the analysis and the\nconsistency of the data sources with each other and the model. We suggest the\nuse of a retrospective value of information framework in order to address such\nconcerns. Value of information methods can be computationally difficult. We\nillustrate the use of computational methods that allow these methods to be\napplied even in relatively complicated settings.\n  In illustrating the proposed methods, we focus on an application in\nestimating the size of hard to reach populations. Specifically, we consider\nestimating the number of injection drug users in Ukraine by combining all\navailable data sources spanning over half a decade and numerous sub-national\nareas in the Ukraine. This application is of interest to public health\nresearchers as this hard to reach population that plays a large role in the\nspread of HIV. We apply a Bayesian hierarchical model and evaluate the\ncontribution of each data source in terms of absolute influence, expected\ninfluence, and level of surprise. Finally we apply value of information methods\nto inform suggestions on future data collection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 19:29:10 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Parsons", "Jacob", ""], ["Niu", "Xiaoyue", ""], ["Bao", "Le", ""]]}, {"id": "2009.02528", "submitter": "Wei Chen", "authors": "Wei Chen, Jiusun Zeng, Xiaobin Xu, Shihua Luo, Chuanhou Gao", "title": "Structured Sparsity Modeling for Improved Multivariate Statistical\n  Analysis based Fault Isolation", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the fault diagnosis capability of multivariate\nstatistical methods, this article introduces a fault isolation framework based\non structured sparsity modeling. The developed method relies on the\nreconstruction based contribution analysis and the process structure\ninformation can be incorporated into the reconstruction objective function in\nthe form of structured sparsity regularization terms. The structured sparsity\nterms allow selection of fault variables over structures like blocks or\nnetworks of process variables, hence more accurate fault isolation can be\nachieved. Four structured sparsity terms corresponding to different kinds of\nprocess information are considered, namely, partially known sparse support,\nblock sparsity, clustered sparsity and tree-structured sparsity. The\noptimization problems involving the structured sparsity terms can be solved\nusing the Alternating Direction Method of Multipliers (ADMM) algorithm, which\nis fast and efficient. Through a simulation example and an application study to\na coal-fired power plant, it is verified that the proposed method can better\nisolate faulty variables by incorporating process structure information.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 13:03:52 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 05:05:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Wei", ""], ["Zeng", "Jiusun", ""], ["Xu", "Xiaobin", ""], ["Luo", "Shihua", ""], ["Gao", "Chuanhou", ""]]}, {"id": "2009.02597", "submitter": "Kun Chen", "authors": "Wenjie Wang, Chongliang Luo, Robert H. Aseltine, Fei Wang, Jun Yan,\n  Kun Chen", "title": "Suicide Risk Modeling with Uncertain Diagnostic Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the pressing need for suicide prevention through improving\nbehavioral healthcare, we use medical claims data to study the risk of\nsubsequent suicide attempts for patients who were hospitalized due to suicide\nattempts and later discharged. Understanding the risk behaviors of such\npatients at elevated suicide risk is an important step towards the goal of\n\"Zero Suicide\". An immediate and unconventional challenge is that the\nidentification of suicide attempts from medical claims contains substantial\nuncertainty: almost 20\\% of \"suspected\" suicide attempts are identified from\ndiagnostic codes indicating external causes of injury and poisoning with\nundermined intent. It is thus of great interest to learn which of these\nundetermined events are more likely actual suicide attempts and how to properly\nutilize them in survival analysis with severe censoring. To tackle these\ninterrelated problems, we develop an integrative Cox cure model with\nregularization to perform survival regression with uncertain events and a\nlatent cure fraction. We apply the proposed approach to study the risk of\nsubsequent suicide attempt after suicide-related hospitalization for adolescent\nand young adult population, using medical claims data from Connecticut. The\nidentified risk factors are highly interpretable; more intriguingly, our method\ndistinguishes the risk factors that are most helpful in assessing either\nsusceptibility or timing of subsequent attempt. The predicted statuses of the\nuncertain attempts are further investigated, leading to several new insights on\nsuicide event identification.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 20:47:16 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Wenjie", ""], ["Luo", "Chongliang", ""], ["Aseltine", "Robert H.", ""], ["Wang", "Fei", ""], ["Yan", "Jun", ""], ["Chen", "Kun", ""]]}, {"id": "2009.02601", "submitter": "Rocio Joo", "authors": "Roc\\'io Joo, Nicolas Bez, Marie-Pierre Etienne, Pablo Marin, Nicolas\n  Goascoz, J\\'er\\^ome Roux, St\\'ephanie Mah\\'evas", "title": "Identifying partners at sea on contrasting fisheries around the world", "comments": "18 pages; 6 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an approach to identify partners at sea based on fishing\ntrack analysis, and describe this behaviour in six different fleets: 1) pelagic\npair trawlers, 2) large bottom otter trawlers, 3) small bottom otter trawlers,\n4) mid-water otter trawlers, all operating in the North-East Atlantic Ocean, 5)\nanchovy purse-seiners in the South-East Pacific Ocean, and 6) tuna\npurse-seiners in the Western Indian Ocean. This type of behaviour is known to\nexist within pelagic pair trawlers. Since these vessels need to be in pairs for\ntheir fishing operations, in practice some of them decide to move together\nthroughout their whole fishing trips, and others for only a segment of their\ntrips. To identify partners at sea, we used a heuristic approach based on\njoint-movement metrics and Gaussian mixture models. The models were first\nfitted on the pelagic pair trawlers and then used on the other fleets. From all\nof these fisheries, only the tuna purse-seiners did not present partners at\nsea. We then analysed the connections at the scale of vessels and identified\nexclusive partners. This work shows that there are collective tactics at least\nat a pairwise level in diverse fisheries in the world.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 21:02:58 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Joo", "Roc\u00edo", ""], ["Bez", "Nicolas", ""], ["Etienne", "Marie-Pierre", ""], ["Marin", "Pablo", ""], ["Goascoz", "Nicolas", ""], ["Roux", "J\u00e9r\u00f4me", ""], ["Mah\u00e9vas", "St\u00e9phanie", ""]]}, {"id": "2009.02654", "submitter": "Vladimir Minin", "authors": "Jonathan Fintzi, Damon Bayer, Isaac Goldstein, Keith Lumbard, Emily\n  Ricotta, Sarah Warner, Lindsay M. Busch, Jeffrey R. Strich, Daniel S.\n  Chertow, Daniel M. Parker, Bernadette Boden-Albala, Alissa Dratch, Richard\n  Chhuon, Nichole Quick, Matthew Zahn, and Vladimir N. Minin", "title": "Using multiple data streams to estimate and forecast SARS-CoV-2\n  transmission dynamics, with application to the virus spread in Orange County,\n  California", "comments": "50 pages, 6 figures, 1 table in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near real-time monitoring of outbreak transmission dynamics and evaluation of\npublic health interventions are critical for interrupting the spread of the\nnovel coronavirus (SARS-CoV-2) and mitigating morbidity and mortality caused by\ncoronavirus disease (COVID-19). Formulating a regional mechanistic model of\nSARS-CoV-2 transmission dynamics and frequently estimating parameters of this\nmodel using streaming surveillance data offers one way to accomplish\ndata-driven decision making. For example, to detect an increase in new\nSARS-CoV-2 infections due to relaxation of previously implemented mitigation\nmeasures one can monitor estimates of the basic and effective reproductive\nnumbers. However, parameter estimation can be imprecise, and sometimes even\nimpossible, because surveillance data are noisy and not informative about all\naspects of the mechanistic model, even for reasonably parsimonious epidemic\nmodels. To overcome this obstacle, at least partially, we propose a Bayesian\nmodeling framework that integrates multiple surveillance data streams. Our\nmodel uses both COVID-19 incidence and mortality time series to estimate our\nmodel parameters. Importantly, our data generating model for incidence data\ntakes into account changes in the total number of tests performed. We apply our\nBayesian data integration method to COVID-19 surveillance data collected in\nOrange County, California. Our results suggest that California Department of\nPublic Health stay-at-home order, issued on March 19, 2020, lowered the\nSARS-CoV-2 effective reproductive number $R_{e}$ in Orange County below 1.0,\nwhich means that the order was successful in suppressing SARS-CoV-2 infections.\nHowever, subsequent \"re-opening\" steps took place when thousands of infectious\nindividuals remained in Orange County, so $R_{e}$ increased to approximately\n1.0 by mid-June and above 1.0 by mid-July.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 06:27:39 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Fintzi", "Jonathan", ""], ["Bayer", "Damon", ""], ["Goldstein", "Isaac", ""], ["Lumbard", "Keith", ""], ["Ricotta", "Emily", ""], ["Warner", "Sarah", ""], ["Busch", "Lindsay M.", ""], ["Strich", "Jeffrey R.", ""], ["Chertow", "Daniel S.", ""], ["Parker", "Daniel M.", ""], ["Boden-Albala", "Bernadette", ""], ["Dratch", "Alissa", ""], ["Chhuon", "Richard", ""], ["Quick", "Nichole", ""], ["Zahn", "Matthew", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "2009.02776", "submitter": "Marco Morucci", "authors": "Marco Morucci and Cynthia Rudin", "title": "Matching Bounds: How Choice of Matching Algorithm Impacts Treatment\n  Effects Estimates and What to Do about It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Different matches on the same data may produce different treatment effect\nestimates, even when they achieve similar balance or minimize the same loss\nfunction. We discuss reasons and consequences of this problem. We present\nevidence of this problem by replicating ten papers that use matching. We\nintroduce Matching Bounds: a finite-sample, non-stochastic method that allows\nanalysts to know whether a matched sample that produces different results with\nthe same levels of balance and overall match quality could be obtained from\ntheir data. We apply Matching Bounds to a replication of a matching study of\nthe effects of foreign aid on civil conflict and find that results could change\nbased on the matched sample chosen.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:03:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 23:35:26 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Morucci", "Marco", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2009.02781", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein, Eva Bartz, Frederik Rehbach, Olaf Mersmann", "title": "Optimization of High-dimensional Simulation Models Using Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation models are valuable tools for resource usage estimation and\ncapacity planning. In many situations, reliable data is not available. We\nintroduce the BuB simulator, which requires only the specification of plausible\nintervals for the simulation parameters. By performing a surrogate-model based\noptimization, improved simulation model parameters can be determined.\nFurthermore, a detailed statistical analysis can be performed, which allows\ndeep insights into the most important model parameters and their interactions.\nThis information can be used to screen the parameters that should be further\ninvestigated. To exemplify our approach, a capacity and resource planning task\nfor a hospital was simulated and optimized. The study explicitly covers\ndifficulties caused by the COVID-19 pandemic. It can be shown, that even if\nonly limited real-world data is available, the BuB simulator can be\nbeneficially used to consider worst- and best-case scenarios. The BuB simulator\ncan be extended in many ways, e.g., by adding further resources (personal\nprotection equipment, staff, pharmaceuticals) or by specifying several cohorts\n(based on age, health status, etc.).\n  Keywords: Synthetic data, discrete-event simulation, surrogate-model-based\noptimization, COVID-19, machine learning, artificial intelligence, hospital\nresource planning, prediction tool, capacity planning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:21:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""], ["Bartz", "Eva", ""], ["Rehbach", "Frederik", ""], ["Mersmann", "Olaf", ""]]}, {"id": "2009.02992", "submitter": "Gabriel Calvo", "authors": "Gabriel Calvo, Carmen Armero, Maria Grazia Pennino, Luigi Spezia", "title": "Bayesian shared-parameter models for analysing sardine fishing in the\n  Mediterranean Sea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  European sardine is experiencing an overfishing around the world. The\ndynamics of the industrial and artisanal fishing in the Mediterranean Sea from\n1970 to 2014 by country was assessed by means of Bayesian joint longitudinal\nmodelling that uses the random effects to generate an association structure\nbetween both longitudinal measures. Model selection was based on Bayes factors\napproximated through the harmonic mean.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:18:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Calvo", "Gabriel", ""], ["Armero", "Carmen", ""], ["Pennino", "Maria Grazia", ""], ["Spezia", "Luigi", ""]]}, {"id": "2009.02993", "submitter": "Raphael Sonabend", "authors": "Raphael Sonabend and Franz Kiraly", "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R", "comments": "Accepted in The R Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  distr6 is an object-oriented (OO) probability distributions interface\nleveraging the extensibility and scalability of R6, and the speed and\nefficiency of Rcpp. Over 50 probability distributions are currently implemented\nin the package with `core' methods including density, distribution, and\ngenerating functions, and more `exotic' ones including hazards and distribution\nfunction anti-derivatives. In addition to simple distributions, distr6 supports\ncompositions such as truncation, mixtures, and product distributions. This\npaper presents the core functionality of the package and demonstrates examples\nfor key use-cases. In addition this paper provides a critical review of the\nobject-oriented programming paradigms in R and describes some novel\nimplementations for design patterns and core object-oriented features\nintroduced by the package for supporting distr6 components.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:20:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 12:19:34 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 11:04:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sonabend", "Raphael", ""], ["Kiraly", "Franz", ""]]}, {"id": "2009.03129", "submitter": "Mason Terrett", "authors": "Mason Terrett, Daniel Fryer, Tanya Doody, Hien Nguyen, Pascal\n  Castellazzi", "title": "SARGDV: Efficient identification of groundwater-dependent vegetation\n  using synthetic aperture radar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Groundwater depletion impacts the sustainability of numerous\ngroundwater-dependent vegetation (GDV) globally, placing significant stress on\ntheir capacity to provide environmental and ecological support for flora,\nfauna, and anthropic benefits. Industries such as mining, agriculture, and\nplantations are heavily reliant on groundwater, the over-exploitation of which\nrisks impacting groundwater regimes, quality, and accessibility for nearby\nGDVs. Cost effective methods of GDV identification will enable strategic\nprotection of these critical ecological systems, through improved and\nsustainable groundwater management by communities and industry. Recent\napplication of synthetic aperture radar (SAR) earth observation data in\nAustralia has demonstrated the utility of radar for identifying terrestrial\ngroundwater-dependent ecosystems at scale. We propose a robust classification\nmethod to advance identification of GDVs at scale using processed SAR data\nproducts adapted from a recent previous method. The method includes the\ndevelopment of SARGDV, a binary classification model, which uses the extreme\ngradient boosting (XGBoost) algorithm in conjunction with three data cubes\ncomposed of Sentinel-1 SAR interferometric wide images. The images were\ncollected as a one-year time series over Mount Gambier, a region in South\nAustralia, known to support GDVs. The SARGDV model demonstrated high\nperformance for classifying GDVs with 77% precision, 76% true positive rate and\n96% accuracy. This method may be used to support the protection of GDV\ncommunities globally by providing a long term, cost-effective solution to\nidentify GDVs over variable regions and climates, via the use of freely\navailable, high-resolution, globally available Sentinel-1 SAR data sets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:35:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Terrett", "Mason", ""], ["Fryer", "Daniel", ""], ["Doody", "Tanya", ""], ["Nguyen", "Hien", ""], ["Castellazzi", "Pascal", ""]]}, {"id": "2009.03160", "submitter": "Bo Tranberg", "authors": "Neeraj Bokde, Bo Tranberg, Gorm Bruun Andresen", "title": "A graphical approach to carbon-efficient spot market scheduling for\n  Power-to-X applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.enconman.2020.113461", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Paris agreement of 2015, it was decided to reduce the CO2 emissions of\nthe energy sector to zero by 2050 and to restrict the global mean temperature\nincrease to 1.5 degree Celcius above the pre-industrial level. Such commitments\nare possible only with practically CO2-free power generation based on variable\nrenewable technologies. Historically, the main point of criticism regarding\nrenewable power is the variability driven by weather dependence. Power-to-X\nsystems, which convert excess power to other stores of energy for later use,\ncan play an important role in offsetting the variability of renewable power\nproduction. In order to do so, however, these systems have to be scheduled\nproperly to ensure they are being powered by low-carbon technologies. In this\npaper, we introduce a graphical approach for scheduling power-to-X plants in\nthe day-ahead market by minimizing carbon emissions and electricity costs. This\ngraphical approach is simple to implement and intuitively explain to\nstakeholders. In a simulation study using historical prices and CO2 intensity\nfor four different countries, we find that the price and CO2 intensity tends to\ndecrease with increasing scheduling horizon. The effect diminishes when\nrequiring an increasing amount of full load hours per year. Additionally,\ninvestigating the trade-off between optimizing for price or CO2 intensity shows\nthat it is indeed a trade-off: it is not possible to obtain the lowest price\nand CO2 intensity at the same time.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:12:14 GMT"}], "update_date": "2021-01-03", "authors_parsed": [["Bokde", "Neeraj", ""], ["Tranberg", "Bo", ""], ["Andresen", "Gorm Bruun", ""]]}, {"id": "2009.03262", "submitter": "Rodrigo Rivera-Castro", "authors": "Rodrigo Rivera-Castro, Ivan Nazarov, Yuke Xiang, Ivan Maksimov,\n  Aleksandr Pletnev, Evgeny Burnaev", "title": "An industry case of large-scale demand forecasting of hierarchical\n  components", "comments": null, "journal-ref": "2019 18th IEEE International Conference On Machine Learning And\n  Applications (ICMLA)", "doi": "10.1109/ICMLA.2019.00029", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand forecasting of hierarchical components is essential in manufacturing.\nHowever, its discussion in the machine-learning literature has been limited,\nand judgemental forecasts remain pervasive in the industry. Demand planners\nrequire easy-to-understand tools capable of delivering state-of-the-art\nresults. This work presents an industry case of demand forecasting at one of\nthe largest manufacturers of electronics in the world. It seeks to support\npractitioners with five contributions: (1) A benchmark of fourteen demand\nforecast methods applied to a relevant data set, (2) A data transformation\ntechnique yielding comparable results with state of the art, (3) An alternative\nto ARIMA based on matrix factorization, (4) A model selection technique based\non topological data analysis for time series and (5) A novel data set.\nOrganizations seeking to up-skill existing personnel and increase forecast\naccuracy will find value in this work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:33:03 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Rivera-Castro", "Rodrigo", ""], ["Nazarov", "Ivan", ""], ["Xiang", "Yuke", ""], ["Maksimov", "Ivan", ""], ["Pletnev", "Aleksandr", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2009.03450", "submitter": "Nicolas Franco", "authors": "Nicolas Franco", "title": "Covid-19 Belgium: Extended SEIR-QD model with nursing homes and\n  long-term scenarios-based forecasts", "comments": "23 pages, 13 figures, revised version with improved fitting procedure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the spread of the covid-19 pandemic and pending the establishment\nof vaccination campaigns, several non pharmaceutical interventions such as\npartial and full lockdown, quarantine and measures of physical distancing have\nbeen imposed in order to reduce the spread of the disease and to lift the\npressure on healthcare system. Mathematical models are important tools for\nestimating the impact of these interventions, for monitoring the current\nevolution of the epidemic at a national level and for estimating the potential\nlong-term consequences of relaxation of measures. In this paper, we model the\nevolution of the covid-19 epidemic in Belgium with a deterministic\nage-structured extended compartmental model. Our model takes special\nconsideration for nursing homes which are modelled as separate entities from\nthe general population in order to capture the specific delay and dynamics\nwithin these entities. The model integrates social contact data and is fitted\non hospitalisations data (admission and discharge), on the daily number of\ncovid-19 deaths (with a distinction between general population and nursing\nhomes related deaths) and results from serological studies. The sensitivity\nanalysis of the estimated parameters relies on a Bayesian approach using Markov\nChain Monte Carlo methods. We present the situation as in November 2020 with\nthe estimation of some characteristics of the covid-19 deduced from the model.\nWe also present several mid-term and long-term projections based on scenarios\nof reinforcement or relaxation of social contacts for different general\nsectors, with a lot of uncertainties remaining.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 23:02:49 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 18:33:22 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 20:03:48 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Franco", "Nicolas", ""]]}, {"id": "2009.03474", "submitter": "Rodrigo Rivera-Castro", "authors": "Aleksandr Pletnev, Rodrigo Rivera-Castro, Evgeny Burnaev", "title": "Graph Neural Networks for Model Recommendation using Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series prediction aims to predict future values to help stakeholders\nmake proper strategic decisions. This problem is relevant in all industries and\nareas, ranging from financial data to demand to forecast. However, it remains\nchallenging for practitioners to select the appropriate model to use for\nforecasting tasks. With this in mind, we present a model architecture based on\nGraph Neural Networks to provide model recommendations for time series\nforecasting. We validate our approach on three relevant datasets and compare it\nagainst more than sixteen techniques. Our study shows that the proposed method\nperforms better than target baselines and state of the art, including\nmeta-learning. The results show the relevancy and suitability of GNN as methods\nfor model recommendations in time series forecasting.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 01:17:57 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Pletnev", "Aleksandr", ""], ["Rivera-Castro", "Rodrigo", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2009.03646", "submitter": "Maike Hohberg", "authors": "Maike Hohberg, Francesco Donat, Giampiero Marra and Thomas Kneib", "title": "Beyond unidimensional poverty analysis using distributional copula\n  models for mixed ordered-continuous outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poverty is a multidimensional concept often comprising a monetary outcome and\nother welfare dimensions such as education, subjective well-being or health,\nthat are measured on an ordinal scale. In applied research, multidimensional\npoverty is ubiquitously assessed by studying each poverty dimension\nindependently in univariate regression models or by combining several poverty\ndimensions into a scalar index. This inhibits a thorough analysis of the\npotentially varying interdependence between the poverty dimensions. We propose\na multivariate copula generalized additive model for location, scale and shape\n(copula GAMLSS or distributional copula model) to tackle this challenge. By\nrelating the copula parameter to covariates, we specifically examine if certain\nfactors determine the dependence between poverty dimensions. Furthermore,\nspecifying the full conditional bivariate distribution, allows us to derive\nseveral features such as poverty risks and dependence measures coherently from\none model for different individuals. We demonstrate the approach by studying\ntwo important poverty dimensions: income and education. Since the level of\neducation is measured on an ordinal scale while income is continuous, we extend\nthe bivariate copula GAMLSS to the case of mixed ordered-continuous outcomes.\nThe new model is integrated into the GJRM package in R and applied to data from\nIndonesia. Particular emphasis is given to the spatial variation of the\nincome-education dependence and groups of individuals at risk of being\nsimultaneously poor in both education and income dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:31:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hohberg", "Maike", ""], ["Donat", "Francesco", ""], ["Marra", "Giampiero", ""], ["Kneib", "Thomas", ""]]}, {"id": "2009.03650", "submitter": "Martin Roessler", "authors": "Martin Roessler, Jochen Schmitt, Olaf Schoffer", "title": "Can we trust the standardized mortality ratio? A formal analysis and\n  evaluation based on axiomatic requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The standardized mortality ratio (SMR) is often used to assess\nand compare hospital performance. While it has been recognized that hospitals\nmay differ in their SMRs due to differences in patient composition, there is a\nlack of rigorous analysis of this and other - largely unrecognized - properties\nof the SMR. Methods: This paper proposes five axiomatic requirements for\nadequate standardized mortality measures: strict monotonicity, case-mix\ninsensitivity, scale insensitivity, equivalence principle, and dominance\nprinciple. Given these axiomatic requirements, effects of variations in patient\ncomposition, hospital size, and actual and expected mortality rates on the SMR\nwere examined using basic algebra and calculus. In this regard, we\ndistinguished between standardization using expected mortality rates derived\nfrom a different dataset (external standardization) and standardization based\non a dataset including the considered hospitals (internal standardization).\nResults: Under external standardization, the SMR fulfills the axiomatic\nrequirements of strict monotonicity and scale insensitivity but violates the\nrequirement of case-mix insensitivity, the equivalence principle, and the\ndominance principle. All axiomatic requirements not fulfilled under external\nstandardization are also not fulfilled under internal standardization. In\naddition, the SMR under internal standardization is scale sensitive and\nviolates the axiomatic requirement of strict monotonicity. Conclusions: The SMR\nfulfills only two (none) out of the five proposed axiomatic requirements under\nexternal (internal) standardization. Generally, the SMRs of hospitals are\ndifferently affected by variations in case mix and actual and expected\nmortality rates unless the hospitals are identical in these characteristics.\nThese properties hamper valid assessment and comparison of hospital performance\nbased on the SMR.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:44:20 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Roessler", "Martin", ""], ["Schmitt", "Jochen", ""], ["Schoffer", "Olaf", ""]]}, {"id": "2009.03677", "submitter": "Chaouki ben Issaid", "authors": "Chaouki Ben Issaid and Mohamed-Slim Alouini and and Raul Tempone", "title": "Efficient Importance Sampling for the Left Tail of Positive Gaussian\n  Quadratic Forms", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.09174", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the left tail of quadratic forms in Gaussian random vectors is of\nmajor practical importance in many applications. In this letter, we propose an\nefficient importance sampling estimator that is endowed with the bounded\nrelative error property. This property significantly reduces the number of\nsimulation runs required by the proposed estimator compared to naive Monte\nCarlo (MC), especially when the probability of interest is very small. Selected\nsimulation results are presented to illustrate the efficiency of our estimator\ncompared to naive MC as well as some of the well-known approximations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 06:02:08 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Issaid", "Chaouki Ben", ""], ["Alouini", "Mohamed-Slim", ""], ["Tempone", "and Raul", ""]]}, {"id": "2009.03703", "submitter": "Lara Vomfell", "authors": "Lara Vomfell, Wolfgang Karl H\\\"ardle, Stefan Lessmann", "title": "Improving Crime Count Forecasts Using Twitter and Taxi Data", "comments": null, "journal-ref": "Decision Support Systems, vol 113, pp. 73-85, 2018", "doi": "10.1016/j.dss.2018.07.003", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crime prediction is crucial to criminal justice decision makers and efforts\nto prevent crime. The paper evaluates the explanatory and predictive value of\nhuman activity patterns derived from taxi trip, Twitter and Foursquare data.\nAnalysis of a six-month period of crime data for New York City shows that these\ndata sources improve predictive accuracy for property crime by 19% compared to\nusing only demographic data. This effect is strongest when the novel features\nare used together, yielding new insights into crime prediction. Notably and in\nline with social disorganization theory, the novel features cannot improve\npredictions for violent crimes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:46:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Vomfell", "Lara", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Lessmann", "Stefan", ""]]}, {"id": "2009.03712", "submitter": "Nicoletta D'Angelo", "authors": "Nicoletta D'Angelo, Antonino Abbruzzo and Giada Adelfio", "title": "Spatial Bayesian Hierarchical Modelling with Integrated Nested Laplace\n  Approximation", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider latent Gaussian fields for modelling spatial dependence in the\ncontext of both spatial point patterns and areal data, providing two different\napplications. The inhomogeneous Log-Gaussian Cox Process model is specified to\ndescribe a seismic sequence occurred in Greece, resorting to the Stochastic\nPartial Differential Equations. The Besag-York-Mollie model is fitted for\ndisease mapping of the Covid-19 infection in the North of Italy. These models\nboth belong to the class of Bayesian hierarchical models with latent Gaussian\nfields whose posterior is not available in closed form. Therefore, the\ninference is performed with the Integrated Nested Laplace Approximation, which\nprovides accurate and relatively fast analytical approximations to the\nposterior quantities of interest.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 13:08:27 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["D'Angelo", "Nicoletta", ""], ["Abbruzzo", "Antonino", ""], ["Adelfio", "Giada", ""]]}, {"id": "2009.03726", "submitter": "Duc Minh Nguyen", "authors": "Duc Minh Nguyen, Mustafa A. Kishk, and Mohamed-Slim Alouini", "title": "Modeling and Analysis of Dynamic Charging for EVs: A Stochastic Geometry\n  Approach", "comments": "25 pages, submitted to IEEE Open Journal of Vehicular Technology\n  (OJVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for greener and more energy efficient\ntransportation solutions, electric vehicles (EVs) have emerged to be the future\nof transportation across the globe. However, currently, one of the biggest\nbottlenecks of EVs is the battery. Small batteries limit the EVs driving range,\nwhile big batteries are expensive and not environmentally friendly. One\npotential solution to this challenge is the deployment of charging roads, i.e.,\ndynamic wireless charging systems installed under the roads that enable EVs to\nbe charged while driving. In this paper, we use tools from stochastic geometry\nto establish a framework that enables evaluating the performance of charging\nroads deployment in metropolitan cities. We first present the course of actions\nthat a driver should take when driving from a random source to a random\ndestination in order to maximize dynamic charging during the trip. Next, we\nanalyze the distribution of the distance to the nearest charging road. This\ndistribution is vital for studying multiple performance metrics such as the\ntrip efficiency, which we define as the fraction of the total trip spent on\ncharging roads. Next, we derive the probability that a given trip passes\nthrough at least one charging road. The derived probability distributions can\nbe used to assist urban planners and policy makers in designing the deployment\nplans of dynamic wireless charging systems. In addition, they can also be used\nby drivers and automobile manufacturers in choosing the best driving routes\ngiven the road conditions and level of energy of EV battery.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 13:20:54 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nguyen", "Duc Minh", ""], ["Kishk", "Mustafa A.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2009.03798", "submitter": "Georg Heiler", "authors": "Georg Heiler and Allan Hanbury and Peter Filzmoser", "title": "The impact of COVID-19 on relative changes in aggregated mobility using\n  mobile-phone data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating relative changes leads to additional insights which would remain\nhidden when only evaluating absolute changes. We analyze a dataset describing\nmobility of mobile phones in Austria before, during COVID-19 lock-down measures\nuntil recent. By applying compositional data analysis we show that formerly\nhidden information becomes available: we see that the elderly population groups\nincrease relative mobility and that the younger groups especially on weekends\nalso do not decrease their mobility as much as the others.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:53:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Heiler", "Georg", ""], ["Hanbury", "Allan", ""], ["Filzmoser", "Peter", ""]]}, {"id": "2009.03851", "submitter": "Iwona Hawryluk", "authors": "Iwona Hawryluk, Swapnil Mishra, Seth Flaxman, Samir Bhatt and Thomas\n  A. Mellan", "title": "Referenced Thermodynamic Integration for Bayesian Model Selection:\n  Application to COVID-19 Model Selection", "comments": "27 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is a fundamental part of the applied Bayesian statistical\nmethodology. Metrics such as the Akaike Information Criterion are commonly used\nin practice to select models but do not incorporate the uncertainty of the\nmodels' parameters and can give misleading choices. One approach that uses the\nfull posterior distribution is to compute the ratio of two models' normalising\nconstants, known as the Bayes factor. Often in realistic problems, this\ninvolves the integration of analytically intractable, high-dimensional\ndistributions, and therefore requires the use of stochastic methods such as\nthermodynamic integration (TI). In this paper we apply a variation of the TI\nmethod, referred to as referenced TI, which computes a single model's\nnormalising constant in an efficient way by using a judiciously chosen\nreference density. The advantages of the approach and theoretical\nconsiderations are set out, along with explicit pedagogical 1 and 2D examples.\nBenchmarking is presented with comparable methods and we find favourable\nconvergence performance. The approach is shown to be useful in practice when\napplied to a real problem - to perform model selection for a semi-mechanistic\nhierarchical Bayesian model of COVID-19 transmission in South Korea involving\nthe integration of a 200D density.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:32:06 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 17:26:52 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 22:21:50 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Hawryluk", "Iwona", ""], ["Mishra", "Swapnil", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""], ["Mellan", "Thomas A.", ""]]}, {"id": "2009.03966", "submitter": "Sayanti Mukherjee", "authors": "Zhiyuan Wei and Sayanti Mukherjee", "title": "Health-behaviors associated with the growing risk of adolescent suicide\n  attempts: A data-driven cross-sectional study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Identify and examine the associations between health behaviors and\nincreased risk of adolescent suicide attempts, while controlling for\nsocioeconomic and demographic differences. Design: A data-driven analysis using\ncross-sectional data. Setting: Communities in the state of Montana from 1999 to\n2017. Subjects: Selected 22,447 adolescents of whom 1,631 adolescents attempted\nsuicide at least once. Measures: Overall 29 variables (predictors) accounting\nfor psychological behaviors, illegal substances consumption, daily activities\nat schools and demographic backgrounds, were considered. Analysis: A library of\nmachine learning algorithms along with the traditionally-used logistic\nregression were used to model and predict suicide attempt risk. Model\nperformances (goodness-of-fit and predictive accuracy) were measured using\naccuracy, precision, recall and F-score metrics. Results: The non-parametric\nBayesian tree ensemble model outperformed all other models, with 80.0% accuracy\nin goodness-of-fit (F-score:0.802) and 78.2% in predictive accuracy\n(F-score:0.785). Key health-behaviors identified include: being sad/hopeless,\nfollowed by safety concerns at school, physical fighting, inhalant usage,\nillegal drugs consumption at school, current cigarette usage, and having first\nsex at an early age (below 15 years of age). Additionally, the minority groups\n(American Indian/Alaska Natives, Hispanics/Latinos), and females are also found\nto be highly vulnerable to attempting suicides. Conclusion: Significant\ncontribution of this work is understanding the key health-behaviors and health\ndisparities that lead to higher frequency of suicide attempts among\nadolescents, while accounting for the non-linearity and complex interactions\namong the outcome and the exposure variables.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:29:18 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Wei", "Zhiyuan", ""], ["Mukherjee", "Sayanti", ""]]}, {"id": "2009.04023", "submitter": "Wai Tong Chung", "authors": "Wai Tong Chung, Aashwin Ananda Mishra, Nikolaos Perakis, Matthias Ihme", "title": "Data-assisted combustion simulations with dynamic submodel assignment\n  using random forests", "comments": "Accepted version; 23 pages, 12 figures", "journal-ref": "Combustion and Flame 227 (2021) 172-185", "doi": "10.1016/j.combustflame.2020.12.041", "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this investigation, we outline a data-assisted approach that employs\nrandom forest classifiers for local and dynamic combustion submodel assignment\nin turbulent-combustion simulations. This method is applied in simulations of a\nsingle-element GOX/GCH4 rocket combustor; a priori as well as a posteriori\nassessments are conducted to (i) evaluate the accuracy and adjustability of the\nclassifier for targeting different quantities-of-interest (QoIs), and (ii)\nassess improvements, resulting from the data-assisted combustion model\nassignment, in predicting target QoIs during simulation runtime. Results from\nthe a priori study show that random forests, trained with local flow properties\nas input variables and combustion model errors as training labels, assign three\ndifferent combustion models - finite-rate chemistry (FRC), flamelet progress\nvariable (FPV) model, and inert mixing (IM) - with reasonable classification\nperformance even when targeting multiple QoIs. Applications in a posteriori\nstudies demonstrate improved predictions from data-assisted simulations, in\ntemperature and CO mass fraction, when compared with monolithic FPV\ncalculations. These results demonstrate that this data-driven framework holds\npromise for the dynamic combustion submodel assignment in reacting flow\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 23:06:22 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 23:36:26 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 20:24:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chung", "Wai Tong", ""], ["Mishra", "Aashwin Ananda", ""], ["Perakis", "Nikolaos", ""], ["Ihme", "Matthias", ""]]}, {"id": "2009.04137", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, T. Kypraios, P. D. O'Neill, T. J. Hagenaars", "title": "A Bayesian Nonparametric Analysis of the 2003 Outbreak of Highly\n  Pathogenic Avian Influenza in the Netherlands", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases on farms pose both public and animal health risks, so\nunderstanding how they spread between farms is crucial for developing disease\ncontrol strategies to prevent future outbreaks. We develop novel Bayesian\nnonparametric methodology to fit spatial stochastic transmission models in\nwhich the infection rate between any two farms is a function that depends on\nthe distance between them, but without assuming a specified parametric form.\nMaking nonparametric inference in this context is challenging since the\nlikelihood function of the observed data is intractable because the underlying\ntransmission process is unobserved. We adopt a fully Bayesian approach by\nassigning a transformed Gaussian Process prior distribution to the infection\nrate function, and then develop an efficient data augmentation Markov Chain\nMonte Carlo algorithm to perform Bayesian inference. We use the posterior\npredictive distribution to simulate the effect of different disease control\nmethods and their economic impact. We analyse a large outbreak of Avian\nInfluenza in the Netherlands and infer the between-farm infection rate, as well\nas the unknown infection status of farms which were pre-emptively culled. We\nuse our results to analyse ring-culling strategies, and conclude that although\neffective, ring-culling has limited impact in high density areas.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:19:49 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Seymour", "R. G.", ""], ["Kypraios", "T.", ""], ["O'Neill", "P. D.", ""], ["Hagenaars", "T. J.", ""]]}, {"id": "2009.04171", "submitter": "Smit Marvaniya", "authors": "Ayush Jain, Smit Marvaniya, Shantanu Godbole, and Vitobha Munigala", "title": "A Framework for Crop Price Forecasting in Emerging Economies by\n  Analyzing the Quality of Time-series Data", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accuracy of crop price forecasting techniques is important because it enables\nthe supply chain planners and government bodies to take appropriate actions by\nestimating market factors such as demand and supply. In emerging economies such\nas India, the crop prices at marketplaces are manually entered every day, which\ncan be prone to human-induced errors like the entry of incorrect data or entry\nof no data for many days. In addition to such human prone errors, the\nfluctuations in the prices itself make the creation of stable and robust\nforecasting solution a challenging task. Considering such complexities in crop\nprice forecasting, in this paper, we present techniques to build robust crop\nprice prediction models considering various features such as (i) historical\nprice and market arrival quantity of crops, (ii) historical weather data that\ninfluence crop production and transportation, (iii) data quality-related\nfeatures obtained by performing statistical analysis. We additionally propose a\nframework for context-based model selection and retraining considering factors\nsuch as model stability, data quality metrics, and trend analysis of crop\nprices. To show the efficacy of the proposed approach, we show experimental\nresults on two crops - Tomato and Maize for 14 marketplaces in India and\ndemonstrate that the proposed approach not only improves accuracy metrics\nsignificantly when compared against the standard forecasting techniques but\nalso provides robust models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:06:07 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Jain", "Ayush", ""], ["Marvaniya", "Smit", ""], ["Godbole", "Shantanu", ""], ["Munigala", "Vitobha", ""]]}, {"id": "2009.04296", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Lei Fang, Wolfgang K. H\\\"ardle, Juhyun Park", "title": "A Mortality Model for Multi-populations: A Semi-Parametric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mortality is different across countries, states and regions. Several\nempirical research works however reveal that mortality trends exhibit a common\npattern and show similar structures across populations. The key element in\nanalyzing mortality rate is a time-varying indicator curve. Our main interest\nlies in validating the existence of the common trends among these curves, the\nsimilar gender differences and their variability in location among the curves\nat the national level. Motivated by the empirical findings, we make the study\nof estimating and forecasting mortality rates based on a semi-parametric\napproach, which is applied to multiple curves with the shape-related nonlinear\nvariation. This approach allows us to capture the common features contained in\nthe curve functions and meanwhile provides the possibility to characterize the\nnonlinear variation via a few deviation parameters. These parameters carry an\ninstructive summary of the time-varying curve functions and can be further used\nto make a suggestive forecast analysis for countries with barren data sets. In\nthis research the model is illustrated with mortality rates of Japan and China,\nand extended to incorporate more countries.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:42:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Fang", "Lei", ""], ["H\u00e4rdle", "Wolfgang K.", ""], ["Park", "Juhyun", ""]]}, {"id": "2009.04359", "submitter": "Rodrigo Rivera-Castro", "authors": "Rodrigo Rivera-Castro, Ivan Nazarov, Evgeny Burnaev", "title": "Towards forecast techniques for business analysts of large commercial\n  data sets using matrix factorization methods", "comments": "Journal of Physics: Conference Series, 2018", "journal-ref": null, "doi": "10.1088/1742-6596/1117/1/012010", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research article suggests that there are significant benefits in\nexposing demand planners to forecasting methods using matrix completion\ntechniques. This study aims to contribute to a better understanding of the\nfield of forecasting with multivariate time series prediction by focusing on\nthe dimension of large commercial data sets with hierarchies. This research\nhighlights that there has neither been sufficient academic research in this\nsub-field nor dissemination among practitioners in the business sector. This\nstudy seeks to innovate by presenting a matrix completion method for short-term\ndemand forecast of time series data on relevant commercial problems. Albeit\ncomputing intensive, this method outperforms the state of the art while\nremaining accessible to business users. The object of research is matrix\ncompletion for time series in a big data context within the industry. The\nsubject of the research is forecasting product demand using techniques for\nmultivariate hierarchical time series prediction that are both precise and\naccessible to non-technical business experts. Apart from a methodological\ninnovation, this research seeks to introduce practitioners to novel methods for\nhierarchical multivariate time series prediction. The research outcome is of\ninterest for organizations requiring precise forecasts yet lacking the\nappropriate human capital to develop them.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 15:29:02 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Rivera-Castro", "Rodrigo", ""], ["Nazarov", "Ivan", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2009.04547", "submitter": "Pablo G. Morato", "authors": "P. G. Morato, K.G. Papakonstantinou, C.P. Andriotis, J.S. Nielsen and\n  P. Rigo", "title": "Optimal Inspection and Maintenance Planning for Deteriorating Structures\n  through Dynamic Bayesian Networks and Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Civil and maritime engineering systems, among others, from bridges to\noffshore platforms and wind turbines, must be efficiently managed as they are\nexposed to deterioration mechanisms throughout their operational life, such as\nfatigue or corrosion. Identifying optimal inspection and maintenance policies\ndemands the solution of a complex sequential decision-making problem under\nuncertainty, with the main objective of efficiently controlling the risk\nassociated with structural failures. Addressing this complexity, risk-based\ninspection planning methodologies, supported often by dynamic Bayesian\nnetworks, evaluate a set of pre-defined heuristic decision rules to reasonably\nsimplify the decision problem. However, the resulting policies may be\ncompromised by the limited space considered in the definition of the decision\nrules. Avoiding this limitation, Partially Observable Markov Decision Processes\n(POMDPs) provide a principled mathematical methodology for stochastic optimal\ncontrol under uncertain action outcomes and observations, in which the optimal\nactions are prescribed as a function of the entire, dynamically updated, state\nprobability distribution. In this paper, we combine dynamic Bayesian networks\nwith POMDPs in a joint framework for optimal inspection and maintenance\nplanning, and we provide the formulation for developing both infinite and\nfinite horizon POMDPs in a structural reliability context. The proposed\nmethodology is implemented and tested for the case of a structural component\nsubject to fatigue deterioration, demonstrating the capability of\nstate-of-the-art point-based POMDP solvers for solving the underlying planning\noptimization problem. Within the numerical experiments, POMDP and\nheuristic-based policies are thoroughly compared, and results showcase that\nPOMDPs achieve substantially lower costs as compared to their counterparts,\neven for traditional problem settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:03:42 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Morato", "P. G.", ""], ["Papakonstantinou", "K. G.", ""], ["Andriotis", "C. P.", ""], ["Nielsen", "J. S.", ""], ["Rigo", "P.", ""]]}, {"id": "2009.04576", "submitter": "Gregory Matthews", "authors": "Ryan T. Elmore and Gregory J. Matthews", "title": "Bang the Can Slowly: An Investigation into the 2017 Houston Astros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript is a statistical investigation into the 2017 Major League\nBaseball scandal involving the Houston Astros, the World Series championship\nwinner that same year. The Astros were alleged to have stolen their opponents'\npitching signs in order to provide their batters with a potentially unfair\nadvantage. This work finds compelling evidence that the Astros on-field\nperformance was significantly affected by their sign-stealing ploy and\nquantifies the effects. The three main findings in the manuscript are: 1) the\nAstros' odds of swinging at a pitch were reduced by approximately 27% (OR:\n0.725, 95% CI: (0.618, 0.850)) when the sign was stolen, 2) when an Astros\nplayer swung, the odds of making contact with the ball increased roughly 80%\n(OR: 1.805, 95% CI: (1.342, 2.675)) on non-fastball pitches, and 3) when the\nAstros made contact with a ball on a pitch in which the sign was known, the\nball's exit velocity (launch speed) increased on average by 2.386 (95% CI:\n(0.334, 4.451)) miles per hour.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 21:15:28 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Elmore", "Ryan T.", ""], ["Matthews", "Gregory J.", ""]]}, {"id": "2009.04702", "submitter": "Gergely Palla", "authors": "Bianka Kov\\'acs and Gergely Palla", "title": "Optimisation of the coalescent hyperbolic embedding of complex networks", "comments": null, "journal-ref": "Sci Rep 11, 8350 (2021)", "doi": "10.1038/s41598-021-87333-5", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several observations indicate the existence of a latent hyperbolic space\nbehind real networks that makes their structure very intuitive in the sense\nthat the probability for a connection is decreasing with the hyperbolic\ndistance between the nodes. A remarkable network model generating random graphs\nalong this line is the popularity-similarity optimisation (PSO) model, offering\na scale-free degree distribution, high clustering and the small world property\nat the same time. These results provide a strong motivation for the development\nof hyperbolic embedding algorithms, that tackle the problem of finding the\noptimal hyperbolic coordinates of the nodes based on the network structure. A\nvery promising recent approach for hyperbolic embedding is provided by the\nnoncentered minimum curvilinear embedding (ncMCE) method, belonging to the\nfamily of coalescent embedding algorithms. This approach offers a high quality\nembedding at a low running time. In the present work we propose a further\noptimisation of the angular coordinates in this framework that seems to reduce\nthe logarithmic loss and increase the greedy routing score of the embedding\ncompared to the original version, thereby adding an extra improvement to the\nquality of the inferred hyperbolic coordinates.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:38:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kov\u00e1cs", "Bianka", ""], ["Palla", "Gergely", ""]]}, {"id": "2009.04710", "submitter": "Abhik Ghosh PhD", "authors": "Soumya Chakraborty, Ayanendranath Basu, Abhik Ghosh", "title": "Robust Clustering with Normal Mixture Models: A Pseudo\n  $\\beta$-Likelihood Approach", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in other estimation scenarios, likelihood based estimation in the normal\nmixture set-up is highly non-robust against model misspecification and presence\nof outliers (apart from being an ill-posed optimization problem). We propose a\nrobust alternative to the ordinary likelihood approach for this estimation\nproblem which performs simultaneous estimation and data clustering and leads to\nsubsequent anomaly detection. To invoke robustness, we follow, in spirit, the\nmethodology based on the minimization of the density power divergence (or\nalternatively, the maximization of the $\\beta$-likelihood) under suitable\nconstraints. An iteratively reweighted least squares approach has been followed\nin order to compute our estimators for the component means (or equivalently\ncluster centers) and component dispersion matrices which leads to simultaneous\ndata clustering. Some exploratory techniques are also suggested for anomaly\ndetection, a problem of great importance in the domain of statistics and\nmachine learning. Existence and consistency of the estimators are established\nunder the aforesaid constraints. We validate our method with simulation studies\nunder different set-ups; it is seen to perform competitively or better compared\nto the popular existing methods like K-means and TCLUST, especially when the\nmixture components (i.e., the clusters) share regions with significant overlap\nor outlying clusters exist with small but non-negligible weights. Two real\ndatasets are also used to illustrate the performance of our method in\ncomparison with others along with an application in image processing. It is\nobserved that our method detects the clusters with lower misclassification\nrates and successfully points out the outlying (anomalous) observations from\nthese datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:50:08 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chakraborty", "Soumya", ""], ["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""]]}, {"id": "2009.04747", "submitter": "Mohammad Ghorbani Dr.", "authors": "Mohammad Ghorbani, Nafiseh Vafaei, Ji\\v{r}\\'i Dvo\\v{r}\\'ak, Mari\n  Myllym\\\"aki", "title": "Testing the first-order separability hypothesis for spatio-temporal\n  point patterns", "comments": "21 pages, 8 Figures (21 plots)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order separability of a spatio-temporal point process plays a\nfundamental role in the analysis of spatio-temporal point pattern data. While\nit is often a convenient assumption that simplifies the analysis greatly,\nexisting non-separable structures should be accounted for in the model\nconstruction. We propose three different tests to investigate this hypothesis\nas a step of preliminary data analysis. The first two tests are exact or\nasymptotically exact for Poisson processes. The first test based on\npermutations and global envelopes allows us to detect at which spatial and\ntemporal locations or lags the data deviate from the null hypothesis. The\nsecond test is a simple and computationally cheap $\\chi^2$-test. The third test\nis based on statistical reconstruction method and can be generally applied for\nnon-Poisson processes. The performance of the first two tests is studied in a\nsimulation study for Poisson and non-Poisson models. The third test is applied\nto the real data of the UK 2001 epidemic foot and mouth disease.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:35:16 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ghorbani", "Mohammad", ""], ["Vafaei", "Nafiseh", ""], ["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "2009.04832", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Luke J Keele, Dylan S Small, Marshall M Joffe", "title": "A note on post-treatment selection in studying racial discrimination in\n  policing", "comments": "Accepted for publication in the American Political Science Review on\n  14th June, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss some causal estimands used to study racial discrimination in\npolicing. A central challenge is that not all police-civilian encounters are\nrecorded in administrative datasets and available to researchers. One possible\nsolution is to consider the average causal effect of race conditional on the\ncivilian already being detained by the police. We find that such an estimand\ncan be quite different from the more familiar ones in causal inference and\nneeds to be interpreted with caution. We propose using an estimand new for this\ncontext -- the causal risk ratio, which has more transparent interpretation and\nrequires weaker identification assumptions. We demonstrate this through a\nreanalysis of the NYPD Stop-and-Frisk dataset. Our reanalysis shows that the\nnaive estimator that ignores the post-treatment selection in administrative\nrecords may severely underestimate the disparity in police violence between\nminorities and whites in these and similar data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:17:37 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:26:53 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 21:11:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Keele", "Luke J", ""], ["Small", "Dylan S", ""], ["Joffe", "Marshall M", ""]]}, {"id": "2009.04843", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini and Alfredo Esposito", "title": "Checking individuals and sampling populations with imperfect tests", "comments": "113 pages, 34 figures; R and JAGS code provided in Appendix\n  (available for download from http://www.roma1.infn.it/~dagos/prob+stat.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last months, due to the emergency of Covid-19, questions related to\nthe fact of belonging or not to a particular class of individuals (`infected or\nnot infected'), after being tagged as `positive' or `negative' by a test, have\nnever been so popular. Similarly, there has been strong interest in estimating\nthe proportion of a population expected to hold a given characteristics\n(`having or having had the virus'). Taking the cue from the many related\ndiscussions on the media, in addition to those to which we took part, we\nanalyze these questions from a probabilistic perspective (`Bayesian'),\nconsidering several effects that play a role in evaluating the probabilities of\ninterest. The resulting paper, written with didactic intent, is rather general\nand not strictly related to pandemics: the basic ideas of Bayesian inference\nare introduced and the uncertainties on the performances of the tests are\ntreated using the metrological concepts of `systematics', and are propagated\ninto the quantities of interest following the rules of probability theory; the\nseparation of `statistical' and `systematic' contributions to the uncertainty\non the inferred proportion of infectees allows to optimize the sample size; the\nrole of `priors', often overlooked, is stressed, however recommending the use\nof `flat priors', since the resulting posterior distribution can be `reshaped'\nby an `informative prior' in a later step; details on the calculations are\ngiven, also deriving useful approximated formulae, the tough work being however\ndone with the help of direct Monte Carlo simulations and Markov Chain Monte\nCarlo, implemented in R and JAGS (relevant code provided in appendix).\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 20:51:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["D'Agostini", "Giulio", ""], ["Esposito", "Alfredo", ""]]}, {"id": "2009.05015", "submitter": "Somit Gupta", "authors": "Ali Mahmoudzadeh, Sophia Liu, Sol Sadeghi, Paul Luo Li, Somit Gupta", "title": "Bias Variance Tradeoff in Analysis of Online Controlled Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many organizations utilize large-scale online controlled experiments (OCEs)\nto accelerate innovation. Having high statistical power to detect small\ndifferences between control and treatment accurately is critical, as even small\nchanges in key metrics can be worth millions of dollars or indicate user\ndissatisfaction for a very large number of users. For large-scale OCE, the\nduration is typically short (e.g. two weeks) to expedite changes and\nimprovements to the product. In this paper, we examine two common approaches\nfor analyzing usage data collected from users within the time window of an\nexperiment, which can differ in accuracy and power. The open approach includes\nall relevant usage data from all active users for the entire duration of the\nexperiment. The bounded approach includes data from a fixed period of\nobservation for each user (e.g. seven days after exposure) after the first time\na user became active in the experiment window.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:28:14 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Mahmoudzadeh", "Ali", ""], ["Liu", "Sophia", ""], ["Sadeghi", "Sol", ""], ["Li", "Paul Luo", ""], ["Gupta", "Somit", ""]]}, {"id": "2009.05044", "submitter": "Madhuchhanda Bhattacharjee Prof.", "authors": "Madhuchhanda Bhattacharjee and Arup Bose", "title": "Modelling COVID-19 -- I A dynamic SIR(D) with application to Indian data", "comments": "Supplementary file available on request from authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an epidemiological model using an adaptive dynamic three\ncompartment (with four states) SIR(D) model. Our approach is similar to\nnon-parametric curve fitting in spirit and automatically adapts to key external\nfactors, such as interventions, while retaining the parsimonious nature of the\nstandard SIR(D) model. Initial dynamic temporal estimates of the model\nparameters are obtained by minimising the aggregate residual sum of squares\nacross the number of infections, recoveries, and fatalities, over a chosen lag\nperiod. Then a geometric smoother is applied to obtain the final time series of\nestimates. These estimates are used to obtain dynamic temporal robust estimates\nof the key feature of this pandemic, namely the \"reproduction number\". We\nillustrate our method on the Indian COVID-19 data for the period March 14 -\nAugust 31, 2020. The time series data plots of the 36 states and union\nterritories shows a clear presence of inter-regional variation in the prognosis\nof the epidemic. This is also bourne out by the estimates of the underlying\nparameters, including the reproduction numbers for the 36 regions. Due to this,\nan SIR(D) model, dynamic or otherwise, on the national aggregate data is not\nsuited for robust local predictions. The time series of estimates of the model\nenables us to carry out daily, weekly and also long term predictions, including\nconstruction of predictive bands. We obtain an excellent agreement between the\nactual data and the model predicted data at the regional level. Our estimates\nof the current reproduction number turn out to be more than 2 in three regions\n(Andhra Pradesh, Maharashtra and Uttar Pradesh) and between 1.5 and 2 in 13\nregions. Each of these regions have experienced an individual trajectory, which\ntypically involves initial phase of shock(s) followed by a relatively steady\nlower level of the reproduction number.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:26:08 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Bhattacharjee", "Madhuchhanda", ""], ["Bose", "Arup", ""]]}, {"id": "2009.05142", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Piotr Majer, Peter N.C. Mohr, Hauke R. Heekeren, Wolfgang K. H\\\"ardle", "title": "Portfolio Decisions and Brain Reactions via the CEAD method", "comments": null, "journal-ref": "Psychometrika Vol. 81, No. 3, 881-903 (2016)", "doi": "10.1007/s11336-015-9441-5", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision making can be a complex process requiring the integration of several\nattributes of choice options. Understanding the neural processes underlying\n(uncertain) investment decisions is an important topic in neuroeconomics. We\nanalyzed functional magnetic resonance imaging (fMRI) data from an investment\ndecision study for stimulus-related effects. We propose a new technique for\nidentifying activated brain regions: Cluster, Estimation, Activation and\nDecision (CEAD) method. Our analysis is focused on clusters of voxels rather\nthan voxel units. Thus, we achieve a higher signal to noise ratio within the\nunit tested and a smaller number of hypothesis tests compared with the often\nused General Linear Model (GLM). We propose to first conduct the brain\nparcellation by applying spatially constrained spectral clustering. The\ninformation within each cluster can then be extracted by the flexible Dynamic\nSemiparametric Factor Model (DSFM) dimension reduction technique and finally be\ntested for differences in activation between conditions. This sequence of\nCluster, Estimation, Activation and Decision admits a model-free analysis of\nthe local fMRI signal. Applying a GLM on the DSFM-based time series resulted in\na significant correlation between the risk of choice options and changes in\nfMRI signal in the anterior insula and dorsomedial prefrontal cortex.\nAdditionally, individual differences in decision-related reactions within the\nDSFM time series predicted individual differences in risk attitudes as modeled\nwith the framework of the mean-variance model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 20:38:52 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Majer", "Piotr", ""], ["Mohr", "Peter N. C.", ""], ["Heekeren", "Hauke R.", ""], ["H\u00e4rdle", "Wolfgang K.", ""]]}, {"id": "2009.05171", "submitter": "David Thompson", "authors": "David M. Thompson", "title": "Power and sample size for cluster randomized and stepped wedge trials:\n  Comparing estimates obtained by applying design effects or by direct\n  estimation in GLMM", "comments": "43 pages, 3 figures, 2 tables Article replaced on 3-12-2021 to\n  include less restrictive, more general SAS syntax on pages 6 and 40; results\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When observations are independent, formulae and software are readily\navailable to plan and design studies of appropriate size and power to detect\nimportant associations. When observations are correlated or clustered, results\nobtained from the standard software require adjustment. This tutorial compares\ntwo approaches, using examples that illustrate various designs for both\nindependent and clustered data.\n  One approach obtains initial estimates using software that assume\nindependence among observations, then adjusts these estimates using a design\neffect (DE), also called a variance inflation factor (VIF). A second approach\ngenerates estimates using generalized linear mixed models (GLMM) that account\ndirectly for patterns of clustering and correlation.\n  The two approaches generally produce similar estimates and so validate one\nanother. For certain clustered designs, small differences in power estimates\nemphasize the importance of specifying an alternative hypothesis in terms of\nmeans but also in terms of expected variances and covariances. Both approaches\nto power estimation are sensitive to assumptions concerning the structure or\npattern of independence or correlation among clustered outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 22:53:10 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 12:02:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Thompson", "David M.", ""]]}, {"id": "2009.05198", "submitter": "Santosh Kumar Radha", "authors": "Santosh Kumar Radha", "title": "Information flow in political elections: a stochastic perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often times, a candidate's attractiveness is directly associated with his\nclear ideologies and opinions on various policies and social issues. Using the\nideas of stochastic differential equations and Ornstein-Uhlenbeck Process, we\ndevelop a phenomenological model to understand the effect of (un)clearly\ncommunicating a candidate's stance on policies to the voting public. We will\nshow that, counter intuitively, there are quantifiable advantages to be vague\non one's stance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 02:06:07 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Radha", "Santosh Kumar", ""]]}, {"id": "2009.05203", "submitter": "Chad Babcock", "authors": "Chad Babcock and Andrew O. Finley and Nathaniel Looker", "title": "A Bayesian hierarchical model to estimate land surface phenology\n  parameters with harmonized Landsat 8 and Sentinel-2 images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian Land Surface Phenology (LSP) model and examine its\nperformance using Enhanced Vegetation Index (EVI) observations derived from the\nHarmonized Landsat Sentinel-2 (HLS) dataset. Building on previous work, we\npropose a double logistic function that, once couched within a Bayesian model,\nyields posterior distributions for all LSP parameters. We assess the efficacy\nof the Normal, Truncated Normal, and Beta likelihoods to deliver robust LSP\nparameter estimates. Two case studies are presented and used to explore aspects\nof the proposed model. The first, conducted over forested pixels within a HLS\ntile, explores choice of likelihood and space-time varying HLS data\navailability for long-term average LSP parameter point and uncertainty\nestimation. The second, conducted on a small area of interest within the HLS\ntile on an annual time-step, further examines the impact of sample size and\nchoice of likelihood on LSP parameter estimates. Results indicate that while\nthe Truncated Normal and Beta likelihoods are theoretically preferable when the\nvegetation index is bounded, all three likelihoods performed similarly when the\nnumber of index observations is sufficiently large and values are not near the\nindex bounds. Both case studies demonstrate how pixel-level LSP parameter\nposterior distributions can be used to propagate uncertainty through subsequent\nanalysis. As a companion to this article, we provide an open-source \\R package\n\\pkg{rsBayes} and supplementary data and code used to reproduce the analysis\nresults. The proposed model specification and software implementation delivers\ncomputationally efficient, statistically robust, and inferentially rich LSP\nparameter posterior distributions at the pixel-level across massive raster time\nseries datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 02:27:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Babcock", "Chad", ""], ["Finley", "Andrew O.", ""], ["Looker", "Nathaniel", ""]]}, {"id": "2009.05304", "submitter": "Luca Ganassali", "authors": "M. Akian, L. Ganassali, S. Gaubert, L. Massouli\\'e", "title": "Probabilistic and mean-field model of COVID-19 epidemics with user\n  mobility and contact tracing", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a detailed discrete-time model of COVID-19 epidemics coming in two\nflavours, mean-field and probabilistic. The main contribution lies in several\nextensions of the basic model that capture i) user mobility - distinguishing\nrouting, i.e. change of residence, from commuting, i.e. daily mobility - and\nii) contact tracing procedures. We confront this model to public data on daily\nhospitalizations, and discuss its application as well as underlying estimation\nprocedures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:29:07 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Akian", "M.", ""], ["Ganassali", "L.", ""], ["Gaubert", "S.", ""], ["Massouli\u00e9", "L.", ""]]}, {"id": "2009.05319", "submitter": "Sabarinath Vinod Nair", "authors": "Sabarinath Vinod Nair, Shreya Sharma and Swarnava Ghosh", "title": "A Study on the Possible Effects of the Implementation of the Nordic\n  Model in India on Crime Rates and Sexually Transmitted Diseases", "comments": "9 pages, 8 tables, 3 figures, Presented the research paper in a\n  National Conference on Theoretical and Applied Statistics held at Kristu\n  Jayanti College, Bengaluru. (January 2020), The Research paper was presented\n  at the 'Science Exhibition' held at CHRIST (Deemed to be University) and\n  secured the Second Runners' up. (March 2020)", "journal-ref": "Volume 5 Issue 9, September-2020", "doi": null, "report-no": "IJSDR2009049", "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostitution is one of the root causes of sex trafficking and the\ntransmission of sexual diseases. The rules and regulations followed by the\nIndian government to regulate the same, fall under the umbrella of the\nabolitionism model. Neo-abolitionism (also known as the Nordic model) is a new\nlegislative model that has been introduced by the Nordic countries to regulate\nprostitution. The purpose of this research paper is to examine the possible\neffects of the application of the Nordic model on the crime rates and the\nspread of sexually transmitted diseases in India. Further, we also aim to study\nthe effects of the implementation of Neo-abolitionism in Sweden.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:05:26 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Nair", "Sabarinath Vinod", ""], ["Sharma", "Shreya", ""], ["Ghosh", "Swarnava", ""]]}, {"id": "2009.05417", "submitter": "Antonio P. Ramos", "authors": "Antonio P. Ramos and Martin J. Flores and Leiwen Gao and Patrick\n  Heuveline and Robert E. Weiss", "title": "Explaining the Decline of Child Mortality in 44 Developing Countries: A\n  Bayesian Extension of Oaxaca Decomposition Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the decline of infant mortality in 42 low and middle income\ncountries (LMIC) using detailed micro data from 84 Demographic and Health\nSurveys. We estimate infant mortality risk for each infant in our data and\ndevelop a novel extension of Oaxaca decomposition to understand the sources of\nthese changes. We find that the decline in infant mortality is due to a\ndeclining propensity for parents with given characteristics to experience the\ndeath of an infant rather than due to changes in the distributions of these\ncharacteristics over time. Our results suggest that technical progress and\npolicy health interventions in the form of public goods are the main drivers of\nthe the recent decline in infant mortality in LMIC.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:04:28 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Ramos", "Antonio P.", ""], ["Flores", "Martin J.", ""], ["Gao", "Leiwen", ""], ["Heuveline", "Patrick", ""], ["Weiss", "Robert E.", ""]]}, {"id": "2009.05418", "submitter": "Calum Hand", "authors": "James Hook, Calum Hand, Emma Whitfield", "title": "Bayesian Screening: Multi-test Bayesian Optimization Applied to in\n  silico Material Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present new multi-test Bayesian optimization models and algorithms for use\nin large scale material screening applications. Our screening problems are\ndesigned around two tests, one expensive and one cheap. This paper differs from\nother recent work on multi-test Bayesian optimization through use of a flexible\nmodel that allows for complex, non-linear relationships between the cheap and\nexpensive test scores. This additional modeling flexibility is essential in the\nmaterial screening applications which we describe. We demonstrate the power of\nour new algorithms on a family of synthetic toy problems as well as on real\ndata from two large scale screening studies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:07:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Hook", "James", ""], ["Hand", "Calum", ""], ["Whitfield", "Emma", ""]]}, {"id": "2009.05422", "submitter": "Peng Liu", "authors": "Peng Liu, Ying Chen, Chung-Piaw Teo", "title": "Limousine Service Management: Capacity Planning with Predictive\n  Analytics and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limousine service in luxury hotels is an integral component of the whole\ncustomer journey in the hospitality industry. One of the largest hotels in\nSingapore manages a fleet of both in-house and outsourced vehicles around the\nclock, serving 9000 trips per month on average. The need for vehicles may scale\nup rapidly, especially during special events and festive periods in the\ncountry. The excess demand is met by having additional outsourced vehicles on\nstandby, incurring millions of dollars of additional expenses per year for the\nhotel. Determining the required number of limousines by hour of the day is a\nchallenging service capacity planning problem. In this paper, a recent\ntransformational journey to manage this problem in the hotel is introduced,\ndriving up to S\\$3.2 million of savings per year with improved service level.\nThe approach builds on widely available open-source statistical and spreadsheet\noptimization tools, along with robotic process automation, to optimize the\nschedule of its fleet of limousines and drivers, and to support decision-making\nfor planners/controllers to drive sustained business value.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:15:31 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Liu", "Peng", ""], ["Chen", "Ying", ""], ["Teo", "Chung-Piaw", ""]]}, {"id": "2009.05446", "submitter": "Jonathan Baxter", "authors": "Jonathan Baxter", "title": "Bayesian Beta-Binomial Prevalence Estimation Using an Imperfect Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following [Diggle 2011, Greenland 1995], we give a simple formula for the\nBayesian posterior density of a prevalence parameter based on unreliable\ntesting of a population. This problem is of particular importance when the\nfalse positive test rate is close to the prevalence in the population being\ntested. An efficient Monte Carlo algorithm for approximating the posterior\ndensity is presented, and applied to estimating the Covid-19 infection rate in\nSanta Clara county, CA using the data reported in [Bendavid 2020]. We show that\nthe true Bayesian posterior places considerably more mass near zero, resulting\nin a prevalence estimate of 5,000--70,000 infections (median: 42,000) (2.17%\n(95CI 0.27%--3.63%)), compared to the estimate of 48,000--81,000 infections\nderived in [Bendavid 2020] using the delta method.\n  A demonstration, with code and additional examples, is available at\ntestprev.com.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:49:33 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Baxter", "Jonathan", ""]]}, {"id": "2009.05482", "submitter": "Vartan Choulakian", "authors": "J. Allard, S. Champigny, V. Choulakian, S. Mahdi", "title": "TCA and TLRA: A comparison on contingency tables and compositional data", "comments": "22 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two popular general approaches for the analysis and visualization\nof a contingency table and a compositional data set: Correspondence analysis\n(CA) and log ratio analysis (LRA). LRA includes two independently well\ndeveloped methods: association models and compositional data analysis. The\napplication of either CA or LRA to a contingency table or to compositional data\nset includes a preprocessing centering step. In CA the centering step is\nmultiplicative, while in LRA it is log bi-additive. A preprocessed matrix is\ndouble-centered, so it is a residuel matrix; which implies that it affects the\nfinal results of the analysis. This paper introduces a novel index named the\nintrinsic measure of the quality of the signs of the residuals (QSR) for the\nchoice of the preprocessing, and consequently of the method. The criterion is\nbased on taxicab singular value decomposition (TSVD) on which the package\nTaxicabCA in R is developed. We present a minimal R script that can be executed\nto obtain the numerical results and the maps in this paper. Three relatively\nsmall sized data sets available freely on the web are used as examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:00:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Allard", "J.", ""], ["Champigny", "S.", ""], ["Choulakian", "V.", ""], ["Mahdi", "S.", ""]]}, {"id": "2009.05642", "submitter": "Paul Parker", "authors": "Paul A. Parker, Scott H. Holan, and Ryan Janicki", "title": "Computationally Efficient Bayesian Unit-Level Models for Non-Gaussian\n  Data Under Informative Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical estimates from survey samples have traditionally been obtained\nvia design-based estimators. In many cases, these estimators tend to work well\nfor quantities such as population totals or means, but can fall short as sample\nsizes become small. In today's \"information age,\" there is a strong demand for\nmore granular estimates. To meet this demand, using a Bayesian\npseudo-likelihood, we propose a computationally efficient unit-level modeling\napproach for non-Gaussian data collected under informative sampling designs.\nSpecifically, we focus on binary and multinomial data. Our approach is both\nmultivariate and multiscale, incorporating spatial dependence at the\narea-level. We illustrate our approach through an empirical simulation study\nand through a motivating application to health insurance estimates using the\nAmerican Community Survey.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 19:53:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""], ["Janicki", "Ryan", ""]]}, {"id": "2009.05830", "submitter": "Jeffrey Simonoff", "authors": "Arjun Goyal and Jeffrey S. Simonoff", "title": "Hot Racquet or Not? An Exploration of Momentum in Grand Slam Tennis\n  Matches", "comments": "This submission has been removed by arXiv administrators because the\n  submitter did not have the right to agree to the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of momentum in sports, where the outcome of a previous event\naffects a following event, is a belief often held by fans, and has been the\nsubject of statistical study for many sports. This paper investigates the\npresence and manifestation of momentum in Grand Slam tennis matches from 2014\nthrough 2019 for men and women, to see if there is evidence of any carryover\neffect from the outcome of previous point(s)/game(s)/set(s) to the current one\nthat cannot be accounted for by player quality, fitness or fatigue. Generalized\nlinear mixed effect models (GLMMs) are used to explore the effects of the\noutcomes of previous sets, games, or points on the odds of winning a set, game,\nor point, while incorporating control variables that account for differences in\nplayer quality and the current status of the match. We find strong evidence of\ncarryover effects at the set, game, and point level. Holding one's serve in\nprior service games is strongly related to winning a current game, but losing a\npast game is associated with higher estimated odds of winning the next game.\nWinning the previous two or three points in a row is associated with highest\nestimated odds of winning the next point.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 17:17:34 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Goyal", "Arjun", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "2009.06059", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston", "title": "Functional random effects modeling of brain shape and connectivity", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical framework that jointly models brain shape and\nfunctional connectivity, which are two complex aspects of the brain that have\nbeen classically studied independently. We adopt a Riemannian modeling approach\nto account for the non-Euclidean geometry of the space of shapes and the space\nof connectivity that constrains trajectories of co-variation to be valid\nstatistical estimates. In order to disentangle genetic sources of variability\nfrom those driven by unique environmental factors, we embed a functional random\neffects model in the Riemannian framework. We apply the proposed model to the\nHuman Connectome Project dataset to explore spontaneous co-variation between\nbrain shape and connectivity in young healthy individuals.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:33:25 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:30:50 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""]]}, {"id": "2009.06183", "submitter": "Andrew Herren", "authors": "Andrew Herren, P. Richard Hahn", "title": "Semi-supervised learning and the question of true versus estimated\n  propensity scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A straightforward application of semi-supervised machine learning to the\nproblem of treatment effect estimation would be to consider data as \"unlabeled\"\nif treatment assignment and covariates are observed but outcomes are\nunobserved. According to this formulation, large unlabeled data sets could be\nused to estimate a high dimensional propensity function and causal inference\nusing a much smaller labeled data set could proceed via weighted estimators\nusing the learned propensity scores. In the limiting case of infinite unlabeled\ndata, one may estimate the high dimensional propensity function exactly.\nHowever, longstanding advice in the causal inference community suggests that\nestimated propensity scores (from labeled data alone) are actually preferable\nto true propensity scores, implying that the unlabeled data is actually useless\nin this context. In this paper we examine this paradox and propose a simple\nprocedure that reconciles the strong intuition that a known propensity\nfunctions should be useful for estimating treatment effects with the previous\nliterature suggesting otherwise. Further, simulation studies suggest that\ndirect regression may be preferable to inverse-propensity weight estimators in\nmany circumstances.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:13:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Herren", "Andrew", ""], ["Hahn", "P. Richard", ""]]}, {"id": "2009.06214", "submitter": "Xing He", "authors": "Xing He, Robert Qiu, Qian Ai, Tianyi Zhu", "title": "A Hybrid Framework for Topology Identification of Distribution Grid with\n  Renewables Integration", "comments": "11 pages. Accepted by IEEE TPWRS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology identification (TI) is a key task for state estimation (SE) in\ndistribution grids, especially the one with high-penetration renewables. The\nuncertainties, initiated by the time-series behavior of renewables, will almost\ncertainly lead to bad TI results without a proper treatment. These\nuncertainties are analytically intractable under conventional framework--they\nare usually jointly spatial-temporal dependent, and hence cannot be simply\ntreated as white noise. For this purpose, a hybrid framework is suggested in\nthis paper to handle these uncertainties in a systematic and theoretical way;\nin particular, big data analytics are studied to harness the jointly\nspatial-temporal statistical properties of those uncertainties. With some prior\nknowledge, a model bank is built first to store the countable typical models of\nnetwork configurations; therefore, the difference between the SE outputs of\neach bank model and our observation is capable of being defined as a matrix\nvariate--the so-called random matrix. In order to gain insight into the random\nmatrix, a well-designed metric space is needed. Auto-regression (AR) model,\nfactor analysis (FA), and random matrix theory (RMT) are tied together for the\nmetric space design, followed by jointly temporal-spatial analysis of those\nmatrices which is conducted in a high-dimensional (vector) space. Under the\nproposed framework, some big data analytics and theoretical results are\nobtained to improve the TI performance. Our framework is validated using IEEE\nstandard distribution network with some field data in practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 06:09:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["He", "Xing", ""], ["Qiu", "Robert", ""], ["Ai", "Qian", ""], ["Zhu", "Tianyi", ""]]}, {"id": "2009.06229", "submitter": "Sourabh Bhattacharya", "authors": "Sucharita Roy and Sourabh Bhattacharya", "title": "Bayesian Appraisal of Random Series Convergence with Application to\n  Climate Change", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roy and Bhattacharya (2020) provided Bayesian characterization of infinite\nseries, and their most important application, namely, to the Dirichlet series\ncharacterizing the (in)famous Riemann Hypothesis, revealed insights that are\nnot in support of the most celebrated conjecture for over 150 years.\n  In contrast with deterministic series considered by Roy and Bhattacharya\n(2020), in this article we take up random infinite series for our\ninvestigation. Remarkably, our method does not require any simplifying\nassumption. Albeit the Bayesian characterization theory for random series is no\ndifferent from that for the deterministic setup, construction of effective\nupper bounds for partial sums, required for implementation, turns out to be a\nchallenging undertaking in the random setup. In this article, we construct\nparametric and nonparametric upper bound forms for the partial sums of random\ninfinite series and demonstrate the generality of the latter in comparison to\nthe former. Simulation studies exhibit high accuracy and efficiency of the\nnonparametric bound in all the setups that we consider.\n  Finally, exploiting the property that the summands tend to zero in the case\nof series convergence, we consider application of our nonparametric bound\ndriven Bayesian method to global climate change analysis. Specifically,\nanalyzing the global average temperature record over the years 1850--2016 and\nHolocene global average temperature reconstruction data 12,000 years before\npresent, we conclude, in spite of the current global warming situation, that\nglobal climate dynamics is subject to temporary variability only, the current\nglobal warming being an instance, and long term global warming or cooling\neither in the past or in the future, are highly unlikely.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 07:04:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Roy", "Sucharita", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2009.06257", "submitter": "Kumiko Tanaka-Ishii", "authors": "Kumiko Tanaka-Ishii and Shuntaro Takahashi", "title": "A Comparison of Two Fluctuation Analyses for Natural Language Clustering\n  Phenomena: Taylor and Ebeling & Neiman Methods", "comments": null, "journal-ref": "Fractals, in 2021, No.2.\n  https://www.worldscientific.com/toc/fractals/0/ja", "doi": "10.1142/S0218348X2150033X", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the fluctuation analysis methods of Taylor and Ebeling\n& Neiman. While both have been applied to various phenomena in the statistical\nmechanics domain, their similarities and differences have not been clarified.\nAfter considering their analytical aspects, this article presents a large-scale\napplication of these methods to text. It is found that both methods can\ndistinguish real text from independently and identically distributed (i.i.d.)\nsequences. Furthermore, it is found that the Taylor exponents acquired from\nwords can roughly distinguish text categories; this is also the case for\nEbeling and Neiman exponents, but to a lesser extent. Additionally, both\nmethods show some possibility of capturing script kinds.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:30:24 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tanaka-Ishii", "Kumiko", ""], ["Takahashi", "Shuntaro", ""]]}, {"id": "2009.06305", "submitter": "Zhuozhao Zhan", "authors": "Edwin R. van den Heuvel, Osama Almalik, Zhuozhao Zhan", "title": "Simulation Models for Aggregated Data Meta-Analysis: Evaluation of\n  Pooling Effect Sizes and Publication Biases", "comments": "23 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies are commonly used to evaluate the performance of newly\ndeveloped meta-analysis methods. For methodology that is developed for an\naggregated data meta-analysis, researchers often resort to simulation of the\naggregated data directly, instead of simulating individual participant data\nfrom which the aggregated data would be calculated in reality. Clearly,\ndistributional characteristics of the aggregated data statistics may be derived\nfrom distributional assumptions of the underlying individual data, but they are\noften not made explicit in publications. This paper provides the distribution\nof the aggregated data statistics that were derived from a heteroscedastic\nmixed effects model for continuous individual data. As a result, we provide a\nprocedure for directly simulating the aggregated data statistics. We also\ncompare our distributional findings with other simulation approaches of\naggregated data used in literature by describing their theoretical differences\nand by conducting a simulation study for three meta-analysis methods:\nDerSimonian and Laird's pooled estimate and the Trim & Fill and PET-PEESE\nmethod for adjustment of publication bias. We demonstrate that the choices of\nsimulation model for aggregated data may have a relevant impact on (the\nconclusions of) the performance of the meta-analysis method. We recommend the\nuse of multiple aggregated data simulation models for investigation of new\nmethodology to determine sensitivity or otherwise make the individual\nparticipant data model explicit that would lead to the distributional choices\nof the aggregated data statistics used in the simulation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:07:40 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Heuvel", "Edwin R. van den", ""], ["Almalik", "Osama", ""], ["Zhan", "Zhuozhao", ""]]}, {"id": "2009.06379", "submitter": "Anh Nguyen Duc", "authors": "Anh Nguyen Duc, Dominik Heinzmann, Claude Berge and Marcel Wolbers", "title": "A pragmatic adaptive enrichment design for selecting the right target\n  population for cancer immunotherapies", "comments": "10 pages, 1 figures", "journal-ref": null, "doi": "10.1002/pst.2066", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in the design of confirmatory trials is to deal with\nuncertainties regarding the optimal target population for a novel drug.\nAdaptive enrichment designs (AED) which allow for a data-driven selection of\none or more pre-specified biomarker subpopulations at an interim analysis have\nbeen proposed in this setting but practical case studies of AEDs are still\nrelatively rare. We present the design of an AED with a binary endpoint in the\nhighly dynamic setting of cancer immunotherapy. The trial was initiated as a\nconventional trial in early triple-negative breast cancer but amended to an AED\nbased on emerging data external to the trial suggesting that PD-L1 status could\nbe a predictive biomarker. Operating characteristics are discussed including\nthe concept of a minimal detectable difference, that is, the smallest observed\ntreatment effect that would lead to a statistically significant result in at\nleast one of the target populations at the interim or the final analysis,\nrespectively, in the setting of AED.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:46:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Duc", "Anh Nguyen", ""], ["Heinzmann", "Dominik", ""], ["Berge", "Claude", ""], ["Wolbers", "Marcel", ""]]}, {"id": "2009.06403", "submitter": "Annika Pick", "authors": "Annika Pick, Sebastian Ginzel, Stefan R\\\"uping, Jil Sander, Ann\n  Christina Foldenauer, Michaela K\\\"ohm", "title": "Aligning Subjective Ratings in Clinical Decision Making", "comments": "Accepted at the ECML 2020 workshop on Machine Learning for Pharma and\n  Healthcare Applications (PharML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to objective indicators (e.g. laboratory values), clinical data\noften contain subjective evaluations by experts (e.g. disease severity\nassessments). While objective indicators are more transparent and robust, the\nsubjective evaluation contains a wealth of expert knowledge and intuition. In\nthis work, we demonstrate the potential of pairwise ranking methods to align\nthe subjective evaluation with objective indicators, creating a new score that\ncombines their advantages and facilitates diagnosis. In a case study on\npatients at risk for developing Psoriatic Arthritis, we illustrate that the\nresulting score (1) increases classification accuracy when detecting disease\npresence/absence, (2) is sparse and (3) provides a nuanced assessment of\nseverity for subsequent analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 12:32:35 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Pick", "Annika", ""], ["Ginzel", "Sebastian", ""], ["R\u00fcping", "Stefan", ""], ["Sander", "Jil", ""], ["Foldenauer", "Ann Christina", ""], ["K\u00f6hm", "Michaela", ""]]}, {"id": "2009.06501", "submitter": "Caterina May", "authors": "Jesus Lopez-Fidalgo, Caterina May, Jose Antonio Moler", "title": "Designing experiments for estimating an appropriate outlet size for a\n  silo type problem", "comments": "11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of jam formation during the discharge by gravity of granular\nmaterial through a two-dimensional silo has a number of practical applications.\nIn many problems the estimation of the minimum outlet size which guarantees\nthat the time to the next jamming event is long enough is crucial. Assuming\nthat the time is modeled by an exponential distribution with two unknown\nparameters, this goal translates to the optimal estimation of a non-linear\ntransformation of the parameters. We obtain $c$-optimum experimental designs\nwith that purpose, applying the graphic Elfving method. Since the optimal\ndesigns depend on the nominal values of the parameters, a sensitivity study is\nadditionally provided. Finally, a simulation study checks the performance of\nthe approximations made, first with the Fisher Information matrix, then with\nthe linearization of the function to be estimated. The results are useful for\nexperimenting in a laboratory and translating then the results to a larger\nscenario. Apart from the application a general methodology is developed in the\npaper for the problem of precise estimation of a one-dimensional parametric\ntransformation in a non-linear model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:04:54 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lopez-Fidalgo", "Jesus", ""], ["May", "Caterina", ""], ["Moler", "Jose Antonio", ""]]}, {"id": "2009.06527", "submitter": "Joseph De Vilmarest", "authors": "David Obst, Joseph de Vilmarest, Yannig Goude", "title": "Adaptive Methods for Short-Term Electricity Load Forecasting During\n  COVID-19 Lockdown in France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) pandemic has urged many governments\nin the world to enforce a strict lockdown where all nonessential businesses are\nclosed and citizens are ordered to stay at home. One of the consequences of\nthis policy is a significant change in electricity consumption patterns. Since\nload forecasting models rely on calendar or meteorological information and are\ntrained on historical data, they fail to capture the significant break caused\nby the lockdown and have exhibited poor performances since the beginning of the\npandemic. This makes the scheduling of the electricity production challenging,\nand has a high cost for both electricity producers and grid operators. In this\npaper we introduce adaptive generalized additive models using Kalman filters\nand fine-tuning to adjust to new electricity consumption patterns.\nAdditionally, knowledge from the lockdown in Italy is transferred to anticipate\nthe change of behavior in France. The proposed methods are applied to forecast\nthe electricity demand during the French lockdown period, where they\ndemonstrate their ability to significantly reduce prediction errors compared to\ntraditional models. Finally expert aggregation is used to leverage the\nspecificities of each predictions and enhance results even further.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:41:36 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Obst", "David", ""], ["de Vilmarest", "Joseph", ""], ["Goude", "Yannig", ""]]}, {"id": "2009.06615", "submitter": "Ales Zahorski", "authors": "Ales Zahorski", "title": "Multilevel regression with poststratification for the national level\n  Viber/Street poll on the 2020 presidential election in Belarus", "comments": "45 pages, 23 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent sociological polls are forbidden in Belarus. Online polls\nperformed without sound scientific rigour do not yield representative results.\nYet, both inside and outside Belarus it is of great importance to obtain\nprecise estimates of the ratings of all candidates. These ratings could\nfunction as reliable proxies for the election's outcomes. We conduct an\nindependent poll based on the combination of the data collected via Viber and\non the streets of Belarus. The Viber and the street data samples consist of\nalmost 45000 and 1150 unique observations respectively. Bayesian regressions\nwith poststratification were build to estimate ratings of the candidates and\nrates of early voting turnout for the population as a whole and within various\nfocus subgroups. We show that both the officially announced results of the\nelection and early voting rates are highly improbable. With a probability of at\nleast 95%, Sviatlana Tikhanouskaya's rating lies between 75% and 80%, whereas\nAliaksandr Lukashenka's rating lies between 13% and 18% and early voting rate\npredicted by the method ranges from 9% to 13% of those who took part in the\nelection. These results contradict the officially announced outcomes, which are\n10.12%, 80.11%, and 49.54% respectively and lie far outside even the 99.9%\ncredible intervals predicted by our model. The only marginal groups of people\nwhere the upper bounds of the 99.9% credible intervals of the rating of\nLukashenka are above 50% are people older than 60 and uneducated people. For\nall other marginal subgroups, including rural residents, even the upper bounds\nof 99.9% credible intervals for Lukashenka are far below 50%. The same is true\nfor the population as a whole. Thus, with a probability of at least 99.9%\nLukashenka could not have had enough electoral support to win the 2020\npresidential election in Belarus.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:55:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zahorski", "Ales", ""]]}, {"id": "2009.06670", "submitter": "Lawrence Bardwell", "authors": "Alexander T. M. Fisch, Lawrence Bardwell and Idris A. Eckley", "title": "Real Time Anomaly Detection And Categorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to quickly and accurately detect anomalous structure within data\nsequences is an inference challenge of growing importance. This work extends\nrecently proposed post-hoc (offline) anomaly detection methodology to the\nsequential setting. The resultant procedure is capable of real-time analysis\nand categorisation between baseline and two forms of anomalous structure: point\nand collective anomalies. Various theoretical properties of the procedure are\nderived. These, together with an extensive simulation study, highlight that the\naverage run length to false alarm and the average detection delay of the\nproposed online algorithm are very close to that of the offline version.\nExperiments on simulated and real data are provided to demonstrate the benefits\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:11:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Bardwell", "Lawrence", ""], ["Eckley", "Idris A.", ""]]}, {"id": "2009.06699", "submitter": "Kathrin M\\\"ollenhoff", "authors": "Kathrin M\\\"ollenhoff, Achim Tresch", "title": "Survival analysis under non-proportional hazards: investigating\n  non-inferiority or equivalence in time-to-event data", "comments": "Supplemental Material available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to analyze time-to-event data, e.g. in clinical\ntrials, is to fit Kaplan-Meier curves yielding the treatment effect as the\nhazard ratio between treatment groups. Afterwards commonly a log-rank test is\nperformed in order to investigate whether there is a difference in survival,\nor, depending on additional covariates, a Cox proportional hazard model is\nused. However, in numerous trials these approaches fail due to the presence of\nnon-proportional hazards, resulting in difficulties of interpreting the hazard\nratio and a loss of power. When considering equivalence or non-inferiority\ntrials, the commonly performed log-rank based tests are similarly affected by a\nviolation of this assumption. Here we propose a parametric framework to assess\nequivalence or non-inferiority for survival data. We derive pointwise\nconfidence bands for both, the hazard ratio and the difference of the survival\ncurves. Further we propose a test procedure addressing non-inferiority and\nequivalence by directly comparing the survival functions at certain time points\nor over an entire range of time. We demonstrate the validity of the methods by\na clinical trial example and by numerous simulation results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:19:00 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["M\u00f6llenhoff", "Kathrin", ""], ["Tresch", "Achim", ""]]}, {"id": "2009.06750", "submitter": "Niander Assis", "authors": "Niander Assis, Renato Assun\\c{c}\\~ao and Pedro O. S. Vaz-De-Melo", "title": "Stop the Clock: Are Timeout Effects Real?", "comments": "Accepted at ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timeout is a short interruption during games used to communicate a change in\nstrategy, to give the players a rest or to stop a negative flow in the game.\nWhatever the reason, coaches expect an improvement in their team's performance\nafter a timeout. But how effective are these timeouts in doing so? The simple\naverage of the differences between the scores before and after the timeouts has\nbeen used as evidence that there is an effect and that it is substantial. We\nclaim that these statistical averages are not proper evidence and a more sound\napproach is needed. We applied a formal causal framework using a large dataset\nof official NBA play-by-play tables and drew our assumptions about the data\ngeneration process in a causal graph. Using different matching techniques to\nestimate the causal effect of timeouts, we concluded that timeouts have no\neffect on teams' performances. Actually, since most timeouts are called when\nthe opposing team is scoring more frequently, the moments that follow resemble\nan improvement in the team's performance but are just the natural game tendency\nto return to its average state. This is another example of what statisticians\ncall the regression to the mean phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 21:21:41 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Assis", "Niander", ""], ["Assun\u00e7\u00e3o", "Renato", ""], ["Vaz-De-Melo", "Pedro O. S.", ""]]}, {"id": "2009.06935", "submitter": "Pallavi Basu", "authors": "Pallavi Basu and Dylan S. Small", "title": "Constructing a More Closely Matched Control Group in a\n  Difference-in-Differences Analysis: Its Effect on History Interacting with\n  Group Bias", "comments": "Accepted to Observational Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences analysis with a control group that differs\nconsiderably from a treated group is vulnerable to bias from historical events\nthat have different effects on the groups. Constructing a more closely matched\ncontrol group by matching a subset of the overall control group to the treated\ngroup may result in less bias. We study this phenomenon in simulation studies.\nWe study the effect of mountaintop removal mining (MRM) on mortality using a\ndifference-in-differences analysis that makes use of the increase in MRM\nfollowing the 1990 Clean Air Act Amendments. For a difference-in-differences\nanalysis of the effect of MRM on mortality, we constructed a more closely\nmatched control group and found a 95\\% confidence interval that contains\nsubstantial adverse effects along with no effect and small beneficial effects.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 09:08:31 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Basu", "Pallavi", ""], ["Small", "Dylan S.", ""]]}, {"id": "2009.07103", "submitter": "Aditya Singh", "authors": "Aditya Singh, Akram Mohammed, Lokesh Chinthala, Rishikesan\n  Kamaleswaran", "title": "Machine learning predicts early onset of fever from continuous\n  physiological data of critically ill patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fever can provide valuable information for diagnosis and prognosis of various\ndiseases such as pneumonia, dengue, sepsis, etc., therefore, predicting fever\nearly can help in the effectiveness of treatment options and expediting the\ntreatment process. This study aims to develop novel algorithms that can\naccurately predict fever onset in critically ill patients by applying machine\nlearning technique on continuous physiological data. We analyzed continuous\nphysiological data collected every 5-minute from a cohort of over 200,000\ncritically ill patients admitted to an Intensive Care Unit (ICU) over a 2-year\nperiod. Each episode of fever from the same patient were considered as an\nindependent event, with separations of at least 24 hours. We extracted\ndescriptive statistical features from six physiological data streams, including\nheart rate, respiration, systolic and diastolic blood pressure, mean arterial\npressure, and oxygen saturation, and use these features to independently\npredict the onset of fever. Using a bootstrap aggregation method, we created a\nbalanced dataset of 7,801 afebrile and febrile patients and analyzed features\nup to 4 hours before the fever onset. We found that supervised machine learning\nmethods can predict fever up to 4 hours before onset in critically ill patients\nwith high recall, precision, and F1-score. This study demonstrates the\nviability of using machine learning to predict fever among hospitalized adults.\nThe discovery of salient physiomarkers through machine learning and deep\nlearning techniques has the potential to further accelerate the development and\nimplementation of innovative care delivery protocols and strategies for\nmedically vulnerable patients.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:16:09 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Singh", "Aditya", ""], ["Mohammed", "Akram", ""], ["Chinthala", "Lokesh", ""], ["Kamaleswaran", "Rishikesan", ""]]}, {"id": "2009.07345", "submitter": "R. Noah Padgett", "authors": "R. Noah Padgett and Rebecca J. Tipton", "title": "Identifying latent classes with ordered categorical indicators", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Monte Carlo simulation was used to determine which assumptions for ordered\ncategorical data, continuity vs. discrete categories, most frequently\nidentifies the underlying factor structure when a response variable has five\nordered categories. The impact of infrequently endorsed response categories was\nalso examined, a condition that has not been fully explored. The typical method\nfor overcoming infrequently endorsed categories in applied research is to\ncollapse response options with adjacent categories resulting in less response\ncategories that are endorsed more frequently, but this approach may not\nnecessarily provide useful information. Response category endorsement issues\nhave been studied in Item Response Theory, but this issue has not been\naddressed in classification analyses nor has fit measure performance been\nexamined under these conditions. We found that the performance of commonly used\nfit statistics to identify the true number of latent class depends on the\nwhether continuity is assumed, sample size, and convergence. Fit statistics\nperformed best when the five response options are assumed to be categorical.\nHowever, in situations with lower sample sizes and when convergence is an\nissue, assuming continuity and using the adjusted Lo-Mendell-Rubin likelihood\nratio test may be useful.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 20:40:02 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Padgett", "R. Noah", ""], ["Tipton", "Rebecca J.", ""]]}, {"id": "2009.07356", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu and Alexander Bukharin and Liyan Xie and Mauricio\n  Santillana and Shihao Yang and Yao Xie", "title": "High-resolution Spatio-temporal Model for County-level COVID-19 Activity\n  in the U.S", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interpretable high-resolution spatio-temporal model to estimate\nCOVID-19 deaths together with confirmed cases one-week ahead of the current\ntime, at the county-level and weekly aggregated, in the United States. A\nnotable feature of our spatio-temporal model is that it considers the (a)\ntemporal auto- and pairwise correlation of the two local time series (confirmed\ncases and death of the COVID-19), (b) dynamics between locations (propagation\nbetween counties), and (c) covariates such as local within-community mobility\nand social demographic factors. The within-community mobility and demographic\nfactors, such as total population and the proportion of the elderly, are\nincluded as important predictors since they are hypothesized to be important in\ndetermining the dynamics of COVID-19. To reduce the model's\nhigh-dimensionality, we impose sparsity structures as constraints and emphasize\nthe impact of the top ten metropolitan areas in the nation, which we refer (and\ntreat within our models) as hubs in spreading the disease. Our retrospective\nout-of-sample county-level predictions were able to forecast the subsequently\nobserved COVID-19 activity accurately. The proposed multi-variate predictive\nmodels were designed to be highly interpretable, with clear identification and\nquantification of the most important factors that determine the dynamics of\nCOVID-19. Ongoing work involves incorporating more covariates, such as\neducation and income, to improve prediction accuracy and model\ninterpretability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 21:22:23 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 02:14:33 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 04:32:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhu", "Shixiang", ""], ["Bukharin", "Alexander", ""], ["Xie", "Liyan", ""], ["Santillana", "Mauricio", ""], ["Yang", "Shihao", ""], ["Xie", "Yao", ""]]}, {"id": "2009.07568", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Alona Zharova, Janine Tellinger-Rice, Wolfgang Karl H\\\"ardle", "title": "How to Measure the Performance of a Collaborative Research Center", "comments": null, "journal-ref": "Scientometrics 2018, volume 117", "doi": "10.1007/s11192-018-2910-8", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New Public Management helps universities and research institutions to perform\nin a highly competitive research environment. Evaluating publicly financed\nresearch improves transparency, helps in reflection and self-assessment, and\nprovides information for strategic decision making. In this paper we provide\nempirical evidence using data from a Collaborative Research Center (CRC) on\nfinancial inputs and research output from 2005 to 2016. After selecting\nperformance indicators suitable for a CRC, we describe main properties of the\ndata using visualization techniques. To study the relationship between the\ndimensions of research performance, we use a time fixed effects panel data\nmodel and fixed effects Poisson model. With the help of year dummy variables,\nwe show how the pattern of research productivity changes over time after\ncontrolling for staff and travel costs. The joint depiction of the time fixed\neffects and the research project's life cycle allows a better understanding of\nthe development of the number of discussion papers over time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:23:11 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zharova", "Alona", ""], ["Tellinger-Rice", "Janine", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.07594", "submitter": "Andrew Golightly", "authors": "Holly F. Fisher and Richard J. Boys and Colin S. Gillespie and Carole\n  J. Proctor and Andrew Golightly", "title": "Parameter inference for a stochastic kinetic model of expanded\n  polyglutamine proteins", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of protein aggregates in cells is a known feature of many human\nage-related diseases, such as Huntington's disease. Simulations using fixed\nparameter values in a model of the dynamic evolution of expanded polyglutamine\n(PolyQ) proteins in cells have been used to gain a better understanding of the\nbiological system, how to focus drug development and how to construct more\nefficient designs of future laboratory-based in vitro experiments. However,\nthere is considerable uncertainty about the values of some of the parameters\ngoverning the system. Currently, appropriate values are chosen by ad hoc\nattempts to tune the parameters so that the model output matches experimental\ndata. The problem is further complicated by the fact that the data only offer a\npartial insight into the underlying biological process: the data consist only\nof the proportions of cell death and of cells with inclusion bodies at a few\ntime points, corrupted by measurement error.\n  Developing inference procedures to estimate the model parameters in this\nscenario is a significant task. The model probabilities corresponding to the\nobserved proportions cannot be evaluated exactly and so they are estimated\nwithin the inference algorithm by repeatedly simulating realisations from the\nmodel. In general such an approach is computationally very expensive and we\ntherefore construct Gaussian process emulators for the key quantities and\nreformulate our algorithm around these fast stochastic approximations. We\nconclude by examining the fit of our model and highlight appropriate values of\nthe model parameters leading to new insights into the underlying biological\nprocesses such as the kinetics of aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 10:41:19 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Fisher", "Holly F.", ""], ["Boys", "Richard J.", ""], ["Gillespie", "Colin S.", ""], ["Proctor", "Carole J.", ""], ["Golightly", "Andrew", ""]]}, {"id": "2009.07701", "submitter": "Casper Solheim Bojer", "authors": "Casper Solheim Bojer and Jens Peder Meldgaard", "title": "Kaggle forecasting competitions: An overlooked learning opportunity", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2020.07.007", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitions play an invaluable role in the field of forecasting, as\nexemplified through the recent M4 competition. The competition received\nattention from both academics and practitioners and sparked discussions around\nthe representativeness of the data for business forecasting. Several\ncompetitions featuring real-life business forecasting tasks on the Kaggle\nplatform has, however, been largely ignored by the academic community. We\nbelieve the learnings from these competitions have much to offer to the\nforecasting community and provide a review of the results from six Kaggle\ncompetitions. We find that most of the Kaggle datasets are characterized by\nhigher intermittence and entropy than the M-competitions and that global\nensemble models tend to outperform local single models. Furthermore, we find\nthe strong performance of gradient boosted decision trees, increasing success\nof neural networks for forecasting, and a variety of techniques for adapting\nmachine learning models to the forecasting task.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:14:41 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Bojer", "Casper Solheim", ""], ["Meldgaard", "Jens Peder", ""]]}, {"id": "2009.07887", "submitter": "Anna Yanchenko", "authors": "Anna K. Yanchenko", "title": "Network Analysis of Orchestral Concert Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orchestral concert programming is a challenging, yet critical task for\nexpanding audience engagement and is usually driven by qualitative heuristics\nand common musical practices. Quantitative analysis of orchestral programming\nhas been limited, but has become more possible as many orchestras archive their\nperformance history online. The contribution of this work is to use statistical\nnetwork models to quantitatively explore orchestral concert programming,\nfocusing on which factors determine if two composers are programmed together in\nthe same concert by the Boston Symphony Orchestra. We find that the type of\ncomposition is the most important covariate in determining which composers are\nperformed together and the additive and multiplicative effects are logical from\nan orchestral programming perspective. These results suggest that a network\nanalysis is a promising approach for the analysis of concert programming, with\nseveral directions for future extensions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:37:12 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Yanchenko", "Anna K.", ""]]}, {"id": "2009.07955", "submitter": "Mohammad Noorbakhsh", "authors": "Mohammad Noorbakhsh, Colm Connaughton, Francisco A. Rodrigues", "title": "Discovering causal factors of drought in Ethiopia", "comments": "Conference paper, 8 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drought is a costly natural hazard, many aspects of which remain poorly\nunderstood. It has many contributory factors, driving its outset, duration, and\nseverity, including land surface, anthropogenic activities, and, most\nimportantly, meteorological anomalies. Prediction plays a crucial role in\ndrought preparedness and risk mitigation. However, this is a challenging task\nat socio-economically critical lead times (1-2 years), because meteorological\nanomalies operate at a wide range of temporal and spatial scales. Among them,\npast studies have shown a correlation between the Sea Surface Temperature (SST)\nanomaly and the amount of precipitation in various locations in Africa. In its\nEastern part, the cooling phase of El Nino-Southern Oscillation (ENSO) and SST\nanomaly in the Indian ocean are correlated with the lack of rainfall. Given the\nintrinsic shortcomings of correlation coefficients, we investigate the\nassociation among SST modes of variability and the monthly fraction of grid\npoints in Ethiopia, which are in drought conditions in terms of causality.\nUsing the empirical extreme quantiles of precipitation distribution as a proxy\nfor drought, We show that the level of SST second mode of variability in the\nprior year influences the occurrence of drought in Ethiopia. The causal link\nbetween these two variables has a negative coefficient that verifies the\nconclusion of past studies that rainfall deficiency in the Horn of Africa is\nassociated with ENSO's cooling phase.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 22:06:39 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Noorbakhsh", "Mohammad", ""], ["Connaughton", "Colm", ""], ["Rodrigues", "Francisco A.", ""]]}, {"id": "2009.08007", "submitter": "Peter Boyd", "authors": "Peter Boyd, James Molyneux", "title": "Assessing the contagiousness of mass shootings with nonparametric Hawkes\n  processes", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0248437", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gun violence and mass shootings are high-profile epidemiological issues\nfacing the United States with questions regarding their contagiousness gaining\nprevalence in news media. Through the use of nonparametric Hawkes processes, we\nexamine the evidence for the existence of contagiousness within a catalog of\nmass shootings and highlight the broader benefits of using such nonparametric\npoint process models in modeling the occurrence of such events.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:41:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Boyd", "Peter", ""], ["Molyneux", "James", ""]]}, {"id": "2009.08011", "submitter": "Xiang Lyu", "authors": "Xiang Lyu, Jian Kang, Lexin Li", "title": "Statistical Inference for High-Dimensional Vector Autoregression with\n  Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional vector autoregression with measurement error is frequently\nencountered in a large variety of scientific and business applications. In this\narticle, we study statistical inference of the transition matrix under this\nmodel. While there has been a large body of literature studying sparse\nestimation of the transition matrix, there is a paucity of inference solutions,\nespecially in the high-dimensional scenario. We develop inferential procedures\nfor both the global and simultaneous testing of the transition matrix. We first\ndevelop a new sparse expectation-maximization algorithm to estimate the model\nparameters, and carefully characterize their estimation precisions. We then\nconstruct a Gaussian matrix, after proper bias and variance corrections, from\nwhich we derive the test statistics. Finally, we develop the testing procedures\nand establish their asymptotic guarantees. We study the finite-sample\nperformance of our tests through intensive simulations, and illustrate with a\nbrain connectivity analysis example.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:57:45 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lyu", "Xiang", ""], ["Kang", "Jian", ""], ["Li", "Lexin", ""]]}, {"id": "2009.08025", "submitter": "Thao Tran Phuong", "authors": "Tran Phuong Thao", "title": "Location-based Behavioral Authentication Using GPS Distance Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current user authentication systems are based on PIN code,\npassword, or biometrics traits which can have some limitations in usage and\nsecurity. Lifestyle authentication has become a new research approach. A\npromising idea for it is to use the location history since it is relatively\nunique. Even when people are living in the same area or have occasional travel,\nit does not vary from day to day. For Global Positioning System (GPS) data, the\nprevious work used the longitude, the latitude, and the timestamp as the\nfeatures for the classification. In this paper, we investigate a new approach\nutilizing the distance coherence which can be extracted from the GPS itself\nwithout the need to require other information. We applied three ensemble\nclassification RandomForest, ExtraTrees, and Bagging algorithms; and the\nexperimental result showed that the approach can achieve 99.42%, 99.12%, and\n99.25% of accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:26:20 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Thao", "Tran Phuong", ""]]}, {"id": "2009.08071", "submitter": "Yunyi Zhang", "authors": "Yunyi Zhang and Dimitris N. Politis", "title": "Ridge Regression Revisited: Debiasing, Thresholding and Bootstrap", "comments": "2 figures, 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of the Lasso in the era of high-dimensional data can be\nattributed to its conducting an implicit model selection, i.e., zeroing out\nregression coefficients that are not significant. By contrast, classical ridge\nregression can not reveal a potential sparsity of parameters, and may also\nintroduce a large bias under the high-dimensional setting. Nevertheless, recent\nwork on the Lasso involves debiasing and thresholding, the latter in order to\nfurther enhance the model selection. As a consequence, ridge regression may be\nworth another look since -- after debiasing and thresholding -- it may offer\nsome advantages over the Lasso, e.g., it can be easily computed using a\nclosed-form expression. % and it has similar performance to threshold Lasso. In\nthis paper, we define a debiased and thresholded ridge regression method, and\nprove a consistency result and a Gaussian approximation theorem. We further\nintroduce a wild bootstrap algorithm to construct confidence regions and\nperform hypothesis testing for a linear combination of parameters. In addition\nto estimation, we consider the problem of prediction, and present a novel,\nhybrid bootstrap algorithm tailored for prediction intervals. Extensive\nnumerical simulations further show that the debiased and thresholded ridge\nregression has favorable finite sample performance and may be preferable in\nsome settings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 05:04:10 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 17:38:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhang", "Yunyi", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "2009.08363", "submitter": "Panpan Zhang", "authors": "Chen Tang and Tiandong Wang and Panpan Zhang", "title": "Functional data analysis: An application to COVID-19 data in the United\n  States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic so far has caused huge negative impacts on different\nareas all over the world, and the United States (US) is one of the most\naffected countries. In this paper, we use methods from the functional data\nanalysis to look into the COVID-19 data in the US. We explore the modes of\nvariation of the data through a functional principal component analysis (FPCA),\nand study the canonical correlation between confirmed and death cases. In\naddition, we run a cluster analysis at the state level so as to investigate the\nrelation between geographical locations and the clustering structure. Lastly,\nwe consider a functional time series model fitted to the cumulative confirmed\ncases in the US, and make forecasts based on the dynamic FPCA. Both point and\ninterval forecasts are provided, and the methods for assessing the accuracy of\nthe forecasts are also included.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:23:43 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 18:58:49 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tang", "Chen", ""], ["Wang", "Tiandong", ""], ["Zhang", "Panpan", ""]]}, {"id": "2009.08405", "submitter": "Bora Jin", "authors": "Bora Jin, David B. Dunson, Julia E. Rager, David Reif, Stephanie M.\n  Engel, and Amy H. Herring", "title": "Bayesian Matrix Completion for Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput screening (HTS) is a well-established technology that rapidly\nand efficiently screens thousands of chemicals for potential toxicity. Massive\ntesting using HTS primarily aims to differentiate active vs inactive chemicals\nfor different types of biological endpoints. However, even using\nhigh-throughput technology, it is not feasible to test all possible\ncombinations of chemicals and assay endpoints, resulting in a majority of\nmissing combinations. Our goal is to derive posterior probabilities of activity\nfor each chemical by assay endpoint combination, addressing the sparsity of HTS\ndata. We propose a Bayesian hierarchical framework, which borrows information\nacross different chemicals and assay endpoints in a low-dimensional latent\nspace. This framework facilitates out-of-sample prediction of bioactivity\npotential for new chemicals not yet tested. Furthermore, this paper makes a\nnovel attempt in toxicology to simultaneously model heteroscedastic errors as\nwell as a nonparametric mean function. It leads to a broader definition of\nactivity whose need has been suggested by toxicologists. Simulation studies\ndemonstrate that our approach shows superior performance with more realistic\ninferences on activity than current standard methods. Application to an HTS\ndata set identifies chemicals that are most likely active for two disease\noutcomes: neurodevelopmental disorders and obesity. Code is available on\nGithub.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:28:32 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 14:15:35 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 23:14:03 GMT"}, {"version": "v4", "created": "Wed, 24 Mar 2021 20:53:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Jin", "Bora", ""], ["Dunson", "David B.", ""], ["Rager", "Julia E.", ""], ["Reif", "David", ""], ["Engel", "Stephanie M.", ""], ["Herring", "Amy H.", ""]]}, {"id": "2009.08432", "submitter": "Dinah Shender", "authors": "Dinah Shender, Ali Nasiri Amini, Xinlong Bao, Mert Dikmen, Amy\n  Richardson, Jing Wang", "title": "A Time To Event Framework For Multi-touch Attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-touch attribution (MTA) estimates the relative contributions of the\nmultiple ads a user may see prior to any observed conversions. Increasingly,\nadvertisers also want to base budget and bidding decisions on these\nattributions, spending more on ads that drive more conversions. We describe two\nrequirements for an MTA system to be suitable for this application: First, it\nmust be able to handle continuously updated and incomplete data. Second, it\nmust be sufficiently flexible to capture that an ad's effect will change over\ntime. We describe an MTA system, consisting of a model for user conversion\nbehavior and a credit assignment algorithm, that satisfies these requirements.\nOur model for user conversion behavior treats conversions as occurrences in an\ninhomogeneous Poisson process, while our attribution algorithm is based on\niteratively removing the last ad in the path.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:27:06 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Shender", "Dinah", ""], ["Amini", "Ali Nasiri", ""], ["Bao", "Xinlong", ""], ["Dikmen", "Mert", ""], ["Richardson", "Amy", ""], ["Wang", "Jing", ""]]}, {"id": "2009.08691", "submitter": "Sharmodeep Bhattacharyya", "authors": "Neil Hwang and Shirshendu Chatterjee and Yanming Di and Sharmodeep\n  Bhattacharyya", "title": "Estimating the treatment effect of the juvenile stay-at-home order on\n  SARS-CoV-2 infection spread in Saline County, Arkansas", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the treatment effect of the juvenile stay-at-home order\n(JSAHO) adopted in Saline County, Arkansas, from April 6 to May 7, in\nmitigating the growth of SARS-CoV-2 infection rates. To estimate the\ncounterfactual control outcome for Saline County, we apply\nDifference-in-Differences and Synthetic Control design methodologies. Both\napproaches show that stay-at-home order (SAHO) significantly reduced the growth\nrate of the infections in Saline County during the period the policy was in\neffect, contrary to some of the findings in the literature that cast doubt on\nthe general causal impact of SAHO with narrower scopes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:48:54 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Hwang", "Neil", ""], ["Chatterjee", "Shirshendu", ""], ["Di", "Yanming", ""], ["Bhattacharyya", "Sharmodeep", ""]]}, {"id": "2009.08798", "submitter": "Yu Guan", "authors": "Xi Chen, Yu Guan, Jian-Qing Shi, Xiu-Li Du, Janet Eyre", "title": "Automated Stroke Rehabilitation Assessment using Wearable Accelerometers\n  in Free-Living Environments", "comments": "submitted to ACM IMWUT", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stroke is known as a major global health problem, and for stroke survivors it\nis key to monitor the recovery levels. However, traditional stroke\nrehabilitation assessment methods (such as the popular clinical assessment) can\nbe subjective and expensive, and it is also less convenient for patients to\nvisit clinics in a high frequency. To address this issue, in this work based on\nwearable sensing and machine learning techniques, we developed an automated\nsystem that can predict the assessment score in an objective manner. With\nwrist-worn sensors, accelerometer data was collected from 59 stroke survivors\nin free-living environments for a duration of 8 weeks, and we aim to map the\nweek-wise accelerometer data (3 days per week) to the assessment score by\ndeveloping signal processing and predictive model pipeline. To achieve this, we\nproposed two types of new features, which can encode the rehabilitation\ninformation from both paralysed/non-paralysed sides while suppressing the\nhigh-level noises such as irrelevant daily activities. Based on the proposed\nfeatures, we further developed the longitudinal mixed-effects model with\nGaussian process prior (LMGP), which can model the random effects caused by\ndifferent subjects and time slots (during the 8 weeks). Comprehensive\nexperiments were conducted to evaluate our system on both acute and chronic\npatients, and the results suggested its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:10:48 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:47:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Xi", ""], ["Guan", "Yu", ""], ["Shi", "Jian-Qing", ""], ["Du", "Xiu-Li", ""], ["Eyre", "Janet", ""]]}, {"id": "2009.08850", "submitter": "Norman Fenton Prof", "authors": "Norman Fenton, Allan Jamieson, Sara Gomes, Martin Neil", "title": "On the limitations of probabilistic claims about the probative value of\n  mixed DNA profile evidence", "comments": "24 pages, plus 19 pages appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The likelihood ratio (LR) is a commonly used measure for determining the\nstrength of forensic match evidence. When a forensic expert determines a high\nLR for DNA found at a crime scene matching the DNA profile of a suspect they\ntypically report that 'this provides strong support for the prosecution\nhypothesis that the DNA comes from the suspect'. However, even with a high LR,\nthe evidence might not support the prosecution hypothesis if the defence\nhypothesis used to determine the LR is not the negation of the prosecution\nhypothesis (such as when the alternative is 'DNA comes from a person unrelated\nto the defendant' instead of 'DNA does not come from the suspect'). For DNA\nmixture profiles, especially low template DNA (LTDNA), the value of a high LR\nfor a 'match' - typically computed from probabilistic genotyping software - can\nbe especially questionable. But this is not just because of the use of\nnon-exhaustive hypotheses in such cases. In contrast to single profile DNA\n'matches', where the only residual uncertainty is whether a person other than\nthe suspect has the same matching DNA profile, it is possible for all the\ngenotypes of the suspect's DNA profile to appear at each locus of a DNA\nmixture, even though none of the contributors has that DNA profile. In fact, in\nthe absence of other evidence, we show it is possible to have a very high LR\nfor the hypothesis 'suspect is included in the mixture' even though the\nposterior probability that the suspect is included is very low. Yet, in such\ncases a forensic expert will generally still report a high LR as 'strong\nsupport for the suspect being a contributor'. Our observations suggest that, in\ncertain circumstances, the use of the LR may have led lawyers and jurors into\ngrossly overestimating the probative value of a LTDNA mixed profile 'match'\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:15:28 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Fenton", "Norman", ""], ["Jamieson", "Allan", ""], ["Gomes", "Sara", ""], ["Neil", "Martin", ""]]}, {"id": "2009.08883", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian and Diego Fern\\'andez", "title": "An Independence Test Based on Recurrence Rates. An empirical study and\n  applications to real data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose several variants to perform the independence test\nbetween two random elements based on recurrence rates. We will show how to\ncalculate the test statistic in each one of these cases. From simulations we\nobtain that in high dimension, our test clearly outperforms, in almost all\ncases, the other widely used competitors. The test was performed on two data\nsets including small and large sample sizes and we show that in both ases the\napplication of the test allows us to obtain interesting conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:12:04 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kalemkerian", "Juan", ""], ["Fern\u00e1ndez", "Diego", ""]]}, {"id": "2009.09034", "submitter": "Marina Vannucci", "authors": "Matthew D. Koslovsky, Emily T. Hebert, Michael S. Businelle and Marina\n  Vannucci", "title": "A Bayesian Time-Varying Effect Model for Behavioral mHealth Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of mobile health (mHealth) devices into behavioral health\nresearch has fundamentally changed the way researchers and interventionalists\nare able to collect data as well as deploy and evaluate intervention\nstrategies. In these studies, researchers often collect intensive longitudinal\ndata (ILD) using ecological momentary assessment methods, which aim to capture\npsychological, emotional, and environmental factors that may relate to a\nbehavioral outcome in near real-time. In order to investigate ILD collected in\na novel, smartphone-based smoking cessation study, we propose a Bayesian\nvariable selection approach for time-varying effect models, designed to\nidentify dynamic relations between potential risk factors and smoking behaviors\nin the critical moments around a quit attempt. We use parameter-expansion and\ndata-augmentation techniques to efficiently explore how the underlying\nstructure of these relations varies over time and across subjects. We achieve\ndeeper insights into these relations by introducing nonparametric priors for\nregression coefficients that cluster similar effects for risk factors while\nsimultaneously determining their inclusion. Results indicate that our approach\nis well-positioned to help researchers effectively evaluate, design, and\ndeliver tailored intervention strategies in the critical moments surrounding a\nquit attempt.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 19:24:47 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Koslovsky", "Matthew D.", ""], ["Hebert", "Emily T.", ""], ["Businelle", "Michael S.", ""], ["Vannucci", "Marina", ""]]}, {"id": "2009.09067", "submitter": "Antoine Mazieres", "authors": "Antoine Mazieres and Telmo Menezes and Camille Roth", "title": "Computational appraisal of gender representativeness in popular movies", "comments": "13 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender representation in mass media has long been mainly studied by\nqualitatively analyzing content. This article illustrates how automated\ncomputational methods may be used in this context to scale up such empirical\nobservations and increase their resolution and significance. We specifically\napply a face and gender detection algorithm on a broad set of popular movies\nspanning more than three decades to carry out a large-scale appraisal of the\non-screen presence of women and men. Beyond the confirmation of a strong\nunder-representation of women, we exhibit a clear temporal trend towards a\nfairer representativeness. We further contrast our findings with respect to\nmovie genre, budget, and various audience-related features such as movie gross\nand user ratings. We lastly propose a fine description of significant\nasymmetries in the mise-en-sc\\`ene and mise-en-cadre of characters in relation\nto their gender and the spatial composition of a given frame.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:15:11 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 13:00:25 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 07:28:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Mazieres", "Antoine", ""], ["Menezes", "Telmo", ""], ["Roth", "Camille", ""]]}, {"id": "2009.09106", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Street Network Models and Indicators for Every Urban Area in the World", "comments": null, "journal-ref": "Geographical Analysis, 2021", "doi": "10.1111/gean.12281", "report-no": null, "categories": "physics.soc-ph math.GN physics.app-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cities worldwide exhibit a variety of street network patterns and\nconfigurations that shape human mobility, equity, health, and livelihoods. This\nstudy models and analyzes the street networks of every urban area in the world,\nusing boundaries derived from the Global Human Settlement Layer. Street network\ndata are acquired and modeled from OpenStreetMap with the open-source OSMnx\nsoftware. In total, this study models over 160 million OpenStreetMap street\nnetwork nodes and over 320 million edges across 8,914 urban areas in 178\ncountries, and attaches elevation and grade data. This article presents the\nstudy's reproducible computational workflow, introduces two new open data\nrepositories of ready-to-use global street network models and calculated\nindicators, and discusses summary findings on street network form worldwide. It\nmakes four contributions. First, it reports the methodological advances of this\nopen-source workflow. Second, it produces an open data repository containing\nstreet network models for each urban area. Third, it analyzes these models to\nproduce an open data repository containing street network form indicators for\neach urban area. No such global urban street network indicator dataset has\npreviously existed. Fourth, it presents a summary analysis of urban street\nnetwork form, reporting the first such worldwide results in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:13:14 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 03:19:16 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 17:59:45 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "2009.09310", "submitter": "Shanshan Cao", "authors": "Kai Ni, Shanshan Cao, and Xiaoming Huo", "title": "Fast and Asymptotically Powerful Detection for Filamentary Objects in\n  Digital Images", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an inhomogeneous chain embedded in a noisy image, we consider the\nconditions under which such an embedded chain is detectable. Many applications,\nsuch as detecting moving objects, detecting ship wakes, can be abstracted as\nthe detection on the existence of chains. In this work, we provide the\ndetection algorithm with low order of computation complexity to detect the\nchain and the optimal theoretical detectability regarding SNR (signal to noise\nratio) under the normal distribution model. Specifically, we derive an\nanalytical threshold that specifies what is detectable. We design a longest\nsignificant chain detection algorithm, with computation complexity in the order\nof $O(n\\log n)$. We also prove that our proposed algorithm is asymptotically\npowerful, which means, as the dimension $n \\rightarrow \\infty$, the probability\nof false detection vanishes. We further provide some simulated examples and a\nreal data example, which validate our theory.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 22:28:58 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ni", "Kai", ""], ["Cao", "Shanshan", ""], ["Huo", "Xiaoming", ""]]}, {"id": "2009.09352", "submitter": "Dong Xu", "authors": "Dong Xu, Chao Meng, Qingpeng Zhang, Puneet Bhardwaj, Young-Jun Son", "title": "A Hybrid Simulation-based Duopoly Game Framework for Analysis of Supply\n  Chain and Marketing Activities", "comments": "39 pages, 10 figures, 9 tables", "journal-ref": "In chapter 11 of book \"Applications of Multi-Criteria and Game\n  Theory Approaches\" published by Springer-Verlag and edited by L. Benyoucef et\n  al. (eds.), 2013", "doi": "10.1007/978-1-4471-5295-8", "report-no": null, "categories": "cs.GT cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid simulation-based framework involving system dynamics and agent-based\nsimulation is proposed to address duopoly game considering multiple strategic\ndecision variables and rich payoff, which cannot be addressed by traditional\napproaches involving closed-form equations. While system dynamics models are\nused to represent integrated production, logistics, and pricing determination\nactivities of duopoly companies, agent-based simulation is used to mimic\nenhanced consumer purchasing behavior considering advertisement, promotion\neffect, and acquaintance recommendation in the consumer social network. The\npayoff function of the duopoly companies is assumed to be the net profit based\non the total revenue and various cost items such as raw material, production,\ntransportation, inventory and backorder. A unique procedure is proposed to\nsolve and analyze the proposed simulation-based game, where the procedural\ncomponents include strategy refinement, data sampling, gaming solving, and\nperformance evaluation. First, design of experiment and estimated\nconformational value of information techniques are employed for strategy\nrefinement and data sampling, respectively. Game solving then focuses on pure\nstrategy equilibriums, and performance evaluation addresses game stability,\nequilibrium strictness, and robustness. A hypothetical case scenario involving\nsoft-drink duopoly on Coke and Pepsi is considered to illustrate and\ndemonstrate the proposed approach. Final results include P-values of\nstatistical tests, confidence intervals, and simulation steady state analysis\nfor different pure equilibriums.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 05:18:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Xu", "Dong", ""], ["Meng", "Chao", ""], ["Zhang", "Qingpeng", ""], ["Bhardwaj", "Puneet", ""], ["Son", "Young-Jun", ""]]}, {"id": "2009.09420", "submitter": "Emiko Dupont", "authors": "Emiko Dupont, Simon N. Wood, Nicole Augustin", "title": "Spatial+: a novel approach to spatial confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial regression models, collinearity between covariates and spatial\neffects can lead to significant bias in effect estimates. This problem, known\nas spatial confounding, is encountered modelling forestry data to assess the\neffect of temperature on tree health. Reliable inference is difficult as\nresults depend on whether or not spatial effects are included in the model. The\nmechanism behind spatial confounding is poorly understood and methods for\ndealing with it are limited. We propose a novel approach, spatial+, in which\ncollinearity is reduced by replacing the covariates in the spatial model by\ntheir residuals after spatial dependence has been regressed away. Using a thin\nplate spline model formulation, we recognise spatial confounding as a\nsmoothing-induced bias identified by Rice (1986), and through asymptotic\nanalysis of the effect estimates, we show that spatial+ avoids the bias\nproblems of the spatial model. This is also demonstrated in a simulation study.\nSpatial+ is straight-forward to implement using existing software and, as the\nresponse variable is the same as that of the spatial model, standard model\nselection criteria can be used for comparisons. A major advantage of the method\nis also that it extends to models with non-Gaussian response distributions.\nFinally, while our results are derived in a thin plate spline setting, the\nspatial+ methodology transfers easily to other spatial model formulations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 12:11:35 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Dupont", "Emiko", ""], ["Wood", "Simon N.", ""], ["Augustin", "Nicole", ""]]}, {"id": "2009.09471", "submitter": "Yue Zhao", "authors": "Zheng Li, Yue Zhao, Jialin Fu", "title": "SYNC: A Copula based Framework for Generating Synthetic Data from\n  Aggregated Sources", "comments": "Proceedings of the 2020 IEEE International Conference on Data Mining\n  Workshops (ICDMW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A synthetic dataset is a data object that is generated programmatically, and\nit may be valuable to creating a single dataset from multiple sources when\ndirect collection is difficult or costly. Although it is a fundamental step for\nmany data science tasks, an efficient and standard framework is absent. In this\npaper, we study a specific synthetic data generation task called downscaling, a\nprocedure to infer high-resolution, harder-to-collect information (e.g.,\nindividual level records) from many low-resolution, easy-to-collect sources,\nand propose a multi-stage framework called SYNC (Synthetic Data Generation via\nGaussian Copula). For given low-resolution datasets, the central idea of SYNC\nis to fit Gaussian copula models to each of the low-resolution datasets in\norder to correctly capture dependencies and marginal distributions, and then\nsample from the fitted models to obtain the desired high-resolution subsets.\nPredictive models are then used to merge sampled subsets into one, and finally,\nsampled datasets are scaled according to low-resolution marginal constraints.\nWe make four key contributions in this work: 1) propose a novel framework for\ngenerating individual level data from aggregated data sources by combining\nstate-of-the-art machine learning and statistical techniques, 2) perform\nsimulation studies to validate SYNC's performance as a synthetic data\ngeneration algorithm, 3) demonstrate its value as a feature engineering tool,\nas well as an alternative to data collection in situations where gathering is\ndifficult through two real-world datasets, 4) release an easy-to-use framework\nimplementation for reproducibility and scalability at the production level that\neasily incorporates new data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 16:36:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Zheng", ""], ["Zhao", "Yue", ""], ["Fu", "Jialin", ""]]}, {"id": "2009.09495", "submitter": "Omar Garcia Crespillo", "authors": "Omar Garcia Crespillo, Mathieu Joerger and Steve Langel", "title": "Tight Bounds for Uncertain Time-Correlated Errors with Gauss-Markov\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety-critical navigation applications require that estimation errors be\nreliably quantified and bounded. This can be challenging for linear dynamic\nsystems if the process noise or measurement errors have uncertain time\ncorrelation. In many systems (e.g., in satellite-based or inertial navigation\nsystems), there are sources of time-correlated sensor errors that can be well\nmodeled using Gauss-Markov processes (GMP). However, uncertainty in the GMP\nparameters, particularly in the correlation time constant, can cause misleading\nerror estimation. In this paper, we develop new time-correlated models that\nensure tight upper bounds on the estimation error variance, assuming that the\nactual error is a stationary GMP with a time constant that is only known to\nreside within an interval. We first use frequency-domain analysis to derive a\nstationary GMP model both in continuous and discrete time domain, which\noutperforms models previously described in the literature. Then, we achieve an\neven tighter estimation error bound using a non-stationary GMP model, for which\nwe determine the minimum initial variance that guarantees bounding conditions.\nIn both cases, the model can easily be implemented in a linear estimator like a\nKalman filter.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 18:40:05 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Crespillo", "Omar Garcia", ""], ["Joerger", "Mathieu", ""], ["Langel", "Steve", ""]]}, {"id": "2009.09634", "submitter": "Saswata Sahoo Dr", "authors": "Saswata Sahoo and Souradip Chakraborty", "title": "Learning Representation for Mixed Data Types with a Nonlinear Deep\n  Encoder-Decoder Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation of data on mixed variables, numerical and categorical types to\nget suitable feature map is a challenging task as important information lies in\na complex non-linear manifold. The feature transformation should be able to\nincorporate marginal information of the individual variables and complex\ncross-dependence structure among the mixed type of variables simultaneously. In\nthis work, we propose a novel nonlinear Deep Encoder-Decoder framework to\ncapture the cross-domain information for mixed data types. The hidden layers of\nthe network connect the two types of variables through various non-linear\ntransformations to give latent feature maps. We encode the information on the\nnumerical variables in a number of hidden nonlinear units. We use these units\nto recreate categorical variables through further nonlinear transformations. A\nseparate and similar network is developed switching the roles of the numerical\nand categorical variables. The hidden representational units are stacked one\nnext to the others and transformed into a common space using a locality\npreserving projection. The derived feature maps are used to explore the\nclusters in the data. Various standard datasets are investigated to show nearly\nthe state of the art performance in clustering using the feature maps with\nsimple K-means clustering.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 06:29:49 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sahoo", "Saswata", ""], ["Chakraborty", "Souradip", ""]]}, {"id": "2009.09653", "submitter": "Hideo Hirose", "authors": "Hideo Hirose", "title": "A Relationship Between SIR Model and Generalized Logistic Distribution\n  with Applications to SARS and COVID-19", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that the generalized logistic distribution model is derived\nfrom the well-known compartment model, consisting of susceptible, infected and\nrecovered compartments, abbreviated as the SIR model, under certain conditions.\nIn the SIR model, there are uncertainties in predicting the final values for\nthe number of infected population and the infectious parameter. However, by\nutilizing the information obtained from the generalized logistic distribution\nmodel, we can perform the SIR numerical computation more stably and more\naccurately. Applications to severe acute respiratory syndrome (SARS) and\nCoronavirus disease 2019 (COVID-19) using this combined method are also\nintroduced.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:43:08 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 02:44:50 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hirose", "Hideo", ""]]}, {"id": "2009.10025", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels", "title": "Misspecification and Unreliable Interpretations in Psychology and Social\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The replicability crisis has drawn attention to numerous weaknesses in\npsychology and social science research practice. In this work we focus on three\nissues that cannot be addressed with replication alone, and which deserve more\nattention: Functional misspecification, structural misspecification, and\nunreliable interpretation of results. We demonstrate a number of possible\nconsequences via simulation, and provide recommendations for researchers to\nimprove their research practice. Psychologists and social scientists should\nengage with these areas of analytical and statistical improvement, as they have\nthe potential to seriously hinder scientific progress. Every research question\nand hypothesis may present its own unique challenges, and it is only through an\nawareness and understanding of varied statistical methods for predictive and\ncausal modeling, that researchers will have the tools with which to\nappropriately address them.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:00:48 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 09:36:33 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 07:44:42 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Vowels", "Matthew J.", ""]]}, {"id": "2009.10117", "submitter": "Zhengyang Zhou", "authors": "Zhengyang Zhou, Dateng Li, Song Zhang", "title": "Sample Size Calculation for Cluster Randomized Trials with Zero-inflated\n  Count Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster randomized trails (CRT) have been widely employed in medical and\npublic health research. Many clinical count outcomes, such as the number of\nfalls in nursing homes, exhibit excessive zero values. In the presence of zero\ninflation, traditional power analysis methods for count data based on Poisson\nor negative binomial distribution may be inadequate. In this study, we present\na sample size method for CRTs with zero-inflated count outcomes. It is\ndeveloped based on GEE regression directly modeling the marginal mean of a ZIP\noutcome, which avoids the challenge of testing two intervention effects under\ntraditional modeling approaches. A closed-form sample size formula is derived\nwhich properly accounts for zero inflation, ICCs due to clustering, unbalanced\nrandomization, and variability in cluster size. Robust approaches, including\nt-distribution-based approximation and Jackknife re-sampling variance\nestimator, are employed to enhance trial properties under small sample sizes.\nExtensive simulations are conducted to evaluate the performance of the proposed\nmethod. An application example is presented in a real clinical trial setting.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:22:16 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Zhou", "Zhengyang", ""], ["Li", "Dateng", ""], ["Zhang", "Song", ""]]}, {"id": "2009.10126", "submitter": "Hamed Honari", "authors": "Hamed Honari (1), Ann S. Choe (2 and 3 and 4), Martin A. Lindquist (5)\n  ((1) Department of Electrical and Computer Engineering, Johns Hopkins\n  University, USA (2) F. M. Kirby Research Center for Functional Brain Imaging,\n  Kennedy Krieger Institute, USA (3) International Center for Spinal Cord\n  Injury, Kennedy Krieger Institute, USA (4) Russell H. Morgan Department of\n  Radiology and Radiological Science, Johns Hopkins School of Medicine, USA (5)\n  Department of Biostatistics, Johns Hopkins University, USA)", "title": "Evaluating phase synchronization methods in fMRI: a comparison study and\n  new approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been growing interest in measuring time-varying\nfunctional connectivity between different brain regions using resting-state\nfunctional magnetic resonance imaging (rs-fMRI) data. One way to assess the\nrelationship between signals from different brain regions is to measure their\nphase synchronization (PS) across time. There are several ways to perform such\nanalyses, and here we compare methods that utilize a PS metric together with a\nsliding window, referred to here as windowed phase synchronization (WPS), with\nthose that directly measure the instantaneous phase synchronization (IPS). In\nparticular, IPS has recently gained popularity as it offers single time-point\nresolution of time-resolved fMRI connectivity. In this paper, we discuss the\nunderlying assumptions required for performing PS analyses and emphasize the\nnecessity of band-pass filtering the data to obtain valid results. We review\nvarious methods for evaluating PS and introduce a new approach within the IPS\nframework denoted the cosine of the relative phase (CRP). We contrast methods\nthrough a series of simulations and application to rs-fMRI data. Our results\nindicate that CRP outperforms other tested methods and overcomes issues related\nto undetected temporal transitions from positive to negative associations\ncommon in IPS analysis. Further, in contrast to phase coherence, CRP unfolds\nthe distribution of PS measures, which benefits subsequent clustering of PS\nmatrices into recurring brain states.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:38:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Honari", "Hamed", "", "2 and 3 and 4"], ["Choe", "Ann S.", "", "2 and 3 and 4"], ["Lindquist", "Martin A.", ""]]}, {"id": "2009.10265", "submitter": "Zhengyang Zhou", "authors": "Zhengyang Zhou, Minge Xie, David Huh, Eun-Young Mun", "title": "A Bias Correction Method in Meta-analysis of Randomized Clinical Trials\n  with no Adjustments for Zero-inflated Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clinical endpoint measures, such as the number of standard drinks\nconsumed per week or the number of days that patients stayed in the hospital,\nare count data with excessive zeros. However, the zero-inflated nature of such\noutcomes is sometimes ignored in analyses of clinical trials. This leads to\nbiased estimates of study-level intervention effect and, consequently, a biased\nestimate of the overall intervention effect in a meta-analysis. The current\nstudy proposes a novel statistical approach, the Zero-inflation Bias Correction\n(ZIBC) method, that can account for the bias introduced when using the Poisson\nregression model, despite a high rate of inflated zeros in the outcome\ndistribution of a randomized clinical trial. This correction method only\nrequires summary information from individual studies to correct intervention\neffect estimates as if they were appropriately estimated using the\nzero-inflated Poisson regression model, thus it is attractive for meta-analysis\nwhen individual participant-level data are not available in some studies.\nSimulation studies and real data analyses showed that the ZIBC method performed\nwell in correcting zero-inflation bias in most situations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:32:15 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 02:05:23 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 18:32:53 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Zhengyang", ""], ["Xie", "Minge", ""], ["Huh", "David", ""], ["Mun", "Eun-Young", ""]]}, {"id": "2009.10426", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Markus Granl\\\"of, Jun Yu", "title": "Effects of winter climate on high speed passenger trains in\n  Botnia-Atlantica region", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harsh winter climate can cause various problems for both public and private\nsectors in Sweden, especially in the northern part for railway industry. To\nhave a better understanding of winter climate impacts, this study investigates\neffects of the winter climate including atmospheric icing on the performance of\nhigh speed passenger trains in the Botnia-Atlantica region. The investigation\nis done with train operational data together with simulated weather data from\nthe Weather Research and Forecast model over January - February 2017.\n  Two different measurements of the train performance are analysed. One is\ncumulative delay which measures the increment in delay in terms of running time\nwithin two consecutive measuring spots, the other is current delay which is the\ndelay in terms of arrival time at each measuring spot compared to the schedule.\nCumulative delay is investigated through a Cox model and the current delay is\nstudied using a Markov chain model.\n  The results show that the weather factors have impacts on the train\nperformance. Therein temperature and humidity have significant impacts on both\nthe occurrence of cumulative delay and the transition probabilities between\n(current) delayed and non-delayed states.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:04:40 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 08:11:08 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Wang", "Jianfeng", ""], ["Granl\u00f6f", "Markus", ""], ["Yu", "Jun", ""]]}, {"id": "2009.10476", "submitter": "Sara Martino", "authors": "Guido Fioravanti, Sara Martino, Michela Cameletti, Giorgio Cattani", "title": "Spatio-temporal modelling of $\\text{PM}_{10}$ daily concentrations in\n  Italy using the SPDE approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.atmosenv.2021.118192", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates the main results of a spatio-temporal interpolation\nprocess of $\\text{PM}_{10}$ concentrations at daily resolution using a set of\n410 monitoring sites, distributed throughout the Italian territory, for the\nyear 2015. The interpolation process is based on a Bayesian hierarchical model\nwhere the spatial-component is represented through the Stochastic Partial\nDifferential Equation (SPDE) approach with a lag-1 temporal autoregressive\ncomponent (AR1). Inference is performed through the Integrated Nested Laplace\nApproximation (INLA). Our model includes 11 spatial and spatio-temporal\npredictors, including meteorological variables and Aerosol Optical Depth. As\nthe predictors' impact varies across months, the regression is based on 12\nmonthly models with the same set of covariates. The predictive model\nperformance has been analyzed using a cross-validation study. Our results show\nthat the predicted and the observed values are well in accordance (correlation\nrange: 0.79 - 0.91; bias: 0.22 - 1.07 $\\mu \\text{g}/\\text{m}^3$; RMSE: 4.9 -\n13.9 $\\mu \\text{g}/\\text{m}^3$). The model final output is a set of 365 gridded\n(1km $\\times$ 1km) daily $\\text{PM}_{10}$ maps over Italy equipped with an\nuncertainty measure. The spatial prediction performance shows that the\ninterpolation procedure is able to reproduce the large scale data features\nwithout unrealistic artifacts in the generated $\\text{PM}_{10}$ surfaces. The\npaper presents also two illustrative examples of practical applications of our\nmodel, exceedance probability and population exposure maps.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:56:48 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Fioravanti", "Guido", ""], ["Martino", "Sara", ""], ["Cameletti", "Michela", ""], ["Cattani", "Giorgio", ""]]}, {"id": "2009.10498", "submitter": "Weijian Luo", "authors": "Weijian Luo and Yongxian Long", "title": "ABM: an automatic supervised feature engineering method for loss based\n  models based on group and fused lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital problem in solving classification or regression problem is to apply\nfeature engineering and variable selection on data before fed into models.One\nof a most popular feature engineering method is to discretisize continous\nvariable with some cutting points,which is refered to as bining processing.Good\ncutting points are important for improving model's ability, because wonderful\nbining may ignore some noisy variance in continous variable range and keep\nuseful leveled information with good ordered encodings.However, to our best\nknowledge a majority of cutting point selection is done via researchers domain\nknownledge or some naive methods like equal-width cutting or equal-frequency\ncutting.In this paper we propose an end-to-end supervised cutting point\nselection method based on group and fused lasso along with the automatically\nvariable selection effect.We name our method \\textbf{ABM}(automatic bining\nmachine). We firstly cut each variable range into fine grid bins and train\nmodel with our group and group fused lasso regularization on each successive\nbins.It is a method that integrates feature engineering,variable selection and\nmodel training simultanously.And one more inspiring thing is that the method is\nflexible such that it can be taken into a bunch of loss function based model\nincluding deep neural networks.We have also implemented the method in R and\nopen the source code to other researchers.A Python version will also meet the\ncommunity in days.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 12:42:22 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Luo", "Weijian", ""], ["Long", "Yongxian", ""]]}, {"id": "2009.10518", "submitter": "Cynthia Huber", "authors": "Cynthia Huber, Norbert Benda, Tim Friede", "title": "Subgroup identification in individual patient data meta-analysis using\n  model-based recursive partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based recursive partitioning (MOB) can be used to identify subgroups\nwith differing treatment effects. The detection rate of treatment-by-covariate\ninteractions and the accuracy of identified subgroups using MOB depend strongly\non the sample size. Using data from multiple randomized controlled clinical\ntrials can overcome the problem of too small samples. However, naively pooling\ndata from multiple trials may result in the identification of spurious\nsubgroups as differences in study design, subject selection and other sources\nof between-trial heterogeneity are ignored. In order to account for\nbetween-trial heterogeneity in individual participant data (IPD) meta-analysis\nrandom-effect models are frequently used. Commonly, heterogeneity in the\ntreatment effect is modelled using random effects whereas heterogeneity in the\nbaseline risks is modelled by either fixed effects or random effects. In this\narticle, we propose metaMOB, a procedure using the generalized mixed-effects\nmodel tree (GLMM tree) algorithm for subgroup identification in IPD\nmeta-analysis. Although the application of metaMOB is potentially wider, e.g.\nrandomized experiments with participants in social sciences or preclinical\nexperiments in life sciences, we focus on randomized controlled clinical\ntrials. In a simulation study, metaMOB outperformed GLMM trees assuming a\nrandom intercept only and model-based recursive partitioning (MOB), whose\nalgorithm is the basis for GLMM trees, with respect to the false discovery\nrates, accuracy of identified subgroups and accuracy of estimated treatment\neffect. The most robust and therefore most promising method is metaMOB with\nfixed effects for modelling the between-trial heterogeneity in the baseline\nrisks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:17:07 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Huber", "Cynthia", ""], ["Benda", "Norbert", ""], ["Friede", "Tim", ""]]}, {"id": "2009.10532", "submitter": "Robert Miller", "authors": "Robert Miller and Phil Maguire", "title": "A rapidly updating stratified mix-adjusted median property price index\n  model", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homeowners, first-time buyers, banks, governments and construction companies\nare highly interested in following the state of the property market. Currently,\nproperty price indexes are published several months out of date and hence do\nnot offer the up-to-date information which housing market stakeholders need in\norder to make informed decisions. In this article, we present an updated\nversion of a central-price tendency based property price index which uses\ngeospatial property data and stratification in order to compare similar houses.\nThe expansion of the algorithm to include additional parameters owing to a new\ndata structure implementation and a richer dataset allows for the construction\nof a far smoother and more robust index than the original algorithm produced.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:23:03 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Miller", "Robert", ""], ["Maguire", "Phil", ""]]}, {"id": "2009.10645", "submitter": "Chen Zhang", "authors": "Jie Guo, Hao Yan, Chen Zhang, Steven Hoi", "title": "Partially Observable Online Change Detection via Smooth-Sparse\n  Decomposition", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online change detection of high dimensional data streams with\nsparse changes, where only a subset of data streams can be observed at each\nsensing time point due to limited sensing capacities. On the one hand, the\ndetection scheme should be able to deal with partially observable data and\nmeanwhile have efficient detection power for sparse changes. On the other, the\nscheme should be able to adaptively and actively select the most important\nvariables to observe to maximize the detection power. To address these two\npoints, in this paper, we propose a novel detection scheme called CDSSD. In\nparticular, it describes the structure of high dimensional data with sparse\nchanges by smooth-sparse decomposition, whose parameters can be learned via\nspike-slab variational Bayesian inference. Then the posterior Bayes factor,\nwhich incorporates the learned parameters and sparse change information, is\nformulated as a detection statistic. Finally, by formulating the statistic as\nthe reward of a combinatorial multi-armed bandit problem, an adaptive sampling\nstrategy based on Thompson sampling is proposed. The efficacy and applicability\nof our method in practice are demonstrated with numerical studies and a real\ncase study.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:03:04 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Guo", "Jie", ""], ["Yan", "Hao", ""], ["Zhang", "Chen", ""], ["Hoi", "Steven", ""]]}, {"id": "2009.10808", "submitter": "Anuj Tiwari Dr", "authors": "Anuj Tiwari, Arya V. Dadhania, Vijay Avin Balaji Ragunathrao, Edson R.\n  A. Oliveira", "title": "Using Machine Learning to Develop a Novel COVID-19 Vulnerability Index\n  (C19VI)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID19 is now one of the most leading causes of death in the United States.\nSystemic health, social and economic disparities have put the minorities and\neconomically poor communities at a higher risk than others. There is an\nimmediate requirement to develop a reliable measure of county-level\nvulnerabilities that can capture the heterogeneity of both vulnerable\ncommunities and the COVID19 pandemic. This study reports a COVID19\nVulnerability Index (C19VI) for identification and mapping of vulnerable\ncounties in the United States. We proposed a Random Forest machine learning\nbased COVID19 vulnerability model using CDC sociodemographic and\nCOVID19-specific themes. An innovative COVID19 Impact Assessment algorithm was\nalso developed using homogeneity and trend assessment technique for evaluating\nseverity of the pandemic in all counties and train RF model. Developed C19VI\nwas statistically validated and compared with the CDC COVID19 Community\nVulnerability Index (CCVI). Finally, using C19VI along with census data, we\nexplored racial inequalities and economic disparities in COVID19 health\noutcomes amongst different regions in the United States. Our C19VI index\nindicates that 18.30% of the counties falls into very high vulnerability class,\n24.34% in high, 23.32% in moderate, 22.34% in low, and 11.68% in very low.\nFurthermore, C19VI reveals that 75.57% of racial minorities and 82.84% of\neconomically poor communities are very high or high COVID19 vulnerable regions.\nThe proposed approach of vulnerability modeling takes advantage of both the\nwell-established field of statistical analysis and the fast-evolving domain of\nmachine learning. C19VI provides an accurate and more reliable way to measure\ncounty level vulnerability in the United States. This index aims at helping\nemergency planners to develop more effective mitigation strategies especially\nfor the disproportionately impacted communities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 20:48:19 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Tiwari", "Anuj", ""], ["Dadhania", "Arya V.", ""], ["Ragunathrao", "Vijay Avin Balaji", ""], ["Oliveira", "Edson R. A.", ""]]}, {"id": "2009.11074", "submitter": "Albert Lee III", "authors": "Albert H. Lee III", "title": "Application of Dynamic Linear Models to Random Allocation Clinical\n  Trials with Covariates", "comments": "arXiv admin note: text overlap with arXiv:2008.00339", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent method using Dynamic Linear Models to improve preferred treatment\nallocation budget in random allocation models was proposed by Lee, Boone, et al\n(2020). However this model failed to include the impact covariates such as\nsmoking, gender, etc, had on model performance. The current paper addresses\nrandom allocation to treatments using the DLM in Bayesian Adaptive Allocation\nModels with a single covariate. We show a reduced treatment allocation budget\nalong with a reduced time to locate preferred treatment. Furthermore, a\nsensitivity analysis is performed on mean and variance parameters and a power\nanalysis is conducted using Bayes Factor. This power analysis is used to\ndetermine the proportion of unallocated patient budgets above a specified\ncutoff value. Additionally a sensitivity analysis is conducted on covariate\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 12:02:51 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lee", "Albert H.", "III"]]}, {"id": "2009.11242", "submitter": "Chieh Wu T", "authors": "Shi Dong, Zlatan Feric, Guangyu Li, Chieh Wu, April Z. Gu, Jennifer\n  Dy, John Meeker, Ingrid Y. Padilla, Jose Cordero, Carmen Velez Vega, Zaira\n  Rosario, Akram Alshawabkeh, David Kaeli", "title": "Using Undersampling with Ensemble Learning to Identify Factors\n  Contributing to Preterm Birth", "comments": null, "journal-ref": "ICMLA 2020", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Ensemble Learning models to identify factors\ncontributing to preterm birth. Our work leverages a rich dataset collected by a\nNIEHS P42 Center that is trying to identify the dominant factors responsible\nfor the high rate of premature births in northern Puerto Rico. We investigate\nanalytical models addressing two major challenges present in the dataset: 1)\nthe significant amount of incomplete data in the dataset, and 2) class\nimbalance in the dataset. First, we leverage and compare two types of missing\ndata imputation methods: 1) mean-based and 2) similarity-based, increasing the\ncompleteness of this dataset. Second, we propose a feature selection and\nevaluation model based on using undersampling with Ensemble Learning to address\nclass imbalance present in the dataset. We leverage and compare multiple\nEnsemble Feature selection methods, including Complete Linear Aggregation\n(CLA), Weighted Mean Aggregation (WMA), Feature Occurrence Frequency (OFA), and\nClassification Accuracy Based Aggregation (CAA). To further address missing\ndata present in each feature, we propose two novel methods: 1) Missing Data\nRate and Accuracy Based Aggregation (MAA), and 2) Entropy and Accuracy Based\nAggregation (EAA). Both proposed models balance the degree of data variance\nintroduced by the missing data handling during the feature selection process\nwhile maintaining model performance. Our results show a 42\\% improvement in\nsensitivity versus fallout over previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:32:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dong", "Shi", ""], ["Feric", "Zlatan", ""], ["Li", "Guangyu", ""], ["Wu", "Chieh", ""], ["Gu", "April Z.", ""], ["Dy", "Jennifer", ""], ["Meeker", "John", ""], ["Padilla", "Ingrid Y.", ""], ["Cordero", "Jose", ""], ["Vega", "Carmen Velez", ""], ["Rosario", "Zaira", ""], ["Alshawabkeh", "Akram", ""], ["Kaeli", "David", ""]]}, {"id": "2009.11401", "submitter": "Sharmistha Guha", "authors": "Sharmistha Guha and Abel Rodriguez", "title": "High Dimensional Bayesian Network Classification with Network\n  Global-Local Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel Bayesian classification framework for networks\nwith labeled nodes. While literature on statistical modeling of network data\ntypically involves analysis of a single network, the recent emergence of\ncomplex data in several biological applications, including brain imaging\nstudies, presents a need to devise a network classifier for subjects. This\narticle considers an application from a brain connectome study, where the\noverarching goal is to classify subjects into two separate groups based on\ntheir brain network data, along with identifying influential regions of\ninterest (ROIs) (referred to as nodes). Existing approaches either treat all\nedge weights as a long vector or summarize the network information with a few\nsummary measures. Both these approaches ignore the full network structure, may\nlead to less desirable inference in small samples and are not designed to\nidentify significant network nodes. We propose a novel binary logistic\nregression framework with the network as the predictor and a binary response,\nthe network predictor coefficient being modeled using a novel class\nglobal-local shrinkage priors. The framework is able to accurately detect nodes\nand edges in the network influencing the classification. Our framework is\nimplemented using an efficient Markov Chain Monte Carlo algorithm.\nTheoretically, we show asymptotically optimal classification for the proposed\nframework when the number of network edges grows faster than the sample size.\nThe framework is empirically validated by extensive simulation studies and\nanalysis of a brain connectome data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:21:07 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Guha", "Sharmistha", ""], ["Rodriguez", "Abel", ""]]}, {"id": "2009.11407", "submitter": "Alexander Rodr\\'iguez", "authors": "Alexander Rodr\\'iguez, Nikhil Muralidhar, Bijaya Adhikari, Anika\n  Tabassum, Naren Ramakrishnan, B. Aditya Prakash", "title": "Steering a Historical Disease Forecasting Model Under a Pandemic: Case\n  of Flu and COVID-19", "comments": "Appears in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting influenza in a timely manner aids health organizations and\npolicymakers in adequate preparation and decision making. However, effective\ninfluenza forecasting still remains a challenge despite increasing research\ninterest. It is even more challenging amidst the COVID pandemic, when the\ninfluenza-like illness (ILI) counts are affected by various factors such as\nsymptomatic similarities with COVID-19 and shift in healthcare seeking patterns\nof the general population. Under the current pandemic, historical influenza\nmodels carry valuable expertise about the disease dynamics but face\ndifficulties adapting. Therefore, we propose CALI-Net, a neural transfer\nlearning architecture which allows us to 'steer' a historical disease\nforecasting model to new scenarios where flu and COVID co-exist. Our framework\nenables this adaptation by automatically learning when it should emphasize\nlearning from COVID-related signals and when it should learn from the\nhistorical model. Thus, we exploit representations learned from historical ILI\ndata as well as the limited COVID-related signals. Our experiments demonstrate\nthat our approach is successful in adapting a historical forecasting model to\nthe current pandemic. In addition, we show that success in our primary goal,\nadaptation, does not sacrifice overall performance as compared with\nstate-of-the-art influenza forecasting approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:35:43 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 04:08:35 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Rodr\u00edguez", "Alexander", ""], ["Muralidhar", "Nikhil", ""], ["Adhikari", "Bijaya", ""], ["Tabassum", "Anika", ""], ["Ramakrishnan", "Naren", ""], ["Prakash", "B. Aditya", ""]]}, {"id": "2009.11409", "submitter": "Yanyi Song", "authors": "Yanyi Song, Xiang Zhou, Jian Kang, Max T. Aung, Min Zhang, Wei Zhao,\n  Belinda L. Needham, Sharon L. R. Kardia, Yongmei Liu, John D. Meeker,\n  Jennifer A. Smith, Bhramar Mukherjee", "title": "Bayesian Hierarchical Models for High-Dimensional Mediation Analysis\n  with Coordinated Selection of Correlated Mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian high-dimensional mediation analysis to identify among a\nlarge set of correlated potential mediators the active ones that mediate the\neffect from an exposure variable to an outcome of interest. Correlations among\nmediators are commonly observed in modern data analysis; examples include the\nactivated voxels within connected regions in brain image data, regulatory\nsignals driven by gene networks in genome data and correlated exposure data\nfrom the same source. When correlations are present among active mediators,\nmediation analysis that fails to account for such correlation can be\nsub-optimal and may lead to a loss of power in identifying active mediators.\nBuilding upon a recent high-dimensional mediation analysis framework, we\npropose two Bayesian hierarchical models, one with a Gaussian mixture prior\nthat enables correlated mediator selection and the other with a Potts mixture\nprior that accounts for the correlation among active mediators in mediation\nanalysis. We develop efficient sampling algorithms for both methods. Various\nsimulations demonstrate that our methods enable effective identification of\ncorrelated active mediators, which could be missed by using existing methods\nthat assume prior independence among active mediators. The proposed methods are\napplied to the LIFECODES birth cohort and the Multi-Ethnic Study of\nAtherosclerosis (MESA) and identified new active mediators with important\nbiological implications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:40:27 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Song", "Yanyi", ""], ["Zhou", "Xiang", ""], ["Kang", "Jian", ""], ["Aung", "Max T.", ""], ["Zhang", "Min", ""], ["Zhao", "Wei", ""], ["Needham", "Belinda L.", ""], ["Kardia", "Sharon L. R.", ""], ["Liu", "Yongmei", ""], ["Meeker", "John D.", ""], ["Smith", "Jennifer A.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2009.11452", "submitter": "Xiaoke Zhang", "authors": "Rui Miao, Xiaoke Zhang, Raymond K. W. Wong", "title": "A Wavelet-Based Independence Test for Functional Data with an\n  Application to MEG Functional Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring and testing the dependency between multiple random functions is\noften an important task in functional data analysis. In the literature, a\nmodel-based method relies on a model which is subject to the risk of model\nmisspecification, while a model-free method only provides a correlation measure\nwhich is inadequate to test independence. In this paper, we adopt the\nHilbert-Schmidt Independence Criterion (HSIC) to measure the dependency between\ntwo random functions. We develop a two-step procedure by first pre-smoothing\neach function based on its discrete and noisy measurements and then applying\nthe HSIC to recovered functions. To ensure the compatibility between the two\nsteps such that the effect of the pre-smoothing error on the subsequent HSIC is\nasymptotically negligible, we propose to use wavelet soft-thresholding for\npre-smoothing and Besov-norm-induced kernels for HSIC. We also provide the\ncorresponding asymptotic analysis. The superior numerical performance of the\nproposed method over existing ones is demonstrated in a simulation study.\nMoreover, in an magnetoencephalography (MEG) data application, the functional\nconnectivity patterns identified by the proposed method are more anatomically\ninterpretable than those by existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 02:21:37 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Miao", "Rui", ""], ["Zhang", "Xiaoke", ""], ["Wong", "Raymond K. W.", ""]]}, {"id": "2009.11499", "submitter": "Dorota Toczydlowska", "authors": "Dorota Toczydlowska, Gareth W. Peters, Pavel V. Shevchenko", "title": "Parsimonious Feature Extraction Methods: Extending Robust Probabilistic\n  Projections with Generalized Skew-t", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generalisation to the Student-t Probabilistic Principal\nComponent methodology which: (1) accounts for an asymmetric distribution of the\nobservation data; (2) is a framework for grouped and generalised\nmultiple-degree-of-freedom structures, which provides a more flexible approach\nto modelling groups of marginal tail dependence in the observation data; and\n(3) separates the tail effect of the error terms and factors. The new feature\nextraction methods are derived in an incomplete data setting to efficiently\nhandle the presence of missing values in the observation vector. We discuss\nvarious special cases of the algorithm being a result of simplified assumptions\non the process generating the data. The applicability of the new framework is\nillustrated on a data set that consists of crypto currencies with the highest\nmarket capitalisation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 05:53:41 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Toczydlowska", "Dorota", ""], ["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "2009.12110", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Dimitrios Spiliotopoulos", "title": "Similarity of multiple dose-response curves in interlaboratory studies\n  in regulatory toxicology", "comments": "2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To claim similarity of multiple dose-response curves in interlaboratory\nstudies in regulatory toxicology is a relevant issue during the assay\nvalidation process. Here we demonstrated the use of dose-by-laboratory\ninteraction contrasts, particularly Williams-type by total mean contrasts. With\nthe CRAN packages statint and multcomp in the open-source software R the\nestimation of adjusted p-values or compatible simultaneous confidence intervals\nis relatively easy. The interpretation in terms of global or partial\nequivalence, i.e. similarity, is challenging, because thresholds are not\navailable a-priori. This approach is demonstrated by selected in-vitro Ames MPF\nassay data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:05:00 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Spiliotopoulos", "Dimitrios", ""]]}, {"id": "2009.12165", "submitter": "Juan Manuel Carrillo Garcia", "authors": "Juan Carrillo, Mark Crowley", "title": "Integration of Roadside Camera Images and Weather Data for Monitoring\n  Winter Road Surface Conditions", "comments": "For associated GitHub repository see\n  https://github.com/jmcarrillog/data-integration-for-road-monitoring", "journal-ref": "29th CARSP Conference, Calgary, Alberta, May 26-29, 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the winter season, real-time monitoring of road surface conditions is\ncritical for the safety of drivers and road maintenance operations. Previous\nresearch has evaluated the potential of image classification methods for\ndetecting road snow coverage by processing images from roadside cameras\ninstalled in RWIS (Road Weather Information System) stations. However, there\nare a limited number of RWIS stations across Ontario, Canada; therefore, the\nnetwork has reduced spatial coverage. In this study, we suggest improving\nperformance on this task through the integration of images and weather data\ncollected from the RWIS stations with images from other MTO (Ministry of\nTransportation of Ontario) roadside cameras and weather data from Environment\nCanada stations. We use spatial statistics to quantify the benefits of\nintegrating the three datasets across Southern Ontario, showing evidence of a\nsix-fold increase in the number of available roadside cameras and therefore\nimproving the spatial coverage in the most populous ecoregions in Ontario.\nAdditionally, we evaluate three spatial interpolation methods for inferring\nweather variables in locations without weather measurement instruments and\nidentify the one that offers the best tradeoff between accuracy and ease of\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:43:27 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Carrillo", "Juan", ""], ["Crowley", "Mark", ""]]}, {"id": "2009.12217", "submitter": "Fui Swen Kuh", "authors": "F. Swen Kuh, Grace S. Chiu, Anton H. Westveld", "title": "Latent Causal Socioeconomic Health Index", "comments": "31 pages. arXiv admin note: substantial text overlap with\n  arXiv:1911.00512", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. We build upon the latent health factor\nindex (LHFI) approach that has been used to assess the unobservable\necological/ecosystem health. This framework integratively models the\nrelationship between metrics, the latent health, and the covariates that drive\nthe notion of health. In this paper, the LHFI structure is integrated with\nspatial modeling and statistical causal modeling, so as to evaluate the impact\nof a continuous policy variable (mandatory maternity leave days and\ngovernment's expenditure on healthcare, respectively) on a nation's\nsocioeconomic health, while formally accounting for spatial dependency among\nthe nations. A novel visualization technique for evaluating covariate balance\nis also introduced for the case of a continuous policy (treatment) variable. We\napply our LACSH model to countries around the world using data on various\nmetrics and potential covariates pertaining to different aspects of societal\nhealth. The approach is structured in a Bayesian hierarchical framework and\nresults are obtained by Markov chain Monte Carlo techniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 07:00:46 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Kuh", "F. Swen", ""], ["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "2009.12267", "submitter": "Daniele Durante", "authors": "Sirio Legramanti, Tommaso Rigon, Daniele Durante", "title": "Bayesian Testing for Exogenous Partition Structures in Stochastic Block\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data often exhibit block structures characterized by clusters of\nnodes with similar patterns of edge formation. When such relational data are\ncomplemented by additional information on exogenous node partitions, these\nsources of knowledge are typically included in the model to supervise the\ncluster assignment mechanism or to improve inference on edge probabilities.\nAlthough these solutions are routinely implemented, there is a lack of formal\napproaches to test if a given external node partition is in line with the\nendogenous clustering structure encoding stochastic equivalence patterns among\nthe nodes in the network. To fill this gap, we develop a formal Bayesian\ntesting procedure which relies on the calculation of the Bayes factor between a\nstochastic block model with known grouping structure defined by the exogenous\nnode partition and an infinite relational model that allows the endogenous\nclustering configurations to be unknown, random and fully revealed by the\nblock-connectivity patterns in the network. A simple Markov chain Monte Carlo\nmethod for computing the Bayes factor and quantifying uncertainty in the\nendogenous groups is proposed. This routine is evaluated in simulations and in\nan application to study exogenous equivalence structures in brain networks of\nAlzheimer's patients.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:33:59 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Legramanti", "Sirio", ""], ["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""]]}, {"id": "2009.12351", "submitter": "Andrew Raim", "authors": "Ryan Janicki, Andrew M. Raim, Scott H. Holan, and Jerry Maples", "title": "Bayesian Nonparametric Multivariate Spatial Mixture Mixed Effects Models\n  with Application to American Community Survey Special Tabulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging multivariate spatial dependence to improve the precision of\nestimates using American Community Survey data and other sample survey data has\nbeen a topic of recent interest among data-users and federal statistical\nagencies. One strategy is to use a multivariate spatial mixed effects model\nwith a Gaussian observation model and latent Gaussian process model. In\npractice, this works well for a wide range of tabulations. Nevertheless, in\nsituations that exhibit heterogeneity among geographies and/or sparsity in the\ndata, the Gaussian assumptions may be problematic and lead to underperformance.\nTo remedy these situations, we propose a multivariate hierarchical Bayesian\nnonparametric mixed effects spatial mixture model to increase model\nflexibility. The number of clusters is chosen automatically in a data-driven\nmanner. The effectiveness of our approach is demonstrated through a simulation\nstudy and motivating application of special tabulations for American Community\nSurvey data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 17:29:48 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Janicki", "Ryan", ""], ["Raim", "Andrew M.", ""], ["Holan", "Scott H.", ""], ["Maples", "Jerry", ""]]}, {"id": "2009.12386", "submitter": "Eduardo M. Vasconcelos", "authors": "Eduardo M. Vasconcelos and Adriano Gouveia de Souza", "title": "Regressor: A C program for Combinatorial Regressions", "comments": "6 pages, 3 figures, 1 algorithm and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics, researchers use Regression models for data analysis and\nprediction in many productive sectors (industry, business, academy, etc.).\nRegression models are mathematical functions representing an approximation of\ndependent variable $Y$ from n independent variables $X_i \\in X$. The literature\npresents many regression methods divided into single and multiple regressions.\nThere are several procedures to generate regression models and sets of\ncommercial and academic tools that implement these procedures. This work\npresents one open-source program called Regressor that makes models from a\nspecific variation of polynomial regression. These models relate the\nindependent variables to generate an approximation of the original output\ndependent data. In many tests, Regressor was able to build models five times\nmore accurate than commercial tools.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:10:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Vasconcelos", "Eduardo M.", ""], ["de Souza", "Adriano Gouveia", ""]]}, {"id": "2009.12523", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung and Ying Hung", "title": "Efficient calibration for imperfect epidemic models with applications to\n  the analysis of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of unknown parameters in simulations, also known as\ncalibration, is crucial for practical management of epidemics and prediction of\npandemic risk. A simple yet widely used approach is to estimate the parameters\nby minimizing the sum of the squared distances between actual observations and\nsimulation outputs. It is shown in this paper that this method is inefficient,\nparticularly when the epidemic models are developed based on certain\nsimplifications of reality, also known as imperfect models which are commonly\nused in practice. To address this issue, a new estimator is introduced that is\nasymptotically consistent, has a smaller estimation variance than the least\nsquares estimator, and achieves the semiparametric efficiency. Numerical\nstudies are performed to examine the finite sample performance. The proposed\nmethod is applied to the analysis of the COVID-19 pandemic for 20 countries\nbased on the SEIR (Susceptible-Exposed-Infectious-Recovered) model with both\ndeterministic and stochastic simulations. The estimation of the parameters,\nincluding the basic reproduction number and the average incubation period,\nreveal the risk of disease outbreaks in each country and provide insights to\nthe design of public health interventions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 06:00:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sung", "Chih-Li", ""], ["Hung", "Ying", ""]]}, {"id": "2009.12558", "submitter": "Arnald Puy", "authors": "Arnald Puy, Samuele Lo Piano, Andrea Saltelli", "title": "Is VARS more intuitive and efficient than Sobol' indices?", "comments": "Currently under review in Environmental Modelling & Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Variogram Analysis of Response Surfaces (VARS) has been proposed by\nRazavi and Gupta as a new comprehensive framework in sensitivity analysis.\nAccording to these authors, VARS provides a more intuitive notion of\nsensitivity and it is much more computationally efficient than Sobol' indices.\nHere we review these arguments and critically compare the performance of\nVARS-TO, for total-order index, against the total-order Jansen estimator. We\nargue that, unlike classic variance-based methods, VARS lacks a clear\ndefinition of what an \"important\" factor is, and show that the alleged\ncomputational superiority of VARS does not withstand scrutiny. We conclude that\nwhile VARS enriches the spectrum of existing methods for sensitivity analysis,\nespecially for a diagnostic use of mathematical models, it complements rather\nthan substitutes classic estimators used in variance-based sensitivity\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 10:25:10 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 17:13:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Puy", "Arnald", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "2009.12625", "submitter": "\\'Alvaro Briz-Red\\'on", "authors": "\\'Alvaro Briz-Red\\'on", "title": "The impact of modelling choices on modelling outcomes: a spatio-temporal\n  study of the association between COVID-19 spread and environmental conditions\n  in Catalonia (Spain)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choices that researchers make while conducting a statistical analysis\nusually have a notable impact on the results. This fact has become evident in\nthe ongoing research of the association between the environment and the\nevolution of the COVID-19 pandemic, in light of the hundreds of contradictory\nstudies that have already been published on this issue in just a few months. In\nthis paper, a COVID-19 dataset containing the number of daily cases registered\nin the regions of Catalonia (Spain) since the start of the pandemic is\nanalysed. Specifically, the possible effect of several environmental variables\n(solar exposure, mean temperature, and wind speed) on the number of cases is\nassessed. Thus, the first objective of the paper is to show how the choice of a\ncertain type of statistical model to conduct the analysis can have a severe\nimpact on the associations that are inferred between the covariates and the\nresponse variable. Secondly, it is shown how the use of spatio-temporal models\naccounting for the nature of the data allows understanding the evolution of the\npandemic in space and time.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 15:45:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Briz-Red\u00f3n", "\u00c1lvaro", ""]]}, {"id": "2009.12649", "submitter": "Piet Groeneboom", "authors": "Piet Groeneboom", "title": "Estimation of the incubation time distribution for COVID-19", "comments": "26 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider smooth nonparametric estimation of the incubation time\ndistribution of COVID-19, in connection with the investigation of researchers\nfrom the National Institute for Public Health and the Environment (Dutch: RIVM)\nof 88 travelers from Wuhan: Backer et al (2020). The advantages of the smooth\nnonparametric approach w.r.t. the parametric approach, using three parametric\ndistributions (Weibull, log-normal and gamma) in Backer et al (2020) is\ndiscussed.\n  It is shown that the typical rate of convergence of the smooth estimate of\nthe density is $n^{2/7}$ in a continuous version of the model, where $n$ is the\nsample size. The (non-smoothed) nonparametric maximum likelihood estimator\n(MLE) itself is computed by the iterative convex minorant algorithm (Groeneboom\nand Jongbloed (2014)). All computations are available as {\\tt R} scripts in\nGroeneboom (2020).\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 17:25:08 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:44:39 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 22:24:55 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2020 12:38:14 GMT"}, {"version": "v5", "created": "Thu, 21 Jan 2021 10:26:34 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Groeneboom", "Piet", ""]]}, {"id": "2009.12686", "submitter": "Abhik Ghosh PhD", "authors": "Amarnath Nandy, Abhik Ghosh, Ayanendranath Basu, Leandro Pardo", "title": "Robust Hypothesis Testing and Model Selection for Parametric\n  Proportional Hazard Regression Models", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semi-parametric Cox proportional hazards regression model has been widely\nused for many years in several applied sciences. However, a fully parametric\nproportional hazards model, if appropriately assumed, can often lead to more\nefficient inference. To tackle the extreme non-robustness of the traditional\nmaximum likelihood estimator in the presence of outliers in the data under such\nfully parametric proportional hazard models, a robust estimation procedure has\nrecently been proposed extending the concept of the minimum density power\ndivergence estimator (MDPDE) under this set-up. In this paper, we consider the\nproblem of statistical inference under the parametric proportional hazards\nmodel and develop robust Wald-type hypothesis testing and model selection\nprocedures using the MDPDEs. We have also derived the necessary asymptotic\nresults which are used to construct the testing procedure for general composite\nhypothesis and study its asymptotic powers. The claimed robustness properties\nare studied theoretically via appropriate influence function analyses. We have\nstudied the finite sample level and power of the proposed MDPDE based Wald type\ntest through extensive simulations where comparisons are also made with the\nexisting semi-parametric methods. The important issue of the selection of\nappropriate robustness tuning parameter is also discussed. The practical\nusefulness of the proposed robust testing and model selection procedures is\nfinally illustrated through three interesting real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 21:01:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nandy", "Amarnath", ""], ["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "2009.12780", "submitter": "Anna Jenul", "authors": "Anna Jenul, Stefan Schrunner, Kristian Hovde Liland, Ulf Geir Indahl,\n  Cecilia Marie Futsaether, Oliver Tomic", "title": "RENT -- Repeated Elastic Net Technique for Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an essential step in data science pipelines to reduce\nthe complexity of models trained on large datasets. While a major part of\nfeature selection research focuses on optimizing predictive performance, there\nare only few studies that investigate the integration of feature selection\nstability into the feature selection process. Taking advantage of feature\nselection stability has the potential to enhance interpretability of machine\nlearning models whilst maintaining predictive performance. In this study we\npresent the RENT feature selector for binary classification and regression\nproblems. The proposed methodology is based on an ensemble of elastic net\nregularized models, trained on unique subsets of the dataset. RENT selects\nfeatures based on three criteria evaluating the weight distributions of\nfeatures across all elementary models. Compared to conventional approaches,\nRENT simultaneously performs high-quality feature selection while gathering\nuseful information for model interpretation. In addition, the proposed\nensemble-based selection criteria guarantee robustness of the model by\nselecting features with high stability. In an experimental evaluation, we\ncompare feature selection quality on eight multivariate datasets: six for\nbinary classification and two for regression. We benchmark RENT against six\nestablished feature selectors. In terms of both, number of features selected\nand predictive performance, RENT delivers on-par results with the best\nperforming competitors. The additional information on stability provided by\nRENT can be integrated in an exploratory post-hoc analysis for further insight\nas demonstrated in a use-case from the healthcare domain.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 07:55:52 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:55:58 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Jenul", "Anna", ""], ["Schrunner", "Stefan", ""], ["Liland", "Kristian Hovde", ""], ["Indahl", "Ulf Geir", ""], ["Futsaether", "Cecilia Marie", ""], ["Tomic", "Oliver", ""]]}, {"id": "2009.12964", "submitter": "Riccardo Rastelli", "authors": "Hugo Dolan and Riccardo Rastelli", "title": "A model-based approach to assess epidemic risk", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how international flights can facilitate the spread of an epidemic\nto a worldwide scale. We combine an infrastructure network of flight\nconnections with a population density dataset to derive the mobility network,\nand then we define an epidemic framework to model the spread of the disease.\nOur approach combines a compartmental SEIRS model with a graph diffusion model\nto capture the clusteredness of the distribution of the population. The\nresulting model is characterised by the dynamics of a metapopulation SEIRS,\nwith amplification or reduction of the infection rate which is determined also\nby the mobility of individuals. We use simulations to characterise and study a\nvariety of realistic scenarios that resemble the recent spread of COVID-19.\nCrucially, we define a formal framework that can be used to design epidemic\nmitigation strategies: we propose an optimisation approach based on genetic\nalgorithms that can be used to identify an optimal airport closure strategy,\nand that can be employed to aid decision making for the mitigation of the\nepidemic, in a timely manner.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:08:57 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 21:36:40 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Dolan", "Hugo", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "2009.12983", "submitter": "Yuezhou Zhang", "authors": "Yuezhou Zhang, Amos A Folarin, Shaoxiong Sun, Nicholas Cummins,\n  Rebecca Bendayan Yatharth Ranjan, Zulqarnain Rashid, Pauline Conde, Callum\n  Stewart, Petroula Laiou, Faith Matcham, Katie White, Femke Lamers, Sara\n  Siddi, Sara Simblett, Inez Myin-Germeys, Aki Rintala, Til Wykes, Josep Maria\n  Haro, Brenda WJH Pennix, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson", "title": "The Relationship between Major Depression Symptom Severity and Sleep\n  Collected Using a Wristband Wearable Device: Multi-centre Longitudinal\n  Observational Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in mental health has implicated sleep pathologies with depression.\nHowever, the gold standard for sleep assessment, polysomnography, is not\nsuitable for long-term, continuous, monitoring of daily sleep, and methods such\nas sleep diaries rely on subjective recall, which is qualitative and\ninaccurate. Wearable devices, on the other hand, provide a low-cost and\nconvenient means to monitor sleep in home settings. The main aim of this study\nwas to devise and extract sleep features, from data collected using a wearable\ndevice, and analyse their correlation with depressive symptom severity and\nsleep quality, as measured by the self-assessed Patient Health Questionnaire\n8-item. Daily sleep data were collected passively by Fitbit wristband devices,\nand depressive symptom severity was self-reported every two weeks by the PHQ-8.\nThe data used in this paper included 2,812 PHQ-8 records from 368 participants\nrecruited from three study sites in the Netherlands, Spain, and the UK.We\nextracted 21 sleep features from Fitbit data which describe sleep in the\nfollowing five aspects: sleep architecture, sleep stability, sleep quality,\ninsomnia, and hypersomnia. Linear mixed regression models were used to explore\nassociations between sleep features and depressive symptom severity. The z-test\nwas used to evaluate the significance of the coefficient of each feature. We\ntested our models on the entire dataset and individually on the data of three\ndifferent study sites. We identified 16 sleep features that were significantly\ncorrelated with the PHQ-8 score on the entire dataset. Associations between\nsleep features and the PHQ-8 score varied across different sites, possibly due\nto the difference in the populations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 23:53:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Yuezhou", ""], ["Folarin", "Amos A", ""], ["Sun", "Shaoxiong", ""], ["Cummins", "Nicholas", ""], ["Ranjan", "Rebecca Bendayan Yatharth", ""], ["Rashid", "Zulqarnain", ""], ["Conde", "Pauline", ""], ["Stewart", "Callum", ""], ["Laiou", "Petroula", ""], ["Matcham", "Faith", ""], ["White", "Katie", ""], ["Lamers", "Femke", ""], ["Siddi", "Sara", ""], ["Simblett", "Sara", ""], ["Myin-Germeys", "Inez", ""], ["Rintala", "Aki", ""], ["Wykes", "Til", ""], ["Haro", "Josep Maria", ""], ["Pennix", "Brenda WJH", ""], ["Narayan", "Vaibhav A", ""], ["Hotopf", "Matthew", ""], ["Dobson", "Richard JB", ""]]}, {"id": "2009.13129", "submitter": "Yueh Wang", "authors": "Yueh Wang and Hung Hung", "title": "Statistical Inference on the Cure Time", "comments": "77 pages, 29 figures, PhD dissertation, Ins. of Epidemiology and\n  Preventive Medicine, National Taiwan University", "journal-ref": null, "doi": "10.6342/NTU201904221", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population-based cancer survival analysis, the net survival is important\nfor government to assess health care programs. For decades, it is observed that\nthe net survival reaches a plateau after long-term follow-up, this is so called\n``statistical cure''. Several methods were proposed to address the statistical\ncure. Besides, the cure time can be used to evaluate the time period of a\nhealth care program for a specific patient population, and it also can be\nhelpful for a clinician to explain the prognosis for patients, therefore the\ncure time is an important health care index. However, those proposed methods\nassume the cure time to be infinity, thus it is inconvenient to make inference\non the cure time. In this dissertation, we define a more general concept of\nstatistical cure via conditional survival. Based on the newly defined\nstatistical cure, the cure time is well defined. We develop cure time model\nmethodologies and show a variety of properties through simulation. In data\nanalysis, cure times are estimated for 22 major cancers in Taiwan, we further\nuse colorectal cancer data as an example to conduct statistical inference via\ncure time model with covariate sex, age group, and stage. This dissertation\nprovides a methodology to obtain cure time estimate, which can contribute to\npublic health policy making.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:20:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Yueh", ""], ["Hung", "Hung", ""]]}, {"id": "2009.13141", "submitter": "Mario Di Mauro", "authors": "Mario Di Mauro, Maurizio Longo, Fabio Postiglione", "title": "Availability Evaluation of Multi-tenant Service Function Chaining\n  Infrastructures by Multidimensional Universal Generating Function", "comments": null, "journal-ref": null, "doi": "10.1109/TSC.2018.2885748", "report-no": null, "categories": "cs.NI cs.PF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network Function Virtualization (NFV) paradigm has been devised as an\nenabler of next generation network infrastructures by speeding up the\nprovisioning and the composition of novel network services. The latter are\nimplemented via a chain of virtualized network functions, a process known as\nService Function Chaining. In this paper, we evaluate the availability of\nmulti-tenant SFC infrastructures, where every network function is modeled as a\nmulti-state system and is shared among different and independent tenants. To\nthis aim, we propose a Universal Generating Function (UGF) approach, suitably\nextended to handle performance vectors, that we call Multidimensional UGF. This\nnovel methodology is validated in a realistic multi-tenant telecommunication\nnetwork scenario, where the service chain is composed by the network elements\nof an IP Multimedia Subsystem implemented via NFV. A steady-state availability\nevaluation of such an exemplary system is presented and a redundancy\noptimization problem is solved, so providing the SFC infrastructure which\nminimizes deployment cost while respecting a given availability requirement.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:39:53 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Di Mauro", "Mario", ""], ["Longo", "Maurizio", ""], ["Postiglione", "Fabio", ""]]}, {"id": "2009.13335", "submitter": "Antoine Bichat", "authors": "Bichat Antoine, Ambroise Christophe and Mariadassou Mahendra", "title": "Hierarchical correction of $p$-values via a tree running\n  Ornstein-Uhlenbeck process", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical testing is classically used as an exploratory tool to search for\nassociation between a phenotype and many possible explanatory variables. This\napproach often leads to multiple dependence testing under dependence. We assume\na hierarchical structure between tests via an Ornstein-Uhlenbeckprocess on a\ntree. The process correlation structure is used for smoothing the p-values. We\ndesign a penalized estimation of the mean of the OU process for p-value\ncomputation. The performances of the algorithm are assessed via simulations.\nIts ability to discover new associations is demonstrated on a metagenomic\ndataset. The corresponding R package is available from\nhttps://github.com/abichat/zazou.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:00:24 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 11:55:28 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Antoine", "Bichat", ""], ["Christophe", "Ambroise", ""], ["Mahendra", "Mariadassou", ""]]}, {"id": "2009.13384", "submitter": "Michael B\\\"ucker", "authors": "Michael B\\\"ucker and Gero Szepannek and Alicja Gosiewska and\n  Przemyslaw Biecek", "title": "Transparency, Auditability and eXplainability of Machine Learning Models\n  in Credit Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.GN q-fin.EC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major requirement for credit scoring models is to provide a maximally\naccurate risk prediction. Additionally, regulators demand these models to be\ntransparent and auditable. Thus, in credit scoring, very simple predictive\nmodels such as logistic regression or decision trees are still widely used and\nthe superior predictive power of modern machine learning algorithms cannot be\nfully leveraged. Significant potential is therefore missed, leading to higher\nreserves or more credit defaults. This paper works out different dimensions\nthat have to be considered for making credit scoring models understandable and\npresents a framework for making ``black box'' machine learning models\ntransparent, auditable and explainable. Following this framework, we present an\noverview of techniques, demonstrate how they can be applied in credit scoring\nand how results compare to the interpretability of score cards. A real world\ncase study shows that a comparable degree of interpretability can be achieved\nwhile machine learning techniques keep their ability to improve predictive\npower.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:00:13 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["B\u00fccker", "Michael", ""], ["Szepannek", "Gero", ""], ["Gosiewska", "Alicja", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2009.13404", "submitter": "Soichiro Yamauchi", "authors": "Soichiro Yamauchi", "title": "Difference-in-Differences for Ordinal Outcomes: Application to the\n  Effect of Mass Shootings on Attitudes toward Gun Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difference-in-differences (DID) design is widely used in observational\nstudies to estimate the causal effect of a treatment when repeated observations\nover time are available. Yet, almost all existing methods assume linearity in\nthe potential outcome (parallel trends assumption) and target the additive\neffect. In social science research, however, many outcomes of interest are\nmeasured on an ordinal scale. This makes the linearity assumption inappropriate\nbecause the difference between two ordinal potential outcomes is not well\ndefined. In this paper, I propose a method to draw causal inferences for\nordinal outcomes under the DID design. Unlike existing methods, the proposed\nmethod utilizes the latent variable framework to handle the non-numeric nature\nof the outcome, enabling identification and estimation of causal effects based\non the assumption on the quantile of the latent continuous variable. The paper\nalso proposes an equivalence-based test to assess the plausibility of the key\nidentification assumption when additional pre-treatment periods are available.\nThe proposed method is applied to a study estimating the causal effect of mass\nshootings on the public's support for gun control. I find little evidence for a\nuniform shift toward pro-gun control policies as found in the previous study,\nbut find that the effect is concentrated on left-leaning respondents who\nexperienced the shooting for the first time in more than a decade.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:22:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yamauchi", "Soichiro", ""]]}, {"id": "2009.13423", "submitter": "Marwah Soliman", "authors": "Marwah Soliman, Vyacheslav Lyubchich, Yulia R. Gel", "title": "Ensemble Forecasting of the Zika Space-TimeSpread with Topological Data\n  Analysis", "comments": "29 page, 5 figures", "journal-ref": "Environmetrics, 2020", "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As per the records of theWorld Health Organization, the first formally\nreported incidence of Zika virus occurred in Brazil in May 2015. The disease\nthen rapidly spread to other countries in Americas and East Asia, affecting\nmore than 1,000,000 people. Zika virus is primarily transmitted through bites\nof infected mosquitoes of the species Aedes (Aedes aegypti and Aedes\nalbopictus). The abundance of mosquitoes and, as a result, the prevalence of\nZika virus infections are common in areas which have high precipitation, high\ntemperature, and high population density.Nonlinear spatio-temporal dependency\nof such data and lack of historical public health records make prediction of\nthe virus spread particularly challenging. In this article, we enhance Zika\nforecasting by introducing the concepts of topological data analysis and,\nspecifically, persistent homology of atmospheric variables, into the virus\nspread modeling. The topological summaries allow for capturing higher order\ndependencies among atmospheric variables that otherwise might be unassessable\nvia conventional spatio-temporal modeling approaches based on geographical\nproximity assessed via Euclidean distance. We introduce a new concept of\ncumulative Betti numbers and then integrate the cumulative Betti numbers as\ntopological descriptors into three predictive machine learning models: random\nforest, generalized boosted regression, and deep neural network. Furthermore,\nto better quantify for various sources of uncertainties, we combine the\nresulting individual model forecasts into an ensemble of the Zika spread\npredictions using Bayesian model averaging. The proposed methodology is\nillustrated in application to forecasting of the Zika space-time spread in\nBrazil in the year 2018.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:42:19 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Soliman", "Marwah", ""], ["Lyubchich", "Vyacheslav", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2009.13484", "submitter": "Marcelo Medeiros", "authors": "Carlos B. Carneiro, I\\'uri H. Ferreira, Marcelo C. Medeiros, Henrique\n  F. Pires and Eduardo Zilberman", "title": "Lockdown effects in US states: an artificial counterfactual approach", "comments": "Updated versions of this paper will be available on\n  http://139.82.34.174/mcm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM econ.GN q-fin.EC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopt an artificial counterfactual approach to assess the impact of\nlockdowns on the short-run evolution of the number of cases and deaths in some\nUS states. To do so, we explore the different timing in which US states adopted\nlockdown policies, and divide them among treated and control groups. For each\ntreated state, we construct an artificial counterfactual. On average, and in\nthe very short-run, the counterfactual accumulated number of cases would be two\ntimes larger if lockdown policies were not implemented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:21:03 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 18:27:55 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Carneiro", "Carlos B.", ""], ["Ferreira", "I\u00fari H.", ""], ["Medeiros", "Marcelo C.", ""], ["Pires", "Henrique F.", ""], ["Zilberman", "Eduardo", ""]]}, {"id": "2009.13577", "submitter": "Abdollah Jalilian", "authors": "Abdollah Jalilian and Jorge Mateu", "title": "A hierarchical spatio-temporal model to analyze relative risk variations\n  of COVID-19: a focus on Spain, Italy and Germany", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.1007/s00477-021-02003-2", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus disease (COVID-19) has spread rapidly across the world\nin a short period of time and with a heterogeneous pattern. Understanding the\nunderlying temporal and spatial dynamics in the spread of COVID-19 can result\nin informed and timely public health policies. In this paper, we use a\nspatio-temporal stochastic model to explain the temporal and spatial variations\nin the daily number of new confirmed cases in Spain, Italy and Germany from\nlate February to mid September 2020. Using a hierarchical Bayesian framework,\nwe found that the temporal trend of the epidemic in the three countries rapidly\nreached their peaks and slowly started to decline at the beginning of April and\nthen increased and reached their second maximum in August. However decline and\nincrease of the temporal trend seems to be sharper in Spain and smoother in\nGermany. The spatial heterogeneity of the relative risk of COVID-19 in Spain is\nalso more pronounced than Italy and Germany.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:49:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Jalilian", "Abdollah", ""], ["Mateu", "Jorge", ""]]}, {"id": "2009.13595", "submitter": "Kasun Chandrarathna", "authors": "Kasun Chandrarathna, Arman Edalati, AhmadReza Fourozan tabar", "title": "Forecasting Short-term load using Econometrics time series model with\n  T-student Distribution", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "e-ISSN:2582-5208", "categories": "q-fin.ST cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By significant improvements in modern electrical systems, planning for unit\ncommitment and power dispatching of them are two big concerns between the\nresearchers. Short-term load forecasting plays a significant role in planning\nand dispatching them. In recent years, numerous works have been done on\nShort-term load forecasting. Having an accurate model for predicting the load\ncan be beneficial for optimizing the electrical sources and protecting energy.\nSeveral models such as Artificial Intelligence and Statistics model have been\nused to improve the accuracy of load forecasting. Among the statistics models,\ntime series models show a great performance. In this paper, an Autoregressive\nintegrated moving average (SARIMA) - generalized autoregressive conditional\nheteroskedasticity (GARCH) model as a powerful tool for modeling the\nconditional mean and volatility of time series with the T-student Distribution\nis used to forecast electric load in short period of time. The attained model\nis compared with the ARIMA model with Normal Distribution. Finally, the\neffectiveness of the proposed approach is validated by applying real electric\nload data from the Electric Reliability Council of Texas (ERCOT). KEYWORDS:\nElectricity load, Forecasting, Econometrics Time Series Forecasting, SARIMA\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 19:33:33 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Chandrarathna", "Kasun", ""], ["Edalati", "Arman", ""], ["tabar", "AhmadReza Fourozan", ""]]}, {"id": "2009.13629", "submitter": "Soraia Pereira", "authors": "M A Amaral Turkman, K F Turkman, P de Zea Bermudez, S Pereira, P\n  Pereira and M Carvalho", "title": "Calibration methods for spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an environmental framework, extreme values of certain spatio-temporal\nprocesses, for example wind speeds, are the main cause of severe damage in\nproperty, such as electrical networks, transport and agricultural\ninfrastructures. Therefore, availability of accurate data on such processes is\nhighly important in risk analysis, and in particular in producing probability\nmaps showing the spatial distribution of damage risks. Typically, as is the\ncase of wind speeds, data are available at few stations with many missing\nobservations and consequently simulated data are often used to augment\ninformation, due to simulated environmental data being available at high\nspatial and temporal resolutions. However, simulated data often mismatch\nobserved data, particularly at tails, therefore calibrating and bringing it in\nline with observed data may offer practitioners more reliable and richer data\nsources. Although the calibration methods that we describe in this manuscript\nmay equally apply to other environmental variables, we describe the methods\nspecifically with reference to wind data and its consequences. Since most\ndamages are caused by extreme winds, it is particularly important to calibrate\nthe right tail of simulated data based on observations. Response relationships\nbetween the extremes of simulated and observed data are by nature highly\nnon-linear and non-Gaussian, therefore data fusion techniques available for\nspatial data may not be adequate for this purpose. After giving a brief\ndescription of standard calibration and data fusion methods to update simulated\ndata based on the observed data, we propose and describe in detail a specific\nconditional quantile matching calibration method and show how our wind speed\ndata can be calibrated using this method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:14:41 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Turkman", "M A Amaral", ""], ["Turkman", "K F", ""], ["Bermudez", "P de Zea", ""], ["Pereira", "S", ""], ["Pereira", "P", ""], ["Carvalho", "M", ""]]}, {"id": "2009.13670", "submitter": "Christian Sampson", "authors": "Christian Sampson, Alberto Carrassi, Ali Aydo\\u{g}du, and Chris K.R.T\n  Jones", "title": "Ensemble Kalman Filter for non-conservative moving mesh solvers with a\n  joint physics and mesh location update", "comments": "29 Pages, 16 Figures, Submitted to Quarterly Journal of the Royal\n  Meteorological Society", "journal-ref": null, "doi": "10.1002/qj.3980", "report-no": null, "categories": "stat.AP cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical solvers using adaptive meshes can focus computational power on\nimportant regions of a model domain capturing important or unresolved physics.\nThe adaptation can be informed by the model state, external information, or\nmade to depend on the model physics. In this latter case, one can think of the\nmesh configuration {\\it as part of the model state}. If observational data is\nto be assimilated into the model, the question of updating the mesh\nconfiguration with the physical values arises. Adaptive meshes present\nsignificant challenges when using popular ensemble Data Assimilation (DA)\nmethods. We develop a novel strategy for ensemble-based DA for which the\nadaptive mesh is updated along with the physical values. This involves\nincluding the node locations as a part of the model state itself allowing them\nto be updated automatically at the analysis step. This poses a number of\nchallenges which we resolve to produce an effective approach that promises to\napply with some generality. We evaluate our strategy with two testbed models in\n1-d comparing to a strategy that does not update the mesh configuration. We\nfind updating the mesh improves the fidelity and convergence of the filter.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 22:46:56 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Sampson", "Christian", ""], ["Carrassi", "Alberto", ""], ["Aydo\u011fdu", "Ali", ""], ["Jones", "Chris K. R. T", ""]]}, {"id": "2009.13974", "submitter": "Marion Hoffman", "authors": "Marion Hoffman, Per Block, Tom A.B. Snijders", "title": "Modeling partitions of individuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the central role of self-assembled groups in animal and human\nsocieties, statistical tools to explain their composition are limited. We\nintroduce a statistical framework for cross-sectional observations of groups\nwith exclusive membership to illuminate the social and organizational\nmechanisms that bring people together. Drawing from stochastic models for\nnetworks and partitions, the proposed framework introduces an exponential\nfamily of distributions for partitions. We derive its main mathematical\nproperties and suggest strategies to specify and estimate such models. A case\nstudy on hackathon events applies the developed framework to the study of\nmechanisms underlying the formation of self-assembled project teams.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:49:11 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hoffman", "Marion", ""], ["Block", "Per", ""], ["Snijders", "Tom A. B.", ""]]}, {"id": "2009.14127", "submitter": "Kaleb Phipps", "authors": "Kaleb Phipps, Sebastian Lerch, Maria Andersson, Ralf Mikut, Veit\n  Hagenmeyer, and Nicole Ludwig", "title": "Evaluating Ensemble Post-Processing for Wind Power Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the uncertainty in probabilistic wind power forecasts is\nchallenging, especially when uncertain input variables such as the weather,\nplay a role. Since ensemble weather predictions aim to capture the uncertainty\nin the weather system, they can be used to propagate this uncertainty through\nto subsequent wind power forecasting models. However, as weather ensemble\nsystems are known to be biased and underdispersed, meteorologists post-process\nthe ensembles. This post-processing can successfully correct the biases in the\nweather variables but has not been evaluated thoroughly in the context of\nsubsequent forecasts, such as wind power generation forecasts.\n  The present paper evaluates multiple strategies for applying ensemble\npost-processing to probabilistic wind power forecasts. We use Ensemble Model\nOutput Statistics (EMOS) as the post-processing method and evaluate four\npossible strategies: only using the raw ensembles without post-processing, a\none-step strategy where only the weather ensembles are post-processed, a\none-step strategy where we only post-process the power ensembles, and a\ntwo-step strategy where we post-process both the weather and power ensembles.\nResults show that post-processing the final wind power ensemble improves\nforecast performance regarding both calibration and sharpness, whilst only\npost-processing the weather ensembles does not necessarily lead to increased\nforecast performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:21:00 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 22:04:08 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Phipps", "Kaleb", ""], ["Lerch", "Sebastian", ""], ["Andersson", "Maria", ""], ["Mikut", "Ralf", ""], ["Hagenmeyer", "Veit", ""], ["Ludwig", "Nicole", ""]]}, {"id": "2009.14249", "submitter": "Kai Zhou", "authors": "Kai Zhou, Jiong Tang", "title": "Enhanced Bayesian Model Updating with Incomplete Modal Information Using\n  Parallel, Interactive and Adaptive Markov Chains", "comments": null, "journal-ref": null, "doi": "10.1016/j.jsv.2021.116331", "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element model updating is challenging because 1) the problem is\noftentimes underdetermined while the measurements are limited and/or\nincomplete; 2) many combinations of parameters may yield responses that are\nsimilar with respect to actual measurements; and 3) uncertainties inevitably\nexist. The aim of this research is to leverage upon computational intelligence\nthrough statistical inference to facilitate an enhanced, probabilistic finite\nelement model updating using incomplete modal response measurement. This new\nframework is built upon efficient inverse identification through optimization,\nwhereas Bayesian inference is employed to account for the effect of\nuncertainties. To overcome the computational cost barrier, we adopt Markov\nchain Monte Carlo (MCMC) to characterize the target function/distribution.\nInstead of using single Markov chain in conventional Bayesian approach, we\ndevelop a new sampling theory with multiple parallel, interactive and adaptive\nMarkov chains and incorporate into Bayesian inference. This can harness the\ncollective power of these Markov chains to realize the concurrent search of\nmultiple local optima. The number of required Markov chains and their\nrespective initial model parameters are automatically determined via Monte\nCarlo simulation-based sample pre-screening followed by K-means clustering\nanalysis. These enhancements can effectively address the aforementioned\nchallenges in finite element model updating. The validity of this framework is\nsystematically demonstrated through case studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:32:55 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2009.14310", "submitter": "J\\'er\\^ome-Alexis Chevalier", "authors": "J\\'er\\^ome-Alexis Chevalier, Alexandre Gramfort, Joseph Salmon,\n  Bertrand Thirion", "title": "Statistical control for spatio-temporal MEG/EEG source imaging with\n  desparsified multi-task Lasso", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting where and when brain regions activate in a cognitive task or in a\ngiven clinical condition is the promise of non-invasive techniques like\nmagnetoencephalography (MEG) or electroencephalography (EEG). This problem,\nreferred to as source localization, or source imaging, poses however a\nhigh-dimensional statistical inference challenge. While sparsity promoting\nregularizations have been proposed to address the regression problem, it\nremains unclear how to ensure statistical control of false detections.\nMoreover, M/EEG source imaging requires to work with spatio-temporal data and\nautocorrelated noise. To deal with this, we adapt the desparsified Lasso\nestimator -- an estimator tailored for high dimensional linear model that\nasymptotically follows a Gaussian distribution under sparsity and moderate\nfeature correlation assumptions -- to temporal data corrupted with\nautocorrelated noise. We call it the desparsified multi-task Lasso (d-MTLasso).\nWe combine d-MTLasso with spatially constrained clustering to reduce data\ndimension and with ensembling to mitigate the arbitrary choice of clustering;\nthe resulting estimator is called ensemble of clustered desparsified multi-task\nLasso (ecd-MTLasso). With respect to the current procedures, the two advantages\nof ecd-MTLasso are that i)it offers statistical guarantees and ii)it allows to\ntrade spatial specificity for sensitivity, leading to a powerful adaptive\nmethod. Extensive simulations on realistic head geometries, as well as\nempirical results on various MEG datasets, demonstrate the high recovery\nperformance of ecd-MTLasso and its primary practical benefit: offer a\nstatistically principled way to threshold MEG/EEG source maps.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 21:17:16 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 10:49:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""], ["Thirion", "Bertrand", ""]]}, {"id": "2009.14401", "submitter": "Daniel Simpson", "authors": "Lauren Kennedy, Katharine Khanna, Daniel Simpson, Andrew Gelman", "title": "Using sex and gender in survey adjustment", "comments": "19 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accounting for sex and gender characteristics is a complex, structural\nchallenge in social science research. While other methodology papers consider\nissues surrounding appropriate measurement, we consider how gender and sex\nimpact adjustments for non-response patterns in sampling and survey estimates.\nWe consider the problem of survey adjustment arising from the recent push\ntoward measuring sex or gender as a non-binary construct. This is challenging\nnot only in that response categories differ between sex and gender measurement,\nbut also in that both of these attributes are potentially multidimensional. In\nthis manuscript we reflect on similarities to measuring race/ethnicity before\nconsidering the ethical and statistical implications of the options available\nto us. We do not conclude with a single best recommendation but rather an\nawareness of the complexity of the issues surrounding this challenge and the\nbenefits and weaknesses of different approaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 02:59:35 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kennedy", "Lauren", ""], ["Khanna", "Katharine", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "2009.14610", "submitter": "R\\'emy Garnier", "authors": "R\\'emy Garnier", "title": "Concurrent Neural Network : A model of competition between times series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competition between times series often arises in sales prediction, when\nsimilar products are on sale on a marketplace. This article provides a model of\nthe presence of cannibalization between times series. This model creates a\n\"competitiveness\" function that depends on external features such as price and\nmargin. It also provides a theoretical guaranty on the error of the model under\nsome reasonable conditions, and implement this model using a neural network to\ncompute this competitiveness function. This implementation outperforms other\ntraditional time series methods and classical neural networks for market share\nprediction on a real-world data set.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:34:56 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 13:43:00 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Garnier", "R\u00e9my", ""]]}, {"id": "2009.14677", "submitter": "Karsten W\\\"ullems", "authors": "Karsten W\\\"ullems, Tim W. Nattkemper", "title": "SoRC -- Evaluation of Computational Molecular Co-Localization Analysis\n  in Mass Spectrometry Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational analysis of Mass Spectrometry Imaging (MSI) data aims at\nthe identification of interesting mass co-localizations and the visualization\nof their lateral distribution in the sample, usually a tissue cross section.\nBut as the morphological structure of tissues and the different kinds of mass\nco-localization naturally show a huge diversity, the selection and tuning of\nthe computational method is a time-consuming effort. In this work we address\nthe special problem of computationally grouping mass channel images according\nto their similarities in their lateral distribution patterns. Such an analysis\nis driven by the idea, that groups of molecules that feature a similar\ndistribution pattern may have a functional relation. But the selection of the\nsimilarity function and other parameters is often done by a time-consuming and\nunsatsifactory trial and error. We propose a new flexible workflow scheme\ncalled SoRC (sum of ranked cluster indices) for automating this tuning step and\nmaking it much more efficient. We test SoRC using three different data sets\nacquired from the lab for three different kinds of samples (barley seed, mouse\nbladder tissue, human PXE skin). We show, that SORC can be applied to score and\nvisualize the results obtained with the applied methods in short time without\ntoo much effort. In our application example, the SoRC results for the three\ndata sets reveal that a) some well-known similarity functions are suited to\nachieve good results for all three data sets and b) for the MSI data featuring\na higher degree of irregularity improved results can be achieved by applying\nnon-standard similarity functions. The SoRC scores computed with our approach\nindicate that an automated testing and scoring of different methods for mass\nchannel image grouping can improve the final outcome of a study by finally\nselecting the methods of the highest scores.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:24:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["W\u00fcllems", "Karsten", ""], ["Nattkemper", "Tim W.", ""]]}, {"id": "2009.14745", "submitter": "Jun Tang", "authors": "Jun Tang and Dale Zimmerman", "title": "Space-Time Covariance Models on Networks with An Application on Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-order, small-scale dependence structure of a stochastic process\ndefined in the space-time domain is key to prediction (or kriging). While great\nefforts have been dedicated to developing models for cases in which the spatial\ndomain is either a finite-dimensional Euclidean space or a unit sphere,\ncounterpart developments on a generalized linear network are practically\nnon-existent. To fill this gap, we develop a broad range of parametric,\nnon-separable space-time covariance models on generalized linear networks and\nthen an important subgroup -- Euclidean trees by the space embedding technique\n-- in concert with the generalized Gneiting class of models and 1-symmetric\ncharacteristic functions in the literature, and the scale mixture approach. We\ngive examples from each class of models and investigate the geometric features\nof these covariance functions near the origin and at infinity. We also show the\nlinkage between different classes of space-time covariance models on Euclidean\ntrees. We illustrate the use of models constructed by different methodologies\non a daily stream temperature data set and compare model predictive performance\nby cross validation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:32:42 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tang", "Jun", ""], ["Zimmerman", "Dale", ""]]}]