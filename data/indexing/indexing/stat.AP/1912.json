[{"id": "1912.00013", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Youssef Esstafa (LMB), Bruno\n  Saussereau (LMB)", "title": "Diagnostic checking in FARIMA models with uncorrelated but\n  non-independent error terms", "comments": "arXiv admin note: text overlap with arXiv:1902.03000,\n  arXiv:1910.07213", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of modified portmanteau tests for testing the\nadequacy of FARIMA models under the assumption that the errors are uncorrelated\nbut not necessarily independent (i.e. weak FARIMA). We first study the joint\ndistribution of the least squares estimator and the noise empirical\nautocovariances. We then derive the asymp-totic distribution of residual\nempirical autocovariances and autocorrelations. We deduce the asymptotic\ndistribution of the Ljung-Box (or Box-Pierce) modified portmanteau statistics\nfor weak FARIMA models. We also propose another method based on a\nself-normalization approach to test the adequacy of FARIMA models. Finally some\nsimulation studies are presented to corroborate our theoretical work. An\napplication to the Standard \\& Poor's 500 and Nikkei returns also illustrate\nthe practical relevance of our theoretical results. AMS 2000 subject\nclassifications: Primary 62M10, 62F03, 62F05; secondary 91B84, 62P05.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:51:14 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:40:06 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Esstafa", "Youssef", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1912.00053", "submitter": "Boyue Fang", "authors": "Boyue Fang and Yutong Feng", "title": "Drug dissemination strategy with an SEIR-based SUC model", "comments": "20pages, 10figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the features of drug addiction, this paper constructs an\nSEIR-based SUC model to describe and predict the spread of drug addiction.\nPredictions are that the number of drug addictions will continue to fluctuate\nwith reduced amplitude and eventually stabilize. To seek the fountainhead of\nheroin, we identified the most likely origins of drugs in Philadelphia, PA,\nCuyahoga and Hamilton, OH, Jefferson, KY, Kanawha, WV, and Bedford, VA. Based\non the facts, advised concentration includes the spread of Oxycodone,\nHydrocodone, Heroin, and Buprenorphine. In other words, drug transmission in\nthe two states of Ohio and Pennsylvania require awareness. According to the\npropagation curve predicted by our model, the transfer of KY state is still in\nits early stage, while that of VA, WV is in the middle point, and OH, PA in its\nlatter ones. As a result of this, the number of drug addictions in KY, OH, and\nVA is projected to increase in three years. For methodology, with the Principal\ncomponent analysis technique, 22 variables in socio-economic data related to\nthe continuous use of Opioid drugs was filtered, where the 'Relationship' Part\ndeserves a highlight.\n  Based on them, by using the K-means algorithm, 464 counties were categorized\ninto three baskets. To combat the opioid crisis, a specific action will discuss\nin the sensitivity analysis section. After modeling and analytics, innovation\nis required to control addicts and advocate anti-drug news campaigns. This part\nalso verified the effectiveness of model when $d_1<0.2; r_1,r_2,r_3<0.3;\n15<\\beta_1,\\beta_2,\\beta_3<25$. In other words, if such boundary exceeded, the\nnumber of drug addictions may rocket and peak in a short period.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:47:21 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 17:27:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Fang", "Boyue", ""], ["Feng", "Yutong", ""]]}, {"id": "1912.00111", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi, Sameer K. Deshpande, Edward I. George and Shane T.\n  Jensen", "title": "Crime in Philadelphia: Bayesian Clustering with Particle Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the change in crime over time is a critical first step\ntowards better understanding of public safety in large urban environments.\nBayesian hierarchical modeling is a natural way to study spatial variation in\nurban crime dynamics at the neighborhood level, since it facilitates principled\n\"sharing of information\" between spatially adjacent neighborhoods. Typically,\nhowever, cities contain many physical and social boundaries that may manifest\nas spatial discontinuities in crime patterns. In this situation, standard prior\nchoices often yield overly-smooth parameter estimates, which can ultimately\nproduce miscalibrated forecasts. To prevent potential over-smoothing, we\nintroduce a prior that partitions the set of neighborhoods into several\nclusters and encourages spatial smoothness within each cluster. In terms of\nmodel implementation, conventional stochastic search techniques are\ncomputationally prohibitive, as they must traverse a combinatorially vast space\nof partitions. We introduce an ensemble optimization procedure that\nsimultaneously identifies several high probability partitions by solving one\noptimization problem using a new local search strategy. We then use the\nidentified partitions to estimate crime trends in Philadelphia between 2006 and\n2017. On simulated and real data, our proposed method demonstrates good\nestimation and partition selection performance. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:45:18 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 18:37:16 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Deshpande", "Sameer K.", ""], ["George", "Edward I.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1912.00263", "submitter": "Tim Friede", "authors": "Regina Stegherr, Jan Beyersmann, Valentine Jehl, Kaspar Rufibach,\n  Friedhelm Leverkus, Claudia Schmoor and Tim Friede (on behalf of the SAVVY\n  project group)", "title": "Survival analysis for AdVerse events with VarYing follow-up times\n  (SAVVY): Rationale and statistical concept of a meta-analytic study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of safety is an important aspect of the evaluation of new\ntherapies in clinical trials, with analyses of adverse events being an\nessential part of this. Standard methods for the analysis of adverse events\nsuch as the incidence proportion, i.e. the number of patients with a specific\nadverse event out of all patients in the treatment groups, do not account for\nboth varying follow-up times and competing risks. Alternative approaches such\nas the Aalen-Johansen estimator of the cumulative incidence function have been\nsuggested. Theoretical arguments and numerical evaluations support the\napplication of these more advanced methodology, but as yet there is to our\nknowledge only insufficient empirical evidence whether these methods would lead\nto different conclusions in safety evaluations. The Survival analysis for\nAdVerse events with VarYing follow-up times (SAVVY) project strives to close\nthis gap in evidence by conducting a meta-analytical study to assess the impact\nof the methodology on the conclusion of the safety assessment empirically. Here\nwe present the rationale and statistical concept of the empirical study\nconducted as part of the SAVVY project. The statistical methods are presented\nin unified notation and examples of their implementation in R and SAS are\nprovided.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 21:06:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Stegherr", "Regina", "", "on behalf of the SAVVY\n  project group"], ["Beyersmann", "Jan", "", "on behalf of the SAVVY\n  project group"], ["Jehl", "Valentine", "", "on behalf of the SAVVY\n  project group"], ["Rufibach", "Kaspar", "", "on behalf of the SAVVY\n  project group"], ["Leverkus", "Friedhelm", "", "on behalf of the SAVVY\n  project group"], ["Schmoor", "Claudia", "", "on behalf of the SAVVY\n  project group"], ["Friede", "Tim", "", "on behalf of the SAVVY\n  project group"]]}, {"id": "1912.00295", "submitter": "Samiran Sinha", "authors": "Tong Wang, Kejun He, Wei Ma, Dipankar Bandyopadhyay and Samiran Sinha", "title": "Efficient Estimation of Mixture Cure Frailty Model for Clustered Current\n  Status Data", "comments": "Unstable EM algorithm due to limited information in current status\n  data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current status data abounds in the field of epidemiology and public health,\nwhere the only observable data for a subject is the random inspection time and\nthe event status at inspection. Motivated by such a current status data from a\nperiodontal study where data are inherently clustered, we propose a unified\nmethodology to analyze such complex data. We allow the time-to-event to follow\nthe semiparametric GOR model with a cure fraction, and develop a unified\nestimation scheme powered by the EM algorithm. The within-subject correlation\nis accounted for by a random (frailty) effect, and the non-parametric component\nof the GOR model is approximated via penalized splines, with a set of knot\npoints that increases with the sample size. Proposed methodology is accompanied\nby a rigorous asymptotic theory, and the related semiparametric efficiency. The\nfinite sample performance of our model parameters are assessed via simulation\nstudies. Furthermore, the proposed methodology is illustrated via application\nto the oral health data, accompanied by diagnostic checks to identify\ninfluential observations. An easy to use R package CRFCSD is also available for\nimplementation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:41:22 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 15:07:41 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 15:32:29 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Wang", "Tong", ""], ["He", "Kejun", ""], ["Ma", "Wei", ""], ["Bandyopadhyay", "Dipankar", ""], ["Sinha", "Samiran", ""]]}, {"id": "1912.00326", "submitter": "Cheoljoon Jeong", "authors": "Cheoljoon Jeong and Xiaolei Fang", "title": "Two-Dimensional Variable Selection and Its Applications in the\n  Diagnostics of Product Quality Defects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The root-cause diagnostics of product quality defects in multistage\nmanufacturing processes often requires a joint identification of crucial stages\nand process variables. To meet this requirement, this paper proposes a novel\npenalized matrix regression methodology for two-dimensional variable selection.\nThe method regresses a scalar response variable against a matrix-based\npredictor using a generalized linear model. The unknown regression coefficient\nmatrix is decomposed as a product of two factor matrices. The rows of the first\nfactor matrix and the columns of the second factor matrix are simultaneously\npenalized to inspire sparsity. To estimate the parameters, we develop a block\ncoordinate proximal descent (BCPD) optimization algorithm, which cyclically\nsolves two convex sub-optimization problems. We have proved that the BCPD\nalgorithm always converges to a critical point with any initialization. In\naddition, we have also proved that each of the sub-optimization problems has a\nclosed-form solution if the response variable follows a distribution whose\n(negative) log-likelihood function has a Lipschitz continuous gradient. A\nsimulation study and a dataset from a real-world application are used to\nvalidate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 05:51:38 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:50:18 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 18:05:50 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 07:34:24 GMT"}, {"version": "v5", "created": "Tue, 9 Jun 2020 19:25:35 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Jeong", "Cheoljoon", ""], ["Fang", "Xiaolei", ""]]}, {"id": "1912.00327", "submitter": "Cheoljoon Jeong", "authors": "Cheoljoon Jeong", "title": "The Effect of Real Estate Auction Events on Mortality Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study has investigated the mortality rate of parties at real estate\nauctions compared to that of the overall population in South Korea by using\nvarious variables, including age, real estate usage, cumulative number of real\nestate auction events, disposal of real estate, and appraisal price. In each\ncase, there has been a significant difference between mortality rate of parties\nat real estate auctions and that of the overall population, which provides a\nnew insight regarding utilization of the information on real estate auctions.\nDespite the need for further detailed analysis on the correlation between real\nestate auction events and death, because the result from this study is still\nmeaningful, the result is summarized for informational purposes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 05:54:45 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Jeong", "Cheoljoon", ""]]}, {"id": "1912.00360", "submitter": "Meng Xu", "authors": "Meng Xu and Philip T. Reiss", "title": "Distribution-Free Pointwise Adjusted P-Values for Functional Hypotheses", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-47756-1_32", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical tests assess whether a function of interest departs from an\nenvelope of functions generated under a simulated null distribution. This\napproach originated in spatial statistics, but has recently gained some\npopularity in functional data analysis. Whereas such envelope tests examine\ndeviation from a functional null distribution in an omnibus sense, in some\napplications we wish to do more: to obtain p-values at each point in the\nfunction domain, adjusted to control the familywise error rate. Here we derive\npointwise adjusted p-values based on envelope tests, and relate these to\nprevious approaches for functional data under distributional assumptions. We\nthen present two alternative distribution-free p-value adjustments that offer\ngreater power. The methods are illustrated with an analysis of age-varying sex\neffects on cortical thickness in the human brain.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 08:58:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Xu", "Meng", ""], ["Reiss", "Philip T.", ""]]}, {"id": "1912.00434", "submitter": "Iain Carmichael", "authors": "Iain Carmichael, Benjamin C. Calhoun, Katherine A. Hoadley, Melissa A.\n  Troester, Joseph Geradts, Heather D. Couture, Linnea Olsson, Charles M.\n  Perou, Marc Niethammer, Jan Hannig, J.S. Marron", "title": "Joint and individual analysis of breast cancer histologic images and\n  genomic covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in modern data analysis is understanding connections between\ncomplex and differing modalities of data. For example, two of the main\napproaches to the study of breast cancer are histopathology (analyzing visual\ncharacteristics of tumors) and genetics. While histopathology is the gold\nstandard for diagnostics and there have been many recent breakthroughs in\ngenetics, there is little overlap between these two fields. We aim to bridge\nthis gap by developing methods based on Angle-based Joint and Individual\nVariation Explained (AJIVE) to directly explore similarities and differences\nbetween these two modalities. Our approach exploits Convolutional Neural\nNetworks (CNNs) as a powerful, automatic method for image feature extraction to\naddress some of the challenges presented by statistical analysis of\nhistopathology image data. CNNs raise issues of interpretability that we\naddress by developing novel methods to explore visual modes of variation\ncaptured by statistical algorithms (e.g. PCA or AJIVE) applied to CNN features.\nOur results provide many interpretable connections and contrasts between\nhistopathology and genetics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 16:01:33 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 02:25:26 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 00:41:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Carmichael", "Iain", ""], ["Calhoun", "Benjamin C.", ""], ["Hoadley", "Katherine A.", ""], ["Troester", "Melissa A.", ""], ["Geradts", "Joseph", ""], ["Couture", "Heather D.", ""], ["Olsson", "Linnea", ""], ["Perou", "Charles M.", ""], ["Niethammer", "Marc", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "1912.00540", "submitter": "Alan Pearse", "authors": "Alan R. Pearse, James M. McGree, Nicholas A. Som, Catherine Leigh, Jay\n  M. Ver Hoef, Paul Maxwell and Erin E. Peterson", "title": "SSNdesign -- an R package for pseudo-Bayesian optimal and adaptive\n  sampling designs on stream networks", "comments": "Main document: 18 pages, 7 figures Supp Info A: 11 pages, 0 figures\n  Supp Info B: 24 pages, 6 figures Supp Info C: 3 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streams and rivers are biodiverse and provide valuable ecosystem services.\nMaintaining these ecosystems is an important task, so organisations often\nmonitor the status and trends in stream condition and biodiversity using field\nsampling and, more recently, autonomous in-situ sensors. However, data\ncollection is often costly and so effective and efficient survey designs are\ncrucial to maximise information while minimising costs. Geostatistics and\noptimal and adaptive design theory can be used to optimise the placement of\nsampling sites in freshwater studies and aquatic monitoring programs.\nGeostatistical modelling and experimental design on stream networks pose\nstatistical challenges due to the branching structure of the network, flow\nconnectivity and directionality, and differences in flow volume. Thus, unique\nchallenges of geostatistics and experimental design on stream networks\nnecessitates the development of new open-source software for implementing the\ntheory. We present SSNdesign, an R package for solving optimal and adaptive\ndesign problems on stream networks that integrates with existing open-source\nsoftware. We demonstrate the mathematical foundations of our approach, and\nillustrate the functionality of SSNdesign using two case studies involving real\ndata from Queensland, Australia. In both case studies we demonstrate that the\noptimal or adaptive designs outperform random and spatially balanced survey\ndesigns. The SSNdesign package has the potential to boost the efficiency of\nfreshwater monitoring efforts and provide much-needed information for\nfreshwater conservation and management.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:48:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pearse", "Alan R.", ""], ["McGree", "James M.", ""], ["Som", "Nicholas A.", ""], ["Leigh", "Catherine", ""], ["Hoef", "Jay M. Ver", ""], ["Maxwell", "Paul", ""], ["Peterson", "Erin E.", ""]]}, {"id": "1912.00694", "submitter": "Rapha\\\"el Huser", "authors": "Rapha\\\"el Huser", "title": "Editorial: EVA 2019 data competition on spatio-temporal prediction of\n  Red Sea surface temperature extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, non-stationary spatio-temporal data are ubiquitous in modern\nstatistical applications, and the modeling of spatio-temporal extremes is\ncrucial for assessing risks in environmental sciences among others. While the\nmodeling of extremes is challenging in itself, the prediction of rare events at\nunobserved spatial locations and time points is even more difficult. In this\neditorial, we describe the data competition that was organized for the 11th\ninternational conference on Extreme-Value Analysis (EVA 2019), for which\nseveral teams modeled and predicted Red Sea surface temperature extremes over\nspace and time. After introducing the dataset and the goal of the competition,\nwe disclose the final ranking of the teams, and we finally discuss some\ninteresting outcomes and future challenges.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:43:36 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Huser", "Rapha\u00ebl", ""]]}, {"id": "1912.00913", "submitter": "Daniel Miller", "authors": "Craig Boucher, Ulf Knoblich, Daniel Miller, Sasha Patotski, Amin\n  Saied, Venky Venkateshaiah", "title": "Automated metrics calculation in a dynamic heterogeneous environment", "comments": "5 pages, MIT Code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A consistent theme in software experimentation at Microsoft has been solving\nproblems of experimentation at scale for a diverse set of products. Running\nexperiments at scale (i.e., many experiments on many users) has become state of\nthe art across the industry. However, providing a single platform that allows\nsoftware experimentation in a highly heterogenous and constantly evolving\necosystem remains a challenge. In our case, heterogeneity spans multiple\ndimensions. First, we need to support experimentation for many types of\nproducts: websites, search engines, mobile apps, operating systems, cloud\nservices and others. Second, due to the diversity of the products and teams\nusing our platform, it needs to be flexible enough to analyze data in multiple\ncompute fabrics (e.g. Spark, Azure Data Explorer), with a way to easily add\nsupport for new fabrics if needed. Third, one of the main factors in\nfacilitating growth of experimentation culture in an organization is to\ndemocratize metric definition and analysis processes. To achieve that, our\nsystem needs to be simple enough to be used not only by data scientists, but\nalso engineers, product managers and sales teams. Finally, different personas\nmight need to use the platform for different types of analyses, e.g. dashboards\nor experiment analysis, and the platform should be flexible enough to\naccommodate that. This paper presents our solution to the problems of\nheterogeneity listed above.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:40:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Boucher", "Craig", ""], ["Knoblich", "Ulf", ""], ["Miller", "Daniel", ""], ["Patotski", "Sasha", ""], ["Saied", "Amin", ""], ["Venkateshaiah", "Venky", ""]]}, {"id": "1912.00984", "submitter": "Andrew Elliott", "authors": "Andrew Elliott, Angus Chiu, Marya Bazzi, Gesine Reinert and Mihai\n  Cucuringu", "title": "Core-Periphery Structure in Directed Networks", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2019.0783", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While studies of meso-scale structures in networks often focus on community\nstructure, core--periphery structures can reveal new insights. This structure\ntypically consists of a well-connected core and a periphery that is well\nconnected to the core but sparsely connected internally. Most studies of\ncore--periphery structure focus on undirected networks.\n  We propose a generalisation of core-periphery structure to directed networks.\nOur approach yields a family of core-periphery block model formulations in\nwhich core and periphery sets are edge-direction dependent. We mainly focus on\na particular core--periphery structure consisting of two core sets and two\nperiphery sets which we motivate empirically.\n  To detect this directed core-periphery structure we propose four different\nmethods, with different trade-offs between computational complexity and\naccuracy. We assess these methods on three benchmarks and compare to four\nstandard methods. On simulated data, the proposed methods match or outperform\nthe standard methods. Applying our methods to three empirical networks -- a\npolitical blogs networks, a faculty hiring network, and a trade network --\nillustrates that this directed core--periphery structure can offer novel\ninsights about the underlying dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:23:19 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Elliott", "Andrew", ""], ["Chiu", "Angus", ""], ["Bazzi", "Marya", ""], ["Reinert", "Gesine", ""], ["Cucuringu", "Mihai", ""]]}, {"id": "1912.01233", "submitter": "Rapha\\\"el Huser", "authors": "Luigi Lombardo, Thomas Opitz, Francesca Ardizzone, Fausto Guzzetti and\n  Rapha\\\"el Huser", "title": "Space-Time Landslide Predictive Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landslides are nearly ubiquitous phenomena and pose severe threats to people,\nproperties, and the environment. Investigators have for long attempted to\nestimate landslide hazard to determine where, when, and how destructive\nlandslides are expected to be in an area. This information is useful to design\nlandslide mitigation strategies, and to reduce landslide risk and societal and\neconomic losses. In the geomorphology literature, most attempts at predicting\nthe occurrence of populations of landslides rely on the observation that\nlandslides are the result of multiple interacting, conditioning and triggering\nfactors. Here, we propose a novel Bayesian modelling framework for the\nprediction of space-time landslide occurrences of the slide type caused by\nweather triggers. We consider log-Gaussian cox processes, assuming that\nindividual landslides stem from a point process described by an unknown\nintensity function. We tested our prediction framework in the Collazzone area,\nUmbria, Central Italy, for which a detailed multi-temporal landslide inventory\nspanning 1941-2014 is available together with lithological and bedding data. We\ntested five models of increasing complexity. Our most complex model includes\nfixed effects and latent spatio-temporal effects, thus largely fulfilling the\ncommon definition of landslide hazard in the literature. We quantified the\nspatio-temporal predictive skill of our model and found that it performed\nbetter than simpler alternatives. We then developed a novel classification\nstrategy and prepared an intensity-susceptibility landslide map, providing more\ninformation than traditional susceptibility zonations for land planning and\nmanagement. We expect our novel approach to lead to better projections of\nfuture landslides, and to improve our collective understanding of the evolution\nof landscapes dominated by mass-wasting processes under geophysical and weather\ntriggers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:01:41 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lombardo", "Luigi", ""], ["Opitz", "Thomas", ""], ["Ardizzone", "Francesca", ""], ["Guzzetti", "Fausto", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1912.01266", "submitter": "Simon Meyer Lauritsen", "authors": "Simon Meyer Lauritsen, Mads Kristensen, Mathias Vassard Olsen, Morten\n  Skaarup Larsen, Katrine Meyer Lauritsen, Marianne Johansson J{\\o}rgensen,\n  Jeppe Lange, Bo Thiesson", "title": "Explainable artificial intelligence model to predict acute critical\n  illness from electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an explainable artificial intelligence (AI) early warning score\n(xAI-EWS) system for early detection of acute critical illness. While\nmaintaining a high predictive performance, our system explains to the clinician\non which relevant electronic health records (EHRs) data the prediction is\ngrounded. Acute critical illness is often preceded by deterioration of\nroutinely measured clinical parameters, e.g., blood pressure and heart rate.\nEarly clinical prediction is typically based on manually calculated screening\nmetrics that simply weigh these parameters, such as Early Warning Scores (EWS).\nThe predictive performance of EWSs yields a tradeoff between sensitivity and\nspecificity that can lead to negative outcomes for the patient. Previous work\non EHR-trained AI systems offers promising results with high levels of\npredictive performance in relation to the early, real-time prediction of acute\ncritical illness. However, without insight into the complex decisions by such\nsystem, clinical translation is hindered. In this letter, we present our\nxAI-EWS system, which potentiates clinical translation by accompanying a\nprediction with information on the EHR data explaining it.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:52:20 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lauritsen", "Simon Meyer", ""], ["Kristensen", "Mads", ""], ["Olsen", "Mathias Vassard", ""], ["Larsen", "Morten Skaarup", ""], ["Lauritsen", "Katrine Meyer", ""], ["J\u00f8rgensen", "Marianne Johansson", ""], ["Lange", "Jeppe", ""], ["Thiesson", "Bo", ""]]}, {"id": "1912.01422", "submitter": "Anthony Constantinou", "authors": "Norman Fenton, Martin Neil and Anthony Constantinou", "title": "Simpson's Paradox and the implications for medical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Simpson's paradox, and explains its serious implications\nfor randomised control trials. In particular, we show that for any number of\nvariables we can simulate the result of a controlled trial which uniformly\npoints to one conclusion (such as 'drug is effective') for every possible\ncombination of the variable states, but when a previously unobserved\nconfounding variable is included every possible combination of the variables\nstate points to the opposite conclusion ('drug is not effective'). In other\nwords no matter how many variables are considered, and no matter how\n'conclusive' the result, one cannot conclude the result is truly 'valid' since\nthere is theoretically an unobserved confounding variable that could completely\nreverse the result.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:47:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fenton", "Norman", ""], ["Neil", "Martin", ""], ["Constantinou", "Anthony", ""]]}, {"id": "1912.01443", "submitter": "Aleksey Buzmakov", "authors": "Aleksey Buzmakov, Daria Semenova, Maria Temirkaeva", "title": "The Comparison of Methods for Individual Treatment Effect Detection", "comments": "12 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, treatment effect estimation at the individual level is a vital problem\nin many areas of science and business. For example, in marketing, estimates of\nthe treatment effect are used to select the most efficient promo-mechanics; in\nmedicine, individual treatment effects are used to determine the optimal dose\nof medication for each patient and so on. At the same time, the question on\nchoosing the best method, i.e., the method that ensures the smallest predictive\nerror (for instance, RMSE) or the highest total (average) value of the effect,\nremains open. Accordingly, in this paper we compare the effectiveness of\nmachine learning methods for estimation of individual treatment effects. The\ncomparison is performed on the Criteo Uplift Modeling Dataset. In this paper we\nshow that the combination of the Logistic Regression method and the Difference\nScore method as well as Uplift Random Forest method provide the best\ncorrectness of Individual Treatment Effect prediction on the top 30\\%\nobservations of the test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 15:05:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Semenova", "Daria", ""], ["Temirkaeva", "Maria", ""]]}, {"id": "1912.01505", "submitter": "Shu-Chuan Chen", "authors": "Shu-Chuan Chen, Lung-An Li, and Jiping He", "title": "An integrated heterogeneous Poisson model for neuron functions in hand\n  movement during reaching and grasp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand potential encoding mechanism of motor cortical neurons for\ncontrol commands during reach-to-grasp movements, experiments to record\nneuronal activities from primary motor cortical regions have been conducted in\nmany research laboratories (for example, (7), (17)). The most popular approach\nin neuroscience community is to fit the Analysis of Variance (ANOVA) model\nusing the firing rates of individual neurons. In addition to consider neural\nfiring counts but also temporal intervals, (5) proposed to apply Analysis of\nCovariance (ANCOVA) model. Due to the nature of the data, in this paper we\npropose to apply an integrated method, called heterogeneous Poisson regression\nmodel, to categorize different neural activities. Three scenarios are discussed\nto show that the proposed heterogeneous Poisson regression model can overcome\nsome disadvantages of the traditional Poisson regression model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:30:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chen", "Shu-Chuan", ""], ["Li", "Lung-An", ""], ["He", "Jiping", ""]]}, {"id": "1912.01517", "submitter": "Denis Talbot", "authors": "Diop S. Arona, Duchesne Thierry, Cumming Steven, Diop Awa, Talbot\n  Denis", "title": "Confounding Adjustment Methods for Multi-level Treatment Comparisons\n  Under Lack of Positivity and Unknown Model Specification", "comments": "15 pages, 6 tables", "journal-ref": "Journal of Applied Statistics by Taylor & Francis, 2021", "doi": "10.1080/02664763.2021.1911966", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imbalances in covariates between treatment groups are frequent in\nobservational studies and can lead to biased comparisons. Various adjustment\nmethods can be employed to correct these biases in the context of multi-level\ntreatments ($>$ 2). However, analytical challenges, such as positivity\nviolations and incorrect model specification, may affect their ability to yield\nunbiased estimates. Adjustment methods that present the best potential to deal\nwith those challenges were identified: the overlap weights, augmented overlap\nweights, bias-corrected matching and targeted maximum likelihood. A simple\nvariance estimator for the overlap weight estimators that can naturally be\ncombined with machine learning algorithms is proposed. In a simulation study,\nwe investigated the empirical performance of these methods as well as those of\nsimpler alternatives, standardization, inverse probability weighting and\nmatching. Our proposed variance estimator performed well, even at a sample size\nof 500. Adjustment methods that included an outcome modeling component\nperformed better than those that only modeled the treatment mechanism.\nAdditionally, a machine learning implementation was observed to efficiently\ncompensate for the unknown model specification for the former methods, but not\nthe latter. Based on these results, the wildfire data were analyzed using the\naugmented overlap weight estimator. With respect to effectiveness of alternate\nfire-suppression interventions, the results were counter-intuitive, indeed the\nopposite of what would be expected on subject-matter grounds. This suggests the\npresence in the data of unmeasured confounding bias.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:43:12 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:25:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Arona", "Diop S.", ""], ["Thierry", "Duchesne", ""], ["Steven", "Cumming", ""], ["Awa", "Diop", ""], ["Denis", "Talbot", ""]]}, {"id": "1912.01543", "submitter": "Inder Tecuapetla-G\\'omez", "authors": "Inder Tecuapetla-G\\'omez and Gabriela Villamil-Cortez and Mar\\'ia\n  Isabel Cruz-L\\'opez", "title": "On the potential of BFAST for monitoring burned areas using\n  multi-temporal Landsat-7 images", "comments": "10 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-automatic approach to map burned areas and\nassess burn severity that does not require prior knowledge of the fire date.\nFirst, we apply BFAST to NDVI time series and estimate statistically abrupt\nchanges in NDVI trends. These estimated changes are then used as plausible fire\ndates to calculate dNBR following a typical pre-post fire assessment. In\naddition to its statistical guarantees, this method depends only on a tuning\nparameter (the bandwidth of the test statistic for changes). This method was\napplied to Landsat-7 images taken over La Primavera Flora and Fauna Protection\nArea, in Jalisco, Mexico, from 2003 to 2016. We evaluated BFAST's ability to\nestimate vegetation changes based on time series with significant observation\ngaps. We discussed burn severity maps associated with massive wildfires (2005\nand 2012) and another with smaller dimensions (2008) that might have been\nexcluded from official records. We validated our 2012 burned area map against a\nhigh resolution burned area map obtained from RapidEye images; in zones with\nmoderate data quality, the overall accuracy of our map is 92%.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:42:47 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 15:51:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Tecuapetla-G\u00f3mez", "Inder", ""], ["Villamil-Cortez", "Gabriela", ""], ["Cruz-L\u00f3pez", "Mar\u00eda Isabel", ""]]}, {"id": "1912.01574", "submitter": "Samuel Henry", "authors": "Samuel Henry", "title": "Improving upon NBA point-differential rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some time, point-differential has been thought to be a better predictor\nfor future NBA success than pure win-loss record. Most ranking and team\nperformance predictions rely largely on point-differential, often with some\nnormalizations built-in. In this work, various capping and weighting functions\nare proposed to further improve indicator performance. A gradient descent\nalgorithm is also employed to discover the optimized weighting/capping function\napplied to individual game scores throughout the season.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:15:52 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Henry", "Samuel", ""]]}, {"id": "1912.01590", "submitter": "Timothy Wolock", "authors": "Timothy M Wolock, Seth R Flaxman, Jeffrey W Eaton", "title": "Inferring HIV incidence trends and transmission dynamics with a\n  spatio-temporal HIV epidemic model", "comments": "28 pages, 9 figures, submitted to Epidemics 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reliable estimation of spatio-temporal trends in population-level HIV\nincidence is becoming an increasingly critical component of HIV prevention\npolicy-making. However, direct measurement is nearly impossible. Current,\nwidely used models infer incidence from survey and surveillance seroprevalence\ndata, but they require unrealistic assumptions about spatial independence\nacross spatial units. In this study, we present an epidemic model of HIV that\nexplicitly simulates the spatial dynamics of HIV over many small, interacting\nareal units. By integrating all available population-level data, we are able to\ninfer not only spatio-temporally varying incidence, but also ART initiation\nrates and patient counts. Our study illustrates the feasibility of applying\ncompartmental models to larger inferential problems than those to which they\nare typically applied, as well as the value of data fusion approaches to\ninfectious disease modeling.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:40:01 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wolock", "Timothy M", ""], ["Flaxman", "Seth R", ""], ["Eaton", "Jeffrey W", ""]]}, {"id": "1912.01835", "submitter": "Feilong Wang", "authors": "Feilong Wang, Jingxing Wang, Jinzhou Cao, Cynthia Chen, Xuegang (Jeff)\n  Ban", "title": "Extracting Trips from Multi-Sourced Data for Mobility Pattern Analysis:\n  An App-Based Data Example", "comments": "The manuscript contains some issues and further works are needed. The\n  authors have not reached a consensus about when it will be ready for the\n  public and decide to withdraw it", "journal-ref": null, "doi": "10.1016/j.trc.2019.05.028", "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passively-generated data, such as GPS data and cellular data, bring\ntremendous opportunities for human mobility analysis and transportation\napplications. Since their primary purposes are often non-transportation\nrelated, the passively-generated data need to be processed to extract trips.\nMost existing trip extraction methods rely on data that are generated via a\nsingle positioning technology such as GPS or triangulation through cellular\ntowers (thereby called single-sourced data), and methods to extract trips from\ndata generated via multiple positioning technologies (or, multi-sourced data)\nare absent. And yet, multi-sourced data are now increasingly common. Generated\nusing multiple technologies (e.g., GPS, cellular network- and WiFi-based),\nmulti-sourced data contain high variances in their temporal and spatial\nproperties. In this study, we propose a 'Divide, Conquer and Integrate' (DCI)\nframework to extract trips from multi-sourced data. We evaluate the proposed\nframework by applying it to an app-based data, which is multi-sourced and has\nhigh variances in both location accuracy and observation interval (i.e. time\ninterval between two consecutive observations). On a manually labeled sample of\nthe app-based data, the framework outperforms the state-of-the-art SVM model\nthat is designed for GPS data. The effectiveness of the framework is also\nillustrated by consistent mobility patterns obtained from the app-based data\nand an externally collected household travel survey data for the same region\nand the same period.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:46:41 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 21:24:02 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wang", "Feilong", "", "Jeff"], ["Wang", "Jingxing", "", "Jeff"], ["Cao", "Jinzhou", "", "Jeff"], ["Chen", "Cynthia", "", "Jeff"], ["Xuegang", "", "", "Jeff"], ["Ban", "", ""]]}, {"id": "1912.02076", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "UEFA against the champions? An evaluation of the recent reform of the\n  Champions League qualification", "comments": "24 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UEFA Champions League is the major European club football competition\norganised by the Union of European Football Associations (UEFA). It contains 32\nteams, hence the national champions of most UEFA associations have to play in\nthe qualification to receive a slot in its group stage. The paper evaluates the\nimpact of reforming the Champions Path of the qualifying system, effective from\nthe 2018/19 season. While it is anticipated that the reduction in the number of\nberths decreases the probability of advancing to the group stage, the\ndistribution of the losses among the national associations can only be\nestimated via Monte-Carlo simulations. In contrast to previous studies, our\nmethodology considers five seasons instead of one to filter out any possible\nseason-specific attributes. Almost all of the 45 countries are found to gain\nless prize money on average. Several champions, including the Cypriot, the\nSwiss, and the Scottish, might face a loss of over one million Euros. Since the\nnegative effects depend to a large extent on the somewhat arbitrary differences\nbetween the positions of the access list, we propose to introduce some\nrandomness into the determination of entry stages.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:07:05 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 07:42:50 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 09:26:41 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2021 12:13:47 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 07:06:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1912.02232", "submitter": "M. Leticia Rubio Puzzo PhD", "authors": "Juan Cruz Moreno, M. Leticia Rubio Puzzo and Wolfgang Paul", "title": "Collective dynamics of pedestrians in a non-panic evacuation scenario", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.bio-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of pedestrian motion along a corridor in a non-panic\nregime (e.g., schools, hospitals or airports). Such situations have been\ndiscussed so far within the Social Force Model (SFM). We suggest to enrich this\nmodel by interactions based on the velocity of the particles and some\nrandomness, both of which we introduce using the ideas of the Vicsek Model\n(VM). This new model allows to introduce fluctuations for a given average speed\nand geometry, and considering that the alignment interactions are modulated by\nan external control parameter (the noise $\\eta$) allows to introduce phase\ntransitions between ordered and disordered states. We have compared simulations\nof pedestrian motion along a corridor using (a) the VM with two boundary\nconditions (periodic and bouncing back) and with or without desired direction\nof motion, (b) the SFM, and (c) the new model SFM+VM. The study of steady-state\nconfigurations in the VM with confined geometry shows the expected bands\nperpendicular to the motion direction, while in the SFM and SFM+VM particles\norder in stripes of a given width $w$ along the direction of motion. The\nresults in the SFM+VM case show that $w(t)\\simeq t^\\alpha$ has a diffusive-like\nbehavior at low noise $\\eta$ (dynamic exponent $\\alpha \\approx 1/2$), while it\nis sub-diffusive at high values of external noise ($\\alpha \\approx 1/4$). We\nobserve the order-disorder transition in the VM with both boundary conditions,\nbut the application of a desired direction condition inhibits the existence of\ndisorder as expected. For the SFM+VM case we find a susceptibility maximum\nwhich increases with system size as a function of noise strength indicative of\na order-disorder transition in the whole range of densities and speeds studied.\nFrom our results we conclude that the new SFM+VM model is a well-suited model\nto describe non-panic evacuation with diverse degrees of disorder.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 20:00:16 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Moreno", "Juan Cruz", ""], ["Puzzo", "M. Leticia Rubio", ""], ["Paul", "Wolfgang", ""]]}, {"id": "1912.02359", "submitter": "Yazhuo Deng", "authors": "Yazhuo Deng, David R. Paul, Audrey Q. Fu", "title": "The Autoregressive Structural Model for analyzing longitudinal health\n  data of an aging population in China", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to elucidate the impact of social activity, physical activity and\nfunctional health status (factors) on depressive symptoms (outcome) in the\nChina Health and Retirement Longitudinal Study (CHARLS), a multi-year study of\naging involving 20,000 participants 45 years of age and older. Although a\nvariety of statistical methods are available for analyzing longitudinal data,\nmodeling the dynamics within a complex system remains a difficult\nmethodological challenge. We develop an Autoregressive Structural Model (ASM)\nto examine these factors on depressive symptoms while accounting for temporal\ndependence. The ASM builds on the structural equation model and also consists\nof two components: a measurement model that connects observations to latent\nfactors, and a structural model that delineates the mechanism among latent\nfactors. Our ASM further incorporates autoregressive dependence into both\ncomponents for repeated measurements. The results from applying the ASM to the\nCHARLS data indicate that social and physical activity independently and\nconsistently mitigated depressive symptoms over the course of five years, by\nmediating through functional health status.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:12:57 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Deng", "Yazhuo", ""], ["Paul", "David R.", ""], ["Fu", "Audrey Q.", ""]]}, {"id": "1912.02389", "submitter": "Lukasz Wiklendt", "authors": "Lukasz Wiklendt, Marcello Costa, Simon Brookes, Phil G. Dinning", "title": "Bayesian Functional Mixed-Effects Model with Gaussian Process Responses\n  for Wavelet Spectra of Spatiotemporal Colonic Manometry Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We present a technique for identification and statistical analysis\nof quasiperiodic spatiotemporal pressure signals recorded from multiple closely\nspaced sensors in the human colon. Methods: Identification is achieved by\ncomputing the continuous wavelet transform and cross-wavelet transform of these\nrecorded signals. Statistical analysis is achieved by modelling the resulting\ntime-averaged amplitudes or coherences in the frequency and frequency-phase\ndomains as Gaussian processes over a regular grid, under the influence of\ncategorical and numerical predictors that are specified by the experimental\ndesign as a mixed-effects model. Parameters of the model are inferred with\nHamiltonian Monte Carlo. Results and Conclusion: We present an application of\nthis method to colonic manometry data in healthy controls, to determine\nstatistical differences in the spectra of pressure signals between different\ncolonic regions and in response to a meal intervention. We are able to\nsuccessfully identify and contrast features in the spectra of pressure signals\nbetween various predictors. Significance: This novel method provides fast\nanalysis of manometric signals at levels of detail orders of magnitude beyond\nwhat was previously available. The proposed tractable mixed-effects model is\nbroadly applicable to experimental designs with functional responses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:10:51 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Wiklendt", "Lukasz", ""], ["Costa", "Marcello", ""], ["Brookes", "Simon", ""], ["Dinning", "Phil G.", ""]]}, {"id": "1912.02423", "submitter": "Kevin Kuo", "authors": "Kevin Kuo", "title": "Generative Synthesis of Insurance Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the impediments in advancing actuarial research and developing open\nsource assets for insurance analytics is the lack of realistic publicly\navailable datasets. In this work, we develop a workflow for synthesizing\ninsurance datasets leveraging CTGAN, a recently proposed neural network\narchitecture for generating tabular data. Applying the proposed workflow to\npublicly available data in the domains of general insurance pricing and life\ninsurance shock lapse modeling, we evaluate the synthesized datasets from a few\nperspectives: machine learning efficacy, distributions of variables, and\nstability of model parameters. This workflow is implemented via an R interface\nto promote adoption by researchers and data owners.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:49:31 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 03:46:00 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kuo", "Kevin", ""]]}, {"id": "1912.02655", "submitter": "Mehak Gupta", "authors": "Mehak Gupta, Thao-Ly T. Phan, Timothy Bunnell, Rahmatollah Beheshti", "title": "Obesity Prediction with EHR Data: A deep learning approach with\n  interpretable elements", "comments": "19 pages, 4 Tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Childhood obesity is a major public health challenge. Obesity in early\nchildhood and adolescence can lead to obesity and other health problems in\nadulthood. Early prediction and identification of the children at a high risk\nof developing childhood obesity may help in engaging earlier and more effective\ninterventions to prevent and manage this and other related health conditions.\nExisting predictive tools designed for childhood obesity primarily rely on\ntraditional regression-type methods without exploiting longitudinal patterns of\nchildren's data (ignoring data temporality). In this paper, we present a\nmachine learning model specifically designed for predicting future obesity\npatterns from generally available items on children's medical history. To do\nthis, we have used a large unaugmented EHR (Electronic Health Record) dataset\nfrom a major pediatric health system in the US. We adopt a general LSTM (long\nshort-term memory) network architecture for our model for training over dynamic\n(sequential) and static (demographic) EHR data. We have additionally included a\nset embedding and attention layers to compute the feature ranking of each\ntimestamp and attention scores of each hidden layer corresponding to each input\ntimestamp. These feature ranking and attention scores added interpretability at\nboth the features and the timestamp-level.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:41:27 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 23:26:34 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 16:10:04 GMT"}, {"version": "v4", "created": "Thu, 30 Jan 2020 19:21:07 GMT"}, {"version": "v5", "created": "Fri, 29 May 2020 21:58:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gupta", "Mehak", ""], ["Phan", "Thao-Ly T.", ""], ["Bunnell", "Timothy", ""], ["Beheshti", "Rahmatollah", ""]]}, {"id": "1912.02745", "submitter": "Fabrizio De Vico Fallani", "authors": "Tiziana Cattai, Stefania Colonnese, Marie-Constance Corsi, Danielle S.\n  Bassett, Gaetano Scarano, Fabrizio De Vico Fallani", "title": "Phase/amplitude synchronization of brain signals during motor imagery\n  BCI tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of brain functioning features is a crucial step in the\ndefinition of brain-computer interfaces (BCIs). In the last decade, functional\nconnectivity (FC) estimators have been increasingly explored based on their\nability to capture synchronization between multivariate brain signals. However,\nthe underlying neurophysiological mechanisms and the extent to which they can\nimprove performance in BCI-related tasks, is still poorly understood. To\naddress this gap in knowledge, we considered a group of 20 healthy subjects\nduring an EEG-based hand motor imagery (MI) task. We studied two\nwell-established FC estimators, i.e. spectral- and imaginary-coherence, and\ninvestigated how they were modulated by the MI task. We characterized the\nresulting FC networks by extracting the strength of connectivity of each EEG\nsensor and compared the discriminant power with respect to standard power\nspectrum features. At the group level, results showed that while\nspectral-coherence based network features were increasing the controlateral\nmotor area, those based on imaginary-coherence were decreasing. We demonstrated\nthat this opposite, but complementary, behavior was respectively determined by\nthe increase in amplitude and phase synchronization between the brain signals.\nAt the individual level, we proved that including these network connectivity\nfeatures in the classification of MI mental states led to an overall\nimprovement in accuracy. Taken together, our results provide fresh insights\ninto the oscillatory mechanisms subserving brain network changes during MI and\noffer new perspectives to improve BCI performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:33:12 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Cattai", "Tiziana", ""], ["Colonnese", "Stefania", ""], ["Corsi", "Marie-Constance", ""], ["Bassett", "Danielle S.", ""], ["Scarano", "Gaetano", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "1912.02867", "submitter": "Dominika Mik\\v{s}ov\\'a", "authors": "Dominika Mik\\v{s}ov\\'a, Christopher Rieser, Peter Filzmoser", "title": "Identification of mineralization in geochemistry along a transect based\n  on the spatial curvature of log-ratios", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting subcropping mineralizations but also deeply buried mineralizations\nis one important goal in geochemical exploration. The identification of useful\nindicators for mineralization is a difficult task as mineralization might be\ninfluenced by many factors, such as location, investigated media, depth, etc.\nWe propose a statistical method which indicates chemical elements related to\nmineralization along a transect. Moreover, the method determines along a\ntransect the potential area of the deposit. The identification is based on\nGeneral Additive Models (GAMs) for the element concentrations across the\nspatial coordinate(s). The log-ratios of the GAM fits are taken to compute the\ncurvature, where high and narrow curvature is supposed to indicate the\nmineralization area. By defining a measure for the quantification of high\ncurvature, the log-ratios can be ranked, and elements can be identified that\nare indicative of the anomaly patterns.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:41:12 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mik\u0161ov\u00e1", "Dominika", ""], ["Rieser", "Christopher", ""], ["Filzmoser", "Peter", ""]]}, {"id": "1912.02966", "submitter": "Omid Sedehi", "authors": "Omid Sedehi, Costas Papadimitriou, Lambros S. Katafygiotis", "title": "Data-Driven Uncertainty Quantification and Propagation in Structural\n  Dynamics through a Hierarchical Bayesian Framework", "comments": null, "journal-ref": "Probabilistic Engineering Mechanics, Volume 60, April 2020, 103047", "doi": "10.1016/j.probengmech.2020.103047", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of modeling errors, the mainstream Bayesian methods seldom\ngive a realistic account of uncertainties as they commonly underestimate the\ninherent variability of parameters. This problem is not due to any\nmisconception in the Bayesian framework since it is absolutely robust with\nrespect to the modeling assumptions and the observed data. Rather, this issue\nhas deep roots in users' inability to develop an appropriate class of\nprobabilistic models. This paper bridges this significant gap, introducing a\nnovel Bayesian hierarchical setting, which breaks time-history vibrational\nresponses into several segments so as to capture and identify the variability\nof inferred parameters over multiple segments. Since computation of the\nposterior distributions in hierarchical models is expensive and cumbersome,\nnovel marginalization strategies, asymptotic approximations, and maximum a\nposteriori estimations are proposed and outlined under a computational\nalgorithm aiming to handle both uncertainty quantification and propagation\ntasks. For the first time, the connection between the ensemble covariance\nmatrix and hyper distribution parameters is characterized through approximate\nestimations. Experimental and numerical examples are employed to illustrate the\nefficacy and efficiency of the proposed method. It is observed that, when the\nsegments correspond to various system conditions and input characteristics, the\nproposed method delivers robust parametric uncertainties with respect to\nunknown phenomena such as ambient conditions, input characteristics, and\nenvironmental factors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 03:28:13 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sedehi", "Omid", ""], ["Papadimitriou", "Costas", ""], ["Katafygiotis", "Lambros S.", ""]]}, {"id": "1912.02989", "submitter": "Ziming Liu", "authors": "Ziming Liu, Yixuan Wang, Zizhao Han and Dian Wu", "title": "Influenza Modeling Based on Massive Feature Engineering and\n  International Flow Deconvolution", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we focus on the analysis of the potential factors driving\nthe spread of influenza, and possible policies to mitigate the adverse effects\nof the disease. To be precise, we first invoke discrete Fourier transform (DFT)\nto conclude a yearly periodic regional structure in the influenza activity,\nthus safely restricting ourselves to the analysis of the yearly influenza\nbehavior. Then we collect a massive number of possible region-wise indicators\ncontributing to the influenza mortality, such as consumption, immunization,\nsanitation, water quality, and other indicators from external data, with $1170$\ndimensions in total. We extract significant features from the high dimensional\nindicators using a combination of data analysis techniques, including matrix\ncompletion, support vector machines (SVM), autoencoders, and principal\ncomponent analysis (PCA). Furthermore, we model the international flow of\nmigration and trade as a convolution on regional influenza activity, and solve\nthe deconvolution problem as higher-order perturbations to the linear\nregression, thus separating regional and international factors related to the\ninfluenza mortality. Finally, both the original model and the perturbed model\nare tested on regional examples, as validations of our models. Pertaining to\nthe policy, we make a proposal based on the connectivity data along with the\npreviously extracted significant features to alleviate the impact of influenza,\nas well as efficiently propagate and carry out the policies. We conclude that\nenvironmental features and economic features are of significance to the\ninfluenza mortality. The model can be easily adapted to model other types of\ninfectious diseases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:11:31 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Liu", "Ziming", ""], ["Wang", "Yixuan", ""], ["Han", "Zizhao", ""], ["Wu", "Dian", ""]]}, {"id": "1912.03018", "submitter": "Lucas Mentch", "authors": "Lucas Mentch", "title": "On Racial Disparities in Recent Fatal Police Shootings", "comments": "Accepted at Statistics and Public Policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fatal police shootings in the United States continue to be a polarizing\nsocial and political issue. Clear disagreement between racial proportions of\nvictims and nationwide racial demographics together with graphic video footage\nhas created fertile ground for controversy. However, simple population level\nsummary statistics fail to take into account fundamental local characteristics\nsuch as county-level racial demography, local arrest demography, and law\nenforcement density. Utilizing data on fatal police shootings between January\n2015 and July 2016, we implement a number of straightforward resampling\nprocedures designed to carefully examine how unlikely the victim totals from\neach race are with respect to these local population characteristics if no\nracial bias were present in the decision to shoot by police. We present several\napproaches considering the shooting locations both as fixed and also as a\nrandom sample. In both cases, we find overwhelming evidence of a racial\ndisparity in shooting victims with respect to local population demographics but\nsubstantially less disparity after accounting for local arrest demographics. We\nconclude our analyses by examining the effect of police-worn body cameras and\nfind no evidence that the presence of such cameras impacts the racial\ndistribution of victims.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:24:29 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mentch", "Lucas", ""]]}, {"id": "1912.03023", "submitter": "Anna Tovo", "authors": "Anna Tovo, Samuele Stivanello, Amos Maritan, Samir Suweis, Stefano\n  Favaro and Marco Formentin", "title": "Upscaling human activity data: an ecological perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years we have witnessed an explosion of data collected for\ndifferent human dynamics, from email communication to social networks\nactivities. Extract useful information from these huge data sets represents a\nmajor challenge. In the last decades, statistical regularities has been widely\nobserved in human activities and various models have been proposed. Here we\nmove from modeling to inference and propose a statistical framework capable to\npredict global features of human activities from local knowledge. We consider\nfour data sets of human activities: email communication, Twitter posts,\nWikipedia articles and Gutenberg books. From the statistics of local\nactivities, such as sent emails per senders, post per hashtags and word\noccurrences collected in a small sample of the considered dataset, we infer\nglobal features, as the number of senders, hashtags and words at the global\nscale. Our estimates are robust and accurate with a small relative error.\nMoreover, we predict how abundance of a hashtag or of a word may change through\nscales. Thus, observing a small portion of tweets and the popularity of a given\nhashtag among them, we can estimate whether it will remain popular or not in\nthe unseen part of the network. Our approach is grounded on statistical ecology\nas we discover inference of unseen human activity hallmarks can be mapped into\nthe unseen species problem in biodiversity. Our findings may have applications\nto different areas, from resource management in emails to collective attention\nmonitoring in Twitter and to language learning process in word databases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:45:26 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Tovo", "Anna", ""], ["Stivanello", "Samuele", ""], ["Maritan", "Amos", ""], ["Suweis", "Samir", ""], ["Favaro", "Stefano", ""], ["Formentin", "Marco", ""]]}, {"id": "1912.03343", "submitter": "Ganesh Ghimire", "authors": "Ganesh R. Ghimire, Navid Jadidoleslam, Witold F. Krajewski, Anastasios\n  A. Tsonis", "title": "Insights On Streamflow Predictability Across Scales Using Horizontal\n  Visibility Graph Based Networks", "comments": "34 pages and 12 figures", "journal-ref": null, "doi": "10.3389/frwa.2020.00017", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Streamflow is a dynamical process that integrates water movement in space and\ntime within basin boundaries. The authors characterize the dynamics associated\nwith streamflow time series data from about seventy-one U.S. Geological Survey\n(USGS) stream-gauge stations in the state of Iowa. They employ a novel approach\ncalled visibility graph (VG). It uses the concept of mapping time series into\ncomplex networks to investigate the time evolutionary behavior of dynamical\nsystem. The authors focus on a simple variant of VG algorithm called horizontal\nvisibility graph (HVG). The tracking of dynamics and hence, the predictability\nof streamflow processes, are carried out by extracting two key pieces of\ninformation called characteristic exponent, {\\lambda} of degree distribution\nand global clustering coefficient, GC pertaining to HVG derived network. The\nauthors use these two measures to identify whether streamflow process has its\norigin in random or chaotic processes. They show that the characterization of\nstreamflow dynamics is sensitive to data attributes. Through a systematic and\ncomprehensive analysis, the authors illustrate that streamflow dynamics\ncharacterization is sensitive to the normalization, and the time-scale of\nstreamflow time-series. At daily scale, streamflow at all stations used in the\nanalysis, reveals randomness with strong spatial scale (basin size) dependence.\nThis has implications for predictability of streamflow and floods. The authors\ndemonstrate that dynamics transition through potentially chaotic to randomly\ncorrelated process as the averaging time-scale increases. Finally, the temporal\ntrends of {\\lambda} and GC are statistically significant at about 40% of the\ntotal number of stations analyzed. Attributing this trend to factors such as\nchanging climate or land use requires further research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:09:49 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ghimire", "Ganesh R.", ""], ["Jadidoleslam", "Navid", ""], ["Krajewski", "Witold F.", ""], ["Tsonis", "Anastasios A.", ""]]}, {"id": "1912.03358", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir, Julio Isidro Sanchez", "title": "Adventures in Multi-Omics I: Combining heterogeneous data sets via\n  relationships matrices", "comments": "This project was supported by WheatSustain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.GN stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a covariance based method for combining partial\ndata sets in the genotype to phenotype spectrum. In particular, an\nexpectation-maximization algorithm that can be used to combine partially\noverlapping relationship/covariance matrices is introduced. Combining data this\nway, based on relationship matrices, can be contrasted with a feature\nimputation based approach. We used several public genomic data sets to explore\nthe accuracy of combining genomic relationship matrices. We have also used the\nheterogeneous genotype/phenotype data sets in the https://triticeaetoolbox.org/\nto illustrate how this new method can be used in genomic prediction, phenomics,\nand graphical modeling.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:39:58 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 15:55:39 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Akdemir", "Deniz", ""], ["Sanchez", "Julio Isidro", ""]]}, {"id": "1912.03463", "submitter": "Erdem Varol", "authors": "Erdem Varol, Amin Nejatbakhsh, Conor McGrory", "title": "Temporal Wasserstein non-negative matrix factorization for non-rigid\n  motion segmentation and spatiotemporal deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV cs.LG eess.IV q-bio.QM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motion segmentation for natural images commonly relies on dense optic flow to\nyield point trajectories which can be grouped into clusters through various\nmeans including spectral clustering or minimum cost multicuts. However, in\nbiological imaging scenarios, such as fluorescence microscopy or calcium\nimaging, where the signal to noise ratio is compromised and intensity\nfluctuations occur, optical flow may be difficult to approximate. To this end,\nwe propose an alternative paradigm for motion segmentation based on optimal\ntransport which models the video frames as time-varying mass represented as\nhistograms. Thus, we cast motion segmentation as a temporal non-linear matrix\nfactorization problem with Wasserstein metric loss. The dictionary elements of\nthis factorization yield segmentation of motion into coherent objects while the\nloading coefficients allow for time-varying intensity signal of the moving\nobjects to be captured. We demonstrate the use of the proposed paradigm on a\nsimulated multielectrode drift scenario, as well as calcium indicating\nfluorescence microscopy videos of the nematode Caenorhabditis elegans (C.\nelegans). The latter application has the added utility of extracting neural\nactivity of the animal in freely conducted behavior.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 08:30:23 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Varol", "Erdem", ""], ["Nejatbakhsh", "Amin", ""], ["McGrory", "Conor", ""]]}, {"id": "1912.03603", "submitter": "Yishu Xue", "authors": "Hou-Cheng Yang, Lijiang Geng, Yishu Xue, Guanyu Hu", "title": "Spatial Weibull Regression with Multivariate Log Gamma Process and Its\n  Applications to China Earthquake Economic Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian spatial modeling of heavy-tailed distributions has become\nincreasingly popular in various areas of science in recent decades. We propose\na Weibull regression model with spatial random effects for analyzing extreme\neconomic loss. Model estimation is facilitated by a computationally efficient\nBayesian sampling algorithm utilizing the multivariate Log-Gamma distribution.\nSimulation studies are carried out to demonstrate better empirical performances\nof the proposed model than the generalized linear mixed effects model. An\nearthquake data obtained from Yunnan Seismological Bureau, China is analyzed.\nLogarithm of the Pseudo-marginal likelihood values are obtained to select the\noptimal model, and Value-at-risk, expected shortfall, and tail-value-at-risk\nbased on posterior predictive distribution of the optimal model are calculated\nunder different confidence levels.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 03:14:10 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Yang", "Hou-Cheng", ""], ["Geng", "Lijiang", ""], ["Xue", "Yishu", ""], ["Hu", "Guanyu", ""]]}, {"id": "1912.03706", "submitter": "Mattia Zanella", "authors": "M. Herty, A. Tosin, G. Visconti, M. Zanella", "title": "Reconstruction of traffic speed distributions from kinetic models with\n  uncertainties", "comments": null, "journal-ref": "In G. Puppo, A. Tosin, Eds., Mathematical Descriptions of Traffic\n  Flow: Micro, Macro and Kinetic Models SEMA SIMAI Springer Series, volume 12,\n  pages 1-16. Springer, 2021", "doi": "10.1007/978-3-030-66560-9_1", "report-no": null, "categories": "nlin.AO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the ability of a kinetic approach for traffic\ndynamics to predict speed distributions obtained through rough data. The\npresent approach adopts the formalism of uncertainty quantification, since\nreaction strengths are uncertain and linked to different types of driver\nbehaviour or different classes of vehicles present in the flow. Therefore, the\ncalibration of the expected speed distribution has to face the reconstruction\nof the distribution of the uncertainty. We adopt experimental microscopic\nmeasurements recorded on a German motorway, whose speed distribution shows a\nmultimodal trend. The calibration is performed by extrapolating the uncertainty\nparameters of the kinetic distribution via a constrained optimisation approach.\nThe results confirm the validity of the theoretical set-up.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:50:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Herty", "M.", ""], ["Tosin", "A.", ""], ["Visconti", "G.", ""], ["Zanella", "M.", ""]]}, {"id": "1912.03781", "submitter": "Pierfrancesco Alaimo Di Loro", "authors": "Giovanna Tagliaferri, Daria Scacciatelli, Pierfrancesco Alaimo Di Loro", "title": "VAT tax gap prediction: a 2-steps Gradient Boosting approach", "comments": "27 pages, 4 figures, 8 tables Presented at NTTS 2019 conference Under\n  review at another peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tax evasion is the illegal evasion of taxes by individuals, corporations, and\ntrusts. The revenue loss from tax avoidance can undermine the effectiveness and\nequity of the government policies. A standard measure of tax evasion is the tax\ngap, that can be estimated as the difference between the total amounts of tax\ntheoretically collectable and the total amounts of tax actually collected in a\ngiven period. This paper presents an original contribution to bottom-up\napproach, based on results from fiscal audits, through the use of Machine\nLearning. The major disadvantage of bottom-up approaches is represented by\nselection bias when audited taxpayers are not randomly selected, as in the case\nof audits performed by the Italian Revenue Agency. Our proposal, based on a\n2-steps Gradient Boosting model, produces a robust tax gap estimate and, embeds\na solution to correct for the selection bias which do not require any\nassumptions on the underlying data distribution. The 2-steps Gradient Boosting\napproach is used to estimate the Italian Value-added tax (VAT) gap on\nindividual firms on the basis of fiscal and administrative data income tax\nreturns gathered from Tax Administration Data Base, for the fiscal year 2011.\nThe proposed method significantly boost the performance in predicting with\nrespect to the classical parametric approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:16:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 21:29:07 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 23:06:27 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tagliaferri", "Giovanna", ""], ["Scacciatelli", "Daria", ""], ["Di Loro", "Pierfrancesco Alaimo", ""]]}, {"id": "1912.03861", "submitter": "Sami Malek", "authors": "Sami A. Malek, Alexandre M. Bayen, Steven D. Glaser", "title": "Daily Data Assimilation of a Hydrologic Model Using the Ensemble Kalman\n  Filter", "comments": "18 pages, 5 figures, 4 tables and supplement", "journal-ref": "Published as Masters thesis here:\n  https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-101.html", "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate runoff forecasting is crucial for reservoir operators as it allows\noptimized water management, flood control and hydropower generation. Land\nsurface models in mountainous regions depend on climatic inputs such as\nprecipitation, temperature and solar radiation to model the water and energy\ndynamics and produce runoff as output. With the rapid development of cheap\nelectronics applied in various systems, such as Wireless Sensor Networks\n(WSNs), satellite and airborne technologies, the prospect of practically\nmeasuring spatial Snow Water Equivalent in a dense temporal scale is\nincreasing. We present a framework for updating the Precipitation Runoff\nModeling System (PRMS) with Snow Water Equivalent (SWE) maps and runoff\nmeasurements on a daily timescale based on the Ensemble Kalman Filter (ENKF).\nResults show that by assimilating SWE daily, the modeled SWE gets updated\naccordingly, however no improvement is observed at the runoff model output.\nInstead, a deterioration consistently occurs. Augmenting the state space with\nmodel parameters and runoff model output allows for filter update with previous\nday measured runoff using the joint state-parameter method, and showed a\nconsiderable improvement in the daily runoff output of up to 60% reduction in\nRMSE for the wet water year 2011 relative to the no assimilation scenario, and\nimprovement of up to 28% compared to a naive autoregressive AR(1) filter.\nAdditional simulation years showed consistent improvement compared to no\nassimilation, but varied relative to the previous day autoregressive forecast\nduring the dry year 2014.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 05:50:17 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Malek", "Sami A.", ""], ["Bayen", "Alexandre M.", ""], ["Glaser", "Steven D.", ""]]}, {"id": "1912.04045", "submitter": "Yuchen Shi", "authors": "Yuchen Shi and Nan Chen", "title": "Phase I analysis of hidden operating status for wind turbine", "comments": null, "journal-ref": null, "doi": "10.1109/IEEM44572.2019.8978833", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven methods based on Supervisory Control and Data Acquisition (SCADA)\nbecome a recent trend for wind turbine condition monitoring. However, SCADA\ndata are known to be of low quality due to low sampling frequency and complex\nturbine working dynamics. In this work, we focus on the phase I analysis of\nSCADA data to better understand turbines' operating status. As one of the most\nimportant characterization, the power curve is used as a benchmark to represent\nnormal performance. A powerful distribution-free control chart is applied after\nthe power generation is adjusted by an accurate power curve model, which\nexplicitly takes into account the known factors that can affect turbines'\nperformance. Informative out-of-control segments have been revealed in real\nfield case studies. This phase I analysis can help improve wind turbine's\nmonitoring, reliability, and maintenance for a smarter wind energy system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 13:58:32 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 12:15:48 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Shi", "Yuchen", ""], ["Chen", "Nan", ""]]}, {"id": "1912.04086", "submitter": "Yinzhi Wang", "authors": "Erik B{\\o}lviken and Yinzhi Wang", "title": "Optimal reinsurance for risk over surplus ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal reinsurance when Value at Risk and expected surplus is balanced\nthrough their ratio is studied, and it is demonstrated how results for\nrisk-adjusted surplus can be utilized. Simplifications for large portfolios are\nderived, and this large-portfolio study suggests a new condition on the\nreinsurance pricing regime which is crucial for the results obtained. One or\ntwo-layer contracts now become optimal for both risk-adjusted surplus and the\nrisk over expected surplus ratio, but there is no second layer when portfolios\nare large or when reinsurance prices are below some threshold. Simple\napproximations of the optimum portfolio are considered, and their degree of\ndegradation compared to the optimum is studied which leads to theoretical\ndegradation rates as the number of policies grows. The theory is supported by\nnumerical experiments which suggest that the shape of the claim severity\ndistributions may not be of primary importance when designing an optimal\nreinsurance program. It is argued that the approach can be applied to\nConditional Value at Risk as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:40:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["B\u00f8lviken", "Erik", ""], ["Wang", "Yinzhi", ""]]}, {"id": "1912.04151", "submitter": "Forrest Crawford", "authors": "Xiaoxuan Cai, Wen Wei Loh, Forrest W. Crawford", "title": "Identification of causal intervention effects under contagion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining and identifying causal intervention effects for transmissible\ninfectious disease outcomes is challenging because a treatment -- such as a\nvaccine -- given to one individual may affect the infection outcomes of others.\nEpidemiologists have proposed causal estimands to quantify effects of\ninterventions under contagion using a two-person partnership model. These\nsimple conceptual models have helped researchers develop causal estimands\nrelevant to clinical evaluation of vaccine effects. However, many of these\npartnership models are formulated under structural assumptions that preclude\nrealistic infectious disease transmission dynamics, limiting their conceptual\nusefulness in defining and identifying causal treatment effects in empirical\nintervention trials. In this paper, we propose causal intervention effects in\ntwo-person partnerships under arbitrary infectious disease transmission\ndynamics, and give nonparametric identification results showing how effects can\nbe estimated in empirical trials using time-to-infection or binary outcome\ndata. The key insight is that contagion is a causal phenomenon that induces\nconditional independencies on infection outcomes that can be exploited for the\nidentification of clinically meaningful causal estimands. These new estimands\nare compared to existing quantities, and results are illustrated using a\nrealistic simulation of an HIV vaccine trial.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:22:05 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:49:28 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Cai", "Xiaoxuan", ""], ["Loh", "Wen Wei", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1912.04171", "submitter": "Shovan Chowdhury", "authors": "Amarjit Kundu, Shovan Chowdhury, Narayanaswamy Balakrishnan", "title": "Ordering properties of the smallest and largest lifetimes in\n  Gompertz-Makeham model", "comments": "arXiv admin note: text overlap with arXiv:1710.00769", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gompertz-Makeham distribution, which is used commonly to represent\nlifetimes based on laws of mortality, is one of the most popular choices for\nmortality modelling in the field of actuarial science. This paper investigates\nordering properties of the smallest and largest lifetimes arising from two sets\nof heterogeneous groups of insurees following respective Gompertz-Makeham\ndistributions. Some sufficient conditions are provided in the sense of usual\nstochastic ordering to compare the smallest and largest lifetimes from two sets\nof dependent variables. Comparison results on the smallest lifetimes in the\nsense of hazard rate ordering and ageing faster ordering are established for\ntwo groups of heterogeneous independent lifetimes. Under similar set-up, no\nreversed hazard rate ordering is shown to exist between the largest lifetimes\nwith the use of a counter-example. Finally, we present sufficient conditions to\nstochastically compare two sets of independent heterogeneous lifetimes under\nrandom shocks by means of usual stochastic ordering. Such comparisons for the\nsmallest lifetimes are also carried out in terms of hazard rate ordering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:40:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Kundu", "Amarjit", ""], ["Chowdhury", "Shovan", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "1912.04175", "submitter": "Yinzhi Wang", "authors": "Yinzhi Wang and Erik B{\\o}lviken", "title": "How much is optimal reinsurance degraded by error?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on optimal reinsurance does not deal with how much the\neffectiveness of such solutions is degraded by errors in parameters and models.\nThe issue is investigated through both asymptotics and numerical studies. It is\nshown that the rate of degradation is often $O(1/n)$ as the sample size $n$ of\nhistorical observations becomes infinite. Criteria based on Value at Risk are\nexceptions that may achieve only $O(1/\\sqrt{n})$. These theoretical results are\nsupported by numerical studies. A Bayesian perspective on how to integrate risk\ncaused by parameter error is offered as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:50:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Yinzhi", ""], ["B\u00f8lviken", "Erik", ""]]}, {"id": "1912.04202", "submitter": "Helmi Shat", "authors": "Helmi Shat and Rainer Schwabe", "title": "Optimal Stress Levels in Accelerated Degradation Testing for Various\n  Degradation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated degradation tests are used to provide accurate estimation of\nlifetime characteristics of highly reliable products within a relatively short\ntesting time. Data from particular tests at high levels of stress (e.g.,\ntemperature, voltage, or vibration) are extrapolated, through a physically\nmeaningful statistical model, to attain estimates of lifetime quantiles at\nnormal use conditions. The gamma process is a natural model for estimating the\ndegradation increments over certain degradation paths, which exhibit a monotone\nand strictly increasing degradation pattern. In this work, we derive first an\nalgorithm-based optimal design for a repeated measures degradation test with\nsingle failure mode that corresponds to a single response component. The\nunivariate degradation process is expressed using a gamma model where a\ngeneralized linear model is introduced to facilitate the derivation of an\noptimal design. Consequently, we extend the univariate model and characterize\noptimal designs for accelerated degradation tests with bivariate degradation\nprocesses. The first bivariate model includes two gamma processes as marginal\ndegradation models. The second bivariate models is expressed by a gamma process\nalong with a mixed effects linear model. We derive optimal designs for\nminimizing the asymptotic variance for estimating some quantile of the failure\ntime distribution at the normal use conditions. Sensitivity analysis is\nconducted to study the behavior of the resulting optimal designs under\nmisspecifications of adopted nominal values.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 17:35:46 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:13:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Shat", "Helmi", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1912.04406", "submitter": "Sule Sahin", "authors": "Gary Venter and \\c{S}ule \\c{S}ahin", "title": "Semiparametric Regression for Dual Population Mortality", "comments": "39 pages, 8 graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter shrinkage applied optimally can always reduce error and projection\nvariances from those of maximum likelihood estimation. Many variables that\nactuaries use are on numerical scales, like age or year, which require\nparameters at each point. Rather than shrinking these towards zero, nearby\nparameters are better shrunk towards each other. Semiparametric regression is a\nstatistical discipline for building curves across parameter classes using\nshrinkage methodology. It is similar to but more parsimonious than cubic\nsplines. We introduce it in the context of Bayesian shrinkage and apply it to\njoint mortality modeling for related populations. Bayesian shrinkage of slope\nchanges of linear splines is an approach to semiparametric modeling that\nevolved in the actuarial literature. It has some theoretical and practical\nadvantages, like closed-form curves, direct and transparent determination of\ndegree of shrinkage and of placing knots for the splines, and quantifying\ngoodness of fit. It is also relatively easy to apply to the many nonlinear\nmodels that arise in actuarial work. We find that it compares well to a more\ncomplex state-of-the-art statistical spline shrinkage approach on a popular\nexample from that literature.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 22:19:34 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 20:26:38 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 23:26:11 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Venter", "Gary", ""], ["\u015eahin", "\u015eule", ""]]}, {"id": "1912.04435", "submitter": "Thomas Lumley", "authors": "Thomas Lumley", "title": "Stylised Choropleth Maps for New Zealand Regions and District Health\n  Boards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New Zealand has two top-level sets of administrative divisions: the District\nHealth Boards and the Regions. In this note I describe a hexagonal layout for\ncreating stylised maps of these divisions, and using colour, size, and\ntriangular subdivisions to compare data between divisions and across multiple\nvariables. I present an implementation in the DHBins package for R using both\nbase graphics and ggplot2; the concepts and specific hexagonal layout could be\nused in any software.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:01:27 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lumley", "Thomas", ""]]}, {"id": "1912.04523", "submitter": "Victoria Lin", "authors": "Victoria Lin, Jeffrey M. Girard, Louis-Philippe Morency", "title": "Context-Dependent Models for Predicting and Characterizing Facial\n  Expressiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, extensive research has emerged in affective computing on\ntopics like automatic emotion recognition and determining the signals that\ncharacterize individual emotions. Much less studied, however, is\nexpressiveness, or the extent to which someone shows any feeling or emotion.\nExpressiveness is related to personality and mental health and plays a crucial\nrole in social interaction. As such, the ability to automatically detect or\npredict expressiveness can facilitate significant advancements in areas ranging\nfrom psychiatric care to artificial social intelligence. Motivated by these\npotential applications, we present an extension of the BP4D+ dataset with human\nratings of expressiveness and develop methods for (1) automatically predicting\nexpressiveness from visual data and (2) defining relationships between\ninterpretable visual signals and expressiveness. In addition, we study the\nemotional context in which expressiveness occurs and hypothesize that different\nsets of signals are indicative of expressiveness in different contexts (e.g.,\nin response to surprise or in response to pain). Analysis of our statistical\nmodels confirms our hypothesis. Consequently, by looking at expressiveness\nseparately in distinct emotional contexts, our predictive models show\nsignificant improvements over baselines and achieve comparable results to human\nperformance in terms of correlation with the ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:10:25 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lin", "Victoria", ""], ["Girard", "Jeffrey M.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1912.04677", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Testing and Estimating Change-Points in the Covariance Matrix of a\n  High-Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies methods for testing and estimating change-points in the\ncovariance structure of a high-dimensional linear time series. The assumed\nframework allows for a large class of multivariate linear processes (including\nvector autoregressive moving average (VARMA) models) of growing dimension and\nspiked covariance models. The approach uses bilinear forms of the centered or\nnon-centered sample variance-covariance matrix. Change-point testing and\nestimation are based on maximally selected weighted cumulated sum (CUSUM)\nstatistics. Large sample approximations under a change-point regime are\nprovided including a multivariate CUSUM transform of increasing dimension. For\nthe unknown asymptotic variance and covariance parameters associated to (pairs\nof) CUSUM statistics we propose consistent estimators. Based on weak laws of\nlarge numbers for their sequential versions, we also consider stopped sample\nestimation where observations until the estimated change-point are used. Finite\nsample properties of the procedures are investigated by simulations and their\napplication is illustrated by analyzing a real data set from environmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 13:28:05 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 13:18:21 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1912.04924", "submitter": "Sven Buitendag Mr", "authors": "Jan Beirlant, Sven Buitendag, Eustasio del Bario, Marc Hallin", "title": "Center-outward quantiles and the measurement of multivariate risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All multivariate extensions of the univariate theory of risk measurement run\ninto the same fundamental problem of the absence, in dimension d > 1, of a\ncanonical ordering of Rd. Based on measure transportation ideas, several\nattempts have been made recently in the statistical literature to overcome that\nconceptual difficulty. In Hallin (2017), the concepts of center-outward\ndistribution and quantile functions are developed as generalisations of the\nclassical univariate concepts of distribution and quantile functions, along\nwith their empirical versions. We propose a class of smooth approximations as\nan alternative to the interpolation developed in del Barrio et al. (2018). This\napproximation allows for the computation of some new empirical risk measures,\nbased either on the convex potential associated with the proposed transports,\nor on the volumes of the resulting empirical quantile regions. We also discuss\nthe role of such transports in the evaluation of the risk associated with\nmultivariate regularly varying distributions. Some simulations and applications\nto case studies illustrate the value of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:02:05 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Beirlant", "Jan", ""], ["Buitendag", "Sven", ""], ["del Bario", "Eustasio", ""], ["Hallin", "Marc", ""]]}, {"id": "1912.05025", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e, Angelos T. Vouldis", "title": "European banks' business models and their credit risk: A cluster\n  analysis in a high-dimensional context", "comments": null, "journal-ref": null, "doi": "10.1007/s10479-021-04045-9", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the credit risk in the loan portfolio of banks\nfollowing different business models. We develop a data-driven methodology for\nidentifying the business models of the 365 largest European banks that is\nsuitable for very granular harmonised supervisory data. Our dataset allows us\nto take into account the full range of the activities in which banks are\ninvolved. The proposed method combines in an optimal way data clustering,\ndimensionality reduction and outlier detection. We identify four business\nmodels and exclude as 'outliers' banks that follow idiosyncratic business\nmodels. Furthermore, empirical evidence is provided that banks following\ndifferent business models differ significantly with respect to the credit risk\nthey undertake in their loan portfolios. Traditional commercial banks are\ncharacterized by the lowest levels of credit risk while the loan portfolios of\nsecurities holding banks are riskier compared to the other banks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:13:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Vouldis", "Angelos T.", ""]]}, {"id": "1912.05045", "submitter": "James Bagrow", "authors": "Abigail Hotaling and James P. Bagrow", "title": "Efficient crowdsourcing of crowd-generated microtasks", "comments": "12 pages, 5 figures", "journal-ref": "PLoS ONE 15(12): e0244245, 2020", "doi": "10.1371/journal.pone.0244245", "report-no": null, "categories": "cs.HC cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing members of the crowd to propose novel microtasks for one another is\nan effective way to combine the efficiencies of traditional microtask work with\nthe inventiveness and hypothesis generation potential of human workers.\nHowever, microtask proposal leads to a growing set of tasks that may overwhelm\nlimited crowdsourcer resources. Crowdsourcers can employ methods to utilize\ntheir resources efficiently, but algorithmic approaches to efficient\ncrowdsourcing generally require a fixed task set of known size. In this paper,\nwe introduce *cost forecasting* as a means for a crowdsourcer to use efficient\ncrowdsourcing algorithms with a growing set of microtasks. Cost forecasting\nallows the crowdsourcer to decide between eliciting new tasks from the crowd or\nreceiving responses to existing tasks based on whether or not new tasks will\ncost less to complete than existing tasks, efficiently balancing resources as\ncrowdsourcing occurs. Experiments with real and synthetic crowdsourcing data\nshow that cost forecasting leads to improved accuracy. Accuracy and efficiency\ngains for crowd-generated microtasks hold the promise to further leverage the\ncreativity and wisdom of the crowd, with applications such as generating more\ninformative and diverse training data for machine learning applications and\nimproving the performance of user-generated content and question-answering\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 23:23:54 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 19:24:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hotaling", "Abigail", ""], ["Bagrow", "James P.", ""]]}, {"id": "1912.05125", "submitter": "Sebastian Kurtek", "authors": "James Matuk, Karthik Bharath, Oksana Chkrebtii, Sebastian Kurtek", "title": "Bayesian Framework for Simultaneous Registration and Estimation of\n  Noisy, Sparse and Fragmented Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, smooth processes generate data that is recorded under a\nvariety of observation regimes, such as dense, sparse or fragmented\nobservations that are often contaminated with error. The statistical goal of\nregistering and estimating the individual underlying functions from discrete\nobservations has thus far been mainly approached sequentially without formal\nuncertainty propagation, or in an application-specific manner. We propose a\nunified Bayesian framework for simultaneous registration and estimation, which\nis flexible enough to accommodate inference on individual functions under\ngeneral observation regimes. Our ability to do this relies on the specification\nof strongly informative prior models over the amplitude component of function\nvariability. We provide two strategies for this critical choice: a data-driven\napproach that defines an empirical basis for the amplitude subspace based on\ntraining data, and a shape-restricted approach when the relative location and\nnumber of local extrema is well-understood. The proposed methods build on\nelastic functional data analysis, which separately models amplitude and phase\nvariability inherent in functional data. We emphasize the importance of\nuncertainty quantification and visualization of these two components as they\nprovide complementary information about the estimated functions. We validate\nthe framework using simulations and real applications to medical imaging and\nbiometrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:48:00 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Matuk", "James", ""], ["Bharath", "Karthik", ""], ["Chkrebtii", "Oksana", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1912.05129", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz, Jacob Mortensen, and Luke Bornn", "title": "Measuring Spatial Allocative Efficiency in Basketball", "comments": "The first and second authors contributed equally to this work. 28\n  pages, 19 figures", "journal-ref": "Journal of Quantitative Analysis in Sports, vol. 16, no. 4, 2020,\n  pp. 271-289", "doi": "10.1515/jqas-2019-0126", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every shot in basketball has an opportunity cost; one player's shot\neliminates all potential opportunities from their teammates for that play. For\nthis reason, player-shot efficiency should ultimately be considered relative to\nthe lineup. This aspect of efficiency---the optimal way to allocate shots\nwithin a lineup---is the focus of our paper. Allocative efficiency should be\nconsidered in a spatial context since the distribution of shot attempts within\na lineup is highly dependent on court location. We propose a new metric for\nspatial allocative efficiency by comparing a player's field goal percentage\n(FG%) to their field goal attempt (FGA) rate in context of both their four\nteammates on the court and the spatial distribution of their shots. Leveraging\npublicly available data provided by the National Basketball Association (NBA),\nwe estimate player FG% at every location in the offensive half court using a\nBayesian hierarchical model. Then, by ordering a lineup's estimated FG%s and\npairing these rankings with the lineup's empirical FGA rate rankings, we detect\nareas where the lineup exhibits inefficient shot allocation. Lastly, we analyze\nthe impact that sub-optimal shot allocation has on a team's overall offensive\npotential, demonstrating that inefficient shot allocation correlates with\nreduced scoring.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:56:55 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 18:50:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Mortensen", "Jacob", ""], ["Bornn", "Luke", ""]]}, {"id": "1912.05133", "submitter": "Ozgur Asar", "authors": "Lisa McFetridge, Ozgur Asar, Jonas Wallin", "title": "Robust joint modelling of longitudinal and survival data with a\n  time-varying degrees-of-freedom parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Repeated measures of biomarkers have the potential of explaining hazards of\nsurvival outcomes. In practice, these measurements are intermittently measured\nand are known to be subject to substantial measurement error. Joint modelling\nof longitudinal and survival data enables us to associate intermittently\nmeasured error-prone biomarkers with risks of survival outcomes. Most of the\njoint models available in the literature have been built on the Gaussian\nassumption. This makes them sensitive to outliers. In this work, we study a\nrange of robust models to address this issue. For medical data, it has been\nobserved that outliers might occur with different frequencies over time. To\naddress this, a new model with a time varying robustness is introduced. Through\nboth a simulation study and analysis of two real-life data examples, this\nresearch not only stresses the need to account for longitudinal outliers in\njoint modelling research but also highlights the bias and inefficiency from not\nproperly estimating the degrees-of-freedom parameter. Each technique presented\nin this work can be fitted using the R package robjm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:20:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["McFetridge", "Lisa", ""], ["Asar", "Ozgur", ""], ["Wallin", "Jonas", ""]]}, {"id": "1912.05258", "submitter": "Martina McMenamin", "authors": "Martina McMenamin, Jessica K. Barrett, Anna Berglind, James M.S. Wason", "title": "Sample Size Estimation using a Latent Variable Model for Mixed Outcome\n  Co-Primary, Multiple Primary and Composite Endpoints", "comments": "36 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed outcome endpoints that combine multiple continuous and discrete\ncomponents to form co-primary, multiple primary or composite endpoints are\noften employed as primary outcome measures in clinical trials. There are many\nadvantages to joint modelling the individual outcomes using a latent variable\nframework, however in order to make use of the model in practice we require\ntechniques for sample size estimation. In this paper we show how the latent\nvariable model can be applied to the three types of joint endpoints and propose\nappropriate hypotheses, power and sample size estimation methods for each. We\nillustrate the techniques using a numerical example based on the four\ndimensional endpoint in the MUSE trial and find that the sample size required\nfor the co-primary endpoint is larger than that required for the individual\nendpoint with the smallest effect size. Conversely, the sample size required\nfor the multiple primary endpoint is reduced from that required for the\nindividual outcome with the largest effect size. We show that the analytical\ntechnique agrees with the empirical power from simulation studies. We further\nillustrate the reduction in required sample size that may be achieved in trials\nof mixed outcome composite endpoints through a simulation study and find that\nthe sample size primarily depends on the components driving response and the\ncorrelation structure and much less so on the treatment effect structure in the\nindividual endpoints.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:24:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["McMenamin", "Martina", ""], ["Barrett", "Jessica K.", ""], ["Berglind", "Anna", ""], ["Wason", "James M. S.", ""]]}, {"id": "1912.05503", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Emanuel Parzen", "title": "Nonparametric Universal Copula Modeling", "comments": "A perspective on \"60 years of Copula\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle the ubiquitous problem of \"dependence learning,\" copulas are\nquickly becoming a pervasive tool across a wide range of data-driven\ndisciplines encompassing neuroscience, finance, econometrics, genomics, social\nscience, machine learning, healthcare and many more. Copula (or connection)\nfunctions were invented in 1959 by Abe Sklar in response to a query of Maurice\nFrechet. After 60 years, where do we stand now? This article provides a history\nof the key developments and offers a unified perspective.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:07:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1912.05588", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Parametric mode regression for bounded responses", "comments": "To appear in Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new parametric frameworks of regression analysis with the\nconditional mode of a bounded response as the focal point of interest.\nCovariate effects estimation and prediction based on the maximum likelihood\nmethod under two new classes of regression models are demonstrated. We also\ndevelop graphical and numerical diagnostic tools to detect various sources of\nmodel misspecification. Predictions based on different central tendency\nmeasures inferred using various regression models are compared using synthetic\ndata in simulations. Finally, we conduct regression analysis for data from the\nAlzheimer's Disease Neuroimaging Initiative to demonstrate practical\nimplementation of the proposed methods. Supplementary materials that contain\ntechnical details, and additional simulation and data analysis results are\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:25:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 16:18:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1912.05595", "submitter": "Sourish Chakravarty", "authors": "Sourish Chakravarty, Zachary D. Threlkeld, Yelena G. Bodien, Brian L.\n  Edlow, Emery N. Brown", "title": "A state-space model for dynamic functional connectivity", "comments": "Presented in 53rd Annual Asilomar Conference on Signals, Systems, and\n  Computers, Pacific Grove, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity (DFC) analysis involves measuring correlated\nneural activity over time across multiple brain regions. Significant regional\ncorrelations among neural signals, such as those obtained from resting-state\nfunctional magnetic resonance imaging (fMRI), may represent neural circuits\nassociated with rest. The conventional approach of estimating the correlation\ndynamics as a sequence of static correlations from sliding time-windows has\nstatistical limitations. To address this issue, we propose a multivariate\nstochastic volatility model for estimating DFC inspired by recent work in\neconometrics research. This model assumes a state-space framework where the\ncorrelation dynamics of a multivariate normal observation sequence is governed\nby a positive-definite matrix-variate latent process. Using this statistical\nmodel within a sequential Bayesian estimation framework, we use blood\noxygenation level dependent activity from multiple brain regions to estimate\nposterior distributions on the correlation trajectory. We demonstrate the\nutility of this DFC estimation framework by analyzing its performance on\nsimulated data, and by estimating correlation dynamics in resting state fMRI\ndata from a patient with a disorder of consciousness (DoC). Our work advances\nthe state-of-the-art in DFC analysis and its principled use in DoC biomarker\nexploration.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 20:01:45 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chakravarty", "Sourish", ""], ["Threlkeld", "Zachary D.", ""], ["Bodien", "Yelena G.", ""], ["Edlow", "Brian L.", ""], ["Brown", "Emery N.", ""]]}, {"id": "1912.05622", "submitter": "Rongjie Liu", "authors": "Rongjie Liu, Meng Li, Li Ma", "title": "Efficient in-situ image and video compression through probabilistic\n  image representation", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective image compression for multi-dimensional images has become\nincreasingly important for efficient storage and transfer of massive amounts of\nhigh-resolution images and videos. Desirable properties in compression methods\ninclude (1) high reconstruction quality at a wide range of compression rates\nwhile preserving key local details, (2) computational scalability, (3)\napplicability to a variety of different image/video types and of different\ndimensions, (4) progressive transmission, and (5) ease of tuning. We present\nsuch a method for multi-dimensional image compression called Compression via\nAdaptive Recursive Partitioning (CARP). CARP uses an optimal permutation of the\nimage pixels inferred from a Bayesian probabilistic model on recursive\npartitions of the image to reduce its effective dimensionality, achieving a\nparsimonious representation that preserves information. CARP uses a multi-layer\nBayesian hierarchical model to achieve in-situ compression along with\nself-tuning and regularization, with just one single parameter to be specified\nby the user to achieve the desired compression rate. Extensive numerical\nexperiments using a variety of datasets including 2D still images, real-life\nYouTube videos, and surveillance videos show that CARP dominates the\nstate-of-the-art image/video compression approaches---including JPEG, JPEG2000,\nBPG, MPEG4, HEVC and a neural network-based method---for all of these different\nimage types and on nearly all of the individual images and videos over some\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:00:08 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:32:49 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 00:54:48 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Liu", "Rongjie", ""], ["Li", "Meng", ""], ["Ma", "Li", ""]]}, {"id": "1912.05657", "submitter": "Raphael Huser", "authors": "Arnab Hazra and Rapha\\\"el Huser", "title": "Estimating high-resolution Red Sea surface temperature hotspots, using a\n  low-rank semiparametric spatial model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we estimate extreme sea surface temperature (SST) hotspots,\ni.e., high threshold exceedance regions, for the Red Sea, a vital region of\nhigh biodiversity. We analyze high-resolution satellite-derived SST data\ncomprising daily measurements at 16703 grid cells across the Red Sea over the\nperiod 1985-2015. We propose a semiparametric Bayesian spatial mixed-effects\nlinear model with a flexible mean structure to capture spatially-varying trend\nand seasonality, while the residual spatial variability is modeled through a\nDirichlet process mixture (DPM) of low-rank spatial Student-$t$ processes\n(LTPs). By specifying cluster-specific parameters for each LTP mixture\ncomponent, the bulk of the SST residuals influence tail inference and hotspot\nestimation only moderately. Our proposed model has a nonstationary mean,\ncovariance and tail dependence, and posterior inference can be drawn\nefficiently through Gibbs sampling. In our application, we show that the\nproposed method outperforms some natural parametric and semiparametric\nalternatives. Moreover, we show how hotspots can be identified and we estimate\nextreme SST hotspots for the whole Red Sea, projected until the year 2100,\nbased on the Representative Concentration Pathways 4.5 and 8.5. The estimated\n95\\% credible region for joint high threshold exceedances include large areas\ncovering major endangered coral reefs in the southern Red Sea.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:53:44 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 08:43:00 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 13:53:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hazra", "Arnab", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1912.05965", "submitter": "Oliver Stoner", "authors": "Oliver Stoner, Theo Economou and Alba Halliday", "title": "A Powerful Modelling Framework for Nowcasting and Forecasting COVID-19\n  and Other Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has highlighted delayed reporting as a significant\nimpediment to effective disease surveillance and decision-making. In the\nabsence of timely data, statistical models which account for delays can be\nadopted to nowcast and forecast cases or deaths. We discuss the four key\nsources of systematic and random variability in available data for COVID-19 and\nother diseases, and critically evaluate current state-of-the-art methods with\nrespect to appropriately separating and capturing this variability. We present\na general spatio-temporal hierarchical framework for correcting delayed\nreporting and demonstrate its application to daily English hospital deaths from\nCOVID-19 and Severe Acute Respiratory Infection cases in Brazil. We compare our\napproach to competing models with respect to theoretical flexibility and\nquantitative metrics from a rolling nowcasting experiment imitating a realistic\noperational scenario. Based on consistent and compelling leads in nowcasting\naccuracy, bias, and precision, we demonstrate that our approach represents the\ncurrent best-practice for correcting delayed reporting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:01:25 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 15:40:52 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Stoner", "Oliver", ""], ["Economou", "Theo", ""], ["Halliday", "Alba", ""]]}, {"id": "1912.05974", "submitter": "Nicola Rennie", "authors": "Nicola Rennie, Catherine Cleophas, Adam M. Sykulski, Florian Dost", "title": "Identifying and Responding to Outlier Demand in Revenue Management", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2021.01.002", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revenue management strongly relies on accurate forecasts. Thus, when\nextraordinary events cause outlier demand, revenue management systems need to\nrecognise this and adapt both forecast and controls. Many passenger transport\nservice providers, such as railways and airlines, control the sale of tickets\nthrough revenue management. State-of-the-art systems in these industries rely\non analyst expertise to identify outlier demand both online (within the booking\nhorizon) and offline (in hindsight). So far, little research focuses on\nautomating and evaluating the detection of outlier demand in this context. To\nremedy this, we propose a novel approach, which detects outliers using\nfunctional data analysis in combination with time series extrapolation. We\nevaluate the approach in a simulation framework, which generates outliers by\nvarying the demand model. The results show that functional outlier detection\nyields better detection rates than alternative approaches for both online and\noffline analyses. Depending on the category of outliers, extrapolation further\nincreases online detection performance. We also apply the procedure to a set of\nempirical data to demonstrate its practical implications. By evaluating the\nfull feedback-driven system of forecast and optimisation, we generate insight\non the asymmetric effects of positive and negative demand outliers. We show\nthat identifying instances of outlier demand and adjusting the forecast in a\ntimely fashion substantially increases revenue compared to what is earned when\nignoring outliers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:20:10 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:14:14 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 14:48:36 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Rennie", "Nicola", ""], ["Cleophas", "Catherine", ""], ["Sykulski", "Adam M.", ""], ["Dost", "Florian", ""]]}, {"id": "1912.06030", "submitter": "Paramita Chakraborty", "authors": "Paramita Chakraborty, Chong Ma, John Grego and James Lynch", "title": "Exploratory data analysis for large-scale multiple testing problems and\n  its application in gene expression studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale multiple testing problems, a two-class empirical Bayes\napproach can be used to control the false discovery rate (Fdr) for the entire\narray of hypotheses under study. A sample splitting step is incorporated to\nmodify that approach where one part of the data is used for model fitting and\nthe other part for detecting the significant cases by a screening technique\nfeaturing the empirical Bayes mode of Fdr control. Cases with high detection\nfrequency across repeated random sample splits are considered true discoveries.\nA critical detection frequency is set to control the overall false discovery\nrate. The proposed method helps to balance out unwanted sources of variation\nand addresses potential statistical overfitting of the core empirical model by\ncross-validation through resampling. Further, concurrent detection frequencies\nare used to provide visual tools to explore the inter-relationship between\nsignificant cases. The methodology is illustrated using a microarray data set,\nRNA-sequencing data set, and several simulation studies. A power analysis is\npresented to understand the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:27:48 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chakraborty", "Paramita", ""], ["Ma", "Chong", ""], ["Grego", "John", ""], ["Lynch", "James", ""]]}, {"id": "1912.06049", "submitter": "Maurizio Daniele", "authors": "Maurizio Daniele, Julie Schnaitmann", "title": "A Regularized Factor-augmented Vector Autoregressive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a regularized factor-augmented vector autoregressive (FAVAR) model\nthat allows for sparsity in the factor loadings. In this framework, factors may\nonly load on a subset of variables which simplifies the factor identification\nand their economic interpretation. We identify the factors in a data-driven\nmanner without imposing specific relations between the unobserved factors and\nthe underlying time series. Using our approach, the effects of structural\nshocks can be investigated on economically meaningful factors and on all\nobserved time series included in the FAVAR model. We prove consistency for the\nestimators of the factor loadings, the covariance matrix of the idiosyncratic\ncomponent, the factors, as well as the autoregressive parameters in the dynamic\nmodel. In an empirical application, we investigate the effects of a monetary\npolicy shock on a broad range of economically relevant variables. We identify\nthis shock using a joint identification of the factor model and the structural\ninnovations in the VAR model. We find impulse response functions which are in\nline with economic rationale, both on the factor aggregates and observed time\nseries level.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:54:24 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Daniele", "Maurizio", ""], ["Schnaitmann", "Julie", ""]]}, {"id": "1912.06092", "submitter": "Yoann Altmann", "authors": "Quentin Legros and Sylvain Meignen and Stephen McLaughlin and Yoann\n  Altmann", "title": "EM-based approach to 3D reconstruction from single-waveform\n  multispectral Lidar data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel Bayesian approach for estimating spectral\nand range profiles from single-photon Lidar waveforms associated with single\nsurfaces in the photon-limited regime. In contrast to classical multispectral\nLidar signals, we consider a single Lidar waveform per pixel, whereby a single\ndetector is used to acquire information simultaneously at multiple wavelengths.\nA new observation model based on a mixture of distributions is developed. It\nrelates the unknown parameters of interest to the observed waveforms containing\ninformation from multiple wavelengths. Adopting a Bayesian approach, several\nprior models are investigated and a stochastic Expectation-Maximization\nalgorithm is proposed to estimate the spectral and depth profiles. The\nreconstruction performance and computational complexity of our approach are\nassessed, for different prior models, through a series of experiments using\nsynthetic and real data under different observation scenarios. The results\nobtained demonstrate a significant speed-up without significant degradation of\nthe reconstruction performance when compared to existing methods in the\nphoton-starved regime.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:36:19 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Legros", "Quentin", ""], ["Meignen", "Sylvain", ""], ["McLaughlin", "Stephen", ""], ["Altmann", "Yoann", ""]]}, {"id": "1912.06313", "submitter": "Ashwini Venkatasubramaniam", "authors": "Ashwini Venkatasubramaniam, Brandon Koch, Lauren Erickson, Simone\n  French, David Vock and Julian Wolfson", "title": "Assessing effect heterogeneity of a randomized treatment using\n  conditional inference trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effect heterogeneity occurs when individual characteristics\ninfluence the effect of a treatment. We propose a novel approach that combines\nprognostic score matching and conditional inference trees to characterize\neffect heterogeneity of a randomized binary treatment. One key feature that\ndistinguishes our method from alternative approaches is that it controls the\nType I error rate, i.e., the probability of identifying effect heterogeneity if\nnone exists and retains the underlying subgroups. This feature makes our\ntechnique particularly appealing in the context of clinical trials, where there\nmay be significant costs associated with erroneously declaring that effects\ndiffer across population subgroups. TEHTrees are able to identify heterogeneous\nsubgroups, characterize the relevant subgroups and estimate the associated\ntreatment effects. We demonstrate the efficacy of the proposed method using a\ncomprehensive simulation study and illustrate our method using a nutrition\ntrial dataset to evaluate effect heterogeneity within a patient population.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:48:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 17:29:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Venkatasubramaniam", "Ashwini", ""], ["Koch", "Brandon", ""], ["Erickson", "Lauren", ""], ["French", "Simone", ""], ["Vock", "David", ""], ["Wolfson", "Julian", ""]]}, {"id": "1912.06398", "submitter": "Zhuozhao Zhan", "authors": "Zhuozhao Zhan, Vasan S. Ramachandran, Edwin R. van den Heuvel", "title": "Joint modeling with time-dependent treatment and heteroskedasticity:\n  Bayesian analysis with application to the Framingham Heart Study", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical studies for chronic disease are often interested in the relation\nbetween longitudinal risk factor profiles and individuals' later life disease\noutcomes. These profiles may typically be subject to intermediate structural\nchanges due to treatment or environmental influences. Analysis of such studies\nmay be handled by the joint model framework. However, current joint modeling\ndoes not consider structural changes in the residual variability of the risk\nprofile nor consider the influence of subject-specific residual variability on\nthe time-to-event outcome. In the present paper, we extend the joint model\nframework to address these two heterogeneous intra-individual variabilities. A\nBayesian approach is used to estimate the unknown parameters and simulation\nstudies are conducted to investigate the performance of the method. The\nproposed joint model is applied to the Framingham Heart Study to investigate\nthe influence of anti-hypertensive medication on the systolic blood pressure\nvariability together with its effect on the risk of developing cardiovascular\ndisease. We show that anti-hypertensive medication is associated with elevated\nsystolic blood pressure variability and increased variability elevates risk of\ndeveloping cardiovascular disease.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:36:29 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 15:10:13 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Zhan", "Zhuozhao", ""], ["Ramachandran", "Vasan S.", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "1912.06667", "submitter": "Naim Rashid", "authors": "Naim U. Rashid, Daniel J. Luckett, Jingxiang Chen, Michael T. Lawson,\n  Longshaokan Wang, Yunshu Zhang, Eric B. Laber, Yufeng Liu, Jen Jen Yeh,\n  Donglin Zeng, Michael R. Kosorok", "title": "High dimensional precision medicine from patient-derived xenografts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of human cancer often results in significant heterogeneity in\nresponse to treatment. Precision medicine offers potential to improve patient\noutcomes by leveraging this heterogeneity. Individualized treatment rules\n(ITRs) formalize precision medicine as maps from the patient covariate space\ninto the space of allowable treatments. The optimal ITR is that which maximizes\nthe mean of a clinical outcome in a population of interest. Patient-derived\nxenograft (PDX) studies permit the evaluation of multiple treatments within a\nsingle tumor and thus are ideally suited for estimating optimal ITRs. PDX data\nare characterized by correlated outcomes, a high-dimensional feature space, and\na large number of treatments. Existing methods for estimating optimal ITRs do\nnot take advantage of the unique structure of PDX data or handle the associated\nchallenges well. In this paper, we explore machine learning methods for\nestimating optimal ITRs from PDX data. We analyze data from a large PDX study\nto identify biomarkers that are informative for developing personalized\ntreatment recommendations in multiple cancers. We estimate optimal ITRs using\nregression-based approaches such as Q-learning and direct search methods such\nas outcome weighted learning. Finally, we implement a superlearner approach to\ncombine a set of estimated ITRs and show that the resulting ITR performs better\nthan any of the input ITRs, mitigating uncertainty regarding user choice of any\nparticular ITR estimation methodology. Our results indicate that PDX data are a\nvaluable resource for developing individualized treatment strategies in\noncology.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:17:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Rashid", "Naim U.", ""], ["Luckett", "Daniel J.", ""], ["Chen", "Jingxiang", ""], ["Lawson", "Michael T.", ""], ["Wang", "Longshaokan", ""], ["Zhang", "Yunshu", ""], ["Laber", "Eric B.", ""], ["Liu", "Yufeng", ""], ["Yeh", "Jen Jen", ""], ["Zeng", "Donglin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.06804", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "The UEFA Champions League seeding is not strategy-proof since the\n  2015/16 season", "comments": "11 pages, 1 figure, 1 table", "journal-ref": "Annals of Operations Research, 292(1): 161-169, 2020", "doi": "10.1007/s10479-020-03637-1", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness has several interpretations in sports, one of them being that the\nrules should guarantee incentive compatibility, namely, a team cannot be worse\noff due to better results in any feasible scenario. The current seeding regime\nof the most prestigious annual European club football tournament, the UEFA\n(Union of European Football Associations) Champions League, is shown to violate\nthis requirement since the 2015/16 season. In particular, if the titleholder\nqualifies for the first pot by being a champion in a high-ranked league, its\nslot is given to a team from a lower-ranked association, which can harm a top\nclub from the domestic championship of the titleholder. However, filling all\nvacancies through the national leagues excludes the presence of perverse\nincentives. UEFA is encouraged to introduce this policy from the 2021-24 cycle\nonwards.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:23:35 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 16:20:02 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 14:53:52 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1912.06830", "submitter": "Somayeh Aghashahi", "authors": "Somayeh Aghashahi, Samaneh Aghashahi, Zolfa Zeinalpour-Yazdi, Aliakbar\n  Tadaion, Arash Asadi", "title": "Stochastic Modeling of Beam Management in mmWave Vehicular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility management is a major challenge for the wide-spread deployment of\nmillimeter-wave (mmWave) cellular networks. In particular, directional\nbeamforming in mmWave devices renders high-speed mobility support very complex.\nThis complexity, however, is not limited to system design but also the\nperformance estimation and evaluation. Hence, some have turned their attention\nto stochastic modeling of mmWave vehicular communication to derive closed-form\nexpressions characterizing the coverage and rate behavior of the network. In\nthis article, we model and analyze the beam management for mmWave vehicular\nnetworks. To the best of our knowledge, this is the first work that goes beyond\ncoverage and rate analysis. Specifically, we focus on a multi-lane divided\nhighway scenario in which base stations and vehicles are present on both sides\nof the highway. In addition to providing analytical expressions for the average\nnumber of beam switching and handover events, we provide design insights for\nthe network operators to fine-tune their network within the flexibility\nprovided by the standard in the choice of system parameters, including the\nnumber of resources dedicated to channel feedback and beam alignment\noperations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 12:09:46 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Aghashahi", "Somayeh", ""], ["Aghashahi", "Samaneh", ""], ["Zeinalpour-Yazdi", "Zolfa", ""], ["Tadaion", "Aliakbar", ""], ["Asadi", "Arash", ""]]}, {"id": "1912.06946", "submitter": "Jennifer Starling", "authors": "Jennifer E. Starling, Catherine E. Aiken, Jared S. Murray, Annettee\n  Nakimuli, James G. Scott", "title": "Monotone function estimation in the presence of extreme data coarsening:\n  Analysis of preeclampsia and birth weight in urban Uganda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Bayesian hierarchical model to characterize the\nrelationship between birth weight and maternal pre-eclampsia across gestation\nat a large maternity hospital in urban Uganda. Key scientific questions we\ninvestigate include: 1) how pre-eclampsia compares to other maternal-fetal\ncovariates as a predictor of birth weight; and 2) whether the impact of\npre-eclampsia on birthweight varies across gestation. Our model addresses\nseveral key statistical challenges: it correctly encodes the prior medical\nknowledge that birth weight should vary smoothly and monotonically with\ngestational age, yet it also avoids assumptions about functional form along\nwith assumptions about how birth weight varies with other covariates. Our model\nalso accounts for the fact that a high proportion (83%) of birth weights in our\ndata set are rounded to the nearest 100 grams. Such extreme data coarsening is\nrare in maternity hospitals in high resource obstetrics settings but common for\ndata sets collected in low and middle-income countries (LMICs); this introduces\na substantial extra layer of uncertainty into the problem and is a major reason\nwhy we adopt a Bayesian approach.\n  Our proposed non-parametric regression model, which we call Projective Smooth\nBART (psBART), builds upon the highly successful Bayesian Additive Regression\nTree (BART) framework. This model captures complex nonlinear relationships and\ninteractions, induces smoothness and monotonicity in a single target covariate,\nand provides a full posterior for uncertainty quantification. The results of\nour analysis show that pre-eclampsia is a dominant predictor of birth weight in\nthis urban Ugandan setting, and therefore an important risk factor for\nperinatal mortality.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 23:15:06 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Starling", "Jennifer E.", ""], ["Aiken", "Catherine E.", ""], ["Murray", "Jared S.", ""], ["Nakimuli", "Annettee", ""], ["Scott", "James G.", ""]]}, {"id": "1912.06995", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "On function-on-function regression: Partial least squares approach", "comments": "24 pages, 7 figures", "journal-ref": "Environmental and Ecological Statistics, 2020, 27, 95-114", "doi": "10.1007/s10651-019-00436-1", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis tools, such as function-on-function regression\nmodels, have received considerable attention in various scientific fields\nbecause of their observed high-dimensional and complex data structures. Several\nstatistical procedures, including least squares, maximum likelihood, and\nmaximum penalized likelihood, have been proposed to estimate such\nfunction-on-function regression models. However, these estimation techniques\nproduce unstable estimates in the case of degenerate functional data or are\ncomputationally intensive. To overcome these issues, we proposed a partial\nleast squares approach to estimate the model parameters in the\nfunction-on-function regression model. In the proposed method, the B-spline\nbasis functions are utilized to convert discretely observed data into their\nfunctional forms. Generalized cross-validation is used to control the degrees\nof roughness. The finite-sample performance of the proposed method was\nevaluated using several Monte-Carlo simulations and an empirical data analysis.\nThe results reveal that the proposed method competes favorably with existing\nestimation techniques and some other available function-on-function regression\nmodels, with significantly shorter computational time.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 07:21:11 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "1912.07099", "submitter": "Ian Laga", "authors": "Ian Laga, Xiaoyue Niu, Le Bao", "title": "Modeling the Marked Presence-only Data: A Case Study of Estimating the\n  Female Sex Worker Size in Malawi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain subpopulations like female sex workers (FSW), men who have sex with\nmen (MSM), and people who inject drugs (PWID) often have higher prevalence of\nHIV/AIDS and are difficult to map directly due to stigma, discrimination, and\ncriminalization. Fine-scale mapping of those populations contributes to the\nprogress towards reducing the inequalities and ending the AIDS epidemic. In\n2016 and 2017, the PLACE surveys were conducted at 3,290 venues in 20 out of\nthe total 28 districts in Malawi to estimate the FSW sizes. These venues\nrepresent a presence-only data set where, instead of knowing both where people\nlive and do not live (presence-absence data), only information about visited\nlocations is available. In this study, we develop a Bayesian model for\npresence-only data and utilize the PLACE data to estimate the FSW size and\nuncertainty interval at a $1.5 \\times 1.5$-km resolution for all of Malawi. The\nestimates can also be aggregated to any desirable level (city/district/region)\nfor implementing targeted HIV prevention and treatment programs in FSW\ncommunities, which have been successful in lowering the incidence of HIV and\nother sexually transmitted infections.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:54:00 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:19:59 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 15:37:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Laga", "Ian", ""], ["Niu", "Xiaoyue", ""], ["Bao", "Le", ""]]}, {"id": "1912.07123", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rinc\\'on, Valeria Muro, Carlos D'Giano, Jorge\n  Prendes, Hadj Batatia", "title": "A novel spike-and-wave automatic detection in EEG signals", "comments": "8 pages, 3 figures", "journal-ref": "Computers (MDPI AG) 2020", "doi": "10.3390/computers9040085", "report-no": "computers9040085", "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-and-wave discharge (SWD) pattern classification in\nelectroencephalography (EEG) signals is a key problem in signal processing. It\nis particularly important to develop a SWD automatic detection method in\nlong-term EEG recordings since the task of marking the patters manually is time\nconsuming, difficult and error-prone. This paper presents a new detection\nmethod with a low computational complexity that can be easily trained if\nstandard medical protocols are respected. The detection procedure is as\nfollows: First, each EEG signal is divided into several time segments and for\neach time segment, the Morlet 1-D decomposition is applied. Then three\nparameters are extracted from the wavelet coefficients of each segment: scale\n(using a generalized Gaussian statistical model), variance and median. This is\nfollowed by a k-nearest neighbors (k-NN) classifier to detect the\nspike-and-wave pattern in each EEG channel from these three parameters. A total\nof 106 spike-and-wave and 106 non-spike-and-wave were used for training, while\n69 new annotated EEG segments from six subjects were used for classification.\nIn these circumstances, the proposed methodology achieved 100% accuracy. These\nresults generate new research opportunities for the underlying causes of the\nso-called absence epilepsy in long-term EEG recordings.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:42:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Quintero-Rinc\u00f3n", "Antonio", ""], ["Muro", "Valeria", ""], ["D'Giano", "Carlos", ""], ["Prendes", "Jorge", ""], ["Batatia", "Hadj", ""]]}, {"id": "1912.07137", "submitter": "Meng Xu", "authors": "Meng Xu, Philip T. Reiss, Ivor Cribben", "title": "Generalized reliability based on distances", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13287", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intraclass correlation coefficient (ICC) is a classical index of\nmeasurement reliability. With the advent of new and complex types of data for\nwhich the ICC is not defined, there is a need for new ways to assess\nreliability. To meet this need, we propose a new distance-based intraclass\ncorrelation coefficient (dbICC), defined in terms of arbitrary distances among\nobservations. We introduce a bias correction to improve the coverage of\nbootstrap confidence intervals for the dbICC, and demonstrate its efficacy via\nsimulation. We illustrate the proposed method by analyzing the test-retest\nreliability of brain connectivity matrices derived from a set of repeated\nfunctional magnetic resonance imaging scans. The Spearman-Brown formula, which\nshows how more intensive measurement increases reliability, is extended to\nencompass the dbICC.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 23:48:14 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 19:49:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Xu", "Meng", ""], ["Reiss", "Philip T.", ""], ["Cribben", "Ivor", ""]]}, {"id": "1912.07211", "submitter": "Yukun Zhang", "authors": "Yukun Zhang, Longsheng Zhou", "title": "Fairness Assessment for Artificial Intelligence in Financial Industry", "comments": "Robust AI in FS 2019 : NeurIPS 2019 Workshop on Robust AI in\n  Financial Services: Data, Fairness, Explainability, Trustworthiness, and\n  Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) is an important driving force for the\ndevelopment and transformation of the financial industry. However, with the\nfast-evolving AI technology and application, unintentional bias, insufficient\nmodel validation, immature contingency plan and other underestimated threats\nmay expose the company to operational and reputational risks. In this paper, we\nfocus on fairness evaluation, one of the key components of AI Governance,\nthrough a quantitative lens. Statistical methods are reviewed for imbalanced\ndata treatment and bias mitigation. These methods and fairness evaluation\nmetrics are then applied to a credit card default payment example.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 06:09:39 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhang", "Yukun", ""], ["Zhou", "Longsheng", ""]]}, {"id": "1912.07287", "submitter": "Oluwasegun Ojo", "authors": "Oluwasegun Taiwo Ojo, Antonio Fern\\'andez Anta, Rosa E. Lillo, Carlo\n  Sguera", "title": "Detecting and Classifying Outliers in Big Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose two new outlier detection methods, for identifying and classifying\ndifferent types of outliers in (big) functional data sets. The proposed methods\nare based on an existing method called Massive Unsupervised Outlier Detection\n(MUOD). MUOD detects and classifies outliers by computing for each curve, three\nindices, all based on the concept of linear regression and correlation, which\nmeasure outlyingness in terms of shape, magnitude and amplitude, relative to\nthe other curves in the data. 'Semifast-MUOD', the first method, uses a sample\nof the observations in computing the indices, while 'Fast-MUOD', the second\nmethod, uses the point-wise or L1 median in computing the indices. The\nclassical boxplot is used to separate the indices of the outliers from those of\nthe typical observations. Performance evaluation of the proposed methods using\nsimulated data show significant improvements compared to MUOD, both in outlier\ndetection and computational time. We show that Fast-MUOD is especially well\nsuited to handling big and dense functional datasets with very small\ncomputational time compared to other methods. Further comparisons with some\nrecent outlier detection methods for functional data also show superior or\ncomparable outlier detection accuracy of the proposed methods. We apply the\nproposed methods on weather, population growth, and video data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 10:52:27 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 18:26:55 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ojo", "Oluwasegun Taiwo", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Lillo", "Rosa E.", ""], ["Sguera", "Carlo", ""]]}, {"id": "1912.07310", "submitter": "Anne Presanis", "authors": "Anne M Presanis, Peter Kirwan, Ada Miltz, Sara Croxford, Ross Harris,\n  Ellen Heinsbroek, Chris Jackson, Hamish Mohammed, Alison Brown, Valerie\n  Delpech, O Noel Gill, and Daniela De Angelis", "title": "Implications for HIV elimination by 2030 of recent trends in undiagnosed\n  infection in England: an evidence synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A target to eliminate Human Immuno-deficiency Virus (HIV) transmission in\nEngland by 2030 was set in early 2019. Estimates of recent trends in HIV\nprevalence, particularly the number of people living with undiagnosed HIV, by\nexposure group, ethnicity, gender, age group and region, are essential to\nmonitor progress towards elimination. A Bayesian synthesis of evidence from\nmultiple surveillance, demographic and survey datasets relevant to HIV in\nEngland is employed to estimate trends in: the number of people living with HIV\n(PLWH); the proportion of these people unaware of their HIV infection; and the\ncorresponding prevalence of undiagnosed HIV. All estimates are stratified by\nexposure group, ethnicity, gender, age group (15-34, 35-44, 45-59, 60-74),\nregion (London, outside London) and year (2012-2017). The total number of PLWH\naged 15-74 in England increased from 82,400 (95% credible interval, CrI, 78,700\nto 89,100) in 2012 to 89,500 (95% CrI 87,400 to 93,300) in 2017. The proportion\ndiagnosed steadily increased from 84% (95% CrI 77 to 88%) to 92% (95% CrI 89 to\n94%) over the same time period, corresponding to a halving in the number of\nundiagnosed infections from 13,500 (95% CrI 9,800 to 20,200) to 6,900 (95% CrI\n4,900 to 10,700). This decrease is equivalent to a halving in prevalence of\nundiagnosed infection and is reflected in all sub-groups of gay, bisexual and\nother men who have sex with men and most sub-groups of black African\nheterosexuals. However, decreases were not detected for some sub-groups of\nother ethnicity heterosexuals, particularly outside London. In 2016, the Joint\nUnited Nations Programme on HIV/ AIDS target of diagnosing 90% of people living\nwith HIV was reached in England. To achieve HIV elimination by 2030, current\ntesting efforts should be enhanced to address the numbers of heterosexuals\nliving with undiagnosed HIV, especially outside London.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 12:02:45 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Presanis", "Anne M", ""], ["Kirwan", "Peter", ""], ["Miltz", "Ada", ""], ["Croxford", "Sara", ""], ["Harris", "Ross", ""], ["Heinsbroek", "Ellen", ""], ["Jackson", "Chris", ""], ["Mohammed", "Hamish", ""], ["Brown", "Alison", ""], ["Delpech", "Valerie", ""], ["Gill", "O Noel", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1912.07359", "submitter": "Michele Zemplenyi", "authors": "Michele Zemplenyi (1), Mark J. Meyer (2), Andres Cardenas (3),\n  Marie-France Hivert (4 and 9), Sheryl L. Rifas-Shiman (4), Heike Gibson (5),\n  Itai Kloog (6), Joel Schwartz (5 and 8), Emily Oken (4), Dawn L. DeMeo (7),\n  Diane R. Gold (5 and 8) and Brent A. Coull ((1) Department of Biostatistics,\n  Harvard T.H. Chan School of Public Health, (2) Department of Mathematics and\n  Statistics, Georgetown University, (3) Division of Environmental Health\n  Sciences, School of Public Health, University of California, Berkeley, (4)\n  Division of Chronic Disease Research Across the Lifecourse, Department of\n  Population Medicine, Harvard Medical School and Harvard Pilgrim Health Care\n  Institute, (5) Department of Environmental Health, Harvard T.H. Chan School\n  of Public Health, (6) Department of Geography and Environmental Development,\n  Faculty of Humanities and Social Sciences, Ben-Gurion University, (7) Center\n  for Chest Diseases, Brigham and Women's Hospital, (8) Channing Division of\n  Network Medicine, Department of Medicine, Brigham and Women's Hospital,\n  Harvard Medical School, (9) Diabetes Unit, Massachusetts General Hospital)", "title": "Function-on-Function Regression for the Identification of Epigenetic\n  Regions Exhibiting Windows of Susceptibility to Environmental Exposures", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to identify time periods when individuals are most susceptible to\nexposures, as well as the biological mechanisms through which these exposures\nact, is of great public health interest. Growing evidence supports an\nassociation between prenatal exposure to air pollution and epigenetic marks,\nsuch as DNA methylation, but the timing and gene-specific effects of these\nepigenetic changes are not well understood. Here, we present the first study\nthat aims to identify prenatal windows of susceptibility to air pollution\nexposures in cord blood DNA methylation. In particular, we propose a\nfunction-on-function regression model that leverages data from nearby DNA\nmethylation probes to identify epigenetic regions that exhibit windows of\nsusceptibility to ambient particulate matter less than 2.5 microns\n(PM$_{2.5}$). By incorporating the covariance structure among both the\nmultivariate DNA methylation outcome and the time-varying exposure under study,\nthis framework yields greater power to detect windows of susceptibility and\ngreater control of false discoveries than methods that model probes\nindependently. We compare our method to a distributed lag model approach that\nmodels DNA methylation in a probe-by-probe manner, both in simulation and by\napplication to motivating data from the Project Viva birth cohort. In two\nepigenetic regions selected based on prior studies of air pollution effects on\nepigenome-wide methylation, we identify windows of susceptibility to PM$_{2.5}$\nexposure near the beginning and middle of the third trimester of pregnancy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:00:17 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zemplenyi", "Michele", "", "4 and 9"], ["Meyer", "Mark J.", "", "4 and 9"], ["Cardenas", "Andres", "", "4 and 9"], ["Hivert", "Marie-France", "", "4 and 9"], ["Rifas-Shiman", "Sheryl L.", "", "5 and 8"], ["Gibson", "Heike", "", "5 and 8"], ["Kloog", "Itai", "", "5 and 8"], ["Schwartz", "Joel", "", "5 and 8"], ["Oken", "Emily", "", "5 and 8"], ["DeMeo", "Dawn L.", "", "5 and 8"], ["Gold", "Diane R.", "", "5 and 8"], ["Coull", "Brent A.", ""]]}, {"id": "1912.07364", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Hans Van Eetvelde and Christophe Ley and\n  Ulf Brefeld", "title": "Evaluating one-shot tournament predictions", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Tournament Rank Probability Score (TRPS) as a measure to\nevaluate and compare pre-tournament predictions, where predictions of the full\ntournament results are required to be available before the tournament begins.\nThe TRPS handles partial ranking of teams, gives credit to predictions that are\nonly slightly wrong, and can be modified with weights to stress the importance\nof particular features of the tournament prediction. Thus, the Tournament Rank\nPrediction Score is more flexible than the commonly preferred log loss score\nfor such tasks. In addition, we show how predictions from historic tournaments\ncan be optimally combined into ensemble predictions in order to maximize the\nTRPS for a new tournament.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:38:40 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", ""], ["Van Eetvelde", "Hans", ""], ["Ley", "Christophe", ""], ["Brefeld", "Ulf", ""]]}, {"id": "1912.07367", "submitter": "Haolin Fei", "authors": "Haolin Fei and Xiaofeng Wu and Chunbo Luo", "title": "A Model-driven and Data-driven Fusion Framework for Accurate Air Quality\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air quality is closely related to public health. Health issues such as\ncardiovascular diseases and respiratory diseases, may have connection with long\nexposure to highly polluted environment. Therefore, accurate air quality\nforecasts are extremely important to those who are vulnerable. To estimate the\nvariation of several air pollution concentrations, previous researchers used\nvarious approaches, such as the Community Multiscale Air Quality model (CMAQ)\nor neural networks. Although CMAQ model considers a coverage of the historic\nair pollution data and meteorological variables, extra bias is introduced due\nto additional adjustment. In this paper, a combination of model-based strategy\nand data-driven method namely the physical-temporal collection(PTC) model is\nproposed, aiming to fix the systematic error that traditional models deliver.\nIn the data-driven part, the first components are the temporal pattern and the\nweather pattern to measure important features that contribute to the prediction\nperformance. The less relevant input variables will be removed to eliminate\nnegative weights in network training. Then, we deploy a long-short-term-memory\n(LSTM) to fetch the preliminary results, which will be further corrected by a\nneural network (NN) involving the meteorological index as well as other\npollutants concentrations. The data-set we applied for forecasting is from\nJanuary 1st, 2016 to December 31st, 2016. According to the results, our PTC\nachieves an excellent performance compared with the baseline model (CMAQ\nprediction, GRU, DNN and etc.). This joint model-based data-driven method for\nair quality prediction can be easily deployed on stations without extra\nadjustment, providing results with high-time-resolution information for\nvulnerable members to prevent heavy air pollution ahead.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 08:24:11 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Fei", "Haolin", ""], ["Wu", "Xiaofeng", ""], ["Luo", "Chunbo", ""]]}, {"id": "1912.07373", "submitter": "Olivier Gallay PhD", "authors": "Marc-Olivier Boldi, Val\\'erie Chavez-Demoulin, Olivier Gallay", "title": "Intraday Retail Sales Forecast: An Efficient Algorithm for Quantile\n  Additive Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever increasing prominence of data in retail operations, sales\nforecasting has become an essential pillar in the efficient management of\ninventories. When facing high demand, the use of backroom storage and intraday\nshelf replenishment is necessary to avoid stock-out. In that context, the\nmandatory input for any successful replenishment policy to be implemented is\naccess to reliable forecasts for the sales at an intraday granularity. To that\nend, we use quantile regression to adapt different patterns from one product to\nthe other, and we develop a stable and efficient quantile additive model\nalgorithm to compute sales forecasts in an intradaily context. Our algorithm is\ncomputationally fast and is therefore suitable for use in real-time dynamic\nshelf replenishment. As an illustration, we examine the case of a highly\nfrequented store, where the demand for various alimentary products is\naccurately estimated over the day with the help of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:23:09 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Boldi", "Marc-Olivier", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""], ["Gallay", "Olivier", ""]]}, {"id": "1912.07384", "submitter": "Joscha Reimer", "authors": "Joscha Reimer", "title": "Statistical Analysis of the Phosphate Data of the World Ocean Database\n  2013", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phosphate data of the World Ocean Database 2013 are extensively\nstatistically analyzed by splitting the measurement results into a long scale,\ni.e., climatological, and a short scale part. Means, medians, absolute and\nrelative standard deviations, interquartile ranges, quartile coefficients of\ndispersion, correlations and covariances are estimated and analyzed. The\nunderlying probability distributions are investigated using visual inspection\nas well as statistical tests. All presented methods are applicable to other\ndata as long as they satisfy the postulated assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:04:42 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Reimer", "Joscha", ""]]}, {"id": "1912.07403", "submitter": "Juergen Lerner", "authors": "J\\\"urgen Lerner, Mark Tranmer, John Mowbray and Marian-Gabriel Hancean", "title": "REM beyond dyads: relational hyperevent models for multi-actor\n  interaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce relational hyperevent models (RHEM) as a generalization of\nrelational event models to events occurring on hyperedges involving any number\nof actors. RHEM can specify time-varying event rates for the full space of\ndirected or undirected hyperedges and can be applied to model, among others,\nmeetings, team assembly, team performance, or multi-actor communication. We\nillustrate the newly proposed model on two empirical hyperevent networks about\nmeetings of government ministers and co-authoring of scientific papers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:31:54 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lerner", "J\u00fcrgen", ""], ["Tranmer", "Mark", ""], ["Mowbray", "John", ""], ["Hancean", "Marian-Gabriel", ""]]}, {"id": "1912.07412", "submitter": "Joscha Reimer", "authors": "Joscha Reimer", "title": "Optimization of Model Parameters, Uncertainty Quantification and\n  Experimental Designs for a Global Marine Biogeochemical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for model parameter estimation, uncertainty quantification and\nexperimental design are summarized in this paper. They are based on the\ngeneralized least squares estimator and different approximations of its\ncovariance matrix using the first and second derivative of the model regarding\nits parameters. The methods have been applied to a model for phosphate and\ndissolved organic phosphorus concentrations in the global ocean. As a result,\nmodel parameters have been determined which considerably improved the\nconsistency of the model with measurement results. The uncertainties regarding\nthe estimated model parameters caused by uncertainties in the measurement\nresults have been quantified as well as the uncertainties associated with the\ncorresponding model output implied by the uncertainty in the model parameters.\nThis allows to better assess the model parameters as well as the model output.\nFurthermore, it has been determined to what extent new measurements can reduce\nthese uncertainties. For this, the information content of new measurements has\nbeen predicted depending on the measured process as well as the time and the\nlocation of the measurement. This is very useful for planning new measurements.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 18:11:12 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Reimer", "Joscha", ""]]}, {"id": "1912.07442", "submitter": "Samuel Henry", "authors": "Samuel Henry", "title": "Time-based analysis of the NBA hot hand fallacy", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debate surrounding the hot hand in the NBA has been ongoing for many\nyears. However, many of the previous works on this theme has focused on only\nthe very next sequential shot attempt, often on very select players. This work\nlooks in more detail the effect of a made or missed shot on the next series of\nshots over a two-year span, with time between shots shown to be a critical\nfactor in the analysis. Also, multi-year streakiness is analyzed, and all\nindications are that players cannot really sustain their good (or bad) fortune\nfrom year to year.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:53:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Henry", "Samuel", ""]]}, {"id": "1912.07466", "submitter": "Karl Schurter", "authors": "Joris Pinkse and Karl Schurter", "title": "Estimation of Auction Models with Shape Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several new estimation methods that leverage shape constraints\nin auction models to estimate various objects of interest, including the\ndistribution of a bidder's valuations, the bidder's ex ante expected surplus,\nand the seller's counterfactual revenue. The basic approach applies broadly in\nthat (unlike most of the literature) it works for a wide range of auction\nformats and allows for asymmetric bidders. Though our approach is not\nrestrictive, we focus our analysis on first--price, sealed--bid auctions with\nindependent private valuations. We highlight two nonparametric estimation\nstrategies, one based on a least squares criterion and the other on a maximum\nlikelihood criterion. We also provide the first direct estimator of the\nstrategy function. We establish several theoretical properties of our methods\nto guide empirical analysis and inference. In addition to providing the\nasymptotic distributions of our estimators, we identify ways in which\nmethodological choices should be tailored to the objects of their interest. For\nobjects like the bidders' ex ante surplus and the seller's counterfactual\nexpected revenue with an additional symmetric bidder, we show that our\ninput--parameter--free estimators achieve the semiparametric efficiency bound.\nFor objects like the bidders' inverse strategy function, we provide an easily\nimplementable boundary--corrected kernel smoothing and transformation method in\norder to ensure the squared error is integrable over the entire support of the\nvaluations. An extensive simulation study illustrates our analytical results\nand demonstrates the respective advantages of our least--squares and maximum\nlikelihood estimators in finite samples. Compared to estimation strategies\nbased on kernel density estimation, the simulations indicate that the smoothed\nversions of our estimators enjoy a large degree of robustness to the choice of\nan input parameter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:01:16 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pinkse", "Joris", ""], ["Schurter", "Karl", ""]]}, {"id": "1912.07753", "submitter": "Xiaojing Wang", "authors": "Xiaojing Wang, Tianqi Liu, Jingang Miao", "title": "A Deep Probabilistic Model for Customer Lifetime Value Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate predictions of customers' future lifetime value (LTV) given their\nattributes and past purchase behavior enables a more customer-centric marketing\nstrategy. Marketers can segment customers into various buckets based on the\npredicted LTV and, in turn, customize marketing messages or advertising copies\nto serve customers in different segments better. Furthermore, LTV predictions\ncan directly inform marketing budget allocations and improve real-time\ntargeting and bidding of ad impressions.\n  One challenge of LTV modeling is that some customers never come back, and the\ndistribution of LTV can be heavy-tailed. The commonly used mean squared error\n(MSE) loss does not accommodate the significant fraction of zero value LTV from\none-time purchasers and can be sensitive to extremely large LTV's from top\nspenders. In this article, we model the distribution of LTV given associated\nfeatures as a mixture of zero point mass and lognormal distribution, which we\nrefer to as the zero-inflated lognormal (ZILN) distribution. This modeling\napproach allows us to capture the churn probability and account for the\nheavy-tailedness nature of LTV at the same time. It also yields straightforward\nuncertainty quantification of the point prediction. The ZILN loss can be used\nin both linear models and deep neural networks (DNN). For model evaluation, we\nrecommend the normalized Gini coefficient to quantify model discrimination and\ndecile charts to assess model calibration. Empirically, we demonstrate the\npredictive performance of our proposed model on two real-world public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:23:48 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Wang", "Xiaojing", ""], ["Liu", "Tianqi", ""], ["Miao", "Jingang", ""]]}, {"id": "1912.08050", "submitter": "Xiangyu Luo", "authors": "Qiuyu Wu, Xiangyu Luo", "title": "Nonparametric Bayesian Two-Level Clustering for Subject-Level\n  Single-Cell Expression Data", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202020.0337", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of single-cell sequencing opens new avenues for personalized\ntreatment. In this paper, we address a two-level clustering problem of\nsimultaneous subject subgroup discovery (subject level) and cell type detection\n(cell level) for single-cell expression data from multiple subjects. However,\ncurrent statistical approaches either cluster cells without considering the\nsubject heterogeneity or group subjects without using the single-cell\ninformation. To bridge the gap between cell clustering and subject grouping, we\ndevelop a nonparametric Bayesian model, Subject and Cell clustering for\nSingle-Cell expression data (SCSC) model, to achieve subject and cell grouping\nsimultaneously. SCSC does not need to prespecify the subject subgroup number or\nthe cell type number. It automatically induces subject subgroup structures and\nmatches cell types across subjects. Moreover, it directly models the\nsingle-cell raw count data by deliberately considering the data's dropouts,\nlibrary sizes, and over-dispersion. A blocked Gibbs sampler is proposed for the\nposterior inference. Simulation studies and the application to a multi-subject\niPSC scRNA-seq dataset validate the ability of SCSC to simultaneously cluster\nsubjects and cells.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 14:45:31 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 04:03:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Qiuyu", ""], ["Luo", "Xiangyu", ""]]}, {"id": "1912.08149", "submitter": "Xuze Zhang", "authors": "Xuze Zhang, Saumyadipta Pyne and Benjamin Kedem", "title": "Estimation of Residential Radon Concentration in Pennsylvania Counties\n  by Data Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data fusion method for the estimation of residential radon level\ndistribution in any Pennsylvania county is proposed. The method is based on a\nmulti-sample density ratio model with variable tilts and is applied to combined\nradon data from a reference county of interest and its neighboring counties.\nBeaver county and its four immediate neighbors are taken as a case in point.\nThe distribution of radon concentration is estimated in each of six periods,\nand then the analysis is repeated combining the data from all the periods to\nobtain estimates of Beaver threshold probabilities and the corresponding\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:33:43 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhang", "Xuze", ""], ["Pyne", "Saumyadipta", ""], ["Kedem", "Benjamin", ""]]}, {"id": "1912.08272", "submitter": "Micha Mandel PhD", "authors": "Naomi Kaplan Damary (1), Micha Mandel (1), Yoram Yekutieli (2), Sarena\n  Wiesner (3), Yaron Shor (3) ((1) The Hebrew University of Jerusalem, (2)\n  Hadassah Academic College, (3) Israel National Police Division of\n  Identification and Forensic Science)", "title": "Spatial modeling of randomly acquired characteristics on outsoles with\n  application to forensic shoeprint analysis", "comments": "Main article: 23 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Footwear comparison is used to link between a suspect's shoe and a footprint\nfound at a crime scene. Investigators compare the two items using randomly\nacquired characteristics (RACs), such as scratches or holes. However, to date,\nthe distribution of RAC characteristics has not been investigated thoroughly,\nand the evidential value of RACs is yet to be explored. An important question\nconcerns the distribution of the location of RACs on shoe soles, which can\nserve as a benchmark for comparison. The location of RACs is modeled here as a\npoint process over the shoe sole and a data set of 386 independent shoes is\nused to estimate its rate function. The analysis is somewhat complicated as the\nshoes are differentiated by shape, level of wear and tear and contact surface.\nThis paper presents methods that take into account these challenges, either by\nusing natural cubic splines on high resolution data, or by using a\npiecewise-constant model on larger regions defined by experts' knowledge. It is\nshown that RACs are likely to appear at certain locations, corresponding to the\nfoot's morphology. The results can guide investigators in determining the\nevidential value of footprint comparison.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:07:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 08:22:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Damary", "Naomi Kaplan", ""], ["Mandel", "Micha", ""], ["Yekutieli", "Yoram", ""], ["Wiesner", "Sarena", ""], ["Shor", "Yaron", ""]]}, {"id": "1912.08337", "submitter": "Enrique del Castillo", "authors": "Enrique del Castillo and Rainer Goeb", "title": "A Bivariate Dead Band Process Adjustment Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate extension to Box and Jenkins (1963) feedback adjustment problem\nis presented in this paper. The model balances the fixed cost of making an\nadjustment, which is assumed independent of the magnitude of the adjustments,\nwith the cost of running the process off-target, which is assumed quadratic. It\nis also assumed that two controllable factors are available to compensate for\nthe deviations from target of two responses in the presence of a bivariate\nIMA(1,1) disturbance. The optimal policy has the form of a \"dead band\", in\nwhich adjustments are justified only when the predicted process responses\nexceed some boundary in $\\mathbb{R}^2$. This boundary indicates when the\nresponses are predicted to be far enough from their targets that an additional\nadjustment or intervention in the process is justified. Although originally\ndeveloped to control a machine tool, dead band control policies have\napplication in other areas. For example, they could be used to control a\ndisease through the application of a drug to a patient depending on the level\nof a substance in the body (e.g., diabetes control). This paper presents\nanalytical formulae for the computation of the loss function that combines\noff-target and adjustment costs per time unit. Expressions are derived for the\naverage adjustment interval and for the scaled mean square deviations from\ntarget. The minimization of the loss function and the practical use of the\nresulting dead band adjustment strategy is illustrated with an application to a\nsemiconductor manufacturing process.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:46:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["del Castillo", "Enrique", ""], ["Goeb", "Rainer", ""]]}, {"id": "1912.08359", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rincon, Carlos D'Giano, Hadj Batatia", "title": "A quadratic linear-parabolic model-based classification to detect\n  epileptic EEG seizures", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": "The Journal of Biomedical Research, 2020 34(3): 203-210", "doi": "10.7555/JBR.33.20190012", "report-no": null, "categories": "stat.AP eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The two-point central difference is a common algorithm in biological signal\nprocessing and is particularly useful in analyzing physiological signals. In\nthis paper, we develop a model-based classification method to detect epileptic\nseizures that relies on this algorithm to filter EEG signals. The underlying\nidea is to design an EEG filter that enhances the waveform of epileptic\nsignals. The filtered signal is fitted to a quadratic linear-parabolic model\nusing the curve fitting technique. The model fitting is assessed using four\nstatistical parameters, which are used as classification features with a random\nforest algorithm to discriminate seizure and non-seizure events. The proposed\nmethod was applied to 66 epochs from the Children Hospital Boston database.\nResults show that the method achieves fast and accurate detection of epileptic\nseizures, with a 92% sensitivity, 96% specificity, and 94.1% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:03:00 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Quintero-Rincon", "Antonio", ""], ["D'Giano", "Carlos", ""], ["Batatia", "Hadj", ""]]}, {"id": "1912.08400", "submitter": "Yu Xia", "authors": "Jiawei Long and Yu Xia", "title": "Cluster Analysis of High-Dimensional scRNA Sequencing Data", "comments": "9 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ongoing developments and innovations in single-cell RNA sequencing\nmethods, advancements in sequencing performance could empower significant\ndiscoveries as well as new emerging possibilities to address biological and\nmedical investigations. In the study, we will be using the dataset collected by\nthe authors of Systematic comparative analysis of single cell RNA-sequencing\nmethods. The dataset consists of single-cell and single nucleus profiling from\nthree types of samples - cell lines, peripheral blood mononuclear cells, and\nbrain tissue, which offers 36 libraries in six separate experiments in a single\ncenter. Our quantitative comparison aims to identify unique characteristics\nassociated with different single-cell sequencing methods, especially among\nlow-throughput sequencing methods and high-throughput sequencing methods. Our\nprocedures also incorporate evaluations of every method's capacity for\nrecovering known biological information in the samples through clustering\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 06:05:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Long", "Jiawei", ""], ["Xia", "Yu", ""]]}, {"id": "1912.08567", "submitter": "Hans-Michael Kaltenbach", "authors": "Hans-Michael Kaltenbach", "title": "Teaching Design of Experiments using Hasse diagrams", "comments": "18 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hasse diagrams provide a principled means for visualizing the structure of\nstatistical designs constructed by crossing and nesting of experimental\nfactors. They have long been applied for automated construction of linear\nmodels and their associated linear subspaces for complex designs. Here, we\nargue that they could also provide a central component for planning and\nteaching introductory or service courses in experimental design.\n  Specifically, we show how Hasse diagrams allow constructing most elementary\ndesigns and finding many of their properties, such as degrees of freedom, error\nstrata, experimental units and denominators for F-tests. Linear (mixed) models\nfor analysis directly correspond to the diagrams, which facilitates both\ndefining a model and specifying it in statistical software. We demonstrate how\ninstructors can seamlessly use Hasse diagrams to construct designs by combining\nsimple unit- and treatment structures, identify pseudo-replication, and discuss\na design's randomization, unit-treatment versus treatment-treatment\ninteractions, or complete confounding. These features commend Hasse diagrams as\na powerful tool for unifying ideas and concepts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:39:36 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kaltenbach", "Hans-Michael", ""]]}, {"id": "1912.08581", "submitter": "H{\\aa}vard Kvamme", "authors": "H{\\aa}vard Kvamme and {\\O}rnulf Borgan", "title": "The Brier Score under Administrative Censoring: Problems and Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Brier score is commonly used for evaluating probability predictions. In\nsurvival analysis, with right-censored observations of the event times, this\nscore can be weighted by the inverse probability of censoring (IPCW) to retain\nits original interpretation. It is common practice to estimate the censoring\ndistribution with the Kaplan-Meier estimator, even though it assumes that the\ncensoring distribution is independent of the covariates. This paper discusses\nthe general impact of the censoring estimates on the Brier score and shows that\nthe estimation of the censoring distribution can be problematic. In particular,\nwhen the censoring times can be identified from the covariates, the IPCW score\nis no longer valid. For administratively censored data, where the potential\ncensoring times are known for all individuals, we propose an alternative\nversion of the Brier score. This administrative Brier score does not require\nestimation of the censoring distribution and is valid even if the censoring\ntimes can be identified from the covariates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:13:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kvamme", "H\u00e5vard", ""], ["Borgan", "\u00d8rnulf", ""]]}, {"id": "1912.08612", "submitter": "Robert O'Shea", "authors": "Robert O'Shea", "title": "Interpreting Missing Data Patterns in the ICU", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: Clinical examinations are performed on the basis of necessity.\nHowever, our decisions to investigate and document are influenced by various\nother factors, such as workload and preconceptions. Data missingness patterns\nmay contain insights into conscious and unconscious norms of clinical practice.\nMETHODS: We examine data from the SPOTLIGHT study, a multi-centre cohort study\nof the effect of prompt ICU admission on mortality. We identify missing values\nand generate an auxiliary dataset indicating the missing entries. We deploy\nsparse Gaussian Graphical modelling techniques to identify conditional\ndependencies between the observed data and missingness patterns. We quantify\nthese associations with sparse partial correlation, correcting for multiple\ncollinearity. RESULTS: We identify 35 variables which significantly influence\ndata missingness patterns (alpha = 0.01). We identify reduced recording of\nessential monitoring such as temperature (partial corr. = -0.0542, p =\n6.65e-10), respiratory rate (partial corr. = -0.0437, p = 5.15e-07) and urea\n(partial corr. = -0.0263, p = 0.001611) in patients with reduced consciousness.\nWe demonstrate reduction of temperature (partial corr. = -0.04, p = 8.5e-06),\nurine output (partial corr. = -0.05, p = 7.5e-09), lactate (partial corr. =\n-0.03, p = 0.00032) and bilirubin (partial corr. = -0.03, p = 0.00137)\nmonitoring due to winter pressures. We provide statistical evidence of Missing\nNot at Random patterns in FiO2 and SF ratio recording. CONCLUSIONS: Graphical\nmissingness analysis offers valuable insights into critical care delivery,\nidentifying specific areas for quality improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:24:59 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["O'Shea", "Robert", ""]]}, {"id": "1912.08799", "submitter": "Hazem Al-Mofleh", "authors": "Hazem Al-Mofleh and Ahmed Z. Afify", "title": "A generalization of Ramos-Louzada distribution: Properties and\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a new two-parameter model called generalized Ramos-Louzada\n(GRL) distribution is proposed. The new model provides more flexibility in\nmodeling data with increasing, decreasing, j shaped and reversed-J shaped\nhazard rate function. Several statistical and reliability properties of the GRL\nmodel are also presented in this paper. The unknown parameters of the GRL\ndistribution are discussed using eight frequentist estimation approaches. These\napproaches are important to develop a guideline to choose the best method of\nestimation for the GRL parameters, that would be of great interest to\npractitioners and applied statisticians. A detailed numerical simulation study\nis carried out to examine the bias and the mean square error of the proposed\nestimators. We illustrate the performance of the GRL distribution using two\nreal data sets from the fields of medicine and geology and both data sets show\nthat the new model is more appropriate as compared to the gamma, Marshall-Olkin\nexponential, exponentiated exponential, beta exponential, generalized Lindley,\nPoisson-Lomax, Lindley geometric and Lindley distributions, among others.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:52:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Al-Mofleh", "Hazem", ""], ["Afify", "Ahmed Z.", ""]]}, {"id": "1912.08843", "submitter": "Taehee Lee", "authors": "Taehee Lee and Charles E. Lawrence", "title": "Heteroscedastic Gaussian Process Regression on the Alkenone over Sea\n  Surface Temperatures", "comments": "This article has been submitted to \"Dec 2019, Proceedings of the 9th\n  International Workshop on Climate Informatics: CI 2019. NCAR Technical Note\n  NCAR/TN-561+PROC\"", "journal-ref": null, "doi": "10.5065/y82j-f154", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To restore the historical sea surface temperatures (SSTs) better, it is\nimportant to construct a good calibration model for the associated proxies. In\nthis paper, we introduce a new model for alkenone (${\\rm{U}}_{37}^{\\rm{K}'}$)\nbased on the heteroscedastic Gaussian process (GP) regression method. Our\nnonparametric approach not only deals with the variable pattern of noises over\nSSTs but also contains a Bayesian method of classifying potential outliers.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:20:33 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lee", "Taehee", ""], ["Lawrence", "Charles E.", ""]]}, {"id": "1912.08930", "submitter": "Kristijan Petrovski", "authors": "Tamara Dimitrova, Kristijan Petrovski and Ljupco Kocarev", "title": "Graphlets in Multiplex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We develop graphlet analysis for multiplex networks and discuss how this\nanalysis can be extended to multilayer and multilevel networks as well as to\ngraphs with node and/or link categorical attributes. The analysis has been\nadapted for two typical examples of multiplexes: economic trade data\nrepresented as a 957-plex network and 75 social networks each represented as a\n12-plex network. We show that wedges (open triads) occur more often in economic\ntrade networks than in social networks, indicating the tendency of a country to\nproduce/trade of a product in local structure of triads which are not closed.\nMoreover, our analysis provides evidence that the countries with small\ndiversity tend to form correlated triangles. Wedges also appear in the social\nnetworks, however the dominant graphlets in social networks are triangles\n(closed triads). If a multiplex structure indicates a strong tie, the graphlet\nanalysis provides another evidence for the concepts of strong/weak ties and\nstructural holes. In contrast to Granovetter's seminal work on the strength of\nweak ties, in which it has been documented that the wedges with only strong\nties are absent, here we show that for the analyzed 75 social networks, the\nwedges with only strong ties are not only present but also significantly\ncorrelated.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 19:11:14 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Dimitrova", "Tamara", ""], ["Petrovski", "Kristijan", ""], ["Kocarev", "Ljupco", ""]]}, {"id": "1912.09086", "submitter": "Alexis Bellot", "authors": "Alexis Bellot, Mihaela van der Schaar", "title": "A Bayesian Approach to Modelling Longitudinal Data in Electronic Health\n  Records", "comments": "Presented as an abstract at the Machine Learning for Health Workshop,\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing electronic health records (EHR) poses significant challenges\nbecause often few samples are available describing a patient's health and, when\navailable, their information content is highly diverse. The problem we consider\nis how to integrate sparsely sampled longitudinal data, missing measurements\ninformative of the underlying health status and fixed demographic information\nto produce estimated survival distributions updated through a patient's follow\nup. We propose a nonparametric probabilistic model that generates survival\ntrajectories from an ensemble of Bayesian trees that learns variable\ninteractions over time without specifying beforehand the longitudinal process.\nWe show performance improvements on Primary Biliary Cirrhosis patient data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:42:27 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bellot", "Alexis", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1912.09204", "submitter": "Samvel Gasparyan", "authors": "Samvel B. Gasparyan, Folke Folkvaljon, Olof Bengtsson, Joan\n  Buenconsejo, Gary G. Koch", "title": "Adjusted Win Ratio with Stratification: Calculation Methods and\n  Interpretation", "comments": null, "journal-ref": "Statistical Methods in Medical Research 2020", "doi": "10.1177/0962280220942558", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The win ratio is a general method of comparing locations of distributions of\ntwo independent, ordinal random variables, and it can be estimated without\ndistributional assumptions. In this paper we provide a unified theory of win\nratio estimation in the presence of stratification and adjustment by a numeric\nvariable. Building step by step on the estimate of the crude win ratio we\ncompare corresponding tests with well known nonparametric tests of group\ndifference (Wilcoxon rank-sum test, Fligner-Plicello test,\nCochran-Mantel-Haenszel test, test based on the regression on ranks and the\nrank ANCOVA test). We show that the win ratio gives an interpretable treatment\neffect measure with corresponding test to detect treatment effect difference\nunder minimal assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:02:56 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gasparyan", "Samvel B.", ""], ["Folkvaljon", "Folke", ""], ["Bengtsson", "Olof", ""], ["Buenconsejo", "Joan", ""], ["Koch", "Gary G.", ""]]}, {"id": "1912.09273", "submitter": "Ali Reza Fallahi", "authors": "Safoora Zarei and Ali R. Fallahi", "title": "Pay-As-You-Drive Insurance Pricing Model", "comments": null, "journal-ref": "American Journal of Statistics and Actuarial Science, Vol.2, Issue\n  1, pp 1 - 9, 2020", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every time drivers take to the road, and with each mile that they drive,\nexposes themselves and others to the risk of an accident. Insurance premiums\nare only weakly linked to mileage, however, and have lump-sum characteristics\nlargely. The result is too much driving, and too many accidents. In this paper,\nwe introduce some useful theoretical results for Pay-As-You-Drive in Automobile\ninsurances. We consider a counting process and also find the distribution of\ndiscounted collective risk model when the counting process is non-homogeneous\nPoisson.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:58:46 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zarei", "Safoora", ""], ["Fallahi", "Ali R.", ""]]}, {"id": "1912.09318", "submitter": "Aaron Alvero", "authors": "AJ Alvero, Noah Arthurs, anthony lising antonio, Benjamin W. Domingue,\n  Ben Gebre-Medhin, Sonia Giebel, Mitchell L. Stevens", "title": "AI and Holistic Review: Informing Human Reading in College Admissions", "comments": "AIES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  College admissions in the United States is carried out by a human-centered\nmethod of evaluation known as holistic review, which typically involves reading\noriginal narrative essays submitted by each applicant. The legitimacy and\nfairness of holistic review, which gives human readers significant discretion\nover determining each applicant's fitness for admission, has been repeatedly\nchallenged in courtrooms and the public sphere. Using a unique corpus of\n283,676 application essays submitted to a large, selective, state university\nsystem between 2015 and 2016, we assess the extent to which applicant\ndemographic characteristics can be inferred from application essays. We find a\nrelatively interpretable classifier (logistic regression) was able to predict\ngender and household income with high levels of accuracy. Findings suggest that\ndata auditing might be useful in informing holistic review, and perhaps other\nevaluative systems, by checking potential bias in human or computational\nreadings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:08:36 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Alvero", "AJ", ""], ["Arthurs", "Noah", ""], ["antonio", "anthony lising", ""], ["Domingue", "Benjamin W.", ""], ["Gebre-Medhin", "Ben", ""], ["Giebel", "Sonia", ""], ["Stevens", "Mitchell L.", ""]]}, {"id": "1912.09365", "submitter": "Nicolas Couellan", "authors": "Ambre Diet and Nicolas Couellan and Xavier Gendre and Julien Martin", "title": "A statistical approach for robust tolerance design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within an industrial manufacturing process, tolerancing is a key player. The\ndimensions uncertainties management starts during the design phase, with an\nassessment on variability of parts not yet produced. For one assembly step, we\ncan gain knowledge from the tolerance range required for the parts involved. In\norder to assess output uncertainty of this assembly in a reliable way, this\npaper presents an approach based on the deviation of the sum of uniform\ndistributions. As traditional approaches based on Hoeffding inequalities do not\ngive accurate results when the deviation considered is small, we propose an\nimproved upper bound. We then discuss how the stack chain geometry impacts the\nbound definition. Finally, we show an application of the proposed approach in\ntolerance design of an aircraft sub-assembly. The main interest of the\ntechnique compared to existing methodologies is the management of the\nconfidence level and the emphasis of the explicit role of the balance within\nthe stack chain.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:46:47 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Diet", "Ambre", ""], ["Couellan", "Nicolas", ""], ["Gendre", "Xavier", ""], ["Martin", "Julien", ""]]}, {"id": "1912.09426", "submitter": "Johann Baumgartner", "authors": "Johann Baumgartner (1), Katharina Gruber (1), Sofia Simoes (2),\n  Yves-Marie Saint-Drenan (3), Johannes Schmidt (1) ((1) University of Natural\n  Resources and Life Sciences, Vienna, (2) NOVA University Lisbon, (3) MINES\n  ParisTech)", "title": "Machine learning models show similar performance to Renewables.ninja for\n  generation of long-term wind power time series even without location\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Driven by climatic processes, wind power generation is inherently variable.\nLong-term simulated wind power time series are therefore an essential component\nfor understanding the temporal availability of wind power and its integration\ninto future renewable energy systems. In the recent past, mainly power curve\nbased models such as Renewables.ninja (RN) have been used for deriving\nsynthetic time series for wind power generation despite their need for accurate\nlocation information as well as for bias correction, and their insufficient\nreplication of extreme events and short-term power ramps. We assess how time\nseries generated by machine learning models (MLM) compare to RN in terms of\ntheir ability to replicate the characteristics of observed nationally\naggregated wind power generation for Germany. Hence, we apply neural networks\nto one MERRA2 reanalysis wind speed input dataset with no location information\nand one with basic location information. The resulting time series and the RN\ntime series are compared with actual generation. Both MLM time series feature\nequal or even better time series quality than RN depending on the\ncharacteristics considered. We conclude that MLM models can, even when reducing\ninformation on turbine locations and turbine types, produce time series of at\nleast equal quality to RN.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:59:49 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Baumgartner", "Johann", ""], ["Gruber", "Katharina", ""], ["Simoes", "Sofia", ""], ["Saint-Drenan", "Yves-Marie", ""], ["Schmidt", "Johannes", ""]]}, {"id": "1912.09468", "submitter": "Deyu Ming", "authors": "Deyu Ming and Serge Guillas", "title": "Linked Gaussian Process Emulation for Systems of Computer Models using\n  Mat\\'ern Kernels and Adaptive Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art linked Gaussian process offers a way to build analytical\nemulators for systems of computer models. We generalize the closed form\nexpressions for the linked Gaussian process under the squared exponential\nkernel to a class of Mat\\'ern kernels, that are essential in advanced\napplications. An iterative procedure to construct linked Gaussian processes as\nsurrogate models for any feed-forward systems of computer models is presented\nand illustrated on a feed-back coupled satellite system. We also introduce an\nadaptive design algorithm that could increase the approximation accuracy of\nlinked Gaussian process surrogates with reduced computational costs on running\nexpensive computer systems, by allocating runs and refining emulators of\nindividual sub-models based on their heterogeneous functional complexity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 18:51:40 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 15:11:21 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:27:18 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2021 15:49:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ming", "Deyu", ""], ["Guillas", "Serge", ""]]}, {"id": "1912.09472", "submitter": "Mohammad Bhuiyan", "authors": "Mohammad Alfrad Nobel Bhuiyan, Patrick Ryan, Farzan Oroumyeh, Yajna\n  Jathan, Madhumitaa Roy, Siv Balachandran, Cole Brokamp", "title": "Source-specific contributions of particulate matter to asthma-related\n  pediatric emergency department utilization", "comments": "my coauthors doesnt want to keep the paper in the arxiv", "journal-ref": null, "doi": "10.1007/s13755-021-00141-z", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Few studies have linked specific sources of ambient particulate matter\nsmaller than 2.5 $\\mu$m (PM2.5) and asthma. In this study, we estimated the\ncontributions of specific sources to PM2.5 and examined their association with\ndaily asthma hospital utilization in Cincinnati, Ohio, USA. We used Poisson\nregression models to estimate the daily number of asthma ED visits the day of\nand one, and two days following separate increases in PM2.5 and its source\ncomponents, adjusting for temporal trends, holidays, temperature, and humidity.\nIn addition, we used a model-based clustering method to group days with similar\nsource-specific contributions into six distinct clusters. Specifically,\nelevated PM2.5 concentrations occurring on days characterized by low\ncontributions of coal combustion showed a significantly reduced risk of\nhospital utilization for asthma (rate ratio: 0.86, 95% CI: [0.77, 0.95])\ncompared to other clusters. Reducing the contribution of coal combustion to\nPM2.5 levels could be an effective intervention for reducing asthma-related\nhospital utilization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 18:56:40 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 13:26:10 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bhuiyan", "Mohammad Alfrad Nobel", ""], ["Ryan", "Patrick", ""], ["Oroumyeh", "Farzan", ""], ["Jathan", "Yajna", ""], ["Roy", "Madhumitaa", ""], ["Balachandran", "Siv", ""], ["Brokamp", "Cole", ""]]}, {"id": "1912.09504", "submitter": "Mohammad Bhuiyan", "authors": "Mohammad Alfrad Nobel Bhuiyan, Cole Brokamp, Tracy E. Madsen, Jane C.\n  Khoury, Heidi Sucharew, Kathleen Alwell, Charles J. Moomaw, Monir Hossain,\n  Matthew L. Flaherty, Daniel Woo, Jason Mackey, Felipe De Los Rios La Rosa,\n  Sharyl Martini, Simona Ferioli, Opeolu Adeoye, Brett M. Kissela, Dawn\n  Kleindorfer", "title": "Differential impact of acute fine particulate matter exposure on risk of\n  stroke by stroke subtype, age, sex, and race: a case-crossover study", "comments": "One coauthor didnt want to submit the paper on the arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Objective: Association between acute ambient fine particulate matter (PM2.5,\naerodynamic diameter less than 2.5 and cardiovascular events are well\ndocumented. However, it remains unclear whether acute exposure to PM2.5 acts as\na trigger for hemorrhagic (intracerebral or subarachnoid hemorrhage) or\nnon-hemorrhagic (infarct or transient ischemic attack) stroke onset. We,\ntherefore, examined the association between ambient PM2.5 and stroke onset, and\nwhether this relationship differs by stroke subtype, age, sex, and race.\nMethods: We used a time-stratified case-crossover design to examine the\nassociation between exposure to PM2.5 and stroke onset for the calendar year\n2010. Data were collected from the Greater Cincinnati Northern Kentucky Stroke\nStudy. We included patients 20 years and older, initially ascertained through\nhospital ICD-9 discharge codes. Daily ambient concentrations of PM2.5 were\nestimated from the patient's residential addresses using a spatiotemporal\nmodel. Conclusion: Our findings suggest that short-term PM2.5 exposure,\nparticularly at three days before the event, is associated with stroke onset\nbut varies according to stroke subtype, age, sex, and race.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:08:27 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 13:27:31 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bhuiyan", "Mohammad Alfrad Nobel", ""], ["Brokamp", "Cole", ""], ["Madsen", "Tracy E.", ""], ["Khoury", "Jane C.", ""], ["Sucharew", "Heidi", ""], ["Alwell", "Kathleen", ""], ["Moomaw", "Charles J.", ""], ["Hossain", "Monir", ""], ["Flaherty", "Matthew L.", ""], ["Woo", "Daniel", ""], ["Mackey", "Jason", ""], ["La Rosa", "Felipe De Los Rios", ""], ["Martini", "Sharyl", ""], ["Ferioli", "Simona", ""], ["Adeoye", "Opeolu", ""], ["Kissela", "Brett M.", ""], ["Kleindorfer", "Dawn", ""]]}, {"id": "1912.09526", "submitter": "Jeremy Ash", "authors": "Jeremy R. Ash and Jacqueline M. Hughes-Oliver", "title": "Confidence Bands and Hypothesis Test Methods for Recall and Precision\n  Curves at Extremely Small Fractions with Applications to Drug Discovery", "comments": "41 pages, 7 figures, 13 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In virtual screening for drug discovery, recall curves are used to assess the\nperformance of ranking algorithms, in which recall is a function of the\nfraction of data prioritized for experimental testing. Unfortunately,\nresearchers almost never consider the uncertainty in the estimation of the\nrecall curve when benchmarking algorithms. We confirm that a recently developed\nprocedure for estimating pointwise confidence intervals for recall curves --\nand closely related variants, such as precision curves -- can be applied to a\nvariety of simulated data sets representative of those typically encountered in\nvirtual screening. Since it is more desirable in benchmarks to present the\nuncertainty of performance over a range of testing fractions, we extend the\npointwise confidence interval procedure to allow for the estimation of\nconfidence bands for these curves. We also present hypothesis test methods to\ndetermine significant differences between the curves for competing algorithms.\nWe show these methods have high power to detect significant differences at a\nrange of small fractions typically tested, while maintaining control of type I\nerror rate. These methods enable statistically rigorous comparisons of virtual\nscreening algorithms using a metric that quantifies the aspect of performance\nthat is of primary interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:08:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ash", "Jeremy R.", ""], ["Hughes-Oliver", "Jacqueline M.", ""]]}, {"id": "1912.09560", "submitter": "Zhengxiao Li", "authors": "Zhengxiao Li, Jan Beirlant, Shengwang Meng", "title": "Generalizing the log-Moyal distribution and regression models for heavy\n  tailed loss data", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic loss data are known to be heavy-tailed. Practitioners then need\nmodels that are able to capture both tail and modal parts of claim data. To\nthis purpose, a new parametric family of loss distributions is proposed as a\ngamma mixture of the generalized log-Moyal distribution from Bhati and Ravi\n(2018), termed the generalized log-Moyal gamma distribution (GLMGA). We discuss\nthe probabilistic characteristics of the GLMGA, and statistical estimation of\nthe parameters through maximum likelihood. While the GLMGA distribution is a\nspecial case of the GB2 distribution, we show that this simpler model is\neffective in regression modelling of large and modal loss data. A fire claim\ndata set reported in Cummins et al. (1990) and a Chinese earthquake loss data\nset are used to illustrate the applicability of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:55:42 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Zhengxiao", ""], ["Beirlant", "Jan", ""], ["Meng", "Shengwang", ""]]}, {"id": "1912.09676", "submitter": "Gimenez Olivier", "authors": "Julie Louvrier, Julien Papa\\\"ix, Christophe Duchamp, Olivier Gimenez", "title": "A mechanistic-statistical species distribution model to explain and\n  forecast wolf (Canis lupus) colonization in South-Eastern France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Species distribution models (SDMs) are important statistical tools for\necologists to understand and predict species range. However, standard SDMs do\nnot explicitly incorporate dynamic processes like dispersal. This limitation\nmay lead to bias in inference about species distribution. Here, we adopt the\ntheory of ecological diffusion that has recently been introduced in statistical\necology to incorporate spatio-temporal processes in ecological models. As a\ncase study, we considered the wolf (Canis lupus) that has been recolonizing\nEastern France naturally through dispersal from the Apennines since the early\n90's. Using partial differential equations for modelling species diffusion and\ngrowth in a fragmented landscape, we develop a mechanistic-statistical\nspatio-temporal model accounting for ecological diffusion, logistic growth and\nimperfect species detection. We conduct a simulation study and show the ability\nof our model to i) estimate ecological parameters in various situations with\ncontrasted species detection probability and number of surveyed sites and ii)\nforecast the distribution into the future. We found that the growth rate of the\nwolf population in France was explained by the proportion of forest cover, that\ndiffusion was influenced by human density and that species detectability\nincreased with increasing survey effort. Using the parameters estimated from\nthe 2007-2015 period, we then forecasted wolf distribution in 2016 and found\ngood agreement with the actual detections made that year. Our approach may be\nuseful for managing species that interact with human activities to anticipate\npotential conflicts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:54:11 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 12:10:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Louvrier", "Julie", ""], ["Papa\u00efx", "Julien", ""], ["Duchamp", "Christophe", ""], ["Gimenez", "Olivier", ""]]}, {"id": "1912.10326", "submitter": "Adriaan Hilbers", "authors": "Adriaan P Hilbers and David J Brayshaw and Axel Gandy", "title": "Efficient quantification of the impact of demand and weather uncertainty\n  in power system models", "comments": "8 pages. For supplementary material, see the version on IEEExplore", "journal-ref": "IEEE Transactions on Power Systems, 2020", "doi": "10.1109/TPWRS.2020.3031187", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new approach to quantify the impact of forward\npropagated demand and weather uncertainty on power system planning and\noperation models. Recent studies indicate that such sampling uncertainty,\noriginating from demand and weather time series inputs, should not be ignored.\nHowever, established uncertainty quantification approaches fail in this context\ndue to the data and computing resources required for standard Monte Carlo\nanalysis with disjoint samples. The method introduced here uses an m out of n\nbootstrap with shorter time series than the original, enhancing computational\nefficiency and avoiding the need for any additional data. It both quantifies\noutput uncertainty and determines the sample length required for desired\nconfidence levels. Simulations and validation exercises are performed on two\ncapacity expansion planning models and one unit commitment and economic\ndispatch model. A diagnostic for the validity of estimated uncertainty bounds\nis discussed. The models, data and code are made available.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 19:47:29 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 09:41:07 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 09:13:31 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 14:30:57 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hilbers", "Adriaan P", ""], ["Brayshaw", "David J", ""], ["Gandy", "Axel", ""]]}, {"id": "1912.10372", "submitter": "Koen Simons", "authors": "Koen Simons (1), Rebecca Bentley (1) and Lyle Gurrin (1) ((1)\n  University of Melbourne)", "title": "Using Small Domain Estimation to obtain better retrospective Age Period\n  Cohort insights", "comments": "23 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent changes in housing costs relative to income are likely to affect\npeople's propensity to Housing Affordability Stress (HAS), which is known to\nhave a detrimental effect on a range of health outcomes. The magnitude of these\neffects may vary between subgroups of the population, in particular across age\ngroups. Estimating these effect sizes from longitudinal data requires Small\nDomain Estimation (SDE) as available data is generally limited to small sample\nsizes. In this paper we develop the rationale for smoothing-based SDE using two\ncase studies: (1) transitions into and out of HAS and (2) the mental health\neffect associated with HAS. We apply cross-validation to assess the relative\nperformance of multiple SDE methods and discuss how SDE can be embedded into\ng-computation for causal inference.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 02:18:59 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Simons", "Koen", ""], ["Bentley", "Rebecca", ""], ["Gurrin", "Lyle", ""]]}, {"id": "1912.10417", "submitter": "Rodolfo Metulini", "authors": "Paola Zuccolotto and Marco Sandri and Marica Manisera and Rodolfo\n  Metulini", "title": "Modelling basketball players' performance and interactions between\n  teammates with a regime switching approach", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basketball players' performance measurement is of critical importance for a\nbroad spectrum of decisions related to training and game strategy. Despite this\nrecognized central role, the main part of the studies on this topic focus on\nperformance level measurement, neglecting other important characteristics, such\nas variability. In this paper, shooting performance variability is modeled with\na Markov Switching dynamic, assuming the existence of two alternating\nperformance regimes. Then, the relationships between each player's variability\nand the lineup composition is modeled as an ARIMA process with covariates and\ndescribed with network analysis tools, in order to extrapolate positive and\nnegative interactions between teammates\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 10:51:29 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zuccolotto", "Paola", ""], ["Sandri", "Marco", ""], ["Manisera", "Marica", ""], ["Metulini", "Rodolfo", ""]]}, {"id": "1912.10419", "submitter": "Francesco Sanna Passino", "authors": "Francesco Sanna Passino, Anna S. Bertiger, Joshua C. Neil, Nicholas A.\n  Heard", "title": "Link prediction in dynamic networks using random dot product graphs", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, forthcoming (2021)", "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of predicting links in large networks is an important task in a\nvariety of practical applications, including social sciences, biology and\ncomputer security. In this paper, statistical techniques for link prediction\nbased on the popular random dot product graph model are carefully presented,\nanalysed and extended to dynamic settings. Motivated by a practical application\nin cyber-security, this paper demonstrates that random dot product graphs not\nonly represent a powerful tool for inferring differences between multiple\nnetworks, but are also efficient for prediction purposes and for understanding\nthe temporal evolution of the network. The probabilities of links are obtained\nby fusing information at two stages: spectral methods provide estimates of\nlatent positions for each node, and time series models are used to capture\ntemporal dynamics. In this way, traditional link prediction methods, usually\nbased on decompositions of the entire network adjacency matrix, are extended\nusing temporal information. The methods presented in this article are applied\nto a number of simulated and real-world graphs, showing promising results.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 11:05:10 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 14:57:29 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 17:07:56 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 22:06:36 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Passino", "Francesco Sanna", ""], ["Bertiger", "Anna S.", ""], ["Neil", "Joshua C.", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "1912.10508", "submitter": "Gabriel Schamberg", "authors": "Gabriel Schamberg, William Chapman, Shang-Ping Xie, Todd P. Coleman", "title": "Direct and Indirect Effects -- An Information Theoretic Perspective", "comments": null, "journal-ref": null, "doi": "10.3390/e22080854", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic (IT) approaches to quantifying causal influences have\nexperienced some popularity in the literature, in both theoretical and applied\n(e.g. neuroscience and climate science) domains. While these causal measures\nare desirable in that they are model agnostic and can capture non-linear\ninteractions, they are fundamentally different from common statistical notions\nof causal influence in that they (1) compare distributions over the effect\nrather than values of the effect and (2) are defined with respect to random\nvariables representing a cause rather than specific values of a cause. We here\npresent IT measures of direct, indirect, and total causal effects. The proposed\nmeasures are unlike existing IT techniques in that they enable measuring causal\neffects that are defined with respect to specific values of a cause while still\noffering the flexibility and general applicability of IT techniques. We provide\nan identifiability result and demonstrate application of the proposed measures\nin estimating the causal effect of the El Ni\\~no-Southern Oscillation on\ntemperature anomalies in the North American Pacific Northwest.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 18:46:02 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:34:08 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Schamberg", "Gabriel", ""], ["Chapman", "William", ""], ["Xie", "Shang-Ping", ""], ["Coleman", "Todd P.", ""]]}, {"id": "1912.10566", "submitter": "Filipe Zabala Mr.", "authors": "Filipe J. Zabala", "title": "A Bayesian Application in Judicial Decisions", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new tool to support the decision concerning moral\ndamage indemnity values of the judiciary of Rio Grande do Sul, Brazil. A\nBayesian approach is given, in order to allow the assignment of the\nmagistrate's opinion about such indemnity amounts, based on historical values.\nThe solution is delivered in free software using public data, in order to\npermit future audits.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 00:11:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zabala", "Filipe J.", ""]]}, {"id": "1912.10610", "submitter": "Panos Toulis", "authors": "Azeem Shaikh and Panos Toulis", "title": "Randomization Tests in Observational Studies with Staggered Adoption of\n  Treatment", "comments": "30 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inference in observational studies with\ntime-varying adoption of treatment. In addition to an unconfoundedness\nassumption that the potential outcomes are independent of the times at which\nunits adopt treatment conditional on the units' observed characteristics, our\nanalysis assumes that the time at which each unit adopts treatment follows a\nCox proportional hazards model. This assumption permits the time at which each\nunit adopts treatment to depend on the observed characteristics of the unit,\nbut imposes the restriction that the probability of multiple units adopting\ntreatment at the same time is zero. In this context, we study Fisher-style\nrandomization tests of a null hypothesis that specifies that there is no\ntreatment effect for all units and all time periods in a distributional sense.\nWe first show that an infeasible test that treats the parameters of the Cox\nmodel as known has rejection probability no greater than the nominal level in\nfinite samples. We then establish that the feasible test that replaces these\nparameters with consistent estimators has limiting rejection probability no\ngreater than the nominal level. In a simulation study, we examine the practical\nrelevance of our theoretical results, including robustness to misspecification\nof the model for the time at which each unit adopts treatment. Finally, we\nprovide an empirical application of our methodology using the synthetic\ncontrol-based test statistic and tobacco legislation data found in Abadie et.\nal. (2010).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:53:54 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 02:38:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Shaikh", "Azeem", ""], ["Toulis", "Panos", ""]]}, {"id": "1912.10709", "submitter": "Yijian Chuan", "authors": "Yijian Chuan, Lan Wu", "title": "Centralizing-Unitizing Standardized High-Dimensional Directional\n  Statistics and Its Applications in Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-sectional \"Information Coefficient\" (IC) is a widely and deeply\naccepted measure in portfolio management. The paper gives an insight into IC in\nview of high-dimensional directional statistics: IC is a linear operator on the\ncomponents of a centralizing-unitizing standardized random vector of\nnext-period cross-sectional returns. Our primary research first clearly defines\nIC with the high-dimensional directional statistics, discussing its first two\nmoments. We derive the closed-form expressions of the directional statistics'\ncovariance matrix and IC's variance in a homoscedastic condition. Also, we\nsolve the optimization of IC's maximum expectation and minimum variance.\nSimulation intuitively characterizes the standardized directional statistics\nand IC's p.d.f.. The empirical analysis of the Chinese stock market uncovers\ninteresting facts about the standardized vectors of cross-sectional returns and\nhelps obtain the time series of the measure in the real market. The paper\ndiscovers a potential application of directional statistics in finance, proves\nexplicit results of the projected normal distribution, and reveals IC's nature.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:03:38 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:41:20 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Chuan", "Yijian", ""], ["Wu", "Lan", ""]]}, {"id": "1912.10774", "submitter": "Francis Diebold", "authors": "Francis X. Diebold and Glenn D. Rudebusch", "title": "Probability Assessments of an Ice-Free Arctic: Comparing Statistical and\n  Climate Model Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The downward trend in the amount of Arctic sea ice has a wide range of\nenvironmental and economic consequences including important effects on the pace\nand intensity of global climate change. Based on several decades of satellite\ndata, we provide statistical forecasts of Arctic sea ice extent during the rest\nof this century. The best fitting statistical model indicates that overall sea\nice coverage is declining at an increasing rate. By contrast, average\nprojections from the CMIP5 global climate models foresee a gradual slowing of\nArctic sea ice loss even in scenarios with high carbon emissions. Our\nlong-range statistical projections also deliver probability assessments of the\ntiming of an ice-free Arctic. These results indicate almost a 60 percent chance\nof an effectively ice-free Arctic Ocean sometime during the 2030s -- much\nearlier than the average projection from the global climate models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:52:04 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 03:58:55 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 14:07:37 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 12:21:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Diebold", "Francis X.", ""], ["Rudebusch", "Glenn D.", ""]]}, {"id": "1912.10814", "submitter": "Arnald Puy", "authors": "Arnald Puy, Emanuele Borgonovo, Samuele Lo Piano, Andrea Saltelli", "title": "Are the results of the groundwater model robust?", "comments": "Comment on the paper by De Graaf et al. 2019. Environmental flow\n  limits to global groundwater pumping. Nature 574 (7776), 90-94", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  De Graaf et al. (2019) suggest that groundwater pumping will bring 42--79\\%\nof worldwide watersheds close to environmental exhaustion by 2050. We are\nskeptical of these figures due to several non-unique assumptions behind the\ncalculation of irrigation water demands and the perfunctory exploration of the\nmodel's uncertainty space. Their sensitivity analysis reveals a widespread lack\nof elementary concepts of design of experiments among modellers, and can not be\ntaken as a proof that their conclusions are robust.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:48:38 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Puy", "Arnald", ""], ["Borgonovo", "Emanuele", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""]]}, {"id": "1912.10847", "submitter": "Preeti Sah", "authors": "Preeti Sah and Ernest Fokou\\'e", "title": "What do Asian Religions Have in Common? An Unsupervised Text Analytics\n  Exploration", "comments": "18 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main source of various religious teachings is their sacred texts which\nvary from religion to religion based on different factors like the geographical\nlocation or time of the birth of a particular religion. Despite these\ndifferences, there could be similarities between the sacred texts based on what\nlessons it teaches to its followers. This paper attempts to find the similarity\nusing text mining techniques. The corpus consisting of Asian (Tao Te Ching,\nBuddhism, Yogasutra, Upanishad) and non-Asian (four Bible texts) is used to\nexplore findings of similarity measures like Euclidean, Manhattan, Jaccard and\nCosine on raw Document Term Frequency [DTM], normalized DTM which reveals\nsimilarity based on word usage. The performance of Supervised learning\nalgorithms like K-Nearest Neighbor [KNN], Support Vector Machine [SVM] and\nRandom Forest is measured based on its accuracy to predict correct scared text\nfor any given chapter in the corpus. The K-means clustering visualizations on\nEuclidean distances of raw DTM reveals that there exists a pattern of\nsimilarity among these sacred texts with Upanishads and Tao Te Ching is the\nmost similar text in the corpus.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:28:29 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sah", "Preeti", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "1912.10981", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio and Michela Cameletti and Marta Blangiardo", "title": "Missing data analysis and imputation via latent Gaussian Markov random\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we recast the problem of missing values in the covariates of a\nregression model as a latent Gaussian Markov random field (GMRF) model in a\nfully Bayesian framework. Our proposed approach is based on the definition of\nthe covariate imputation sub-model as a latent effect with a GMRF structure. We\nshow how this formulation works for continuous covariates and provide some\ninsight on how this could be extended to categorical covariates.\n  The resulting Bayesian hierarchical model naturally fits within the\nintegrated nested Laplace approximation (INLA) framework, which we use for\nmodel fitting. Hence, our work fills an important gap in the INLA methodology\nas it allows to treat models with missing values in the covariates.\n  As in any other fully Bayesian framework, by relying on INLA for model\nfitting it is possible to formulate a joint model for the data, the imputed\ncovariates and their missingness mechanism. In this way, we are able to tackle\nthe more general problem of assessing the missingness mechanism by conducting a\nsensitivity analysis on the different alternatives to model the non-observed\ncovariates.\n  Finally, we illustrate the proposed approach with two examples on modeling\nhealth risk factors and disease mapping. Here, we rely on two different\nimputation mechanisms based on a typical multiple linear regression and a\nspatial model, respectively. Given the speed of model fitting with INLA we are\nable to fit joint models in a short time, and to easily conduct sensitivity\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:16:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Cameletti", "Michela", ""], ["Blangiardo", "Marta", ""]]}, {"id": "1912.11028", "submitter": "Katarzyna Reluga", "authors": "Katarzyna Reluga, Mar\\'ia-Jos\\'e Lombard\\'ia, Stefan Sperlich", "title": "Simultaneous Inference for Empirical Best Predictors with a Poverty\n  Study in Small Areas", "comments": "46 pages, 20 figures; simulations and data analysis expanded,\n  additional remarks added", "journal-ref": null, "doi": "10.1080/01621459.2021.1942014", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, generalized linear mixed models are broadly used in many fields.\nHowever, the development of tools for performing simultaneous inference has\nbeen largely neglected in this domain. A framework for joint inference is\nindispensable to carry out statistically valid multiple comparisons of\nparameters of interest between all or several clusters. We therefore develop\nsimultaneous confidence intervals and multiple testing procedures for empirical\nbest predictors under generalized linear mixed models. In addition, we\nimplement our methodology to study widely employed examples of mixed models,\nthat is, the unit-level binomial, the area-level Poisson-gamma and the\narea-level Poisson-lognormal mixed models. The asymptotic results are\naccompanied by extensive simulations. A case study on predicting poverty rates\nillustrates applicability and advantages of our simultaneous inference tools.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:49:04 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 12:16:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Reluga", "Katarzyna", ""], ["Lombard\u00eda", "Mar\u00eda-Jos\u00e9", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1912.11172", "submitter": "Tianyi Liu", "authors": "Tianyi Liu and Enlu Zhou", "title": "Online Quantification of Input Model Uncertainty by Two-Layer Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation has been widely used to analyze the performance of\ncomplex stochastic systems and facilitate decision making in those systems.\nStochastic simulation is driven by the input model, which is a collection of\nprobability distributions that model the stochasticity in the system. The input\nmodel is usually estimated using a finite amount of data, which introduces the\nso-called input model uncertainty to the simulation output. How to quantify\ninput uncertainty has been studied extensively, and many methods have been\nproposed for the batch data setting, i.e., when all the data are available at\nonce. However, methods for \"streaming data\" arriving sequentially in time are\nstill in demand, despite that streaming data have become increasingly prevalent\nin modern applications. To fill this gap, we propose a two-layer importance\nsampling framework that incorporates streaming data for online input\nuncertainty quantification. Under this framework, we develop two algorithms\nthat suit different application scenarios: the first scenario is when data come\nat a fast speed and there is no time for any new simulation in between updates;\nthe second is when data come at a moderate speed and a few but limited\nsimulations are allowed at each time stage. We prove the consistency and\nasymptotic convergence rate results, which theoretically show the efficiency of\nour proposed approach. We further demonstrate the proposed algorithms on a\nnumerical example of the news vendor problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 02:02:01 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:13:36 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Liu", "Tianyi", ""], ["Zhou", "Enlu", ""]]}, {"id": "1912.11335", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen", "title": "A Continuous-Time Dynamic Choice Measurement Model for Problem-Solving\n  Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem solving has been recognized as a central skill that today's students\nneed to thrive and shape their world. As a result, the measurement of\nproblem-solving competency has received much attention in education in recent\nyears. A popular tool for the measurement of problem solving is simulated\ninteractive tasks, which require students to uncover some of the information\nneeded to solve the problem through interactions with a computer-simulated\nenvironment. A computer log file records a student's problem-solving process in\ndetails, including his/her actions and the time stamps of these actions. It\nthus provides rich information for the measurement of students' problem-solving\ncompetency. On the other hand, extracting useful information from log files is\na challenging task, due to its complex data structure. In this paper, we show\nhow log file process data can be viewed as a marked point process, based on\nwhich we propose a continuous-time dynamic choice model. The proposed model can\nserve as a measurement model for scaling students along the latent traits of\nproblem-solving competency and action speed, based on data from one or multiple\ntasks. A real data example is given based on data from Program for\nInternational Student Assessment 2012.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:26:27 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 10:06:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chen", "Yunxiao", ""]]}, {"id": "1912.11409", "submitter": "Thomas McAndrew PhD", "authors": "Thomas McAndrew, Nutcha Wattanachit, G. Casey Gibson, Nicholas G.\n  Reich", "title": "Aggregating predictions from experts: a scoping review of statistical\n  methods, experiments, and applications", "comments": "https://github.com/tomcm39/AggregatingExpertElicitedDataForPrediction\n  v0.2: updated funding info", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasts support decision making in a variety of applications. Statistical\nmodels can produce accurate forecasts given abundant training data, but when\ndata is sparse, rapidly changing, or unavailable, statistical models may not be\nable to make accurate predictions. Expert judgmental forecasts---models that\ncombine expert-generated predictions into a single forecast---can make\npredictions when training data is limited by relying on expert intuition to\ntake the place of concrete training data. Researchers have proposed a wide\narray of algorithms to combine expert predictions into a single forecast, but\nthere is no consensus on an optimal aggregation model. This scoping review\nsurveyed recent literature on aggregating expert-elicited predictions. We\ngathered common terminology, aggregation methods, and forecasting performance\nmetrics, and offer guidance to strengthen future work that is growing at an\naccelerated pace.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:07:27 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 20:41:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["McAndrew", "Thomas", ""], ["Wattanachit", "Nutcha", ""], ["Gibson", "G. Casey", ""], ["Reich", "Nicholas G.", ""]]}, {"id": "1912.11652", "submitter": "Yuru Zhu", "authors": "Shinyuu Lee and Yuru Zhu", "title": "Confounder Selection via Support Intersection", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confounding matters in almost all observational studies that focus on\ncausality. In order to eliminate bias caused by connfounders, oftentimes a\nsubstantial number of features need to be collected in the analysis. In this\ncase, large p small n problem can arise and dimensional reduction technique is\nrequired. However, the traditional variable selection methods which focus on\nprediction are problematic in this setting. Throughout this paper, we analyze\nthis issue in detail and assume the sparsity of confounders which is different\nfrom the previous works. Under this assumption we propose several variable\nselection methods based on support intersection to pick out the confounders.\nAlso we discussed the different approaches for estimation of causal effect and\nunconfoundedness test. To aid in our description, finally we provide numerical\nsimulations to support our claims and compare to common heuristic methods, as\nwell as applications on real dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:23:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lee", "Shinyuu", ""], ["Zhu", "Yuru", ""]]}, {"id": "1912.11762", "submitter": "Rory Bunker", "authors": "Rory Bunker (1), Teo Susnjak (2) ((1) Nagoya Institute of Technology,\n  Japan, (2) Massey University, Auckland, New Zealand)", "title": "The Application of Machine Learning Techniques for Predicting Results in\n  Team Sport: A Review", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, Machine Learning (ML) techniques have been\nincreasingly utilized for the purpose of predicting outcomes in sport. In this\npaper, we provide a review of studies that have used ML for predicting results\nin team sport, covering studies from 1996 to 2019. We sought to answer five key\nresearch questions while extensively surveying papers in this field. This paper\noffers insights into which ML algorithms have tended to be used in this field,\nas well as those that are beginning to emerge with successful outcomes. Our\nresearch highlights defining characteristics of successful studies and\nidentifies robust strategies for evaluating accuracy results in this\napplication domain. Our study considers accuracies that have been achieved\nacross different sports and explores the notion that outcomes of some team\nsports could be inherently more difficult to predict than others. Finally, our\nstudy uncovers common themes of future research directions across all surveyed\npapers, looking for gaps and opportunities, while proposing recommendations for\nfuture researchers in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 03:12:21 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Bunker", "Rory", ""], ["Susnjak", "Teo", ""]]}, {"id": "1912.11801", "submitter": "Georgios Papayiannis", "authors": "G. Domazakis, D. Drivaliaris, S. Koukoulas, G. Papayiannis, A.\n  Tsekrekos, A. Yannacopoulos", "title": "Clustering measure-valued data with Wasserstein barycenters", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, learning schemes for measure-valued data are proposed, i.e.\ndata that their structure can be more efficiently represented as probability\nmeasures instead of points on $\\R^d$, employing the concept of probability\nbarycenters as defined with respect to the Wasserstein metric. Such type of\nlearning approaches are highly appreciated in many fields where the\nobservational/experimental error is significant (e.g. astronomy, biology,\nremote sensing, etc.) or the data nature is more complex and the traditional\nlearning algorithms are not applicable or effective to treat them (e.g. network\ndata, interval data, high frequency records, matrix data, etc.). Under this\nperspective, each observation is identified by an appropriate probability\nmeasure and the proposed statistical learning schemes rely on discrimination\ncriteria that utilize the geometric structure of the space of probability\nmeasures through core techniques from the optimal transport theory. The\ndiscussed approaches are implemented in two real world applications: (a)\nclustering eurozone countries according to their observed government bond yield\ncurves and (b) classifying the areas of a satellite image to certain land uses\ncategories which is a standard task in remote sensing. In both case studies the\nresults are particularly interesting and meaningful while the accuracy obtained\nis high.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:46:00 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 10:04:04 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Domazakis", "G.", ""], ["Drivaliaris", "D.", ""], ["Koukoulas", "S.", ""], ["Papayiannis", "G.", ""], ["Tsekrekos", "A.", ""], ["Yannacopoulos", "A.", ""]]}, {"id": "1912.11827", "submitter": "David Ginsbourger", "authors": "Lea Friedli, David Ginsbourger, Jonas Bhend", "title": "Area-covering postprocessing of ensemble precipitation forecasts using\n  topographical and seasonal conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic weather forecasts from ensemble systems require statistical\npostprocessing to yield calibrated and sharp predictive distributions. This\npaper presents an area-covering postprocessing method for ensemble\nprecipitation predictions. We rely on the ensemble model output statistics\n(EMOS) approach, which generates probabilistic forecasts with a parametric\ndistribution whose parameters depend on (statistics of) the ensemble\nprediction. A case study with daily precipitation predictions across\nSwitzerland highlights that postprocessing at observation locations indeed\nimproves high-resolution ensemble forecasts, with 4.5% CRPS reduction on\naverage in the case of a lead time of 1 day. Our main aim is to achieve such an\nimprovement without binding the model to stations, by leveraging topographical\ncovariates. Specifically, regression coefficients are estimated by weighting\nthe training data in relation to the topographical similarity between their\nstation of origin and the prediction location. In our case study, this approach\nis found to reproduce the performance of the local model without using local\nhistorical data for calibration. We further identify that one key difficulty is\nthat postprocessing often degrades the performance of the ensemble forecast\nduring summer and early autumn. To mitigate, we additionally estimate on the\ntraining set whether postprocessing at a specific location is expected to\nimprove the prediction. If not, the direct model output is used. This extension\nreduces the CRPS of the topographical model by up to another 1.7% on average at\nthe price of a slight degradation in calibration. In this case, the highest\nimprovement is achieved for a lead time of 4 days.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 10:24:32 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:16:38 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 12:33:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Friedli", "Lea", ""], ["Ginsbourger", "David", ""], ["Bhend", "Jonas", ""]]}, {"id": "1912.11833", "submitter": "Yuxiao Li", "authors": "Yuxiao Li and Ying Sun", "title": "A Multi-Site Stochastic Weather Generator for High-Frequency\n  Precipitation Using Censored Skew-Symmetric Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic weather generators (SWGs) are digital twins of complex weather\nprocesses and widely used in agriculture and urban design. Due to improved\nmeasuring instruments, an accurate SWG for high-frequency precipitation is now\npossible. However, high-frequency precipitation data are more zero-inflated,\nskewed, and heavy-tailed than common (hourly or daily) precipitation data.\nTherefore, classical methods that either model precipitation occurrence\nindependently of their intensity or assume that the precipitation follows a\ncensored meta-Gaussian process may not be appropriate. In this work, we propose\na novel multi-site precipitation generator that drives both occurrence and\nintensity by a censored non-Gaussian vector autoregression model with\nskew-symmetric dynamics. The proposed SWG is advantageous in modeling skewed\nand heavy-tailed data with direct physical and statistical interpretations. We\napply the proposed model to 30-second precipitation based on the data obtained\nfrom a dense gauge network in Lausanne, Switzerland. In addition to reproducing\nthe high-frequency precipitation, the model can provide accurate predictions as\nthe long short-term memory (LSTM) network but with uncertainties and more\ninterpretable results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 11:02:59 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 13:08:59 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Yuxiao", ""], ["Sun", "Ying", ""]]}, {"id": "1912.11848", "submitter": "Andreas Kryger Jensen", "authors": "Andreas Kryger Jensen and Claus Thorn Ekstr{\\o}m", "title": "Quantifying the Trendiness of Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News media often report that the trend of some public health outcome has\nchanged. These statements are frequently based on longitudinal data, and the\nchange in trend is typically found to have occurred at the most recent data\ncollection time point - if no change had occurred the story is less likely to\nbe reported. Such claims may potentially influence public health decisions on a\nnational level.\n  We propose two measures for quantifying the trendiness of trends. Assuming\nthat reality evolves in continuous time we define what constitutes a trend and\na change in trend, and introduce a probabilistic Trend Direction Index. This\nindex has the interpretation of the probability that a latent characteristic\nhas changed monotonicity at any given time conditional on observed data. We\nalso define an index of Expected Trend Instability quantifying the expected\nnumber of changes in trend on an interval.\n  Using a latent Gaussian Process model we show how the Trend Direction Index\nand the Expected Trend Instability can be estimated in a Bayesian framework and\nuse the methods to analyze the proportion of smokers in Denmark during the last\n20 years, and the development of new COVID-19 cases in Italy from February 24th\nonwards.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:05:03 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 20:09:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jensen", "Andreas Kryger", ""], ["Ekstr\u00f8m", "Claus Thorn", ""]]}, {"id": "1912.12116", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou, Cecilia Turino, Alexander Steblin, Manuel\n  S\\'anchez-de-la-Torre, Ferran Barb\\'e, Eloisa Vargiu", "title": "Comparative Analysis of Predictive Methods for Early Assessment of\n  Compliance with Continuous Positive Airway Pressure Therapy", "comments": "22 pages, 11 figures", "journal-ref": "BMC Medical Informatics and Decision Making. 81 (2018)", "doi": "10.1186/s12911-018-0657-z", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients suffering from obstructive sleep apnea are mainly treated with\ncontinuous positive airway pressure (CPAP). Good compliance with this therapy\nis broadly accepted as more than 4h of CPAP average use nightly. Although it is\na highly effective treatment, compliance with this therapy is problematic to\nachieve with serious consequences for the patients' health. Previous works\nalready reported factors significantly related to compliance with the therapy.\nHowever, further research is still required to support clinicians to early\nanticipate patients' therapy compliance. This work intends to take a further\nstep in this direction by building compliance classifiers with CPAP therapy at\nthree different moments of the patient follow-up (i.e. before the therapy\nstarts and at months 1 and 3 after the baseline). Results of the clinical trial\nconfirmed that month 3 was the time-point with the most accurate classifier\nreaching an f1-score of 87% and 84% in cross-validation and test. At month 1,\nperformances were almost as high as in month 3 with 82% and 84% of f1-score. At\nbaseline, where no information about patients' CPAP use was given yet, the best\nclassifier achieved 73% and 76% of f1-score in cross-validation and test set\nrespectively. Subsequent analyses carried out with the best classifiers of each\ntime point revealed that certain baseline factors (i.e. headaches,\npsychological symptoms, arterial hypertension and EuroQol visual analogue\nscale) were closely related to the prediction of compliance independently of\nthe time-point. In addition, among the variables taken only during the\nfollow-up of the patients, Epworth and the average nighttime hours were the\nmost important to predict compliance with CPAP.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 14:44:21 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Rafael-Palou", "Xavier", ""], ["Turino", "Cecilia", ""], ["Steblin", "Alexander", ""], ["S\u00e1nchez-de-la-Torre", "Manuel", ""], ["Barb\u00e9", "Ferran", ""], ["Vargiu", "Eloisa", ""]]}, {"id": "1912.12228", "submitter": "Kelly Moran", "authors": "Kelly R. Moran, David Dunson, Matthew W. Wheeler, and Amy H. Herring", "title": "Bayesian joint modeling of chemical structure and dose response curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today there are approximately 85,000 chemicals regulated under the Toxic\nSubstances Control Act, with around 2,000 new chemicals introduced each year.\nIt is impossible to screen all of these chemicals for potential toxic effects\neither via full organism in vivo studies or in vitro high-throughput screening\n(HTS) programs. Toxicologists face the challenge of choosing which chemicals to\nscreen, and predicting the toxicity of as-yet-unscreened chemicals. Our goal is\nto describe how variation in chemical structure relates to variation in\ntoxicological response to enable in silico toxicity characterization designed\nto meet both of these challenges. With our Bayesian partially Supervised Sparse\nand Smooth Factor Analysis ($\\text{BS}^3\\text{FA}$) model, we learn a distance\nbetween chemicals targeted to toxicity, rather than one based on molecular\nstructure alone. Our model also enables the prediction of chemical\ndose-response profiles based on chemical structure (that is, without in vivo or\nin vitro testing) by taking advantage of a large database of chemicals that\nhave already been tested for toxicity in HTS programs. We show superior\nsimulation performance in distance learning and modest to large gains in\npredictive ability compared to existing methods. Results from the\nhigh-throughput screening data application elucidate the relationship between\nchemical structure and a toxicity-relevant high-throughput assay. An R package\nfor $\\text{BS}^3\\text{FA}$ is available online at\nhttps://github.com/kelrenmor/bs3fa.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:41:25 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 23:09:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Moran", "Kelly R.", ""], ["Dunson", "David", ""], ["Wheeler", "Matthew W.", ""], ["Herring", "Amy H.", ""]]}, {"id": "1912.12274", "submitter": "Juan Manuel Gorriz Saez Juan M", "authors": "J M Gorriz, SiPBA Group, and CAM neuroscience", "title": "Statistical Agnostic Mapping: a Framework in Neuroimaging based on\n  Concentration Inequalities", "comments": "18 pages, 10 figures, prepared to be submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 70s a novel branch of statistics emerged focusing its effort in\nselecting a function in the pattern recognition problem, which fulfils a\ndefinite relationship between the quality of the approximation and its\ncomplexity. These data-driven approaches are mainly devoted to problems of\nestimating dependencies with limited sample sizes and comprise all the\nempirical out-of sample generalization approaches, e.g. cross validation (CV)\napproaches. Although the latter are \\emph{not designed for testing competing\nhypothesis or comparing different models} in neuroimaging, there are a number\nof theoretical developments within this theory which could be employed to\nderive a Statistical Agnostic (non-parametric) Mapping (SAM) at voxel or\nmulti-voxel level. Moreover, SAMs could relieve i) the problem of instability\nin limited sample sizes when estimating the actual risk via the CV approaches,\ne.g. large error bars, and provide ii) an alternative way of Family-wise-error\n(FWE) corrected p-value maps in inferential statistics for hypothesis testing.\nIn this sense, we propose a novel framework in neuroimaging based on\nconcentration inequalities, which results in (i) a rigorous development for\nmodel validation with a small sample/dimension ratio, and (ii) a\nless-conservative procedure than FWE p-value correction, to determine the brain\nsignificance maps from the inferences made using small upper bounds of the\nactual risk.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:27:50 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gorriz", "J M", ""], ["Group", "SiPBA", ""], ["neuroscience", "CAM", ""]]}, {"id": "1912.12275", "submitter": "Vivianne Olgu\\'in-Arias", "authors": "Vivianne Olgu\\'in-Arias, Sergio Davis, and Gonzalo Guti\\'errez", "title": "A general statistical model for waiting times until collapse of a system", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2020.125198", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of waiting times until the occurrence of a critical event is\na crucial statistical problem across several disciplines in Science. In this\nwork we present a statistical model in which a relevant quantity X accumulates\nuntil overcoming a threshold X*, which defines the collapse. The obtained\nwaiting time distribution is a mixture of gamma distributions, which in turn\ncan be approximated as an effective gamma distribution.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:36:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Olgu\u00edn-Arias", "Vivianne", ""], ["Davis", "Sergio", ""], ["Guti\u00e9rrez", "Gonzalo", ""]]}, {"id": "1912.12356", "submitter": "Aritra Halder", "authors": "Aritra Halder, Shariq Mohammed, Kun Chen and Dipak Dey", "title": "Spatial risk estimation in Tweedie compound Poisson double generalized\n  linear models", "comments": "34 pages, 10 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie exponential dispersion family constitutes a fairly rich sub-class of\nthe celebrated exponential family. In particular, a member, compound Poisson\ngamma (CP-g) model has seen extensive use over the past decade for modeling\nmixed response featuring exact zeros with a continuous response from a gamma\ndistribution. This paper proposes a framework to perform residual analysis on\nCP-g double generalized linear models for spatial uncertainty quantification.\nApproximations are introduced to proposed framework making the procedure\nscalable, without compromise in accuracy of estimation and model complexity;\naccompanied by sensitivity analysis to model mis-specification. Proposed\nframework is applied to modeling spatial uncertainty in insurance loss costs\narising from automobile collision coverage. Scalability is demonstrated by\nchoosing sizable spatial reference domains comprised of groups of states within\nthe United States of America.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:25:53 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:43:00 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Halder", "Aritra", ""], ["Mohammed", "Shariq", ""], ["Chen", "Kun", ""], ["Dey", "Dipak", ""]]}, {"id": "1912.12569", "submitter": "Yan Wang", "authors": "Yan Wang, Xiaowei Yue, Rui Tuo, Jeffrey H. Hunt and Jianjun Shi", "title": "Effective Model Calibration via Sensible Variable Identification and\n  Adjustment, with Application to Composite Fuselage Simulation", "comments": "25 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of model parameters of computer simulators, also known as\ncalibration, is an important topic in many engineering applications. In this\npaper, we consider the calibration of computer model parameters with the help\nof engineering design knowledge. We introduce the concept of sensible\n(calibration) variables. Sensible variables are model parameters which are\nsensitive in the engineering modeling, and whose optimal values differ from the\nengineering design values.We propose an effective calibration method to\nidentify and adjust the sensible variables with limited physical experimental\ndata. The methodology is applied to a composite fuselage simulation problem.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 02:32:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wang", "Yan", ""], ["Yue", "Xiaowei", ""], ["Tuo", "Rui", ""], ["Hunt", "Jeffrey H.", ""], ["Shi", "Jianjun", ""]]}, {"id": "1912.12571", "submitter": "David Frazier", "authors": "Ruben Loaiza-Maya, Gael M. Martin, and David T. Frazier", "title": "Focused Bayesian Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for conducting Bayesian prediction that delivers\naccurate predictions without correctly specifying the unknown true data\ngenerating process. A prior is defined over a class of plausible predictive\nmodels. After observing data, we update the prior to a posterior over these\nmodels, via a criterion that captures a user-specified measure of predictive\naccuracy. Under regularity, this update yields posterior concentration onto the\nelement of the predictive class that maximizes the expectation of the accuracy\nmeasure. In a series of simulation experiments and empirical examples we find\nnotable gains in predictive accuracy relative to conventional likelihood-based\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 02:38:13 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 04:59:57 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""], ["Frazier", "David T.", ""]]}, {"id": "1912.12738", "submitter": "Nancy Ronquillo", "authors": "Nancy Ronquillo, Sung-En Chiu and Tara Javidi", "title": "Sequential Learning of CSI for MmWave Initial Alignment", "comments": "To be published in the 53nd Asilomar Conference on Signals, Systems\n  and Computers 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MmWave communications aim to meet the demand for higher data rates by using\nhighly directional beams with access to larger bandwidth. An inherent challenge\nis acquiring channel state information (CSI) necessary for mmWave transmission.\nWe consider the problem of adaptive and sequential learning of the CSI during\nthe mmWave initial alignment phase of communication. We focus on the\nsingle-user with a single dominant path scenario where the problem is\nequivalent to acquiring an optimal beamforming vector, where ideally, the\nresulting beams point in the direction of the angle of arrival with the desired\nresolution. We extend our prior by proposing two algorithms for adaptively and\nsequentially selecting beamforming vectors for learning of the CSI, and that\nformulate a Bayesian update to account for the time-varying fading model.\nNumerically, we analyze the outage probability and expected spectral efficiency\nof our proposed algorithms and demonstrate improvements over strategies that\nutilize a practical hierarchical codebook.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:31:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ronquillo", "Nancy", ""], ["Chiu", "Sung-En", ""], ["Javidi", "Tara", ""]]}, {"id": "1912.12755", "submitter": "Eugene Geis", "authors": "Eugene Geis", "title": "Stochastic Approximation EM for Exploratory Item Factor Analysis", "comments": "131 pages, 57 figures, A dissertation for completion of PhD in\n  psychometrics at Rutgers Graduate School of Education", "journal-ref": "Statistics in Medicine, Volume 38, Issue 21, page 3997 (2019)", "doi": "10.7282/t3-7k3j-6x67 10.1002/sim.8217", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic approximation EM algorithm (SAEM) is described for the\nestimation of item and person parameters given test data coded as dichotomous\nor ordinal variables. The method hinges upon the eigenanalysis of missing\nvariables sampled as augmented data; the augmented data approach was introduced\nby Albert's seminal work applying Gibbs sampling to Item Response Theory in\n1992. Similar to maximum likelihood factor analysis, the factor structure in\nthis Bayesian approach depends only on sufficient statistics, which are\ncomputed from the missing latent data. A second feature of the SAEM algorithm\nis the use of the Robbins-Monro procedure for establishing convergence.\nContrary to Expectation Maximization methods where costly integrals must be\ncalculated, this method is well-suited for highly multidimensional data, and an\nannealing method is implemented to prevent convergence to a local maximum\nlikelihood. Multiple calculations of errors applied within this framework of\nMarkov Chain Monte Carlo are presented to delineate the uncertainty of\nparameter estimates. Given the nature of EFA (exploratory factor analysis), an\nalgorithm is formalized leveraging the Tracy-Widom distribution for the\nretention of factors extracted from an eigenanalysis of the sufficient\nstatistic of the covariance of the augmented data matrix. Simulation conditions\nof dichotomous and polytomous data, from one to ten dimensions of factor\nloadings, are used to assess statistical accuracy and to gauge computational\ntime of the EFA approach of this IRT-specific implementation of the SAEM\nalgorithm. Finally, three applications of this methodology are also reported\nthat demonstrate the effectiveness of the method for enabling timely analyses\nas well as substantive interpretations when this method is applied to real\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 23:09:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Geis", "Eugene", ""]]}, {"id": "1912.13188", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah and Aarti Singh", "title": "On Testing for Biases in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of biases in scholarly research, specifically, in peer\nreview. There is a long standing debate on whether exposing author identities\nto reviewers induces biases against certain groups, and our focus is on\ndesigning tests to detect the presence of such biases. Our starting point is a\nremarkable recent work by Tomkins, Zhang and Heavlin which conducted a\ncontrolled, large-scale experiment to investigate existence of biases in the\npeer reviewing of the WSDM conference. We present two sets of results in this\npaper. The first set of results is negative, and pertains to the statistical\ntests and the experimental setup used in the work of Tomkins et al. We show\nthat the test employed therein does not guarantee control over false alarm\nprobability and under correlations between relevant variables coupled with any\nof the following conditions, with high probability, can declare a presence of\nbias when it is in fact absent: (a) measurement error, (b) model mismatch, (c)\nreviewer calibration. Moreover, we show that the setup of their experiment may\nitself inflate false alarm probability if (d) bidding is performed in non-blind\nmanner or (e) popular reviewer assignment procedure is employed. Our second set\nof results is positive and is built around a novel approach to testing for\nbiases that we propose. We present a general framework for testing for biases\nin (single vs. double blind) peer review. We then design hypothesis tests that\nunder minimal assumptions guarantee control over false alarm probability and\nnon-trivial power even under conditions (a)--(c) as well as propose an\nalternative experimental setup which mitigates issues (d) and (e). Finally, we\nshow that no statistical test can improve over the non-parametric tests we\nconsider in terms of the assumptions required to control for the false alarm\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:17:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""]]}, {"id": "1912.13242", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison, Ewald Enzinger, Daniel Ramos, Joaqu\\'in\n  Gonz\\'alez-Rodr\\'iguez, Alicia Lozano-D\\'iez", "title": "Statistical Models in Forensic Voice Comparison", "comments": "Morrison, G.S., Enzinger, E., Ramos, D., Gonz\\'alez-Rodr\\'iguez, J.,\n  Lozano-D\\'iez, A. (2020). Statistical models in forensic voice comparison. In\n  Banks, D.L., Kafadar, K., Kaye, D.H., Tackett, M. (Eds.) Handbook of Forensic\n  Statistics (Ch. 20, pp. 451-497). Boca Raton, FL: CRC", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This chapter describes a number of signal-processing and statistical-modeling\ntechniques that are commonly used to calculate likelihood ratios in\nhuman-supervised automatic approaches to forensic voice comparison. Techniques\ndescribed include mel-frequency cepstral coefficients (MFCCs) feature\nextraction, Gaussian mixture model - universal background model (GMM-UBM)\nsystems, i-vector - probabilistic linear discriminant analysis (i-vector PLDA)\nsystems, deep neural network (DNN) based systems (including senone posterior\ni-vectors, bottleneck features, and embeddings / x-vectors), mismatch\ncompensation, and score-to-likelihood-ratio conversion (aka calibration).\nEmpirical validation of forensic-voice-comparison systems is also covered. The\naim of the chapter is to bridge the gap between general introductions to\nforensic voice comparison and the highly technical\nautomatic-speaker-recognition literature from which the signal-processing and\nstatistical-modeling techniques are mostly drawn. Knowledge of the\nlikelihood-ratio framework for the evaluation of forensic evidence is assumed.\nIt is hoped that the material presented here will be of value to students of\nforensic voice comparison and to researchers interested in learning about\nstatistical modeling techniques that could potentially also be applied to data\nfrom other branches of forensic science.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 09:45:05 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 12:44:41 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""], ["Enzinger", "Ewald", ""], ["Ramos", "Daniel", ""], ["Gonz\u00e1lez-Rodr\u00edguez", "Joaqu\u00edn", ""], ["Lozano-D\u00edez", "Alicia", ""]]}, {"id": "1912.13336", "submitter": "Melina Freitag", "authors": "Melina A. Freitag", "title": "Numerical Linear Algebra in Data Assimilation", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation is a method that combines observations (that is, real world\ndata) of a state of a system with model output for that system in order to\nimprove the estimate of the state of the system and thereby the model output.\nThe model is usually represented by a discretised partial differential\nequation. The data assimilation problem can be formulated as a large scale\nBayesian inverse problem. Based on this interpretation we will derive the most\nimportant variational and sequential data assimilation approaches, in\nparticular three-dimensional and four-dimensional variational data assimilation\n(3D-Var and 4D-Var) and the Kalman filter. We will then consider more advanced\nmethods which are extensions of the Kalman filter and variational data\nassimilation and pay particular attention to their advantages and\ndisadvantages. The data assimilation problem usually results in a very large\noptimisation problem and/or a very large linear system to solve (due to\ninclusion of time and space dimensions). Therefore, the second part of this\narticle aims to review advances and challenges, in particular from the\nnumerical linear algebra perspective, within the various data assimilation\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:04:07 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:51:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Freitag", "Melina A.", ""]]}, {"id": "1912.13351", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rincon, Maria Eugenia Fontecha, Carlos D'Giano", "title": "Driver fatigue EEG signals detection by using robust univariate analysis", "comments": "9 pages, 4 figures, 1 table, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver fatigue is a major cause of traffic accidents and the\nelectroencephalogram (EEG) is considered one of the most reliable predictors of\nfatigue. This paper proposes a novel, simple and fast method for driver fatigue\ndetection that can be implemented in real-time systems by using a\nsingle-channel on the scalp. The method based on the robust univariate analysis\nof EEG signals is composed of two stages. First, the most significant channel\nfrom EEG raw is selected according to the maximum variance. In the second\nstage, this single channel will be used to detect the fatigue EEG signal by\nextracting four feature parameters. Two parameters estimated from the robust\nunivariate analysis, namely mean and covariance, and two classical statistics\nparameters such as variance and covariance that help to tune the robust\nanalysis. Next, an ensemble bagged decision trees classifier is used in order\nto discriminate fatigue signals from alert signals. The proposed algorithm is\ndemonstrated on 24 EEG signals from the Jiangxi University of Technology\ndatabase using only the most significant channel found, which is located in the\nleft tempo-parietal region where spatial awareness and visual-spatial\nnavigation are shared, in terms of 92.7% accuracy with 1.8 seconds of time\ndelay.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:22:24 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Quintero-Rincon", "Antonio", ""], ["Fontecha", "Maria Eugenia", ""], ["D'Giano", "Carlos", ""]]}]