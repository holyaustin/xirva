[{"id": "1604.00055", "submitter": "Erik Gjesfjeld", "authors": "Erik Gjesfjeld, Jonathan Chang, Daniele Silvestro, Christopher Kelty\n  and Michael Alfaro", "title": "Competition and extinction explain the evolution of diversity in\n  American automobiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most remarkable aspects of our species is that while we show\nsurprisingly little genetic diversity, we demonstrate astonishing amounts of\ncultural diversity. Perhaps most impressive is the diversity of our\ntechnologies, broadly defined as all the physical objects we produce and the\nskills we use to produce them. Despite considerable focus on the evolution of\ntechnology by social scientists and philosophers, there have been few attempts\nto systematically quantify technological diversity and therefore the dynamics\nof technological change remain poorly understood. Here we show a novel Bayesian\nmodel for examining technological diversification adopted from paleontological\nanalysis of occurrence data. We use this framework to estimate the tempo of\ndiversification in American car and truck models produced between 1896 and 2014\nand to test the relative importance of competition and extrinsic factors in\nshaping changes in macroevolutionary rates. Our results identify a four-fold\ndecrease in the origination and extinction rates of car models and a negative\nnet diversification rate over the last thirty years. We also demonstrate that\ncompetition played a more significant role in car model diversification than\neither changes in oil prices or gross domestic product. Together our analyses\nprovide a set of tools that can enhance current research on technological and\ncultural evolution by providing a flexible and quantitative framework for\nexploring the dynamics of diversification.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 21:26:17 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 15:11:06 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gjesfjeld", "Erik", ""], ["Chang", "Jonathan", ""], ["Silvestro", "Daniele", ""], ["Kelty", "Christopher", ""], ["Alfaro", "Michael", ""]]}, {"id": "1604.00059", "submitter": "Zhen Zhang Dr", "authors": "Zhen Zhang, Chae Young Lim, Tapabrata Maiti and Seiji Kato", "title": "Spatial Clustering of Curves with Functional Covariates: A Bayesian\n  Partitioning Model with Application to Spectra Radiance in Climate Study", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In climate change study, the infrared spectral signatures of climate change\nhave recently been conceptually adopted, and widely applied to identifying and\nattributing atmospheric composition change. We propose a Bayesian hierarchical\nmodel for spatial clustering of the high-dimensional functional data based on\nthe effects of functional covariates and local features. We couple the\nfunctional mixed-effects model with a generalized spatial partitioning method\nfor: (1) producing spatially contiguous clusters for the high-dimensional\nspatio-functional data; (2) improving the computational efficiency via parallel\ncomputing over subregions or multi-level partitions; and (3) capturing the\nnear-boundary ambiguity and uncertainty for data-driven partitions. We propose\na generalized partitioning method which puts less constraints on the shape of\nspatial clusters. Dimension reduction in the parameter space is also achieved\nvia Bayesian wavelets to alleviate the increasing model complexity introduced\nby clusters. The model well captures the regional effects of the atmospheric\nand cloud properties on the spectral radiance measurements. The results\nelaborate the importance of exploiting spatially contiguous partitions for\nidentifying regional effects and small-scale variability.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 19:15:17 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Zhang", "Zhen", ""], ["Lim", "Chae Young", ""], ["Maiti", "Tapabrata", ""], ["Kato", "Seiji", ""]]}, {"id": "1604.00082", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez", "title": "A track-before-detect labelled multi-Bernoulli particle filter with\n  label switching", "comments": "Accepted for publication in IEEE Transactions on Aerospace and\n  Electronic Systems", "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, vol. 52,\n  no. 5, pp. 2123-2138, October 2016", "doi": "10.1109/TAES.2016.150343", "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multitarget tracking particle filter (PF) for general\ntrack-before-detect measurement models. The PF is presented in the random\nfinite set framework and uses a labelled multi-Bernoulli approximation. We also\npresent a label switching improvement algorithm based on Markov chain Monte\nCarlo that is expected to increase filter performance if targets get in close\nproximity for a sufficiently long time. The PF is tested in two challenging\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 23:51:02 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""]]}, {"id": "1604.00572", "submitter": "Camila Casquilho-Resende", "authors": "Camila M. Casquilho-Resende, Nhu D. Le, James V. Zidek", "title": "Spatio-temporal Modelling of Temperature Fields in the Pacific Northwest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of modelling temperature fields goes beyond the need to\nunderstand a region's climate and serves too as a starting point for\nunderstanding their socioeconomic, and health consequences. The topography of\nthe study region contributes much to the complexity of modelling these fields\nand demands flexible spatio-temporal models that are able to handle\nnonstationarity and changes in trend. In this paper, we develop a flexible\nstochastic spatio-temporal model for daily temperatures in the Pacific\nNorthwest, and describe a methodology for performing Bayesian spatial\nprediction. A novel aspect of this model, an extension of the spatio-temporal\nmodel proposed in Le and Zidek (1992), is its incorporation of site-specific\nfeatures of a spatio-temporal field in its spatio-temporal mean. Due to the\noften surprising Pacific Northwestern weather, the analysis reported in the\npaper shows the need to incorporate spatio-temporal interactions in that mean\nin order to understand the rapid changes in temperature observed in nearby\nlocations and to get approximately stationary residuals for higher level\nanalysis. No structure is assumed for the spatial covariance matrix of these\nresiduals, thus allowing the model to capture any nonstationary spatial\nstructures remaining in those residuals.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 23:08:31 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Casquilho-Resende", "Camila M.", ""], ["Le", "Nhu D.", ""], ["Zidek", "James V.", ""]]}, {"id": "1604.00627", "submitter": "Alexander Gorban", "authors": "E.M. Mirkes, T.J. Coats, J. Levesley, A.N. Gorban", "title": "Handling missing data in large healthcare dataset: a case study of\n  unknown trauma outcomes", "comments": "Minor editing and additions", "journal-ref": "Computers in Biology and Medicine, 75 (2016) 203-216", "doi": "10.1016/j.compbiomed.2016.06.004", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handling of missed data is one of the main tasks in data preprocessing\nespecially in large public service datasets. We have analysed data from the\nTrauma Audit and Research Network (TARN) database, the largest trauma database\nin Europe. For the analysis we used 165,559 trauma cases. Among them, there are\n19,289 cases (13.19\\%) with unknown outcome. We have demonstrated that these\noutcomes are not missed `completely at random' and, hence, it is impossible\njust to exclude these cases from analysis despite the large amount of available\ndata. We have developed a system of non-stationary Markov models for the\nhandling of missed outcomes and validated these models on the data of 15,437\npatients which arrived into TARN hospitals later than 24 hours but within 30\ndays from injury. We used these Markov models for the analysis of mortality. In\nparticular, we corrected the observed fraction of death. Two na\\\"ive approaches\ngive 7.20\\% (available case study) or 6.36\\% (if we assume that all unknown\noutcomes are `alive'). The corrected value is 6.78\\%. Following the seminal\npaper of Trunkey (1983) the multimodality of mortality curves has become a much\ndiscussed idea. For the whole analysed TARN dataset the coefficient of\nmortality monotonically decreases in time but the stratified analysis of the\nmortality gives a different result: for lower severities the coefficient of\nmortality is a non-monotonic function of the time after injury and may have\nmaxima at the second and third weeks. The approach developed here can be\napplied to various healthcare datasets which experience the problem of lost\npatients and missed outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 12:15:56 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 12:33:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mirkes", "E. M.", ""], ["Coats", "T. J.", ""], ["Levesley", "J.", ""], ["Gorban", "A. N.", ""]]}, {"id": "1604.00698", "submitter": "Peng Ding", "authors": "Xinran Li and Peng Ding and Donald B. Rubin", "title": "Asymptotic Theory of Rerandomization in Treatment-Control Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although complete randomization ensures covariate balance on average, the\nchance for observing significant differences between treatment and control\ncovariate distributions increases with many covariates. Rerandomization\ndiscards randomizations that do not satisfy a predetermined covariate balance\ncriterion, generally resulting in better covariate balance and more precise\nestimates of causal effects. Previous theory has derived finite sample theory\nfor rerandomization under the assumptions of equal treatment group sizes,\nGaussian covariate and outcome distributions, or additive causal effects, but\nnot for the general sampling distribution of the difference-in-means estimator\nfor the average causal effect. To supplement existing results, we develop\nasymptotic theory for rerandomization without these assumptions, which reveals\na non-Gaussian asymptotic distribution for this estimator, specifically a\nlinear combination of a Gaussian random variable and a truncated Gaussian\nrandom variable. This distribution follows because rerandomization affects only\nthe projection of potential outcomes onto the covariate space but does not\naffect the corresponding orthogonal residuals. We also demonstrate that,\ncompared to complete randomization, rerandomization reduces the asymptotic\nsampling variances and quantile ranges of the difference-in-means estimator.\nMoreover, our work allows the construction of accurate large-sample confidence\nintervals for the average causal effect, thereby revealing further advantages\nof rerandomization over complete randomization.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 22:56:09 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 02:24:03 GMT"}, {"version": "v3", "created": "Sat, 11 Feb 2017 08:01:19 GMT"}, {"version": "v4", "created": "Sat, 12 Aug 2017 02:16:13 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1604.00823", "submitter": "Allan Buras", "authors": "Barnim Thees, Allan Buras, Gottfried Jetschke, Eduardo Zorita, Martin\n  Wilmking, Volkmar Liebscher, and Lars Kutzbach", "title": "SINOMA - A new approach for estimating linear relationships between\n  noisy serial data streams", "comments": "52 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructions of past climates are based on the calibration of available\nproxy data. This calibration is usually achieved by means of linear regression\nmodels. In the recent paleo-climate literature there is an ongoing discussion\non the validity of highly resolved climate reconstructions. The reason for this\nis that the proxy data are noisy, i.e. in addition to the variability that is\nrelated to the climate variable of interest, they contain other sources of\nvariability. Inadequate treatment of such noise leads to a biased estimation of\nregression slopes, resulting in a wrong representation of the real amplitude of\npast climate variations. Methods to overcome this problem have had a limited\nsuccess so far. Here, we present a new approach - SINOMA - for noisy serial\ndata streams that are characterized by different spectral characteristics of\nsignal and noise. SINOMA makes use of specific properties of the data streams\ntemporal or spatial structure and by this is able to deliver a precise estimate\nof the true regression slope and, simultaneously, of the ratio of noise\nvariances present in the predictor and predictand. The paper introduces the\nunderlying mathematics as well as a general description of the presented\nalgorithm. The validity of SINOMA is illustrated with two test data-sets.\nFinally we address methodological limitations and further potential\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 11:45:28 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Thees", "Barnim", ""], ["Buras", "Allan", ""], ["Jetschke", "Gottfried", ""], ["Zorita", "Eduardo", ""], ["Wilmking", "Martin", ""], ["Liebscher", "Volkmar", ""], ["Kutzbach", "Lars", ""]]}, {"id": "1604.00912", "submitter": "Murat Bilgel", "authors": "Murat Bilgel, Jerry L. Prince, Dean F. Wong, Susan M. Resnick, Bruno\n  M. Jedynak", "title": "A multivariate nonlinear mixed effects model for longitudinal image\n  analysis: Application to amyloid imaging", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2016.04.001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is important to characterize the temporal trajectories of disease-related\nbiomarkers in order to monitor progression and identify potential points of\nintervention. This is especially important for neurodegenerative diseases, as\ntherapeutic intervention is most likely to be effective in the preclinical\ndisease stages prior to significant neuronal damage. Longitudinal neuroimaging\nallows for the measurement of structural, functional, and metabolic integrity\nof the brain over time at the level of voxels. However, commonly used\nlongitudinal analysis approaches, such as linear mixed effects models, do not\naccount for the fact that individuals enter a study at various disease stages\nand progress at different rates, and generally consider each voxelwise measure\nindependently. We propose a multivariate nonlinear mixed effects model for\nestimating the trajectories of voxelwise neuroimaging biomarkers from\nlongitudinal data that accounts for such differences across individuals. The\nmethod involves the prediction of a progression score for each visit based on a\ncollective analysis of voxelwise biomarker data within an\nexpectation-maximization framework that efficiently handles large amounts of\nmeasurements and variable number of visits per individual, and accounts for\nspatial correlations among voxels. This score allows individuals with similar\nprogressions to be aligned and analyzed together, which enables the\nconstruction of a trajectory of brain changes as a function of an underlying\nprogression or disease stage. Application of our method to studying images of\nbeta-amyloid deposition, a hallmark of preclinical Alzheimer's disease,\nsuggests that precuneus is the earliest cortical region to accumulate amyloid.\nThe proposed method can be applied to other types of longitudinal imaging data,\nincluding metabolism, blood flow, tau, and structural imaging-derived measures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:28:58 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Bilgel", "Murat", ""], ["Prince", "Jerry L.", ""], ["Wong", "Dean F.", ""], ["Resnick", "Susan M.", ""], ["Jedynak", "Bruno M.", ""]]}, {"id": "1604.01055", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Ross L Prentice, Brian M Bot, Mike Kellen, Stephen\n  H Friend, Andrew D Trister, Larsson Omberg, Lara Mangravite", "title": "Towards personalized causal inference of medication response in mobile\n  health: an instrumental variable approach for randomized trials with\n  imperfect compliance", "comments": "Main text, appendixes, and supplementary materials were re-organized", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile health studies can leverage longitudinal sensor data from smartphones\nto guide the application of personalized medical interventions. In this paper,\nwe propose that adoption of an instrumental variable approach for randomized\ntrials with imperfect compliance provides a natural framework for personalized\ncausal inference of medication response in mobile health studies. Randomized\ntreatment suggestions can be easily delivered to the study participants via\nelectronic messages popping up on the smart-phone screen. Under quite general\nassumptions we can identify the causal effect of the actual treatment on the\nresponse in the presence of unobserved confounders. We implement a personalized\nrandomization test of the null hypothesis of no causal effect of treatment on\nresponse, and evaluate its performance in a large scale simulation study\nencompassing data generated from linear and non-linear time series models under\nseveral simulation conditions. In particular, we evaluate the empirical power\nof the proposed test under varying degrees of compliance between the suggested\nand actual treatment adopted by the participant. Our investigations provide\nencouraging results in terms of power and control of type I error rates.\nFinally, we compare the proposed instrumental variable approach to a simple\nintent-to-treat strategy, and develop randomization confidence intervals for\nthe causal effects.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 20:49:31 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 17:45:40 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 21:04:55 GMT"}, {"version": "v4", "created": "Mon, 31 Jul 2017 14:20:26 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Prentice", "Ross L", ""], ["Bot", "Brian M", ""], ["Kellen", "Mike", ""], ["Friend", "Stephen H", ""], ["Trister", "Andrew D", ""], ["Omberg", "Larsson", ""], ["Mangravite", "Lara", ""]]}, {"id": "1604.01210", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Veronica Vinciotti, Ernst C. Wit", "title": "NEAT: an efficient network enrichment analysis test", "comments": "The original version of the paper is freely available (Open Access)\n  from the editor's website\n  (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1203-6)", "journal-ref": "BMC Bioinformatics (2016), 17:352", "doi": "10.1186/s12859-016-1203-6", "report-no": null, "categories": "stat.AP q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network enrichment analysis is a powerful method, which allows to integrate\ngene enrichment analysis with the information on relationships between genes\nthat is provided by gene networks. Existing tests for network enrichment\nanalysis deal only with undirected networks, they can be computationally slow\nand are based on normality assumptions.\n  We propose NEAT, a test for network enrichment analysis. The test is based on\nthe hypergeometric distribution, which naturally arises as the null\ndistribution in this context. NEAT can be applied not only to undirected, but\nto directed and partially directed networks as well. Our simulations indicate\nthat NEAT is considerably faster than alternative resampling-based methods, and\nthat its capacity to detect enrichments is at least as good as the one of\nalternative tests. We discuss applications of NEAT to network analyses in yeast\nby testing for enrichment of the Environmental Stress Response target gene set\nwith GO Slim and KEGG functional gene sets, and also by testing for\nassociations between GO Slim categories themselves.\n  NEAT is a flexible and efficient test for network enrichment analysis that\naims to overcome some limitations of existing resampling-based tests. The\nmethod is implemented in the R package neat, which can be freely downloaded\nfrom CRAN (http://cran.r-project.org/package=neat).\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 10:21:16 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 15:33:38 GMT"}, {"version": "v3", "created": "Sat, 17 Sep 2016 20:15:46 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Signorelli", "Mirko", ""], ["Vinciotti", "Veronica", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1604.01339", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki, Ann B. Lee, Peter E. Freeman", "title": "Photo-z Estimation: An Example of Nonparametric Conditional Density\n  Estimation under Selection Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redshift is a key quantity for inferring cosmological model parameters. In\nphotometric redshift estimation, cosmologists use the coarse data collected\nfrom the vast majority of galaxies to predict the redshift of individual\ngalaxies. To properly quantify the uncertainty in the predictions, however, one\nneeds to go beyond standard regression and instead estimate the full\nconditional density f(z|x) of a galaxy's redshift z given its photometric\ncovariates x. The problem is further complicated by selection bias: usually\nonly the rarest and brightest galaxies have known redshifts, and these galaxies\nhave characteristics and measured covariates that do not necessarily match\nthose of more numerous and dimmer galaxies of unknown redshift. Unfortunately,\nthere is not much research on how to best estimate complex multivariate\ndensities in such settings. Here we describe a general framework for properly\nconstructing and assessing nonparametric conditional density estimators under\nselection bias, and for combining two or more estimators for optimal\nperformance. We propose new improved photo-z estimators and illus- trate our\nmethods on data from the Sloan Data Sky Survey and an application to\ngalaxy-galaxy lensing. Although our main application is photo-z estimation, our\nmethods are relevant to any high-dimensional regression setting with\ncomplicated asymmetric and multimodal distributions in the response variable.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:26:12 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""], ["Freeman", "Peter E.", ""]]}, {"id": "1604.01443", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Analysis of distributional variation through multi-scale Beta-Binomial\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical analyses involve the comparison of multiple data sets\ncollected under different conditions in order to identify the difference in the\nunderlying distributions. A common challenge in multi-sample comparison is the\npresence of various confounders, or extraneous causes other than the conditions\nof interest that also contribute to the difference across the distributions.\nThey result in false findings, i.e., identified differences that are not\nreplicable in follow-up investigations. We consider an ANOVA approach to\naddressing this issue in multi-sample comparison---by collecting replicate data\nsets under each condition, thereby allowing the identification of the\ninteresting distributional variation from the extraneous ones. We introduce a\nmulti-scale Bayesian hierarchical model for the analysis of distributional\nvariation (ANDOVA) under this design, based on a collection of Beta-Binomial\ntests targeting variations of different scales at different locations across\nthe sample space. Instead treating the tests independently, the model employs a\ngraphical structure to introduce dependency among the individual tests thereby\nallowing borrowing of strength among them. We derive efficient inference recipe\nthrough a combination of numerical integration and message passing, and\nevaluate the ability of our method to effectively address ANDOVA through\nextensive simulation. We utilize our method to analyze a DNase-seq data set for\nidentifying differences in transcriptional factor binding.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 22:35:22 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1604.01544", "submitter": "Marcel Ausloos", "authors": "Marcel Ausloos, Olgica Nedic, and Aleksandar Dekanski", "title": "Day of the week effect in paper submission/acceptance/rejection to/in/by\n  peer review journals", "comments": "15 pages, 4 figures, 2 tables, 24 references; prepared for Physica A", "journal-ref": "Physica A 456 (2016) 197-203", "doi": "10.1016/j.physa.2016.03.032", "report-no": null, "categories": "physics.soc-ph cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing an introduction to the behavior of authors\nsubmitting a paper to a scientific journal. Dates of electronic submission of\npapers to the Journal of the Serbian Chemical Society have been recorded from\nthe 1st January 2013 till the 31st December 2014, thus over 2 years.\n  There is no Monday or Friday effect like in financial markets, but rather a\nTuesday-Wednesday effect occurs: papers are more often submitted on Wednesday;\nhowever, the relative number of going to be accepted papers is larger if these\nare submitted on Tuesday. On the other hand, weekend days (Saturday and Sunday)\nare not the best days to finalize and submit manuscripts. An interpretation\nbased on the type of submitted work (\"experimental chemistry\") and on the\ninfluence of (senior) coauthors is presented. A thermodynamic connection is\nproposed within an entropy context. A (new) entropic distance is defined in\norder to measure the \"opaqueness\" = disorder) of the submission process.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 08:54:46 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Ausloos", "Marcel", ""], ["Nedic", "Olgica", ""], ["Dekanski", "Aleksandar", ""]]}, {"id": "1604.01617", "submitter": "Ioanna Manolopoulou", "authors": "Ioanna Manolopoulou, Axel Hille and Brent Emerson", "title": "BPEC: An R Package for Bayesian Phylogeographic and Ecological\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BPEC is an R package for Bayesian Phylogeographic and Ecological Clustering\nwhich allows geographical, environmental and phenotypic measurements to be\ncombined with DNA sequences in order to reveal clustered structure resulting\nfrom migration events. DNA sequences are modelled using a collapsed version of\na simplified coalescent model projected onto haplotype trees, which\nsubsequently give rise to constrained clusterings as migrations occur. Within\neach cluster, a multivariate Gaussian distribution of the covariates\n(geographical, environmental, phenotypic) is used. Inference follows tailored\nReversible Jump Markov chain Monte Carlo sampling so that the number of\nclusters (i.e., migrations) does not need to be pre-specified. A number of\noutput plots and visualizations are provided which reflect the posterior\ndistribution of the parameters of interest. BPEC also includes functions that\ncreate output files which can be loaded into Google Earth. The package commands\nare illustrated through an example dataset of the polytypic Near Eastern brown\nfrog Rana macrocnemis analysed using BPEC.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 13:39:18 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 16:46:33 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 12:03:19 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 11:46:44 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Manolopoulou", "Ioanna", ""], ["Hille", "Axel", ""], ["Emerson", "Brent", ""]]}, {"id": "1604.01715", "submitter": "Francesco Marass", "authors": "Francesco Marass, Florent Mouliere, Ke Yuan, Nitzan Rosenfeld, Florian\n  Markowetz", "title": "A phylogenetic latent feature model for clonal deconvolution", "comments": null, "journal-ref": null, "doi": "10.1214/16-AOAS986", "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumours develop in an evolutionary process, in which the accumulation of\nmutations produces subpopulations of cells with distinct mutational profiles,\ncalled clones. This process leads to the genetic heterogeneity widely observed\nin tumour sequencing data, but identifying the genotypes and frequencies of the\ndifferent clones is still a major challenge. Here, we present Cloe, a\nphylogenetic latent feature model to deconvolute tumour sequencing data into a\nset of related genotypes. Our approach extends latent feature models by placing\nthe features as nodes in a latent tree. The resulting model can capture both\nthe acquisition and the loss of mutations, as well as episodes of convergent\nevolution. We establish the validity of Cloe on synthetic data and assess its\nperformance on controlled biological data, comparing our reconstructions to\nthose of several published state-of-the-art methods. We show that our method\nprovides highly accurate reconstructions and identifies the number of clones,\ntheir genotypes and frequencies even at a modest sequencing depth. As a proof\nof concept we apply our model to clinical data from three cases with chronic\nlymphocytic leukaemia, and one case with acute myeloid leukaemia.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 18:21:19 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Marass", "Francesco", ""], ["Mouliere", "Florent", ""], ["Yuan", "Ke", ""], ["Rosenfeld", "Nitzan", ""], ["Markowetz", "Florian", ""]]}, {"id": "1604.02024", "submitter": "Brian Bader", "authors": "Brian Bader, Jun Yan, Xuebin Zhang", "title": "Automated Threshold Selection for Extreme Value Analysis via\n  Goodness-of-Fit Tests with Application to Batched Return Level Mapping", "comments": "21 pages (text and figures), 27 pages total with reference and title,\n  9 figures, 1 table", "journal-ref": "Ann. Appl. Stat. Volume 12, Number 1 (2018), 310-329", "doi": "10.1214/17-AOAS1092", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold selection is a critical issue for extreme value analysis with\nthreshold-based approaches. Under suitable conditions, exceedances over a high\nthreshold have been shown to follow the generalized Pareto distribution (GPD)\nasymptotically. In practice, however, the threshold must be chosen. If the\nchosen threshold is too low, the GPD approximation may not hold and bias can\noccur. If the threshold is chosen too high, reduced sample size increases the\nvariance of parameter estimates. To process batch analyses, commonly used\nselection methods such as graphical diagnosis are subjective and cannot be\nautomated, while computational methods may not be feasible. We propose to test\na set of thresholds through the goodness-of-fit of the GPD for the exceedances,\nand select the lowest one, above which the data provides adequate fit to the\nGPD. Previous attempts in this setting are not valid due to the special feature\nthat the multiple tests are done in an ordered fashion. We apply two recently\navailable stopping rules that control the false discovery rate or familywise\nerror rate to ordered goodness-of-fit tests to automate threshold selection.\nVarious model specification tests such as the Cramer-von Mises,\nAnderson-Darling, Moran's, and a score test are investigated. The performance\nof the method is assessed in a large scale simulation study that mimics\npractical return level estimation. This procedure was repeated at hundreds of\nsites in the western US to generate return level maps of extreme precipitation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 14:53:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Bader", "Brian", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "1604.02051", "submitter": "Vasanthan Raghavan", "authors": "Vasanthan Raghavan, Alexander G. Tartakovsky", "title": "Tracking Changes in Resilience and Level of Coordination in Terrorist\n  Groups", "comments": "22 pages + 14 pages supplementary material, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity profiles of terrorist groups show frequent spurts and downfalls\ncorresponding to changes in the underlying organizational dynamics. In\nparticular, it is of interest in understanding changes in attributes such as\nintentions/ideology, tactics/strategies, capabilities/resources, etc., that\ninfluence and impact the activity. The goal of this work is the quick detection\nof such changes and in general, tracking of macroscopic as well as microscopic\ntrends in group dynamics. Prior work in this area are based on parametric\napproaches and rely on time-series analysis techniques, self-exciting hurdle\nmodels (SEHM), or hidden Markov models (HMM). While these approaches detect\nspurts and downfalls reasonably accurately, they are all based on model\nlearning --- a task that is difficult in practice because of the \"rare\" nature\nof terrorist attacks from a model learning perspective. In this paper, we\npursue an alternate non-parametric approach for spurt detection in activity\nprofiles. Our approach is based on binning the count data of terrorist activity\nto form observation vectors that can be compared with each other. Motivated by\na majorization theory framework, these vectors are then transformed via certain\nfunctionals and used in spurt classification. While the parametric approaches\noften result in either a large number of missed detections of real changes or\nfalse alarms of unoccurred changes, the proposed approach is shown to result in\na small number of missed detections and false alarms. Further, the\nnon-parametric nature of the approach makes it attractive for ready\napplications in a practical context.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:56:32 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Raghavan", "Vasanthan", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1604.02123", "submitter": "Talayeh Razzaghi", "authors": "Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nicholas Marko", "title": "Multilevel Weighted Support Vector Machine for Classification on\n  Healthcare Data with Missing Values", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.06250", "journal-ref": null, "doi": "10.1371/journal.pone.0155119", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the needs of predictive analytics on healthcare\ndata as represented by Electronic Medical Records. Such data is invariably\nproblematic: noisy, with missing entries, with imbalance in classes of\ninterests, leading to serious bias in predictive modeling. Since standard data\nmining methods often produce poor performance measures, we argue for\ndevelopment of specialized techniques of data-preprocessing and classification.\nIn this paper, we propose a new method to simultaneously classify large\ndatasets and reduce the effects of missing values. It is based on a multilevel\nframework of the cost-sensitive SVM and the expected maximization imputation\nmethod for missing values, which relies on iterated regression analyses. We\ncompare classification results of multilevel SVM-based algorithms on public\nbenchmark datasets with imbalanced classes and missing values as well as real\ndata in health applications, and show that our multilevel SVM-based method\nproduces fast, and more accurate and robust classification results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:19:52 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Roderick", "Oleg", ""], ["Safro", "Ilya", ""], ["Marko", "Nicholas", ""]]}, {"id": "1604.02158", "submitter": "Ming Yuan", "authors": "Shulei Wang, Jianqing Fan, Ginger Pocock and Ming Yuan", "title": "Structured Correlation Detection with Application to Colocalization\n  Analysis in Dual-Channel Fluorescence Microscopic Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of colocalization analysis in fluorescence\nmicroscopic imaging, we study in this paper structured detection of correlated\nregions between two random processes observed on a common domain. We argue that\nalthough intuitive, direct use of the maximum log-likelihood statistic suffers\nfrom potential bias and substantially reduced power, and introduce a simple\nsize-based normalization to overcome this problem. We show that scanning with\nthe proposed size-corrected likelihood ratio statistics leads to optimal\ncorrelation detection over a large collection of structured correlation\ndetection problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 20:04:39 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Wang", "Shulei", ""], ["Fan", "Jianqing", ""], ["Pocock", "Ginger", ""], ["Yuan", "Ming", ""]]}, {"id": "1604.02668", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Christian Hennig, Yu-Fen Li, Chien-Ju Lin", "title": "Distance for Functional Data Clustering Based on Smoothing Parameter\n  Commutation", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 27 (2018)", "doi": "10.1177/0962280217710050", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to determine the dissimilarity between subjects for\nfunctional data clustering. Spline smoothing or interpolation is common to deal\nwith data of such type. Instead of estimating the best-representing curve for\neach subject as fixed during clustering, we measure the dissimilarity between\nsubjects based on varying curve estimates with commutation of smoothing\nparameters pair-by-pair (of subjects). The intuitions are that smoothing\nparameters of smoothing splines reflect inverse signal-to-noise ratios and that\napplying an identical smoothing parameter the smoothed curves for two similar\nsubjects are expected to be close. The effectiveness of our proposal is shown\nthrough simulations comparing to other dissimilarity measures. It also has\nseveral pragmatic advantages. First, missing values or irregular time points\ncan be handled directly, thanks to the nature of smoothing splines. Second,\nconventional clustering method based on dissimilarity can be employed\nstraightforward, and the dissimilarity also serves as a useful tool for outlier\ndetection. Third, the implementation is almost handy since subroutines for\nsmoothing splines and numerical integration are widely available. Fourth, the\ncomputational complexity does not increase and is parallel with that in\ncalculating Euclidean distance between curves estimated by smoothing splines.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 10:55:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Hennig", "Christian", ""], ["Li", "Yu-Fen", ""], ["Lin", "Chien-Ju", ""]]}, {"id": "1604.02883", "submitter": "Xin Li Mr", "authors": "Xin Li, Kathleen Gray, Karin Verspoor, Stephen Barnett", "title": "Analysing health professionals' learning interactions in online social\n  networks: A social network analysis approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Networking may be a way to support health professionals' need\nfor continuous learning through interaction with peers and experts.\nUnderstanding and evaluating such learning is important but difficult, and\nSocial Network Analysis (SNA) offers a solution. This paper demonstrates how\nSNA can be used to study levels of participation as well as the patterns of\ninteractions that take place among health professionals in a large online\nprofessional learning network. Our analysis has shown that their learning\nnetwork is highly centralised and loosely connected. The level of participation\nis low in general, and most interactions are structured around a small set of\nusers consisting of moderators and core members. The structural patterns of\ninteraction indicates there is a chance of small group learning occurring and\nrequires further investigation to identify those potential learning groups.\nThis first stage of analysis, to be followed by longitudinal study of the\ndynamics of interaction and complemented by content analysis of their\ndiscussion, may contribute to greater sophistication in the analysis and\nutilisation of new environments for health professional learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 11:10:51 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Li", "Xin", ""], ["Gray", "Kathleen", ""], ["Verspoor", "Karin", ""], ["Barnett", "Stephen", ""]]}, {"id": "1604.03019", "submitter": "Andreas Trier Poulsen", "authors": "Andreas Trier Poulsen, Simon Kamronn, Jacek Dmochowski, Lucas C.\n  Parra, and Lars Kai Hansen", "title": "EEG in the classroom: Synchronised neural recordings during video\n  presentation", "comments": "14 pages, 5 figures, 3 tables. Preprint version. Revision of original\n  preprint. Supplementary materials added as ancillary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We performed simultaneous recordings of electroencephalography (EEG) from\nmultiple students in a classroom, and measured the inter-subject correlation\n(ISC) of activity evoked by a common video stimulus. The neural reliability, as\nquantified by ISC, has been linked to engagement and attentional modulation in\nearlier studies that used high-grade equipment in laboratory settings. Here we\nreproduce many of the results from these studies using portable low-cost\nequipment, focusing on the robustness of using ISC for subjects experiencing\nnaturalistic stimuli. The present data shows that stimulus-evoked neural\nresponses, known to be modulated by attention, can be tracked in for groups of\nstudents with synchronized EEG acquisition. This is a step towards real-time\ninference of engagement in the classroom.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 16:19:16 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 14:07:43 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 21:38:47 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Poulsen", "Andreas Trier", ""], ["Kamronn", "Simon", ""], ["Dmochowski", "Jacek", ""], ["Parra", "Lucas C.", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1604.03186", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande and Shane T. Jensen", "title": "Estimating an NBA player's impact on his team's chances of winning", "comments": "To appear in the Journal of Quantitative Analysis of Sport", "journal-ref": null, "doi": "10.1515/jqas-2015-0027", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional NBA player evaluation metrics are based on scoring differential\nor some pace-adjusted linear combination of box score statistics like points,\nrebounds, assists, etc. These measures treat performances with the outcome of\nthe game still in question (e.g. tie score with five minutes left) in exactly\nthe same way as they treat performances with the outcome virtually decided\n(e.g. when one team leads by 30 points with one minute left). Because they\nignore the context in which players perform, these measures can result in\nmisleading estimates of how players help their teams win. We instead use a win\nprobability framework for evaluating the impact NBA players have on their\nteams' chances of winning. We propose a Bayesian linear regression model to\nestimate an individual player's impact, after controlling for the other players\non the court. We introduce several posterior summaries to derive rank-orderings\nof players within their team and across the league. This allows us to identify\nhighly paid players with low impact relative to their teammates, as well as\nplayers whose high impact is not captured by existing metrics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 00:47:55 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1604.03248", "submitter": "Natasha Markuzon", "authors": "Mallory Sheth, Roy Welsch, Natasha Markuzon", "title": "The Univariate Flagging Algorithm (UFA): a Fully-Automated Approach for\n  Identifying Optimal Thresholds in Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data classification problems, there is no linear relationship between\nan explanatory and the dependent variables. Instead, there may be ranges of the\ninput variable for which the observed outcome is signficantly more or less\nlikely. This paper describes an algorithm for automatic detection of such\nthresholds, called the Univariate Flagging Algorithm (UFA). The algorithm\nsearches for a separation that optimizes the difference between separated areas\nwhile providing the maximum support. We evaluate its performance using three\nexamples and demonstrate that thresholds identified by the algorithm align well\nwith visual inspection and subject matter expertise. We also introduce two\nclassification approaches that use UFA and show that the performance attained\non unseen test data is equal to or better than that of more traditional\nclassifiers. We demonstrate that the proposed algorithm is robust against\nmissing data and noise, is scalable, and is easy to interpret and visualize. It\nis also well suited for problems where incidence of the target is low.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 05:04:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Sheth", "Mallory", ""], ["Welsch", "Roy", ""], ["Markuzon", "Natasha", ""]]}, {"id": "1604.03325", "submitter": "Sean Elvidge", "authors": "Sean Elvidge and Matthew J. Angling", "title": "Using Extreme Value Theory for Determining the Probability of\n  Carrington-Like Solar Flares", "comments": "13 pages, 4 figures; updated content following reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.SR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space weather events can negatively affect satellites, the electricity grid,\nsatellite navigation systems and human health. As a consequence, extreme space\nweather has been added to the UK and other national risk registers. By their\nvery nature, extreme space weather events occur rarely and, therefore,\nstatistical methods are required to determine the probability of their\noccurrence. Space weather events can be characterised by a number of natural\nphenomena such as X-ray (solar) flares, solar energetic particle (SEP) fluxes,\ncoronal mass ejections and various geophysical indices (Dst, Kp, F10.7). In\nthis paper extreme value theory (EVT) is used to investigate the probability of\nextreme solar flares. Previous work has assumed that the distribution of solar\nflares follows a power law. However such an approach can lead to a poor\nestimation of the return times of such events due to uncertainties in the tails\nof the probability distribution function. Using EVT and GOES X-ray flux data it\nis shown that the expected 150-year return level is approximately an X60 flare\nwhilst a Carrington-like flare is a one in a 100-year event. It is also shown\nthat the EVT results are consistent with flare data from the Kepler space\ntelescope mission.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 09:54:07 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 12:20:06 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 13:40:17 GMT"}, {"version": "v4", "created": "Thu, 19 Oct 2017 13:21:45 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Elvidge", "Sean", ""], ["Angling", "Matthew J.", ""]]}, {"id": "1604.03480", "submitter": "Adam Kapelner", "authors": "Adam Kapelner, Abba Krieger and William J. Blanford", "title": "Optimal Experimental Designs for Estimating Henry's Law Constants via\n  the Method of Phase Ratio Variation", "comments": "21 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When measuring Henry's Law constants ($k_H$) using the phase ratio method via\nheadspace gas chromatography (GC), the value of $k_H$ of the compound under\ninvestigation is calculated from the ratio of the slope to the intercept of a\nlinear regression of the the inverse GC response versus the ratio of gas to\nliquid volumes of a series of vials drawn from the same parent solution. Thus,\nan experimenter will collect measurements consisting of the independent\nvariable (the gas/liquid volume ratio) and dependent variable (the inverse GC\npeak area). There is a choice of values of the independent variable during\nmeasurement. A review of the literature found that the common approach is a\nsimple uniformly spacing of the liquid volumes. We present an optimal\nexperimental design which estimates $k_H$ with minimum error and provides\nmultiple means for building confidence intervals for such estimates. We\nillustrate efficiency improvements of our new design with an example measuring\nthe $k_H$ for napthalene in aqueous solution as well as simulations on previous\nstudies. The designs can be easily computed using our open source software\noptDesignSlopeInt, an R package on CRAN. We also discuss applicability of this\nmethod to other fields.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 17:03:31 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Kapelner", "Adam", ""], ["Krieger", "Abba", ""], ["Blanford", "William J.", ""]]}, {"id": "1604.03558", "submitter": "Sandra K\\\"onig", "authors": "Sandra K\\\"onig", "title": "Error Propagation Through a Network With Non-Uniform Failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central concern of network operators is to estimate the probability of an\nincident that affects a significant part and thus may yield to a breakdown. We\nanswer this question by modeling how a failure of either a node or an edge will\naffect the rest of the network using percolation theory. Our model is general\nin the sense that it only needs two inputs: the topology of the network and the\nchances of failure of its components. These chances may vary to represent\ndifferent types of edges having different tendencies to fail. We illustrate the\napproach by an example, for which we can even obtain closed form expressions\nfor the likelihood of an outbreak remaining bounded or spreading unlimitedly.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 12:52:10 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:57:50 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["K\u00f6nig", "Sandra", ""]]}, {"id": "1604.03614", "submitter": "Guanhao Feng", "authors": "Guanhao Feng, Nicholas G. Polson, Jianeng Xu", "title": "The Market for English Premier League (EPL) Odds", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports, 12.4 (2017): 167-178", "doi": "10.1515/jqas-2016-0039", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper employs a Skellam process to represent real-time betting odds for\nEnglish Premier League (EPL) soccer games. Given a matrix of market odds on all\npossible score outcomes, we estimate the expected scoring rates for each team.\nThe expected scoring rates then define the implied volatility of an EPL game.\nAs events in the game evolve, we re-estimate the expected scoring rates and our\nimplied volatility measure to provide a dynamic representation of the market's\nexpectation of the game outcome. Using a dataset of 1520 EPL games from\n2012-2016, we show how our model calibrates well to the game outcome. We\nillustrate our methodology on real-time market odds data for a game between\nEverton and West Ham in the 2015-2016 season. We show how the implied\nvolatility for the outcome evolves as goals, red cards, and corner kicks occur.\nFinally, we conclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 23:22:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 19:17:40 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 21:15:25 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 15:25:42 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 20:42:09 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Feng", "Guanhao", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1604.03622", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Edmund Zelnio, Alfred Hero III", "title": "Kronecker STAP and SAR GMTI", "comments": "12 pgs, to appear at SPIE 2016. arXiv admin note: text overlap with\n  arXiv:1501.07481", "journal-ref": null, "doi": "10.1117/12.2223896", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a high resolution radar imaging modality, SAR detects and localizes\nnon-moving targets accurately, giving it an advantage over lower resolution\nGMTI radars. Moving target detection is more challenging due to target smearing\nand masking by clutter. Space-time adaptive processing (STAP) is often used on\nmultiantenna SAR to remove the stationary clutter and enhance the moving\ntargets. In (Greenewald et al., 2016) it was shown that the performance of STAP\ncan be improved by modeling the clutter covariance as a space vs. time\nKronecker product with low rank factors, providing robustness and reducing the\nnumber of training samples required. In this work, we present a massively\nparallel algorithm for implementing Kronecker product STAP, enabling\napplication to very large SAR datasets (such as the 2006 Gotcha data\ncollection) using GPUs. Finally, we develop an extension of Kronecker STAP that\nuses information from multiple passes to improve moving target detection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 00:27:26 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Zelnio", "Edmund", ""], ["Hero", "Alfred", "III"]]}, {"id": "1604.03647", "submitter": "Yihong Yuan", "authors": "Yihong Yuan", "title": "Modeling Inter-Country Connection from Geotagged News Reports: A\n  Time-Series Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of theories and techniques for big data analytics offers\ntremendous flexibility for investigating large-scale events and patterns that\nemerge over space and time. In this research, we utilize a unique open-access\ndataset \"The Global Data on Events, Location and Tone\" (GDELT) to model the\nimage of China in mass media, specifically, how China has related to the rest\nof the world and how this connection has evolved upon time based on an\nautoregressive integrated moving average (ARIMA) model. The results of this\nresearch contribute both in methodological and empirical perspectives: We\nexamined the effectiveness of time series models in predicting trends in\nlong-term mass media data. In addition, we identified various types of\nconnection strength patterns between China and its top 15 related countries.\nThis study generates valuable input to interpret China's diplomatic and\nregional relations based on mass media data, as well as providing\nmethodological references for investigating international relations in other\ncountries and regions in the big data era.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 03:53:53 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Yuan", "Yihong", ""]]}, {"id": "1604.03776", "submitter": "Ma{\\l}gorzata Snarska", "authors": "Daniel Kosiorowski, Jerzy P. Rydlewski, Ma{\\l}gorzata Snarska", "title": "Detecting a Structural Change in Functional Time Series Using Local\n  Wilcoxon Statistic", "comments": "17 pages, 19 figures, LaTeX svjour3 class The final publication is\n  available at link.springer.com DOI: 10.1007/s00362-017-0891-y", "journal-ref": "Statistical Papers, October 2019, Volume 60, Issue 5, pp 1677 -\n  1698", "doi": "10.1007/s00362-017-0891-y", "report-no": null, "categories": "stat.ME q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) is a part of modern multivariate statistics\nthat analyses data providing information about curves, surfaces or anything\nelse varying over a certain continuum. In economics and empirical finance we\noften have to deal with time series of functional data, where we cannot easily\ndecide, whether they are to be considered as homogeneous or heterogeneous. At\npresent a discussion on adequate tests of homogenity for functional data is\ncarried. We propose a novel statistic for detetecting a structural change in\nfunctional time series based on a local Wilcoxon statistic induced by a local\ndepth function proposed by Paindaveine and Van Bever (2013).\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 14:00:30 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 23:22:21 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 08:08:00 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1604.04002", "submitter": "Xin Ding", "authors": "Xin Ding, Ziyi Qiu and Xiaohui Chen", "title": "Sparse transition matrix estimation for high-dimensional and locally\n  stationary vector autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the transition matrix in the high-dimensional\ntime-varying vector autoregression (TV-VAR) models. Our model builds on a\ngeneral class of locally stationary VAR processes that evolve smoothly in time.\nWe propose a hybridized kernel smoothing and $\\ell^1$-regularized method to\ndirectly estimate the sequence of time-varying transition matrices. Under the\nsparsity assumption on the transition matrix, we establish the rate of\nconvergence of the proposed estimator and show that the convergence rate\ndepends on the smoothness of the locally stationary VAR processes only through\nthe smoothness of the transition matrix function. In addition, for our\nestimator followed by thresholding, we prove that the false positive rate (type\nI error) and false negative rate (type II error) in the pattern recovery can\nasymptotically vanish in the presence of weak signals without assuming the\nminimum nonzero signal strength condition. Favorable finite sample performances\nover the $\\ell^2$-penalized least-squares estimator and the unstructured\nmaximum likelihood estimator are shown on simulated data. We also provide two\nreal examples on estimating the dependence structures on financial stock prices\nand economic exchange rates datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 00:37:59 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 20:49:22 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 07:58:57 GMT"}, {"version": "v4", "created": "Fri, 8 Sep 2017 01:49:49 GMT"}, {"version": "v5", "created": "Sat, 30 Sep 2017 03:02:52 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Ding", "Xin", ""], ["Qiu", "Ziyi", ""], ["Chen", "Xiaohui", ""]]}, {"id": "1604.04013", "submitter": "Sean Meyn", "authors": "Yue Chen and Ana Bu\\v{s}i\\'c and Sean Meyn", "title": "Ergodic Theory for Controlled Markov Chains with Stationary Inputs", "comments": null, "journal-ref": null, "doi": "10.1214/17-AAP1300", "report-no": null, "categories": "cs.PF cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a stochastic process $\\{X(t)\\}$ on a finite state space $ {\\sf\nX}=\\{1,\\dots, d\\}$. It is conditionally Markov, given a real-valued `input\nprocess' $\\{\\zeta(t)\\}$. This is assumed to be small, which is modeled through\nthe scaling, \\[ \\zeta_t = \\varepsilon \\zeta^1_t, \\qquad 0\\le \\varepsilon \\le\n1\\,, \\] where $\\{\\zeta^1(t)\\}$ is a bounded stationary process. The following\nconclusions are obtained, subject to smoothness assumptions on the controlled\ntransition matrix and a mixing condition on $\\{\\zeta(t)\\}$:\n  (i) A stationary version of the process is constructed, that is coupled with\na stationary version of the Markov chain $\\{X^\\bullet$(t)\\}obtained with\n$\\{\\zeta(t)\\}\\equiv 0$. The triple $(\\{X(t)\\}, \\{X^\\bullet(t)\\},\\{\\zeta(t)\\})$\nis a jointly stationary process satisfying \\[ {\\sf P}\\{X(t) \\neq X^\\bullet(t)\\}\n= O(\\varepsilon) \\] Moreover, a second-order Taylor-series approximation is\nobtained: \\[ {\\sf P}\\{X(t) =i \\} ={\\sf P}\\{X^\\bullet(t) =i \\} + \\varepsilon^2\n\\varrho(i) + o(\\varepsilon^2),\\quad 1\\le i\\le d, \\] with an explicit formula\nfor the vector $\\varrho\\in\\mathbb{R}^d$.\n  (ii) For any $m\\ge 1$ and any function $f\\colon \\{1,\\dots,d\\}\\times\n\\mathbb{R}\\to\\mathbb{R}^m$, the stationary stochastic process $Y(t) =\nf(X(t),\\zeta(t))$ has a power spectral density $\\text{S}_f$ that admits a\nsecond order Taylor series expansion: A function $\\text{S}^{(2)}_f\\colon\n[-\\pi,\\pi] \\to \\mathbb{C}^{ m\\times m}$ is constructed such that \\[\n\\text{S}_f(\\theta) = \\text{S}^\\bullet_f(\\theta) + \\varepsilon^2\n\\text{S}_f^{(2)}(\\theta) + o(\\varepsilon^2),\\quad \\theta\\in [-\\pi,\\pi] . \\] An\nexplicit formula for the function $\\text{S}_f^{(2)}$ is obtained, based in part\non the bounds in (i).\n  The results are illustrated using a version of the timing channel of\nAnantharam and Verdu.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 02:16:52 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 09:39:26 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Chen", "Yue", ""], ["Bu\u0161i\u0107", "Ana", ""], ["Meyn", "Sean", ""]]}, {"id": "1604.04235", "submitter": "Robert Wolstenholme", "authors": "R. J. Wolstenholme, A. T. Walden", "title": "A Sampling Strategy for Projecting to Permutations in the Graph Matching\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the graph matching problem we propose a novel method for\nprojecting a matrix $Q$, which may be a doubly stochastic matrix, to a\npermutation matrix $P.$ We observe that there is an intuitve mapping, depending\non a given $Q,$ from the set of $n$-dimensional permutation matrices to sets of\npoints in $\\mathbb{R}^n$. The mapping has a number of geometrical properties\nthat allow us to succesively sample points in $\\mathbb{R}^n$ in a manner\nsimilar to simulated annealing, where our objective is to minimise the graph\nmatching norm found using the permutation matrix corresponding to each of the\npoints. Our sampling strategy is applied to the QAPLIB benchmark library and\noutperforms the PATH algorithm in two-thirds of cases. Instead of using linear\nassignment, the incorporation of our sampling strategy as a projection step\ninto algorithms such as PATH itself has the potential to achieve even better\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 17:50:24 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wolstenholme", "R. J.", ""], ["Walden", "A. T.", ""]]}, {"id": "1604.04322", "submitter": "Elizabeth Hou", "authors": "Elizabeth Hou, Yasin Y{\\i}lmaz, Alfred O. Hero", "title": "Diversion Detection in Partially Observed Nuclear Fuel Cycle Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nuclear fuel cycle contains several facilities with different purposes such\nas mining, conversion, enrichment, and fuel rod fabrication. These facilities\nform a network, which is naturally sparse in the number of connections (i.e.,\nedges) since not every facility directly interacts with all the others. Given\nthe knowledge of a network baseline, we are interested in detecting anomalous\nactivities in this network, which may signal the diversion of nuclear\nmaterials. Anomalies can take the form of a new or missing edge or abnormal\nrates of interaction. However, often it is not possible to observe the entire\nnetwork traffic directly due to some constraints such as cost, physical\nlimitations, or laws. By treating the unobserved network traffic as latent\nvariables, we propose estimators for the true network traffic, including the\nanomalous activity, to use in testing for significant deviations from the\nbaseline. We provide simulation results of a simple network of facilities and\nshow that our estimators have superior performance over existing alternatives.\nAdditionally, we establish that while a good estimate of the network traffic is\nnecessary, perfect reconstruction is not required to effectively detect\nanomalous network activity. Instead it suffices to detect perturbations within\nthe network at an aggregate or global scale.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 00:48:05 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 00:47:10 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Hou", "Elizabeth", ""], ["Y\u0131lmaz", "Yasin", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1604.04484", "submitter": "Sean Simmons", "authors": "Sean Simmons, Cenk Sahinalp, and Bonnie Berger", "title": "Enabling Privacy-Preserving GWAS in Heterogeneous Human Populations", "comments": "To be presented at RECOMB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The projected increase of genotyping in the clinic and the rise of large\ngenomic databases has led to the possibility of using patient medical data to\nperform genomewide association studies (GWAS) on a larger scale and at a lower\ncost than ever before. Due to privacy concerns, however, access to this data is\nlimited to a few trusted individuals, greatly reducing its impact on biomedical\nresearch. Privacy preserving methods have been suggested as a way of allowing\nmore people access to this precious data while protecting patients. In\nparticular, there has been growing interest in applying the concept of\ndifferential privacy to GWAS results. Unfortunately, previous approaches for\nperforming differentially private GWAS are based on rather simple statistics\nthat have some major limitations. In particular, they do not correct for\npopulation stratification, a major issue when dealing with the genetically\ndiverse populations present in modern GWAS. To address this concern we\nintroduce a novel computational framework for performing GWAS that tailors\nideas from differential privacy to protect private phenotype information, while\nat the same time correcting for population stratification. This framework\nallows us to produce privacy preserving GWAS results based on two of the most\ncommonly used GWAS statistics: EIGENSTRAT and linear mixed model (LMM) based\nstatistics. We test our differentially private statistics, PrivSTRAT and\nPrivLMM, on both simulated and real GWAS datasets and find that they are able\nto protect privacy while returning meaningful GWAS results.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 13:06:47 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Simmons", "Sean", ""], ["Sahinalp", "Cenk", ""], ["Berger", "Bonnie", ""]]}, {"id": "1604.04527", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Deep Learning for Short-Term Traffic Flow Prediction", "comments": null, "journal-ref": null, "doi": "10.1016/j.trc.2017.02.024", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a deep learning model to predict traffic flows. The main\ncontribution is development of an architecture that combines a linear model\nthat is fitted using $\\ell_1$ regularization and a sequence of $\\tanh$ layers.\nThe challenge of predicting traffic flows are the sharp nonlinearities due to\ntransitions between free flow, breakdown, recovery and congestion. We show that\ndeep learning architectures can capture these nonlinear spatio-temporal\neffects. The first layer identifies spatio-temporal relations among predictors\nand other layers model nonlinear relations. We illustrate our methodology on\nroad sensor data from Interstate I-55 and predict traffic flows during two\nspecial events; a Chicago Bears football game and an extreme snowstorm event.\nBoth cases have sharp traffic flow regime changes, occurring very suddenly, and\nwe show how deep learning provides precise short term traffic flow predictions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 14:54:30 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 23:29:33 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 22:58:03 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1604.04687", "submitter": "Andrew L. Johnson", "authors": "Jos\\'e Luis Preciado Arreola, Daisuke Yagi and Andrew L. Johnson", "title": "Insights from Machine Learning for Evaluating Production Function\n  Estimators on Manufacturing Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations like U.S. Census Bureau rely on non-exhaustive surveys to\nestimate industry-level production functions in years in which a full Census is\nnot conducted. When analyzing data from non-census years, we propose selecting\nan estimator based on a weighting of its in-sample and predictive performance.\nWe compare Cobb-Douglas functional assumptions to existing nonparametric shape\nconstrained estimators and a newly proposed estimator. For simulated data, we\nfind that our proposed estimator has the lowest weighted errors. For actual\ndata, specifically the 2010 Chilean Annual National Industrial Survey, a\nCobb-Douglas specification describes at least 90\\% as much variance as the best\nalternative estimators in practically all cases considered providing two\ninsights: the benefits of using application data for selecting an estimator,\nand the benefits of structure in noisy data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 04:05:18 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 02:44:15 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Arreola", "Jos\u00e9 Luis Preciado", ""], ["Yagi", "Daisuke", ""], ["Johnson", "Andrew L.", ""]]}, {"id": "1604.04732", "submitter": "Stephanie Thiemichen", "authors": "Stephanie Thiemichen and G\\\"oran Kauermann", "title": "Stable Exponential Random Graph Models with Non-parametric Components\n  for Large Dense Networks", "comments": "26 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential Random Graph Models (ERGM) behave peculiar in large networks with\nthousand(s) of actors (nodes). Standard models containing two-star or triangle\ncounts as statistics are often unstable leading to completely full or empty\nnetworks. Moreover, numerical methods break down which makes it complicated to\napply ERGMs to large networks. In this paper we propose two strategies to\ncircumvent these obstacles. First, we fit a model to a subsampled network and\nsecondly, we show how linear statistics (like two-stars etc.) can be replaced\nby smooth functional components. These two steps in combination allow to fit\nstable models to large network data, which is illustrated by a data example\nincluding a residual analysis.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 12:03:51 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thiemichen", "Stephanie", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1604.04931", "submitter": "Arno Solin", "authors": "Arno Solin, Pasi Jyl\\\"anki, Jaakko Kauram\\\"aki, Tom Heskes, Marcel A.\n  J. van Gerven, Simo S\\\"arkk\\\"a", "title": "Regularizing Solutions to the MEG Inverse Problem Using Space-Time\n  Separable Covariance Functions", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In magnetoencephalography (MEG) the conventional approach to source\nreconstruction is to solve the underdetermined inverse problem independently\nover time and space. Here we present how the conventional approach can be\nextended by regularizing the solution in space and time by a Gaussian process\n(Gaussian random field) model. Assuming a separable covariance function in\nspace and time, the computational complexity of the proposed model becomes\n(without any further assumptions or restrictions) $\\mathcal{O}(t^3 + n^3 +\nm^2n)$, where $t$ is the number of time steps, $m$ is the number of sources,\nand $n$ is the number of sensors. We apply the method to both simulated and\nempirical data, and demonstrate the efficiency and generality of our Bayesian\nsource reconstruction approach which subsumes various classical approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 21:16:37 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Solin", "Arno", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Kauram\u00e4ki", "Jaakko", ""], ["Heskes", "Tom", ""], ["van Gerven", "Marcel A. J.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1604.05224", "submitter": "Jingjing Yang", "authors": "Jingjing Yang, Peng Ren", "title": "BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis", "comments": "A tool paper submitted to the Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical\nmodel to smooth multiple functional data with the assumptions of the same\nunderlying Gaussian process distribution, a Gaussian process prior for the mean\nfunction, and an Inverse-Wishart process prior for the covariance function.\nThis model-based approach can borrow strength from all functional data to\nincrease the smoothing accuracy, as well as estimate the mean-covariance\nfunctions simultaneously. An option of approximating the Bayesian inference\nprocess using cubic B-spline basis functions is integrated in BFDA, which\nallows for efficiently dealing with high-dimensional functional data. Examples\nof using BFDA in various scenarios and conducting follow-up functional\nregression are provided. The advantages of BFDA include: (1) Simultaneously\nsmooths multiple functional data and estimates the mean-covariance functions in\na nonparametric way; (2) flexibly deals with sparse and high-dimensional\nfunctional data with stationary and nonstationary covariance functions, and\nwithout the requirement of common observation grids; (3) provides accurately\nsmoothed functional data for follow-up analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 16:06:22 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 16:47:31 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Yang", "Jingjing", ""], ["Ren", "Peng", ""]]}, {"id": "1604.05266", "submitter": "Ikjyot Singh Kohli", "authors": "Ikjyot Singh Kohli", "title": "Finding Common Characteristics Among NBA Playoff and Championship Teams:\n  A Machine Learning Approach", "comments": "Updated contents to reflect most recent data and corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we employ machine learning techniques to analyze seventeen\nseasons (1999-2000 to 2015-2016) of NBA regular season data from every team to\ndetermine the common characteristics among NBA playoff teams. Each team was\ncharacterized by 26 predictor variables and one binary response variable taking\non a value of \"TRUE\" if a team had made the playoffs, and value of \"FALSE\" if a\nteam had missed the playoffs. After fitting an initial classification tree to\nthis problem, this tree was then pruned which decreased the test error rate.\nFurther to this, a random forest of classification trees was grown which\nprovided a very accurate model from which a variable importance plot was\ngenerated to determine which predictor variables had the greatest influence on\nthe response variable. The result of this work was the conclusion that the most\nimportant factors in characterizing a team's playoff eligibility are a team's\nopponent number of assists per game, a team's opponent number of made two point\nshots per game, and a team's number of steals per game. This seems to suggest\nthat defensive factors as opposed to offensive factors are the most important\ncharacteristics shared among NBA playoff teams. We then use neural networks to\nclassify championship teams based on regular season data. From this, we show\nthat the most important factor in a team not winning a championship is that\nteam's opponent number of made three-point shots per game. This once again\nimplies that defensive characteristics are of great importance in not only\ndetermining a team's playoff eligibility, but certainly, one can conclude that\na lack of perimeter defense negatively impacts a team's championship chances in\na given season. Further, it is shown that made two-point shots and defensive\nrebounding are by far the most important factor in a team's chances at winning\na championship in a given season.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:57:21 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 17:25:16 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 21:42:59 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 18:10:17 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 00:54:24 GMT"}, {"version": "v6", "created": "Tue, 21 Feb 2017 16:59:03 GMT"}, {"version": "v7", "created": "Mon, 3 Apr 2017 23:00:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kohli", "Ikjyot Singh", ""]]}, {"id": "1604.05589", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos and Peter G. Moffatt", "title": "Coupling couples with copulas: analysis of assortative matching on risk\n  attitude", "comments": null, "journal-ref": "Economic Inquiry, 2019, 57 (1), 654--666", "doi": "10.1111/ecin.12726", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate patterns of assortative matching on risk attitude, using\nself-reported (ordinal) data on risk attitudes for males and females within\nmarried couples, from the German Socio-Economic Panel over the period\n2004-2012. We apply a novel copula-based bivariate panel ordinal model.\nEstimation is in two steps: firstly, a copula-based Markov model is used to\nrelate the marginal distribution of the response in different time periods,\nseparately for males and females; secondly, another copula is used to couple\nthe males' and females' conditional (on the past) distributions. We find\npositive dependence, both in the middle of the distribution, and in the joint\ntails, and we interpret this as positive assortative matching (PAM). Hence we\nreject standard assortative matching theories based on risk-sharing\nassumptions, and favour models based on alternative assumptions such as the\nability of agents to control income risk. We also find evidence of\n\"assimilation\"; that is, PAM appearing to increase with years of marriage.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:26:29 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 21:18:06 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 19:00:46 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""], ["Moffatt", "Peter G.", ""]]}, {"id": "1604.05643", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos and Emmanouil Mentzakis", "title": "A copula-based model for multivariate ordinal panel data: application to\n  well-being composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel copula-based multivariate panel ordinal model is developed to\nestimate structural relations among components of well-being. Each ordinal\ntime-series is modelled using a copula-based Markov model to relate the\nmarginal distributions of the response at each time of observation and then, at\neach observation time, the conditional distributions of each ordinal\ntime-series are joined using a multivariate t copula. Maximum simulated\nlikelihood based on evaluating the multidimensional integrals of the likelihood\nwith randomized quasi Monte Carlo methods is used for the estimation.\nAsymptotic calculations show that our method is nearly as efficient as maximum\nlikelihood for fully specified multivariate copula models. Our findings\nhighlight the importance of one's relative position in evaluating their\nwell-being with no direct effects of socio-economic characteristics on\nwell-being but strong indirect effects through their impact on components of\nwell-being. Temporal resilience, habit formation and behavioural traits can\nexplain the dependence in the joint tails over time and across well-being\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 16:31:25 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 20:11:28 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""], ["Mentzakis", "Emmanouil", ""]]}, {"id": "1604.05910", "submitter": "Jiechao Xiong", "authors": "Jiechao Xiong, Feng Ruan, and Yuan Yao", "title": "A Tutorial on Libra: R package for the Linearized Bregman Algorithm in\n  High Dimensional Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package, Libra, stands for the LInearized BRegman Al- gorithm in high\ndimensional statistics. The Linearized Bregman Algorithm is a simple iterative\nprocedure to generate sparse regularization paths of model estimation, which\nare rstly discovered in applied mathematics for image restoration and\nparticularly suitable for parallel implementation in large scale problems. The\nlimit of such an algorithm is a sparsity-restricted gradient descent ow, called\nthe Inverse Scale Space, evolving along a par- simonious path of sparse models\nfrom the null model to over tting ones. In sparse linear regression, the\ndynamics with early stopping regularization can provably meet the unbiased\nOracle estimator under nearly the same condition as LASSO, while the latter is\nbiased. Despite their successful applications, statistical consistency theory\nof such dynamical algorithms remains largely open except for some recent\nprogress on linear regression. In this tutorial, algorithmic implementations in\nthe package are discussed for several widely used sparse models in statistics,\nincluding linear regression, logistic regres- sion, and several graphical\nmodels (Gaussian, Ising, and Potts). Besides the simulation examples, various\napplication cases are demonstrated, with real world datasets from diabetes,\npublications of COPSS award winners, as well as social networks of two Chinese\nclassic novels, Journey to the West and Dream of the Red Chamber.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 12:04:38 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Xiong", "Jiechao", ""], ["Ruan", "Feng", ""], ["Yao", "Yuan", ""]]}, {"id": "1604.05976", "submitter": "Zhaobin Kuang", "authors": "Zhaobin Kuang, James Thomson, Michael Caldwell, Peggy Peissig, Ron\n  Stewart, David Page", "title": "Computational Drug Repositioning Using Continuous Self-controlled Case\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Drug Repositioning (CDR) is the task of discovering potential\nnew indications for existing drugs by mining large-scale heterogeneous\ndrug-related data sources. Leveraging the patient-level temporal ordering\ninformation between numeric physiological measurements and various drug\nprescriptions provided in Electronic Health Records (EHRs), we propose a\nContinuous Self-controlled Case Series (CSCCS) model for CDR. As an initial\nevaluation, we look for drugs that can control Fasting Blood Glucose (FBG)\nlevel in our experiments. Applying CSCCS to the Marshfield Clinic EHR,\nwell-known drugs that are indicated for controlling blood glucose level are\nrediscovered. Furthermore, some drugs with recent literature support for the\npotential effect of blood glucose level control are also identified.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 14:28:44 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Kuang", "Zhaobin", ""], ["Thomson", "James", ""], ["Caldwell", "Michael", ""], ["Peissig", "Peggy", ""], ["Stewart", "Ron", ""], ["Page", "David", ""]]}, {"id": "1604.06003", "submitter": "Andrew L. Johnson", "authors": "Daisuke Yagi, Yining Chen, Andrew L. Johnson, and Timo Kuosmanen", "title": "Shape constrained kernel-weighted least squares: Application to\n  production function estimation for Chilean manufacturing industries", "comments": "JEL Codes: C14, D24 Keywords: Kernel Estimation, Multivariate Convex\n  Regression, Nonparametric regression, Shape Constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a novel way of imposing shape constraints on a local\npolynomial kernel estimator. The proposed approach is referred to as Shape\nConstrained Kernel-weighted Least Squares (SCKLS). We prove uniform consistency\nof the SCKLS estimator with monotonicity and convexity/concavity constraints\nand establish its convergence rate. The competitiveness of SCKLS is shown in a\ncomprehensive simulation study. Finally, we analyze Chilean manufacturing data\nusing the SCKLS estimator and quantify production in the plastics and wood\nindustries. The results show that exporting firms have significantly higher\nproductivity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 15:31:20 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 13:50:51 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 04:01:41 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 17:20:48 GMT"}, {"version": "v5", "created": "Thu, 18 Jan 2018 00:10:25 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Yagi", "Daisuke", ""], ["Chen", "Yining", ""], ["Johnson", "Andrew L.", ""], ["Kuosmanen", "Timo", ""]]}, {"id": "1604.06167", "submitter": "Matthew Shum", "authors": "Kirill Pogorelskiy, Emerson Melo, Matthew Shum", "title": "Testing the Quantal Response Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a non-parametric test for consistency of players'\nbehavior in a series of games with the Quantal Response Equilibrium (QRE). The\ntest exploits a characterization of the equilibrium choice probabilities in any\nstructural QRE as the gradient of a convex function, which thus satisfies the\ncyclic monotonicity inequalities. Our testing procedure utilizes recent\neconometric results for moment inequality models. We assess our test using lab\nexperimental data from a series of generalized matching pennies games. We\nreject the QRE hypothesis in the pooled data, but it cannot be rejected in the\nindividual data for over half of the subjects.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 02:55:36 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Pogorelskiy", "Kirill", ""], ["Melo", "Emerson", ""], ["Shum", "Matthew", ""]]}, {"id": "1604.06229", "submitter": "Anna Tovo", "authors": "Anna Tovo, Marco Formentin, Marco Favretti, Amos Maritan", "title": "Application of optimal data-based binning method to spatial analysis of\n  ecological datasets", "comments": "49 pages, 25 figures", "journal-ref": "Spatial Statistics, Volume 16, May 2016, Pages 137-151", "doi": "10.1016/j.spasta.2016.02.006", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of highly structured data sets to unveil statistical\nregularities is of major importance in complex system research. The first step\nis to choose the scale at which to observe the process, the most informative\nscale being the one that includes the important features while disregarding\nnoisy details in the data. In the investigation of spatial patterns, the\noptimal scale defines the optimal bin size of the histogram in which to\nvisualize the empirical density of the pattern. In this paper we investigate a\nmethod proposed recently by K.~H.~Knuth to find the optimal bin size of an\nhistogram as a tool for statistical analysis of spatial point processes. We\ntest it through numerical simulations on various spatial processes which are of\ninterest in ecology. We show that Knuth optimal bin size rule reducing noisy\nfluctuations performs better than standard kernel methods to infer the\nintensity of the underlying process. Moreover it can be used to highlight\nrelevant spatial characteristics of the underlying distribution such as space\nanisotropy and clusterization. We apply these findings to analyse cluster-like\nstructures in plants' arrangement of Barro Colorado Island rainforest.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 09:44:42 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Tovo", "Anna", ""], ["Formentin", "Marco", ""], ["Favretti", "Marco", ""], ["Maritan", "Amos", ""]]}, {"id": "1604.06308", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti and Indrani Mukherjee", "title": "Some estimators of the PDF and CDF of the Lindley Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the different methods of estimation of the probability\ndensity function (PDF) and the cumulative distribution function (CDF) for the\nLindley distribution. Following estimation methods are considered: uniformly\nminimum variance unbiased estimator (UMVUE), maximum likelihood estimator\n(MLE), percentile estimator (PCE), least square estimator (LSE), weighted least\nsquare estimator (WLSE), Cram\\'{e}r-von-Mises estimator (CVME),\nAnderson-Darling estimator (ADE). Monte Carlo simulations are performed to\ncompare the performances of the proposed methods of estimation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 13:54:54 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Mukherjee", "Indrani", ""]]}, {"id": "1604.06335", "submitter": "Adam Kashlak", "authors": "Adam B. Kashlak, Eoin Devane, Helge Dietert, Henry Jackson", "title": "Markov models for ocular fixation locations in the presence and absence\n  of colour", "comments": "12 pages, 8 Figures, 1 Table", "journal-ref": "JRSS-C 67 (2018) 201-215", "doi": "10.1111/rssc.12223", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the fixation locations of the human eye when observing a\nstill image by a Markovian point process in R 2 . Our approach is data driven\nusing k-means clustering of the fixation locations to identify distinct salient\nregions of the image, which in turn correspond to the states of our Markov\nchain. Bayes factors are computed as model selection criterion to determine the\nnumber of clusters. Furthermore, we demonstrate that the behaviour of the human\neye differs from this model when colour information is removed from the given\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 14:50:27 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kashlak", "Adam B.", ""], ["Devane", "Eoin", ""], ["Dietert", "Helge", ""], ["Jackson", "Henry", ""]]}, {"id": "1604.06651", "submitter": "Joshua Snoke", "authors": "Joshua Snoke and Gillian Raab and Beata Nowok and Chris Dibben and\n  Aleksandra Slavkovic", "title": "General and specific utility measures for synthetic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data holders can produce synthetic versions of datasets when concerns about\npotential disclosure restrict the availability of the original records. This\npaper is concerned with methods to judge whether such synthetic data have a\ndistribution that is comparable to that of the original data, what we will term\ngeneral utility. We consider how general utility compares with specific\nutility, the similarity of results of analyses from the synthetic data and the\noriginal data. We adapt a previous general measure of data utility, the\npropensity score mean-squared-error (pMSE), to the specific case of synthetic\ndata and derive its distribution for the case when the correct synthesis model\nis used to create the synthetic data. Our asymptotic results are confirmed by a\nsimulation study. We also consider two specific utility measures, confidence\ninterval overlap and standardized difference in summary statistics, which we\ncompare with the general utility results. We present two examples examining\nthis comparison of general and specific utility to real data syntheses and make\nrecommendations for their use for evaluating synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 13:36:58 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 02:15:46 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Snoke", "Joshua", ""], ["Raab", "Gillian", ""], ["Nowok", "Beata", ""], ["Dibben", "Chris", ""], ["Slavkovic", "Aleksandra", ""]]}, {"id": "1604.06716", "submitter": "Guy Nason Prof.", "authors": "Guy P. Nason, Ben Powell, Duncan Elliott and Paul A. Smith", "title": "Supplementary Material for \"Should we sample a time series more\n  frequently? Decision support via multirate spectrum estimation (with\n  discussion)\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report includes an assortment of technical details and\nextended discussions related to paper \"Should we sample a time series more\nfrequently? Decision support via multirate spectrum estimation (with\ndiscussion)\", which introduces a model for estimating the log-spectral density\nof a stationary discrete time process given systematically missing data and\nmodels the cost implication for changing the sampling rate.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 15:42:25 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Nason", "Guy P.", ""], ["Powell", "Ben", ""], ["Elliott", "Duncan", ""], ["Smith", "Paul A.", ""]]}, {"id": "1604.07031", "submitter": "Elizabeth Lorenzi", "authors": "Elizabeth C. Lorenzi, Stephanie L. Brown, Zhifei Sun, Katherine Heller", "title": "Predictive Hierarchical Clustering: Learning clusters of CPT codes for\n  improving surgical outcomes", "comments": "Accepted at MLHC 2017 to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for\nagglomerative hierarchical clustering of current procedural terminology (CPT)\ncodes. Our predictive hierarchical clustering aims to cluster subgroups, not\nindividual observations, found within our data, such that the clusters\ndiscovered result in optimal performance of a classification model. Therefore,\nmerges are chosen based on a Bayesian hypothesis test, which chooses pairings\nof the subgroups that result in the best model fit, as measured by held out\npredictive likelihoods. We place a Dirichlet prior on the probability of\nmerging clusters, allowing us to adjust the size and sparsity of clusters. The\nmotivation is to predict patient-specific surgical outcomes using data from ACS\nNSQIP (American College of Surgeon's National Surgical Quality Improvement\nProgram). An important predictor of surgical outcomes is the actual surgical\nprocedure performed as described by a CPT code. We use PHC to cluster CPT\ncodes, represented as subgroups, together in a way that enables us to better\npredict patient-specific outcomes compared to currently used clusters based on\nclinical judgment.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 19:02:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Lorenzi", "Elizabeth C.", ""], ["Brown", "Stephanie L.", ""], ["Sun", "Zhifei", ""], ["Heller", "Katherine", ""]]}, {"id": "1604.07088", "submitter": "Shankar Krishnan", "authors": "Shankar Krishnan and Harpreet S. Dhillon", "title": "Effect of User Mobility on the Performance of Device-to-Device Networks\n  with Distributed Caching", "comments": "To appear in IEEE Wireless Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed caching device-to-device (D2D) network in which a\nuser's file of interest is cached as several portions in the storage of other\ndevices in the network. Assuming that the user needs to obtain all these file\nportions, the portions cached farther away naturally become the performance\nbottleneck. This is due to the fact that dominant interferers may be closer to\nthe receiver than the serving device. Using a simple stochastic geometry model,\nwe concretely demonstrate that this bottleneck can be loosened if the users are\nmobile. Gains obtained from mobility are quantified in terms of coverage\nprobability.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 22:31:58 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 22:02:36 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Krishnan", "Shankar", ""], ["Dhillon", "Harpreet S.", ""]]}, {"id": "1604.07093", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Leonid Sigal", "title": "Semi-supervised Vocabulary-informed Learning", "comments": "10 pages, Accepted by CVPR 2016 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in object categorization, in recent years, a\nnumber of important challenges remain, mainly, ability to learn from limited\nlabeled data and ability to recognize object classes within large, potentially\nopen, set of labels. Zero-shot learning is one way of addressing these\nchallenges, but it has only been shown to work with limited sized class\nvocabularies and typically requires separation between supervised and\nunsupervised classes, allowing former to inform the latter but not vice versa.\nWe propose the notion of semi-supervised vocabulary-informed learning to\nalleviate the above mentioned challenges and address problems of supervised,\nzero-shot and open set recognition using a unified framework. Specifically, we\npropose a maximum margin framework for semantic manifold-based recognition that\nincorporates distance constraints from (both supervised and unsupervised)\nvocabulary atoms, ensuring that labeled samples are projected closest to their\ncorrect prototypes, in the embedding space, than to others. We show that\nresulting model shows improvements in supervised, zero-shot, and large open set\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 23:36:36 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Fu", "Yanwei", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.07098", "submitter": "Minkyoung Kang", "authors": "Minkyoung Kang and Brani Vidakovic", "title": "WavmatND: A MATLAB Package for Non-Decimated Wavelet Transform and its\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-decimated wavelet transform (NDWT) is a popular version of wavelet\ntransforms because of its many advantages in applications. The inherent\nredundancy of this transform proved beneficial in tasks of signal denoising and\nscaling assessment. To facilitate the use of NDWT, we built a MATLAB package,\n{\\bf WavmatND}, which has three novel features: First, for signals of moderate\nsize the proposed method reduces computation time of the NDWT by replacing\nrepetitive convolutions with matrix multiplications. Second, submatrices of an\nNDWT matrix can be rescaled, which enables a straightforward inverse transform.\nFinally, the method has no constraints on a size of the input signal in one or\nin two dimensions, so signals of non-dyadic length and rectangular\ntwo-dimensional signals with non-dyadic sides can be readily transformed. We\nprovide illustrative examples and a tutorial to assist users in application of\nthis stand-alone package.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 23:58:02 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Kang", "Minkyoung", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1604.07299", "submitter": "Matthew Moores", "authors": "Matthew Moores, Kirsten Gracie, Jake Carson, Karen Faulds, Duncan\n  Graham, Mark Girolami", "title": "Bayesian modelling and quantification of Raman spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman spectroscopy can be used to identify molecules such as DNA by the\ncharacteristic scattering of light from a laser. It is sensitive at very low\nconcentrations and can accurately quantify the amount of a given molecule in a\nsample. The presence of a large, nonuniform background presents a major\nchallenge to analysis of these spectra. To overcome this challenge, we\nintroduce a sequential Monte Carlo (SMC) algorithm to separate each observed\nspectrum into a series of peaks plus a smoothly-varying baseline, corrupted by\nadditive white noise. The peaks are modelled as Lorentzian, Gaussian, or\npseudo-Voigt functions, while the baseline is estimated using a penalised cubic\nspline. This latent continuous representation accounts for differences in\nresolution between measurements. The posterior distribution can be\nincrementally updated as more data becomes available, resulting in a scalable\nalgorithm that is robust to local maxima. By incorporating this representation\nin a Bayesian hierarchical regression model, we can quantify the relationship\nbetween molecular concentration and peak intensity, thereby providing an\nimproved estimate of the limit of detection, which is of major importance to\nanalytical chemistry.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 15:07:29 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 22:22:30 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Moores", "Matthew", ""], ["Gracie", "Kirsten", ""], ["Carson", "Jake", ""], ["Faulds", "Karen", ""], ["Graham", "Duncan", ""], ["Girolami", "Mark", ""]]}, {"id": "1604.07304", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen and Luca Rossini and Cristiano Villa", "title": "A Note on the Posterior Inference for the Yule-Simon Distribution", "comments": "Forthcoming in the \"Journal of Statistical Computation and\n  Simulation\" - 12 pages, 4 Figures, 3 Tables", "journal-ref": "Journal of Statistical Computation and Simulation (2017), 87:6,\n  1179-1188", "doi": "10.1080/00949655.2016.1255741", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule--Simon distribution has been out of the radar of the Bayesian\ncommunity, so far. In this note, we propose an explicit Gibbs sampling scheme\nwhen a Gamma prior is chosen for the shape parameter. The performance of the\nalgorithm is illustrated with simulation studies, including count data\nregression, and a real data application to text analysis. We compare our\nproposal to the frequentist counterparts showing better performance of our\nalgorithm when a small sample size is considered.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 15:14:28 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 16:23:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "1604.07689", "submitter": "Peter Klimek", "authors": "Ra\\'ul Jim\\'enez, Manuel Hidalgo, Peter Klimek", "title": "Testing for voter rigging in small polling stations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the 1970s there has been a large number of countries that combine\nformal democratic institutions with authoritarian practices. Although in such\ncountries the ruling elites may receive considerable voter support they often\nemploy several manipulation tools to control election outcomes. A common\npractice of these regimes is the coercion and mobilization of a significant\namount of voters to guarantee the electoral victory. This electoral\nirregularity is known as voter rigging, distinguishing it from vote rigging,\nwhich involves ballot stuffing or stealing. Here we develop a statistical test\nto quantify to which extent the results of a particular election display traces\nof voter rigging. Our key hypothesis is that small polling stations are more\nsusceptible to voter rigging, because it is easier to identify opposing\nindividuals, there are less eye witnesses, and supposedly less visits from\nelection observers. We devise a general statistical method for testing whether\nvoting behavior in small polling stations is significantly different from the\nbehavior of their neighbor stations in a way that is consistent with the\nwidespread occurrence of voter rigging. Based on a comparative analysis, the\nmethod enables to rule out whether observed differences in voting behavior\nmight be explained by geographic heterogeneities in vote preferences. We\nanalyze 21 elections in ten different countries and find significant anomalies\ncompatible with voter rigging in Russia from 2007-2011, in Venezuela from\n2006-2013, and in Uganda 2011. Particularly disturbing is the case of Venezuela\nwhere these distortions have been outcome-determinative in the 2013\npresidential elections.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 14:29:08 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Jim\u00e9nez", "Ra\u00fal", ""], ["Hidalgo", "Manuel", ""], ["Klimek", "Peter", ""]]}, {"id": "1604.07969", "submitter": "Keren Shen", "authors": "Keren Shen, Jianfeng Yao and Wai Keung Li", "title": "On the Surprising Explanatory Power of Higher Realized Moments in\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realized moments of higher order computed from intraday returns are\nintroduced in recent years. The literature indicates that realized skewness is\nan important factor in explaining future asset returns. However, the literature\nmainly focuses on the whole market and on the monthly or weekly scale. In this\npaper, we conduct an extensive empirical analysis to investigate the\nforecasting abilities of realized skewness and realized kurtosis towards\nindividual stock's future return and variance in the daily scale. It is found\nthat realized kurtosis possesses significant forecasting power for the stock's\nfuture variance. In the meanwhile, realized skewness is lack of explanatory\npower for the future daily return for individual stocks with a short horizon,\nin contrast with the existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 08:05:59 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Shen", "Keren", ""], ["Yao", "Jianfeng", ""], ["Li", "Wai Keung", ""]]}, {"id": "1604.08462", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Denny Borsboom and Eiko I. Fried", "title": "Estimating Psychological Networks and their Accuracy: A Tutorial Paper", "comments": "Accepted for publication in Behavior Research Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of psychological networks that conceptualize psychological behavior\nas a complex interplay of psychological and other components has gained\nincreasing popularity in various fields of psychology. While prior publications\nhave tackled the topics of estimating and interpreting such networks, little\nwork has been conducted to check how accurate (i.e., prone to sampling\nvariation) networks are estimated, and how stable (i.e., interpretation remains\nsimilar with less observations) inferences from the network structure (such as\ncentrality indices) are. In this tutorial paper, we aim to introduce the reader\nto this field and tackle the problem of accuracy under sampling variation. We\nfirst introduce the current state-of-the-art of network estimation. Second, we\nprovide a rationale why researchers should investigate the accuracy of\npsychological networks. Third, we describe how bootstrap routines can be used\nto (A) assess the accuracy of estimated network connections, (B) investigate\nthe stability of centrality indices, and (C) test whether network connections\nand centrality estimates for different variables differ from each other. We\nintroduce two novel statistical methods: for (B) the correlation stability\ncoefficient, and for (C) the bootstrapped difference test for edge-weights and\ncentrality indices. We conducted and present simulation studies to assess the\nperformance of both methods. Finally, we developed the free R-package bootnet\nthat allows for estimating psychological networks in a generalized framework in\naddition to the proposed bootstrap methods. We showcase bootnet in a tutorial,\naccompanied by R syntax, in which we analyze a dataset of 359 women with\nposttraumatic stress disorder available online.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 15:24:48 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 23:27:29 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 10:02:17 GMT"}, {"version": "v4", "created": "Fri, 20 Jan 2017 09:52:22 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Epskamp", "Sacha", ""], ["Borsboom", "Denny", ""], ["Fried", "Eiko I.", ""]]}, {"id": "1604.08654", "submitter": "Eric Lock", "authors": "Eric F. Lock and David B. Dunson", "title": "Bayesian Genome- and Epigenome-wide Association Studies with Gene Level\n  Dependence", "comments": "23 pages, 7 figures", "journal-ref": "Biometrics 73(3), 1018-1028, 2017", "doi": "10.1111/biom.12649", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput genetic and epigenetic data are often screened for\nassociations with an observed phenotype. For example, one may wish to test\nhundreds of thousands of genetic variants, or DNA methylation sites, for an\nassociation with disease status. These genomic variables can naturally be\ngrouped by the gene they encode, among other criteria. However, standard\npractice in such applications is independent screening with a universal\ncorrection for multiplicity. We propose a Bayesian approach in which the prior\nprobability of an association for a given genomic variable depends on its gene,\nand the gene-specific probabilities are modeled nonparametrically. This\nhierarchical model allows for appropriate gene and genome-wide multiplicity\nadjustments, and can be incorporated into a variety of Bayesian association\nscreening methodologies with negligible increase in computational complexity.\nWe describe an application to screening for differences in DNA methylation\nbetween lower grade glioma and glioblastoma multiforme tumor samples from The\nCancer Genome Atlas. Software is available via the package BayesianScreening\nfor R at https://github.com/lockEF/BayesianScreening .\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 00:12:38 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Lock", "Eric F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1604.08853", "submitter": "Rui Martins", "authors": "Rui Martins", "title": "Joint Dispersion Model with a Flexible Link", "comments": "27 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to model longitudinal and survival data jointly taking into\naccount the dependence between the two responses in a real HIV/AIDS dataset\nusing a shared parameter approach inside a Bayesian framework. We propose a\nlinear mixed effects dispersion model to adjust the CD4 longitudinal biomarker\ndata with a between-individual heterogeneity in the mean and variance. In doing\nso we are relaxing the usual assumption of a common variance for the\nlongitudinal residuals. A hazard regression model is considered in addition to\nmodel the time since HIV/AIDS diagnostic until failure, being the coefficients,\naccounting for the linking between the longitudinal and survival processes,\ntime-varying. This flexibility is specified using Penalized Splines and allows\nthe relationship to vary in time. Because heteroscedasticity may be related\nwith the survival, the standard deviation is considered as a covariate in the\nhazard model, thus enabling to study the effect of the CD4 counts' stability on\nthe survival. The proposed framework outperforms the most used joint models,\nhighlighting the importance in correctly taking account the individual\nheterogeneity for the measurement errors variance and the evolution of the\ndisease over time in bringing new insights to better understand this\nbiomarker-survival relation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 14:32:51 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Martins", "Rui", ""]]}]