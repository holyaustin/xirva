[{"id": "1311.0343", "submitter": "Robin Willink", "authors": "R. Willink and B. D. Hall", "title": "An extension to GUM methodology: degrees-of-freedom calculations for\n  correlated multidimensional estimates", "comments": "30 pages with 2 embedded figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Guide to the Expression of Uncertainty in Measurement advocates the use\nof an 'effective number of degrees of freedom' for the calculation of an\ninterval of measurement uncertainty. However, it does not describe how this\nnumber is to be calculated when (i) the measurand is a vector quantity or (ii)\nwhen the errors in the estimates of the quantities defining the measurand (the\n'input quantities') are not incurred independently. An appropriate analysis for\na vector-valued measurand has been described (Metrologia 39 (2002) 361-9), and\na method for a one-dimensional measurand with dependent errors has also been\ngiven (Metrologia 44 (2007) 340-9). This paper builds on those analyses to\npresent a method for the situation where the problem is multidimensional and\ninvolves correlated errors. The result is an explicit general procedure that\nreduces to simpler procedures where appropriate. The example studied is from\nthe field of radio-frequency metrology, where measured quantities are often\ncomplex-valued and can be regarded as vectors of two elements.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 05:06:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Willink", "R.", ""], ["Hall", "B. D.", ""]]}, {"id": "1311.0376", "submitter": "Fabrizio Lecci", "authors": "Fr\\'ed\\'eric Chazal, Brittany Terese Fasy, Fabrizio Lecci, Alessandro\n  Rinaldo, Aarti Singh, Larry Wasserman", "title": "On the Bootstrap for Persistence Diagrams and Landscapes", "comments": null, "journal-ref": "Modeling and Analysis of Information Systems, 20(6), 96-105", "doi": null, "report-no": null, "categories": "math.AT cs.CG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology probes topological properties from point clouds and\nfunctions. By looking at multiple scales simultaneously, one can record the\nbirths and deaths of topological features as the scale varies. In this paper we\nuse a statistical technique, the empirical bootstrap, to separate topological\nsignal from topological noise. In particular, we derive confidence sets for\npersistence diagrams and confidence bands for persistence landscapes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 13:07:38 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 19:15:20 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Fasy", "Brittany Terese", ""], ["Lecci", "Fabrizio", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1311.0407", "submitter": "Joan Bruna", "authors": "Joan Bruna and St\\'ephane Mallat", "title": "Audio Texture Synthesis with Scattering Moments", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an audio texture synthesis algorithm based on scattering\nmoments. A scattering transform is computed by iteratively decomposing a signal\nwith complex wavelet filter banks and computing their amplitude envelop.\nScattering moments provide general representations of stationary processes\ncomputed as expected values of scattering coefficients. They are estimated with\nlow variance estimators from single realizations. Audio signals having\nprescribed scattering moments are synthesized with a gradient descent\nalgorithms. Audio synthesis examples show that scattering representation\nprovide good synthesis of audio textures with much fewer coefficients than the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 20:37:34 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1311.0416", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Elizaveta Levina, Kerby A. Shedden", "title": "Structured functional regression models for high-dimensional spatial\n  spectroscopy data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and analysis of spectroscopy data is an active area of research with\napplications to chemistry and biology. This paper focuses on analyzing Raman\nspectra obtained from a bone fracture healing experiment, although the\nfunctional regression model for predicting a scalar response from\nhigh-dimensional tensors can be applied to any spectroscopy data. The\nregression model is built on a sparse functional representation of the spectra,\nand accommodates multiple spatial dimensions. We apply our models to the task\nof predicting bone-mineral-density (BMD), an important indicator of fracture\nhealing, from Raman spectra, in both the in vivo and ex vivo settings of the\nbone fracture healing experiment. To illustrate the general applicability of\nthe method, we also use it to predict lipoprotein concentrations from spectra\nobtained by nuclear magnetic resonance (NMR) spectroscopy.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 23:13:39 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Amini", "Arash A.", ""], ["Levina", "Elizaveta", ""], ["Shedden", "Kerby A.", ""]]}, {"id": "1311.0431", "submitter": "Ian Johnston", "authors": "Ian Johnston, Timothy Hancock, Hiroshi Mamitsuka and Luis Carvalho", "title": "Gene-Proximity Models For Genome-Wide Association Studies", "comments": "27 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the important problem of detecting association between genetic\nmarkers and binary traits in genome-wide association studies, we present a\nnovel Bayesian model that establishes a hierarchy between markers and genes by\ndefining weights according to gene lengths and distances from genes to markers.\nThe proposed hierarchical model uses these weights to define unique prior\nprobabilities of association for markers based on their proximities to genes\nthat are believed to be relevant to the trait of interest. We use an\nexpectation-maximization algorithm in a filtering step to first reduce the\ndimensionality of the data and then sample from the posterior distribution of\nthe model parameters to estimate posterior probabilities of association for the\nmarkers. We offer practical and meaningful guidelines for the selection of the\nmodel tuning parameters and propose a pipeline that exploits a singular value\ndecomposition on the raw data to make our model run efficiently on large data\nsets. We demonstrate the performance of the model in a simulation study and\nconclude by discussing the results of a case study using a real-world dataset\nprovided by the Wellcome Trust Case Control Consortium.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 05:36:14 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 11:07:39 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Johnston", "Ian", ""], ["Hancock", "Timothy", ""], ["Mamitsuka", "Hiroshi", ""], ["Carvalho", "Luis", ""]]}, {"id": "1311.0510", "submitter": "Jakob Vovnoboy", "authors": "Jakob Vovnoboy and Ami Wiesel", "title": "Compressed matched filter for non-Gaussian noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of a deterministic unknown parameter vector in a\nlinear model with non-Gaussian noise. In the Gaussian case, dimensionality\nreduction via a linear matched filter provides a simple low dimensional\nsufficient statistic which can be easily communicated and/or stored for future\ninference. Such a statistic is usually unknown in the general non-Gaussian\ncase. Instead, we propose a hybrid matched filter coupled with a randomized\ncompressed sensing procedure, which together create a low dimensional\nstatistic. We also derive a complementary algorithm for robust reconstruction\ngiven this statistic. Our recovery method is based on the fast iterative\nshrinkage and thresholding algorithm which is used for outlier rejection given\nthe compressed data. We demonstrate the advantages of the proposed framework\nusing synthetic simulations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 19:14:16 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Vovnoboy", "Jakob", ""], ["Wiesel", "Ami", ""]]}, {"id": "1311.0644", "submitter": "Ozgur Asar", "authors": "Ozgur Asar, Ozlem Ilk", "title": "Forecasting multivariate longitudinal binary data with marginal and\n  marginally specified models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Forecasting with longitudinal data has been rarely studied. Most of the\navailable studies are for continuous response and all of them are for\nunivariate response. In this study, we consider forecasting multivariate\nlongitudinal binary data. Five different models including simple ones,\nunivariate and multivariate marginal models, and complex ones, marginally\nspecified models, are studied to forecast such data. Model forecasting\nabilities are illustrated via a real life data set and a simulation study. The\nsimulation study includes a model independent data generation to provide a fair\nenvironment for model competitions. Independent variables are forecast as well\nas the dependent ones to mimic the real life cases best. Several accuracy\nmeasures are considered to compare model forecasting abilities. Results show\nthat complex models yield better forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 10:58:28 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 22:51:43 GMT"}, {"version": "v3", "created": "Wed, 12 Mar 2014 02:39:05 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Asar", "Ozgur", ""], ["Ilk", "Ozlem", ""]]}, {"id": "1311.0660", "submitter": "Craig Anderson Mr", "authors": "Craig Anderson, Duncan Lee, Nema Dean", "title": "Identifying Clusters in Bayesian Disease Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease mapping is the field of spatial epidemiology interested in estimating\nthe spatial pattern in disease risk across $n$ areal units. One aim is to\nidentify units exhibiting elevated disease risks, so that public health\ninterventions can be made. Bayesian hierarchical models with a spatially smooth\nconditional autoregressive prior are used for this purpose, but they cannot\nidentify the spatial extent of high-risk clusters. Therefore we propose a two\nstage solution to this problem, with the first stage being a spatially adjusted\nhierarchical agglomerative clustering algorithm. This algorithm is applied to\ndata prior to the study period, and produces $n$ potential cluster structures\nfor the disease data. The second stage fits a separate Poisson log-linear model\nto the study data for each cluster structure, which allows for step-changes in\nrisk where two clusters meet. The most appropriate cluster structure is chosen\nby model comparison techniques, specifically by minimising the Deviance\nInformation Criterion. The efficacy of the methodology is established by a\nsimulation study, and is illustrated by a study of respiratory disease risk in\nGlasgow, Scotland.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 12:04:42 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Anderson", "Craig", ""], ["Lee", "Duncan", ""], ["Dean", "Nema", ""]]}, {"id": "1311.1039", "submitter": "Roland Langrock", "authors": "Th\\'eo Michelot, Roland Langrock, Thomas Kneib, Ruth King", "title": "Maximum penalized likelihood estimation in semiparametric\n  capture-recapture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the semiparametric modeling of mark-recapture-recovery data where\nthe temporal and/or individual variation of model parameters is explained via\ncovariates. Typically, in such analyses a fixed (or mixed) effects parametric\nmodel is specified for the relationship between the model parameters and the\ncovariates of interest. In this paper, we discuss the modeling of the\nrelationship via the use of penalized splines, to allow for considerably more\nflexible functional forms. Corresponding models can be fitted via numerical\nmaximum penalized likelihood estimation, employing cross-validation to choose\nthe smoothing parameters in a data-driven way. Our contribution builds on and\nextends the existing literature, providing a unified inferential framework for\nsemiparametric mark-recapture-recovery models for open populations, where the\ninterest typically lies in the estimation of survival probabilities. The\napproach is applied to two real datasets, corresponding to grey herons (Ardea\nCinerea), where we model the survival probability as a function of\nenvironmental condition (a time-varying global covariate), and Soay sheep (Ovis\nAries), where we model the survival probability as a function of individual\nweight (a time-varying individual-specific covariate). The proposed\nsemiparametric approach is compared to a standard parametric (logistic)\nregression and new interesting underlying dynamics are observed in both cases.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 13:04:59 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 09:36:43 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Michelot", "Th\u00e9o", ""], ["Langrock", "Roland", ""], ["Kneib", "Thomas", ""], ["King", "Ruth", ""]]}, {"id": "1311.1454", "submitter": "Mark Steel", "authors": "Catalina A. Vallejos and Mark F.J. Steel", "title": "On posterior propriety for the Student-$t$ linear regression model under\n  Jeffreys priors", "comments": "minor editorial changes in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with fat-tailed error terms are an increasingly popular\nchoice to obtain more robust inference to the presence of outlying\nobservations. This article focuses on Bayesian inference for the Student-$t$\nlinear regression model under objective priors that are based on the Jeffreys\nrule. Posterior propriety results presented in Fonseca et al. (2008) are\nrevisited and corrected. In particular, it is shown that the standard\nJeffreys-rule prior precludes the existence of a proper posterior distribution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 17:34:08 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 11:07:02 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Vallejos", "Catalina A.", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1311.1495", "submitter": "Igor Baskin", "authors": "Igor I. Baskin and Nelly I. Zhokhova", "title": "Continuous Molecular Fields Approach Applied to Structure-Activity\n  Modeling", "comments": "To be published in Applications of Computational Techniques in\n  Pharmacy and Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Method of Continuous Molecular Fields is a universal approach to predict\nvarious properties of chemical compounds, in which molecules are represented by\nmeans of continuous fields (such as electrostatic, steric, electron density\nfunctions, etc). The essence of the proposed approach consists in performing\nstatistical analysis of functional molecular data by means of joint application\nof kernel machine learning methods and special kernels which compare molecules\nby computing overlap integrals of their molecular fields. This approach is an\nalternative to traditional methods of building 3D structure-activity and\nstructure-property models based on the use of fixed sets of molecular\ndescriptors. The methodology of the approach is described in this chapter,\nfollowed by its application to building regression 3D-QSAR models and\nconducting virtual screening based on one-class classification models. The main\ndirections of the further development of this approach are outlined at the end\nof the chapter.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 20:50:19 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Baskin", "Igor I.", ""], ["Zhokhova", "Nelly I.", ""]]}, {"id": "1311.1797", "submitter": "Alexandre Janon", "authors": "Fabrice Gamboa (UMR CNRS 5219), Alexandre Janon (LM-Orsay, -\n  M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques),\n  Thierry Klein (IMT), Agn\\`es Lagnoux (IMT)", "title": "Sensitivity analysis for multidimensional and functional outputs", "comments": "Fixed missing references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X:=(X_1, \\ldots, X_p)$ be random objects (the inputs), defined on some\nprobability space $(\\Omega,{\\mathcal{F}}, \\mathbb P)$ and valued in some\nmeasurable space $E=E_1\\times\\ldots \\times E_p$. Further, let $Y:=Y = f(X_1,\n\\ldots, X_p)$ be the output. Here, $f$ is a measurable function from $E$ to\nsome Hilbert space $\\mathbb{H}$ ($\\mathbb{H}$ could be either of finite or\ninfinite dimension). In this work, we give a natural generalization of the\nSobol indices (that are classically defined when $Y\\in\\mathbb R$ ), when the\noutput belongs to $\\mathbb{H}$. These indices have very nice properties. First,\nthey are invariant. under isometry and scaling. Further they can be, as in\ndimension $1$, easily estimated by using the so-called Pick and Freeze method.\nWe investigate the asymptotic behaviour of such estimation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 20:09:54 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 09:42:36 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Gamboa", "Fabrice", "", "UMR CNRS 5219"], ["Janon", "Alexandre", "", "LM-Orsay, -\n  M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux", "Agn\u00e8s", "", "IMT"]]}, {"id": "1311.1981", "submitter": "Gyorgy Terdik DR", "authors": "T. Subba Rao and Gy. Terdik", "title": "A space-time covariance function for spatio-temporal random processes\n  and spatio-temporal prediction (kriging)", "comments": "29 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary spatio-temporal random process and assume that we\nhave a sample. By defining a sequence of discrete Fourier transforms at\ncanonical frequencies at each location, and using these complex valued random\nvarables as observed sample, we obtain expressions for the spatio-temporal\ncovariance functions and the spectral density functions of the spatio-temporal\nrandom processes. These spectra correspond to non separable class of random\nprocesses. The spatio-temporal covariance functions, obtained here are\nfunctions of the spatial distance and the temporal frequency and are similar to\nMatern class. These are in terms of modified Bessel functions of the second\nkind. and the parameters are in terms of the second order spectral density\nfunctions of the random proces and the spatial distances. We consider the\nestimation of the parameters of the covariance function and also briefly\nmention their asymptotic properties. The estimation of the entire data at a\nknown location, and also the estimation of a value given the above sample is\nalso considered. The predictors are obtained using the vectors of Discrete\nFourier Transforms. We also describe a statistical test for testing the\nindependence of the m spatial time series (testing for spatial independence)\nusing the Finite Fourier Transforms and it is based on the likelihood ratio\ntest of complex valued random variables The methods are illustrated with real\ndata.\n  Keywords: Discrete Fourier Transforms, Covariance functions, Spectral density\nfunctions, Space-Time Processses, Prediction(kriging) Laplacian operators,\nFrequency Variogram, Tests for independence, Whittle likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 14:23:44 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 20:20:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Rao", "T. Subba", ""], ["Terdik", "Gy.", ""]]}, {"id": "1311.2610", "submitter": "Xiaoyue Niu", "authors": "Xiaoyue Niu and Peter D. Hoff", "title": "Joint Mean and Covariance Modeling of Multiple Health Outcome Measures", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health exams determine a patient's health status by comparing the patient's\nmeasurement with a population reference range, a 95% interval derived from a\nhomogeneous reference population. Similarly, most of the established relation\namong health problems are assumed to hold for the entire population. We use\ndata from the 2009 - 2010 National Health and Nutrition Examination Survey\n(NHANES) on four major health problems in the U.S. and apply a joint mean and\ncovariance model to study how the reference ranges and associations of those\nhealth outcomes could vary among subpopulations. We discuss guidelines for\nmodel selection and evaluation, using standard criteria such as AIC in\nconjunction with posterior predictive checks. The results from the proposed\nmodel can help identify subpopulations in which more data need to be collected\nto refine the reference range and to study the specific associations among\nthose health problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 21:20:54 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 21:45:49 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2013 20:06:49 GMT"}, {"version": "v4", "created": "Wed, 26 Mar 2014 17:31:20 GMT"}, {"version": "v5", "created": "Sat, 9 Apr 2016 01:32:38 GMT"}, {"version": "v6", "created": "Fri, 10 Nov 2017 21:20:37 GMT"}, {"version": "v7", "created": "Thu, 31 May 2018 18:00:37 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Niu", "Xiaoyue", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1311.3723", "submitter": "Igor Baskin", "authors": "Igor I. Baskin, Alexandre Varnek", "title": "Fragment Descriptors in Virtual Screening", "comments": "To be published in Advances in Combinatorial Chemistry & High\n  Throughput Screening", "journal-ref": "Advances in Combinatorial Chemistry & High Throughput Screening\n  (2014), 1, 36-59", "doi": "10.2174/9781608057450113010005", "report-no": null, "categories": "physics.chem-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the application of fragment descriptors at different\nstages of virtual screening: filtering, similarity search, and direct activity\nassessment using QSAR/QSPR models. Several case studies are considered. It is\ndemonstrated that the power of fragment descriptors stems from their\nuniversality, very high computational efficiency, simplicity of interpretation\nand versatility.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 04:23:09 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Baskin", "Igor I.", ""], ["Varnek", "Alexandre", ""]]}, {"id": "1311.3799", "submitter": "Ugo Bardi", "authors": "Ugo Bardi and Virginia Pierini", "title": "Declining trends of healthy life years expectancy (HLYE) in Europe", "comments": "8 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the trends in Healthy Life Years Expectancy (HLYE) at birth during\nthe past few decades in Europe. We observe that several European countries show\na significant drop in HLYE at birth starting from 2003; interrupting what had\nbeen a previous continuous increase. We discuss the possible causes of this\ndrop, including a possible correlation with climate change in terms of the heat\nwave experienced in Europe in 2003. It is not possible, at present, to propose\na single explanation for this phenomenon, however the trend is worrisome and\nits causes should be investigated further.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 10:33:19 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 18:47:56 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2013 13:51:28 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Bardi", "Ugo", ""], ["Pierini", "Virginia", ""]]}, {"id": "1311.3981", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Robust Bayesian FDR Control using Bayes Factors, with Applications to\n  Multi-tissue eQTL Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the genomic application of expression quantitative trait loci\n(eQTL) mapping, we propose a new procedure to perform simultaneous testing of\nmultiple hypotheses using Bayes factors as input test statistics. One of the\nmost significant features of this method is its robustness in controlling the\ntargeted false discovery rate (FDR) even under misspecifications of parametric\nalternative models. Moreover, the proposed procedure is highly computationally\nefficient, which is ideal for treating both complex system and big data in\ngenomic applications. We discuss the theoretical properties of the new\nprocedure and demonstrate its power and computational efficiency in\napplications of single-tissue and multi-tissue eQTL mapping.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 21:22:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 17:39:42 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1311.4043", "submitter": "arXiv Admin", "authors": "Gong Zi Jiang Nan", "title": "High Dimensional Tests Based on U-Statistics for Generalized Linear\n  Models", "comments": "This submission has been withdrawn by arXiv administrators due to\n  disputed authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose two U-statistics to test coefficients in generalized linear models.\nOne of them is used to deal with global hypothesis and the other one to test\nwith the nuisance parameter. Both the statistics proposed are within\nhigh-dimensional setting which means the number of coefficients is much larger\nthan the sample size. The statistics are based on quasi-likelihood function so\nthat they have wilder applications. I theoretically analyze the asymptotic\ndistribution of the statistics under the null hypothesis and the power\nfunctions under the local and fixed alternatives. To serve as a comparison, the\npower functions of the test proposed by Goeman et al. (2011) are also derived.\nSome simulation studies are carried out and I apply my methods to an empirical\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 10:07:59 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 21:15:23 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Nan", "Gong Zi Jiang", ""]]}, {"id": "1311.4104", "submitter": "Joan Bruna", "authors": "Joan Bruna, St\\'ephane Mallat, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy", "title": "Intermittent process analysis with scattering moments", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1276 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 323-351", "doi": "10.1214/14-AOS1276", "report-no": "IMS-AOS-AOS1276", "categories": "stat.ME math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattering moments provide nonparametric models of random processes with\nstationary increments. They are expected values of random variables computed\nwith a nonexpansive operator, obtained by iteratively applying wavelet\ntransforms and modulus nonlinearities, which preserves the variance. First- and\nsecond-order scattering moments are shown to characterize intermittency and\nself-similarity properties of multiscale processes. Scattering moments of\nPoisson processes, fractional Brownian motions, L\\'{e}vy processes and\nmultifractal random walks are shown to have characteristic decay. The\nGeneralized Method of Simulated Moments is applied to scattering moments to\nestimate data generating models. Numerical applications are shown on financial\ntime-series and on energy dissipation of turbulent flows.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 00:40:18 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2013 17:59:31 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2015 12:50:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""], ["Bacry", "Emmanuel", ""], ["Muzy", "Jean-Fran\u00e7ois", ""]]}, {"id": "1311.4136", "submitter": "Gilles Guillot", "authors": "Gilles Guillot, Ren\\'e Schilling, Emilio Porcu, Moreno Bevilacqua", "title": "Validity of covariance models for the analysis of geographical variation", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to the availability of large molecular data-sets, covariance models are\nincreasingly used to describe the structure of genetic variation as an\nalternative to more heavily parametrised biological models. We focus here on a\nclass of parametric covariance models that received sustained attention lately\nand show that the conditions under which they are valid mathematical models\nhave been overlooked so far. We provide rigorous results for the construction\nof valid covariance models in this family. We also outline how to construct\nalternative covariance models for the analysis of geographical variation that\nare both mathematically well behaved and easily implementable.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 09:35:43 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2013 15:40:33 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2013 08:13:01 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Guillot", "Gilles", ""], ["Schilling", "Ren\u00e9", ""], ["Porcu", "Emilio", ""], ["Bevilacqua", "Moreno", ""]]}, {"id": "1311.4163", "submitter": "Earnest Akofor", "authors": "Earnest Akofor, Biao Chen", "title": "Interactive Distributed Detection: Architecture and Performance Analysis", "comments": "32 pages, 7 figures, Final version, Published in IEEE Transactions on\n  Information Theory", "journal-ref": "Interactive Distributed Detection: Architecture and Performance\n  Analysis, IEEE Trans. Info. Theory, Vol. 60, Issue 10, pgs. 6456 - 6473, 2014", "doi": "10.1109/TIT.2014.2346497", "report-no": null, "categories": "cs.IT math.IT math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the impact of interactive fusion on detection performance\nin tandem fusion networks with conditionally independent observations. Within\nthe Neyman-Pearson framework, two distinct regimes are considered: the fixed\nsample size test and the large sample test. For the former, it is established\nthat interactive distributed detection may strictly outperform the one-way\ntandem fusion structure. However, for the large sample regime, it is shown that\ninteractive fusion has no improvement on the asymptotic performance\ncharacterized by the Kullback-Leibler (KL) distance compared with the simple\none-way tandem fusion. The results are then extended to interactive fusion\nsystems where the fusion center and the sensor may undergo multiple steps of\nmemoryless interactions or that involve multiple peripheral sensors, as well as\nto interactive fusion with soft sensor outputs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 14:27:15 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 01:24:53 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 02:45:12 GMT"}, {"version": "v4", "created": "Mon, 10 Mar 2014 19:35:54 GMT"}, {"version": "v5", "created": "Mon, 28 Jul 2014 17:07:47 GMT"}, {"version": "v6", "created": "Thu, 18 Sep 2014 01:31:45 GMT"}, {"version": "v7", "created": "Fri, 19 Sep 2014 00:49:22 GMT"}, {"version": "v8", "created": "Mon, 29 Sep 2014 17:41:52 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Akofor", "Earnest", ""], ["Chen", "Biao", ""]]}, {"id": "1311.4276", "submitter": "Michael (Micky) Fire", "authors": "Michael Fire and Yuval Elovici", "title": "Data Mining of Online Genealogy Datasets for Revealing Lifespan Patterns\n  in Human Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online genealogy datasets contain extensive information about millions of\npeople and their past and present family connections. This vast amount of data\ncan assist in identifying various patterns in human population. In this study,\nwe present methods and algorithms which can assist in identifying variations in\nlifespan distributions of human population in the past centuries, in detecting\nsocial and genetic features which correlate with human lifespan, and in\nconstructing predictive models of human lifespan based on various features\nwhich can easily be extracted from genealogy datasets.\n  We have evaluated the presented methods and algorithms on a large online\ngenealogy dataset with over a million profiles and over 9 million connections,\nall of which were collected from the WikiTree website. Our findings indicate\nthat significant but small positive correlations exist between the parents'\nlifespan and their children's lifespan. Additionally, we found slightly higher\nand significant correlations between the lifespans of spouses. We also\ndiscovered a very small positive and significant correlation between longevity\nand reproductive success in males, and a small and significant negative\ncorrelation between longevity and reproductive success in females. Moreover,\nour machine learning algorithms presented better than random classification\nresults in predicting which people who outlive the age of 50 will also outlive\nthe age of 80.\n  We believe that this study will be the first of many studies which utilize\nthe wealth of data on human populations, existing in online genealogy datasets,\nto better understand factors which influence human lifespan. Understanding\nthese factors can assist scientists in providing solutions for successful\naging.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 06:23:25 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2014 10:21:06 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Fire", "Michael", ""], ["Elovici", "Yuval", ""]]}, {"id": "1311.4411", "submitter": "Andrew Hart PhD", "authors": "Andrew Hart and Servet Mart\\'inez", "title": "Markovianness and Conditional Independence in Annotated Bacterial DNA", "comments": "23 pages (including a 5 page appendix), 1 figure, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM math.PR q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the probabilistic structure of DNA in a number of bacterial\ngenomes and conclude that a form of Markovianness is present at the boundaries\nbetween coding and non-coding regions, that is, the sequence of START and STOP\ncodons annotated for the bacterial genome. This sequence is shown to satisfy a\nconditional independence property which allows its governing Markov chain to be\nuniquely identified from the abundances of START and STOP codons. Furthermore,\nthe annotated sequence is shown to comply with Chargaff's second parity rule at\nthe codon level.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 14:47:05 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Hart", "Andrew", ""], ["Mart\u00ednez", "Servet", ""]]}, {"id": "1311.4731", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Loet Leydesdorff and Jian Wang", "title": "How to improve the prediction based on citation impact percentiles for\n  years shortly after the publication date?", "comments": "Accepted for publication in the Journal of Informetrics. arXiv admin\n  note: text overlap with arXiv:1306.4454", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The findings of Bornmann, Leydesdorff, and Wang (in press) revealed that the\nconsideration of journal impact improves the prediction of long-term citation\nimpact. This paper further explores the possibility of improving citation\nimpact measurements on the base of a short citation window by the consideration\nof journal impact and other variables, such as the number of authors, the\nnumber of cited references, and the number of pages. The dataset contains\n475,391 journal papers published in 1980 and indexed in Web of Science (WoS,\nThomson Reuters), and all annual citation counts (from 1980 to 2010) for these\npapers. As an indicator of citation impact, we used percentiles of citations\ncalculated using the approach of Hazen (1914). Our results show that citation\nimpact measurement can really be improved: If factors generally influencing\ncitation impact are considered in the statistical analysis, the explained\nvariance in the long-term citation impact can be much increased. However, this\nincrease is only visible when using the years shortly after publication but not\nwhen using later years.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 13:27:14 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""], ["Wang", "Jian", ""]]}, {"id": "1311.4922", "submitter": "Yipeng Liu Dr.", "authors": "Yurrit Avonds, Yipeng Liu, Sabine Van Huffel", "title": "Simultaneous Greedy Analysis Pursuit for Compressive Sensing of\n  Multi-Channel ECG Signals", "comments": "8 pages, 2 figures, Internal Report 13-82, ESAT-STADIUS, University\n  of Leuven, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses compressive sensing for multi-channel ECG. Compared to\nthe traditional sparse signal recovery approach which decomposes the signal\ninto the product of a dictionary and a sparse vector, the recently developed\ncosparse approach exploits sparsity of the product of an analysis matrix and\nthe original signal. We apply the cosparse Greedy Analysis Pursuit (GAP)\nalgorithm for compressive sensing of ECG signals. Moreover, to reduce\nprocessing time, classical signal-channel GAP is generalized to the\nmulti-channel GAP algorithm, which simultaneously reconstructs multiple signals\nwith similar support. Numerical experiments show that the proposed method\noutperforms the classical sparse multi-channel greedy algorithms in terms of\naccuracy and the single-channel cosparse approach in terms of processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 00:02:00 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Avonds", "Yurrit", ""], ["Liu", "Yipeng", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1311.4924", "submitter": "Yipeng Liu Prof.", "authors": "Yipeng Liu", "title": "Robust Compressed Sensing Under Matrix Uncertainties", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT math.RT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) shows that a signal having a sparse or compressible\nrepresentation can be recovered from a small set of linear measurements. In\nclassical CS theory, the sampling matrix and representation matrix are assumed\nto be known exactly in advance. However, uncertainties exist due to sampling\ndistortion, finite grids of the parameter space of dictionary, etc. In this\npaper, we take a generalized sparse signal model, which simultaneously\nconsiders the sampling and representation matrix uncertainties. Based on the\nnew signal model, a new optimization model for robust sparse signal\nreconstruction is proposed. This optimization model can be deduced with\nstochastic robust approximation analysis. Both convex relaxation and greedy\nalgorithms are used to solve the optimization problem. For the convex\nrelaxation method, a sufficient condition for recovery by convex relaxation is\ngiven; For the greedy algorithm, it is realized by the introduction of a\npre-processing of the sensing matrix and the measurements. In numerical\nexperiments, both simulated data and real-life ECG data based results show that\nthe proposed method has a better performance than the current methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 00:20:50 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 10:35:33 GMT"}, {"version": "v3", "created": "Wed, 19 Mar 2014 17:17:46 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 15:23:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Liu", "Yipeng", ""]]}, {"id": "1311.5312", "submitter": "Brian Kent", "authors": "Brian P. Kent, Alessandro Rinaldo, Fang-Cheng Yeh, Timothy Verstynen", "title": "Mapping Topographic Structure in White Matter Pathways with Level Set\n  Trees", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0093344", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiber tractography on diffusion imaging data offers rich potential for\ndescribing white matter pathways in the human brain, but characterizing the\nspatial organization in these large and complex data sets remains a challenge.\nWe show that level set trees---which provide a concise representation of the\nhierarchical mode structure of probability density functions---offer a\nstatistically-principled framework for visualizing and analyzing topography in\nfiber streamlines. Using diffusion spectrum imaging data collected on\nneurologically healthy controls (N=30), we mapped white matter pathways from\nthe cortex into the striatum using a deterministic tractography algorithm that\nestimates fiber bundles as dimensionless streamlines. Level set trees were used\nfor interactive exploration of patterns in the endpoint distributions of the\nmapped fiber tracks and an efficient segmentation of the tracks that has\nempirical accuracy comparable to standard nonparametric clustering methods. We\nshow that level set trees can also be generalized to model pseudo-density\nfunctions in order to analyze a broader array of data types, including entire\nfiber streamlines. Finally, resampling methods show the reliability of the\nlevel set tree as a descriptive measure of topographic structure, illustrating\nits potential as a statistical descriptor in brain imaging analysis. These\nresults highlight the broad applicability of level set trees for visualizing\nand analyzing high-dimensional data like fiber tractography output.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 04:32:10 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Kent", "Brian P.", ""], ["Rinaldo", "Alessandro", ""], ["Yeh", "Fang-Cheng", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1311.5380", "submitter": "Christina Bohk", "authors": "Christina Bohk and Roland Rau", "title": "Probabilistic Mortality Forecasting with Varying Age-Specific Survival\n  Improvements", "comments": "28 pages, 12 figures; added probabilistic mortality forecasts in\n  section 3 (figures 5 and 7), results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic mortality forecasting model that can be applied to\nderive forecasts for populations with regular and irregular mortality\ndevelopments. Our model (1) uses rates of mortality improvement to model\ndynamic age patterns of mortality change and it can (2) optionally complement\nthe mortality trend of a country of interest with that of at least one\nreference country. Retrospective mortality forecasts for British and Danish\nwomen from 1991 to 2011 suggest that our model can generate smaller forecast\nerrors than other widely accepted approaches like, for instance, the Lee-Carter\nmodel or the UN Bayesian approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 12:35:54 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 15:00:33 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Bohk", "Christina", ""], ["Rau", "Roland", ""]]}, {"id": "1311.5406", "submitter": "Gustavo Camps-Valls", "authors": "Jos\\'e Luis Rojo-\\'Alvarez, Manel Mart\\'inez-Ram\\'on, Jordi\n  Mu\\~noz-Mar\\'i and Gustavo Camps-Valls", "title": "A Unified SVM Framework for Signal Estimation", "comments": "22 pages, 13 figures. Digital Signal Processing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified framework to tackle estimation problems in\nDigital Signal Processing (DSP) using Support Vector Machines (SVMs). The use\nof SVMs in estimation problems has been traditionally limited to its mere use\nas a black-box model. Noting such limitations in the literature, we take\nadvantage of several properties of Mercer's kernels and functional analysis to\ndevelop a family of SVM methods for estimation in DSP. Three types of signal\nmodel equations are analyzed. First, when a specific time-signal structure is\nassumed to model the underlying system that generated the data, the linear\nsignal model (so called Primal Signal Model formulation) is first stated and\nanalyzed. Then, non-linear versions of the signal structure can be readily\ndeveloped by following two different approaches. On the one hand, the signal\nmodel equation is written in reproducing kernel Hilbert spaces (RKHS) using the\nwell-known RKHS Signal Model formulation, and Mercer's kernels are readily used\nin SVM non-linear algorithms. On the other hand, in the alternative and not so\ncommon Dual Signal Model formulation, a signal expansion is made by using an\nauxiliary signal model equation given by a non-linear regression of each time\ninstant in the observed time series. These building blocks can be used to\ngenerate different novel SVM-based methods for problems of signal estimation,\nand we deal with several of the most important ones in DSP. We illustrate the\nusefulness of this methodology by defining SVM algorithms for linear and\nnon-linear system identification, spectral analysis, nonuniform interpolation,\nsparse deconvolution, and array processing. The performance of the developed\nSVM methods is compared to standard approaches in all these settings. The\nexperimental results illustrate the generality, simplicity, and capabilities of\nthe proposed SVM framework for DSP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 13:55:22 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Rojo-\u00c1lvarez", "Jos\u00e9 Luis", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Camps-Valls", "Gustavo", ""]]}, {"id": "1311.5517", "submitter": "Helene Hill", "authors": "Joel H Pitt and Helene Z Hill", "title": "Statistical Detection of Potentially Fabricated Data", "comments": "31 pages of text including 2 figures, 3 tables and an Appendix\n  containing the mathematical derivation of a model for detecting and\n  quantifying the probability for the occurrence of the average of 3 counts as\n  one of those counts. 166 pages of raw data that were used in the analyses", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific fraud is an increasingly vexing problem. Many current programs for\nfraud detection focus on image manipulation, while techniques for detection\nbased on anomalous patterns that may be discoverable in the underlying\nnumerical data get much less attention, even though these techniques are often\neasy to apply. We employed three such techniques in a case study in which we\nconsidered data sets from several hundred experiments. We compared patterns in\nthe data sets from one research teaching specialist (RTS), to those of 9 other\nmembers of the same laboratory and from 3 outside laboratories. Application of\ntwo conventional statistical tests and a newly developed test for anomalous\npatterns in the triplicate data commonly produced in such research to various\ndata sets reported by the RTS resulted in repeated rejection of the hypotheses\n(often at p-levels well below 0.001) that anomalous patterns in his data may\nhave occurred by chance. This analysis emphasizes the importance of access to\nraw data that form the bases of publications, reports and grant applications in\norder to evaluate the correctness of the conclusions, as well as the utility of\nmethods for detecting anomalous, especially fabricated, numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 19:03:55 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Pitt", "Joel H", ""], ["Hill", "Helene Z", ""]]}, {"id": "1311.5665", "submitter": "Lutz Bornmann Dr.", "authors": "Werner Marx, Lutz Bornmann", "title": "Tracing the origin of a scientific legend by Reference Publication Year\n  Spectroscopy (RPYS): the legend of the Darwin finches", "comments": "Accepted for publication in Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previews paper we introduced the quantitative method named Reference\nPublication Year Spectroscopy (RPYS). With this method one can determine the\nhistorical roots of research fields and quantify their impact on current\nresearch. RPYS is based on the analysis of the frequency with which references\nare cited in the publications of a specific research field in terms of the\npublication years of these cited references. In this study, we illustrate that\nRPYS can also be used to reveal the origin of scientific legends. We selected\nDarwin finches as an example for illustration. Charles Darwin, the originator\nof evolutionary theory, was given credit for finches he did not see and for\nobservations and insights about the finches he never made. We have shown that a\nbook published in 1947 is the most-highly cited early reference cited within\nthe relevant literature. This book had already been revealed as the origin of\nthe term Darwin finches by Sulloway through careful historical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 07:51:32 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Marx", "Werner", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1311.5772", "submitter": "Guillaume Kon Kam King", "authors": "Guillaume Kon Kam King and Philippe Veber and Sandrine Charles and\n  Marie Laure Delignette-Muller", "title": "MOSAIC_SSD: a new web-tool for the Species Sensitivity Distribution,\n  allowing to include censored data by maximum likelihood", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Censored data are seldom taken into account in Species Sensitivity\nDistribution (SSD) analysis. However, they are found in virtually every dataset\nand sometimes represent the better part of the data. Stringent recommendations\non data quality often lead to discard a lot of this meaningful data, often\nresulting in datasets of reduced size, which lack representativeness of any\nrealistic community. However, it is reasonably simple to include censored data\ninto SSD by using an extension of the standard maximum likelihood method. In\nthis paper, we detail this approach based on the use of the R-package\n\\emph{fitdistrplus}, dedicated to the fit of parametric probability\ndistributions. In particular, we introduce the new web-tool MOSAIC$\\_$SSD to\nfit an SSD on datasets containing any kind of data, censored or not.\nMOSAIC$\\_$SSD allows predicting any Hazardous Concentration (HC) and provides\nin addition bootstrap confidence intervals on the prediction. In the end,\ntaking examples from published data, we illustrate the added value of including\ncensored data in SSD.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 14:53:09 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 13:54:35 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["King", "Guillaume Kon Kam", ""], ["Veber", "Philippe", ""], ["Charles", "Sandrine", ""], ["Delignette-Muller", "Marie Laure", ""]]}, {"id": "1311.5989", "submitter": "Yipeng Liu Dr.", "authors": "Yurrit Avonds and Yipeng Liu and Sabine Van Huffel", "title": "Robust Cosparse Greedy Signal Reconstruction for Compressive Sensing\n  with Multiplicative and Additive Noise", "comments": "This paper has been withdrawn by the author due to errors (missed\n  \\gamma in the 2nd term on the right) in equation 10, equation 11, and\n  equation 12, which leads to further error in Algorithm 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy algorithms are popular in compressive sensing for their high\ncomputational efficiency. But the performance of current greedy algorithms can\nbe degenerated seriously by noise (both multiplicative noise and additive\nnoise). A robust version of greedy cosparse greedy algorithm (greedy analysis\npursuit) is presented in this paper. Comparing with previous methods, The\nproposed robust greedy analysis pursuit algorithm is based on an optimization\nmodel which allows both multiplicative noise and additive noise in the data\nfitting constraint. Besides, a new stopping criterion that is derived. The new\nalgorithm is applied to compressive sensing of ECG signals. Numerical\nexperiments based on real-life ECG signals demonstrate the performance\nimprovement of the proposed greedy algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 11:29:06 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 10:41:29 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Avonds", "Yurrit", ""], ["Liu", "Yipeng", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1311.6014", "submitter": "Velimir Vesselinov", "authors": "Velimir V. Vesselinov, Daniel O'Malley, and Danny Katzman", "title": "Robust decision analysis for environmental management of groundwater\n  contamination sites", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equations 7 and 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to many other engineering fields, the uncertainties in subsurface\nprocesses (e.g., fluid flow and contaminant transport in aquifers) and their\nparameters are notoriously difficult to observe, measure, and characterize.\nThis causes severe uncertainties that need to be addressed in any decision\nanalysis related to optimal management and remediation of groundwater\ncontamination sites. Furthermore, decision analyses typically rely heavily on\ncomplex data analyses and/or model predictions, which are often poorly\nconstrained as well. Recently, we have developed a model-driven\ndecision-support framework (called MADS; http://mads.lanl.gov) for the\nmanagement and remediation of subsurface contamination sites in which severe\nuncertainties and complex physics-based models are coupled to perform\nscientifically defensible decision analyses. The decision analyses are based on\nInformation Gap Decision Theory (IGDT). We demonstrate the MADS capabilities by\nsolving a decision problem related to optimal monitoring network design.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 15:58:45 GMT"}, {"version": "v2", "created": "Sun, 3 Aug 2014 14:11:54 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Vesselinov", "Velimir V.", ""], ["O'Malley", "Daniel", ""], ["Katzman", "Danny", ""]]}, {"id": "1311.6039", "submitter": "Nicolas Chauffert", "authors": "Nicolas Chauffert (INRIA Saclay - Ile de France), Philippe Ciuciu\n  (INRIA Saclay - Ile de France), Jonas Kahn, Pierre Weiss (ITAV)", "title": "Variable density sampling with continuous trajectories. Application to\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing acquisition time is a crucial challenge for many imaging techniques.\nCompressed Sensing (CS) theory offers an appealing framework to address this\nissue since it provides theoretical guarantees on the reconstruction of sparse\nsignals by projection on a low dimensional linear subspace. In this paper, we\nfocus on a setting where the imaging device allows to sense a fixed set of\nmeasurements. We first discuss the choice of an optimal sampling subspace\n(smallest subset) allowing perfect reconstruction of sparse signals. Its\nstandard design relies on the random drawing of independent measurements. We\ndiscuss how to select the drawing distribution and show that a mixed strategy\ninvolving partial deterministic sampling and independent drawings can help\nbreaking the so-called \"coherence barrier\". Unfortunately, independent random\nsampling is irrelevant for many acquisition devices owing to acquisition\nconstraints. To overcome this limitation, the notion of Variable Density\nSamplers (VDS) is introduced and defined as a stochastic process with a\nprescribed limit empirical measure. It encompasses samplers based on\nindependent measurements or continuous curves. The latter are crucial to extend\nCS results to actual applications. Our main contribution lies in two original\ncontinuous VDS. The first one relies on random walks over the acquisition space\nwhereas the second one is heuristically driven and rests on the approximate\nsolution of a Traveling Salesman Problem. Theoretical analysis and\nretrospective CS simulations in magnetic resonance imaging highlight that the\nTSP-based solution provides improved reconstructed images in terms of\nsignal-to-noise ratio compared to standard sampling schemes (spiral, radial, 3D\niid...).\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 19:05:18 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 13:54:53 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2013 18:13:21 GMT"}, {"version": "v4", "created": "Tue, 15 Jul 2014 18:16:58 GMT"}, {"version": "v5", "created": "Wed, 16 Jul 2014 15:33:38 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Chauffert", "Nicolas", "", "INRIA Saclay - Ile de France"], ["Ciuciu", "Philippe", "", "INRIA Saclay - Ile de France"], ["Kahn", "Jonas", "", "ITAV"], ["Weiss", "Pierre", "", "ITAV"]]}, {"id": "1311.6051", "submitter": "Babak Seyfe", "authors": "Hamid Asadi and Babak Seyfe", "title": "Source Number Estimation via Entropy Estimation of Eigenvalues (EEE) in\n  Gaussian and Non-Gaussian Noise", "comments": "24 Pages, 9 Figures, This paper is submitted to IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method based on the entropy estimation of the\nobservation space eigenvalues is proposed to estimate the number of the sources\nin Gaussian and Non-Gaussian noise. In this method, the eigenvalues of\ncorrelation matrix of the observation space will be divided by two sets:\neigenvalues of signal subspace and eigenvalues of noise subspace. We will use\nestimated entropy of eigenvalues to determine the number of sources. In this\nmethod we do not need any a priory information about signals and noise. The\nadvantages of the proposed algorithm based on the performance is compared with\nthe existing methods in the presence of Gaussian and Non-Gaussian noise. We\nhave shown that our proposed method outperforms those methods in the\nliterature, for different values of observation time and Signal to Noise Ratio,\ni. e. SNR. It is shown that the algorithm is consistent and also its\nprobability of false alarm and probability of missed detection tend to zero for\nlong observation time.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 21:00:22 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 17:16:56 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Asadi", "Hamid", ""], ["Seyfe", "Babak", ""]]}, {"id": "1311.6139", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Distributed multinomial regression", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS831 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1394-1414", "doi": "10.1214/15-AOAS831", "report-no": "IMS-AOAS-AOAS831", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a model-based approach to distributed computing for\nmultinomial logistic (softmax) regression. We treat counts for each response\ncategory as independent Poisson regressions via plug-in estimates for fixed\neffects shared across categories. The work is driven by the\nhigh-dimensional-response multinomial models that are used in analysis of a\nlarge number of random counts. Our motivating applications are in text\nanalysis, where documents are tokenized and the token counts are modeled as\narising from a multinomial dependent upon document attributes. We estimate such\nmodels for a publicly available data set of reviews from Yelp, with text\nregressed onto a large set of explanatory variables (user, business, and rating\ninformation). The fitted models serve as a basis for exploring the connection\nbetween words and variables of interest, for reducing dimension into supervised\nfactor scores, and for prediction. We argue that the approach herein provides\nan attractive option for social scientists and other text analysts who wish to\nbring familiar regression tools to bear on text data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 16:06:19 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 17:50:16 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 21:02:35 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2015 08:05:58 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1311.6311", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis", "title": "Bias in parametric estimation: reduction and useful side-effects", "comments": null, "journal-ref": "WIREs.Compu.Stat. 6 (2014) 185-196", "doi": "10.1002/wics.1296", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bias of an estimator is defined as the difference of its expected value\nfrom the parameter to be estimated, where the expectation is with respect to\nthe model. Loosely speaking, small bias reflects the desire that if an\nexperiment is repeated indefinitely then the average of all the resultant\nestimates will be close to the parameter value that is estimated. The current\npaper is a review of the still-expanding repository of methods that have been\ndeveloped to reduce bias in the estimation of parametric models. The review\nprovides a unifying framework where all those methods are seen as attempts to\napproximate the solution of a simple estimating equation. Of particular focus\nis the maximum likelihood estimator, which despite being asymptotically\nunbiased under the usual regularity conditions, has finite-sample bias that can\nresult in significant loss of performance of standard inferential procedures.\nAn informal comparison of the methods is made revealing some useful practical\nside-effects in the estimation of popular models in practice including: i)\nshrinkage of the estimators in binomial and multinomial regression models that\nguarantees finiteness even in cases of data separation where the maximum\nlikelihood estimator is infinite, and ii) inferential benefits for models that\nrequire the estimation of dispersion or precision parameters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 14:10:49 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kosmidis", "Ioannis", ""]]}, {"id": "1311.6376", "submitter": "Brian Blais", "authors": "Caitlyn Witkowski and Brian Blais", "title": "Bayesian Analysis of Epidemics - Zombies, Influenza, and other Diseases", "comments": "16 pages, 6 figures, 2 tables. Corrected email address typo from\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models of epidemic dynamics offer significant insight into\npredicting and controlling infectious diseases. The dynamics of a disease model\ngenerally follow a susceptible, infected, and recovered (SIR) model, with some\nstandard modifications. In this paper, we extend the work of Munz et.al (2009)\non the application of disease dynamics to the so-called \"zombie apocalypse\",\nand then apply the identical methods to influenza dynamics. Unlike Munz et.al\n(2009), we include data taken from specific depictions of zombies in popular\nculture films and apply Markov Chain Monte Carlo (MCMC) methods on improved\ndynamical representations of the system. To demonstrate the usefulness of this\napproach, beyond the entertaining example, we apply the identical methodology\nto Google Trend data on influenza to establish infection and recovery rates.\nFinally, we discuss the use of the methods to explore hypothetical intervention\npolicies regarding disease outbreaks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 17:39:52 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 15:04:51 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Witkowski", "Caitlyn", ""], ["Blais", "Brian", ""]]}, {"id": "1311.6449", "submitter": "Tadilo Bogale E", "authors": "Tadilo Endeshaw Bogale and Luc Vandendorpe", "title": "Multi-cycle Cyclostationary based Spectrum Sensing Algorithm for OFDM\n  Signals with Noise Uncertainty in Cognitive Radio Networks", "comments": "MILCOM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple multi-cycle cyclostationary based signal\ndetection (spectrum sensing) algorithm for Orthogonal Frequency Division\nMultiplexed (OFDM) signals in cognitive radio networks. We assume that the\nnoise samples are independent and identically distributed (i.i.d) random\nvariables all with unknown (imperfect) variance. Our detection algorithm employ\nthe following three steps. First, we formulate the test statistics as a ratio\nof two quadratic cyclic autocorrelation functions. Second, we derive a closed\nform expression for the false alarm probability. Third, we evaluate the\ndetection probability of our algorithm for a given false alarm probability. The\ntheoretical probability of false alarm expression matches with that of the\nsimulation result. Moreover, we have observed that the proposed multi-cycle\nalgorithm exhibits significantly superior probability of detection compared to\nthe existing low complexity cyclostationary based and the well known energy\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 20:40:04 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Bogale", "Tadilo Endeshaw", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1311.6454", "submitter": "Tadilo Bogale E", "authors": "Tadilo Endeshaw Bogale and Luc Vandendorpe", "title": "Moment based Spectrum Sensing Algorithm for Cognitive Radio Networks\n  with Noise Variance Uncertainty", "comments": "IEEE CISS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes simple moment based spectrum sensing algorithm for\ncognitive radio networks in a flat fading channel. It is assumed that the\ntransmitted signal samples are binary (quadrature) phase-shift keying BPSK\n(QPSK), Mary quadrature amplitude modulation (QAM) or continuous uniformly\ndistributed random variables and the noise samples are independent and\nidentically distributed circularly symmetric complex Gaussian random variables\nall with unknown (imperfect) variance. Under these assumptions, we propose a\nsimple test statistics employing a ratio of fourth and second moments. For this\nstatistics, we provide analytical expressions for both probability of false\nalarm (Pf) and probability of detection (Pd) in an additive white Gaussian\nnoise (AWGN) channel.We confirm the theoretical expressions by computer\nsimulation. Furthermore, under noise variance uncertainty, simulation results\ndemonstrate that the proposed moment based detector gives better detection\nperformance compared to that of energy detector in AWGN and Rayleigh fading\nchannels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 20:43:55 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Bogale", "Tadilo Endeshaw", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1311.6567", "submitter": "Frederic Pascal", "authors": "Frederic Pascal, Yacine Chitour and Yihui Quek", "title": "Generalized robust shrinkage estimator and its application to STAP\n  detection problem", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2355779", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, in the context of covariance matrix estimation, in order to improve\nas well as to regularize the performance of the Tyler's estimator [1] also\ncalled the Fixed-Point Estimator (FPE) [2], a \"shrinkage\" fixed-point estimator\nhas been introduced in [3]. First, this work extends the results of [3,4] by\ngiving the general solution of the \"shrinkage\" fixed-point algorithm. Secondly,\nby analyzing this solution, called the generalized robust shrinkage estimator,\nwe prove that this solution converges to a unique solution when the shrinkage\nparameter $\\beta$ (losing factor) tends to 0. This solution is exactly the FPE\nwith the trace of its inverse equal to the dimension of the problem. This\ngeneral result allows one to give another interpretation of the FPE and more\ngenerally, on the Maximum Likelihood approach for covariance matrix estimation\nwhen constraints are added. Then, some simulations illustrate our theoretical\nresults as well as the way to choose an optimal shrinkage factor. Finally, this\nwork is applied to a Space-Time Adaptive Processing (STAP) detection problem on\nreal STAP data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 06:56:42 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 17:14:34 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Pascal", "Frederic", ""], ["Chitour", "Yacine", ""], ["Quek", "Yihui", ""]]}, {"id": "1311.6864", "submitter": "Eftychios A. Pnevmatikakis", "authors": "Eftychios A. Pnevmatikakis, Josh Merel, Ari Pakman, Liam Paninski", "title": "Bayesian spike inference from calcium imaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient Bayesian methods for extracting neuronal spiking\ninformation from calcium imaging data. The goal of our methods is to sample\nfrom the posterior distribution of spike trains and model parameters (baseline\nconcentration, spike amplitude etc) given noisy calcium imaging data. We\npresent discrete time algorithms where we sample the existence of a spike at\neach time bin using Gibbs methods, as well as continuous time algorithms where\nwe sample over the number of spikes and their locations at an arbitrary\nresolution using Metropolis-Hastings methods for point processes. We provide\nRao-Blackwellized extensions that (i) marginalize over several model parameters\nand (ii) provide smooth estimates of the marginal spike posterior distribution\nin continuous time. Our methods serve as complements to standard point\nestimates and allow for quantification of uncertainty in estimating the\nunderlying spike train and model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 03:59:13 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Pnevmatikakis", "Eftychios A.", ""], ["Merel", "Josh", ""], ["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1311.6976", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen", "title": "Dimensionality reduction for click-through rate prediction: Dense versus\n  sparse representation", "comments": "Presented at the Probabilistic Models for Big Data workshop at NIPS\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, display ads are increasingly being placed based on\nreal-time auctions where the advertiser who wins gets to serve the ad. This is\ncalled real-time bidding (RTB). In RTB, auctions have very tight time\nconstraints on the order of 100ms. Therefore mechanisms for bidding\nintelligently such as clickthrough rate prediction need to be sufficiently\nfast. In this work, we propose to use dimensionality reduction of the\nuser-website interaction graph in order to produce simplified features of users\nand websites that can be used as predictors of clickthrough rate. We\ndemonstrate that the Infinite Relational Model (IRM) as a dimensionality\nreduction offers comparable predictive performance to conventional\ndimensionality reduction schemes, while achieving the most economical usage of\nfeatures and fastest computations at run-time. For applications such as\nreal-time bidding, where fast database I/O and few computations are key to\nsuccess, we thus recommend using IRM based features as predictors to exploit\nthe recommender effects from bipartite graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 14:19:21 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 09:21:28 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""], ["Hansen", "Toke Jansen", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1311.7244", "submitter": "Jennifer Hill", "authors": "Jennifer Hill, Yu-Sung Su", "title": "Assessing lack of common support in causal inference using Bayesian\n  nonparametrics: Implications for evaluating the effect of breastfeeding on\n  children's cognitive outcomes", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS630 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1386-1420", "doi": "10.1214/13-AOAS630", "report-no": "IMS-AOAS-AOAS630", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference in observational studies typically requires making\ncomparisons between groups that are dissimilar. For instance, researchers\ninvestigating the role of a prolonged duration of breastfeeding on child\noutcomes may be forced to make comparisons between women with substantially\ndifferent characteristics on average. In the extreme there may exist\nneighborhoods of the covariate space where there are not sufficient numbers of\nboth groups of women (those who breastfed for prolonged periods and those who\ndid not) to make inferences about those women. This is referred to as lack of\ncommon support. Problems can arise when we try to estimate causal effects for\nunits that lack common support, thus we may want to avoid inference for such\nunits. If ignorability is satisfied with respect to a set of potential\nconfounders, then identifying whether, or for which units, the common support\nassumption holds is an empirical question. However, in the high-dimensional\ncovariate space often required to satisfy ignorability such identification may\nnot be trivial. Existing methods used to address this problem often require\nreliance on parametric assumptions and most, if not all, ignore the information\nembedded in the response variable. We distinguish between the concepts of\n\"common support\" and common causal support.\" We propose a new approach for\nidentifying common causal support that addresses some of the shortcomings of\nexisting methods. We motivate and illustrate the approach using data from the\nNational Longitudinal Survey of Youth to estimate the effect of breastfeeding\nat least nine months on reading and math achievement scores at age five or six.\nWe also evaluate the comparative performance of this method in hypothetical\nexamples and simulations where the true treatment effect is known.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 09:31:53 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Hill", "Jennifer", ""], ["Su", "Yu-Sung", ""]]}, {"id": "1311.7257", "submitter": "Joshua P. French", "authors": "Joshua P. French, Stephan R. Sain", "title": "Spatio-temporal exceedance locations and confidence regions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS631 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1421-1449", "doi": "10.1214/13-AOAS631", "report-no": "IMS-AOAS-AOAS631", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exceedance region is the set of locations in a spatial domain where a\nprocess exceeds some threshold. Examples of exceedance regions include areas\nwhere ozone concentrations exceed safety standards, there is high risk for\ntornadoes or floods, or heavy-metal levels are dangerously high. Identifying\nthese regions in a spatial or spatio-temporal setting is an important\nresponsibility in environmental monitoring. Exceedance regions are often\nestimated by finding the areas where predictions from a statistical model\nexceed some threshold. Even when estimation error is quantifiable at individual\nlocations, the overall estimation error of the estimated exceedance region is\nstill unknown. A method is presented for constructing a confidence region\ncontaining the true exceedance region of a spatio-temporal process at a certain\ntime. The underlying latent process and any measurement error are assumed to be\nGaussian. Conventional techniques are used to model the spatio-temporal data,\nand then conditional simulation is combined with hypothesis testing to create\nthe desired confidence region. A simulation study is used to validate the\napproach for several levels of spatial and temporal dependence. The methodology\nis used to identify regions of Oregon having high precipitation levels and also\nused in comparing climate models and assessing climate change using climate\nmodels from the North American Regional Climate Change Assessment Program.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 10:08:51 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["French", "Joshua P.", ""], ["Sain", "Stephan R.", ""]]}, {"id": "1311.7261", "submitter": "Tevfik Aktekin", "authors": "Tevfik Aktekin, Refik Soyer, Feng Xu", "title": "Assessment of mortgage default risk via Bayesian state space models", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS632 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1450-1473", "doi": "10.1214/13-AOAS632", "report-no": "IMS-AOAS-AOAS632", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing risk at the aggregate level is crucial for banks and financial\ninstitutions as required by the Basel III framework. In this paper, we\nintroduce discrete time Bayesian state space models with Poisson measurements\nto model aggregate mortgage default rate. We discuss parameter updating,\nfiltering, smoothing, forecasting and estimation using Markov chain Monte Carlo\nmethods. In addition, we investigate the dynamic behavior of the default rate\nand the effects of macroeconomic variables. We illustrate the use of the\nproposed models using actual U.S. residential mortgage data and discuss\ninsights gained from Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 10:25:36 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Aktekin", "Tevfik", ""], ["Soyer", "Refik", ""], ["Xu", "Feng", ""]]}, {"id": "1311.7265", "submitter": "Qing Pan", "authors": "Qing Pan, Joseph L. Gastwirth", "title": "Estimating restricted mean job tenures in semi-competing risk data\n  compensating victims of discrimination", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS637 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1474-1496", "doi": "10.1214/13-AOAS637", "report-no": "IMS-AOAS-AOAS637", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When plaintiffs prevail in a discrimination case, a major component of the\ncalculation of economic loss is the length of time they would have been in the\nhigher position had they been treated fairly during the period in which the\nemployer practiced discrimination. This problem is complicated by the fact that\none's eligibility for promotion is subject to termination by retirement and\nboth the promotion and retirement processes may be affected by discriminatory\npractices. This semi-competing risk setup is decomposed into a retirement\nprocess and a promotion process among the employees. Predictions for the\npurpose of compensation are made by utilizing the expected promotion and\nretirement probabilities of similarly qualified members of the nondiscriminated\ngroup. The restricted mean durations of three periods are estimated - the time\nan employee would be at the lower position, at the higher level and in\nretirement. The asymptotic properties of the estimators are presented and\nexamined through simulation studies. The proposed restricted mean job duration\nestimators are shown to be robust in the presence of an independent frailty\nterm. Data from the reverse discrimination case, Alexander v. Milwaukee, where\nWhite-male lieutenants were discriminated in promotion to captain are\nreanalyzed. While the appellate court upheld liability, it reversed the\noriginal damage calculations, which heavily depended on the time a plaintiff\nwould have been in each position. The results obtained by the proposed method\nare compared to those made at the first trial. Substantial differences in both\ndirections are observed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 10:34:12 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Pan", "Qing", ""], ["Gastwirth", "Joseph L.", ""]]}, {"id": "1311.7279", "submitter": "George Mohler", "authors": "George Mohler", "title": "Modeling and estimation of multi-source clustering in crime and security\n  data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS647 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1525-1539", "doi": "10.1214/13-AOAS647", "report-no": "IMS-AOAS-AOAS647", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the presence of clustering in crime and security event data is well\nestablished, the mechanism(s) by which clustering arises is not fully\nunderstood. Both contagion models and history independent correlation models\nare applied, but not simultaneously. In an attempt to disentangle contagion\nfrom other types of correlation, we consider a Hawkes process with background\nrate driven by a log Gaussian Cox process. Our inference methodology is an\nefficient Metropolis adjusted Langevin algorithm for filtering of the intensity\nand estimation of the model parameters. We apply the methodology to property\nand violent crime data from Chicago, terrorist attack data from Northern\nIreland and Israel, and civilian casualty data from Iraq. For each data set we\nquantify the uncertainty in the levels of contagion vs. history independent\ncorrelation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 11:31:35 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Mohler", "George", ""]]}, {"id": "1311.7319", "submitter": "Stefano Castruccio", "authors": "Stefano Castruccio, Michael L. Stein", "title": "Global space-time models for climate ensembles", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS656 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1593-1611", "doi": "10.1214/13-AOAS656", "report-no": "IMS-AOAS-AOAS656", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global climate models aim to reproduce physical processes on a global scale\nand predict quantities such as temperature given some forcing inputs. We\nconsider climate ensembles made of collections of such runs with different\ninitial conditions and forcing scenarios. The purpose of this work is to show\nhow the simulated temperatures in the ensemble can be reproduced (emulated)\nwith a global space/time statistical model that addresses the issue of\ncapturing nonstationarities in latitude more effectively than current\nalternatives in the literature. The model we propose leads to a computationally\nefficient estimation procedure and, by exploiting the gridded geometry of the\ndata, we can fit massive data sets with millions of simulated data within a few\nhours. Given a training set of runs, the model efficiently emulates temperature\nfor very different scenarios and therefore is an appealing tool for impact\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 13:55:19 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Castruccio", "Stefano", ""], ["Stein", "Michael L.", ""]]}, {"id": "1311.7326", "submitter": "Thomas Rusch", "authors": "Thomas Rusch, Ilro Lee, Kurt Hornik, Wolfgang Jank, Achim Zeileis", "title": "Influencing elections with statistics: Targeting voters with logistic\n  regression trees", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS648 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1612-1639", "doi": "10.1214/13-AOAS648", "report-no": "IMS-AOAS-AOAS648", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In political campaigning substantial resources are spent on voter\nmobilization, that is, on identifying and influencing as many people as\npossible to vote. Campaigns use statistical tools for deciding whom to target\n(\"microtargeting\"). In this paper we describe a nonpartisan campaign that aims\nat increasing overall turnout using the example of the 2004 US presidential\nelection. Based on a real data set of 19,634 eligible voters from Ohio, we\nintroduce a modern statistical framework well suited for carrying out the main\ntasks of voter targeting in a single sweep: predicting an individual's turnout\n(or support) likelihood for a particular cause, party or candidate as well as\ndata-driven voter segmentation. Our framework, which we refer to as LORET (for\nLOgistic REgression Trees), contains standard methods such as logistic\nregression and classification trees as special cases and allows for a synthesis\nof both techniques. For our case study, we explore various LORET models with\ndifferent regressors in the logistic model components and different\npartitioning variables in the tree components; we analyze them in terms of\ntheir predictive accuracy and compare the effect of using the full set of\navailable variables against using only a limited amount of information. We find\nthat augmenting a standard set of variables (such as age and voting history)\nwith additional predictor variables (such as the household composition in terms\nof party affiliation) clearly improves predictive accuracy. We also find that\nLORET models based on tree induction beat the unpartitioned models.\nFurthermore, we illustrate how voter segmentation arises from our framework and\ndiscuss the resulting profiles from a targeting point of view.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 14:21:41 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Rusch", "Thomas", ""], ["Lee", "Ilro", ""], ["Hornik", "Kurt", ""], ["Jank", "Wolfgang", ""], ["Zeileis", "Achim", ""]]}, {"id": "1311.7333", "submitter": "Ying Huang", "authors": "Ying Huang, Margaret S. Pepe, Ziding Feng", "title": "Logistic regression analysis with standardized markers", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS634 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1640-1662", "doi": "10.1214/13-AOAS634", "report-no": "IMS-AOAS-AOAS634", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two different approaches to analysis of data from diagnostic biomarker\nstudies are commonly employed. Logistic regression is used to fit models for\nprobability of disease given marker values, while ROC curves and risk\ndistributions are used to evaluate classification performance. In this paper we\npresent a method that simultaneously accomplishes both tasks. The key step is\nto standardize markers relative to the nondiseased population before including\nthem in the logistic regression model. Among the advantages of this method are\nthe following: (i) ensuring that results from regression and performance\nassessments are consistent with each other; (ii) allowing covariate adjustment\nand covariate effects on ROC curves to be handled in a familiar way, and (iii)\nproviding a mechanism to incorporate important assumptions about structure in\nthe ROC curve into the fitted risk model. We develop the method in detail for\nthe problem of combining biomarker data sets derived from multiple studies,\npopulations or biomarker measurement platforms, when ROC curves are similar\nacross data sources. The methods are applicable to both cohort and case-control\nsampling designs. The data set motivating this application concerns Prostate\nCancer Antigen 3 (PCA3) for diagnosis of prostate cancer in patients with or\nwithout previous negative biopsy where the ROC curves for PCA3 are found to be\nthe same in the two populations. The estimated constrained maximum likelihood\nand empirical likelihood estimators are derived. The estimators are compared in\nsimulation studies and the methods are illustrated with the PCA3 data set.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 14:34:57 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Huang", "Ying", ""], ["Pepe", "Margaret S.", ""], ["Feng", "Ziding", ""]]}, {"id": "1311.7401", "submitter": "Eva-Maria Didden", "authors": "Eva-Maria Didden, Thordis Linda Thorarinsdottir, Alex Lenkoski,\n  Christoph Schn\\\"orr", "title": "Shape from Texture using Locally Scaled Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape from texture refers to the extraction of 3D information from 2D images\nwith irregular texture. This paper introduces a statistical framework to learn\nshape from texture where convex texture elements in a 2D image are represented\nthrough a point process. In a first step, the 2D image is preprocessed to\ngenerate a probability map corresponding to an estimate of the unnormalized\nintensity of the latent point process underlying the texture elements. The\nlatent point process is subsequently inferred from the probability map in a\nnon-parametric, model free manner. Finally, the 3D information is extracted\nfrom the point pattern by applying a locally scaled point process model where\nthe local scaling function represents the deformation caused by the projection\nof a 3D surface onto a 2D image.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 19:17:39 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Didden", "Eva-Maria", ""], ["Thorarinsdottir", "Thordis Linda", ""], ["Lenkoski", "Alex", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1311.7472", "submitter": "Joseph Guinness", "authors": "Joseph Guinness, Michael L. Stein", "title": "Interpolation of nonstationary high frequency spatial-temporal\n  temperature data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS633 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1684-1708", "doi": "10.1214/13-AOAS633", "report-no": "IMS-AOAS-AOAS633", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Atmospheric Radiation Measurement program is a U.S. Department of Energy\nproject that collects meteorological observations at several locations around\nthe world in order to study how weather processes affect global climate change.\nAs one of its initiatives, it operates a set of fixed but irregularly-spaced\nmonitoring facilities in the Southern Great Plains region of the U.S. We\ndescribe methods for interpolating temperature records from these fixed\nfacilities to locations at which no observations were made, which can be useful\nwhen values are required on a spatial grid. We interpolate by conditionally\nsimulating from a fitted nonstationary Gaussian process model that accounts for\nthe time-varying statistical characteristics of the temperatures, as well as\nthe dependence on solar radiation. The model is fit by maximizing an\napproximate likelihood, and the conditional simulations result in\nwell-calibrated confidence intervals for the predicted temperatures. We also\ndescribe methods for handling spatial-temporal jumps in the data to interpolate\na slow-moving cold front.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 06:35:15 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Guinness", "Joseph", ""], ["Stein", "Michael L.", ""]]}, {"id": "1311.7478", "submitter": "Lixun Zhang", "authors": "Lixun Zhang, Yongtao Guan, Brian P. Leaderer, Theodore R. Holford", "title": "Estimating daily nitrogen dioxide level: Exploring traffic effects", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS642 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1763-1777", "doi": "10.1214/13-AOAS642", "report-no": "IMS-AOAS-AOAS642", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data used to assess acute health effects from air pollution typically have\ngood temporal but poor spatial resolution or the opposite. A modified\nlongitudinal model was developed that sought to improve resolution in both\ndomains by bringing together data from three sources to estimate daily levels\nof nitrogen dioxide ($\\mathrm {NO}_2$) at a geographic location. Monthly\n$\\mathrm {NO}_2$ measurements at 316 sites were made available by the Study of\nTraffic, Air quality and Respiratory health (STAR). Four US Environmental\nProtection Agency monitoring stations have hourly measurements of $\\mathrm\n{NO}_2$. Finally, the Connecticut Department of Transportation provides data on\ntraffic density on major roadways, a primary contributor to $\\mathrm {NO}_2$\npollution. Inclusion of a traffic variable improved performance of the model,\nand it provides a method for estimating exposure at points that do not have\ndirect measurements of the outcome. This approach can be used to estimate daily\nvariation in levels of $\\mathrm {NO}_2$ over a region.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 08:08:19 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Zhang", "Lixun", ""], ["Guan", "Yongtao", ""], ["Leaderer", "Brian P.", ""], ["Holford", "Theodore R.", ""]]}, {"id": "1311.7480", "submitter": "Lingsong Zhang", "authors": "Lingsong Zhang, Haipeng Shen, Jianhua Z. Huang", "title": "Robust regularized singular value decomposition with application to\n  mortality data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS649 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1540-1561", "doi": "10.1214/13-AOAS649", "report-no": "IMS-AOAS-AOAS649", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a robust regularized singular value decomposition (RobRSVD) method\nfor analyzing two-way functional data. The research is motivated by the\napplication of modeling human mortality as a smooth two-way function of age\ngroup and year. The RobRSVD is formulated as a penalized loss minimization\nproblem where a robust loss function is used to measure the reconstruction\nerror of a low-rank matrix approximation of the data, and an appropriately\ndefined two-way roughness penalty function is used to ensure smoothness along\neach of the two functional domains. By viewing the minimization problem as two\nconditional regularized robust regressions, we develop a fast iterative\nreweighted least squares algorithm to implement the method. Our implementation\nnaturally incorporates missing values. Furthermore, our formulation allows\nrigorous derivation of leave-one-row/column-out cross-validation and\ngeneralized cross-validation criteria, which enable computationally efficient\ndata-driven penalty parameter selection. The advantages of the new robust\nmethod over nonrobust ones are shown via extensive simulation studies and the\nmortality rate application.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 08:13:43 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Zhang", "Lingsong", ""], ["Shen", "Haipeng", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "1311.7482", "submitter": "Charles J. Geyer", "authors": "Charles J. Geyer, Caroline E. Ridley, Robert G. Latta, Julie R.\n  Etterson, Ruth G. Shaw", "title": "Local adaptation and genetic effects on fitness: Calculations for\n  exponential family models with random effects", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS653 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1778-1795", "doi": "10.1214/13-AOAS653", "report-no": "IMS-AOAS-AOAS653", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random effects are implemented for aster models using two approximations\ntaken from Breslow and Clayton [J. Amer. Statist. Assoc. 88 (1993) 9-25].\nRandom effects are analytically integrated out of the Laplace approximation to\nthe complete data log likelihood, giving a closed-form expression for an\napproximate missing data log likelihood. Third and higher derivatives of the\ncomplete data log likelihood with respect to the random effects are ignored,\ngiving a closed-form expression for second derivatives of the approximate\nmissing data log likelihood, hence approximate observed Fisher information.\nThis method is applicable to any exponential family random effects model. It is\nimplemented in the CRAN package aster (R Core Team [R: A Language and\nEnvironment for Statistical Computing (2012) R Foundation for Statistical\nComputing], Geyer [R package aster (2012)\nhttp://cran.r-project.org/package=aster]). Applications are analyses of local\nadaptation in the invasive California wild radish (Raphanus sativus) and the\nslender wild oat (Avena barbata) and of additive genetic variance for fitness\nin the partridge pea (Chamaecrista fasciculata).\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 08:24:12 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Geyer", "Charles J.", ""], ["Ridley", "Caroline E.", ""], ["Latta", "Robert G.", ""], ["Etterson", "Julie R.", ""], ["Shaw", "Ruth G.", ""]]}, {"id": "1311.7485", "submitter": "Lei Nie", "authors": "Lei Nie, Zhiwei Zhang, Daniel Rubin, Jianxiong Chu", "title": "Likelihood reweighting methods to reduce potential bias in\n  noninferiority trials which rely on historical data to make inference", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS655 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1796-1813", "doi": "10.1214/13-AOAS655", "report-no": "IMS-AOAS-AOAS655", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally believed that bias is minimized in well-controlled randomized\nclinical trials. However, bias can arise in active controlled noninferiority\ntrials because the inference relies on a previously estimated effect size\nobtained from a historical trial that may have been conducted for a different\npopulation. By implementing a likelihood reweighting method through propensity\nscoring, a study designed to estimate a treatment effect in one trial\npopulation can be used to estimate the treatment effect size in a different\ntarget population. We illustrate this method in active controlled\nnoninferiority trials, although it can also be used in other types of studies,\nsuch as historically controlled trials, meta-analyses, and comparative\neffectiveness analyses.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 08:47:45 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Nie", "Lei", ""], ["Zhang", "Zhiwei", ""], ["Rubin", "Daniel", ""], ["Chu", "Jianxiong", ""]]}, {"id": "1311.7616", "submitter": "Avik Kumar Mahata", "authors": "Avik Kumar Mahata, Utpal Borah, Aravind Da Vinci, B.Ravishankar, Shaju\n  Albert", "title": "Smooth Curve from noisy 2-Dimensional Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will be represent the transformation of a noisy dataset into\na regular and smooth curve. We performed torsion test on 15Cr 15Ni Titanium\nmodified austenitic stainless steel up to its rupture. We did it these torsion\ntests multiple strain rates varying from 0.001 to 25 per Sec and obtained huge\nnumber of data points has been obtained from with a data acquisition rate of\n100 Hz. We also have few more data of 1500Hz of the same experiment on a\ndifferent material e.g 316 L Austenitic Stainless Steel. Machine is actually\nacquiring only torque value and the angle of rotation. The torque vs. twist\ndata itself will be having a noise, which gets multiplied when we take the\nfirst derivative of torque-twist data. The noise becomes huge and it fluctuates\nfrom desired material properties, although some serrated flow should be present\nbut the range of serration should not be as the fluctuation observed in our\ncurve so smoothing was necessary. We will be documenting final shear\nstress-strain curve we achieved through Nonparametric Regression Lowess and\nLoess, Savitzky Golay filtering, and also the robust Nonparametric Regressions,\nbody splines and shape preserving curves. The most acceptable one is Savitzky\nGolay method for our experiment or one can choose splines or loess or lowess\naccording to the object of interest.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 16:12:37 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Mahata", "Avik Kumar", ""], ["Borah", "Utpal", ""], ["Da Vinci", "Aravind", ""], ["Ravishankar", "B.", ""], ["Albert", "Shaju", ""]]}, {"id": "1311.7650", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Michael Habeck and Bernhard Schoelkopf", "title": "Adaptive nonparametric detection in cryo-electron microscopy", "comments": "Proceedings of the 58-th World Statistical Congress (2011)", "journal-ref": "Proceedings of the 58-th World Statistical Congress (2011),\n  Session: High Dimensional Data, pp. 4456 - 4461", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) is an emerging experimental method to\ncharacterize the structure of large biomolecular assemblies. Single particle\ncryo-EM records 2D images (so-called micrographs) of projections of the\nthree-dimensional particle, which need to be processed to obtain the\nthree-dimensional reconstruction. A crucial step in the reconstruction process\nis particle picking which involves detection of particles in noisy 2D\nmicrographs with low signal-to-noise ratios of typically 1:10 or even lower.\nTypically, each picture contains a large number of particles, and particles\nhave unknown irregular and nonconvex shapes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:05:50 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Habeck", "Michael", ""], ["Schoelkopf", "Bernhard", ""]]}]