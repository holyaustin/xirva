[{"id": "2004.00111", "submitter": "Victor M. Yakovenko", "authors": "Gregor Semieniuk and Victor M. Yakovenko", "title": "Historical Evolution of Global Inequality in Carbon Emissions and\n  Footprints versus Redistributive Scenarios", "comments": "26 pages, 6 figures, accepted to Journal of Cleaner Production", "journal-ref": "Journal of Cleaner Production 264, 121420 (2020)", "doi": "10.1016/j.jclepro.2020.121420", "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambitious scenarios of carbon emission redistribution for mitigating climate\nchange in line with the Paris Agreement and reaching the sustainable\ndevelopment goal of eradicating poverty have been proposed recently. They imply\na strong reduction in carbon footprint inequality by 2030 that effectively\nhalves the Gini coefficient to about 0.25. This paper examines feasibility of\nthese scenarios by analyzing the historical evolution of both weighted\ninternational inequality in CO2 emissions attributed territorially and global\ninequality in carbon footprints attributed to end consumers. For the latter, a\nnew dataset is constructed that is more comprehensive than existing ones. In\nboth cases, we find a decreasing trend in global inequality, partially\nattributed to the move of China from the lower to the middle part of the\ndistribution, with footprints more unequal than territorial emissions. These\nresults show that realization of the redistributive scenarios would require an\nunprecedented reduction in global inequality far below historical levels.\nMoreover, the territorial emissions data, available for more recent years up to\n2017, show a saturation of the decreasing Gini coefficient at a level of 0.5.\nThis observation confirms an earlier prediction based on maximal entropy\nreasoning that the Lorenz curve converges to the exponential distribution. This\nsaturation further undermines feasibility of the redistributive scenarios,\nwhich are also hindered by structural tendencies that reinforce carbon\nfootprint inequality under global capitalism. One way out of this conundrum is\na fast decarbonization of the global energy supply in order to decrease global\ncarbon emissions without relying crucially on carbon inequality reduction.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 00:51:40 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Semieniuk", "Gregor", ""], ["Yakovenko", "Victor M.", ""]]}, {"id": "2004.00125", "submitter": "Yusuke Sakumoto", "authors": "Yusuke Sakumoto and Masaki Aida", "title": "The Wigner's Semicircle Law of Weighted Random Networks", "comments": null, "journal-ref": "IEICE Transactions on Communications, vol.E104-B, No.3,\n  pp.251-261, Mar. 2021", "doi": "10.1587/transcom.2020EBP3051", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral graph theory provides an algebraical approach to investigate the\ncharacteristics of weighted networks using the eigenvalues and eigenvectors of\na matrix (e.g., normalized Laplacian matrix) that represents the structure of\nthe network. However, it is difficult for large-scale and complex networks\n(e.g., social network) to represent their structure as a matrix correctly. If\nthere is a universality that the eigenvalues are independent of the detailed\nstructure in large-scale and complex network, we can avoid the difficulty. In\nthis paper, we clarify the Wigner's Semicircle Law for weighted networks as\nsuch a universality. The law indicates that the eigenvalues of the normalized\nLaplacian matrix for weighted networks can be calculated from the a few network\nstatistics (the average degree, the average link weight, and the square average\nlink weight) when the weighted networks satisfy the sufficient condition of the\nnode degrees and the link weights.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:25:06 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 05:40:34 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sakumoto", "Yusuke", ""], ["Aida", "Masaki", ""]]}, {"id": "2004.00382", "submitter": "Gaetano Perone", "authors": "Gaetano Perone", "title": "An ARIMA model to forecast the spread and the final size of COVID-2019\n  epidemic in Italy", "comments": "11 pages, 7 figures plus 3 in the Appendix, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus disease (COVID-2019) is a severe ongoing novel pandemic that is\nspreading quickly across the world. Italy, that is widely considered one of the\nmain epicenters of the pandemic, has registered the highest COVID-2019 death\nrates and death toll in the world, to the present day. In this article I\nestimate an autoregressive integrated moving average (ARIMA) model to forecast\nthe epidemic trend over the period after April 4, 2020, by using the Italian\nepidemiological data at national and regional level. The data refer to the\nnumber of daily confirmed cases officially registered by the Italian Ministry\nof Health (www.salute.gov.it) for the period February 20 to April 4, 2020. The\nmain advantage of this model is that it is easy to manage and fit. Moreover, it\nmay give a first understanding of the basic trends, by suggesting the\nhypothetic epidemic's inflection point and final size.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:29:37 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 12:30:08 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Perone", "Gaetano", ""]]}, {"id": "2004.00537", "submitter": "Luigi Lombardo", "authors": "Luigi Lombardo and Hakan Tanyas", "title": "From scenario-based seismic hazard to scenario-based landslide hazard:\n  fast-forwarding to the future via statistical simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ground motion scenarios exists for most of the seismically active areas\naround the globe. They essentially correspond to shaking level maps at given\nearthquake return times which are used as reference for the likely areas under\nthreat from future ground displacements. Being landslides in seismically\nactively regions closely controlled by the ground motion, one would expect that\nlandslide susceptibility maps should change as the ground motion patterns\nchange in space and time. However, so far, statistically-based landslide\nsusceptibility assessments have primarily been used as time-invariant.In other\nwords, the vast majority of the statistical models does not include the\ntemporal effect of the main trigger in future landslide scenarios. In this\nwork, we present an approach aimed at filling this gap, bridging current\npractices in the seismological community to those in the geomorphological and\nstatistical ones. More specifically, we select an earthquake-induced landslide\ninventory corresponding to the 1994 Northridge earthquake and build a Bayesian\nGeneralized Additive Model of the binomial family, featuring common\nmorphometric and thematic covariates as well as the Peak Ground Acceleration\ngenerated by the Northridge earthquake. Once each model component has been\nestimated, we have run 1000 simulations for each of the 217 possible ground\nmotion scenarios for the study area.\n  From each batch of 1000 simulations, we have estimated the mean and 95\\%\nCredible Interval to represent the mean susceptibility pattern under a specific\nearthquake scenario, together with its uncertainty level. Because each\nearthquake scenario has a specific return time, our simulations allow to\nincorporate the temporal dimension into any susceptibility model, therefore\ndriving the results toward the definition of landslide hazard.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:06:18 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Lombardo", "Luigi", ""], ["Tanyas", "Hakan", ""]]}, {"id": "2004.00539", "submitter": "Luigi Lombardo", "authors": "Luguang Luo, Luigi Lombardo, Cees van Westen, Xiangjun Pei, Runqiu\n  Huang", "title": "From scenario-based seismic hazard to scenario-based landslide hazard:\n  rewinding to the past via statistical simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vast majority of landslide susceptibility studies assumes the slope\ninstability process to be time-invariant under the definition that \"the past\nand present are keys to the future\". This assumption may generally be valid.\nHowever, the trigger, be it a rainfall or an earthquake event, clearly varies\nover time. And yet, the temporal component of the trigger is rarely included in\nlandslide susceptibility studies and only confined to hazard assessment. In\nthis work, we investigate a population of landslides triggered in response to\nthe 2017 Jiuzhaigou earthquake ($M_w = 6.5$) including the associated ground\nmotion in the analyses, these being carried out at the Slope Unit (SU) level.\nWe do this by implementing a Bayesian version of a Generalized Additive Model\nand assuming that the slope instability across the SUs in the study area\nbehaves according to a Bernoulli probability distribution. This procedure would\ngenerally produce a susceptibility map reflecting the spatial pattern of the\nspecific trigger and therefore of limited use for land use planning. However,\nwe implement this first analytical step to reliably estimate the ground motion\neffect, and its distribution, on unstable SUs. We then assume the effect of the\nground motion to be time-invariant, enabling statistical simulations for any\nground motion scenario that occurred in the area from 1933 to 2017. As a\nresult, we obtain the full spectrum of potential susceptibility patterns over\nthe last century and compress this information into a susceptibility model/map\nrepresentative of all the possible ground motion patterns since 1933. This\nbackward statistical simulations can also be further exploited in the opposite\ndirection where, by accounting for scenario-based ground motion, one can also\nuse it in a forward direction to estimate future unstable slopes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:08:44 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Luo", "Luguang", ""], ["Lombardo", "Luigi", ""], ["van Westen", "Cees", ""], ["Pei", "Xiangjun", ""], ["Huang", "Runqiu", ""]]}, {"id": "2004.00852", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Gareth W. Peters, Tomoko Matsui, and Yoshiki\n  Yamagata", "title": "Spatiotemporal analysis of urban heatwaves using Tukey g-and-h random\n  field models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical quantification of temperature processes for the analysis of\nurban heat island (UHI) effects and local heat-waves is an increasingly\nimportant application domain in smart city dynamic modelling. This leads to the\nincreased importance of real-time heatwave risk management on a fine-grained\nspatial resolution. This study attempts to analyze and develop new methods for\nmodelling the spatio-temporal behavior of ground temperatures. The developed\nmodels consider higher-order stochastic spatial properties such as skewness and\nkurtosis, which are key components for understanding and describing local\ntemperature fluctuations and UHI's. The developed models are applied to the\ngreater Tokyo metropolitan area for a detailed real-world data case study. The\nanalysis also demonstrates how to statistically incorporate a variety of real\ndata sets. This includes remotely sensed imagery and a variety of ground-based\nmonitoring site data to build models linking city and urban covariates to air\ntemperature. The air temperature models are then used to capture\nhigh-resolution spatial emulator outputs for ground surface temperature\nmodelling. The main class of processes studied includes the Tukey g-and-h\nprocesses for capturing spatial and temporal aspects of heat processes in urban\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:42:22 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Murakami", "Daisuke", ""], ["Peters", "Gareth W.", ""], ["Matsui", "Tomoko", ""], ["Yamagata", "Yoshiki", ""]]}, {"id": "2004.00908", "submitter": "Chuansai Zhou", "authors": "Chuansai Zhou, Wen Yuan, Jun Wang, Haiyong Xu, Yong Jiang, Xinmin\n  Wang, Qiuzi Han Wen and Pingwen Zhang", "title": "Detecting Suspected Epidemic Cases Using Trajectory Big Data", "comments": null, "journal-ref": "CSIAM Transactions on Applied Mathematics. 1(2020).186-206", "doi": "10.4208/csiam-am.2020-0006", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging infectious diseases are existential threats to human health and\nglobal stability. The recent outbreaks of the novel coronavirus COVID-19 have\nrapidly formed a global pandemic, causing hundreds of thousands of infections\nand huge economic loss. The WHO declares that more precise measures to track,\ndetect and isolate infected people are among the most effective means to\nquickly contain the outbreak. Based on trajectory provided by the big data and\nthe mean field theory, we establish an aggregated risk mean field that contains\ninformation of all risk-spreading particles by proposing a spatio-temporal\nmodel named HiRES risk map. It has dynamic fine spatial resolution and high\ncomputation efficiency enabling fast update. We then propose an objective\nindividual epidemic risk scoring model named HiRES-p based on HiRES risk maps,\nand use it to develop statistical inference and machine learning methods for\ndetecting suspected epidemic-infected individuals. We conduct numerical\nexperiments by applying the proposed methods to study the early outbreak of\nCOVID-19 in China. Results show that the HiRES risk map has strong ability in\ncapturing global trend and local variability of the epidemic risk, thus can be\napplied to monitor epidemic risk at country, province, city and community\nlevels, as well as at specific high-risk locations such as hospital and\nstation. HiRES-p score seems to be an effective measurement of personal\nepidemic risk. The accuracy of both detecting methods are above 90\\% when the\npopulation infection rate is under 20\\%, which indicates great application\npotential in epidemic risk prevention and control practice.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 09:55:46 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 02:17:19 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 09:03:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhou", "Chuansai", ""], ["Yuan", "Wen", ""], ["Wang", "Jun", ""], ["Xu", "Haiyong", ""], ["Jiang", "Yong", ""], ["Wang", "Xinmin", ""], ["Wen", "Qiuzi Han", ""], ["Zhang", "Pingwen", ""]]}, {"id": "2004.00992", "submitter": "Lijun Sun Mr", "authors": "Zhanhong Cheng, Martin Trepanier and Lijun Sun", "title": "Incorporating travel behavior regularity into passenger flow forecasting", "comments": null, "journal-ref": "Transportation Research Part C: Emerging Technologies, 128, 103200\n  (2021)", "doi": "10.1016/j.trc.2021.103200", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasting of passenger flow (i.e., ridership) is critical to the\noperation of urban metro systems. Previous studies mainly model passenger flow\nas time series by aggregating individual trips and then perform forecasting\nbased on the values in the past several steps. However, this approach\nessentially overlooks the fact that passenger flow consists of trips from each\nindividual traveler. For example, a traveler's work trip in the morning can\nhelp predict his/her home trip in the evening, while this causal structure\ncannot be explicitly encoded in standard time series models. In this paper, we\npropose a new forecasting framework for boarding flow by incorporating the\ngenerative mechanism into standard time series models and leveraging the strong\nregularity rooted in travel behavior. In doing so, we introduce returning flow\nfrom previous alighting trips as a new covariate, which captures the causal\nstructure and long-range dependencies in passenger flow data based on travel\nbehavior. We develop the return probability parallelogram (RPP) to summarize\nthe causal relationships and estimate the return flow. The proposed framework\nis evaluated using real-world passenger flow data, and the results confirm that\nthe returning flow -- a single covariate -- can substantially and consistently\nimprove various forecasting tasks, including one-step ahead forecasting,\nmulti-step ahead forecasting, and forecasting under special events. And the\nproposed method is more effective for business-type stations with most\npassengers come and return within the same day. This study can be extended to\nother modes of transport, and it also sheds new light on general demand time\nseries forecasting problems, in which causal structure and long-range\ndependencies are generated by the user behavior.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 13:46:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 20:50:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Cheng", "Zhanhong", ""], ["Trepanier", "Martin", ""], ["Sun", "Lijun", ""]]}, {"id": "2004.01031", "submitter": "Samuel Thiriot", "authors": "Samuel Thiriot, Jean-Daniel Kant", "title": "Generate Country-Scale Networks of Interaction from Scattered Statistics", "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:2003.02213", "journal-ref": "in Proceedings of The Fifth Conference of the European Social\n  Simulation Association (ESSA'2008), Brescia, Italy", "doi": null, "report-no": null, "categories": "cs.MA cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to define the structure of interactions among a population of\nagents by a network. Most of agent-based models were shown highly sensitive to\nthat network, so the relevance of simulation results directely depends on the\ndescriptive power of that network. When studying social dynamics in large\npopulations, that network cannot be collected, and is rather generated by\nalgorithms which aim to fit general properties of social networks. However,\nmore precise data is available at a country scale in the form of\nsocio-demographic studies, census or sociological studies. These \"scattered\nstatistics\" provide rich information, especially on agents' attributes, similar\nproperties of tied agents and affiliations. In this paper, we propose a generic\nmethodology to bring up together these scattered statistics with bayesian\nnetworks. We explain how to generate a population of heterogeneous agents, and\nhow to create links by using both scattered statistics and knowledge on social\nselection processes. The methodology is illustrated by generating an\ninteraction network for rural Kenya which includes familial structure,\ncolleagues and friendship constrained given field studies and statistics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:38:40 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Thiriot", "Samuel", ""], ["Kant", "Jean-Daniel", ""]]}, {"id": "2004.01252", "submitter": "Grace Yi", "authors": "Grace Yi, Wenqing He, Dennis Kon-Jin Lin and Chun-Ming Yu", "title": "COVID-19: Should We Test Everyone?", "comments": "In total 27 pages, with 7 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the beginning of 2020, the coronavirus disease 2019 (COVID-19) has\nspread rapidly in the city of Wuhan, P.R. China, and subsequently, across the\nworld. The swift spread of the virus is largely attributed to its stealth\ntransmissions in which infected patients may be asymptomatic. Undetected\ntransmissions present a remarkable challenge for the containment of the virus\nand pose an appalling threat to the public. An urgent question that has been\nasked by the public is \"Should I be tested for COVID-19 if I am sick?\". While\ndifferent regions established their own criteria for screening infected cases,\nthe screening criteria have been modified based on new evidence and\nunderstanding of the virus as well as the availability of resources. The\nshortage of test kits and medical personnel has considerably limited our\nability to do as many tests as possible. Public health officials and clinicians\nare facing a dilemma of balancing the limited resources and unlimited demands.\nOn one hand, they are striving to achieve the best outcome by optimizing the\nusage of the scant resources. On the other hand, they are challenged by the\npatients' frustrations and anxieties, stemming from the concerns of not being\ntested for COVID-19 for not meeting the definition of PUI (person under\ninvestigation). In this paper, we evaluate the situation from the statistical\nviewpoint by factoring into the considerations of the uncertainty and\ninaccuracy of the test, an issue that is often overlooked by the general\npublic. We aim to shed light on the tough situation by providing evidence-based\nreasoning from the statistical angle, and we expect this examination will help\nthe general public understand and assess the situation rationally. Most\nimportantly, the development offers recommendations for physicians to make\nsensible evaluations to optimally use the limited resources for the best\nmedical outcome.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:42:55 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yi", "Grace", ""], ["He", "Wenqing", ""], ["Lin", "Dennis Kon-Jin", ""], ["Yu", "Chun-Ming", ""]]}, {"id": "2004.01291", "submitter": "Christopher Manning", "authors": "Daniel Ramage, Christopher D. Manning and Daniel A. McFarland", "title": "Mapping Three Decades of Intellectual Change in Academia", "comments": "10 pages and 6 figures plus appendix of 5 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on the development of science has focused on the creation of\nmultidisciplinary teams. However, while this coming together of people is\nsymmetrical, the ideas, methods, and vocabulary of science have a directional\nflow. We present a statistical model of the text of dissertation abstracts from\n1980 to 2010, revealing for the first time the large-scale flow of language\nacross fields. Results of the analysis include identifying methodological\nfields that export broadly, emerging topical fields that borrow heavily and\nexpand, and old topical fields that grow insular and retract. Particular\nfindings show a growing split between molecular and ecological forms of biology\nand a sea change in the humanities and social sciences driven by the rise of\ngender and ethnic studies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:34:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 17:15:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Ramage", "Daniel", ""], ["Manning", "Christopher D.", ""], ["McFarland", "Daniel A.", ""]]}, {"id": "2004.01292", "submitter": "Wagner Barreto-Souza", "authors": "Luiza Sette C\\^amara Piancastelli, Wagner Barreto-Souza and Vin\\'icius\n  Diniz Mayrink", "title": "Generalized inverse-Gaussian frailty models with application to TARGET\n  neuroblastoma data", "comments": null, "journal-ref": "Annals of the Institute of Statistical Mathematics (2021)", "doi": "10.1007/s10463-020-00774-z", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of survival frailty models based on the Generalized\nInverse-Gaussian (GIG) distributions is proposed. We show that the GIG frailty\nmodels are flexible and mathematically convenient like the popular gamma\nfrailty model. Furthermore, our proposed class is robust and does not present\nsome computational issues experienced by the gamma model. By assuming a\npiecewise-exponential baseline hazard function, which gives a semiparametric\nflavour for our frailty class, we propose an EM-algorithm for estimating the\nmodel parameters and provide an explicit expression for the information matrix.\nSimulated results are addressed to check the finite sample behavior of the\nEM-estimators and also to study the performance of the GIG models under\nmisspecification. We apply our methodology to a TARGET (Therapeutically\nApplicable Research to Generate Effective Treatments) data about survival time\nof patients with neuroblastoma cancer and show some advantages of the GIG\nfrailties over existing models in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:35:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Piancastelli", "Luiza Sette C\u00e2mara", ""], ["Barreto-Souza", "Wagner", ""], ["Mayrink", "Vin\u00edcius Diniz", ""]]}, {"id": "2004.01341", "submitter": "Bledar Konomi", "authors": "Si Cheng, Bledar A. Konomi, Jessica L. Matthews, Georgios Karagiannis,\n  Emily L. Kang", "title": "Hierarchical Bayesian Nearest Neighbor Co-Kriging Gaussian Process\n  Models; An Application to Intersatellite Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in remote sensing technology and the increasing size of\nsatellite constellations allows massive geophysical information to be gathered\ndaily on a global scale by numerous platforms of different fidelity. The\nauto-regressive co-kriging model is a suitable framework to analyse such data\nsets because it accounts for cross-dependencies among different fidelity\nsatellite outputs. However, its implementation in multifidelity large spatial\ndata-sets is practically infeasible because its computational complexity\nincreases cubically with the total number of observations. In this paper, we\npropose a nearest neighbour co-kriging Gaussian process that couples the\nauto-regressive model and nearest neighbour GP by using augmentation ideas;\nreducing the computational complexity to be linear with the total number of\nspatial observed locations. The latent process of the nearest neighbour GP is\naugmented in a manner which allows the specification of semi-conjugate priors.\nThis facilitates the design of an efficient MCMC sampler involving mostly\ndirect sampling updates which can be implemented in parallel computational\nenvironments. The good predictive performance of the proposed method is\ndemonstrated in a simulation study. We use the proposed method to analyze\nHigh-resolution Infrared Radiation Sounder data gathered from two NOAA polar\norbiting satellites.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 02:26:38 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 06:27:39 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 21:38:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cheng", "Si", ""], ["Konomi", "Bledar A.", ""], ["Matthews", "Jessica L.", ""], ["Karagiannis", "Georgios", ""], ["Kang", "Emily L.", ""]]}, {"id": "2004.01373", "submitter": "Xu Liang", "authors": "German A. Villalba, Xu Liang, and Yao Liang", "title": "Estimation of daily streamflow from multiple donor catchments with\n  Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm is introduced to improve estimations of daily streamflow\ntime series at sites with incomplete records based on the concept of\nconditional independence in graphical models. The goal is to fill in gaps of\nhistorical data or extend records at streamflow stations no longer in operation\nor even estimate streamflow at ungauged locations. This is achieved by first\nselecting relevant stations in the hydrometric network as reference (donor)\nstations and then using them to infer the missing data. The selection process\ntransforms fully connected streamflow stations in the hydrometric network into\na sparsely connected network represented by a precision matrix using a Gaussian\ngraphical model. The underlying graph encodes conditional independence\nconditions which allow determination of an optimum set of reference stations\nfrom the fully connected hydrometric network for a study area. The sparsity of\nthe precision matrix is imposed by using the Graphical Lasso algorithm with an\nL1-norm regularization parameter and a thresholding parameter. The two\nparameters are determined by a multi-objective optimization process. In\naddition, an algorithm based on the conditional independence concept is\npresented to allow a removal of gauges with the least loss of information. Our\napproaches are illustrated with daily streamflow data from a hydrometric\nnetwork of 34 gauges between 1 January 1950 and 31 December 1980 over the Ohio\nRiver basin. Our results show that the use of conditional independence\nconditions can lead to more accurate streamflow estimates than the widely used\napproaches which are based on either distance or pair-wise correlation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 04:58:16 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:46:08 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Villalba", "German A.", ""], ["Liang", "Xu", ""], ["Liang", "Yao", ""]]}, {"id": "2004.01764", "submitter": "Nathaniel Bastian PhD", "authors": "Kathleen Kerwin and Nathaniel D. Bastian", "title": "Stacked Generalizations in Imbalanced Fraud Data Sets using Resampling\n  Methods", "comments": "19 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses stacked generalization, which is a two-step process of\ncombining machine learning methods, called meta or super learners, for\nimproving the performance of algorithms in step one (by minimizing the error\nrate of each individual algorithm to reduce its bias in the learning set) and\nthen in step two inputting the results into the meta learner with its stacked\nblended output (demonstrating improved performance with the weakest algorithms\nlearning better). The method is essentially an enhanced cross-validation\nstrategy. Although the process uses great computational resources, the\nresulting performance metrics on resampled fraud data show that increased\nsystem cost can be justified. A fundamental key to fraud data is that it is\ninherently not systematic and, as of yet, the optimal resampling methodology\nhas not been identified. Building a test harness that accounts for all\npermutations of algorithm sample set pairs demonstrates that the complex,\nintrinsic data structures are all thoroughly tested. Using a comparative\nanalysis on fraud data that applies stacked generalizations provides useful\ninsight needed to find the optimal mathematical formula to be used for\nimbalanced fraud data sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 20:38:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kerwin", "Kathleen", ""], ["Bastian", "Nathaniel D.", ""]]}, {"id": "2004.01991", "submitter": "Jack Noonan", "authors": "Anatoly Zhigljavsky, Roger Whitaker, Ivan Fesenko, Kobi Kremnizer,\n  Jack Noonan, Paul Harper, Jonathan Gillard, Thomas Woolley, Daniel Gartner,\n  Jasmine Grimsley, Edilson de Arruda, Val Fedorov, and Tom Crick MBE", "title": "Generic probabilistic modelling and non-homogeneity issues for the UK\n  epidemic of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus COVID-19 spreads through the population mostly based on social\ncontact. To gauge the potential for widespread contagion, to cope with\nassociated uncertainty and to inform its mitigation, more accurate and robust\nmodelling is centrally important for policy making. We provide a flexible\nmodelling approach that increases the accuracy with which insights can be made.\nWe use this to analyse different scenarios relevant to the COVID-19 situation\nin the UK. We present a stochastic model that captures the inherently\nprobabilistic nature of contagion between population members. The computational\nnature of our model means that spatial constraints (e.g., communities and\nregions), the susceptibility of different age groups and other factors such as\nmedical pre-histories can be incorporated with ease. We analyse different\npossible scenarios of the COVID-19 situation in the UK. Our model is robust to\nsmall changes in the parameters and is flexible in being able to deal with\ndifferent scenarios.\n  This approach goes beyond the convention of representing the spread of an\nepidemic through a fixed cycle of susceptibility, infection and recovery (SIR).\nIt is important to emphasise that standard SIR-type models, unlike our model,\nare not flexible enough and are also not stochastic and hence should be used\nwith extreme caution. Our model allows both heterogeneity and inherent\nuncertainty to be incorporated. Due to the scarcity of verified data, we draw\ninsights by calibrating our model using parameters from other relevant sources,\nincluding agreement on average (mean field) with parameters in SIR-based\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 18:26:59 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 09:39:16 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Zhigljavsky", "Anatoly", ""], ["Whitaker", "Roger", ""], ["Fesenko", "Ivan", ""], ["Kremnizer", "Kobi", ""], ["Noonan", "Jack", ""], ["Harper", "Paul", ""], ["Gillard", "Jonathan", ""], ["Woolley", "Thomas", ""], ["Gartner", "Daniel", ""], ["Grimsley", "Jasmine", ""], ["de Arruda", "Edilson", ""], ["Fedorov", "Val", ""], ["MBE", "Tom Crick", ""]]}, {"id": "2004.02007", "submitter": "Anastasios Papanikos Tasos", "authors": "Tasos Papanikos, John R Thompson, Keith R Abrams, Sylwia Bujkiewicz", "title": "A novel approach to bivariate meta-analysis of binary outcomes and its\n  application in the context of surrogate endpoints", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bivariate meta-analysis provides a useful framework for combining information\nacross related studies and has been widely utilised to combine evidence from\nclinical studies in order to evaluate treatment efficacy. Bivariate\nmeta-analysis has also been used to investigate surrogacy patterns between\ntreatment effects on the surrogate and the final outcome. Surrogate endpoints\nplay an important role in drug development when they can be used to measure\ntreatment effect early compared to the final clinical outcome and to predict\nclinical benefit or harm. The standard bivariate meta-analytic approach models\nthe observed treatment effects on the surrogate and final outcomes jointly, at\nboth the within-study and between-studies levels, using a bivariate normal\ndistribution. For binomial data a normal approximation can be used on log odds\nratio scale, however, this method may lead to biased results when the\nproportions of events are close to one or zero, affecting the validation of\nsurrogate endpoints. In this paper, two Bayesian meta-analytic approaches are\nintroduced which allow for modelling the within-study variability using\nbinomial data directly. The first uses independent binomial likelihoods to\nmodel the within-study variability avoiding to approximate the observed\ntreatment effects, however, ignores the within-study association. The second,\nmodels the summarised events in each arm jointly using a bivariate copula with\nbinomial marginals. This allows the model to take into account the within-study\nassociation through the copula dependence parameter. We applied the methods to\nan illustrative example in chronic myeloid leukemia to investigate the\nsurrogate relationship between complete cytogenetic response (CCyR) and\nevent-free-survival (EFS).\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:05:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Papanikos", "Tasos", ""], ["Thompson", "John R", ""], ["Abrams", "Keith R", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "2004.02228", "submitter": "Fengqing Chao Dr.", "authors": "Fengqing Chao, Christophe Z. Guilmoto, Samir K.C., Hernando Ombao", "title": "Probabilistic Projection of the Sex Ratio at Birth and Missing Female\n  Births by State and Union Territory in India", "comments": null, "journal-ref": "PLoS ONE 2020, Vol. 15, No. 8, e0236673", "doi": "10.1371/journal.pone.0236673", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sex ratio at birth (SRB) in India has been reported imbalanced since the\n1970s. Previous studies have shown a great variation in the SRB across\ngeographic locations in India till 2016. As one of the most populous countries\nand in view of its great regional heterogeneity, it is crucial to produce\nprobabilistic projections for the SRB in India at state level for the purpose\nof population projection and policy planning. In this paper, we implement a\nBayesian hierarchical time series model to project SRB in India by state. We\ngenerate SRB probabilistic projections from 2017 to 2030 for 29 States and\nUnion Territories (UTs) in India, and present results in 21 States/UTs with\ndata from the Sample Registration System. Our analysis takes into account two\nstate-specific factors that contribute to sex-selective abortion and resulting\nsex imbalances at birth: intensity of son preference and fertility squeeze. We\nproject that the largest contribution to female births deficits is in Uttar\nPradesh, with cumulative number of missing female births projected to be 2.0\n(95% credible interval [1.9; 2.2]) million from 2017 to 2030. The total female\nbirth deficits during 2017-2030 for the whole India is projected to be 6.8\n[6.6; 7.0] million.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 15:11:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chao", "Fengqing", ""], ["Guilmoto", "Christophe Z.", ""], ["C.", "Samir K.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.02333", "submitter": "Carlos Relvas", "authors": "Carlos Relvas and Andr\\'e Fujita", "title": "Stage I non-small cell lung cancer stratification by using a model-based\n  clustering algorithm with covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is currently the leading cause of cancer deaths. Among various\nsubtypes, the number of patients diagnosed with stage I non-small cell lung\ncancer (NSCLC), particularly adenocarcinoma, has been increasing. It is\nestimated that 30 - 40\\% of stage I patients will relapse, and 10 - 30\\% will\ndie due to recurrence, clearly suggesting the presence of a subgroup that could\nbe benefited by additional therapy. We hypothesize that current attempts to\nidentify stage I NSCLC subgroup failed due to covariate effects, such as the\nage at diagnosis and differentiation, which may be masking the results. In this\ncontext, to stratify stage I NSCLC, we propose CEM-Co, a model-based clustering\nalgorithm that removes/minimizes the effects of undesirable covariates during\nthe clustering process. We applied CEM-Co on a gene expression data set\ncomposed of 129 subjects diagnosed with stage I NSCLC and successfully\nidentified a subgroup with a significantly different phenotype (poor\nprognosis), while standard clustering algorithms failed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:12:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Relvas", "Carlos", ""], ["Fujita", "Andr\u00e9", ""]]}, {"id": "2004.02386", "submitter": "Victor Sal y Rosas Giancarlo", "authors": "Cristian Bayes, Victor Sal y Rosas and Luis Valdivieso", "title": "Modelling death rates due to COVID-19: A Bayesian approach", "comments": "12 pages, 4 Figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To estimate the number of deaths in Peru due to COVID-19. Design:\nWith a priori information obtained from the daily number of deaths due to\nCODIV-19 in China and data from the Peruvian authorities, we constructed a\npredictive Bayesian non-linear model for the number of deaths in Peru.\nExposure: COVID-19. Outcome: Number of deaths. Results: Assuming an\nintervention level similar to the one implemented in China, the total number of\ndeaths in Peru is expected to be 612 (95%CI: 604.3 - 833.7) persons. Sixty four\ndays after the first reported death, the 99% of expected deaths will be\nobserved. The inflexion point in the number of deaths is estimated to be around\nday 26 (95%CI: 25.1 - 26.8) after the first reported death. Conclusion: These\nestimates can help authorities to monitor the epidemic and implement strategies\nin order to manage the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 02:57:15 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 16:37:30 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bayes", "Cristian", ""], ["Rosas", "Victor Sal y", ""], ["Valdivieso", "Luis", ""]]}, {"id": "2004.02390", "submitter": "Xu Liang", "authors": "Ruochen Sun, Felipe Hern\\'andez, Xu Liang, and Huiling Yuan", "title": "A calibration framework for high-resolution hydrological models using a\n  multiresolution and heterogeneous strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing spatial and temporal resolution of numerical models continues to\npropel progress in hydrological sciences, but, at the same time, it has\nstrained the ability of modern automatic calibration methods to produce\nrealistic model parameter combinations for these models. This paper presents a\nnew reliable and fast automatic calibration framework to address this issue. In\nessence, the proposed framework, adopting a divide and conquer strategy, first\npartitions the parameters into groups of different resolutions based on their\nsensitivity or importance, in which the most sensitive parameters are\nprioritized with highest resolution in parameter search space, while the least\nsensitive ones are explored with the coarsest resolution at beginning. This is\nfollowed by an optimization based iterative calibration procedure consisting of\na series of sub-tasks or runs. Between consecutive runs, the setup\nconfiguration is heterogeneous with parameter search ranges and resolutions\nvarying among groups. At the completion of each sub-task, the parameter ranges\nwithin each group are systematically refined from their previously estimated\nranges which are initially based on a priori information. Parameters attain\nstable convergence progressively with each run. A comparison of this new\ncalibration framework with a traditional optimization-based approach was\nperformed using a quasi-synthetic double-model setup experiment to calibrate\n134 parameters and two well-known distributed hydrological models: the Variable\nInfiltration Capacity (VIC) model and the Distributed Hydrology Soil Vegetation\nModel (DHSVM). The results demonstrate statistically that the proposed\nframework can better mitigate equifinality problem, yields more realistic model\nparameter estimates, and is computationally more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 03:37:37 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 03:35:27 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sun", "Ruochen", ""], ["Hern\u00e1ndez", "Felipe", ""], ["Liang", "Xu", ""], ["Yuan", "Huiling", ""]]}, {"id": "2004.02449", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel and Norbert Hilger", "title": "Score Predictor Factor Analysis as model for the identification of\n  single-item indicators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score Predictor Factor Analysis (SPFA) was introduced as a method that allows\nto compute factor score predictors that are -- under some conditions -- more\nhighly correlated with the common factors resulting from factor analysis than\nthe factor score predictors computed from the common factor model. In the\npresent study, we investigate SPFA as a model in its own rights. In order to\nprovide a basis for this, the properties and the utility of SPFA factor score\npredictors and the possibility to identify single-item indicators in SPFA\nloading matrices were investigated. Regarding the factor score predictors, the\nmain result is that the best linear predictor of the score predictor factor\nanalysis has not only perfect determinacy but is also correlation preserving.\nRegarding the SPFA loadings it was found in a simulation study that five or\nmore population factors that are represented by only one variable with a rather\nsubstantial loading can more accurately be identified by means of SPFA than\nwith conventional factor analysis. Moreover, the percentage of correctly\nidentified single-item indicators was substantially larger for SPFA than for\nthe common factor model. It is therefore argued that SPFA is a tool that can be\nespecially helpful when very short scales or single-item indicators are to be\nidentified.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:46:36 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Hilger", "Norbert", ""]]}, {"id": "2004.02605", "submitter": "James Johndrow", "authors": "James Johndrow, Kristian Lum, Maria Gargiulo, Patrick Ball", "title": "Estimating the number of SARS-CoV-2 infections and the impact of social\n  distancing in the United States", "comments": "Update of the previous version of the manuscript now including\n  state-level analysis and some early estimates of the effect of social\n  distancing policies on transmission. Basic model/approach remains the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the number of individuals who have been infected with the novel\ncoronavirus SARS-CoV-2, and the extent to which social distancing policies have\nbeen effective at limiting its spread, are critical for effective policy going\nforward. Here we present estimates of the extent to which confirmed cases in\nthe United States undercount the true number of infections, and analyze how\neffective social distancing measures have been at mitigating or suppressing the\nvirus. Our analysis uses a Bayesian model of COVID-19 fatalities with a\nlikelihood based on an underlying differential equation model of the epidemic.\nWe provide analysis for four states with significant epidemics: California,\nFlorida, New York, and Washington. Our short-term forecasts suggest that these\nstates may be following somewhat different trajectories for growth of the\nnumber of cases and fatalities.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 12:32:02 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 00:16:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Johndrow", "James", ""], ["Lum", "Kristian", ""], ["Gargiulo", "Maria", ""], ["Ball", "Patrick", ""]]}, {"id": "2004.02670", "submitter": "Olivier Scaillet", "authors": "Stelios Arvanitis, Olivier Scaillet, Nikolas Topaloglou", "title": "Spanning analysis of stock market anomalies under Prospect Stochastic\n  Dominance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.EM q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and implement methods for determining whether introducing new\nsecurities or relaxing investment constraints improves the investment\nopportunity set for prospect investors. We formulate a new testing procedure\nfor prospect spanning for two nested portfolio sets based on subsampling and\nLinear Programming. In an application, we use the prospect spanning framework\nto evaluate whether well-known anomalies are spanned by standard factors. We\nfind that of the strategies considered, many expand the opportunity set of the\nprospect type investors, thus have real economic value for them. In-sample and\nout-of-sample results prove remarkably consistent in identifying genuine\nanomalies for prospect investors.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:41:32 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Arvanitis", "Stelios", ""], ["Scaillet", "Olivier", ""], ["Topaloglou", "Nikolas", ""]]}, {"id": "2004.02692", "submitter": "Claudia Kirch", "authors": "Idris Eckley, Claudia Kirch, Silke Weber", "title": "A novel change point approach for the detection of gas emission sources\n  using remotely contained concentration data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an example from remote sensing of gas emission sources, we\nderive two novel change point procedures for multivariate time series where, in\ncontrast to classical change point literature, the changes are not required to\nbe aligned in the different components of the time series. Instead the change\npoints are described by a functional relationship where the precise shape\ndepends on unknown parameters of interest such as the source of the gas\nemission in the above example. Two different types of tests and the\ncorresponding estimators for the unknown parameters describing the change\nlocations are proposed. We derive the null asymptotics for both tests under\nweak assumptions on the error time series and show asymptotic consistency under\nalternatives. Furthermore, we prove consistency for the corresponding\nestimators of the parameters of interest. The small sample behavior of the\nmethodology is assessed by means of a simulation study and the above remote\nsensing example analyzed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:17:53 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Eckley", "Idris", ""], ["Kirch", "Claudia", ""], ["Weber", "Silke", ""]]}, {"id": "2004.02799", "submitter": "Mike Pereira", "authors": "Mike Pereira, Nicolas Desassis, C\\'edric Magneron, Nathan Palmer", "title": "A matrix-free approach to geostatistical filtering", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to geostatistical filtering which\ntackles two challenges encountered when applying this method to complex spatial\ndatasets: modeling the non-stationarity of the data while still being able to\nwork with large datasets. The approach is based on a finite element\napproximation of Gaussian random fields expressed as an expansion of the\neigenfunctions of a Laplace--Beltrami operator defined to account for local\nanisotropies. The numerical approximation of the resulting random fields using\na finite element approach is then leveraged to solve the scalability issue\nthrough a matrix-free approach. Finally, two cases of application of this\napproach, on simulated and real seismic data are presented.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:44:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pereira", "Mike", ""], ["Desassis", "Nicolas", ""], ["Magneron", "C\u00e9dric", ""], ["Palmer", "Nathan", ""]]}, {"id": "2004.03127", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Modeling and presentation of vaccination coverage estimates using data\n  from household surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly popular to produce high-resolution maps of\nvaccination coverage by fitting Bayesian geostatistical models to data from\nhousehold surveys. Often, the surveys adopt a stratified cluster sampling\ndesign. We discuss a number of crucial choices with respect to two key aspects\nof the map production process: the acknowledgment of the survey design in\nmodeling, and the appropriate presentation of estimates and their\nuncertainties. Specifically, we consider the importance of accounting for\nsurvey stratification and cluster-level non-spatial excess variation in survey\noutcomes when fitting geostatistical models. We also discuss the trade-off\nbetween the geographical scale and precision of model-based estimates, and\ndemonstrate visualization methods for mapping and ranking that emphasize the\nprobabilistic interpretation of results. A novel approach to coverage map\npresentation is proposed to allow comparison and control of the overall map\nuncertainty level. We use measles vaccination coverage in Nigeria as a\nmotivating example and illustrate the different issues using data from the 2018\nNigeria Demographic and Health Survey.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:52:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2004.03130", "submitter": "Soudeep Deb", "authors": "Soudeep Deb", "title": "Analyzing count data using a time series model with an exponentially\n  decaying covariance structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data appears in various disciplines. In this work, a new method to\nanalyze time series count data has been proposed. The method assumes\nexponentially decaying covariance structure, a special class of the Mat\\'ern\ncovariance function, for the latent variable in a Poisson regression model. It\nis implemented in a Bayesian framework, with the help of Gibbs sampling and\nARMS sampling techniques. The proposed approach provides reliable estimates for\nthe covariate effects and estimates the extent of variability explained by the\ntemporally dependent process and the white noise process. The method is\nflexible, allows irregular spaced data, and can be extended naturally to bigger\ndatasets. The Bayesian implementation helps us to compute the posterior\npredictive distribution and hence is more appropriate and attractive for count\ndata forecasting problems. Two real life applications of different flavors are\nincluded in the paper. These two examples and a short simulation study\nestablish that the proposed approach has good inferential and predictive\nabilities and performs better than the other competing models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:57:54 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 12:19:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Deb", "Soudeep", ""]]}, {"id": "2004.03187", "submitter": "Erlis Ruli", "authors": "Paolo Girardi, Luca Greco, Valentina Mameli, Monica Musio, Walter\n  Racugno, Erlis Ruli, Laura Ventura", "title": "Robust inference for nonlinear regression models from the Tsallis score:\n  application to Covid-19 contagion in Italy", "comments": "15 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach for fitting robust nonlinear regression models, which\ncan be employed to model and predict the contagion dynamics of the Covid-19 in\nItaly. The focus is on the analysis of epidemic data using robust dose-response\ncurves, but the functionality is applicable to arbitrary nonlinear regression\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:15:33 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 07:38:14 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Girardi", "Paolo", ""], ["Greco", "Luca", ""], ["Mameli", "Valentina", ""], ["Musio", "Monica", ""], ["Racugno", "Walter", ""], ["Ruli", "Erlis", ""], ["Ventura", "Laura", ""]]}, {"id": "2004.03322", "submitter": "Itsik Bergel", "authors": "Itsik Bergel", "title": "Variable pool testing for infection spread estimation", "comments": "This work is relevant for COVID-19 testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for efficient estimation of the prevalence of infection\nin a population with high accuracy using only a small number of tests. The\npresented approach uses pool testing with a mix of pool sizes of various sizes.\nThe test results are then combined to generate an accurate estimation over a\nwide range of infection probabilities. This method does not require an initial\nguess on the infection probability. We show that, using the suggested method,\neven a set of only $50$ tests with a total of only $1000$ samples can produce\nreasonable estimation over a wide range of probabilities. A measurement set\nwith only $100$ tests is shown to achieve $25\\%$ accuracy over infection\nprobabilities from $0.001$ to $0.5$. The presented method is applicable to\nCOVID-19 testing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 12:59:08 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bergel", "Itsik", ""]]}, {"id": "2004.03384", "submitter": "Kerstin Ritter", "authors": "Matthias Ritter, Derek V.M. Ott, Friedemann Paul, John-Dylan Haynes,\n  Kerstin Ritter", "title": "Covid-19 -- A simple statistical model for predicting ICU load in early\n  phases of the disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One major bottleneck in the ongoing COVID-19 pandemic is the limited number\nof critical care beds. Due to the dynamic development of infections and the\ntime lag between when patients are infected and when a proportion of them\nenters an intensive care unit (ICU), the need for future intensive care can\neasily be underestimated. To infer future ICU load from reported infections, we\nsuggest a simple statistical model that (1) accounts for time lags and (2)\nallows for making predictions depending on different future growth of\ninfections. We have evaluated our model for three regions, namely Berlin\n(Germany), Lombardy (Italy), and Madrid (Spain). Before extensive containment\nmeasures made an impact, we first estimate the region-specific model\nparameters. Whereas for Berlin, an ICU rate of 6%, a time lag of 6 days, and an\naverage stay of 12 days in ICU provide the best fit of the data, for Lombardy\nand Madrid the ICU rate was higher (18% and 15%) and the time lag (0 and 3\ndays) and the average stay (4 and 8 days) in ICU shorter. The region-specific\nmodels are then used to predict future ICU load assuming either a continued\nexponential phase with varying growth rates (0-15%) or linear growth. Thus, the\nmodel can help to predict a potential exceedance of ICU capacity. Although our\npredictions are based on small data sets and disregard non-stationary dynamics,\nour model is simple, robust, and can be used in early phases of the disease\nwhen data are scarce.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:54:18 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 14:50:28 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ritter", "Matthias", ""], ["Ott", "Derek V. M.", ""], ["Paul", "Friedemann", ""], ["Haynes", "John-Dylan", ""], ["Ritter", "Kerstin", ""]]}, {"id": "2004.03481", "submitter": "Lijun Sun Mr", "authors": "Lijun Sun, Xinyu Chen, Zhaocheng He and Luis F. Miranda-Moreno", "title": "Routine pattern discovery and anomaly detection in individual travel\n  behavior", "comments": null, "journal-ref": "Networks and Spatial Economics (2021)", "doi": "10.1007/s11067-021-09542-9", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering patterns and detecting anomalies in individual travel behavior is\na crucial problem in both research and practice. In this paper, we address this\nproblem by building a probabilistic framework to model individual\nspatiotemporal travel behavior data (e.g., trip records and trajectory data).\nWe develop a two-dimensional latent Dirichlet allocation (LDA) model to\ncharacterize the generative mechanism of spatiotemporal trip records of each\ntraveler. This model introduces two separate factor matrices for the spatial\ndimension and the temporal dimension, respectively, and use a two-dimensional\ncore structure at the individual level to effectively model the joint\ninteractions and complex dependencies. This model can efficiently summarize\ntravel behavior patterns on both spatial and temporal dimensions from very\nsparse trip sequences in an unsupervised way. In this way, complex travel\nbehavior can be modeled as a mixture of representative and interpretable\nspatiotemporal patterns. By applying the trained model on future/unseen\nspatiotemporal records of a traveler, we can detect her behavior anomalies by\nscoring those observations using perplexity. We demonstrate the effectiveness\nof the proposed modeling framework on a real-world license plate recognition\n(LPR) data set. The results confirm the advantage of statistical learning\nmethods in modeling sparse individual travel behavior data. This type of\npattern discovery and anomaly detection applications can provide useful\ninsights for traffic monitoring, law enforcement, and individual travel\nbehavior profiling.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:28:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sun", "Lijun", ""], ["Chen", "Xinyu", ""], ["He", "Zhaocheng", ""], ["Miranda-Moreno", "Luis F.", ""]]}, {"id": "2004.03855", "submitter": "Jouni Takalo", "authors": "Jouni Takalo and Kalevi Mursula", "title": "Comparison of the shape and temporal evolution of even and odd solar\n  cycles", "comments": "10 pages, 13 figures", "journal-ref": "A&A 636, A11 (2020)", "doi": "10.1051/0004-6361/202037488", "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results. The PCA confirms the existence of the Gnevyshev gap (GG) for solar\ncycles at about 40% from the start of the cycle. The temporal evolution of\nsunspot area data for even cycles shows that the GG exists at least at the 95%\nconfidence level for all sizes of sunspots. On the other hand, the GG is\nshorter and statistically insignificant for the odd cycles of aerial sunspot\ndata. Furthermore, the analysis of sunspot area sizes for even and odd cycles\nof SC12-SC23 shows that the greatest difference is at 4.2-4.6 years, where even\ncycles have a far smaller total area than odd cycles. The average area of the\nindividual sunspots of even cycles is also smaller in this interval. The\nstatistical analysis of the temporal evolution shows that northern sunspot\ngroups maximise earlier than southern groups for even cycles, but are\nconcurrent for odd cycles. Furthermore, the temporal distributions of odd\ncycles are slightly more leptokurtic than distributions of even cycles. The\nskewnesses are 0.37 and 0.49 and the kurtoses 2.79 and 2.94 for even and odd\ncycles, respectively. The correlation coefficient between skewness and kurtosis\nfor even cycles is 0.69, and for odd cycles, it is 0.90. Conclusions. The\nseparate PCAs for even and odd sunspot cycles show that odd cycles are more\ninhomogeneous than even cycles, especially in GSN data. Even cycles, however,\nhave two anomalous cycles: SC4 and SC6. According to the analysis of the\nsunspot area size data, the GG is more distinct in even than odd cycles. We\nalso present another Waldmeier-type rule, that is, we find a correlation\nbetween skewness and kurtosis of the sunspot group cycles.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:45:34 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Takalo", "Jouni", ""], ["Mursula", "Kalevi", ""]]}, {"id": "2004.04041", "submitter": "Derek Chang", "authors": "Derek Chang, Devendra Shelar, Saurabh Amin", "title": "Stochastic Resource Allocation for Electricity Distribution Network\n  Resilience", "comments": "6 pages, 5 figures, accepted to 2020 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, it has become crucial to improve the resilience of\nelectricity distribution networks (DNs) against storm-induced failures.\nMicrogrids enabled by Distributed Energy Resources (DERs) can significantly\nhelp speed up re-energization of loads, particularly in the complete absence of\nbulk power supply. We describe an integrated approach which considers a\npre-storm DER allocation problem under the uncertainty of failure scenarios as\nwell as a post-storm dispatch problem in microgrids during the multi-period\nrepair of the failed components. This problem is computationally challenging\nbecause the number of scenarios (resp. binary variables) increases\nexponentially (resp. quadratically) in the network size. Our overall solution\napproach for solving the resulting two-stage mixed-integer linear program\n(MILP) involves implementing the sample average approximation (SAA) method and\nBenders Decomposition. Additionally, we implement a greedy approach to reduce\nthe computational time requirements of the post-storm repair scheduling and\ndispatch problem. The optimality of the resulting solution is evaluated on a\nmodified IEEE 36-node network.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:14:53 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chang", "Derek", ""], ["Shelar", "Devendra", ""], ["Amin", "Saurabh", ""]]}, {"id": "2004.04150", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton, Ali Shojaie", "title": "Directed Graphical Models and Causal Discovery for Zero-Inflated Data", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern RNA sequencing technologies provide gene expression measurements from\nsingle cells that promise refined insights on regulatory relationships among\ngenes. Directed graphical models are well-suited to explore such (cause-effect)\nrelationships. However, statistical analyses of single cell data are\ncomplicated by the fact that the data often show zero-inflated expression\npatterns. To address this challenge, we propose directed graphical models that\nare based on Hurdle conditional distributions parametrized in terms of\npolynomials in parent variables and their 0/1 indicators of being zero or\nnonzero. While directed graphs for Gaussian models are only identifiable up to\nan equivalence class in general, we show that, under a natural and weak\nassumption, the exact directed acyclic graph of our zero-inflated models can be\nidentified. We propose methods for graph recovery, apply our model to real\nsingle-cell RNA-seq data on T helper cells, and show simulated experiments that\nvalidate the identifiability and graph estimation methods in practice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:59:12 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2004.04251", "submitter": "Noah Haber", "authors": "Noah A Haber, Mollie E Wood, Sarah Wieten, Alexander Breskin", "title": "DAG With Omitted Objects Displayed (DAGWOOD): A framework for revealing\n  causal assumptions in DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) are frequently used in epidemiology as a guide\nto encode causal inference assumptions. We propose the DAGWOOD framework to\nbring many of those encoded assumptions to the forefront.\n  DAGWOOD combines a root DAG (the DAG in the proposed analysis) and a set of\nbranch DAGs (alternative hidden assumptions to the root DAG). All branch DAGs\nshare a common ruleset, and must 1) change the root DAG, 2) be a valid DAG, and\neither 3a) change the minimally sufficient adjustment set or 3b) change the\nnumber of frontdoor paths. Branch DAGs comprise a list of assumptions which\nmust be justified as negligible. We define two types of branch DAGs: exclusion\nbranch DAGs add a single- or bidirectional pathway between two nodes in the\nroot DAG (e.g. direct pathways and colliders), while misdirection branch DAGs\nrepresent alternative pathways that could be drawn between objects (e.g.,\ncreating a collider by reversing the direction of causation for a controlled\nconfounder).\n  The DAGWOOD framework 1) organizes causal model assumptions, 2) reinforces\nbest DAG practices, 3) provides a framework for evaluation of causal models,\nand 4) can be used for generating causal models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:59:42 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:27:51 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 02:14:21 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 22:30:25 GMT"}, {"version": "v5", "created": "Fri, 2 Oct 2020 16:50:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Haber", "Noah A", ""], ["Wood", "Mollie E", ""], ["Wieten", "Sarah", ""], ["Breskin", "Alexander", ""]]}, {"id": "2004.04258", "submitter": "Seungyong Hwang", "authors": "Seungyong Hwang, Thomas C.M. Lee, Debashis Paul, Jie Peng", "title": "Estimating Fiber Orientation Distribution through Blockwise Adaptive\n  Thresholding with Application to HCP Young Adults Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent technological advances, large brain imaging data sets can now\nbe collected. Such data are highly complex so extraction of meaningful\ninformation from them remains challenging. Thus, there is an urgent need for\nstatistical procedures that are computationally scalable and can provide\naccurate estimates that capture the neuronal structures and their\nfunctionalities. We propose a fast method for estimating the fiber orientation\ndistribution(FOD) based on diffusion MRI data. This method models the observed\ndMRI signal at any voxel as a convolved and noisy version of the underlying\nFOD, and utilizes the spherical harmonics basis for representing the FOD, where\nthe spherical harmonic coefficients are adaptively and nonlinearly shrunk by\nusing a James-Stein type estimator. To further improve the estimation accuracy\nby enhancing the localized peaks of the FOD, as a second step a\nsuper-resolution sharpening process is then applied. The resulting estimated\nFODs can be fed to a fiber tracking algorithm to reconstruct the white matter\nfiber tracts. We illustrate the overall methodology using both synthetic data\nand data from the Human Connectome Project.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:12:01 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 18:57:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hwang", "Seungyong", ""], ["Lee", "Thomas C. M.", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""]]}, {"id": "2004.04301", "submitter": "Earl Lawrence", "authors": "K. Sham Bhat, Kary Myers, Earl Lawrence, James Colgan, Elizabeth Judge", "title": "Estimating Scale Discrepancy in Bayesian Model Calibration for ChemCam\n  on the Mars Curiosity Rover", "comments": "21 pages, 10 Figures, submitted to the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": "LA-UR-19-22659", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mars rover Curiosity carries an instrument called ChemCam to determine\nthe composition of the soil and rocks. ChemCam uses laser-induced breakdown\nspectroscopy (LIBS) for this purpose. Los Alamos National Laboratory has\ndeveloped a simulation capability that can predict spectra from ChemCam, but\nthere are major scale differences between the prediction and observation. This\npresents a challenge when using Bayesian model calibration to determine the\nunknown physical parameters that describe the LIBS observations. We present an\nanalysis of LIBS data to support ChemCam based on including a structured\ndiscrepancy model in a Bayesian model calibration scheme. This is both a novel\napplication of Bayesian model calibration and a general purpose approach to\naccounting for such systematic differences between theory and observation in\nthis setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 23:51:53 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bhat", "K. Sham", ""], ["Myers", "Kary", ""], ["Lawrence", "Earl", ""], ["Colgan", "James", ""], ["Judge", "Elizabeth", ""]]}, {"id": "2004.04339", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Yuki Matsushima and Ryota Ishii", "title": "Confidence interval for the AUC of SROC curve and some related methods\n  using bootstrap for meta-analysis of diagnostic accuracy studies", "comments": null, "journal-ref": "Communications in Statistics: Case Studies, Data Analysis and\n  Applications, 2021", "doi": "10.1080/23737484.2021.1894408", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the curve (AUC) of summary receiver operating characteristic\n(SROC) curve is a primary statistical outcome for meta-analysis of diagnostic\ntest accuracy studies (DTA). However, its confidence interval has not been\nreported in most of DTA meta-analyses, because no certain methods and\nstatistical packages have been provided. In this article, we provide a\nbootstrap algorithm for computing the confidence interval of the AUC. Also,\nusing the bootstrap framework, we can conduct a bootstrap test for assessing\nsignificance of the difference of AUCs for multiple diagnostic tests. In\naddition, we provide an influence diagnostic method based on the AUC by\nleave-one-study-out analyses. We present illustrative examples using two DTA\nmet-analyses for diagnostic tests of cervical cancer and asthma. We also\ndeveloped an easy-to-handle R package dmetatools for these computations. The\nvarious quantitative evidence provided by these methods certainly supports the\ninterpretations and precise evaluations of statistical evidence of DTA\nmeta-analyses.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:48:01 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 06:12:32 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:07:41 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Noma", "Hisashi", ""], ["Matsushima", "Yuki", ""], ["Ishii", "Ryota", ""]]}, {"id": "2004.04529", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Ziyi Qiu", "title": "Scenario analysis of non-pharmaceutical interventions on global COVID-19\n  transmissions", "comments": "published in Covid Economics: Vetted and Real-Time Papers, Centre for\n  Economic Policy Research (CEPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a dynamic panel SIR (DP-SIR) model to investigate the\nimpact of non-pharmaceutical interventions (NPIs) on the COVID-19 transmission\ndynamics with panel data from 9 countries across the globe. By constructing\nscenarios with different combinations of NPIs, our empirical findings suggest\nthat countries may avoid the lockdown policy with imposing school closure, mask\nwearing and centralized quarantine to reach similar outcomes on controlling the\nCOVID-19 infection. Our results also suggest that, as of April 4th, 2020,\ncertain countries such as the U.S. and Singapore may require additional\nmeasures of NPIs in order to control disease transmissions more effectively,\nwhile other countries may cautiously consider to gradually lift some NPIs to\nmitigate the costs to the overall economy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:32:25 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 04:03:34 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 01:34:29 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Xiaohui", ""], ["Qiu", "Ziyi", ""]]}, {"id": "2004.04765", "submitter": "Nathaniel Josephs", "authors": "Nathaniel Josephs, Lizhen Lin, Steven Rosenberg and Eric D. Kolaczyk", "title": "Bayesian classification, anomaly detection, and survival analysis using\n  network inputs with application to the microbiome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the study of a single network is well-established, technological\nadvances now allow for the collection of multiple networks with relative ease.\nIncreasingly, anywhere from several to thousands of networks can be created\nfrom brain imaging, gene co-expression data, or microbiome measurements. And\nthese networks, in turn, are being looked to as potentially powerful features\nto be used in modeling. However, with networks being non-Euclidean in nature,\nhow best to incorporate them into standard modeling tasks is not obvious. In\nthis paper, we propose a Bayesian modeling framework that provides a unified\napproach to binary classification, anomaly detection, and survival analysis\nwith network inputs. We encode the networks in the kernel of a Gaussian process\nprior via their pairwise differences and we discuss several choices of provably\npositive definite kernel that can be plugged into our models. Although our\nmethods are widely applicable, we are motivated here in particular by\nmicrobiome research (where network analysis is emerging as the standard\napproach for capturing the interconnectedness of microbial taxa across both\ntime and space) and its potential for reducing preterm delivery and improving\npersonalization of prenatal care.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:26:09 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:08:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Josephs", "Nathaniel", ""], ["Lin", "Lizhen", ""], ["Rosenberg", "Steven", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2004.04813", "submitter": "Vukosi Marivate", "authors": "Vukosi Marivate, Herkulaas MvE Combrink", "title": "Use of Available Data To Inform The COVID-19 Outbreak in South Africa: A\n  Case Study", "comments": "Accepted for publication in the Data Science Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The coronavirus disease (COVID-19), caused by the SARS-CoV-2 virus, was\ndeclared a pandemic by the World Health Organization (WHO) in February 2020.\nCurrently, there are no vaccines or treatments that have been approved after\nclinical trials. Social distancing measures, including travel bans, school\nclosure, and quarantine applied to countries or regions are being used to limit\nthe spread of the disease and the demand on the healthcare infrastructure. The\nseclusion of groups and individuals has led to limited access to accurate\ninformation. To update the public, especially in South Africa, announcements\nare made by the minister of health daily. These announcements narrate the\nconfirmed COVID-19 cases and include the age, gender, and travel history of\npeople who have tested positive for the disease. Additionally, the South\nAfrican National Institute for Communicable Diseases updates a daily\ninfographic summarising the number of tests performed, confirmed cases,\nmortality rate, and the regions affected. However, the age of the patient and\nother nuanced data regarding the transmission is only shared in the daily\nannouncements and not on the updated infographic. To disseminate this\ninformation, the Data Science for Social Impact research group at the\nUniversity of Pretoria, South Africa, has worked on curating and applying\npublicly available data in a way that is computer-readable so that information\ncan be shared to the public - using both a data repository and a dashboard.\nThrough collaborative practices, a variety of challenges related to publicly\navailable data in South Africa came to the fore. These include shortcomings in\nthe accessibility, integrity, and data management practices between\ngovernmental departments and the South African public. In this paper, solutions\nto these problems will be shared by using a publicly available data repository\nand dashboard as a case study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 06:13:37 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 16:36:37 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Marivate", "Vukosi", ""], ["Combrink", "Herkulaas MvE", ""]]}, {"id": "2004.04835", "submitter": "Julia Eisenberg", "authors": "S. Sahin, M.C. Boado-Penas, C. Constantinescu, J. Eisenberg, K.\n  Henshaw, M. Hu, J. Wang, W. Zhu", "title": "COVID-19 in a social reinsurance framework: Forewarned is forearmed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crisis caused by COVID-19 revealed the global unpreparedness to handle\nthe impact of a pandemic. In this paper, we present a statistical analysis of\nthe data related to the COVID-19 outbreak in China, specifically the infection\nspeed, death and fatality rates in Hubei province. By fitting distributions of\nthese quantities we design a parametric reinsurance contract whose trigger and\ncap are based on the probability distributions of the infection speed, death\nand fatality rates. In particular, fitting the distribution for the infection\nspeed and death rates we provide a measure of the effectiveness of a state's\naction during an epidemic, and propose a reinsurance contract as a supplement\nto a state's social insurance to alleviate financial costs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 22:23:30 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Sahin", "S.", ""], ["Boado-Penas", "M. C.", ""], ["Constantinescu", "C.", ""], ["Eisenberg", "J.", ""], ["Henshaw", "K.", ""], ["Hu", "M.", ""], ["Wang", "J.", ""], ["Zhu", "W.", ""]]}, {"id": "2004.04837", "submitter": "Yaakov Malinovsky", "authors": "Gregory Haber, Yaakov Malinovsky, Paul S. Albert", "title": "Is Group Testing Ready for Prime-time in Disease Identification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale disease screening is a complicated process in which high costs\nmust be balanced against pressing public health needs. When the goal is\nscreening for infectious disease, one approach is group testing in which\nsamples are initially tested in pools and individual samples are retested only\nif the initial pooled test was positive. Intuitively, if the prevalence of\ninfection is small, this could result in a large reduction of the total number\nof tests required. Despite this, the use of group testing in medical studies\nhas been limited, largely due to skepticism about the impact of pooling on the\naccuracy of a given assay. While there is a large body of research addressing\nthe issue of testing errors in group testing studies, it is customary to assume\nthat the misclassification parameters are known from an external population\nand/or that the values do not change with the group size. Both of these\nassumptions are highly questionable for many medical practitioners considering\ngroup testing in their study design. In this article, we explore how the\nfailure of these assumptions might impact the efficacy of a group testing\ndesign and, consequently, whether group testing is currently feasible for\nmedical screening. Specifically, we look at how incorrect assumptions about the\nsensitivity function at the design stage can lead to poor estimation of a\nprocedure's overall sensitivity and expected number of tests. Furthermore, if a\nvalidation study is used to estimate the pooled misclassification parameters of\na given assay, we show that the sample sizes required are so large as to be\nprohibitive in all but the largest screening programs\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 22:33:42 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 10:29:51 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 22:25:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Haber", "Gregory", ""], ["Malinovsky", "Yaakov", ""], ["Albert", "Paul S.", ""]]}, {"id": "2004.04871", "submitter": "Satish Viswanath", "authors": "Amir Reza Sadri, Andrew Janowczyk, Ren Zou, Ruchika Verma, Niha Beig,\n  Jacob Antunes, Anant Madabhushi, Pallavi Tiwari, Satish E. Viswanath", "title": "MRQy: An Open-Source Tool for Quality Control of MR Imaging Data", "comments": "28 pages, 7 figures. Submitted to Medical Physics", "journal-ref": null, "doi": "10.1002/mp.14593", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We sought to develop a quantitative tool to quickly determine relative\ndifferences in MRI volumes both within and between large MR imaging cohorts\n(such as available in The Cancer Imaging Archive (TCIA)), in order to help\ndetermine the generalizability of radiomics and machine learning schemes to\nunseen datasets. The tool is intended to help quantify presence of (a) site- or\nscanner-specific variations in image resolution, field-of-view, or image\ncontrast, or (b) imaging artifacts such as noise, motion, inhomogeneity,\nringing, or aliasing; which can adversely affect relative image quality between\ndata cohorts. We present MRQy, a new open-source quality control tool to (a)\ninterrogate MRI cohorts for site- or equipment-based differences, and (b)\nquantify the impact of MRI artifacts on relative image quality; to help\ndetermine how to correct for these variations prior to model development. MRQy\nextracts a series of quality measures (e.g. noise ratios, variation metrics,\nentropy and energy criteria) and MR image metadata (e.g. voxel resolution,\nimage dimensions) for subsequent interrogation via a specialized HTML5 based\nfront-end designed for real-time filtering and trend visualization. MRQy was\nused to evaluate (a) n=133 brain MRIs from TCIA (7 sites), and (b) n=104 rectal\nMRIs (3 local sites). MRQy measures revealed significant site-specific\nvariations in both cohorts, indicating potential batch effects. Marked\ndifferences in specific MRQy measures were also able to identify outlier MRI\ndatasets that needed to be corrected for common MR imaging artifacts. MRQy is\ndesigned to be a standalone, unsupervised tool that can be efficiently run on a\nstandard desktop computer. It has been made freely accessible at\n\\url{http://github.com/ccipd/MRQy} for wider community use and feedback.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 01:30:51 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 17:42:08 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 14:04:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sadri", "Amir Reza", ""], ["Janowczyk", "Andrew", ""], ["Zou", "Ren", ""], ["Verma", "Ruchika", ""], ["Beig", "Niha", ""], ["Antunes", "Jacob", ""], ["Madabhushi", "Anant", ""], ["Tiwari", "Pallavi", ""], ["Viswanath", "Satish E.", ""]]}, {"id": "2004.04984", "submitter": "Michael Pfarrhofer", "authors": "Michael Pfarrhofer", "title": "Forecasts with Bayesian vector autoregressions under real time\n  conditions", "comments": "JEL: C11, C32, C53; Keywords: time-varying parameters, stochastic\n  volatility, nowcasting, global-local shrinkage", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the sensitivity of forecast performance measures to\ntaking a real time versus pseudo out-of-sample perspective. We use monthly\nvintages for the United States (US) and the Euro Area (EA) and estimate a set\nof vector autoregressive (VAR) models of different sizes with constant and\ntime-varying parameters (TVPs) and stochastic volatility (SV). Our results\nsuggest differences in the relative ordering of model performance for point and\ndensity forecasts depending on whether real time data or truncated final\nvintages in pseudo out-of-sample simulations are used for evaluating forecasts.\nNo clearly superior specification for the US or the EA across variable types\nand forecast horizons can be identified, although larger models featuring TVPs\nappear to be affected the least by missing values and data revisions. We\nidentify substantial differences in performance metrics with respect to whether\nforecasts are produced for the US or the EA.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:43:48 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Pfarrhofer", "Michael", ""]]}, {"id": "2004.05027", "submitter": "Marco Mariani", "authors": "Giulio Grossi, Patrizia Lattarulo, Marco Mariani, Alessandra Mattei,\n  \\\"Ozge \\\"Oner", "title": "Synthetic Control Group Methods in the Presence of Interference: The\n  Direct and Spillover Effects of Light Rail on Neighborhood Retail Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Synthetic Control Group (SCG) methods have received great\nattention from scholars and have been subject to extensions and comparisons\nwith alternative approaches for program evaluation. However, the existing\nmethodological literature mainly relies on the assumption of non-interference.\nWe investigate the use of the SCG method in panel comparative case studies\nwhere interference between the treated and the untreated units is plausible. We\nframe our discussion in the potential outcomes approach. Under a partial\ninterference assumption, we formally define relevant direct and spillover\neffects. We also consider the \"unrealized\" spillover effect on the treated unit\nin the hypothetical scenario that another unit in the treated unit's\nneighborhood had been assigned to the intervention. Then we investigate the\nassumptions under which we can identify and estimate the causal effects of\ninterest, and show how they can be estimated using the SCG method. We apply our\napproach to the analysis of an observational study, where the focus is on\nassessing direct and spillover causal effects of a new light rail line recently\nbuilt in Florence (Italy) on the retail density of the street where it was\nbuilt and of the streets in the treated street's neighborhood.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:24:15 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 08:52:52 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 10:30:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Grossi", "Giulio", ""], ["Lattarulo", "Patrizia", ""], ["Mariani", "Marco", ""], ["Mattei", "Alessandra", ""], ["\u00d6ner", "\u00d6zge", ""]]}, {"id": "2004.05048", "submitter": "James Murphy", "authors": "Shukun Zhang and James M. Murphy", "title": "Hyperspectral Image Clustering with Spatially-Regularized Ultrametrics", "comments": "5 pages, 2 columns, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the unsupervised clustering of hyperspectral images\nbased on spatially regularized spectral clustering with ultrametric path\ndistances. The proposed method efficiently combines data density and geometry\nto distinguish between material classes in the data, without the need for\ntraining labels. The proposed method is efficient, with quasilinear scaling in\nthe number of data points, and enjoys robust theoretical performance\nguarantees. Extensive experiments on synthetic and real HSI data demonstrate\nits strong performance compared to benchmark and state-of-the-art methods. In\nparticular, the proposed method achieves not only excellent labeling accuracy,\nbut also efficiently estimates the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 14:27:41 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Shukun", ""], ["Murphy", "James M.", ""]]}, {"id": "2004.05111", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B. D.\n  Sorensen", "title": "Deep transfer learning for improving single-EEG arousal detection", "comments": "Accepted for presentation at EMBC2020", "journal-ref": null, "doi": "10.1109/EMBC44109.2020.9176723", "report-no": null, "categories": "cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets in sleep science present challenges for machine learning algorithms\ndue to differences in recording setups across clinics. We investigate two deep\ntransfer learning strategies for overcoming the channel mismatch problem for\ncases where two datasets do not contain exactly the same setup leading to\ndegraded performance in single-EEG models. Specifically, we train a baseline\nmodel on multivariate polysomnography data and subsequently replace the first\ntwo layers to prepare the architecture for single-channel\nelectroencephalography data. Using a fine-tuning strategy, our model yields\nsimilar performance to the baseline model (F1=0.682 and F1=0.694,\nrespectively), and was significantly better than a comparable single-channel\nmodel. Our results are promising for researchers working with small databases\nwho wish to use deep learning models pre-trained on larger databases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:51:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:18:28 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2004.05217", "submitter": "Marco Pollo Almeida", "authors": "Marco Pollo Almeida, Rafael Paixao, Pedro Ramos, Vera Tomazella,\n  Francisco Louzada, Ricardo Ehlers", "title": "Multiple repairable systems under dependent competing risks with\n  nonparametric Frailty", "comments": "17 pages, 14 figures. This article is part of the doctoral thesis of\n  the first author and appears in the bibliography of this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this article is to analyze data from multiple repairable systems\nunder the presence of dependent competing risks. In order to model this\ndependence structure, we adopted the well-known shared frailty model. This\nmodel provides a suitable theoretical basis for generating dependence between\nthe components failure times in the dependent competing risks model. It is\nknown that the dependence effect in this scenario influences the estimates of\nthe model parameters. Hence, under the assumption that the cause-specific\nintensities follow a PLP, we propose a frailty-induced dependence approach to\nincorporate the dependence among the cause-specific recurrent processes.\nMoreover, the misspecification of the frailty distribution may lead to errors\nwhen estimating the parameters of interest. Because of this, we considered a\nBayesian nonparametric approach to model the frailty density in order to offer\nmore flexibility and to provide consistent estimates for the PLP model, as well\nas insights about heterogeneity among the systems. Both simulation studies and\nreal case studies are provided to illustrate the proposed approaches and\ndemonstrate their validity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 20:16:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Almeida", "Marco Pollo", ""], ["Paixao", "Rafael", ""], ["Ramos", "Pedro", ""], ["Tomazella", "Vera", ""], ["Louzada", "Francisco", ""], ["Ehlers", "Ricardo", ""]]}, {"id": "2004.05272", "submitter": "Claire Donnat", "authors": "Claire Donnat, Susan Holmes", "title": "Modeling the Heterogeneity in COVID-19's Reproductive Number and its\n  Impact on Predictive Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correct evaluation of the reproductive number $R$ for COVID-19 -- which\ncharacterizes the average number of secondary cases generated by each typical\nprimary case -- is central in the quantification of the potential scope of the\npandemic and the selection of an appropriate course of action. In most models,\n$R$ is modeled as a universal constant for the virus across outbreak clusters\nand individuals -- effectively averaging out the inherent variability of the\ntransmission process due to varying individual contact rates, population\ndensities, demographics, or temporal factors amongst many. Yet, due to the\nexponential nature of epidemic growth, the error due to this simplification can\nbe rapidly amplified and lead to inaccurate predictions and/or risk evaluation.\nFrom the statistical modeling perspective, the magnitude of the impact of this\naveraging remains an open question: how can this intrinsic variability be\npercolated into epidemic models, and how can its impact on uncertainty\nquantification and predictive scenarios be better quantified? In this paper, we\npropose to study this question through a Bayesian perspective, creating a\nbridge between the agent-based and compartmental approaches commonly used in\nthe literature. After deriving a Bayesian model that captures at scale the\nheterogeneity of a population and environmental conditions, we simulate the\nspread of the epidemic as well as the impact of different social distancing\nstrategies, and highlight the strong impact of this added variability on the\nreported results. We base our discussion on both synthetic experiments --\nthereby quantifying of the reliability and the magnitude of the effects -- and\nreal COVID-19 data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 00:45:44 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 05:45:14 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 00:39:53 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 17:56:06 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Donnat", "Claire", ""], ["Holmes", "Susan", ""]]}, {"id": "2004.05334", "submitter": "Marco Gramatica", "authors": "Marco Gramatica, Peter Congdon, Silvia Liverani", "title": "Bayesian modelling for spatially misaligned health areal data: a\n  multiple membership approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes prevalence is on the rise in the UK, and for public health strategy,\nestimation of relative disease risk and subsequent mapping is important. We\nconsider an application to London data on diabetes prevalence and mortality. In\norder to improve the estimation of relative risks we analyse jointly prevalence\nand mortality data to ensure borrowing strength over the two outcomes. The\navailable data involves two spatial frameworks, areas (middle level super\noutput areas, MSOAs), and general practices (GPs) recruiting patients from\nseveral areas. This raises a spatial misalignment issue that we deal with by\nemploying the multiple membership principle. Specifically we translate area\nspatial effects to explain GP practice prevalence according to proportions of\nGP populations resident in different areas. A sparse implementation in Stan of\nboth the MCAR and GMCAR allows the comparison of these bivariate priors as well\nas exploring the different implications for the mapping patterns for both\noutcomes. The necessary causal precedence of diabetes prevalence over mortality\nallows a specific conditionality assumption in the GMCAR, not always present in\nthe context of disease mapping.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 08:11:38 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 17:09:01 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 20:12:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gramatica", "Marco", ""], ["Congdon", "Peter", ""], ["Liverani", "Silvia", ""]]}, {"id": "2004.05367", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "A new multilayer network construction via Tensor learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks proved to be suitable in extracting and providing\ndependency information of different complex systems. The construction of these\nnetworks is difficult and is mostly done with a static approach, neglecting\ntime delayed interdependences. Tensors are objects that naturally represent\nmultilayer networks and in this paper, we propose a new methodology based on\nTucker tensor autoregression in order to build a multilayer network directly\nfrom data. This methodology captures within and between connections across\nlayers and makes use of a filtering procedure to extract relevant information\nand improve visualization. We show the application of this methodology to\ndifferent stationary fractionally differenced financial data. We argue that our\nresult is useful to understand the dependencies across three different aspects\nof financial risk, namely market risk, liquidity risk, and volatility risk.\nIndeed, we show how the resulting visualization is a useful tool for risk\nmanagers depicting dependency asymmetries between different risk factors and\naccounting for delayed cross dependencies. The constructed multilayer network\nshows a strong interconnection between the volumes and prices layers across all\nthe stocks considered while a lower number of interconnections between the\nuncertainty measures is identified.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:06:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2004.05374", "submitter": "Simone Riggi", "authors": "S. Riggi, D. Riggi and F. Riggi", "title": "Handling missing data in a neural network approach for the\n  identification of charged particles in a multilayer detector", "comments": "14 pages, 8 figures", "journal-ref": "Nucl. Instr. and Methods in Physics Research A 780 (2015) 81-90", "doi": "10.1016/j.nima.2015.01.063", "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an physics.ins-det stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of charged particles in a multilayer detector by the energy\nloss technique may also be achieved by the use of a neural network. The\nperformance of the network becomes worse when a large fraction of information\nis missing, for instance due to detector inefficiencies. Algorithms which\nprovide a way to impute missing information have been developed over the past\nyears. Among the various approaches, we focused on normal mixtures models in\ncomparison with standard mean imputation and multiple imputation methods.\nFurther, to account for the intrinsic asymmetry of the energy loss data, we\nconsidered skew-normal mixture models and provided a closed form implementation\nin the Expectation-Maximization (EM) algorithm framework to handle missing\npatterns. The method has been applied to a test case where the energy losses of\npions, kaons and protons in a six-layers Silicon detector are considered as\ninput neurons to a neural network. Results are given in terms of reconstruction\nefficiency and purity of the various species in different momentum bins.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 11:43:21 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Riggi", "S.", ""], ["Riggi", "D.", ""], ["Riggi", "F.", ""]]}, {"id": "2004.05429", "submitter": "Debabrota Basu", "authors": "Naheed Anjum Arafat, Debabrota Basu, Laurent Decreusefond, Stephane\n  Bressan", "title": "Construction and Random Generation of Hypergraphs with Prescribed Degree\n  and Dimension Sequences", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose algorithms for construction and random generation of hypergraphs\nwithout loops and with prescribed degree and dimension sequences. The objective\nis to provide a starting point for as well as an alternative to Markov chain\nMonte Carlo approaches. Our algorithms leverage the transposition of properties\nand algorithms devised for matrices constituted of zeros and ones with\nprescribed row- and column-sums to hypergraphs. The construction algorithm\nextends the applicability of Markov chain Monte Carlo approaches when the\ninitial hypergraph is not provided. The random generation algorithm allows the\ndevelopment of a self-normalised importance sampling estimator for hypergraph\nproperties such as the average clustering coefficient.We prove the correctness\nof the proposed algorithms. We also prove that the random generation algorithm\ngenerates any hypergraph following the prescribed degree and dimension\nsequences with a non-zero probability. We empirically and comparatively\nevaluate the effectiveness and efficiency of the random generation algorithm.\nExperiments show that the random generation algorithm provides stable and\naccurate estimates of average clustering coefficient, and also demonstrates a\nbetter effective sample size in comparison with the Markov chain Monte Carlo\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:44:14 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Arafat", "Naheed Anjum", ""], ["Basu", "Debabrota", ""], ["Decreusefond", "Laurent", ""], ["Bressan", "Stephane", ""]]}, {"id": "2004.05443", "submitter": "Phuong T. Vu", "authors": "Phuong T. Vu, Adam A. Szpiro, Noah Simon", "title": "Spatial Matrix Completion for Spatially-Misaligned and High-Dimensional\n  Air Pollution Data", "comments": "20 pages, 4 figures, 1 table, 1 supplemental file (available upon\n  request)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In health-pollution cohort studies, accurate predictions of pollutant\nconcentrations at new locations are needed, since the locations of fixed\nmonitoring sites and study participants are often spatially misaligned. For\nmulti-pollution data, principal component analysis (PCA) is often incorporated\nto obtain low-rank (LR) structure of the data prior to spatial prediction.\nRecently developed predictive PCA modifies the traditional algorithm to improve\nthe overall predictive performance by leveraging both LR and spatial structures\nwithin the data. However, predictive PCA requires complete data or an initial\nimputation step. Nonparametric imputation techniques without accounting for\nspatial information may distort the underlying structure of the data, and thus\nfurther reduce the predictive performance. We propose a convex optimization\nproblem inspired by the LR matrix completion framework and develop a proximal\nalgorithm to solve it. Missing data are imputed and handled concurrently within\nthe algorithm, which eliminates the necessity of a separate imputation step. We\nshow that our algorithm has low computational burden and leads to reliable\npredictive performance as the severity of missing data increases.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 16:51:56 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Vu", "Phuong T.", ""], ["Szpiro", "Adam A.", ""], ["Simon", "Noah", ""]]}, {"id": "2004.05641", "submitter": "Jose R. Zubizarreta", "authors": "Juan D. Diaz, Jose R. Zubizarreta", "title": "Complex Discontinuity Designs Using Covariates for Policy Impact\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity designs are extensively used for causal inference in\nobservational studies. However, they are usually confined to settings with\nsimple treatment rules, determined by a single running variable, with a single\ncutoff. Motivated by the problem of estimating the impact of grade retention on\neducational and juvenile crime outcomes, in this paper we propose a framework\nand methods for complex discontinuity designs that encompasses multiple\ntreatment rules. In this framework, the observed covariates play a central role\nfor identification, estimation, and generalization of causal effects.\nIdentification is non-parametric and relies on a local strong ignorability\nassumption. Estimation proceeds as in any observational study under strong\nignorability, yet in a neighborhood of the cutoffs of the running variables. We\ndiscuss estimation approaches based on matching and weighting, including\ncomplementary regression modeling adjustments. We present assumptions for\ngeneralization; that is, for identification and estimation of average treatment\neffects for target populations. We also describe two approaches to select the\nneighborhood for analysis. We find that grade retention has a negative impact\non future grade retention, but is not associated with dropping out of school or\ncommitting a juvenile crime.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 16:06:23 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 22:15:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Diaz", "Juan D.", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "2004.05895", "submitter": "Audrey B\\\"urki", "authors": "A. B\\\"urki, S. Elbuy, S. Madec, S. Vasishth", "title": "What did we learn from forty years of research on semantic interference?\n  A Bayesian metaanalysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When participants in an experiment have to name pictures while ignoring\ndistractor words superimposed on the picture or presented auditorily (i.e.,\npicture-word interference paradigm), they take more time when the word to be\nnamed (or target) and distractor words are from the same semantic category\n(e.g., cat-dog). This experimental effect is known as the semantic interference\neffect, and is probably one of the most studied in the language production\nliterature. The functional origin of the effect and the exact conditions in\nwhich it occurs are however still debated. Since Lupker reported the effect in\nthe first response time experiment about 40 years ago, more than 300 similar\nexperiments have been conducted. The semantic interference effect was\nreplicated in many experiments, but several studies also reported the absence\nof an effect in a subset of experimental conditions. The aim of the present\nstudy is to provide a comprehensive theoretical review of the existing evidence\nto date and several Bayesian meta-analyses and meta-regressions to determine\nthe size of the effect and explore the experimental conditions in which the\neffect surfaces. The results are discussed in the light of current debates\nabout the functional origin of the semantic interference effect and its\nimplications for our understanding of the language production system.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:07:26 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 09:41:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["B\u00fcrki", "A.", ""], ["Elbuy", "S.", ""], ["Madec", "S.", ""], ["Vasishth", "S.", ""]]}, {"id": "2004.05925", "submitter": "Maryna Prus", "authors": "Maryna Prus and Hans-Peter Piepho", "title": "Optimizing the allocation of trials to sub-regions in multi-environment\n  crop variety testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New crop varieties are extensively tested in multi-environment trials in\norder to obtain a solid empirical basis for recommendations to farmers. When\nthe target population of environments is large and heterogeneous, a division\ninto sub-regions is often advantageous. When designing such trials, the\nquestion arises how to allocate trials to the different subregions. We consider\na solution to this problem assuming a linear mixed model. We propose an\nanalytical approach for computation of optimal designs for best linear unbiased\nprediction of genotype effects and pairwise linear contrasts and illustrate the\nobtained results by a real data example from Indian nation-wide maize variety\ntrials. It is shown that, except in simple cases such as a compound symmetry\nmodel, the optimal allocation depends on the variance-covariance structure for\ngenotypic effects nested within sub-regions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 13:15:50 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Prus", "Maryna", ""], ["Piepho", "Hans-Peter", ""]]}, {"id": "2004.05964", "submitter": "Shusei Eshima", "authors": "Shusei Eshima, Kosuke Imai and Tomoya Sasaki", "title": "Keyword Assisted Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, fully automated content analysis based on probabilistic\ntopic models has become popular among social scientists because of their\nscalability. The unsupervised nature of the models makes them suitable for\nexploring topics in a corpus without prior knowledge. However, researchers find\nthat these models often fail to measure specific concepts of substantive\ninterest by inadvertently creating multiple topics with similar content and\ncombining distinct themes into a single topic. In this paper, we empirically\ndemonstrate that providing a small number of keywords can substantially enhance\nthe measurement performance of topic models. An important advantage of the\nproposed keyword assisted topic model (keyATM) is that the specification of\nkeywords requires researchers to label topics prior to fitting a model to the\ndata. This contrasts with a widespread practice of post-hoc topic\ninterpretation and adjustments that compromises the objectivity of empirical\nfindings. In our application, we find that keyATM provides more interpretable\nresults, has better document classification performance, and is less sensitive\nto the number of topics than the standard topic models. Finally, we show that\nkeyATM can also incorporate covariates and model time trends. An open-source\nsoftware package is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:35:28 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:24:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Eshima", "Shusei", ""], ["Imai", "Kosuke", ""], ["Sasaki", "Tomoya", ""]]}, {"id": "2004.06033", "submitter": "Jan Vandenbroucke", "authors": "Jan P Vandenbroucke, Elizabeth B Brickley, Christina M J E\n  Vandenbroucke-Grauls, Neil Pearce", "title": "The test-negative design with additional population controls: a\n  practical approach to rapidly obtain information on the causes of the\n  SARS-CoV-2 epidemic", "comments": "13 pages, 1 figure, Appendices 9 pages. PMID: 32841988", "journal-ref": "Epidemiology: November 2020 - Volume 31 - Issue 6 - p 836-843", "doi": "10.1097/EDE.0000000000001251", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing of symptomatic persons for infection with SARS-CoV-2 is occurring\nworldwide. We propose two types of case-control studies that can be carried out\njointly in test-settings for symptomatic persons. The first, the test-negative\ncase-control design (TND) is the easiest to implement; it only demands\ncollecting information about potential risk factors for COVID-19 from the\ntested symptomatic persons. The second, standard case-control studies with\npopulation controls, requires the collection of data on one or more population\ncontrols for each person who is tested in the test facilities, so that\ntest-positives and test-negatives can each be compared with population\ncontrols. The TND will detect differences in risk factors between symptomatic\npersons who have COVID-19 (test-positives) and those who have other respiratory\ninfections (test-negatives). However, risk factors with effect sizes of equal\nmagnitude for both COVID-19 and other respiratory infections will not be\nidentified by the TND. Therefore, we discuss how to add population controls to\ncompare with the test-positives and the test-negatives, yielding two additional\ncase-control studies. We describe two options for population control groups:\none composed of accompanying persons to the test facilities, the other drawn\nfrom existing country-wide health care databases. We also describe other\npossibilities for population controls. Combining the TND with population\ncontrols yields a triangulation approach that distinguishes between exposures\nthat are risk factors for both COVID-19 and other respiratory infections, and\nexposures that are risk factors for just COVID-19. This combined design can be\napplied to future epidemics, but also to study causes of non-epidemic disease.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:05:35 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 12:49:59 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 14:34:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vandenbroucke", "Jan P", ""], ["Brickley", "Elizabeth B", ""], ["Vandenbroucke-Grauls", "Christina M J E", ""], ["Pearce", "Neil", ""]]}, {"id": "2004.06054", "submitter": "Xin Gao", "authors": "Xin Gao, Li Li, Li Luo", "title": "Decomposition of Total Effect with the Notion of Natural Counterfactual\n  Interaction Effect", "comments": "72 pages in total, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis serves as a crucial tool to obtain causal inference based\non directed acyclic graphs, which has been widely employed in the areas of\nbiomedical science, social science, epidemiology and psychology. Decomposition\nof total effect provides a deep insight to fully understand the casual\ncontribution from each path and interaction term. Since the four-way\ndecomposition method was proposed to identify the mediated interaction effect\nin counterfactual framework, the idea had been extended to a more sophisticated\nscenario with non-sequential multiple mediators. However, the method exhibits\nlimitations as the causal structure contains direct causal edges between\nmediators, such as inappropriate modeling of dependence and\nnon-identifiability. We develop the notion of natural counterfactual\ninteraction effect and find that the decomposition of total effect can be\nconsistently realized with our proposed notion. Furthermore, natural\ncounterfactual interaction effect overcomes the drawbacks and possesses a clear\nand significant interpretation, which may largely improve the capacity of\nresearchers to analyze highly complex causal structures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:29:58 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Gao", "Xin", ""], ["Li", "Li", ""], ["Luo", "Li", ""]]}, {"id": "2004.06068", "submitter": "Vincenzo Nardelli", "authors": "Giorgio Alleva, Giuseppe Arbia, Piero Demetrio Falorsi, Vincenzo\n  Nardelli and Alberto Zuliani", "title": "A sample approach to the estimation of the critical parameters of the\n  SARS-CoV-2 epidemics: an operational design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the urgent informational needs connected with the diffusion of\ninfection with regard to the COVID-19 pandemic, in this paper, we propose a\nsampling design for building a continuous-time surveillance system. Compared\nwith other observational strategies, the proposed method has three important\nelements of strength and originality: (i) it aims to provide a snapshot of the\nphenomenon at a single moment in time, and it is designed to be a continuous\nsurvey that is repeated in several waves over time, taking different target\nvariables during different stages of the development of the epidemic into\naccount; (ii) the statistical optimality properties of the proposed estimators\nare formally derived and tested with a Monte Carlo experiment; and (iii) it is\nrapidly operational as this property is required by the emergency connected\nwith the diffusion of the virus. The sampling design is thought to be designed\nwith the diffusion of SAR-CoV-2 in Italy during the spring of 2020 in mind.\nHowever, it is very general, and we are confident that it can be easily\nextended to other geographical areas and to possible future epidemic outbreaks.\nFormal proofs and a Monte Carlo exercise highlight that the estimators are\nunbiased and have higher efficiency than the simple random sampling scheme.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:38:40 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:56:55 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 17:02:45 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Alleva", "Giorgio", ""], ["Arbia", "Giuseppe", ""], ["Falorsi", "Piero Demetrio", ""], ["Nardelli", "Vincenzo", ""], ["Zuliani", "Alberto", ""]]}, {"id": "2004.06098", "submitter": "Nick Obradovich", "authors": "James H. Fowler, Seth J. Hill, Remy Levin, Nick Obradovich", "title": "The effect of stay-at-home orders on COVID-19 cases and fatalities in\n  the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governments issue \"stay at home\" orders to reduce the spread of contagious\ndiseases, but the magnitude of such orders' effectiveness is uncertain. In the\nUnited States these orders were not coordinated at the national level during\nthe coronavirus disease 2019 (COVID-19) pandemic, which creates an opportunity\nto use spatial and temporal variation to measure the policies' effect with\ngreater accuracy. Here, we combine data on the timing of stay-at-home orders\nwith daily confirmed COVID-19 cases and fatalities at the county level in the\nUnited States. We estimate the effect of stay-at-home orders using a\ndifference-in-differences design that accounts for unmeasured local variation\nin factors like health systems and demographics and for unmeasured temporal\nvariation in factors like national mitigation actions and access to tests.\nCompared to counties that did not implement stay-at-home orders, the results\nshow that the orders are associated with a 30.2 percent (11.0 to 45.2)\nreduction in weekly cases after one week, a 40.0 percent (23.4 to 53.0)\nreduction after two weeks, and a 48.6 percent (31.1 to 61.7) reduction after\nthree weeks. Stay-at-home orders are also associated with a 59.8 percent (18.3\nto 80.2) reduction in weekly fatalities after three weeks. These results\nsuggest that stay-at-home orders reduced confirmed cases by 390,000 (170,000 to\n680,000) and fatalities by 41,000 (27,000 to 59,000) within the first three\nweeks in localities where they were implemented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 17:57:10 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 01:40:59 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 20:16:39 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 21:26:35 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Fowler", "James H.", ""], ["Hill", "Seth J.", ""], ["Levin", "Remy", ""], ["Obradovich", "Nick", ""]]}, {"id": "2004.06139", "submitter": "Brady West PhD", "authors": "Brady T. West, Roderick J.A. Little, Rebecca R. Andridge, Philip S.\n  Boonstra, Erin B. Ware, Anita Pandit, Fernanda Alvarado-Leiton", "title": "Assessing Selection Bias in Regression Coefficients Estimated from\n  Non-Probability Samples, with Applications to Genetics and Demographic\n  Surveys", "comments": "29 pages, 4 figures, 2 tables, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection bias is a serious potential problem for inference about\nrelationships of scientific interest based on samples without well-defined\nprobability sampling mechanisms. Motivated by the potential for selection bias\nin (a) estimated relationships of polygenic scores (PGSs) with phenotypes in\ngenetic studies of volunteers, and (b) estimated differences in subgroup means\nin surveys of smartphone users, we derive novel measures of selection bias for\nestimates of the coefficients in linear and probit regression models fitted to\nnon-probability samples, when aggregate-level auxiliary data are available for\nthe selected sample and the target population. The measures arise from normal\npattern-mixture models that allow analysts to examine the sensitivity of their\ninferences to assumptions about non-ignorable selection in these samples. We\nexamine the effectiveness of the proposed measures in a simulation study, and\nthen use them to quantify the selection bias in (a) estimated PGS-phenotype\nrelationships in a large study of volunteers recruited via Facebook, and (b)\nestimated subgroup differences in mean past-year employment duration in a\nnon-probability sample of low-educated smartphone users. We evaluate the\nperformance of the measures in these applications using benchmark estimates\nfrom large probability samples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:18:53 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:09:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 17:17:20 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 17:18:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["West", "Brady T.", ""], ["Little", "Roderick J. A.", ""], ["Andridge", "Rebecca R.", ""], ["Boonstra", "Philip S.", ""], ["Ware", "Erin B.", ""], ["Pandit", "Anita", ""], ["Alvarado-Leiton", "Fernanda", ""]]}, {"id": "2004.06145", "submitter": "Jonathan Larson", "authors": "Jonathan Larson and Jukka-Pekka Onnela", "title": "Affiliation network model of HIV transmission in MSM", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black men who have sex with men (MSM) in the U.S. are more likely to be\nHIV-positive than White MSM. Intentional and unintentional segregation of Black\nfrom non-Black MSM in sex partner meeting places may perpetuate this disparity,\na fact that is ignored by current HIV risk indices, which mainly focus on\nindividual behaviors and not systemic factors. This paper capitalizes on recent\nstudies in which the venues where MSM meet their sex partners are known.\nConnecting individuals and venues leads to so-called affiliation networks; we\npropose a model for how HIV might spread along these networks, and we formulate\na new risk index based on this model. We test this new risk index on an\naffiliation network of 466 African-American MSM in Chicago, and in simulation.\nThe new risk index works well when there are two groups of people, one with\nhigher HIV prevalence than the other, with limited overlap in where they meet\ntheir sex partners.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:25:42 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:22:33 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 16:00:36 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Larson", "Jonathan", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2004.06166", "submitter": "Xiaoke Zhang", "authors": "Rui Miao, Wu Xue, Xiaoke Zhang", "title": "Average Treatment Effect Estimation in Observational Studies with\n  Functional Covariates", "comments": "Section 3.1.1: added discussions and Remark 1.3; Section 3.1.2: added\n  Eq. (5) and related discussions; Sections 5 and 6: added discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is an important area in modern statistics and has\nbeen successfully applied in many fields. Although many scientific studies aim\nto find causations, a predominant majority of functional data analysis\napproaches can only reveal correlations. In this paper, average treatment\neffect estimation is studied for observational data with functional covariates.\nThis paper generalizes various state-of-art propensity score estimation methods\nfor multivariate data to functional data. The resulting average treatment\neffect estimators via propensity score weighting are numerically evaluated by a\nsimulation study and applied to a real-world dataset to study the causal effect\nof duloxitine on the pain relief of chronic knee osteoarthritis patients.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:18:11 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 19:53:41 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Miao", "Rui", ""], ["Xue", "Wu", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "2004.06178", "submitter": "Charles Manski", "authors": "Charles F. Manski and Francesca Molinari", "title": "Estimating the COVID-19 Infection Rate: Anatomy of an Inference Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a consequence of missing data on tests for infection and imperfect\naccuracy of tests, reported rates of population infection by the SARS CoV-2\nvirus are lower than actual rates of infection. Hence, reported rates of severe\nillness conditional on infection are higher than actual rates. Understanding\nthe time path of the COVID-19 pandemic has been hampered by the absence of\nbounds on infection rates that are credible and informative. This paper\nexplains the logical problem of bounding these rates and reports illustrative\nfindings, using data from Illinois, New York, and Italy. We combine the data\nwith assumptions on the infection rate in the untested population and on the\naccuracy of the tests that appear credible in the current context. We find that\nthe infection rate might be substantially higher than reported. We also find\nthat the infection fatality rate in Italy is substantially lower than reported.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:04:34 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Manski", "Charles F.", ""], ["Molinari", "Francesca", ""]]}, {"id": "2004.06266", "submitter": "Zhu Su", "authors": "Zongkai Yang, Zhu Su, Sannyuya Liu, Zhi Liu, Wenxiang Ke and Liang\n  Zhao", "title": "Evolution Features and Behavior Characters of Friendship Networks on\n  Campus Life", "comments": null, "journal-ref": "Expert Systems with Applications, 2020", "doi": "10.1016/j.eswa.2020.113519", "report-no": "Volume 158, 15 November 2020, 113519", "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and mining students' behaviors and interactions from big data is an\nessential part of education data mining. Based on the data of campus smart\ncards, which include not only static demographic information but also dynamic\nbehavioral data from more than 30000 anonymous students, in this paper, the\nevolution features of friendship and the relations between behavior characters\nand student interactions are investigated. On the one hand, four different\nevolving friendship networks are constructed by means of the friend ties\nproposed in this paper, which are extracted from monthly consumption records.\nIn addition, the features of the giant connected components (GCCs) of\nfriendship networks are analyzed via social network analysis (SNA) and\npercolation theory. On the other hand, two high-level behavior characters,\norderliness and diligence, are adopted to analyze their associations with\nstudent interactions. Our experiment/empirical results indicate that the sizes\nof friendship networks have declined with time growth and both the small-world\neffect and power-law degree distribution are found in friendship networks.\nSecond, the results of the assortativity coefficient of both orderliness and\ndiligence verify that there are strong peer effects among students. Finally,\nthe percolation analysis of orderliness on friendship networks shows that a\nphase transition exists, which is enlightening in that swarm intelligence can\nbe realized by intervening the key students near the transition point.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 02:01:27 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Yang", "Zongkai", ""], ["Su", "Zhu", ""], ["Liu", "Sannyuya", ""], ["Liu", "Zhi", ""], ["Ke", "Wenxiang", ""], ["Zhao", "Liang", ""]]}, {"id": "2004.06445", "submitter": "Amir Abdollahi", "authors": "Maryam Rahbaralam, Amir Abdollahi, Daniel Fern\\`andez-Garcia, Xavier\n  Sanchez-Vila", "title": "Stochastic modeling of non-linear adsorption with Gaussian kernel\n  density estimators", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adsorption is a relevant process in many fields, such as product\nmanufacturing or pollution remediation in porous materials. Adsorption takes\nplace at the molecular scale, amenable to be modeled by Lagrangian numerical\nmethods. We have proposed a chemical diffusion-reaction model for the\nsimulation of adsorption, based on the combination of a random walk particle\ntracking method involving the use of Gaussian Kernel Density Estimators. The\nmain feature of the proposed model is that it can effectively reproduce the\nnonlinear behavior characteristic of the Langmuir and Freundlich isotherms. In\nthe former, it is enough to add a finite number of sorption sites of\nhomogeneous sorption properties, and to set the process as the combination of\nthe forward and the backward reactions, each one of them with a prespecified\nreaction rate. To model the Freundlich isotherm instead, typical of low to\nintermediate range of solute concentrations, there is a need to assign a\ndifferent equilibrium constant to each specific sorption site, provided they\nare all drawn from a truncated power-law distribution. Both nonlinear models\ncan be combined in a single framework to obtain a typical observed behavior for\na wide range of concentration values.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 12:21:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Rahbaralam", "Maryam", ""], ["Abdollahi", "Amir", ""], ["Fern\u00e0ndez-Garcia", "Daniel", ""], ["Sanchez-Vila", "Xavier", ""]]}, {"id": "2004.06565", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal, Robert E. Tillman, Prashant Reddy, Manuela Veloso", "title": "Bayesian Consensus: Consensus Estimates from Miscalibrated Instruments\n  under Heteroscedastic Noise", "comments": null, "journal-ref": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness and Privacy", "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of aggregating predictions or measurements from a set\nof human forecasters, models, sensors or other instruments which may be subject\nto bias or miscalibration and random heteroscedastic noise. We propose a\nBayesian consensus estimator that adjusts for miscalibration and noise and show\nthat this estimator is unbiased and asymptotically more efficient than naive\nalternatives. We further propose a Hierarchical Bayesian Model that leverages\nour proposed estimator and apply it to two real world forecasting challenges\nthat require consensus estimates from error prone individual estimates:\nforecasting influenza like illness (ILI) weekly percentages and forecasting\nannual earnings of public companies. We demonstrate that our approach is\neffective at mitigating bias and error and results in more accurate forecasts\nthan existing consensus models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:10:21 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 23:49:43 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nagpal", "Chirag", ""], ["Tillman", "Robert E.", ""], ["Reddy", "Prashant", ""], ["Veloso", "Manuela", ""]]}, {"id": "2004.06582", "submitter": "Sebastian Lerch", "authors": "St\\'ephane Vannitsem, John Bj{\\o}rnar Bremnes, Jonathan Demaeyer,\n  Gavin R. Evans, Jonathan Flowerdew, Stephan Hemri, Sebastian Lerch, Nigel\n  Roberts, Susanne Theis, Aitor Atencia, Zied Ben Bouall\\`egue, Jonas Bhend,\n  Markus Dabernig, Lesley De Cruz, Leila Hieta, Olivier Mestre, Lionel Moret,\n  Iris Odak Plenkovi\\'c, Maurice Schmeits, Maxime Taillardat, Joris Van den\n  Bergh, Bert Van Schaeybroeck, Kirien Whan, Jussi Ylhaisi", "title": "Statistical Postprocessing for Weather Forecasts -- Review, Challenges\n  and Avenues in a Big Data World", "comments": "This work has been submitted to the Bulletin of the American\n  Meteorological Society. Copyright in this work may be transferred without\n  further notice", "journal-ref": null, "doi": "10.1175/BAMS-D-19-0308.1", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical postprocessing techniques are nowadays key components of the\nforecasting suites in many National Meteorological Services (NMS), with for\nmost of them, the objective of correcting the impact of different types of\nerrors on the forecasts. The final aim is to provide optimal, automated,\nseamless forecasts for end users. Many techniques are now flourishing in the\nstatistical, meteorological, climatological, hydrological, and engineering\ncommunities. The methods range in complexity from simple bias corrections to\nvery sophisticated distribution-adjusting techniques that incorporate\ncorrelations among the prognostic variables. The paper is an attempt to\nsummarize the main activities going on this area from theoretical developments\nto operational applications, with a focus on the current challenges and\npotential avenues in the field. Among these challenges is the shift in NMS\ntowards running ensemble Numerical Weather Prediction (NWP) systems at the\nkilometer scale that produce very large datasets and require high-density\nhigh-quality observations; the necessity to preserve space time correlation of\nhigh-dimensional corrected fields; the need to reduce the impact of model\nchanges affecting the parameters of the corrections; the necessity for\ntechniques to merge different types of forecasts and ensembles with different\nbehaviors; and finally the ability to transfer research on statistical\npostprocessing to operations. Potential new avenues will also be discussed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:09:07 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Vannitsem", "St\u00e9phane", ""], ["Bremnes", "John Bj\u00f8rnar", ""], ["Demaeyer", "Jonathan", ""], ["Evans", "Gavin R.", ""], ["Flowerdew", "Jonathan", ""], ["Hemri", "Stephan", ""], ["Lerch", "Sebastian", ""], ["Roberts", "Nigel", ""], ["Theis", "Susanne", ""], ["Atencia", "Aitor", ""], ["Bouall\u00e8gue", "Zied Ben", ""], ["Bhend", "Jonas", ""], ["Dabernig", "Markus", ""], ["De Cruz", "Lesley", ""], ["Hieta", "Leila", ""], ["Mestre", "Olivier", ""], ["Moret", "Lionel", ""], ["Plenkovi\u0107", "Iris Odak", ""], ["Schmeits", "Maurice", ""], ["Taillardat", "Maxime", ""], ["Bergh", "Joris Van den", ""], ["Van Schaeybroeck", "Bert", ""], ["Whan", "Kirien", ""], ["Ylhaisi", "Jussi", ""]]}, {"id": "2004.06633", "submitter": "Chaitanya Poolla", "authors": "Chaitanya Poolla, Abraham K. Ishihara, Dan Liddell, Rodney Martin,\n  Steven Rosenberg", "title": "Occupant Plugload Management for Demand Response in Commercial\n  Buildings: Field Experimentation and Statistical Characterization", "comments": "20 pages, 15 figures, 4 tables, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial buildings account for approximately 36% of US electricity\nconsumption, of which nearly two-thirds is met by fossil fuels [1] resulting in\nan adverse impact on the environment. Reducing this impact requires improving\nenergy efficiency and lowering energy consumption. Most existing studies focus\non designing methods to regulate and reduce HVAC and lighting energy\nconsumption. However, few studies have focused on the control of occupant\nplugload energy consumption. In this study, we conducted multiple experiments\nto analyze changes in occupant plugload energy consumption due to monetary\nincentives and/or feedback. The experiments were performed in government office\nand university buildings at NASA Research Park located in Moffett Field, CA.\nAnalysis of the data reveal significant plugload energy reduction can be\nachieved via feedback and/or incentive mechanisms. Autoregressive models are\nused to predict expected plugload savings in the presence of exogenous\nvariables. The results of this study suggest that occupant-in-the-loop control\narchitectures have the potential to reduce energy consumption and hence lower\nthe carbon footprint of commercial buildings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:23:34 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 21:57:47 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 02:07:59 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 15:01:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Poolla", "Chaitanya", ""], ["Ishihara", "Abraham K.", ""], ["Liddell", "Dan", ""], ["Martin", "Rodney", ""], ["Rosenberg", "Steven", ""]]}, {"id": "2004.06764", "submitter": "Thais Fonseca Dr", "authors": "Martine J. Barons, Thais C. O. Fonseca, Andy Davis and Jim Q. Smith", "title": "A decision support system for addressing food security in the UK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an integrating decision support system to model food\nsecurity in the UK. In ever-larger dynamic systems, such as the food system, it\nis increasingly difficult for decision-makers to effectively account for all\nthe variables within the system that may influence the outcomes of interest\nunder enactments of various candidate policies. Each of the influencing\nvariables is likely, themselves, to be dynamic sub-systems with expert domains\nsupported by sophisticated probabilistic models. Recent increases in food\npoverty the UK raised the questions about the main drivers to food insecurity,\nhow this may be changing over time and how evidence can be used in evaluating\npolicy for decision support. In this context, an integrating decision support\nsystem is proposed for household food security to allow decision-makers to\ncompare several candidate policies which may affect the outcome of food\ninsecurity at household level.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 19:26:07 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Barons", "Martine J.", ""], ["Fonseca", "Thais C. O.", ""], ["Davis", "Andy", ""], ["Smith", "Jim Q.", ""]]}, {"id": "2004.06880", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Phuong Anh Vu, and Bernard Wong", "title": "A multivariate evolutionary generalised linear model framework with\n  adaptive estimation for claims reserving", "comments": "Accepted for publication in Insurance: Mathematics and Economics", "journal-ref": "Insurance: Mathematics and Economics, Volume 93, July 2020, Pages\n  50-71", "doi": "10.1016/j.insmatheco.2020.04.007", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a multivariate evolutionary generalised linear\nmodel (GLM) framework for claims reserving, which allows for dynamic features\nof claims activity in conjunction with dependency across business lines to\naccurately assess claims reserves. We extend the traditional GLM reserving\nframework on two fronts: GLM fixed factors are allowed to evolve in a recursive\nmanner, and dependence is incorporated in the specification of these factors\nusing a common shock approach.\n  We consider factors that evolve across accident years in conjunction with\nfactors that evolve across calendar years. This two-dimensional evolution of\nfactors is unconventional as a traditional evolutionary model typically\nconsiders the evolution in one single time dimension. This creates challenges\nfor the estimation process, which we tackle in this paper. We develop the\nformulation of a particle filtering algorithm with parameter learning\nprocedure. This is an adaptive estimation approach which updates evolving\nfactors of the framework recursively over time.\n  We implement and illustrate our model with a simulated data set, as well as a\nset of real data from a Canadian insurer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:49:26 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Vu", "Phuong Anh", ""], ["Wong", "Bernard", ""]]}, {"id": "2004.06958", "submitter": "Azam Yazdani", "authors": "Azam Yazdani", "title": "Mendelian randomization and causal networks for systematic analysis of\n  omics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization implemented through instrumental variable analysis is\nfrequently discussed in causality and recently the number of applications on\nreal data is increasing. However, there are very few discussions to address\nmodern biomedical questions such as the integration of large scale omics in\ncausality. While in the age of large omics, we face several hundred or\nthousands of components with little knowledge about the underlying structures,\nthe focus of the field is on small scales and mostly with known structures. The\navailability of large omic data accentuates the need for techniques to identify\ninterconnectivity among the omic components and reveal the principles that\ngovern the relationships. This study extends instrumental variable techniques\nto identify causal networks in large scales and assess the assumptions.\nLarge-scale causal networks are complex and further analyses are required to\nuncover mechanisms by which the components are related within and between omics\nand linked to disease endpoints. This study will review these utilities of\ncausal networks for mechanistic understanding.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:08:35 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yazdani", "Azam", ""]]}, {"id": "2004.07383", "submitter": "Brian Lucena", "authors": "Brian Lucena", "title": "Exploiting Categorical Structure Using Tree-Based Methods", "comments": "To appear in AISTATS 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods of using categorical variables as predictors either endow\nthem with an ordinal structure or assume they have no structure at all.\nHowever, categorical variables often possess structure that is more complicated\nthan a linear ordering can capture. We develop a mathematical framework for\nrepresenting the structure of categorical variables and show how to generalize\ndecision trees to make use of this structure. This approach is applicable to\nmethods such as Gradient Boosted Trees which use a decision tree as the\nunderlying learner. We show results on weather data to demonstrate the\nimprovement yielded by this approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 22:58:27 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lucena", "Brian", ""]]}, {"id": "2004.07454", "submitter": "Juan C. S.Herrera", "authors": "Juan C. S. Herrera", "title": "Sustainable Recipes. A Food Recipe Sourcing and Recommendation System to\n  Minimize Food Miles", "comments": "9 pages, 3 figures, 1 table, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sustainable Recipes is a tool that (1) connects food recipes ingredient lists\nwith the closest organic providers to minimize the distance that food travels\nfrom farm to food preparation site and (2) recommends recipes given a GPS\ncoordinate to minimize food miles. Sustainable Recipes provides consumers,\nentrepreneurs, cooking enthusiasts, and restauranteurs in the United States and\nelsewhere with an easy to use interface to help them (1) connect with organic\ningredient producers to source ingredients to produce food recipes minimizing\nfood miles and (2) recommend recipes using locally grown food. The main\nacademic contribution of Sustainable Recipes is to bridge the gap between two\nstreams of literature in data science of food recipes: studies of food recipes\nand studies of food supply chains. The outcomes of the interphase are (1) a map\nvisualization that highlights the location of the producers that can supply the\ningredients for a food recipe along with a ticket consisting of their contact\naddresses and the food miles used to produce a recipe and (2) a list of recipes\nthat minimize food miles for a given GPS coordinate in which the recipe is\ngoing to be produced.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 04:42:42 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Herrera", "Juan C. S.", ""]]}, {"id": "2004.07503", "submitter": "Johannes Breidenbach", "authors": "Johannes Breidenbach, Lars T. Waser, Misganu Debella-Gilo, Johannes\n  Schumacher, Johannes Rahlf, Marius Hauglin, Stefano Puliti, Rasmus Astrup", "title": "National mapping and estimation of forest area by dominant tree species\n  using Sentinel-2 data", "comments": "minor changes compared to submitted article; 34 pages, 13 figures, 11\n  tables", "journal-ref": null, "doi": "10.1139/cjfr-2020-0170", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nation-wide Sentinel-2 mosaics were used with National Forest Inventory (NFI)\ndata for modelling and subsequent mapping of spruce, pine and deciduous forest\nin Norway in 16 m $\\times$ 16 m resolution. The accuracies of the best model\nranged between 74% for spruce and 87% for deciduous forest. An overall accuracy\nof 90% was found on stand level using independent data from more than 42.000\nstands. Errors mostly resulting from a forest mask reduced the model accuracies\nby approximately 10%. Nonetheless, efficiencies of national forest area\nestimates increased by 20% to 50% for model-assisted (MA) estimates and up to\n90% for poststratification (PS). Greater minimum number of observations,\nhowever, constrained the use of PS. For estimates of sub-populations\n(municipalities), efficiencies improved by up to a factor of 8. However, even\nfor municipalities with a decent number of NFI plots, direct NFI estimates were\nsometimes more precise than MA estimates. PS estimates were always more precise\nthan direct and MA estimates but were applicable in fewer municipalities. The\ntree species prediction map is freely available as part of the Norwegian forest\nresource map and is used, among others, to improve maps of other variables of\ninterest such as timber volume and biomass.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:49:43 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Breidenbach", "Johannes", ""], ["Waser", "Lars T.", ""], ["Debella-Gilo", "Misganu", ""], ["Schumacher", "Johannes", ""], ["Rahlf", "Johannes", ""], ["Hauglin", "Marius", ""], ["Puliti", "Stefano", ""], ["Astrup", "Rasmus", ""]]}, {"id": "2004.07542", "submitter": "Katrin Madjar", "authors": "Katrin Madjar, Manuela Zucknick, Katja Ickstadt, and J\\\"org\n  Rahnenf\\\"uhrer", "title": "Combining heterogeneous subgroups with graph-structured variable\n  selection priors for Cox regression", "comments": "under review, 19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important objectives in cancer research are the prediction of a patient's\nrisk based on molecular measurements such as gene expression data and the\nidentification of new prognostic biomarkers (e.g. genes). In clinical practice,\nthis is often challenging because patient cohorts are typically small and can\nbe heterogeneous. In classical subgroup analysis, a separate prediction model\nis fitted using only the data of one specific cohort. However, this can lead to\na loss of power when the sample size is small. Simple pooling of all cohorts,\non the other hand, can lead to biased results, especially when the cohorts are\nheterogeneous. For this situation, we propose a new Bayesian approach suitable\nfor continuous molecular measurements and survival outcome that identifies the\nimportant predictors and provides a separate risk prediction model for each\ncohort. It allows sharing information between cohorts to increase power by\nassuming a graph linking predictors within and across different cohorts. The\ngraph helps to identify pathways of functionally related genes and genes that\nare simultaneously prognostic in different cohorts. Results demonstrate that\nour proposed approach is superior to the standard approaches in terms of\nprediction performance and increased power in variable selection when the\nsample size is small.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:13:08 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Madjar", "Katrin", ""], ["Zucknick", "Manuela", ""], ["Ickstadt", "Katja", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""]]}, {"id": "2004.07583", "submitter": "Edward Wheatcroft", "authors": "Edward Wheatcroft", "title": "Assessing the Significance of Model Selection in Ecology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model Selection is a key part of many ecological studies, with Akaike's\nInformation Criterion the most commonly used technique. Typically, a number of\ncandidate models are defined a priori and ranked according to their expected\nout-of-sample performance. Model selection, however, only assesses the relative\nperformance of the models and, as pointed out in a recent paper, a large\nproportion of ecology papers that use model selection do not assess the\nabsolute fit of the `best' model. In this paper, it is argued that assessing\nthe absolute fit of the `best' model alone does not go far enough. This is\nbecause a model that appears to perform well under model selection is also\nlikely to appear to perform well under measures of absolute fit, even when\nthere is no predictive value. A model selection permutation test is proposed\nthat assesses the probability that the model selection statistic of the `best'\nmodel could have occurred by chance alone, whilst taking account of\ndependencies between the models. It is argued that this test should always be\nperformed before formal model selection takes place. The test is demonstrated\non two real population modelling examples of ibex in northern Italy and wild\nreindeer in Norway.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:46:46 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Wheatcroft", "Edward", ""]]}, {"id": "2004.07674", "submitter": "Viet Chi Tran", "authors": "Viet Chi Tran", "title": "Stochastic epidemics in a heterogeneous community (Part III of the book\n  Stochastic Epidemic Models and Inference)", "comments": "Lecture notes of a course given in a CIMPA school in Ziguinchor (Dec.\n  2015)", "journal-ref": "Lecture Notes in Mathematics, Vol. 2255. Springer. 2019", "doi": "10.1007/978-3-030-30900-8", "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the Part III of the book 'Stochastic Epidemic Models with\nInference' edited by Tom Britton and Etienne Pardoux. Plan of the chapter: 1)\nRandom Graphs, 2) The Reproduction Number R0, 3) SIR Epidemics on Configuration\nModel Graphs, 4) Statistical Description of Epidemics Spreading on Networks:\nThe Case of Cuban HIV.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:18:03 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Tran", "Viet Chi", ""]]}, {"id": "2004.07688", "submitter": "Viet Chi Tran", "authors": "Catherine Lar\\'edo, Viet Chi Tran (for Chapter 4)", "title": "Statistical inference for epidemic processes in a homogeneous community\n  (Part IV of the book Stochastic Epidemic Models and Inference)", "comments": null, "journal-ref": "Lecture Notes in Mathematics, Vol. 2255. Springer. 2019", "doi": "10.1007/978-3-030-30900-8", "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the Part IV of the book 'Stochastic Epidemic Models with\nInference' edited by Tom Britton and Etienne Pardoux. It is written by\nCatherine Lar\\'edo, with the contribution of Viet Chi Tran for the Chapter 4.\nEpidemic data present challenging statistical problems, starting from the\nrecurrent issue of handling missing information. We review methods such as\nMCMC, ABC or methods based on diffusion approximations. Plan of this document:\n1) Observations and Asymptotic Frameworks; 2) Inference for Markov Chain\nEpidemic Models; 3) Inference Based on the Diffusion Approximation of Epidemic\nModels; 4) Inference for Continuous Time SIR models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:54:32 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lar\u00e9do", "Catherine", "", "for Chapter 4"], ["Tran", "Viet Chi", "", "for Chapter 4"]]}, {"id": "2004.07696", "submitter": "Guy Nason Prof.", "authors": "Guy P. Nason", "title": "Rapidly evaluating lockdown strategies using spectral analysis: the\n  cycles behind new daily COVID-19 cases and what happens after lockdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral analysis characterises oscillatory time series behaviours such as\ncycles, but accurate estimation requires reasonable numbers of observations.\nCurrent COVID-19 time series for many countries are short: pre- and\npost-lockdown series are shorter still. Accurate estimation of potentially\ninteresting cycles within such series seems beyond reach. We solve the problem\nof obtaining accurate estimates from short time series by using recent Bayesian\nspectral fusion methods. Here we show that transformed new daily COVID-19 cases\nfor many countries generally contain three cycles operating at wavelengths of\naround 2.7, 4.1 and 6.7 days (weekly). We show that the shorter cycles are\nsuppressed after lockdown. The pre- and post lockdown differences suggest that\nthe weekly effect is at least partly due to non-epidemic factors, whereas the\ntwo shorter cycles seem intrinsic to the epidemic. Unconstrained, new cases\ngrow exponentially, but the internal cyclic structure causes periodic falls in\ncases. This suggests that lockdown success might only be indicated by four or\nmore daily falls in cases. Spectral learning for epidemic time series\ncontributes to the understanding of the epidemic process, helping evaluate\ninterventions and assists with forecasting. Spectral fusion is a general\ntechnique that is able to fuse spectra recorded at different sampling rates,\nwhich can be applied to a wide range of time series from many disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:09:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Nason", "Guy P.", ""]]}, {"id": "2004.07742", "submitter": "Rocco Mazza", "authors": "Emma Zavarrone, Maria Gabriella Grassia, Marina Marino, Rasanna\n  Cataldo, Rocco Mazza, Nicola Canestrari", "title": "CO.ME.T.A. -- covid-19 media textual analysis. A dashboard for media\n  monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is to trace how mass media, particularly newspapers,\nhave addressed the issues about the containment of contagion or the explanation\nof epidemiological evolution. We propose an interactive dashboard: CO.ME.T.A..\nDuring crises it is important to shape the best communication strategies in\norder to respond to critical situations. In this regard, it is important to\nmonitor the information that mass media and social platforms convey. The\ndashboard allows to explore the mining of contents extracted and study the\nlexical structure that links the main discussion topics. The dashboard merges\ntogether four methods: text mining, sentiment analysis, textual network\nanalysis and latent topic models. Results obtained on a subset of documents\nshow not only a health-related semantic dimension, but it also extends to\nsocial-economic dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:24:56 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zavarrone", "Emma", ""], ["Grassia", "Maria Gabriella", ""], ["Marino", "Marina", ""], ["Cataldo", "Rasanna", ""], ["Mazza", "Rocco", ""], ["Canestrari", "Nicola", ""]]}, {"id": "2004.07743", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Nianqiao Ju, Sergio Bacallado, Rajen D. Shah", "title": "BETS: The dangers of selection bias in early analyses of the coronavirus\n  disease (COVID-19) pandemic", "comments": "33 pages, 8 figures, 5 tables; Accepted for publication in The Annals\n  of Applied Statistics on 24th September, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) has quickly grown from a regional\noutbreak in Wuhan, China to a global pandemic. Early estimates of the epidemic\ngrowth and incubation period of COVID-19 may have been biased due to sample\nselection. Using detailed case reports from 14 locations in and outside\nmainland China, we obtained 378 Wuhan-exported cases who left Wuhan before an\nabrupt travel quarantine. We developed a generative model we call BETS for four\nkey epidemiological events---Beginning of exposure, End of exposure, time of\nTransmission, and time of Symptom onset (BETS)---and derived explicit formulas\nto correct for the sample selection. We gave a detailed illustration of why\nsome early and highly influential analyses of the COVID-19 pandemic were\nseverely biased. All our analyses, regardless of which subsample and model were\nbeing used, point to an epidemic doubling time of 2 to 2.5 days during the\nearly outbreak in Wuhan. A Bayesian nonparametric analysis further suggests\nthat about 5% of the symptomatic cases may not develop symptoms within 14 days\nof infection and that men may be much more likely than women to develop\nsymptoms within 2 days of infection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:27:24 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 17:16:17 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 10:39:14 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 10:34:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Ju", "Nianqiao", ""], ["Bacallado", "Sergio", ""], ["Shah", "Rajen D.", ""]]}, {"id": "2004.07887", "submitter": "Jordan Bryan", "authors": "Jordan G. Bryan and Peter D. Hoff", "title": "Smaller $p$-values in genomics studies using distilled historical\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical research institutions have generated massive amounts of biological\ndata by genetically profiling hundreds of cancer cell lines. In parallel,\nacademic biology labs have conducted genetic screens on small numbers of cancer\ncell lines under custom experimental conditions. In order to share information\nbetween these two approaches to scientific discovery, this article proposes a\n\"frequentist assisted by Bayes\" (FAB) procedure for hypothesis testing that\nallows historical information from massive genomics datasets to increase the\npower of hypothesis tests in specialized studies. The exchange of information\ntakes place through a novel probability model for multimodal genomics data,\nwhich distills historical information pertaining to cancer cell lines and genes\nacross a wide variety of experimental contexts. If the relevance of the\nhistorical information for a given study is high, then the resulting FAB tests\ncan be more powerful than the corresponding classical tests. If the relevance\nis low, then the FAB tests yield as many discoveries as the classical tests.\nSimulations and practical investigations demonstrate that the FAB testing\nprocedure can increase the number of effects discovered in genomics studies\nwhile still maintaining strict control of type I error and false discovery\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:21:09 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bryan", "Jordan G.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "2004.07947", "submitter": "Viktor Stojkoski MSc", "authors": "Viktor Stojkoski, Zoran Utkovski, Petar Jolakoski, Dragan Tevdovski\n  and Ljupco Kocarev", "title": "The socio-economic determinants of the coronavirus disease (COVID-19)\n  pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the biological and epidemiological factors, a multitude of social and\neconomic criteria also govern the extent of the coronavirus disease spread\nwithin a population. Consequently, there is an active debate regarding the\ncritical socio-economic determinants that contribute to the impact of the\nresulting pandemic. Here, we leverage Bayesian model averaging techniques and\ncountry level data to investigate the potential of 31 determinants, describing\na diverse set of socio-economic characteristics, in explaining the outcome of\nthe first wave of the coronavirus pandemic. We show that the true empirical\nmodel behind the coronavirus outcome is constituted only of few determinants.\nTo understand the relationship between the potential determinants in the\nspecification of the true model, we develop the coronavirus determinants\nJointness space. The extent to which each determinant is able to provide a\ncredible explanation varies between countries due to their heterogeneous\nsocio-economic characteristics. In this aspect, the obtained Jointness map acts\nas a bridge between theoretical investigations and empirical observations and\noffers an alternate view for the joint importance of the socio-economic\ndeterminants when used for developing policies aimed at preventing future\nepidemic crises.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 19:12:57 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:02:46 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 18:56:38 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 19:30:31 GMT"}, {"version": "v5", "created": "Mon, 11 May 2020 21:49:03 GMT"}, {"version": "v6", "created": "Sat, 16 May 2020 21:51:50 GMT"}, {"version": "v7", "created": "Thu, 29 Oct 2020 22:35:11 GMT"}, {"version": "v8", "created": "Sun, 15 Nov 2020 18:50:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Stojkoski", "Viktor", ""], ["Utkovski", "Zoran", ""], ["Jolakoski", "Petar", ""], ["Tevdovski", "Dragan", ""], ["Kocarev", "Ljupco", ""]]}, {"id": "2004.07977", "submitter": "Marcelo Medeiros", "authors": "Marcelo Medeiros, Alexandre Street, Davi Vallad\\~ao, Gabriel\n  Vasconcelos, Eduardo Zilberman", "title": "Short-Term Covid-19 Forecast for Latecomers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of Covid-19 cases is increasing dramatically worldwide. Therefore,\nthe availability of reliable forecasts for the number of cases in the coming\ndays is of fundamental importance. We propose a simple statistical method for\nshort-term real-time forecasting of the number of Covid-19 cases and fatalities\nin countries that are latecomers -- i.e., countries where cases of the disease\nstarted to appear some time after others. In particular, we propose a penalized\n(LASSO) regression with an error correction mechanism to construct a model of a\nlatecomer in terms of the other countries that were at a similar stage of the\npandemic some days before. By tracking the number of cases and deaths in those\ncountries, we forecast through an adaptive rolling-window scheme the number of\ncases and deaths in the latecomer. We apply this methodology to Brazil, and\nshow that (so far) it has been performing very well. These forecasts aim to\nfoster a better short-run management of the health system capacity.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 22:09:14 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:30:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Medeiros", "Marcelo", ""], ["Street", "Alexandre", ""], ["Vallad\u00e3o", "Davi", ""], ["Vasconcelos", "Gabriel", ""], ["Zilberman", "Eduardo", ""]]}, {"id": "2004.08032", "submitter": "Emiliano Valdez", "authors": "Himchan Jeong and Hyunwoong Chang and Emiliano A. Valdez", "title": "A non-convex regularization approach for stable estimation of loss\n  development factors", "comments": "23 pages, 11 Tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we apply non-convex regularization methods in order to\nobtain stable estimation of loss development factors in insurance claims\nreserving. Among the non-convex regularization methods, we focus on the use of\nthe log-adjusted absolute deviation (LAAD) penalty and provide discussion on\noptimization of LAAD penalized regression model, which we prove to converge\nwith a coordinate descent algorithm under mild conditions. This has the\nadvantage of obtaining a consistent estimator for the regression coefficients\nwhile allowing for the variable selection, which is linked to the stable\nestimation of loss development factors. We calibrate our proposed model using a\nmulti-line insurance dataset from a property and casualty insurer where we\nobserved reported aggregate loss along accident years and development periods.\nWhen compared to other regression models, our LAAD penalized regression model\nprovides very promising results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:05:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 13:21:55 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 02:53:24 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Jeong", "Himchan", ""], ["Chang", "Hyunwoong", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2004.08115", "submitter": "Meixia Lin", "authors": "Meixia Lin, Defeng Sun, Kim-Chuan Toh, Chengjing Wang", "title": "Estimation of sparse Gaussian graphical models with hidden clustering\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Gaussian graphical models is important in natural science when\nmodeling the statistical relationships between variables in the form of a\ngraph. The sparsity and clustering structure of the concentration matrix is\nenforced to reduce model complexity and describe inherent regularities. We\npropose a model to estimate the sparse Gaussian graphical models with hidden\nclustering structure, which also allows additional linear constraints to be\nimposed on the concentration matrix. We design an efficient two-phase algorithm\nfor solving the proposed model. We develop a symmetric Gauss-Seidel based\nalternating direction method of the multipliers (sGS-ADMM) to generate an\ninitial point to warm-start the second phase algorithm, which is a proximal\naugmented Lagrangian method (pALM), to get a solution with high accuracy.\nNumerical experiments on both synthetic data and real data demonstrate the good\nperformance of our model, as well as the efficiency and robustness of our\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:43:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lin", "Meixia", ""], ["Sun", "Defeng", ""], ["Toh", "Kim-Chuan", ""], ["Wang", "Chengjing", ""]]}, {"id": "2004.08309", "submitter": "Antik Chakraborty", "authors": "Antik Chakraborty, Otso Ovaskainen, David B. Dunson", "title": "Bayesian semiparametric long memory models for discretized event data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of semiparametric latent variable models for long\nmemory discretized event data. The proposed methodology is motivated by a study\nof bird vocalizations in the Amazon rain forest; the timings of vocalizations\nexhibit self-similarity and long range dependence ruling out models based on\nPoisson processes. The proposed class of FRActional Probit (FRAP) models is\nbased on thresholding of a latent process consisting of an additive expansion\nof a smooth Gaussian process with a fractional Brownian motion. We develop a\nBayesian approach to inference using Markov chain Monte Carlo, and show good\nperformance in simulation studies. Applying the methods to the Amazon bird\nvocalization data, we find substantial evidence for self-similarity and\nnon-Markovian/Poisson dynamics. To accommodate the bird vocalization data, in\nwhich there are many different species of birds exhibiting their own\nvocalization dynamics, a hierarchical expansion of FRAP is provided in\nSupplementary Materials.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 15:42:27 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 23:16:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chakraborty", "Antik", ""], ["Ovaskainen", "Otso", ""], ["Dunson", "David B.", ""]]}, {"id": "2004.08312", "submitter": "Magali Champion", "authors": "Magali Champion (MAP5), Julien Chiquet (MIA-Paris), Pierre Neuvial\n  (IMT, LaMME), Mohamed Elati (ISSB), Fran\\c{c}ois Radvanyi (PSL), Etienne\n  Birmel\\'e (MAP5)", "title": "Identification of deregulated transcription factors involved in subtypes\n  of cancers", "comments": null, "journal-ref": "Proceedings of the 12th International Conference on Bioinformatics\n  and Computational Biology, vol 70, pages 1--10", "doi": "10.29007/v7qj", "report-no": null, "categories": "q-bio.MN q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for the identification of transcription factors\ninvolved in the deregulation of genes in tumoral cells. This strategy is based\non the inference of a reference gene regulatory network that connects\ntranscription factors to their downstream targets using gene expression data.\nThe behavior of genes in tumor samples is then carefully compared to this\nnetwork of reference to detect deregulated target genes. A linear model is\nfinally used to measure the ability of each transcription factor to explain\nthose deregulations. We assess the performance of our method by numerical\nexperiments on a breast cancer data set. We show that the information about\nderegulation is complementary to the expression data as the combination of the\ntwo improves the supervised classification performance of samples into cancer\nsubtypes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 15:46:00 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Champion", "Magali", "", "MAP5"], ["Chiquet", "Julien", "", "MIA-Paris"], ["Neuvial", "Pierre", "", "IMT, LaMME"], ["Elati", "Mohamed", "", "ISSB"], ["Radvanyi", "Fran\u00e7ois", "", "PSL"], ["Birmel\u00e9", "Etienne", "", "MAP5"]]}, {"id": "2004.08416", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa, Markus {\\AA}dahl, Patrik Ryd\\'en, Ottmar Cronie", "title": "Large-scale modelling and forecasting of ambulance calls in northern\n  Sweden using spatio-temporal log-Gaussian Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although ambulance call data typically come in the form of spatio-temporal\npoint patterns, point process-based modelling approaches presented in the\nliterature are scarce. In this paper, we study a unique set of Swedish\nspatio-temporal ambulance call data, which consist of the spatial (GPS)\nlocations of the calls (within the four northernmost regions of Sweden) and the\nassociated days of occurrence of the calls (January 1, 2014, to December 31,\n2018). Motivated by the nature of the data, we here employ log-Gaussian Cox\nprocesses (LGCPs) for the spatio-temporal modelling and forecasting of the\ncalls. To this end, we propose a K-means clustering based bandwidth selection\nmethod for the kernel estimation of the spatial component of the separable\nspatio-temporal intensity function. The temporal component of the intensity\nfunction is modelled using Poisson regression, using different calendar\ncovariates, and the spatio-temporal random field component of the random\nintensity of the LGCP is fitted using the Metropolis-adjusted Langevin\nalgorithm. Spatial hot-spots have been found in the south-eastern part of the\nstudy region, where most people in the region live and our fitted\nmodel/forecasts manage to capture this behavior quite well. Also, there is a\nsignificant association between the expected number of calls and the\nday-of-the-week and the season-of-the-year. A non-parametric second-order\nanalysis indicates that LGCPs seem to be reasonable models for the data.\nFinally, we find that the fitted forecasts generate simulated future spatial\nevent patterns that quite well resemble the actual future data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 18:12:32 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 19:18:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["\u00c5dahl", "Markus", ""], ["Ryd\u00e9n", "Patrik", ""], ["Cronie", "Ottmar", ""]]}, {"id": "2004.08460", "submitter": "Stefano M. Iacus", "authors": "Stefano Maria Iacus, Fabrizio Natale, Carlos Satamaria, Spyridon\n  Spyratos, and Michele Vespe", "title": "Estimating and Projecting Air Passenger Traffic during the COVID-19\n  Coronavirus Outbreak and its Socio-Economic Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main focus of this study is to collect and prepare data on air passengers\ntraffic worldwide with the scope of analyze the impact of travel ban on the\naviation sector. Based on historical data from January 2010 till October 2019,\na forecasting model is implemented in order to set a reference baseline. Making\nuse of airplane movements extracted from online flight tracking platforms and\non-line booking systems, this study presents also a first assessment of recent\nchanges in flight activity around the world as a result of the COVID-19\npandemic. To study the effects of air travel ban on aviation and in turn its\nsocio-economic, several scenarios are constructed based on past pandemic crisis\nand the observed flight volumes. It turns out that, according to this\nhypothetical scenarios, in the first Quarter of 2020 the impact of aviation\nlosses could have negatively reduced World GDP by 0.02% to 0.12% according to\nthe observed data and, in the worst case scenarios, at the end of 2020 the loss\ncould be as high as 1.41-1.67% and job losses may reach the value of 25-30\nmillions. Focusing on EU27, the GDP loss may amount to 1.66-1.98% by the end of\n2020 and the number of job losses from 4.2 to 5 millions in the worst case\nscenarios. Some countries will be more affected than others in the short run\nand most European airlines companies will suffer from the travel ban.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 21:40:31 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 22:15:54 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Iacus", "Stefano Maria", ""], ["Natale", "Fabrizio", ""], ["Satamaria", "Carlos", ""], ["Spyratos", "Spyridon", ""], ["Vespe", "Michele", ""]]}, {"id": "2004.08510", "submitter": "Andrew Ying", "authors": "Andrew Ying, Ronghui Xu, Christina D. Chambers, Kenneth Lyons Jones", "title": "Causal Effects of Prenatal Drug Exposure on Birth Defects with Missing\n  by Terathanasia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the causal effects of drug exposure on birth defects,\nmotivated by a recent cohort study of birth outcomes in pregnancies of women\ntreated with a given medication, that revealed a higher rate of major\nstructural birth defects in infants born to exposed versus unexposed women. An\noutstanding problem in this study was the missing birth defect outcomes among\npregnancy losses resulting from spontaneous abortion. This led to missing not\nat random because, according to the theory of \"terathanasia\", a defected fetus\nis more likely to be spontaneously aborted. In addition, the previous analysis\nstratified on live birth against spontaneous abortion, which was itself a\npost-exposure variable and hence did not lead to causal interpretation of the\nstratified results. In this paper we aimed to estimate and provide inference\nfor the causal parameters of scientific interest, including the principal\neffects, making use of the missing data mechanism informed by terathanasia.\nDuring this process we also dealt with complications in the data including left\ntruncation, observational nature, and rare events. We report our findings which\nshed light on how studies on causal effects of medication or other exposures\nduring pregnancy may be analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 02:38:06 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ying", "Andrew", ""], ["Xu", "Ronghui", ""], ["Chambers", "Christina D.", ""], ["Jones", "Kenneth Lyons", ""]]}, {"id": "2004.08557", "submitter": "Thomas Hotz", "authors": "Thomas Hotz, Matthias Glock, Stefan Heyder, Sebastian Semper, Anne\n  B\\\"ohle, Alexander Kr\\\"amer", "title": "Monitoring the spread of COVID-19 by estimating reproduction numbers\n  over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To control the current outbreak of the Coronavirus Disease 2019, constant\nmonitoring of the epidemic is required since, as of today, no vaccines or\nantiviral drugs against it are known. We provide daily updated estimates of the\nreproduction number over time at\nhttps://stochastik-tu-ilmenau.github.io/COVID-19/. In this document, we\ndescribe the estimator we are using which was developed in (Fraser 2007),\nderive its asymptotic properties, and we give details on its implementation.\nFurthermore, we validate the estimator on simulated data, demonstrate that\nestimates on real data lead to plausible results, and perform a sensitivity\nanalysis. Finally, we discuss why the estimates obtained need to be interpreted\nwith care.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:39:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hotz", "Thomas", ""], ["Glock", "Matthias", ""], ["Heyder", "Stefan", ""], ["Semper", "Sebastian", ""], ["B\u00f6hle", "Anne", ""], ["Kr\u00e4mer", "Alexander", ""]]}, {"id": "2004.08561", "submitter": "Mark Balenzuela", "authors": "Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, and Brett\n  Ninness", "title": "A New Smoothing Algorithm for Jump Markov Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for calculating the smoothed state distribution\nfor Jump Markov Linear Systems. More specifically, the paper details a novel\ntwo-filter smoother that provides closed-form expressions for the smoothed\nhybrid state distribution. This distribution can be expressed as a Gaussian\nmixture with a known, but exponentially increasing, number of Gaussian\ncomponents as the time index increases. This is accompanied by exponential\ngrowth in memory and computational requirements, which rapidly becomes\nintractable. To ameliorate this, we limit the number of allowed mixture terms\nby employing a Gaussian mixture reduction strategy, which results in a\ncomputationally tractable, but approximate smoothed distribution. The\napproximation error can be balanced against computational complexity in order\nto provide an accurate and practical smoothing algorithm that compares\nfavourably to existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:52:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Balenzuela", "Mark P.", ""], ["Wills", "Adrian G.", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "2004.08564", "submitter": "Mark Balenzuela", "authors": "Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, and Brett\n  Ninness", "title": "A Variational Expectation-Maximisation Algorithm for Learning Jump\n  Markov Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump Markov linear systems (JMLS) are a useful class which can be used to\nmodel processes which exhibit random changes in behavior during operation. This\npaper presents a numerically stable method for learning the parameters of jump\nMarkov linear systems using the expectation-maximisation (EM) approach. The\nsolution provided herein is a deterministic algorithm, and is not a Monte Carlo\nbased technique. As a result, simulations show that when compared to\nalternative approaches, a more likely set of system parameters can be found\nwithin a fixed computation time, which better explain the observations of the\nsystem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 09:04:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Balenzuela", "Mark P.", ""], ["Wills", "Adrian G.", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "2004.08565", "submitter": "Mark Balenzuela", "authors": "Mark P. Balenzuela, Adrian G. Wills, Christopher Renton, and Brett\n  Ninness", "title": "Bayesian Parameter Identification for Jump Markov Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian method for identification of jump Markov\nlinear system parameters. A primary motivation is to provide accurate\nquantification of parameter uncertainty without relying on asymptotic in\ndata-length arguments. To achieve this, the paper details a particle-Gibbs\nsampling approach that provides samples from the desired posterior\ndistribution. These samples are produced by utilising a modified discrete\nparticle filter and carefully chosen conjugate priors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 09:13:38 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:03:01 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Balenzuela", "Mark P.", ""], ["Wills", "Adrian G.", ""], ["Renton", "Christopher", ""], ["Ninness", "Brett", ""]]}, {"id": "2004.08584", "submitter": "Francesca Azzolini", "authors": "Geir D. Berentsen, Francesca Azzolini, Hans J. Skaug, Rolv T. Lie,\n  H{\\aa}kon K. Gjessing", "title": "Heritability curves: a local measure of heritability", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new measure of heritability which relaxes the\nclassical assumption that the degree of heritability of a continuous trait can\nbe summarized by a single number.This measure can be used in situations where\nthe trait dependence structure between family members is nonlinear, in which\ncase traditional mixed effects models and covariance (correlation) based\nmethods are inadequate. Our idea is to combine the notion of a correlation\ncurve with traditional correlation based measures of heritability, such as the\nformula of Falconer. For estimation purposes, we use a multivariate Gaussian\nmixture, which is able to capture non-linear dependence and respects certain\ndistributional constraints. We derive an analytical expression for the\nassociated correlation curve, and investigate its limiting behaviour when the\ntrait value becomes either large or small. The result is a measure of\nheritability that varies with the trait value. When applied to birth weight\ndata on Norwegian mother father child trios, the conclusion is that low and\nhigh birth weight are less heritable traits than medium birth weight. On the\nother hand, we find no similar heterogeneity in the heritability of Body Mass\nIndex (BMI) when studying monozygotic and dizygotic twins.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:37:16 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Berentsen", "Geir D.", ""], ["Azzolini", "Francesca", ""], ["Skaug", "Hans J.", ""], ["Lie", "Rolv T.", ""], ["Gjessing", "H\u00e5kon K.", ""]]}, {"id": "2004.08667", "submitter": "Matheus Guerrero", "authors": "Matheus B. Guerrero and Wagner Barreto-Souza and Hernando Ombao", "title": "Integer-valued autoregressive process with flexible marginal and\n  innovation distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  INteger Auto-Regressive (INAR) processes are usually defined by specifying\nthe innovations and the operator, which often leads to difficulties in deriving\nmarginal properties of the process. In many practical situations, a major\nmodeling limitation is that it is difficult to justify the choice of the\noperator. To overcome these drawbacks, we propose a new flexible approach to\nbuild an INAR model: we pre-specify the marginal and innovation distributions.\nHence, the operator is a consequence of specifying the desired marginal and\ninnovation distributions. Our new INAR model has both marginal and innovations\ngeometric distributed, being a direct alternative to the classical Poisson INAR\nmodel. Our proposed process has interesting stochastic properties such as an\nMA($\\infty$) representation, time-reversibility, and closed-forms for the\ntransition probabilities $h$-steps ahead, allowing for coherent forecasting. We\nanalyze time-series counts of skin lesions using our proposed approach,\ncomparing it with existing INAR and INGARCH models. Our model gives more\nadherence to the data and better forecasting performance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 17:31:14 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Guerrero", "Matheus B.", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.08730", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Predicting MMSE Score from Finger-Tapping Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dementia is a leading cause of diseases for the elderly. Early diagnosis is\nvery important for the elderly living with dementias. In this paper, we propose\na method for dementia diagnosis by predicting MMSE score from finger-tapping\nmeasurement with machine learning pipeline. Based on measurement of finger\ntapping movement, the pipeline is first to select finger-tapping attributes\nwith copula entropy and then to predict MMSE score from the selected attributes\nwith predictive models. Experiments on real world data show that the predictive\nmodels such developed present good prediction performance. As a byproduct, the\nassociations between certain finger-tapping attributes (Number of taps and SD\nof inter-tapping interval) and MMSE score are discovered with copula entropy,\nwhich may be interpreted as the biological relationship between cognitive\nability and motor ability and therefore makes the predictive models\nexplainable. The selected finger-tapping attributes can be considered as\ndementia biomarkers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 23:30:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "2004.08809", "submitter": "Lisa Amrhein", "authors": "Lisa Amrhein and Christiane Fuchs", "title": "stochprofML: Stochastic Profiling Using Maximum Likelihood Estimation in\n  R", "comments": "52 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tissues are often heterogeneous in their single-cell molecular expression,\nand this can govern the regulation of cell fate. For the understanding of\ndevelopment and disease, it is important to quantify heterogeneity in a given\ntissue. We introduce the \\proglang{R} package \\pkg{stochprofML} which is\ndesigned to parameterize heterogeneity from the cumulative expression of small\nrandom pools of cells. This method outweighs the demixing of mixed samples with\na saving in cost and effort and less measurement error. The approach uses the\nmaximum likelihood principle and was originally presented in Bajikar et\nal.(2014); its extension to varying pool sizes was used in Tirier et al.\n(2019). We evaluate the algorithm's performance in simulation studies and\npresent further application opportunities.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 10:37:04 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Amrhein", "Lisa", ""], ["Fuchs", "Christiane", ""]]}, {"id": "2004.08897", "submitter": "Abd AlRahman AlMomani", "authors": "Abd AlRahman AlMomani and Erik Bollt", "title": "Informative Ranking of Stand Out Collections of Symptoms: A New\n  Data-Driven Approach to Identify the Strong Warning Signs of COVID 19", "comments": "15 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop here a data-driven approach for disease recognition based on given\nsymptoms, to be efficient tool for anomaly detection. In a clinical setting and\nwhen presented with a patient with a combination of traits, a doctor may wonder\nif a certain combination of symptoms may be especially predictive, such as the\nquestion, \"Are fevers more informative in women than men?\" The answer to this\nquestion is, yes. We develop here a methodology to enumerate such questions, to\nlearn what are the stronger warning signs when attempting to diagnose a\ndisease, called Conditional Predictive Informativity, (CPI), whose ranking we\ncall CPIR. This simple to use process allows us to identify particularly\ninformative combinations of symptoms and traits that may help medical field\nanalysis in general, and possibly to become a new data-driven advised approach\nfor individual medical diagnosis, as well as for broader public policy\ndiscussion. In particular we have been motivated to develop this tool in the\ncurrent enviroment of the pressing world crisis due to the COVID 19 pandemic.\nWe apply the methods here to data collected from national, provincial, and\nmunicipal health reports, as well as additional information from online, and\nthen curated to an online publically available Github repository.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:22:17 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 17:11:21 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["AlMomani", "Abd AlRahman", ""], ["Bollt", "Erik", ""]]}, {"id": "2004.08968", "submitter": "Xiaowei Yue", "authors": "Xinran Shi, Xiaowei Yue, Zhiyong Liang, and Jianjun Shi", "title": "Real-time Data-driven Quality Assessment for Continuous Manufacturing of\n  Carbon Nanotube Buckypaper", "comments": null, "journal-ref": null, "doi": "10.1109/TNANO.2020.2989397", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carbon nanotube (CNT) thin sheet, or buckypaper, has shown great potential as\na multifunctional platform material due to its desirable properties, including\nits lightweight nature, high mechanical properties, and good conductivity.\nHowever, their mass adoption and applications by industry have run into\nsignificant bottlenecks because of large variability and uncertainty in quality\nduring fabrication. There is an urgent demand to produce high-quality,\nhigh-performance buckypaper at an industrial scale. Raman spectroscopy provides\ndetailed nanostructure information within seconds, and the obtained spectra can\nbe decomposed into multiple effects associated with diverse quality\ncharacteristics of buckypaper. However, the decomposed effects are\nhigh-dimensional, and a systematic quantification method for buckypaper quality\nassessment has been lacking. In this paper, we propose a real-time data-driven\nquality assessment method, which fills in the blank of quantifying the quality\nfor continuous manufacturing processes of CNT buckypaper. The composite indices\nderived from the proposed method are developed by analyzing in-line Raman\nspectroscopy sensing data. Weighted cross-correlation and maximum margin\nclustering are used to fuse the fixed effects into an inconsistency index to\nmonitor the long-term mean shift of the process and to fuse the normal effects\ninto a uniformity index to monitor the within-sample normality. Those\nindividual quality indices are then combined into a composite index to reflect\nthe overall quality of buckypaper. A case study indicates that our proposed\napproach can determine the quality rank for ten samples, and can provide\nquantitative quality indices for single-walled carbon nanotube buckypaper after\nacid processing or functionalization. The quality assessment results are\nconsistent with evaluations from the experienced engineers.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 21:44:00 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Shi", "Xinran", ""], ["Yue", "Xiaowei", ""], ["Liang", "Zhiyong", ""], ["Shi", "Jianjun", ""]]}, {"id": "2004.09009", "submitter": "Emmanuel De-Graft Johnson Owusu-Ansah PhD", "authors": "Emmanuel de-Graft Johnson Owusu-Ansah Atinuke O. Adebanji Eric\n  Nimako-Aidoo", "title": "Data Driven Modeling of Projected Mitigation and Suppressing Strategy\n  Interventions for SARS-COV 2 in Ghana", "comments": "21 Pages, 6 Figures 5 Tables This study assess the mitigation and\n  suppression measures undertaken by the government of Ghana for managing\n  COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the midst of pandemic for respiratory illness, the call for\nnon-pharmaceutical interventions become the highest priority for infectious\ndisease and public health experts, while the race towards vaccine or medical\nintervention are ongoing. Individuals may modify their behavior and take\npreventative steps to reduce infection risk in the bid to adhere to the call by\ngovernment officials and experts. As a result, the existence of relationship\nbetween the preliminary and the final transmission rates become feeble. This\nstudy evaluates the behavioral changes (mitigation and suppression measures)\nproposed by public health experts for COVID-19 which had altered human behavior\nand their day to day lives. The dynamics underlying the mitigation and\nsuppression measures reduces the contacts among citizens and significantly\ninterfere with their physical and social behavior. The results show all the\nmeasures have a significant impact on the decline of transmission rate.\nHowever, the mitigation measures might prolong the elimination of the\ntransmission which might lead to a severe economic meltdown, yet, a combination\nof the measures show a possibility of rooting out transmission within 30 days\nif adhered to in an extreme manner. The result shows a peak period of infection\nfor Ghana ranges from 64th day to 74th day of infection time period.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 01:14:55 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nimako-Aidoo", "Emmanuel de-Graft Johnson Owusu-Ansah Atinuke O. Adebanji Eric", ""]]}, {"id": "2004.09032", "submitter": "Franco Marchesoni", "authors": "Franco Marchesoni-Acland, Rodrigo Alonso Su\\'arez", "title": "Intra-day solar irradiation forecast using RLS filters and satellite\n  images", "comments": null, "journal-ref": "Renewable Energy Journal Volume 161, December 2020, Pages\n  1140-1154", "doi": "10.1016/j.renene.2020.07.101", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite-based solar irradiation forecasting is useful for short-term\nintra-day time horizons, outperforming numerical weather predictions up to 3-4\nhours ahead. The main techniques for solar satellite forecast are based on\nsophisticated cloud motion estimates from geostationary satellite images. This\nwork explores the use of satellite information in a simpler way, namely spatial\naverages that require almost no preprocessing. Adaptive auto-regressive models\nare used to assess the impact of this information on the forecasting\nperformance. A complete analysis regarding model selection, the satellite\naveraging window size and the inclusion of satellite past measurements is\nprovided. It is shown that: (i) satellite spatial averages are useful inputs\nand the averaging window size is an important parameter, (ii) satellite lags\nare of limited utility and spatial averages are more useful than weighted time\naverages, and (iii) there is no value in fine-tuning the orders of\nauto-regressive models for each time horizon, as the same performance can be\nobtained by using a fixed well-selected order. These ideas are tested for a\nregion with intermediate solar variability, and the models succeed to\noutperform a proposed optimal smart persistence, used here as an exigent\nperformance benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 02:40:55 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Marchesoni-Acland", "Franco", ""], ["Su\u00e1rez", "Rodrigo Alonso", ""]]}, {"id": "2004.09065", "submitter": "Luis Martinez Lomeli", "authors": "Luis Martinez Lomeli, Abdon Iniguez, Babak Shahbaba, John S Lowengrub,\n  Vladimir Minin", "title": "Optimal Experimental Design for Mathematical Models of Hematopoiesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hematopoietic system has a highly regulated and complex structure in\nwhich cells are organized to successfully create and maintain new blood cells.\nFeedback regulation is crucial to tightly control this system, but the specific\nmechanisms by which control is exerted are not completely understood. In this\nwork, we aim to uncover the underlying mechanisms in hematopoiesis by\nconducting perturbation experiments, where animal subjects are exposed to an\nexternal agent in order to observe the system response and evolution.\nDeveloping a proper experimental design for these studies is an extremely\nchallenging task. To address this issue, we have developed a novel Bayesian\nframework for optimal design of perturbation experiments. We model the numbers\nof hematopoietic stem and progenitor cells in mice that are exposed to a low\ndose of radiation. We use a differential equations model that accounts for\nfeedback and feedforward regulation. A significant obstacle is that the\nexperimental data are not longitudinal, rather each data point corresponds to a\ndifferent animal. This model is embedded in a hierarchical framework with\nlatent variables that capture unobserved cellular population levels. We select\nthe optimum design based on the amount of information gain, measured by the\nKullback-Leibler divergence between the probability distributions before and\nafter observing the data. We evaluate our approach using synthetic and\nexperimental data. We show that a proper design can lead to better estimates of\nmodel parameters even with relatively few subjects. Additionally, we\ndemonstrate that the model parameters show a wide range of sensitivities to\ndesign options. Our method should allow scientists to find the optimal design\nby focusing on their specific parameters of interest and provide insight to\nhematopoiesis. Our approach can be extended to more complex models where latent\ncomponents are used.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:44:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 03:16:58 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lomeli", "Luis Martinez", ""], ["Iniguez", "Abdon", ""], ["Shahbaba", "Babak", ""], ["Lowengrub", "John S", ""], ["Minin", "Vladimir", ""]]}, {"id": "2004.09318", "submitter": "Felix Laumann", "authors": "Felix Laumann, Julius von K\\\"ugelgen, Mauricio Barahona", "title": "Non-linear interlinkages and key objectives amongst the Paris Agreement\n  and the Sustainable Development Goals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The United Nations' ambitions to combat climate change and prosper human\ndevelopment are manifested in the Paris Agreement and the Sustainable\nDevelopment Goals (SDGs), respectively. These are inherently inter-linked as\nprogress towards some of these objectives may accelerate or hinder progress\ntowards others. We investigate how these two agendas influence each other by\ndefining networks of 18 nodes, consisting of the 17 SDGs and climate change,\nfor various groupings of countries. We compute a non-linear measure of\nconditional dependence, the partial distance correlation, given any subset of\nthe remaining 16 variables. These correlations are treated as weights on edges,\nand weighted eigenvector centralities are calculated to determine the most\nimportant nodes. We find that SDG 6, clean water and sanitation, and SDG 4,\nquality education, are most central across nearly all groupings of countries.\nIn developing regions, SDG 17, partnerships for the goals, is strongly\nconnected to the progress of other objectives in the two agendas whilst,\nsomewhat surprisingly, SDG 8, decent work and economic growth, is not as\nimportant in terms of eigenvector centrality.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:26:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Laumann", "Felix", ""], ["von K\u00fcgelgen", "Julius", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2004.09660", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu and Alexander W. Bukharin and Le Lu and He Wang and Yao\n  Xie", "title": "Data-Driven Optimization for Police Beat Design in South Fulton, Georgia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We redesign the police patrol beat in South Fulton, Georgia, in collaboration\nwith the South Fulton Police Department (SFPD), using a predictive data-driven\noptimization approach. Due to rapid urban development and population growth,\nthe original police beat design done in the 1970s was far from efficient, which\nleads to low policing efficiency and long 911 call response time. We balance\nthe police workload among different regions in the city, improve operational\nefficiency, and reduce 911 call response time by redesigning beat boundaries\nfor the SFPD. We discretize the city into small geographical atoms, which\ncorrespond to our decision variables; the decision is to map the atoms into\n``beats'', which are the basic unit of the police operation. We analyze\nworkload and trend in each atom using the rich dataset for police incidents\nreports and U.S. census data and predict future police workload for each atom\nusing spatial statistical regression models. Based on this, we formulate the\noptimal beat design as a mixed-integer programming (MIP) program with\ncontiguity and compactness constraints on the shape of the beats. The\noptimization problem is solved using simulated annealing due to its large-scale\nand non-convex nature. Our resulted beat design can reduce workload variance by\nover 90\\% according to our simulation. We redesign the police patrol beat in\nSouth Fulton, Georgia, in collaboration with the South Fulton Police Department\n(SFPD), using a predictive data-driven optimization approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 22:02:14 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhu", "Shixiang", ""], ["Bukharin", "Alexander W.", ""], ["Lu", "Le", ""], ["Wang", "He", ""], ["Xie", "Yao", ""]]}, {"id": "2004.09774", "submitter": "Alexey Kytmanov A", "authors": "T. A. Kustitskaya, A. A. Kytmanov, M. V. Noskov", "title": "Student-at-risk detection by current learning performance indicators\n  using Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present article is focused on the problem of prediction of student\nfailures with the purpose of their possible prevention by timely introducing\nsupportive measures. We propose a concept for building a predictive model based\non Bayesian networks for an academic course or module taught in a blended\nlearning format. Our empirical studies confirm that the proposed approach is\nperspective for the development of an early warning system for various\nstakeholders of the educational process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 06:57:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kustitskaya", "T. A.", ""], ["Kytmanov", "A. A.", ""], ["Noskov", "M. V.", ""]]}, {"id": "2004.09791", "submitter": "Young-Jin Park", "authors": "Young-Jin Park, Han-Lim Choi", "title": "A Neural Process Approach for Probabilistic Reconstruction of No-Data\n  Gaps in Lunar Digital Elevation Maps", "comments": "24 pages, 5 figures, submitted to Aerospace Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of NASA's lunar reconnaissance orbiter (LRO), a large amount\nof high-resolution digital elevation maps (DEMs) have been constructed by using\nnarrow-angle cameras (NACs) to characterize the Moon's surface. However, NAC\nDEMs commonly contain no-data gaps (voids), which makes the map less reliable.\nTo resolve the issue, this paper provides a deep-learning-based framework for\nthe probabilistic reconstruction of no-data gaps in NAC DEMs. The framework is\nbuilt upon a state of the art stochastic process model, attentive neural\nprocesses (ANP), and predicts the conditional distribution of elevation on the\ntarget coordinates (latitude and longitude) conditioned on the observed\nelevation data in nearby regions. Furthermore, this paper proposes sparse\nattentive neural processes (SANPs) that not only reduces the linear\ncomputational complexity of the ANP O(N) to the constant complexity O(K) but\nenhance the reconstruction performance by preventing overfitting and\nover-smoothing problems. The proposed method is evaluated on the Apollo 17\nlanding site (20.0{\\deg}N and 30.4{\\deg}E), demonstrating that the suggested\napproach successfully reconstructs no-data gaps with uncertainty analysis while\npreserving the high resolution of original NAC DEMs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:37:47 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Park", "Young-Jin", ""], ["Choi", "Han-Lim", ""]]}, {"id": "2004.09952", "submitter": "Raffy Centeno", "authors": "Raffy S. Centeno and Judith P. Marquez", "title": "How much did the Tourism Industry Lost? Estimating Earning Loss of\n  Tourism in the Philippines", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study aimed to forecast the total earnings lost of the tourism industry\nof the Philippines during the COVID-19 pandemic using seasonal autoregressive\nintegrated moving average. Several models were considered based on the\nautocorrelation and partial autocorrelation graphs. Based on the Akaike's\nInformation Criterion (AIC) and Root Mean Squared Error,\nARIMA(1,1,1)$\\times$(1,0,1)$_{12}$ was identified to be the better model among\nthe others with an AIC value of $-414.51$ and RMSE of $47884.85$. Moreover, it\nis expected that the industry will have an estimated earning loss of around\n170.5 billion pesos if the COVID-19 crisis will continue up to July. Possible\nrecommendations to mitigate the problem includes stopping foreign tourism but\nallowing regions for domestic travels if the regions are confirmed to have no\ncases of COVID-19, assuming that every regions will follow the stringent\nguidelines to eliminate or prevent transmissions; or extending this to\ncountries with no COVID-19 cases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:31:28 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Centeno", "Raffy S.", ""], ["Marquez", "Judith P.", ""]]}, {"id": "2004.09996", "submitter": "Tanujit Chakraborty", "authors": "Tanujit Chakraborty and Indrajit Ghosh", "title": "Real-time forecasts and risk assessment of novel coronavirus (COVID-19)\n  cases: A data-driven analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2020.109850", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) has become a public health emergency\nof international concern affecting 201 countries and territories around the\nglobe. As of April 4, 2020, it has caused a pandemic outbreak with more than\n11,16,643 confirmed infections and more than 59,170 reported deaths worldwide.\nThe main focus of this paper is two-fold: (a) generating short term (real-time)\nforecasts of the future COVID-19 cases for multiple countries; (b) risk\nassessment (in terms of case fatality rate) of the novel COVID-19 for some\nprofoundly affected countries by finding various important demographic\ncharacteristics of the countries along with some disease characteristics. To\nsolve the first problem, we presented a hybrid approach based on autoregressive\nintegrated moving average model and Wavelet-based forecasting model that can\ngenerate short-term (ten days ahead) forecasts of the number of daily confirmed\ncases for Canada, France, India, South Korea, and the UK. The predictions of\nthe future outbreak for different countries will be useful for the effective\nallocation of health care resources and will act as an early-warning system for\ngovernment policymakers. In the second problem, we applied an optimal\nregression tree algorithm to find essential causal variables that significantly\naffect the case fatality rates for different countries. This data-driven\nanalysis will necessarily provide deep insights into the study of early risk\nassessments for 50 immensely affected countries.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:15:51 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Ghosh", "Indrajit", ""]]}, {"id": "2004.10231", "submitter": "Wenjia Wang", "authors": "Wenjia Wang, Yi-Hui Zhou", "title": "Eigenvector-based sparse canonical correlation analysis: Fast\n  computation for estimation of multiple canonical vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical canonical correlation analysis (CCA) requires matrices to be low\ndimensional, i.e. the number of features cannot exceed the sample size. Recent\ndevelopments in CCA have mainly focused on the high-dimensional setting, where\nthe number of features in both matrices under analysis greatly exceeds the\nsample size. These approaches impose penalties in the optimization problems\nthat are needed to be solve iteratively, and estimate multiple canonical\nvectors sequentially. In this work, we provide an explicit link between sparse\nmultiple regression with sparse canonical correlation analysis, and an\nefficient algorithm that can estimate multiple canonical pairs simultaneously\nrather than sequentially. Furthermore, the algorithm naturally allows parallel\ncomputing. These properties make the algorithm much efficient. We provide\ntheoretical results on the consistency of canonical pairs. The algorithm and\ntheoretical development are based on solving an eigenvectors problem, which\nsignificantly differentiate our method with existing methods. Simulation\nresults support the improved performance of the proposed approach. We apply\neigenvector-based CCA to analysis of the GTEx thyroid histology images,\nanalysis of SNPs and RNA-seq gene expression data, and a microbiome study. The\nreal data analysis also shows improved performance compared to traditional\nsparse CCA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:32:28 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:10:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Wenjia", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2004.10241", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Michael A. Russell, Linda M. Collins, Predrag Klasnja,\n  Stephanie T. Lanza, Hyesun Yoo, Susan A. Murphy", "title": "The Micro-Randomized Trial for Developing Digital Interventions: Data\n  Analysis Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is much excitement surrounding the use of mobile and wearable\ntechnology for the purposes of delivering interventions as people go through\ntheir day-to-day lives, data analysis methods for constructing and optimizing\ndigital interventions lag behind. Here, we elucidate data analysis methods for\nprimary and secondary analyses of micro-randomized trials (MRTs), an\nexperimental design to optimize digital just-in-time adaptive interventions. We\nprovide a definition of causal \"excursion\" effects suitable for use in digital\nintervention development. We introduce the weighted and centered least-squares\n(WCLS) estimator which provides consistent causal excursion effect estimators\nfor digital interventions from MRT data. We describe how the WCLS estimator\nalong with associated test statistics can be obtained using standard\nstatistical software such as SAS or R. Throughout we use HeartSteps, an MRT\ndesigned to increase physical activity among sedentary individuals, to\nillustrate potential primary and secondary analyses.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:00:32 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Qian", "Tianchen", ""], ["Russell", "Michael A.", ""], ["Collins", "Linda M.", ""], ["Klasnja", "Predrag", ""], ["Lanza", "Stephanie T.", ""], ["Yoo", "Hyesun", ""], ["Murphy", "Susan A.", ""]]}, {"id": "2004.10291", "submitter": "Martin F\\'elix Medina", "authors": "Mart\\'in H. F\\'elix-Medina", "title": "Estimaci\\'on del n\\'umero de reproducci\\'on de la epidemia COVID-19 en\n  Culiac\\'an Sinaloa, M\\'exico", "comments": "12 pages, 7 figures, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently the COVID-19 epidemic is developing in the City of Culiac\\'n\nSinaloa, Mexico, where up to April 20 of this year there have been 35 deaths\nassociated with this epidemic. The reproduction number $(R_0)$ of an epidemic\nrepresents the average number of people infected by an infected person during\ntheir period of infection. In this work we use the data published by the\nSecretary of Health of the State of Sinaloa on the number of new infected cases\nconfirmed per day and we estimate that the value of $R_0$ is 1.562 with a 95%\nconfidence interval given by (1.401,1.742). We also estimate the mortality rate\namong the confirmed cases, which turned out to be 16.8%.\n  - - - - -\n  Actualmente la epidemia COVID-19 se est\\'a desarrollando en la Ciudad de\nCuliac\\'an Sinaloa, M\\'exico, donde hasta el 20 de abril del presente a\\~no han\nocurrido 35 decesos asociados con esta epidemia. El n\\'umero de reproducci\\'on\n$(R_0)$ de una epidemia representa el n\\'umero promedio de personas contagiadas\npor una persona infectada durante su periodo de infecci\\'on. En este trabajo\nusamos los datos publicados por la Secretaria de Salud del Estado de Sinaloa\nsobre el n\\'umero de nuevos casos infectados confirmados por dia y estimamos\nque el valor de $R_0$ es de 1.562 con un intervalo del 95% de confianza dado\npor (1.401,1.742). Estimamos tambi\\'en la tasa de mortalidad entre los casos\nconfirmados, la cual result\\'o ser de 16.8%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:45:18 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["F\u00e9lix-Medina", "Mart\u00edn H.", ""]]}, {"id": "2004.10326", "submitter": "Hyoshin Kim", "authors": "Hyoshin Kim, Nancy McMillan, Jeffrey Geppert and Laura Aume", "title": "Comparison of Clinical Episode Outcomes between Bundled Payments for\n  Care Improvement (BPCI) Initiative Participants and Non-Participants", "comments": "15 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To evaluate differences in major outcomes between Bundled Payments\nfor Care Improvement (BPCI) participating providers and non-participating\nproviders for both Major Joint Replacement of the Lower Extremity (MJRLE) and\nAcute Myocardial Infarction (AMI) episodes. Methods: A\ndifference-in-differences approach estimated the differential change in\noutcomes for Medicare beneficiaries who had an MJRLE or AMI at a BPCI\nparticipating hospital between the baseline (January 2011 through September\n2013) and intervention (October 2013 through December 2016) periods and\nbeneficiaries with the same episode (MJRLE or AMI) at a matched comparison\nhospital. Main Outcomes and Measures: Medicare payments, LOS, and readmissions\nduring the episode, which includes the anchor hospitalization and the 90-day\npost discharge period. Results: Mean total Medicare payments for an MJRLE\nepisode and the 90-day post discharge period declined $444 more (p < 0.0001)\nfor Medicare beneficiaries with episodes initiated in a BPCI-participating\nprovider than for the beneficiaries in a comparison provider. This reduction\nwas mainly due to reduced institutional post-acute care (PAC) payments. Slight\nreductions in carrier payments and LOS were estimated. Readmission rates were\nnot statistically different between the BPCI and the comparison populations.\nThese findings suggest that PAC use can be reduced without adverse effects on\nrecovery from MJRLE. The lack of statistically significant differences in\neffects for AMI could be explained by a smaller sample size or more\nheterogenous recovery paths in AMI. Conclusions: Our findings suggest that, as\ncurrently designed, bundled payments can be effective in reducing payments for\nMJRLE episodes of care, but not necessarily for AMI. Most savings came from the\ndeclines in PAC. These findings are consistent with the results reported in the\nBPCI model evaluation for CMS.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 22:18:03 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kim", "Hyoshin", ""], ["McMillan", "Nancy", ""], ["Geppert", "Jeffrey", ""], ["Aume", "Laura", ""]]}, {"id": "2004.10548", "submitter": "Alje van Dam", "authors": "Alje van Dam, Andres Gomez-Lievano, Frank Neffke, Koen Frenken", "title": "An information-theoretic approach to the analysis of location and\n  co-location patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical framework to quantify location and co-location\nassociations of economic activities using information-theoretic measures. We\nrelate the resulting measures to existing measures of revealed comparative\nadvantage, localization and specialization and show that they can all be seen\nas part of the same framework. Using a Bayesian approach, we provide measures\nof uncertainty of the estimated quantities. Furthermore, the\ninformation-theoretic approach can be readily extended to move beyond pairwise\nco-locations and instead capture multivariate associations. To illustrate the\nframework, we apply our measures to the co-location of occupations in US\ncities, showing the associations between different groups of occupations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:05:21 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["van Dam", "Alje", ""], ["Gomez-Lievano", "Andres", ""], ["Neffke", "Frank", ""], ["Frenken", "Koen", ""]]}, {"id": "2004.10732", "submitter": "Vurukonda Sathish", "authors": "Vurukonda Sathish, Siuli Mukhopadhyay and Rashmi Tiwari", "title": "ARMA Models for Zero Inflated Count Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero inflation is a common nuisance while monitoring disease progression over\ntime. This article proposes a new observation driven model for zero inflated\nand over-dispersed count time series. The counts given the past history of the\nprocess and available information on covariates is assumed to be distributed as\na mixture of a Poisson distribution and a distribution degenerate at zero, with\na time dependent mixing probability, $\\pi_t$. Since, count data usually suffers\nfrom overdispersion, a Gamma distribution is used to model the excess\nvariation, resulting in a zero inflated Negative Binomial (NB) regression model\nwith mean parameter $\\lambda_t$. Linear predictors with auto regressive and\nmoving average (ARMA) type terms, covariates, seasonality and trend are fitted\nto $\\lambda_t$ and $\\pi_t$ through canonical link generalized linear models.\nEstimation is done using maximum likelihood aided by iterative algorithms, such\nas Newton Raphson (NR) and Expectation and Maximization (EM). Theoretical\nresults on the consistency and asymptotic normality of the estimators are\ngiven. The proposed model is illustrated using in-depth simulation studies and\na dengue data set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:34:30 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 10:25:20 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 13:56:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Sathish", "Vurukonda", ""], ["Mukhopadhyay", "Siuli", ""], ["Tiwari", "Rashmi", ""]]}, {"id": "2004.10931", "submitter": "Xiaowei Yue", "authors": "Xiaowei Yue, Yuchen Wen, Jeffrey H. Hunt, and Jianjun Shi", "title": "Active Learning for Gaussian Process Considering Uncertainties with\n  Application to Shape Control of Composite Fuselage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the machine learning domain, active learning is an iterative data\nselection algorithm for maximizing information acquisition and improving model\nperformance with limited training samples. It is very useful, especially for\nthe industrial applications where training samples are expensive,\ntime-consuming, or difficult to obtain. Existing methods mainly focus on active\nlearning for classification, and a few methods are designed for regression such\nas linear regression or Gaussian process. Uncertainties from measurement errors\nand intrinsic input noise inevitably exist in the experimental data, which\nfurther affects the modeling performance. The existing active learning methods\ndo not incorporate these uncertainties for Gaussian process. In this paper, we\npropose two new active learning algorithms for the Gaussian process with\nuncertainties, which are variance-based weighted active learning algorithm and\nD-optimal weighted active learning algorithm. Through numerical study, we show\nthat the proposed approach can incorporate the impact from uncertainties, and\nrealize better prediction performance. This approach has been applied to\nimproving the predictive modeling for automatic shape control of composite\nfuselage.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 02:04:53 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yue", "Xiaowei", ""], ["Wen", "Yuchen", ""], ["Hunt", "Jeffrey H.", ""], ["Shi", "Jianjun", ""]]}, {"id": "2004.10977", "submitter": "Hao Yan", "authors": "Hao Yan, Marco Grasso, Kamran Paynabar, and Bianca Maria Colosimo", "title": "Real-time Detection of Clustered Events in Video-imaging data with\n  Applications to Additive Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of video-imaging data for in-line process monitoring applications has\nbecome more and more popular in the industry. In this framework,\nspatio-temporal statistical process monitoring methods are needed to capture\nthe relevant information content and signal possible out-of-control states.\nVideo-imaging data are characterized by a spatio-temporal variability structure\nthat depends on the underlying phenomenon, and typical out-of-control patterns\nare related to the events that are localized both in time and space. In this\npaper, we propose an integrated spatio-temporal decomposition and regression\napproach for anomaly detection in video-imaging data. Out-of-control events are\ntypically sparse spatially clustered and temporally consistent. Therefore, the\ngoal is to not only detect the anomaly as quickly as possible (\"when\") but also\nlocate it (\"where\"). The proposed approach works by decomposing the original\nspatio-temporal data into random natural events, sparse spatially clustered and\ntemporally consistent anomalous events, and random noise. Recursive estimation\nprocedures for spatio-temporal regression are presented to enable the real-time\nimplementation of the proposed methodology. Finally, a likelihood ratio test\nprocedure is proposed to detect when and where the hotspot happens. The\nproposed approach was applied to the analysis of video-imaging data to detect\nand locate local over-heating phenomena (\"hotspots\") during the layer-wise\nprocess in a metal additive manufacturing process.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:32:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yan", "Hao", ""], ["Grasso", "Marco", ""], ["Paynabar", "Kamran", ""], ["Colosimo", "Bianca Maria", ""]]}, {"id": "2004.11022", "submitter": "Hao Yan", "authors": "Ziyue Li, Hao Yan, Chen Zhang, Fugee Tsung", "title": "Long-Short Term Spatiotemporal Tensor Prediction for Passenger Flow\n  Profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal data is very common in many applications, such as\nmanufacturing systems and transportation systems. It is typically difficult to\nbe accurately predicted given intrinsic complex spatial and temporal\ncorrelations. Most of the existing methods based on various statistical models\nand regularization terms, fail to preserve innate features in data alongside\ntheir complex correlations. In this paper, we focus on a tensor-based\nprediction and propose several practical techniques to improve prediction. For\nlong-term prediction specifically, we propose the \"Tensor Decomposition +\n2-Dimensional Auto-Regressive Moving Average (2D-ARMA)\" model, and an effective\nway to update prediction real-time; For short-term prediction, we propose to\nconduct tensor completion based on tensor clustering to avoid oversimplifying\nand ensure accuracy. A case study based on the metro passenger flow data is\nconducted to demonstrate the improved performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:30:00 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Li", "Ziyue", ""], ["Yan", "Hao", ""], ["Zhang", "Chen", ""], ["Tsung", "Fugee", ""]]}, {"id": "2004.11123", "submitter": "Georgios Leontidis", "authors": "Benedict Delahaye Chivers, John Wallbank, Steven J. Cole, Ondrej\n  Sebek, Simon Stanley, Matthew Fry and Georgios Leontidis", "title": "Imputation of missing sub-hourly precipitation data in a large sensor\n  network: a machine learning approach", "comments": "24 pages, 7 figures, 5 tables", "journal-ref": "Journal of Hydrology 2020", "doi": "10.1016/j.jhydrol.2020.125126", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precipitation data collected at sub-hourly resolution represents specific\nchallenges for missing data recovery by being largely stochastic in nature and\nhighly unbalanced in the duration of rain vs non-rain. Here we present a\ntwo-step analysis utilising current machine learning techniques for imputing\nprecipitation data sampled at 30-minute intervals by devolving the task into\n(a) the classification of rain or non-rain samples, and (b) regressing the\nabsolute values of predicted rain samples. Investigating 37 weather stations in\nthe UK, this machine learning process produces more accurate predictions for\nrecovering precipitation data than an established surface fitting technique\nutilising neighbouring rain gauges. Increasing available features for the\ntraining of machine learning algorithms increases performance with the\nintegration of weather data at the target site with externally sourced rain\ngauges providing the highest performance. This method informs machine learning\nmodels by utilising information in concurrently collected environmental data to\nmake accurate predictions of missing rain data. Capturing complex non-linear\nrelationships from weakly correlated variables is critical for data recovery at\nsub-hourly resolutions. Such pipelines for data recovery can be developed and\ndeployed for highly automated and near instantaneous imputation of missing\nvalues in ongoing datasets at high temporal resolutions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:47:18 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 09:11:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chivers", "Benedict Delahaye", ""], ["Wallbank", "John", ""], ["Cole", "Steven J.", ""], ["Sebek", "Ondrej", ""], ["Stanley", "Simon", ""], ["Fry", "Matthew", ""], ["Leontidis", "Georgios", ""]]}, {"id": "2004.11128", "submitter": "Tom Needham", "authors": "Qitong Jiang, Sebastian Kurtek, Tom Needham", "title": "The Weighted Euler Curve Transform for Shape and Image Analysis", "comments": "To appear in CVPR conference workshop proceedings for DIFF-CVML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euler Curve Transform (ECT) of Turner et al.\\ is a complete invariant of\nan embedded simplicial complex, which is amenable to statistical analysis. We\ngeneralize the ECT to provide a similarly convenient representation for\nweighted simplicial complexes, objects which arise naturally, for example, in\ncertain medical imaging applications. We leverage work of Ghrist et al.\\ on\nEuler integral calculus to prove that this invariant---dubbed the Weighted\nEuler Curve Transform (WECT)---is also complete. We explain how to transform a\nsegmented region of interest in a grayscale image into a weighted simplicial\ncomplex and then into a WECT representation. This WECT representation is\napplied to study Glioblastoma Multiforme brain tumor shape and texture data. We\nshow that the WECT representation is effective at clustering tumors based on\nqualitative shape and texture features and that this clustering correlates with\npatient survival time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:19:34 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Jiang", "Qitong", ""], ["Kurtek", "Sebastian", ""], ["Needham", "Tom", ""]]}, {"id": "2004.11169", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Bernard Wong, and Xinda Yang", "title": "On the modelling of multivariate counts with Cox processes and dependent\n  shot noise intensities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method to model and estimate several, _dependent_\ncount processes, using granular data. Specifically, we develop a multivariate\nCox process with shot noise intensities to jointly model the arrival process of\ncounts (e.g. insurance claims). The dependency structure is introduced via\nmultivariate shot noise _intensity_ processes which are connected with the help\nof L\\'evy copulas. In aggregate, our approach allows for (i) over-dispersion\nand auto-correlation within each line of business; (ii) realistic features\ninvolving time-varying, known covariates; and (iii) parsimonious dependence\nbetween processes without requiring simultaneous primary (e.g. accidents)\nevents.\n  The explicit incorporation of time-varying, known covariates can accommodate\ncharacteristics of real data and hence facilitate implementation in practice.\nIn an insurance context, these could be changes in policy volumes over time, as\nwell as seasonality patterns and trends, which may explain some of the\nrelationship (dependence) between multiple claims processes, or at least help\ntease out those relationships.\n  Finally, we develop a filtering algorithm based on the reversible-jump Markov\nChain Monte Carlo (RJMCMC) method to estimate the latent stochastic intensities\nand illustrate model calibration using real data from the AUSI data set.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:59:44 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:30:58 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Wong", "Bernard", ""], ["Yang", "Xinda", ""]]}, {"id": "2004.11193", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Roula Tsonaka", "title": "Poisson-Tweedie mixed-effects model: a flexible approach for the\n  analysis of longitudinal RNA-seq data", "comments": "The final (published) version of the article can be downloaded for\n  free (Open Access) from the editor's website (click on the DOI link below).\n  Link to the R package ptmixed:\n  https://cran.r-project.org/web/packages/ptmixed/index.html", "journal-ref": "Statistical Modelling (2020)", "doi": "10.1177/1471082X20936017", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modelling approach for longitudinal count data that is\nmotivated by the increasing availability of longitudinal RNA-sequencing\nexperiments. The distribution of RNA-seq counts typically exhibits\noverdispersion, zero-inflation and heavy tails; moreover, in longitudinal\ndesigns repeated measurements from the same subject are typically (positively)\ncorrelated. We propose a generalized linear mixed model based on the\nPoisson-Tweedie distribution that can flexibly handle each of the\naforementioned features of longitudinal overdispersed counts. We develop a\ncomputational approach to accurately evaluate the likelihood of the proposed\nmodel and to perform maximum likelihood estimation. Our approach is implemented\nin the R package ptmixed, which can be freely downloaded from CRAN. We assess\nthe performance of ptmixed on simulated data and we present an application to a\ndataset with longitudinal RNA-sequencing measurements from healthy and\ndystrophic mice. The applicability of the Poisson-Tweedie mixed-effects model\nis not restricted to longitudinal RNA-seq data, but it extends to any scenario\nwhere non-independent measurements of a discrete overdispersed response\nvariable are available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 08:13:01 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2004.11195", "submitter": "Shangzhi Hong", "authors": "Shangzhi Hong, Yuqi Sun, Hanying Li, Henry S. Lynn", "title": "Influence of parallel computing strategies of iterative imputation of\n  missing data: a case study on missForest", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning iterative imputation methods have been well accepted by\nresearchers for imputing missing data, but they can be time-consuming when\nhandling large datasets. To overcome this drawback, parallel computing\nstrategies have been proposed but their impact on imputation results and\nsubsequent statistical analyses are relatively unknown. This study examines the\ntwo parallel strategies (variable-wise distributed computation and model-wise\ndistributed computation) implemented in the random-forest imputation method,\nmissForest. Results from the simulation experiments showed that the two\nparallel strategies can influence both the imputation process and the final\nimputation results differently. Specifically, even though both strategies\nproduced similar normalized root mean squared prediction errors, the\nvariable-wise distributed strategy led to additional biases when estimating the\nmean and inter-correlation of the covariates and their regression coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:41:52 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hong", "Shangzhi", ""], ["Sun", "Yuqi", ""], ["Li", "Hanying", ""], ["Lynn", "Henry S.", ""]]}, {"id": "2004.11267", "submitter": "Ramona Maraia", "authors": "Antti Solonen, Ramona Maraia, Sebastian Springer, Heikki Haario, Marko\n  Laine, Olle R\\\"aty, Jukka-Pekka Jalkanen, Matti Antola", "title": "Hierarchical Bayesian propulsion power models for marine vessels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the magnitude of fuel consumption of marine traffic is a\nchallenging task. The consumption can be reduced by the ways the vessels are\noperated, to achieve both improved cost efficiency and reduced CO2 emissions.\nMathematical models for predicting ships' consumption are in a central role in\nboth of these tasks. Nowadays, many ships are equipped with data collection\nsystems, which enable data-based calibration of the consumption models.\nTypically this calibration procedure is carried out independently for each\nparticular ship, using only data collected from the ship in question. In this\npaper, we demonstrate a hierarchical Bayesian modeling approach, where we fit a\nsingle model over many vessels, with the assumption that the parameters of\nvessels of same type and similar characteristics (e.g. vessel size) are likely\nclose to each other. The benefits of such an approach are two-fold; 1) we can\nborrow information about parameters that are not well informed by the\nvessel-specific data using data from similar ships, and 2) we can use the final\nhierarchical model to predict the behavior of a vessel from which we don't have\nany data, based only on its characteristics. In this paper, we discuss the\nbasic concept and present a first simple version of the model. We apply the\nStan statistical modeling tool for the model fitting and use real data from 64\ncruise ships collected via the widely used commercial Eniram platform. By using\nBayesian statistical methods we obtain uncertainties for the model predictions,\ntoo. The prediction accuracy of the model is compared to an existing data-free\nmodeling approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:59:34 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 06:49:24 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Solonen", "Antti", ""], ["Maraia", "Ramona", ""], ["Springer", "Sebastian", ""], ["Haario", "Heikki", ""], ["Laine", "Marko", ""], ["R\u00e4ty", "Olle", ""], ["Jalkanen", "Jukka-Pekka", ""], ["Antola", "Matti", ""]]}, {"id": "2004.11278", "submitter": "Luca Pappalardo", "authors": "Pietro Bonato, Paolo Cintia, Francesco Fabbri, Daniele Fadda, Fosca\n  Giannotti, Pier Luigi Lopalco, Sara Mazzilli, Mirco Nanni, Luca Pappalardo,\n  Dino Pedreschi, Francesco Penone, Salvatore Rinzivillo, Giulio Rossetti,\n  Marcello Savarese, Lara Tavoschi", "title": "Mobile phone data analytics against the COVID-19 epidemics in Italy:\n  flow diversity and local job markets during the national lockdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding collective mobility patterns is crucial to plan the restart of\nproduction and economic activities, which are currently put in stand-by to\nfight the diffusion of the epidemics. In this report, we use mobile phone data\nto infer the movements of people between Italian provinces and municipalities,\nand we analyze the incoming, outcoming and internal mobility flows before and\nduring the national lockdown (March 9th, 2020) and after the closure of\nnon-necessary productive and economic activities (March 23th, 2020). The\npopulation flow across provinces and municipalities enable for the modelling of\na risk index tailored for the mobility of each municipality or province. Such\nan index would be a useful indicator to drive counter-measures in reaction to a\nsudden reactivation of the epidemics. Mobile phone data, even when aggregated\nto preserve the privacy of individuals, are a useful data source to track the\nevolution in time of human mobility, hence allowing for monitoring the\neffectiveness of control measures such as physical distancing. We address the\nfollowing analytical questions: How does the mobility structure of a territory\nchange? Do incoming and outcoming flows become more predictable during the\nlockdown, and what are the differences between weekdays and weekends? Can we\ndetect proper local job markets based on human mobility flows, to eventually\nshape the borders of a local outbreak?\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:06:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bonato", "Pietro", ""], ["Cintia", "Paolo", ""], ["Fabbri", "Francesco", ""], ["Fadda", "Daniele", ""], ["Giannotti", "Fosca", ""], ["Lopalco", "Pier Luigi", ""], ["Mazzilli", "Sara", ""], ["Nanni", "Mirco", ""], ["Pappalardo", "Luca", ""], ["Pedreschi", "Dino", ""], ["Penone", "Francesco", ""], ["Rinzivillo", "Salvatore", ""], ["Rossetti", "Giulio", ""], ["Savarese", "Marcello", ""], ["Tavoschi", "Lara", ""]]}, {"id": "2004.11292", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri, Aurelie Labbe, Denis Larocque and Laurent Charlin", "title": "Predictive inference for travel time on transportation networks", "comments": "27 main pages, 4 figures and 3 tables. This version includes many\n  changes to the previous one", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent statistical methods fitted on large-scale GPS data are getting close\nto answering the proverbial \"When are we there?\" question. Unfortunately,\ncurrent methods often only provide point predictions for travel time.\nUnderstanding travel time distribution is key for decision-making and\ndownstream applications (e.g., ride share pricing decisions). Empirically,\nsingle road-segment travel time is well-studied, understanding how to aggregate\nsuch information over many segments to arrive at the distribution of travel\ntime over a route is challenging. We develop a novel statistical approach to\nthis problem, where we show that, under general conditions, without assuming a\ndistribution of speed, travel time normalized by distance follows a Gaussian\ndistribution with route-invariant population mean and variance. We develop\nefficient inference methods for such parameters, with which we propose\npopulation prediction intervals for travel time. Our population intervals are\nasymptotically tight and require only two parameter estimates. Using road-level\ninformation (e.g.~traffic density), we further develop a catered trips-specific\nGaussian-based predictive distribution, resulting in tight prediction intervals\nfor short and long trips. Our methods, implemented in an R-package, are\nillustrated in a real-world case study using mobile GPS data, showing that our\ntrip-specific and population intervals both achieve the 95\\% theoretical\ncoverage levels. Compared to alternative approaches, our trip-specific\npredictive distribution achieves (a) the theoretical coverage at every level of\nsignificance, (b) tighter prediction intervals, (c) less predictive bias, and\n(d) more efficient estimation and prediction procedures that only rely on the\nfirst and second moment estimates of speed on edges of the network. This makes\nour approach promising for low latency large-scale transportation applications.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:18:19 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 02:09:15 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 00:29:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Elmasri", "Mohamad", ""], ["Labbe", "Aurelie", ""], ["Larocque", "Denis", ""], ["Charlin", "Laurent", ""]]}, {"id": "2004.11338", "submitter": "Ognyan Kounchev", "authors": "Ognyan Kounchev, Georgi Simeonov, Zhana Kuncheva", "title": "The TVBG-SEIR spline model for analysis of COVID-19 spread, and a Tool\n  for prediction scenarios", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models are traditionally used to analyze the long-term global\nevolution of epidemics, to determine the potential and severity of an outbreak,\nand to provide critical information for identifying the type of disease\ninterventions and intensity. One of the widely used mathematical models of\nlong-term spreading of epidemics are the so-called deterministic compartmental\nmodels (SIR/SEIR type models). One of the main purposes of applying such models\nis to assess how the expensive restriction measures imposed by the authorities\n(home and social isolation/quarantine, travel restrictions, etc.) can\neffectively reduce the control reproduction number of the disease and its\ntransmission risk. However the classical SIR/SEIR models have been primarily\nstudied in what may be called stationary case, where the main parameters, the\nTransmission rate Beta (reflecting the virus spread by infected individuals)\nand the Removed rate Gamma (reflecting the hospitalization/isolation measures)\nremain constant during the whole period of interest. Hence, it is important to\nextend the classical SIR/SEIR models by creating new ansatzes for the dynamics\nof the transmission rates Beta(t) (which we will call further just Beta) and\nremoved rates Gamma(t) (which we will call further just Gamma). The main\npurpose of the present research is to introduce a spline-based SEIR model with\nTime-varying Beta and Gamma parameters, or abbreviated TVBG-SEIR model, which\nis used to estimate the practical implications of the public health\ninterventions and measures. We have designed a Tool based on the TVBG-SEIR\nmodel, which may be used as a Decision Support Tool to assist the health\ndecision- and policy-makers in creating predictive scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:34:27 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 13:49:07 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kounchev", "Ognyan", ""], ["Simeonov", "Georgi", ""], ["Kuncheva", "Zhana", ""]]}, {"id": "2004.11342", "submitter": "Seth Flaxman", "authors": "Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Helen\n  Coupland, Thomas A Mellan, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton,\n  Pablo N P Guzman, Nora Schmit, Lucia Callizo, Imperial College COVID-19\n  Response Team, Charles Whittaker, Peter Winskill, Xiaoyue Xi, Azra Ghani,\n  Christl A. Donnelly, Steven Riley, Lucy C Okell, Michaela A C Vollmer, Neil\n  M. Ferguson and Samir Bhatt", "title": "Estimating the number of infections and the impact of non-pharmaceutical\n  interventions on COVID-19 in European countries: technical description update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the emergence of a novel coronavirus (SARS-CoV-2) and its spread\noutside of China, Europe has experienced large epidemics. In response, many\nEuropean countries have implemented unprecedented non-pharmaceutical\ninterventions including case isolation, the closure of schools and\nuniversities, banning of mass gatherings and/or public events, and most\nrecently, wide-scale social distancing including local and national lockdowns.\n  In this technical update, we extend a semi-mechanistic Bayesian hierarchical\nmodel that infers the impact of these interventions and estimates the number of\ninfections over time. Our methods assume that changes in the reproductive\nnumber - a measure of transmission - are an immediate response to these\ninterventions being implemented rather than broader gradual changes in\nbehaviour. Our model estimates these changes by calculating backwards from\ntemporal data on observed to estimate the number of infections and rate of\ntransmission that occurred several weeks prior, allowing for a probabilistic\ntime lag between infection and death.\n  In this update we extend our original model [Flaxman, Mishra, Gandy et al\n2020, Report #13, Imperial College London] to include (a) population saturation\neffects, (b) prior uncertainty on the infection fatality ratio, (c) a more\nbalanced prior on intervention effects and (d) partial pooling of the lockdown\nintervention covariate. We also (e) included another 3 countries (Greece, the\nNetherlands and Portugal).\n  The model code is available at\nhttps://github.com/ImperialCollegeLondon/covid19model/\n  We are now reporting the results of our updated model online at\nhttps://mrc-ide.github.io/covid19estimates/\n  We estimated parameters jointly for all M=14 countries in a single\nhierarchical model. Inference is performed in the probabilistic programming\nlanguage Stan using an adaptive Hamiltonian Monte Carlo (HMC) sampler.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:38:23 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Flaxman", "Seth", ""], ["Mishra", "Swapnil", ""], ["Gandy", "Axel", ""], ["Unwin", "H Juliette T", ""], ["Coupland", "Helen", ""], ["Mellan", "Thomas A", ""], ["Zhu", "Harrison", ""], ["Berah", "Tresnia", ""], ["Eaton", "Jeffrey W", ""], ["Guzman", "Pablo N P", ""], ["Schmit", "Nora", ""], ["Callizo", "Lucia", ""], ["Team", "Imperial College COVID-19 Response", ""], ["Whittaker", "Charles", ""], ["Winskill", "Peter", ""], ["Xi", "Xiaoyue", ""], ["Ghani", "Azra", ""], ["Donnelly", "Christl A.", ""], ["Riley", "Steven", ""], ["Okell", "Lucy C", ""], ["Vollmer", "Michaela A C", ""], ["Ferguson", "Neil M.", ""], ["Bhatt", "Samir", ""]]}, {"id": "2004.11355", "submitter": "Drew Thomas", "authors": "Drew M Thomas", "title": "Excess registered deaths in England and Wales during the COVID-19\n  pandemic, March 2020 to May 2020", "comments": "27 A4 pages, 4 tables, 14 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Official counts of COVID-19 deaths have been criticized for potentially\nincluding people who did not die of COVID-19 but merely died with COVID-19. I\naddress that critique by fitting a generalized additive model to weekly counts\nof all deaths registered in England and Wales during the 2010s. The model\nproduces baseline rates of death registrations expected without the COVID-19\npandemic, and comparing those baselines to recent counts of registered deaths\nexposes the emergence of excess deaths late in March 2020. By April's end,\nEngland and Wales registered 45,300 $\\pm$ 3200 excess deaths of adults aged\n45+. Through 22 May, the last day of available all-deaths data, 56,600 $\\pm$\n4400 were registered (about 53% of which were of men). Both the ONS's\ncorresponding count of 43,205 death certificates which mention COVID-19, and\nthe Department of Health and Social Care's count of 33,671 deaths, are\nappreciably less, implying that their counting methods have underestimated, not\noverestimated, the pandemic's true death toll. If underreporting rates have\nheld steady during May, about 59,000 direct and indirect COVID-19 deaths might\nhave been registered through the end of May but not yet publicly reported in\nfull.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:54:24 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:58:50 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 18:52:51 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 17:57:28 GMT"}, {"version": "v5", "created": "Mon, 25 May 2020 17:59:26 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2020 17:57:15 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Thomas", "Drew M", ""]]}, {"id": "2004.11369", "submitter": "Vukosi Marivate", "authors": "Henry Wandera, Vukosi Marivate, David Sengeh", "title": "Investigating similarities and differences between South African and\n  Sierra Leonean school outcomes using Machine Learning", "comments": "In review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Available or adequate information to inform decision making for resource\nallocation in support of school improvement is a critical issue globally. In\nthis paper, we apply machine learning and education data mining techniques on\neducation big data to identify determinants of high schools' performance in two\nAfrican countries: South Africa and Sierra Leone. The research objective is to\nbuild predictors for school performance and extract the importance of different\ncommunity and school-level features. We deploy interpretable metrics from\nmachine learning approaches such as SHAP values on tree models and odds ratios\nof LR to extract interactions of factors that can support policy decision\nmaking. Determinants of performance vary in these two countries, hence\ndifferent policy implications and resource allocation recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:29:16 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wandera", "Henry", ""], ["Marivate", "Vukosi", ""], ["Sengeh", "David", ""]]}, {"id": "2004.11422", "submitter": "Ishtiak Ahmed Mr.", "authors": "Ishtiak Ahmed, Billy Williams, M. Shoaib Samandar, Gyounghoon Chun", "title": "Investigating the Relationship between Freeway Rear-end Crash Rates and\n  Macroscopically Modelled Reaction Time", "comments": "30 pages, 2 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the hypothesis that an analytically derived estimate of\nthe required driver reaction time for asymptotic stability, based on the\nmacroscopic Gazis, Herman, and Rothery (GHR) model, can serve as an effective\nindicator of the impact of traffic oscillations on rear-end crashes. If\nseparate GHR models are fit discontinuously for the uncongested and congested\nregimes, the local drop in required reaction time between the two regimes can\nalso be estimated. This study evaluates the relationship between freeway\nrear-end crash rates and this drop in driver reaction time.\n  Traffic data from 28 sensors collected over one year were used to calibrate\nthe two-regime GHR model. Rear-end crash rates for the segments surrounding the\nsensor locations are estimated using archived crash data over four years. The\nrear-end crash rates exhibited a strong positive correlation with the reaction\ntime drop at the density-breakpoint of the congested regime. A linear form\nmodel provided the best fit in terms of R-square, standard error, and\nhomoscedasticity. These results motivate follow-on research to incorporate\nmacroscopically derived reaction time in road-safety planning. More generally,\nthe study demonstrates a useful application of a discontinuous macroscopic\ntraffic model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:50:16 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ahmed", "Ishtiak", ""], ["Williams", "Billy", ""], ["Samandar", "M. Shoaib", ""], ["Chun", "Gyounghoon", ""]]}, {"id": "2004.11470", "submitter": "Wagner Barreto-Souza", "authors": "Gisele O. Maia, Wagner Barreto-Souza, Fernando S. Bastos and Hernando\n  Ombao", "title": "Semiparametric time series models driven by latent factor", "comments": null, "journal-ref": "International Journal of Forecasting (2021)", "doi": "10.1016/j.ijforecast.2020.12.007", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of semiparametric time series models by assuming a\nquasi-likelihood approach driven by a latent factor process. More specifically,\ngiven the latent process, we only specify the conditional mean and variance of\nthe time series and enjoy a quasi-likelihood function for estimating parameters\nrelated to the mean. This proposed methodology has three remarkable features:\n(i) no parametric form is assumed for the conditional distribution of the time\nseries given the latent process; (ii) able for modelling non-negative, count,\nbounded/binary and real-valued time series; (iii) dispersion parameter is not\nassumed to be known. Further, we obtain explicit expressions for the marginal\nmoments and for the autocorrelation function of the time series process so that\na method of moments can be employed for estimating the dispersion parameter and\nalso parameters related to the latent process. Simulated results aiming to\ncheck the proposed estimation procedure are presented. Real data analysis on\nunemployment rate and precipitation time series illustrate the potencial for\npractice of our methodology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:39:14 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Maia", "Gisele O.", ""], ["Barreto-Souza", "Wagner", ""], ["Bastos", "Fernando S.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2004.11710", "submitter": "Hao Yan", "authors": "Yujie Zhao, Hao Yan, Sarah Holte, and Yajun Mei", "title": "Rapid Detection of Hot-spots via Tensor Decomposition with applications\n  to Crime Rate Data", "comments": "arXiv admin note: text overlap with arXiv:2001.11685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient statistical method (denoted as SSR-Tensor) to\nrobustly and quickly detect hot-spots that are sparse and temporal-consistent\nin a spatial-temporal dataset through the tensor decomposition. Our main idea\nis first to build an SSR model to decompose the tensor data into a Smooth\nglobal trend mean, Sparse local hot-spots, and Residuals. Next, tensor\ndecomposition is utilized as follows: bases are introduced to describe\nwithin-dimension correlation, and tensor products are used for\nbetween-dimension interaction. Then, a combination of LASSO and fused LASSO is\nused to estimate the model parameters, where an efficient recursive estimation\nprocedure is developed based on the large-scale convex optimization, where we\nfirst transform the general LASSO optimization into regular LASSO optimization\nand apply FISTA to solve it with the fastest convergence rate. Finally, a CUSUM\nprocedure is applied to detect when and where the hot-spot event occurs. We\ncompare the performance of the proposed method in a numerical simulation study\nand a real-world case study, which contains a dataset including a collection of\nthree types of crime rates for U.S. mainland states during the year 1965-2014.\nIn both cases, the proposed SSR-Tensor is able to achieve the fast detection\nand accurate localization of the hot-spots.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:13:47 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 04:07:46 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 21:20:11 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhao", "Yujie", ""], ["Yan", "Hao", ""], ["Holte", "Sarah", ""], ["Mei", "Yajun", ""]]}, {"id": "2004.11780", "submitter": "Ruda Zhang", "authors": "Ruda Zhang and Patrick Wingo and Rodrigo Duran and Kelly Rose and\n  Jennifer Bauer and Roger Ghanem", "title": "Environmental Economics and Uncertainty: Review and a Machine Learning\n  Outlook", "comments": "24 pages, 7 figures, 1 table. In Oxford Research Encyclopedia of\n  Environmental Science. Oxford University Press", "journal-ref": null, "doi": "10.1093/acrefore/9780199389414.013.572", "report-no": null, "categories": "econ.GN physics.ao-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic assessment in environmental science concerns the measurement or\nvaluation of environmental impacts, adaptation, and vulnerability. Integrated\nassessment modeling is a unifying framework of environmental economics, which\nattempts to combine key elements of physical, ecological, and socioeconomic\nsystems. Uncertainty characterization in integrated assessment varies by\ncomponent models: uncertainties associated with mechanistic physical models are\noften assessed with an ensemble of simulations or Monte Carlo sampling, while\nuncertainties associated with impact models are evaluated by conjecture or\neconometric analysis. Manifold sampling is a machine learning technique that\nconstructs a joint probability model of all relevant variables which may be\nconcentrated on a low-dimensional geometric structure. Compared with\ntraditional density estimation methods, manifold sampling is more efficient\nespecially when the data is generated by a few latent variables. The\nmanifold-constrained joint probability model helps answer policy-making\nquestions from prediction, to response, and prevention. Manifold sampling is\napplied to assess risk of offshore drilling in the Gulf of Mexico.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:40:59 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhang", "Ruda", ""], ["Wingo", "Patrick", ""], ["Duran", "Rodrigo", ""], ["Rose", "Kelly", ""], ["Bauer", "Jennifer", ""], ["Ghanem", "Roger", ""]]}, {"id": "2004.11841", "submitter": "Felix Sattler", "authors": "Felix Sattler, Jackie Ma, Patrick Wagner, David Neumann, Markus\n  Wenzel, Ralf Sch\\\"afer, Wojciech Samek, Klaus-Robert M\\\"uller, Thomas Wiegand", "title": "Risk Estimation of SARS-CoV-2 Transmission from Bluetooth Low Energy\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.PE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contact tracing approaches based on Bluetooth low energy (BLE) have\nthe potential to efficiently contain and delay outbreaks of infectious diseases\nsuch as the ongoing SARS-CoV-2 pandemic. In this work we propose a novel\nmachine learning based approach to reliably detect subjects that have spent\nenough time in close proximity to be at risk of being infected. Our study is an\nimportant proof of concept that will aid the battery of epidemiological\npolicies aiming to slow down the rapid spread of COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 20:10:35 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Sattler", "Felix", ""], ["Ma", "Jackie", ""], ["Wagner", "Patrick", ""], ["Neumann", "David", ""], ["Wenzel", "Markus", ""], ["Sch\u00e4fer", "Ralf", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Wiegand", "Thomas", ""]]}, {"id": "2004.11849", "submitter": "Gabriel Riutort Mayol", "authors": "Gabriel Riutort-Mayol, Virgilio G\\'omez-Rubio, \\'Angel\n  Marqu\\'es-Mateu, Jos\\'e Luis Lerma, Antonio L\\'opez-Qu\\'ilez", "title": "A Bayesian Multilevel Random-Effects Model for Estimating Noise in Image\n  Sensors", "comments": null, "journal-ref": null, "doi": "10.1049/iet-ipr.2018.5926", "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor noise sources cause differences in the signal recorded across pixels\nin a single image and across multiple images. This paper presents a Bayesian\napproach to decomposing and characterizing the sensor noise sources involved in\nimaging with digital cameras. A Bayesian probabilistic model based on the\n(theoretical) model for noise sources in image sensing is fitted to a set of a\ntime-series of images with different reflectance and wavelengths under\ncontrolled lighting conditions. The image sensing model is a complex model,\nwith several interacting components dependent on reflectance and wavelength.\nThe properties of the Bayesian approach of defining conditional dependencies\namong parameters in a fully probabilistic model, propagating all sources of\nuncertainty in inference, makes the Bayesian modeling framework more attractive\nand powerful than classical methods for approaching the image sensing model. A\nfeasible correspondence of noise parameters to their expected theoretical\nbehaviors and well calibrated posterior predictive distributions with a small\nroot mean square error for model predictions have been achieved in this study,\nthus showing that the proposed model accurately approximates the image sensing\nmodel. The Bayesian approach could be extended to formulate further components\naimed at identifying even more specific parameters of the imaging process.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:47:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Riutort-Mayol", "Gabriel", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Marqu\u00e9s-Mateu", "\u00c1ngel", ""], ["Lerma", "Jos\u00e9 Luis", ""], ["L\u00f3pez-Qu\u00edlez", "Antonio", ""]]}, {"id": "2004.11851", "submitter": "Janin Heuer", "authors": "Timo de Wolff, Dirk Pfl\\\"uger, Michael Rehme, Janin Heuer and\n  Martin-Immanuel Bittner", "title": "Evaluation of Pool-based Testing Approaches to Enable Population-wide\n  Screening for COVID-19", "comments": "Revision; 16 pages, 3 figures, 2 tables, 2 supplementary figures", "journal-ref": null, "doi": "10.1371/journal.pone.0243692", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Rapid testing for an infection is paramount during a pandemic to\nprevent continued viral spread and excess morbidity and mortality. This study\naimed to determine whether alternative testing strategies based on sample\npooling can increase the speed and throughput of screening for SARS-CoV-2.\n  Methods: A mathematical modelling approach was chosen to simulate six\ndifferent testing strategies based on key input parameters (infection rate,\ntest characteristics, population size, testing capacity etc.). The situations\nin five countries (US, DE, UK, IT and SG) currently experiencing COVID-19\noutbreaks were simulated to reflect a broad variety of population sizes and\ntesting capacities. The primary study outcome measurements that were finalised\nprior to any data collection were time and number of tests required; number of\ncases identified; and number of false positives.\n  Findings: The performance of all tested methods depends on the input\nparameters, i.e. the specific circumstances of a screening campaign. To screen\none tenth of each country's population at an infection rate of 1% - e.g. when\nprioritising frontline medical staff and public workers -, realistic optimised\ntesting strategies enable such a campaign to be completed in ca. 29 days in the\nUS, 71 in the UK, 25 in Singapore, 17 in Italy and 10 in Germany (ca. eight\ntimes faster compared to individual testing). When infection rates are\nconsiderably lower, or when employing an optimal, yet logistically more complex\npooling method, the gains are more pronounced. Pool-based approaches also\nreduces the number of false positive diagnoses by 50%.\n  Interpretation: The results of this study provide a clear rationale for\nadoption of pool-based testing strategies to increase speed and throughput of\ntesting for SARS-CoV-2. The current individual testing approach unnecessarily\nwastes valuable time and resources.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:51:43 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 13:57:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["de Wolff", "Timo", ""], ["Pfl\u00fcger", "Dirk", ""], ["Rehme", "Michael", ""], ["Heuer", "Janin", ""], ["Bittner", "Martin-Immanuel", ""]]}, {"id": "2004.11953", "submitter": "Johannes Bleher", "authors": "Johannes Bleher, Michael Bleher and Thomas Dimpfl", "title": "From orders to prices: A stochastic description of the limit order book\n  to forecast intraday returns", "comments": "82 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR econ.EM q-fin.MF q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a microscopic model to describe the dynamics of the fundamental\nevents in the limit order book (LOB): order arrivals and cancellations. It is\nbased on an operator algebra for individual orders and describes their effect\non the LOB. The model inputs are arrival and cancellation rate distributions\nthat emerge from individual behavior of traders, and we show how prices and\nliquidity arise from the LOB dynamics. In a simulation study we illustrate how\nthe model works and highlight its sensitivity with respect to assumptions\nregarding the collective behavior of market participants. Empirically, we test\nthe model on a LOB snapshot of XETRA, estimate several linearized model\nspecifications, and conduct in- and out-of-sample forecasts.The in-sample\nresults based on contemporaneous information suggest that our model describes\nreturns very well, resulting in an adjusted $R^2$ of roughly 80%. In the more\nrealistic setting where only past information enters the model, we observe an\nadjusted $R^2$ around 15%. The direction of the next return can be predicted\n(out-of-sample) with an accuracy above 75% for time horizons below 10 minutes.\nOn average, we obtain an RMSPE that is 10 times lower than values documented in\nthe literature.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:28:54 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 22:33:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bleher", "Johannes", ""], ["Bleher", "Michael", ""], ["Dimpfl", "Thomas", ""]]}, {"id": "2004.12011", "submitter": "Sebastian Jaimungal", "authors": "\\'Alvaro Cartea, Sebastian Jaimungal, Tianyi Jia", "title": "Trading Foreign Exchange Triplets", "comments": "35 pages, 14 figures, 1 table", "journal-ref": "Forthcoming, SIAM J. Financial Mathematics, 2020", "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.MF q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the optimal trading strategy for a foreign exchange (FX) broker\nwho must liquidate a large position in an illiquid currency pair. To maximize\nrevenues, the broker considers trading in a currency triplet which consists of\nthe illiquid pair and two other liquid currency pairs. The liquid pairs in the\ntriplet are chosen so that one of the pairs is redundant. The broker is\nrisk-neutral and accounts for model ambiguity in the FX rates to make her\nstrategy robust to model misspecification. When the broker is ambiguity neutral\n(averse) the trading strategy in each pair is independent (dependent) of the\ninventory in the other two pairs in the triplet. We employ simulations to\nillustrate how the robust strategies perform. For a range of ambiguity aversion\nparameters, we find the mean Profit and Loss (P&L) of the strategy increases\nand the standard deviation of the P&L decreases as ambiguity aversion\nincreases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:35:45 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cartea", "\u00c1lvaro", ""], ["Jaimungal", "Sebastian", ""], ["Jia", "Tianyi", ""]]}, {"id": "2004.12012", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Shariq Mohammed, Arvind Rao, Veerabhadran\n  Baladandayuthapani", "title": "Integrative Bayesian models using Post-selective Inference: a case study\n  in Radiogenomics", "comments": "44 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying direct links between genomic pathways and clinical endpoints for\nhighly fatal diseases such as cancer is a formidable task. By selecting\nstatistically relevant associations between a wealth of intermediary variables\nsuch as imaging and genomic measurements, integrative analyses can potentially\nresult in sharper clinical models with interpretable parameters, in terms of\ntheir mechanisms. Estimates of uncertainty in the resulting models are however\nunreliable unless inference accounts for the preceding steps of selection. In\nthis article, we develop selection-aware Bayesian methods which are: (i)\namenable to a flexible class of integrative Bayesian models post a selection of\npromising variables via $\\ell_1$-regularized algorithms; (ii) enjoy\ncomputational efficiency due to a focus on sharp models with meaning; (iii)\nstrike a crucial tradeoff between the quality of model selection and\ninferential power. Central to our selection-aware workflow, a conditional\nlikelihood constructed with a reparameterization map is deployed for obtaining\nuncertainty estimates in integrative models. Investigating the potential of our\nmethods in a radiogenomic analysis, we successfully recover several important\ngene pathways and calibrate uncertainties for their associations with patient\nsurvival times.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:39:26 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 16:19:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Mohammed", "Shariq", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2004.12022", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Yishu Xue, Zhihua Ma", "title": "Bayesian Clustered Coefficients Regression with Auxiliary Covariates\n  Assistant Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In regional economics research, a problem of interest is to detect\nsimilarities between regions, and estimate their shared coefficients in\neconomics models. In this article, we propose a mixture of finite mixtures\n(MFM) clustered regression model with auxiliary covariates that account for\nsimilarities in demographic or economic characteristics over a spatial domain.\nOur Bayesian construction provides both inference for number of clusters and\nclustering configurations, and estimation for parameters for each cluster.\nEmpirical performance of the proposed model is illustrated through simulation\nexperiments, and further applied to a study of influential factors for monthly\nhousing cost in Georgia.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:21:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hu", "Guanyu", ""], ["Xue", "Yishu", ""], ["Ma", "Zhihua", ""]]}, {"id": "2004.12092", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Christoph Bergmeir, Sam Campbell, Deborah Scott, Dan\n  Lubman", "title": "Towards Accurate Predictions and Causal 'What-if' Analyses for Planning\n  and Policy-making: A Case Study in Emergency Medical Services Demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency Medical Services (EMS) demand load has become a considerable burden\nfor many government authorities, and EMS demand is often an early indicator for\nstress in communities, a warning sign of emerging problems. In this paper, we\nintroduce Deep Planning and Policy Making Net (DeepPPMNet), a Long Short-Term\nMemory network based, global forecasting and inference framework to forecast\nthe EMS demand, analyse causal relationships, and perform `what-if' analyses\nfor policy-making across multiple local government areas. Unless traditional\nunivariate forecasting techniques, the proposed method follows the global\nforecasting methodology, where a model is trained across all the available EMS\ndemand time series to exploit the potential cross-series information available.\nDeepPPMNet also uses seasonal decomposition techniques, incorporated in two\ndifferent training paradigms into the framework, to suit various\ncharacteristics of the EMS related time series data. We then explore causal\nrelationships using the notion of Granger Causality, where the global\nforecasting framework enables us to perform `what-if' analyses that could be\nused for the national policy-making process. We empirically evaluate our\nmethod, using a set of EMS datasets related to alcohol, drug use and self-harm\nin Australia. The proposed framework is able to outperform many\nstate-of-the-art techniques and achieve competitive results in terms of\nforecasting accuracy. We finally illustrate its use for policy-making in an\nexample regarding alcohol outlet licenses.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 09:03:10 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bandara", "Kasun", ""], ["Bergmeir", "Christoph", ""], ["Campbell", "Sam", ""], ["Scott", "Deborah", ""], ["Lubman", "Dan", ""]]}, {"id": "2004.12130", "submitter": "Philip Nadler", "authors": "Philip Nadler, Shuo Wang, Rossella Arcucci, Xian Yang, Yike Guo", "title": "An Epidemiological Modelling Approach for Covid19 via Data Assimilation", "comments": "Initial conference version accepted at International Conference of\n  Machine Learning(ICML) workshop. Extended journal version was published in\n  the European Journal of Epidemiology\n  (https://doi.org/10.1007/s10654-020-00676-7). Please cite as accordingly", "journal-ref": null, "doi": "10.1007/s10654-020-00676-7", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global pandemic of the 2019-nCov requires the evaluation of policy\ninterventions to mitigate future social and economic costs of quarantine\nmeasures worldwide. We propose an epidemiological model for forecasting and\npolicy evaluation which incorporates new data in real-time through variational\ndata assimilation. We analyze and discuss infection rates in China, the US and\nItaly. In particular, we develop a custom compartmental SIR model fit to\nvariables related to the epidemic in Chinese cities, named SITR model. We\ncompare and discuss model results which conducts updates as new observations\nbecome available. A hybrid data assimilation approach is applied to make\nresults robust to initial conditions. We use the model to do inference on\ninfection numbers as well as parameters such as the disease transmissibility\nrate or the rate of recovery. The parameterisation of the model is parsimonious\nand extendable, allowing for the incorporation of additional data and\nparameters of interest. This allows for scalability and the extension of the\nmodel to other locations or the adaption of novel data sources.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 12:46:36 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 16:11:32 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 12:48:52 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nadler", "Philip", ""], ["Wang", "Shuo", ""], ["Arcucci", "Rossella", ""], ["Yang", "Xian", ""], ["Guo", "Yike", ""]]}, {"id": "2004.12162", "submitter": "David Holtz", "authors": "David Holtz, Sinan Aral", "title": "Limiting Bias from Test-Control Interference in Online Marketplace\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an A/B test, the typical objective is to measure the total average\ntreatment effect (TATE), which measures the difference between the average\noutcome if all users were treated and the average outcome if all users were\nuntreated. However, a simple difference-in-means estimator will give a biased\nestimate of the TATE when outcomes of control units depend on the outcomes of\ntreatment units, an issue we refer to as test-control interference. Using a\nsimulation built on top of data from Airbnb, this paper considers the use of\nmethods from the network interference literature for online marketplace\nexperimentation. We model the marketplace as a network in which an edge exists\nbetween two sellers if their goods substitute for one another. We then simulate\nseller outcomes, specifically considering a \"status quo\" context and\n\"treatment\" context that forces all sellers to lower their prices. We use the\nsame simulation framework to approximate TATE distributions produced by using\nblocked graph cluster randomization, exposure modeling, and the Hajek estimator\nfor the difference in means. We find that while blocked graph cluster\nrandomization reduces the bias of the naive difference-in-means estimator by as\nmuch as 62%, it also significantly increases the variance of the estimator. On\nthe other hand, the use of more sophisticated estimators produces mixed\nresults. While some provide (small) additional reductions in bias and small\nreductions in variance, others lead to increased bias and variance. Overall,\nour results suggest that experiment design and analysis techniques from the\nnetwork experimentation literature are promising tools for reducing bias due to\ntest-control interference in marketplace experiments.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:54:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Holtz", "David", ""], ["Aral", "Sinan", ""]]}, {"id": "2004.12211", "submitter": "Kamran Javid Mr", "authors": "Kamran Javid, Will Handley, Mike Hobson, Anthony Lasenby", "title": "Compromise-free Bayesian neural networks", "comments": "https://github.com/PolyChord/PolyChordLite;\n  https://github.com/SuperKam91/bnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a thorough analysis of the relationship between the out-of-sample\nperformance and the Bayesian evidence (marginal likelihood) of Bayesian neural\nnetworks (BNNs), as well as looking at the performance of ensembles of BNNs,\nboth using the Boston housing dataset. Using the state-of-the-art in nested\nsampling, we numerically sample the full (non-Gaussian and multimodal) network\nposterior and obtain numerical estimates of the Bayesian evidence, considering\nnetwork models with up to 156 trainable parameters. The networks have between\nzero and four hidden layers, either $\\tanh$ or $ReLU$ activation functions, and\nwith and without hierarchical priors. The ensembles of BNNs are obtained by\ndetermining the posterior distribution over networks, from the posterior\nsamples of individual BNNs re-weighted by the associated Bayesian evidence\nvalues. There is good correlation between out-of-sample performance and\nevidence, as well as a remarkable symmetry between the evidence versus model\nsize and out-of-sample performance versus model size planes. Networks with\n$ReLU$ activation functions have consistently higher evidences than those with\n$\\tanh$ functions, and this is reflected in their out-of-sample performance.\nEnsembling over architectures acts to further improve performance relative to\nthe individual BNNs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:12:56 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:23:29 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 12:03:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Javid", "Kamran", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "2004.12250", "submitter": "Anuj Srivastava", "authors": "Anuj Srivastava", "title": "Agent-Level Pandemic Simulation (ALPS) for Analyzing Effects of Lockdown\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an agent-level simulation model, termed ALPS, for\nsimulating the spread of an infectious disease in a confined community. The\nmechanism of transmission is agent-to-agent contact, using parameters reported\nfor Corona COVID-19 pandemic. The main goal of the ALPS simulation is analyze\neffects of preventive measures -- imposition and lifting of lockdown norms --\non the rates of infections, fatalities and recoveries. The model assumptions\nand choices represent a balance between competing demands of being realistic\nand being efficient for real-time inferences. The model provides quantification\nof gains in reducing casualties by imposition and maintenance of restrictive\nmeasures in place.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 23:14:16 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Srivastava", "Anuj", ""]]}, {"id": "2004.12369", "submitter": "Samuele Centorrino", "authors": "Samuele Centorrino and Mar\\'ia P\\'erez-Urdiales", "title": "Maximum Likelihood Estimation of Stochastic Frontier Models with\n  Endogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a maximum likelihood estimator of stochastic frontier\nmodels with endogeneity in cross-section data when the composite error term may\nbe correlated with inputs and environmental variables. Our framework is a\ngeneralization of the normal half-normal stochastic frontier model with\nendogeneity. We derive the likelihood function in closed form using three\nfundamental assumptions: the existence of control functions that fully capture\nthe dependence between regressors and unobservables; the conditional\nindependence of the two error components given the control functions; and the\nconditional distribution of the stochastic inefficiency term given the control\nfunctions being a folded normal distribution. We also provide a Battese-Coelli\nestimator of technical efficiency. Our estimator is computationally fast and\neasy to implement. We study some of its asymptotic properties, and we showcase\nits finite sample behavior in Monte-Carlo simulations and an empirical\napplication to farmers in Nepal.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:51:11 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 16:16:31 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 15:36:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Centorrino", "Samuele", ""], ["P\u00e9rez-Urdiales", "Mar\u00eda", ""]]}, {"id": "2004.12489", "submitter": "David Holtz", "authors": "David Holtz, Ruben Lobel, Inessa Liskovich, Sinan Aral", "title": "Reducing Interference Bias in Online Marketplace Pricing Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online marketplace designers frequently run A/B tests to measure the impact\nof proposed product changes. However, given that marketplaces are inherently\nconnected, total average treatment effect estimates obtained through Bernoulli\nrandomized experiments are often biased due to violations of the stable unit\ntreatment value assumption. This can be particularly problematic for\nexperiments that impact sellers' strategic choices, affect buyers' preferences\nover items in their consideration set, or change buyers' consideration sets\naltogether. In this work, we measure and reduce bias due to interference in\nonline marketplace experiments by using observational data to create clusters\nof similar listings, and then using those clusters to conduct\ncluster-randomized field experiments. We provide a lower bound on the magnitude\nof bias due to interference by conducting a meta-experiment that randomizes\nover two experiment designs: one Bernoulli randomized, one cluster randomized.\nIn both meta-experiment arms, treatment sellers are subject to a different\nplatform fee policy than control sellers, resulting in different prices for\nbuyers. By conducting a joint analysis of the two meta-experiment arms, we find\na large and statistically significant difference between the total average\ntreatment effect estimates obtained with the two designs, and estimate that\n32.60% of the Bernoulli-randomized treatment effect estimate is due to\ninterference bias. We also find weak evidence that the magnitude and/or\ndirection of interference bias depends on extent to which a marketplace is\nsupply- or demand-constrained, and analyze a second meta-experiment to\nhighlight the difficulty of detecting interference bias when treatment\ninterventions require intention-to-treat analysis.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:09:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Holtz", "David", ""], ["Lobel", "Ruben", ""], ["Liskovich", "Inessa", ""], ["Aral", "Sinan", ""]]}, {"id": "2004.12508", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, Olivier Teboul, Quentin Berthet, Arnaud Doucet,\n  Jean-Philippe Vert", "title": "Noisy Adaptive Group Testing using Bayesian Sequential Experimental\n  Design", "comments": "Latest version, with updated experiments, new conclusions on LBP vs\n  SMC decoding and new approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the infection prevalence of a disease is low, Dorfman showed 80 years\nago that testing groups of people can prove more efficient than testing people\nindividually. Our goal in this paper is to propose new group testing algorithms\nthat can operate in a noisy setting (tests can be mistaken) to decide\nadaptively (looking at past results) which groups to test next, with the goal\nto converge to a good detection, as quickly, and with as few tests as possible.\nWe cast this problem as a Bayesian sequential experimental design problem.\nUsing the posterior distribution of infection status vectors for $n$ patients,\ngiven observed tests carried out so far, we seek to form groups that have a\nmaximal utility. We consider utilities such as mutual information, but also\nquantities that have a more direct relevance to testing, such as the AUC of the\nROC curve of the test. Practically, the posterior distributions on $\\{0,1\\}^n$\nare approximated by sequential Monte Carlo (SMC) samplers and the utility\nmaximized by a greedy optimizer. Our procedures show in simulations significant\nimprovements over both adaptive and non-adaptive baselines, and are far more\nefficient than individual tests when disease prevalence is low. Additionally,\nwe show empirically that loopy belief propagation (LBP), widely regarded as the\nSoTA decoder to decide whether an individual is infected or not given previous\ntests, can be unreliable and exhibit oscillatory behavior. Our SMC decoder is\nmore reliable, and can improve the performance of other group testing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:41:33 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 22:49:38 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 21:10:58 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 17:55:11 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 15:10:39 GMT"}, {"version": "v6", "created": "Wed, 22 Jul 2020 08:19:33 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cuturi", "Marco", ""], ["Teboul", "Olivier", ""], ["Berthet", "Quentin", ""], ["Doucet", "Arnaud", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "2004.12741", "submitter": "Takashi Tanaka", "authors": "Takashi S. T. Tanaka", "title": "Assessment of design and analysis frameworks for on-farm experimentation\n  through a simulation study of wheat yield in Japan", "comments": "12 pages, 4 figures, Accepted by Precision Agriculture", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-farm experiments can provide farmers with information on more efficient\ncrop management in their own fields. Developments in precision agricultural\ntechnologies, such as yield monitoring and variable-rate application\ntechnology, allow farmers to implement on-farm experiments. Research frameworks\nincluding the experimental design and the statistical analysis method strongly\ninfluences the precision of the experiment. Conventional statistical approaches\n(e.g., ordinary least squares regression) may not be appropriate for on-farm\nexperiments because they are not capable of accurately accounting for the\nunderlying spatial variation in a particular response variable (e.g., yield\ndata). The effects of experimental designs and statistical approaches on type I\nerror rates and estimation accuracy were explored through a simulation study\nhypothetically conducted on experiments in three wheat fields in Japan.\nIsotropic and anisotropic spatial linear mixed models were established for\ncomparison with ordinary least squares regression models. The repeated designs\nwere not sufficient to reduce both the risk of a type I error and the\nestimation bias on their own. A combination of a repeated design and an\nanisotropic model is sometimes required to improve the precision of the\nexperiments. Model selection should be performed to determine whether the\nanisotropic model is required for analysis of any specific field. The\nanisotropic model had larger standard errors than the other models, especially\nwhen the estimates had large biases. This finding highlights an advantage of\nanisotropic models since they enable experimenters to cautiously consider the\nreliability of the estimates when they have a large bias.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:29:02 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:08:20 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tanaka", "Takashi S. T.", ""]]}, {"id": "2004.12755", "submitter": "David Norris", "authors": "David C. Norris", "title": "Retrospective analysis of a fatal dose-finding trial", "comments": "5 pages, 4 figures, 3 tables, 19 references with added DOI hyperlinks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The commonplace description of phase 1 clinical trials in oncology as\n\"primarily concerned with safety\" is belied by their near universal adoption of\ndose-escalation practices which are inherently unsafe. In contrast with dose\ntitration, cohort-wise dose escalation regards patients as exchangeable, an\nindefensible assumption in the face of widely appreciated inter-individual\nheterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have\npreviously advanced this argument in terms of a precautionary coherence\nprinciple that brings the well-known coherence notion of Cheung (2005) into\ncontact with modern imperatives of patient-centeredness and precision dosing.\nHere, however, I explore these matters in some mechanistic detail by analyzing\na trial of the bispecific T cell engager AFM11, in which a fatal toxicity\noccurred. To this end, I develop a Bayesian dose-response model for a single\nordinal toxicity. By constructing this model's priors to align with the AFM11\ntrial as designed and conducted, I demonstrate the incompatibility of that\ndesign with any reasonable expectation of safety. Indeed, the model readily\nyields prospective estimates of toxic response probabilities that suggest the\nfatality in this trial could have been foreseen as likely.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:58:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Norris", "David C.", ""]]}, {"id": "2004.12766", "submitter": "Nikolai Gagunashvili", "authors": "Nikolay Gagunashvili", "title": "Parametric unfolding. Method and restrictions", "comments": "14 pages, 9 figures", "journal-ref": "Eur. Phys. J. Plus 135, 540 (2020)", "doi": "10.1140/epjp/s13360-020-00556-9", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric unfolding of a true distribution distorted due to finite\nresolution and limited efficiency for the registration of individual events is\ndiscussed. Details of the computational algorithm of the unfolding procedure\nare presented.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:11:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Gagunashvili", "Nikolay", ""]]}, {"id": "2004.12782", "submitter": "Himanshu Tyagi", "authors": "Aditya Gopalan and Himanshu Tyagi", "title": "How Reliable are Test Numbers for Revealing the COVID-19 Ground Truth\n  and Applying Interventions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG q-bio.PE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of confirmed cases of COVID-19 is often used as a proxy for the\nactual number of ground truth COVID-19 infected cases in both public discourse\nand policy making. However, the number of confirmed cases depends on the\ntesting policy, and it is important to understand how the number of positive\ncases obtained using different testing policies reveals the unknown ground\ntruth. We develop an agent-based simulation framework in Python that can\nsimulate various testing policies as well as interventions such as lockdown\nbased on them. The interaction between the agents can take into account various\ncommunities and mobility patterns. A distinguishing feature of our framework is\nthe presence of another `flu'-like illness with symptoms similar to COVID-19,\nthat allows us to model the noise in selecting the pool of patients to be\ntested. We instantiate our model for the city of Bengaluru in India, using\ncensus data to distribute agents geographically, and traffic flow mobility data\nto model long-distance interactions and mixing. We use the simulation framework\nto compare the performance of three testing policies: Random Symptomatic\nTesting (RST), Contact Tracing (CT), and a new Location Based Testing policy\n(LBT). We observe that if a sufficient fraction of symptomatic patients come\nout for testing, then RST can capture the ground truth quite closely even with\nvery few daily tests. However, CT consistently captures more positive cases.\nInterestingly, our new LBT, which is operationally less intensive than CT,\ngives performance that is comparable with CT. In another direction, we compare\nthe efficacy of these three testing policies in enabling lockdown, and observe\nthat CT flattens the ground truth curve maximally, followed closely by LBT, and\nsignificantly better than RST.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:39:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gopalan", "Aditya", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2004.12972", "submitter": "Yanhong Zhou", "authors": "Yanhong Zhou, J.Jack Lee, Shunguang Wang, Stuart Bailey, and Ying Yuan", "title": "Incorporating historical information to improve phase I clinical trial\n  designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating historical data or real-world evidence has a great potential to\nimprove the efficiency of phase I clinical trials and to accelerate drug\ndevelopment. For model-based designs, such as the continuous reassessment\nmethod (CRM), this can be conveniently carried out by specifying a \"skeleton,\"\ni.e., the prior estimate of dose limiting toxicity (DLT) probability at each\ndose. In contrast, little work has been done to incorporate historical data or\nreal-world evidence into model-assisted designs, such as the Bayesian optimal\ninterval (BOIN), keyboard, and modified toxicity probability interval (mTPI)\ndesigns. This has led to the misconception that model-assisted designs cannot\nincorporate prior information. In this paper, we propose a unified framework\nthat allows for incorporating historical data or real-world evidence into\nmodel-assisted designs. The proposed approach uses the well-established\n\"skeleton\" approach, combined with the concept of prior effective sample size,\nthus it is easy to understand and use. More importantly, our approach maintains\nthe hallmark of model-assisted designs: simplicity---the dose\nescalation/de-escalation rule can be tabulated prior to the trial conduct.\nExtensive simulation studies show that the proposed method can effectively\nincorporate prior information to improve the operating characteristics of\nmodel-assisted designs, similarly to model-based designs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:31:29 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 23:06:54 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 18:34:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhou", "Yanhong", ""], ["Lee", "J. Jack", ""], ["Wang", "Shunguang", ""], ["Bailey", "Stuart", ""], ["Yuan", "Ying", ""]]}, {"id": "2004.13067", "submitter": "Mattia Zanella", "authors": "G. Albi, L. Pareschi, M. Zanella", "title": "Control with uncertain data of socially structured compartmental\n  epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of containment measures to reduce the amplitude of the epidemic\npeak is a key aspect in tackling the rapid spread of an epidemic. Classical\ncompartmental models must be modified and studied to correctly describe the\neffects of forced external actions to reduce the impact of the disease. The\nimportance of social structure, such as the age dependence that proved\nessential in the recent COVID-19 pandemic, must be considered, and in addition,\nthe available data are often incomplete and heterogeneous, so a high degree of\nuncertainty must be incorporated into the model from the beginning. In this\nwork we address these aspects, through an optimal control formulation of a\nsocially structured epidemic model in presence of uncertain data. After the\nintroduction of the optimal control problem, we formulate an instantaneous\napproximation of the control that allows us to derive new feedback controlled\ncompartmental models capable of describing the epidemic peak reduction. The\nneed for long-term interventions shows that alternative actions based on the\nsocial structure of the system can be as effective as the more expensive global\nstrategy. The timing and intensity of interventions, however, is particularly\nrelevant in the case of uncertain parameters on the actual number of infected\npeople. Simulations related to data from the first wave of the recent COVID-19\noutbreak in Italy are presented and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:03:57 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 21:16:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Albi", "G.", ""], ["Pareschi", "L.", ""], ["Zanella", "M.", ""]]}, {"id": "2004.13235", "submitter": "Rodrigo S. Targino", "authors": "Takaaki Koike and Yuri F. Saporito and Rodrigo S. Targino", "title": "Avoiding zero probability events when computing Value at Risk\n  contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the process of risk allocation for a generic\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR).\nWe recast the traditional Euler contributions from an expectation conditional\non an event of zero probability to a ratio involving conditional expectations\nwhose conditioning events have stricktly positive probability. We derive an\nanalytical form of the proposed representation of VaR contributions for various\nparametric models. Our numerical experiments show that the estimator using this\nnovel representation outperforms the standard Monte Carlo estimator in terms of\nbias and variance. Moreover, unlike the existing estimators, the proposed\nestimator is free from hyperparameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:23:44 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 16:51:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Koike", "Takaaki", ""], ["Saporito", "Yuri F.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "2004.13318", "submitter": "Jiangbin Lyu Dr.", "authors": "Jiangbin Lyu and Rui Zhang", "title": "Hybrid Active/Passive Wireless Network Aided by Intelligent Reflecting\n  Surface: System Modeling and Performance Analysis", "comments": "To appear in IEEE Trans. Wireless Commun.. First work to model\n  large-scale multi-cell hybrid network with distributed active BSs and passive\n  IRSs subjected to inter-cell interference, and characterize distributions of\n  signal/interference power, SINR and spatial throughput based on stochastic\n  geometry. Unveil that an optimal IRS/BS density ratio exists in achieving\n  sustainable capacity growth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent reflecting surface (IRS) is a new and promising paradigm to\nsubstantially improve the spectral and energy efficiency of wireless networks,\nby constructing favorable communication channels via tuning massive low-cost\npassive reflecting elements. Despite recent advances in the link-level\nperformance optimization for various IRS-aided wireless systems, it still\nremains an open problem whether the large-scale deployment of IRSs in wireless\nnetworks can be a cost-effective solution to achieve their sustainable capacity\ngrowth in the future. To address this problem, we study in this paper a new\nhybrid wireless network comprising both active base stations (BSs) and passive\nIRSs, and characterize its achievable spatial throughput in the downlink as\nwell as other pertinent key performance metrics averaged over both channel\nfading and random locations of the deployed BSs/IRSs therein based on\nstochastic geometry. Compared to prior works on characterizing the performance\nof wireless networks with active BSs only, our analysis needs to derive the\npower distributions of both the signal and interference reflected by\ndistributed IRSs in the network under spatially correlated channels, which\nexhibit channel hardening effects when the number of IRS elements becomes\nlarge. Extensive numerical results are presented to validate our analysis and\ndemonstrate the effectiveness of deploying distributed IRSs in enhancing the\nhybrid network throughput against the conventional network without IRS, which\nsignificantly boosts the signal power but results in only marginally increased\ninterference in the network. Moreover, it is unveiled that there exists an\noptimal IRS/BS density ratio that maximizes the hybrid network throughput\nsubject to a total deployment cost given their individual costs, while the\nconventional network without IRS is generally suboptimal in terms of throughput\nper unit cost.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:08:37 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 09:30:35 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 02:33:02 GMT"}, {"version": "v4", "created": "Sat, 15 May 2021 11:34:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lyu", "Jiangbin", ""], ["Zhang", "Rui", ""]]}, {"id": "2004.13372", "submitter": "Elena Castilla", "authors": "N. Balakrishnan, E. Castilla, N. Martin and L. Pardo", "title": "Power divergence approach for one-shot device testing under competing\n  risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on one-shot devices assume that there is only one possible cause of\ndevice failure. However, in practice, it is often the case that the products\nunder study can experience any one of various possible causes of failure.\nRobust estimators and Wald-type tests are developed here for the case of\none-shot devices under competing risks. An extensive simulation study\nillustrates the robustness of these divergence-based estimators and test\nprocedures based on them. A data-driven procedure is proposed for choosing the\noptimal estimator for any given data set which is then applied to an example in\nthe context of survival analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:04:56 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Balakrishnan", "N.", ""], ["Castilla", "E.", ""], ["Martin", "N.", ""], ["Pardo", "L.", ""]]}, {"id": "2004.13382", "submitter": "Elena Castilla", "authors": "N. Balakrishnan, E. Castilla, N. Martin and L. Pardo", "title": "Divergence-based robust inference under proportional hazards model for\n  one-shot device life-test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop robust estimators and tests for one-shot device\ntesting under proportional hazards assumption based on divergence measures.\nThrough a detailed Monte Carlo simulation study and a numerical example, the\ndeveloped inferential procedures are shown to be more robust than the classical\nprocedures, based on maximum likelihood estimators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:21:57 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Balakrishnan", "N.", ""], ["Castilla", "E.", ""], ["Martin", "N.", ""], ["Pardo", "L.", ""]]}, {"id": "2004.13427", "submitter": "Johannes Schumacher", "authors": "Johannes Schumacher, Marius Hauglin, Rasmus Astrup, Johannes\n  Breidenbach", "title": "Mapping forest age using National Forest Inventory, airborne laser\n  scanning, and Sentinel-2 data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The age of forest stands is critical information for many aspects of forest\nmanagement and conservation but area-wide information about forest stand age\noften does not exist. In this study, we developed regression models for\nlarge-scale area-wide prediction of age in Norwegian forests. For model\ndevelopment we used more than 4800 plots of the Norwegian National Forest\nInventory (NFI) distributed over Norway between 58{\\deg} and 65{\\deg} northern\nlatitude in a 181,773 km2 study area. Predictor variables were based on\nairborne laser scanning (ALS), Sentinel-2, and existing public map data. We\nperformed model validation on an independent data set consisting of 63 spruce\nstands with known age. The best modelling strategy was to fit independent\nlinear regression models to each observed site index (SI) level and using a SI\nprediction map in the application of the models. The most important predictor\nvariable was an upper percentile of the ALS heights, and\nroot-mean-squared-errors (RMSE) ranged between 3 and 31 years (6% to 26%) for\nSI-specific models, and 21 years (25%) on average. Mean deviance (MD) ranged\nbetween -1 and 3 years. The models improved with increasing SI and the RMSE\nwere largest for low SI stands older than 100 years. Using a mapped SI, which\nis required for practical applications, RMSE and MD on plot-level ranged from\n19 to 56 years (29% to 53%), and 5 to 37 years (5% to 31%), respectively. For\nthe validation stands, the RMSE and MD were 12 (22%) and 2 years (3%). Tree\nheight estimated from airborne laser scanning and predicted site index were the\nmost important variables in the models describing age. Overall, we obtained\ngood results, especially for stands with high SI, that could be considered for\npractical applications but see considerable potential for improvements, if\nbetter SI maps were available.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:21:47 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Schumacher", "Johannes", ""], ["Hauglin", "Marius", ""], ["Astrup", "Rasmus", ""], ["Breidenbach", "Johannes", ""]]}, {"id": "2004.13459", "submitter": "Laura Forastiere", "authors": "Davide Del Prete, Laura Forastiere, Valerio Leone Sciabolazza", "title": "Causal Inference on Networks under Continuous Treatment Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology to draw causal inference in a\nnon-experimental setting subject to network interference. Specifically, we\ndevelop a generalized propensity score-based estimator that allows us to\nestimate both direct and spillover effects of a continuous treatment, which\nspreads through weighted and directed edges of a network. To showcase this\nmethodology, we investigate whether and how spillover effects shape the optimal\nlevel of policy interventions in agricultural markets. Our results show that,\nin this context, neglecting interference may lead to a downward bias when\nassessing policy effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:28:30 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Del Prete", "Davide", ""], ["Forastiere", "Laura", ""], ["Sciabolazza", "Valerio Leone", ""]]}, {"id": "2004.13464", "submitter": "Konstantina Chalkou", "authors": "Konstantina Chalkou, Ewout Steyerberg, Matthias Egger, Andrea Manca,\n  Fabio Pellegrini, Georgia Salanti", "title": "A two-stage prediction model for heterogeneous effects of many treatment\n  options: application to drugs for Multiple Sclerosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effects vary across different patients and estimation of this\nvariability is important for clinical decisions. The aim is to develop a model\nto estimate the benefit of alternative treatment options for individual\npatients. Hence, we developed a two-stage prediction model for heterogeneous\ntreatment effects, by combining prognosis research and network meta-analysis\nmethods when individual patient data is available. In a first stage, we develop\na prognostic model and we predict the baseline risk of the outcome. In the\nsecond stage, we use this baseline risk score from the first stage as a single\nprognostic factor and effect modifier in a network meta-regression model. We\napply the approach to a network meta-analysis of three randomized clinical\ntrials comparing the relapse rate in Natalizumab, Glatiramer Acetate and\nDimethyl Fumarate including 3590 patients diagnosed with relapsing-remitting\nmultiple sclerosis. We find that the baseline risk score modifies the relative\nand absolute treatment effects. Several patient characteristics such as age and\ndisability status impact on the baseline risk of relapse, and this in turn\nmoderates the benefit that may be expected for each of the treatments. For\nhigh-risk patients, the treatment that minimizes the risk to relapse in two\nyears is Natalizumab, whereas for low-risk patients Dimethyl Fumarate Fumarate\nmight be a better option. Our approach can be easily extended to all outcomes\nof interest and has the potential to inform a personalised treatment approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:57:26 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:59:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chalkou", "Konstantina", ""], ["Steyerberg", "Ewout", ""], ["Egger", "Matthias", ""], ["Manca", "Andrea", ""], ["Pellegrini", "Fabio", ""], ["Salanti", "Georgia", ""]]}, {"id": "2004.13483", "submitter": "Shonosuke Sugasawa", "authors": "Genya Kobayashi, Shonosuke Sugasawa, Hiromasa Tamae, Takayuki Ozu", "title": "Predicting Infection of COVID-19 in Japan: State Space Modeling Approach", "comments": "12 pages (main part) + 9 pages (supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of confirmed cases of the coronavirus disease (COVID-19) in Japan\nhas been increasing day by day and has had a serious impact on the society\nespecially after the declaration of the state of emergency on April 7, 2020.\nThis study analyzes the real time data from March 1 to April 22, 2020 by\nadopting a sophisticated statistical modeling tool based on the state space\nmodel combined with the well-known susceptible-exposed-infected (SIR) model.\nThe model estimation and forecasting are conducted using the Bayesian\nmethodology. The present study provides the parameter estimates of the unknown\nparameters that critically determine the epidemic process derived from the SIR\nmodel and prediction of the future transition of the infectious proportion\nincluding the size and timing of the epidemic peak with the prediction\nintervals that naturally accounts for the uncertainty. The prediction results\nunder various scenarios reveals that the temporary reduction in the infection\nrate until the planned lifting of the state on May 6 will only delay the\nepidemic peak slightly. In order to minimize the spread of the epidemic, it is\nstrongly suggested that an intervention is carried out for an extended period\nof time and that the government and individuals make a long term effort to\nreduce the infection rate even after the lifting.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:20:47 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kobayashi", "Genya", ""], ["Sugasawa", "Shonosuke", ""], ["Tamae", "Hiromasa", ""], ["Ozu", "Takayuki", ""]]}, {"id": "2004.13545", "submitter": "Marco Guerzoni", "authors": "Anna Velyka and Marco Guerzoni", "title": "The more you ask, the less you get: the negative impact of collaborative\n  overload on performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the possible negative impact of excessive collaboration\non the performance of top employees. With the rise of participatory culture and\ndevelopments in communications technology, management practices require greater\nconceptual awareness about possible outcomes of increased organizational\ninterconnectivity. While there exists a sound theoretical basis for possible\nburdens brought by collaborative overload, the literature never really manage\nto measure and empirically test this phenomenon. We address this gap by\ndeveloping a methodological framework for the identification of organizational\nactors at risk of operational capacity overload. Drawing on social network\nanalysis as the widely applied approach for the estimation of employees'\ninvolvement in the information exchange networks, this paper describes\npotential personal and organizational causes leading to the emergence of\ncollaborative overload. Relying on primary data gathered through a survey\nconducted among employees in a large insurance company, we present a testable\nmodel for overload detection. A second merit of the paper consists in finding a\nnovel identification strategy for empirical works on cross-sectional network\ndata, which often face the issue of endogeneity. This research suggests that\nactive collaborative activity does not cause a decrease throughout every aspect\nof performance. We found that expertise sharing depends on a few key players\nwho take core knowledge assets upon themselves and thus run higher risks of\nexposure to overload.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:16:49 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Velyka", "Anna", ""], ["Guerzoni", "Marco", ""]]}, {"id": "2004.13611", "submitter": "Rachel Marceau West", "authors": "Devan V. Mehrotra and Rachel Marceau West", "title": "Survival Analysis Using a 5-Step Stratified Testing and Amalgamation\n  Routine in Randomized Clinical Trials", "comments": "40 pages; 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized clinical trials are often designed to assess whether a test\ntreatment prolongs survival relative to a control treatment. Increased patient\nheterogeneity, while desirable for generalizability of results, can weaken the\nability of common statistical approaches to detect treatment differences,\npotentially hampering the regulatory approval of safe and efficacious\ntherapies. A novel solution to this problem is proposed. A list of baseline\ncovariates that have the potential to be prognostic for survival under either\ntreatment is pre-specified in the analysis plan. At the analysis stage, using\nall observed survival times but blinded to patient-level treatment assignment,\n'noise' covariates are removed with elastic net Cox regression. The shortened\ncovariate list is used by a conditional inference tree algorithm to segment the\nheterogeneous trial population into subpopulations of prognostically\nhomogeneous patients (risk strata). After patient-level treatment unblinding, a\ntreatment comparison is done within each formed risk stratum and stratum-level\nresults are combined for overall statistical inference. The impressive\npower-boosting performance of our proposed 5-step stratified testing and\namalgamation routine (5-STAR), relative to that of the logrank test and other\ncommon approaches that do not leverage inherently structured patient\nheterogeneity, is illustrated using a hypothetical and two real datasets along\nwith simulation results. Furthermore, the importance of reporting stratum-level\ncomparative treatment effects (time ratios from accelerated failure time model\nfits in conjunction with model averaging and, as needed, hazard ratios from Cox\nproportional hazard model fits) is highlighted as a potential enabler of\npersonalized medicine. A fiveSTAR R package is available at\nhttps://github.com/rmarceauwest/fiveSTAR.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:44:41 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mehrotra", "Devan V.", ""], ["West", "Rachel Marceau", ""]]}, {"id": "2004.13632", "submitter": "Chinmay Patwardhan Mr.", "authors": "Chinmay Patwardhan", "title": "SARS-COV-2 Pandemic: Understanding the Impact of Lockdown in the Most\n  Affected States of India", "comments": "14 pages, 60 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SARS-COV-2 has stopped the world in its footsteps and a third of the\npopulation has been forced to stay at home. Here we present a comparative study\nof the performance of states of India, in curbing the spread of the disease,\nthat are most affected by the pandemic. We have done so based on the data\ncollected between 14th March, 2020 to 17th April, 2020. We do this by comparing\nthe smoothened time series and percentage changes along with change point\ndetection of the daily confirmed cases. We also discuss the different policies\nand strategies adopted by the states in curbing the disease and the ground\nlevel implementation. We have developed ARIMA(p,d,q) models where (p,d,q) are\nobtained by minimising the AIK(Akaike Information Criterion). These models are\nused to make forecasts for the country which show that by 7th May, 2020, India\nwould have around 50,237 confirmed cases and a doubling rate of 11 days. On the\nbasis of the performance, of each state, we argue that a local level strategy\nwhich is based on the demographics of that particular region be developed\ninstead of a central and uniform one.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:22:49 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Patwardhan", "Chinmay", ""]]}, {"id": "2004.13689", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir O Storvik, Paul E Grini, Melinka A Butenko", "title": "A Bayesian binomial regression model with latent Gaussian processes for\n  modelling DNA methylation", "comments": "10 pages, 3 tables, 3 figures", "journal-ref": "Austrian Journal of Statistics, 2020, 49(4), 46-56", "doi": "10.17713/ajs.v49i4.1124", "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epigenetic observations are represented by the total number of reads from a\ngiven pool of cells and the number of methylated reads, making it reasonable to\nmodel this data by a binomial distribution. There are numerous factors that can\ninfluence the probability of success in a particular region. Moreover, there is\na strong spatial (alongside the genome) dependence of these probabilities. We\nincorporate dependence on the covariates and the spatial dependence of the\nmethylation probability for observations from a pool of cells by means of a\nbinomial regression model with a latent Gaussian field and a logit link\nfunction. We apply a Bayesian approach including prior specifications on model\nconfigurations. We run a mode jumping Markov chain Monte Carlo algorithm\n(MJMCMC) across different choices of covariates in order to obtain the joint\nposterior distribution of parameters and models. This also allows finding the\nbest set of covariates to model methylation probability within the genomic\nregion of interest and individual marginal inclusion probabilities of the\ncovariates.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:44:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir O", ""], ["Grini", "Paul E", ""], ["Butenko", "Melinka A", ""]]}, {"id": "2004.13695", "submitter": "Carlos Meijide-Garc\\'ia", "authors": "Marcos Matabuena, Carlos Meijide-Garc\\'ia, Pablo Rodr\\'iguez-Mier,\n  V\\'ictor Lebor\\'an", "title": "COVID-19: Estimating spread in Spain solving an inverse problem with a\n  probabilistic model", "comments": "36 pag", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new probabilistic model to estimate the real spread of the\nnovel SARS-CoV-2 virus along regions or countries. Our model simulates the\nbehavior of each individual in a population according to a probabilistic model\nthrough an inverse problem; we estimate the real number of recovered and\ninfected people using mortality records. In addition, the model is dynamic in\nthe sense that it takes into account the policy measures introduced when we\nsolve the inverse problem. The results obtained in Spain have particular\npractical relevance: the number of infected individuals can be $17$ times\nhigher than the data provided by the Spanish government on April $26$ $th$ in\nthe worst-case scenario. Assuming that the number of fatalities reflected in\nthe statistics is correct, $9.8$ percent of the population may be contaminated\nor have already been recovered from the virus in Madrid, one of the most\naffected regions in Spain. However, if we assume that the number of fatalities\nis twice as high as the official numbers, the number of infections could have\nreached $19.5\\%$. In Galicia, one of the regions where the effect has been the\nleast, the number of infections does not reach $2.5 \\%$ . Based on our\nfindings, we can: i) estimate the risk of a new outbreak before Autumn if we\nlift the quarantine; ii) may know the degree of immunization of the population\nin each region; and iii) forecast or simulate the effect of the policies to be\nintroduced in the future based on the number of infected or recovered\nindividuals in the population.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:50:44 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 17:01:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Matabuena", "Marcos", ""], ["Meijide-Garc\u00eda", "Carlos", ""], ["Rodr\u00edguez-Mier", "Pablo", ""], ["Lebor\u00e1n", "V\u00edctor", ""]]}, {"id": "2004.13714", "submitter": "Divyam Aggarwal", "authors": "Divyam Aggarwal, Yash Kumar Singh, Dhish Kumar Saxena", "title": "On Learning Combinatorial Patterns to Assist Large-Scale Airline Crew\n  Pairing Optimization", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Airline Crew Pairing Optimization (CPO) aims at generating a set of legal\nflight sequences (crew pairings), to cover an airline's flight schedule, at\nminimum cost. It is usually performed using Column Generation (CG), a\nmathematical programming technique for guided search-space exploration. CG\nexploits the interdependencies between the current and the preceding\nCG-iteration for generating new variables (pairings) during the\noptimization-search. However, with the unprecedented scale and complexity of\nthe emergent flight networks, it has become imperative to learn higher-order\ninterdependencies among the flight-connection graphs, and utilize those to\nenhance the efficacy of the CPO. In first of its kind and what marks a\nsignificant departure from the state-of-the-art, this paper proposes a novel\nadaptation of the Variational Graph Auto-Encoder for learning plausible\ncombinatorial patterns among the flight-connection data obtained through the\nsearch-space exploration by an Airline Crew Pairing Optimizer, AirCROP\n(developed by the authors and validated by the research consortium's industrial\nsponsor, GE Aviation). The resulting flight-connection predictions are combined\non-the-fly using a novel heuristic to generate new pairings for the optimizer.\nThe utility of the proposed approach is demonstrated on large-scale (over 4200\nflights), real-world, complex flight-networks of US-based airlines,\ncharacterized by multiple hub-and-spoke subnetworks and several crew bases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 20:16:22 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 07:57:27 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 11:46:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Aggarwal", "Divyam", ""], ["Singh", "Yash Kumar", ""], ["Saxena", "Dhish Kumar", ""]]}, {"id": "2004.13775", "submitter": "Erich Greene", "authors": "E. J. Greene (1), P. Peduzzi (1), J. Dziura (2 and 1), C. Meng, M. E.\n  Miller (3), T. G. Travison (4), D. Esserman (1) ((1) Department of\n  Biostatistics, Yale School of Public Health, (2) Department of Emergency\n  Medicine, Yale School of Medicine, (3) Department of Emergency Medicine, Wake\n  Forest School of Medicine, (4) Marcus Institute for Aging Research, Hebrew\n  SeniorLife, Harvard Medical School)", "title": "Estimation of ascertainment bias and its effect on power in clinical\n  trials with time-to-event outcomes", "comments": "31 pages, 11 figures; submitted to Statistics in Medicine", "journal-ref": null, "doi": "10.1002/sim.8842", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the gold standard for clinical trials is to blind all parties --\nparticipants, researchers, and evaluators -- to treatment assignment, this is\nnot always a possibility. When some or all of the above individuals know the\ntreatment assignment, this leaves the study open to the introduction of\npost-randomization biases. In the Strategies to Reduce Injuries and Develop\nConfidence in Elders (STRIDE) trial, we were presented with the potential for\nthe unblinded clinicians administering the treatment, as well as the\nindividuals enrolled in the study, to introduce ascertainment bias into some\nbut not all events comprising the primary outcome. In this manuscript, we\npresent ways to estimate the ascertainment bias for a time-to-event outcome,\nand discuss its impact on the overall power of a trial versus changing of the\noutcome definition to a more stringent unbiased definition that restricts\nattention to measurements less subject to potentially differential assessment.\nWe found that for the majority of situations, it is better to revise the\ndefinition to a more stringent definition, as was done in STRIDE, even though\nfewer events may be observed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 18:56:43 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 17:18:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Greene", "E. J.", "", "2 and 1"], ["Peduzzi", "P.", "", "2 and 1"], ["Dziura", "J.", "", "2 and 1"], ["Meng", "C.", ""], ["Miller", "M. E.", ""], ["Travison", "T. G.", ""], ["Esserman", "D.", ""]]}, {"id": "2004.13797", "submitter": "Jackie Shen Dr.", "authors": "Jackie Jianhong Shen", "title": "A Stochastic LQR Model for Child Order Placement in Algorithmic Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR math.OC q-fin.CP q-fin.MF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Algorithmic Trading (\"Algo\") allows institutional investors and\ntraders to liquidate or establish big security positions in a fully automated\nor low-touch manner. Most existing academic or industrial Algos focus on how to\n\"slice\" a big parent order into smaller child orders over a given time horizon.\nFew models rigorously tackle the actual placement of these child orders.\nInstead, placement is mostly done with a combination of empirical signals and\nheuristic decision processes. A self-contained, realistic, and fully functional\nChild Order Placement (COP) model may never exist due to all the inherent\ncomplexities, e.g., fragmentation due to multiple venues, dynamics of limit\norder books, lit vs. dark liquidity, different trading sessions and rules. In\nthis paper, we propose a reductionism COP model that focuses exclusively on the\ninterplay between placing passive limit orders and sniping using aggressive\ntakeout orders. The dynamic programming model assumes the form of a stochastic\nlinear-quadratic regulator (LQR) and allows closed-form solutions under the\nbackward Bellman equations. Explored in detail are model assumptions and\ngeneral settings, the choice of state and control variables and the cost\nfunctions, and the derivation of the closed-form solutions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 20:03:12 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Shen", "Jackie Jianhong", ""]]}, {"id": "2004.13870", "submitter": "Anna Yanchenko", "authors": "Anna K. Yanchenko and Peter D. Hoff", "title": "Hierarchical Multidimensional Scaling for the Comparison of Musical\n  Performance Styles", "comments": "Published in the Annals of Applied Statistics\n  (https://projecteuclid.org/euclid.aoas/1608346888)", "journal-ref": "Annals of Applied Statistics, Volume 14, Number 4 (2020),\n  1581-1603", "doi": "10.1214/20-AOAS1391", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of stylistic differences between musical artists is of\nacademic interest to the music community, and is also useful for other\napplications such as music information retrieval and recommendation systems.\nInformation about stylistic differences can be obtained by comparing the\nperformances of different artists across common musical pieces. In this\narticle, we develop a statistical methodology for identifying and quantifying\nsystematic stylistic differences among artists that are consistent across audio\nrecordings of a common set of pieces, in terms of several musical features. Our\nfocus is on a comparison of ten different orchestras, based on data from audio\nrecordings of the nine Beethoven symphonies. As generative or fully parametric\nmodels of raw audio data can be highly complex, and more complex than necessary\nfor our goal of identifying differences between orchestras, we propose to\nreduce the data from a set of audio recordings down to pairwise distances\nbetween orchestras, based on different musical characteristics of the\nrecordings, such as tempo, dynamics, and timbre. For each of these\ncharacteristics, we obtain multiple pairwise distance matrices, one for each\nmovement of each symphony. We develop a hierarchical multidimensional scaling\n(HMDS) model to identify and quantify systematic differences between orchestras\nin terms of these three musical characteristics, and interpret the results in\nthe context of known qualitative information about the orchestras. This\nmethodology is able to recover several expected systematic similarities between\norchestras, as well as to identify some more novel results. For example, we\nfind that modern recordings exhibit a high degree of similarity to each other,\nas compared to older recordings.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:16:02 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 00:34:26 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yanchenko", "Anna K.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "2004.13880", "submitter": "Marios Papamichalis V.", "authors": "Papamichalis Marios", "title": "Bayesian Model Selection on Random Networks", "comments": "18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A general Bayesian framework for model selection on random network models\nregarding their features is considered. The goal is to develop a principle\nBayesian model selection approach to compare different fittable, not\nnecessarily nested, models for inference on those network realisations. The\ncriterion for random network models regarding the comparison is formulated via\nBayes factors and penalizing using the mostwidely used loss functions.\nParametrizations are different in different spaces. To overcome this problem we\nincorporate and encode different aspects of complexities in terms of observable\nspaces. Thus, given a range of values for a feature, network realisationsare\nextracted. The proposed principle approach is based on finding random network\nmodels, such that a reasonable trade off between the interested feature and the\ncomplexity of the model is preserved, avoiding overfitting problems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:39:27 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Marios", "Papamichalis", ""]]}, {"id": "2004.13892", "submitter": "Jiahua Chen", "authors": "Jiahua Chen, Yukun Liu, and James Zidek", "title": "Permutation tests under a rotating sampling plan with clustered data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a population consisting of clusters of sampling units, evolving\ntemporally, spatially, or according to other dynamics. We wish to monitor the\nevolution of its means, medians, or other parameters. For administrative\nconvenience and informativeness, clustered data are often collected via a\nrotating plan. Under rotating plans, the observations in the same clusters are\ncorrelated, and observations on the same unit collected on different occasions\nare also correlated. Ignoring this correlation structure may lead to invalid\ninference procedures. Accommodating cluster structure in parametric models is\ndifficult or will have a high level of misspecification risk. In this paper, we\nexplore exchangeability in clustered data collected via a rotating sampling\nplan to develop a permutation scheme for testing various hypotheses of\ninterest. We also introduce a semiparametric density ratio model to facilitate\nthe multiple population structure in rotating sampling plans. The combination\nensures the validity of the inference methods while extracting maximum\ninformation from the sampling plan. A simulation study indicates that the\nproposed tests firmly control the type I error whether or not the data are\nclustered. The use of the density ratio model improves the power of the tests.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:29:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chen", "Jiahua", ""], ["Liu", "Yukun", ""], ["Zidek", "James", ""]]}, {"id": "2004.13963", "submitter": "Xuekui Zhang", "authors": "Li Xing, Xuekui Zhang, Ardo van den Hout, Scott Hofer, Graciela Muniz\n  Terrera", "title": "Optimal Study Design for Reducing Variances of Coefficient Estimators in\n  Change-Point Models", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal studies, we observe measurements of the same variables at\ndifferent time points to track the changes in their pattern over time. In such\nstudies, scheduling of the data collection waves (i.e. time of participants'\nvisits) is often pre-determined to accommodate ease of project management and\ncompliance. Hence, it is common to schedule those visits at equally spaced time\nintervals. However, recent publications based on simulated experiments indicate\nthat the power of studies and the precision of model parameter estimators is\nrelated to the participants' visiting schemes.\n  In this paper, we consider the longitudinal studies that investigate the\nchanging pattern of a disease outcome, (e.g. the accelerated cognitive decline\nof senior adults). Such studies are often analyzed by the broken-stick model,\nconsisting of two segments of linear models connected at an unknown\nchange-point. We formulate this design problem into a high-dimensional\noptimization problem and derive its analytical solution. Based on this\nsolution, we propose an optimal design of the visiting scheme that maximizes\nthe power (i.e. reduce the variance of estimators) to identify the onset of\naccelerated decline. Using both simulation studies and evidence from real data,\nwe demonstrate our optimal design outperforms the standard equally-spaced\ndesign.\n  Applying our novel design to plan the longitudinal studies, researchers can\nimprove the power of detecting pattern change without collecting extra data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 05:34:57 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xing", "Li", ""], ["Zhang", "Xuekui", ""], ["Hout", "Ardo van den", ""], ["Hofer", "Scott", ""], ["Terrera", "Graciela Muniz", ""]]}, {"id": "2004.14016", "submitter": "Daisuke Kaji", "authors": "Daisuke Kaji, Kazuho Watanabe, Masahiro Kobayashi", "title": "Multi-Decoder RNN Autoencoder Based on Variational Bayes Method", "comments": "8 pages, 11 figures, accepted for publication in IJCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms have wide applications and play an important role in\ndata analysis fields including time series data analysis. However, in time\nseries analysis, most of the algorithms used signal shape features or the\ninitial value of hidden variable of a neural network. Little has been discussed\non the methods based on the generative model of the time series. In this paper,\nwe propose a new clustering algorithm focusing on the generative process of the\nsignal with a recurrent neural network and the variational Bayes method. Our\nexperiments show that the proposed algorithm not only has a robustness against\nfor phase shift, amplitude and signal length variations but also provide a\nflexible clustering based on the property of the variational Bayes method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:25:07 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kaji", "Daisuke", ""], ["Watanabe", "Kazuho", ""], ["Kobayashi", "Masahiro", ""]]}, {"id": "2004.14103", "submitter": "Lily Wang", "authors": "Li Wang, Guannan Wang, Lei Gao, Xinyi Li, Shan Yu, Myungjin Kim,\n  Yueying Wang and Zhiling Gu", "title": "Spatiotemporal Dynamics, Nowcasting and Forecasting of COVID-19 in the\n  United States", "comments": "37 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemic modeling is an essential tool to understand the spread of the novel\ncoronavirus and ultimately assist in disease prevention, policymaking, and\nresource allocation. In this article, we establish a state of the art interface\nbetween classic mathematical and statistical models and propose a novel\nspace-time epidemic modeling framework to study the spatial-temporal pattern in\nthe spread of infectious disease. We propose a quasi-likelihood approach via\nthe penalized spline approximation and alternatively reweighted least-squares\ntechnique to estimate the model. Furthermore, we provide a short-term and\nlong-term county-level prediction of the infected/death count for the U.S. by\naccounting for the control measures, health service resources, and other local\nfeatures. Utilizing spatiotemporal analysis, our proposed model enhances the\ndynamics of the epidemiological mechanism and dissects the spatiotemporal\nstructure of the spreading disease. To assess the uncertainty associated with\nthe prediction, we develop a projection band based on the envelope of the\nbootstrap forecast paths. The performance of the proposed method is evaluated\nby a simulation study. We apply the proposed method to model and forecast the\nspread of COVID-19 at both county and state levels in the United States.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:54:51 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:58:08 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 05:30:57 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 02:12:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wang", "Li", ""], ["Wang", "Guannan", ""], ["Gao", "Lei", ""], ["Li", "Xinyi", ""], ["Yu", "Shan", ""], ["Kim", "Myungjin", ""], ["Wang", "Yueying", ""], ["Gu", "Zhiling", ""]]}, {"id": "2004.14194", "submitter": "Kieran Kalair", "authors": "Kieran Kalair, Colm Connaughton, Pierfrancesco Alaimo Di Loro", "title": "A non-parametric Hawkes process model of primary and secondary accidents\n  on a UK smart motorway", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-exciting spatio-temporal point process is fitted to incident data from\nthe UK National Traffic Information Service to model the rates of primary and\nsecondary accidents on the M25 motorway in a 12-month period during 2017-18.\nThis process uses a background component to represent primary accidents, and a\nself-exciting component to represent secondary accidents. The background\nconsists of periodic daily and weekly components, a spatial component and a\nlong-term trend. The self-exciting components are decaying, unidirectional\nfunctions of space and time. These components are determined via kernel\nsmoothing and likelihood estimation. Temporally, the background is stable\nacross seasons with a daily double peak structure reflecting commuting\npatterns. Spatially, there are two peaks in intensity, one of which becomes\nmore pronounced during the study period. Self-excitation accounts for 6-7% of\nthe data with associated time and length scales around 100 minutes and 1\nkilometre respectively. In-sample and out-of-sample validation are performed to\nassess the model fit. When we restrict the data to incidents that resulted in\nlarge speed drops on the network, the results remain coherent.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:35:12 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 11:23:02 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kalair", "Kieran", ""], ["Connaughton", "Colm", ""], ["Di Loro", "Pierfrancesco Alaimo", ""]]}, {"id": "2004.14262", "submitter": "Marta Magnani", "authors": "Marta Magnani, Ilaria Baneschi, Mariasilvia Giamberini, Pietro Mosca,\n  Brunella Raco, Antonello Provenzale", "title": "Drivers of carbon fluxes in Alpine tundra: a comparison of three\n  empirical model approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high mountains, the effects of climate change are manifesting most\nrapidly. This is especially critical for the high-altitude carbon cycle, for\nwhich new feedbacks could be triggered. However, mountain carbon dynamics is\nonly partially known. In particular, models of the processes driving carbon\nfluxes in high-altitude grasslands and Alpine tundra need to be improved. Here,\nwe propose a comparison of three empirical approaches using systematic\nstatistical analysis, to identify the environmental variables controlling\n$CO_2$ fluxes. The methods were applied to a complete dataset of simultaneous\nin situ measurements of the net $CO_2$ exchange, ecosystem respiration and\nbasic environmental variables in three sampling sites in the same catchment.\nLarge year-to-year variations in the gross primary production (GPP) and\necosystem respiration (ER) dependences on solar irradiance and temperature were\nobserved,. We thus implemented a multi regression model in which additional\nvariables were introduced as perturbations of the standard exponential and\nrectangular hyperbolic functions for ER and GPP, respectively. A comparison of\nthis model with other common modelling strategies, showed the benefits of this\napproach, resulting in large explained variances (83% to 94%). The optimum\nensemble of variables explaining the inter- and intra-annual flux variability\nincluded solar irradiance, soil moisture and day of the year for GPP, and air\ntemperature, soil moisture, air pressure and day of the year for the ER, in\nagreement with other studies. The modelling approach discussed here provides a\nbasis for selecting drivers of carbon fluxes and understanding their role in\nhigh-altitude Alpine ecosystems, also allowing for future short-range\nassessments of local trends.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:16:26 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Magnani", "Marta", ""], ["Baneschi", "Ilaria", ""], ["Giamberini", "Mariasilvia", ""], ["Mosca", "Pietro", ""], ["Raco", "Brunella", ""], ["Provenzale", "Antonello", ""]]}, {"id": "2004.14423", "submitter": "Chad M. Topaz", "authors": "Jennifer Crodelle, Celeste Vallejo, Markus Schmidtchen, Chad M. Topaz,\n  Maria R. D'Orsogna", "title": "Impacts of California Proposition 47 on Crime in Santa Monica, CA", "comments": "41 pages, 19 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0251199", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine crime patterns in Santa Monica, California before and after\npassage of Proposition 47, a 2014 initiative that reclassified some non-violent\nfelonies to misdemeanors. We also study how the 2016 opening of four new light\nrail stations, and how more community-based policing starting in late 2018,\nimpacted crime. A series of statistical analyses are performed on reclassified\n(larceny, fraud, possession of narcotics, forgery, receiving/possessing stolen\nproperty) and non-reclassified crimes by probing publicly available databases\nfrom 2006 to 2019. We compare data before and after passage of Proposition 47,\ncity-wide and within eight neighborhoods. Similar analyses are conducted within\na 450 meter radius of the new transit stations. Reports of monthly reclassified\ncrimes increased city-wide by approximately 15% after enactment of Proposition\n47, with a significant drop observed in late 2018. Downtown exhibited the\nlargest overall surge. The reported incidence of larceny intensified throughout\nthe city. Two new train stations, including Downtown, reported significant\ncrime increases in their vicinity after service began. While the number of\nreported reclassified crimes increased after passage of Proposition 47, those\nnot affected by the new law decreased or stayed constant, suggesting that\nProposition 47 strongly impacted crime in Santa Monica. Reported crimes\ndecreased in late 2018 concurrent with the adoption of new policing measures\nthat enhanced outreach and patrolling. These findings may be relevant to law\nenforcement and policy-makers. Follow-up studies needed to confirm long-term\ntrends may be affected by the COVID-19 pandemic that drastically changed\nsocietal conditions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 18:37:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Crodelle", "Jennifer", ""], ["Vallejo", "Celeste", ""], ["Schmidtchen", "Markus", ""], ["Topaz", "Chad M.", ""], ["D'Orsogna", "Maria R.", ""]]}, {"id": "2004.14522", "submitter": "Andriy Olenko", "authors": "Nikolai Leonenko, Ravindi Nanayakkara, Andriy Olenko", "title": "Analysis of Spherical Monofractal and Multifractal Random Fields", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": "10.1007/s00477-020-01911-z", "report-no": null, "categories": "math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R\\'enyi function plays an important role in the analysis of multifractal\nrandom fields. For random fields on the sphere, there are three models in the\nliterature where the R\\'enyi function is known explicitly. The theoretical part\nof the article presents multifractal random fields on the sphere and develops\nspecific models where the R\\'enyi function can be computed explicitly. Then\nthese results are applied to the Cosmic Microwave Background Radiation data\ncollected from the Planck mission. The main statistical model used to describe\nthese data in the literature is isotropic Gaussian fields. We present numerical\nmultifractality studies and methodology based on simulating random fields,\ncomputing the R\\'enyi function and the multifractal spectrum for different\nscenarios and actual CMB data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:10:16 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 01:56:41 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 04:14:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Leonenko", "Nikolai", ""], ["Nanayakkara", "Ravindi", ""], ["Olenko", "Andriy", ""]]}, {"id": "2004.14733", "submitter": "Erik Scharw\\\"achter", "authors": "Erik Scharw\\\"achter and Emmanuel M\\\"uller", "title": "Does Terrorism Trigger Online Hate Speech? On the Association of Events\n  and Time Series", "comments": "19 pages, 8 figures, to appear in the Annals of Applied Statistics,\n  source code available at https://github.com/diozaka/pECA", "journal-ref": "Annals of Applied Statistics 14 (3) 2020, pp. 1285-1303", "doi": "10.1214/20-AOAS1338", "report-no": null, "categories": "stat.AP cs.CY cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech is ubiquitous on the Web. Recently, the offline causes that\ncontribute to online hate speech have received increasing attention. A\nrecurring question is whether the occurrence of extreme events offline\nsystematically triggers bursts of hate speech online, indicated by peaks in the\nvolume of hateful social media posts. Formally, this question translates into\nmeasuring the association between a sparse event series and a time series. We\npropose a novel statistical methodology to measure, test and visualize the\nsystematic association between rare events and peaks in a time series. In\ncontrast to previous methods for causal inference or independence tests on time\nseries, our approach focuses only on the timing of events and peaks, and no\nother distributional characteristics. We follow the framework of event\ncoincidence analysis (ECA) that was originally developed to correlate point\nprocesses. We formulate a discrete-time variant of ECA and derive all required\ndistributions to enable analyses of peaks in time series, with a special focus\non serial dependencies and peaks over multiple thresholds. The analysis gives\nrise to a novel visualization of the association via quantile-trigger rate\nplots. We demonstrate the utility of our approach by analyzing whether Islamist\nterrorist attacks in Western Europe and North America systematically trigger\nbursts of hate speech and counter-hate speech on Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:47:22 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 10:15:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Scharw\u00e4chter", "Erik", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "2004.14800", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath and Gianluca Baio", "title": "Methods for Population Adjustment with Limited Access to Individual\n  Patient Data: A Review and Simulation Study", "comments": "73 pages (34 are supplementary appendices and references), 8 figures,\n  2 tables. Full article (following Round 4 of minor revisions). arXiv admin\n  note: text overlap with arXiv:2008.05951", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population-adjusted indirect comparisons estimate treatment effects when\naccess to individual patient data is limited and there are cross-trial\ndifferences in effect modifiers. Popular methods include matching-adjusted\nindirect comparison (MAIC) and simulated treatment comparison (STC). There is\nlimited formal evaluation of these methods and whether they can be used to\naccurately compare treatments. Thus, we undertake a comprehensive simulation\nstudy to compare standard unadjusted indirect comparisons, MAIC and STC across\n162 scenarios. This simulation study assumes that the trials are investigating\nsurvival outcomes and measure continuous covariates, with the log hazard ratio\nas the measure of effect. MAIC yields unbiased treatment effect estimates under\nno failures of assumptions. The typical usage of STC produces bias because it\ntargets a conditional treatment effect where the target estimand should be a\nmarginal treatment effect. The incompatibility of estimates in the indirect\ncomparison leads to bias as the measure of effect is non-collapsible. Standard\nindirect comparisons are systematically biased, particularly under stronger\ncovariate imbalance and interaction effects. Standard errors and coverage rates\nare often valid in MAIC but the robust sandwich variance estimator\nunderestimates variability where effective sample sizes are small. Interval\nestimates for the standard indirect comparison are too narrow and STC suffers\nfrom bias-induced undercoverage. MAIC provides the most accurate estimates and,\nwith lower degrees of covariate overlap, its bias reduction outweighs the loss\nin effective sample size and precision under no failures of assumptions. An\nimportant future objective is the development of an alternative formulation to\nSTC that targets a marginal treatment effect.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:11:24 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 09:15:31 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 11:56:30 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 11:21:04 GMT"}, {"version": "v5", "created": "Wed, 2 Dec 2020 17:18:09 GMT"}, {"version": "v6", "created": "Wed, 10 Feb 2021 10:12:37 GMT"}, {"version": "v7", "created": "Tue, 13 Apr 2021 10:29:23 GMT"}, {"version": "v8", "created": "Wed, 2 Jun 2021 07:03:29 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2004.14817", "submitter": "Matthew Koslovsky", "authors": "Matthew D. Koslovsky, Kristi L. Hoffman, Carrie R. Daniel, Marina\n  Vannucci", "title": "A Bayesian model of microbiome data for simultaneous identification of\n  covariate associations and prediction of phenotypic outcomes", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major research questions regarding human microbiome studies is the\nfeasibility of designing interventions that modulate the composition of the\nmicrobiome to promote health and cure disease. This requires extensive\nunderstanding of the modulating factors of the microbiome, such as dietary\nintake, as well as the relation between microbial composition and phenotypic\noutcomes, such as body mass index (BMI). Previous efforts have modeled these\ndata separately, employing two-step approaches that can produce biased\ninterpretations of the results. Here, we propose a Bayesian joint model that\nsimultaneously identifies clinical covariates associated with microbial\ncomposition data and predicts a phenotypic response using information contained\nin the compositional data. Using spike-and-slab priors, our approach can handle\nhigh-dimensional compositional as well as clinical data. Additionally, we\naccommodate the compositional structure of the data via balances and\noverdispersion typically found in microbial samples. We apply our model to\nunderstand the relations between dietary intake, microbial samples, and BMI. In\nthis analysis, we find numerous associations between microbial taxa and dietary\nfactors that may lead to a microbiome that is generally more hospitable to the\ndevelopment of chronic diseases, such as obesity. Additionally, we demonstrate\non simulated data how our method outperforms two-step approaches and also\npresent a sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:21:07 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Koslovsky", "Matthew D.", ""], ["Hoffman", "Kristi L.", ""], ["Daniel", "Carrie R.", ""], ["Vannucci", "Marina", ""]]}, {"id": "2004.14851", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Magne Thoresen", "title": "A robust variable screening procedure for ultra-high dimensional data", "comments": "Pre-print, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in ultra-high dimensional regression problems has become\nan important issue. In such situations, penalized regression models may face\ncomputational problems and some pre screening of the variables may be\nnecessary. A number of procedures for such pre-screening has been developed;\namong them the sure independence screening (SIS) enjoys some popularity.\nHowever, SIS is vulnerable to outliers in the data, and in particular in small\nsamples this may lead to faulty inference. In this paper, we develop a new\nrobust screening procedure. We build on the density power divergence (DPD)\nestimation approach and introduce DPD-SIS and its extension iterative DPD-SIS.\nWe illustrate the behavior of the methods through extensive simulation studies\nand show that they are superior to both the original SIS and other robust\nmethods when there are outliers in the data. We demonstrate the claimed\nrobustness through use of influence functions, and we discuss appropriate\nchoice of the tuning parameter $\\alpha$. Finally, we illustrate its use on a\nsmall dataset from a study on regulation of lipid metabolism.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:05:15 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Thoresen", "Magne", ""]]}, {"id": "2004.14864", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski and Florian Ziel", "title": "Changes in electricity demand pattern in Europe due to COVID-19\n  shutdowns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SY eess.SY physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article covers electricity demand shift effects due to COVID-19 shutdowns\nin various European countries. We utilize high-dimensional regression\ntechniques to exploit the structural breaks in demand profiles due to the\nshutdowns. We discuss the findings with respect to coronavirus pandemic\nprogress and regulatory measures of the considered countries.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:29:23 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 09:16:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "2004.14912", "submitter": "Luiz Max Carvalho PhD", "authors": "Luiz Max Carvalho and Joseph G. Ibrahim", "title": "On the normalized power prior", "comments": "Code available at\n  https://github.com/maxbiostat/propriety_power_priors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power prior is a popular tool for constructing informative prior\ndistributions based on historical data. The method consists of raising the\nlikelihood to a discounting factor in order to control the amount of\ninformation borrowed from the historical data. It is customary to perform a\nsensitivity analysis reporting results for a range of values of the discounting\nfactor. However, one often wishes to assign it a prior distribution and\nestimate it jointly with the parameters, which in turn necessitates the\ncomputation of a normalising constant. In this paper we are concerned with how\nto recycle computations from a sensitivity analysis in order to approximately\nsample from joint posterior of the parameters and the discounting factor. We\nfirst show a few important properties of the normalising constant and then use\nthese results to motivate a bisection-type algorithm for computing it on a\nfixed budget of evaluations. We give a large array of illustrations and discuss\ncases where the normalising constant is known in closed-form and where it is\nnot. We show that the proposed method produces approximate posteriors that are\nvery close to the exact distributions when those are available and also\nproduces posteriors that cover the data-generating parameters with higher\nprobability in the intractable case. Our results show that proper inclusion the\nnormalising constant is crucial to the correct quantification of uncertainty\nand that the proposed method is an accurate and easy to implement technique to\ninclude this normalisation, being applicable to a large class of models.\n  Key-words: Doubly-intractable; elicitation; historical data; normalisation;\npower prior; sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:11:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Carvalho", "Luiz Max", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2004.14987", "submitter": "Ulrike von Luxburg", "authors": "Sascha Meyen, Iris A. Zerweck, Catarina Amado, Ulrike von Luxburg,\n  Volker H. Franz", "title": "Advancing Research on Unconscious Priming: When can Scientists Claim an\n  Indirect Task Advantage?", "comments": "Accepted for Publication, Journal of Experimental Psychology: General", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current literature holds that many cognitive functions can be performed\noutside consciousness. Evidence for this view comes from unconscious priming.\nIn a typical experiment, visual stimuli are masked, such that participants are\nclose to chance when directly asked to which of two categories the stimuli\nbelong. This close-to-zero sensitivity is seen as evidence that participants\ncannot consciously report the category of the masked stimuli. Nevertheless, the\ncategory of the masked stimuli can indirectly affect responses to other stimuli\n(e.g., reaction times or brain activity). Priming is therefore seen as evidence\nthat there is still some (albeit unconscious) sensitivity to the stimulus\ncategories, thereby indicating processing outside consciousness. Although this\n\"standard reasoning of unconscious priming\" has been used in many studies, we\nshow that it is flawed: Sensitivities are not calculated appropriately, hereby\ncreating the wrong impression that priming indicated better sensitivity than\nthe close-to-zero sensitivity of the direct discrimination. We describe the\nappropriate way to determine sensitivities, replicate the behavioral part of a\nlandmark study, develop a method to estimate sensitivities for published\nstudies from reported summary statistics, and use this method to reanalyze 15\nhighly influential studies. Results show that the interpretations of many\nstudies need to be changed and that a community effort to reassess the vast\nliterature on unconscious priming is needed. This process will allow scientists\nto learn more about the true boundary conditions of unconscious priming,\nthereby advancing the scientific understanding of consciousness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:32:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 07:58:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Meyen", "Sascha", ""], ["Zerweck", "Iris A.", ""], ["Amado", "Catarina", ""], ["von Luxburg", "Ulrike", ""], ["Franz", "Volker H.", ""]]}]