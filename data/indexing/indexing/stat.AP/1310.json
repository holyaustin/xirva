[{"id": "1310.0201", "submitter": "Moreno Coco MIC", "authors": "Moreno I. Coco and Rick Dale", "title": "Cross-Recurrence Quantification Analysis of Categorical and Continuous\n  Time Series: an R package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package crqa to perform cross-recurrence\nquantification analysis of two time series of either a categorical or\ncontinuous nature. Streams of behavioral information, from eye movements to\nlinguistic elements, unfold over time. When two people interact, such as in\nconversation, they often adapt to each other, leading these behavioral levels\nto exhibit recurrent states. In dialogue, for example, interlocutors adapt to\neach other by exchanging interactive cues: smiles, nods, gestures, choice of\nwords, and so on. In order for us to capture closely the goings-on of dynamic\ninteraction, and uncover the extent of coupling between two individuals, we\nneed to quantify how much recurrence is taking place at these levels. Methods\navailable in crqa would allow researchers in cognitive science to pose such\nquestions as how much are two people recurrent at some level of analysis, what\nis the characteristic lag time for one person to maximally match another, or\nwhether one person is leading another. First, we set the theoretical ground to\nunderstand the difference between 'correlation' and 'co-visitation' when\ncomparing two time series, using an aggregative or cross-recurrence approach.\nThen, we describe more formally the principles of cross-recurrence, and show\nwith the current package how to carry out analyses applying them. We end the\npaper by comparing computational efficiency, and results' consistency, of crqa\nR package, with the benchmark MATLAB toolbox crptoolbox. We show perfect\ncomparability between the two libraries on both levels.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 09:20:21 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 16:09:07 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Coco", "Moreno I.", ""], ["Dale", "Rick", ""]]}, {"id": "1310.0364", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Segregation Indices for Disease Clustering", "comments": "31 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-13-1", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial clustering has important implications in various fields. In\nparticular, disease clustering is of major public concern in epidemiology. In\nthis article, we propose the use of two distance-based segregation indices to\ntest the significance of disease clustering among subjects whose locations are\nfrom a homogeneous or an inhomogeneous population. We derive their asymptotic\ndistributions and compare them with other distance-based disease clustering\ntests in terms of empirical size and power by extensive Monte Carlo\nsimulations. The null pattern we consider is the random labeling (RL) of cases\nand controls to the given locations. Along this line, we investigate the\nsensitivity of the size of these tests to the underlying background pattern\n(e.g., clustered or homogenous) on which the RL is applied, the level of\nclustering and number of clusters, or differences in relative abundances of the\nclasses. We demonstrate that differences in relative abundance has the highest\nimpact on the empirical sizes of the tests. We also propose various non-RL\npatterns as alternatives to the RL pattern and assess the empirical power\nperformance of the tests under these alternatives. We illustrate the methods on\ntwo real-life examples from epidemiology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 16:05:20 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2013 18:56:07 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "1310.0424", "submitter": "Susan Holmes", "authors": "Paul J. McMurdie and Susan Holmes", "title": "Waste Not, Want Not: Why Rarefying Microbiome Data is Inadmissible", "comments": "22 pages, 5 figures, 2 supplementary sections", "journal-ref": null, "doi": "10.1371/journal.pcbi.1003531", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of count data originating from the current generation of\nDNA sequencing platforms requires special attention. In particular, the\nper-sample library sizes often vary by orders of magnitude from the same\nsequencing run, and the counts are overdispersed relative to a simple Poisson\nmodel These challenges can be addressed using an appropriate mixture model that\nsimultaneously accounts for library size differences and biological\nvariability. This approach is already well-characterized and implemented for\nRNA-Seq data in R packages such as edgeR and DESeq.\n  We use statistical theory, extensive simulations, and empirical data to show\nthat variance stabilizing normalization using a mixture model like the negative\nbinomial is appropriate for microbiome count data. In simulations detecting\ndifferential abundance, normalization procedures based on a Gamma-Poisson\nmixture model provided systematic improvement in performance over crude\nproportions or rarefied counts -- both of which led to a high rate of false\npositives. In simulations evaluating clustering accuracy, we found that the\nrarefying procedure discarded samples that were nevertheless accurately\nclustered by alternative methods, and that the choice of minimum library size\nthreshold was critical in some settings, but with an optimum that is unknown in\npractice. Techniques that use variance stabilizing transformations by modeling\nmicrobiome count data with a mixture distribution, such as those implemented in\nedgeR and DESeq, substantially improved upon techniques that attempt to\nnormalize by rarefying or crude proportions. Based on these results and\nwell-established statistical theory, we advocate that investigators avoid\nrarefying altogether. We have provided microbiome-specific extensions to these\ntools in the R package, phyloseq.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 18:54:24 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 08:57:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["McMurdie", "Paul J.", ""], ["Holmes", "Susan", ""]]}, {"id": "1310.0446", "submitter": "Sergio Davis", "authors": "Sergio Davis, Yasm\\'in Navarrete, Gonzalo Guti\\'errez", "title": "A maximum entropy model for opinions in social groups", "comments": null, "journal-ref": "Eur. Phys. J. B 87, 78 (2014)", "doi": "10.1140/epjb/e2014-40918-6", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the opinions of a group of individuals determine their spatial\ndistribution and connectivity, through an agent-based model. The interaction\nbetween agents is described by a Potts-like Hamiltonian in which agents are\nallowed to move freely without an underlying lattice (the average network\ntopology connecting them is determined from the parameters). This kind of model\nwas derived using maximum entropy statistical inference under fixed expectation\nvalues of certain probabilities that (we propose) are relevant to social\norganization. Control parameters emerge as Lagrange multipliers of the maximum\nentropy problem, and they can be associated with the level of consequence\nbetween the personal beliefs and external opinions, and the tendency to\nsocialize with peers of similar or opposing views. These parameters define a\nphase diagram for the social system, which we studied using Monte Carlo\nMetropolis simulations. Our model presents both first and second-order phase\ntransitions, depending on the ratio between the internal consequence and the\ninteraction with others. We have found a critical value for the level of\ninternal consequence, below which the personal beliefs of the agents seem to be\nirrelevant.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 19:40:37 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 21:33:30 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Davis", "Sergio", ""], ["Navarrete", "Yasm\u00edn", ""], ["Guti\u00e9rrez", "Gonzalo", ""]]}, {"id": "1310.0606", "submitter": "Ruth Heller", "authors": "Ruth Heller, Marina Bogomolov, and Yoav Benjamini", "title": "Deciding whether follow-up studies have replicated findings in a\n  preliminary large-scale \"omics' study\"", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the United\n  States of America (PNAS), 2014 vol. 111 no. 46, 16262-16267", "doi": "10.1073/pnas.1314814111", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formal method to declare that findings from a primary study have\nbeen replicated in a follow-up study. Our proposal is appropriate for primary\nstudies that involve large-scale searches for rare true positives (i.e. needles\nin a haystack). Our proposal assigns an $r$-value to each finding; this is the\nlowest false discovery rate at which the finding can be called replicated.\nExamples are given and software is available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 07:26:26 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 10:09:26 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Heller", "Ruth", ""], ["Bogomolov", "Marina", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1310.0912", "submitter": "Xiaolin Luo Dr", "authors": "X. Luo and P. V. Shevchenko", "title": "When to Bite the Bullet? - A Study of Optimal Strategies for Reducing\n  Global Warming", "comments": null, "journal-ref": "MODSIM2013, pp. 1447-1453", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is based on the framework proposed by Conrad (1997) to determine\nthe optimal timing of an investment or policy to slow global warming. While\nConrad formulated the problem as a stopping rule option pricing model, we treat\nthe policy decision by considering the total damage function that enables us to\nmake some interesting extensions to the original formulation. We show that\nConrad's framework is equivalent to minmization of the expected value of the\ndamage function under the stochastic optimal stopping rule. We extend Conrad's\nmodel by allowing for policy cost to grow with time. In addition to closed form\nsolution, we also perform Monte Carlo simulations to find the distribution for\nthe total damage and show that at higher quantiles the damage may become too\nlarge and so is the risk on the global economy. We also show that the decision\nto take action largely depends on the cost of the action. For example, in the\ncase of model parameters calibrated as in Conrad (1997) with a constant cost,\nthere is a rather long wait before the action is expected to be taken, but if\nthe cost increases with the same rate as the global economy growth, then action\nhas to be taken immediately to minimize the damage.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 07:12:57 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Luo", "X.", ""], ["Shevchenko", "P. V.", ""]]}, {"id": "1310.0973", "submitter": "Umberto Picchini", "authors": "Umberto Picchini, Julie Lyng Forman", "title": "Accelerating inference for diffusions observed with measurement error\n  and large sample sizes using Approximate Bayesian Computation", "comments": "22 pages, forthcoming in Journal of Statistical Computation and\n  Simulation", "journal-ref": null, "doi": "10.1080/00949655.2014.1002101", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years dynamical modelling has been provided with a range of\nbreakthrough methods to perform exact Bayesian inference. However it is often\ncomputationally unfeasible to apply exact statistical methodologies in the\ncontext of large datasets and complex models. This paper considers a nonlinear\nstochastic differential equation model observed with correlated measurement\nerrors and an application to protein folding modelling. An Approximate Bayesian\nComputation (ABC) MCMC algorithm is suggested to allow inference for model\nparameters within reasonable time constraints. The ABC algorithm uses\nsimulations of \"subsamples\" from the assumed data generating model as well as a\nso-called \"early rejection\" strategy to speed up computations in the ABC-MCMC\nsampler. Using a considerate amount of subsamples does not seem to degrade the\nquality of the inferential results for the considered applications. A\nsimulation study is conducted to compare our strategy with exact Bayesian\ninference, the latter resulting two orders of magnitude slower than ABC-MCMC\nfor the considered setup. Finally the ABC algorithm is applied to a large size\nprotein data. The suggested methodology is fairly general and not limited to\nthe exemplified model and data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 13:23:57 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 10:29:56 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:01:46 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Picchini", "Umberto", ""], ["Forman", "Julie Lyng", ""]]}, {"id": "1310.1068", "submitter": "Matthias Steinr\\\"{u}cken", "authors": "Matthias Steinr\\\"ucken, Anand Bhaskar, Yun S. Song", "title": "A novel spectral method for inferring general diploid selection from\n  time series genetic data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS764 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2203-2222", "doi": "10.1214/14-AOAS764", "report-no": "IMS-AOAS-AOAS764", "categories": "q-bio.PE math.FA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of time series genetic variation data from\nexperimental evolution studies and ancient DNA samples has created new\nopportunities to identify genomic regions under selective pressure and to\nestimate their associated fitness parameters. However, it is a challenging\nproblem to compute the likelihood of nonneutral models for the population\nallele frequency dynamics, given the observed temporal DNA data. Here, we\ndevelop a novel spectral algorithm to analytically and efficiently integrate\nover all possible frequency trajectories between consecutive time points. This\nadvance circumvents the limitations of existing methods which require\nfine-tuning the discretization of the population allele frequency space when\nnumerically approximating requisite integrals. Furthermore, our method is\nflexible enough to handle general diploid models of selection where the\nheterozygote and homozygote fitness parameters can take any values, while\nprevious methods focused on only a few restricted models of selection. We\ndemonstrate the utility of our method on simulated data and also apply it to\nanalyze ancient DNA data from genetic loci associated with coat coloration in\nhorses. In contrast to previous studies, our exploration of the full fitness\nparameter space reveals that a heterozygote advantage form of balancing\nselection may have been acting on these loci.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 19:02:12 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 11:55:42 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Steinr\u00fccken", "Matthias", ""], ["Bhaskar", "Anand", ""], ["Song", "Yun S.", ""]]}, {"id": "1310.1249", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski, Amir Rostami", "title": "Reading Stockholm Riots 2013 in social media by text-mining", "comments": "5p", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The riots in Stockholm in May 2013 were an event that reverberated in the\nworld media for its dimension of violence that had spread through the Swedish\ncapital. In this study we have investigated the role of social media in\ncreating media phenomena via text mining and natural language processing. We\nhave focused on two channels of communication for our analysis: Twitter and\nPoloniainfo.se (Forum of Polish community in Sweden). Our preliminary results\nshow some hot topics driving discussion related mostly to Swedish Police and\nSwedish Politics by counting word usage. Typical features for media\nintervention are presented. We have built networks of most popular phrases,\nclustered by categories (geography, media institution, etc.). Sentiment\nanalysis shows negative connotation with Police. The aim of this preliminary\nexploratory quantitative study was to generate questions and hypotheses, which\nwe could carefully follow by deeper more qualitative methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 13:04:45 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Jarynowski", "Andrzej", ""], ["Rostami", "Amir", ""]]}, {"id": "1310.1267", "submitter": "Anne Cuzol Ms", "authors": "Anne Cuzol and Etienne M\\'emin", "title": "Monte Carlo fixed-lag smoothing in state-space models", "comments": null, "journal-ref": null, "doi": "10.5194/npg-21-633-2014", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for Monte Carlo fixed-lag smoothing in\nstate-space models defined by a diffusion process observed through noisy\ndiscrete-time measurements. Based on a particles approximation of the filtering\nand smoothing distributions, the method relies on a simulation technique of\nconditioned diffusions. The proposed sequential smoother can be applied to\ngeneral non linear and multidimensional models, like the ones used in\nenvironmental applications. The smoothing of a turbulent flow in a\nhigh-dimensional context is given as a practical example.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 13:47:41 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Cuzol", "Anne", ""], ["M\u00e9min", "Etienne", ""]]}, {"id": "1310.1628", "submitter": "Dominik Liebl", "authors": "Dominik Liebl", "title": "Modeling and forecasting electricity spot prices: A functional data\n  perspective", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS652 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1562-1592", "doi": "10.1214/13-AOAS652", "report-no": "IMS-AOAS-AOAS652", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical time series models have serious difficulties in modeling and\nforecasting the enormous fluctuations of electricity spot prices. Markov regime\nswitch models belong to the most often used models in the electricity\nliterature. These models try to capture the fluctuations of electricity spot\nprices by using different regimes, each with its own mean and covariance\nstructure. Usually one regime is dedicated to moderate prices and another is\ndedicated to high prices. However, these models show poor performance and there\nis no theoretical justification for this kind of classification. The merit\norder model, the most important micro-economic pricing model for electricity\nspot prices, however, suggests a continuum of mean levels with a functional\ndependence on electricity demand. We propose a new statistical perspective on\nmodeling and forecasting electricity spot prices that accounts for the merit\norder model. In a first step, the functional relation between electricity spot\nprices and electricity demand is modeled by daily price-demand functions. In a\nsecond step, we parameterize the series of daily price-demand functions using a\nfunctional factor model. The power of this new perspective is demonstrated by a\nforecast study that compares our functional factor model with two established\nclassical time series models as well as two alternative functional data models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 21:04:00 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 12:51:05 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Liebl", "Dominik", ""]]}, {"id": "1310.2076", "submitter": "Yunjin Choi", "authors": "Yunjin Choi and Robert Tibshirani", "title": "An Investigation of Methods for Handling Missing Data with Penalized\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate methods for penalized regression in the presence of missing\nobservations. This paper introduces a method for estimating the parameters\nwhich compensates for the missing observations. We first, derive an unbiased\nestimator of the objective function with respect to the missing data and then,\nmodify the criterion to ensure convexity. Finally, we extend our approach to a\nfamily of models that embraces the mean imputation method. These approaches are\ncompared to the mean imputation method, one of the simplest methods for dealing\nwith missing observations problem, via simulations. We also investigate the\nproblem of making predictions when there are missing values in the test set.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:26:30 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Choi", "Yunjin", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1310.2125", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Sohan Seth and Samuel Kaski", "title": "Retrieval of Experiments with Sequential Dirichlet Process Mixtures in\n  Model Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving relevant experiments given a query\nexperiment, motivated by the public databases of datasets in molecular biology\nand other experimental sciences, and the need of scientists to relate to\nearlier work on the level of actual measurement data. Since experiments are\ninherently noisy and databases ever accumulating, we argue that a retrieval\nengine should possess two particular characteristics. First, it should compare\nmodels learnt from the experiments rather than the raw measurements themselves:\nthis allows incorporating experiment-specific prior knowledge to suppress noise\neffects and focus on what is important. Second, it should be updated\nsequentially from newly published experiments, without explicitly storing\neither the measurements or the models, which is critical for saving storage\nspace and protecting data privacy: this promotes life long learning. We\nformulate the retrieval as a ``supermodelling'' problem, of sequentially\nlearning a model of the set of posterior distributions, represented as sets of\nMCMC samples, and suggest the use of Particle-Learning-based sequential\nDirichlet process mixture (DPM) for this purpose. The relevance measure for\nretrieval is derived from the supermodel through the mixture representation. We\ndemonstrate the performance of the proposed retrieval method on simulated data\nand molecular biological experiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:10:26 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 22:04:33 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Kaski", "Samuel", ""]]}, {"id": "1310.2264", "submitter": "Stephen D. H. Hsu", "authors": "Shashaank Vattikuti, James J. Lee, Christopher C. Chang, Stephen D. H.\n  Hsu, Carson C. Chow", "title": "Application of compressed sensing to genome wide association studies and\n  genomic selection", "comments": "30 pages, 11 figures. Version to appear in journal GigaScience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the signal-processing paradigm known as compressed sensing (CS)\nis applicable to genome-wide association studies (GWAS) and genomic selection\n(GS). The aim of GWAS is to isolate trait-associated loci, whereas GS attempts\nto predict the phenotypic values of new individuals on the basis of training\ndata. CS addresses a problem common to both endeavors, namely that the number\nof genotyped markers often greatly exceeds the sample size. We show using CS\nmethods and theory that all loci of nonzero effect can be identified (selected)\nusing an efficient algorithm, provided that they are sufficiently few in number\n(sparse) relative to sample size. For heritability h2 = 1, there is a sharp\nphase transition to complete selection as the sample size is increased. For\nheritability values less than one, complete selection can still occur although\nthe transition is smoothed. The transition boundary is only weakly dependent on\nthe total number of genotyped markers. The crossing of a transition boundary\nprovides an objective means to determine when true effects are being recovered;\nwe discuss practical methods for detecting the boundary. For h2 = 0.5, we find\nthat a sample size that is thirty times the number of nonzero loci is\nsufficient for good recovery.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 20:16:27 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 01:47:46 GMT"}, {"version": "v3", "created": "Sun, 11 May 2014 21:49:48 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Vattikuti", "Shashaank", ""], ["Lee", "James J.", ""], ["Chang", "Christopher C.", ""], ["Hsu", "Stephen D. H.", ""], ["Chow", "Carson C.", ""]]}, {"id": "1310.2328", "submitter": "Michael LuValle", "authors": "M. LuValle", "title": "A note on linear prediction of large chaotic systems", "comments": "16 pages,10 figures, submitted to science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable prediction of large chaotic sytems in the short to middle time range\nis of interest in a number of fields, including climate, ecology, seismology,\nand economics. In this paper, results from chaos theory, and statistical theory\nare combined to suggest rulse for building linear predictive models of chaotic\nsystems. The rules are tested on a problems identified as hard in the climate\nliterature, interseasonal to interannual prediction of regional seasonal\nprecipitation. In a test of prediction the method yields third season ahead\npredictions in 4 regions over 5 seasons which beat the NOAA climate prediction\ncenters half season predictions for the same region and seasons. In a test\nusing dimensionless climate patterns to infer parameters of the climate system,\nremarkably accurate estimates of increase in average global surface air\ntemperature are produced.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 02:44:02 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2013 18:45:32 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 15:34:52 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2013 17:24:00 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2013 22:33:08 GMT"}, {"version": "v6", "created": "Sat, 14 Dec 2013 03:58:25 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["LuValle", "M.", ""]]}, {"id": "1310.2408", "submitter": "Jun Zhu", "authors": "Jun Zhu, Xun Zheng, Bo Zhang", "title": "Improved Bayesian Logistic Supervised Topic Models with Data\n  Augmentation", "comments": "9 pages, ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models with a logistic likelihood have two issues that\npotentially limit their practical use: 1) response variables are usually\nover-weighted by document word counts; and 2) existing variational inference\nmethods make strict mean-field assumptions. We address these issues by: 1)\nintroducing a regularization constant to better balance the two parts based on\nan optimization formulation of Bayesian inference; and 2) developing a simple\nGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and\ncollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm\nhas analytical forms of each conditional distribution without making any\nrestricting assumptions and can be easily parallelized. Empirical results\ndemonstrate significant improvements on prediction performance and time\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:23:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhu", "Jun", ""], ["Zheng", "Xun", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2557", "submitter": "Steven Pav", "authors": "Steven E. Pav", "title": "SRCEK: A Continuous Embedding of the Channel Selection Problem for\n  weighted PLS Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SRCEK, is a technique for selecting useful channels for affine modeling of a\nresponse by PLS. The technique embeds the discrete channel selection problem\ninto the continuous space of predictor preweighting, then employs a\nQuasi-Newton (or other) optimization algorithm to optimize the preweighting\nvector. Once the weighting vector has been optimized, the magnitudes of the\nweights indicate the relative importance of each channel. The relative\nimportances are used to construct n different models, the kth consisting of the\nk most important channels. The different models are then compared by means of\ncross validation or an information criterion (e.g. BIC), allowing automatic\nselection of a `good' subset of the channels. The analytical Jacobian of the\nPLS regression vector with respect to the predictor weighting is derived to\nfacilitate optimization of the latter. This formulation exploits the reduced\nrank of the predictor matrix to gain some speedup when the number of\nobservations is fewer than the number of predictors (the usual case for e.g. IR\nspectroscopy). The method compares favourably with predictor selection\ntechniques surveyed by Forina et. al.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 17:48:43 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Pav", "Steven E.", ""]]}, {"id": "1310.2567", "submitter": "Salvador Pueyo", "authors": "Salvador Pueyo", "title": "Is it a power law distribution? The case of economic contractions", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO physics.data-an q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the first steps to understand and forecast economic downturns is\nidentifying their frequency distribution, but it remains uncertain. This\nproblem is common in phenomena displaying power-law-like distributions. Power\nlaws play a central role in complex systems theory; therefore, the current\nlimitations in the identification of this distribution in empirical data are a\nmajor obstacle to pursue the insights that the complexity approach offers in\nmany fields. This paper addresses this issue by introducing a reliable\nmethodology with a solid theoretical foundation, the Taylor Series-Based Power\nLaw Range Identification Method. When applied to time series from 39 countries,\nthis method reveals a well-defined power law in the relative per capita GDP\ncontractions that span from 5.53% to 50%, comprising 263 events. However, this\nobservation does not suffice to attribute recessions to some specific\nmechanism, such as self-organized criticality. The paper highlights a set of\npoints requiring more study so as to discriminate among models compatible with\nthe power law, as needed to develop sound tools for the management of\nrecessions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 18:18:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Pueyo", "Salvador", ""]]}, {"id": "1310.2733", "submitter": "Marie Laure Delignette-Muller", "authors": "Marie Laure Delignette-Muller, Christelle Lopes, Philippe Veber,\n  Sandrine Charles", "title": "Statistical handling of reproduction data for exposure-response\n  modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproduction data collected through standard bioassays are classically\nanalyzed by regression in order to fit exposure-response curves and estimate\nECx values (x% Effective Concentration). But regression is often misused on\nsuch data, ignoring statistical issues related to i) the special nature of\nreproduction data (count data), ii) a potential inter-replicate variability and\niii) a possible concomitant mortality. This paper offers new insights in\ndealing with those issues. Concerning mortality, particular attention was paid\nnot to waste any valuable data - by dropping all the replicates with mortality\n- or to bias ECx values. For that purpose we defined a new covariate summing\nthe observation periods during which each individual contributes to the\nreproduction process. This covariate was then used to quantify reproduction -\nfor each replicate at each concentration - as a number of offspring per\nindividual-day. We formulated three exposure-response models differing by their\nstochastic part. Those models were fitted to four datasets and compared using a\nBayesian framework. The individual-day unit proved to be a suitable approach to\nuse all the available data and prevent bias in the estimation of ECx values.\nFurthermore, a non-classical negative-binomial model was shown to correctly\ndescribe the inter-replicate variability observed in the studied datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 08:59:17 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 09:46:22 GMT"}, {"version": "v3", "created": "Sat, 26 Apr 2014 17:57:40 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Delignette-Muller", "Marie Laure", ""], ["Lopes", "Christelle", ""], ["Veber", "Philippe", ""], ["Charles", "Sandrine", ""]]}, {"id": "1310.2873", "submitter": "Jeremie Houssineau", "authors": "Emmanuel Delande, Murat Uney, Jeremie Houssineau, Daniel Clark", "title": "Regional variance for multi-object filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in multi-object filtering has led to algorithms that compute\nthe first-order moment of multi-object distributions based on sensor\nmeasurements. The number of targets in arbitrarily selected regions can be\nestimated using the first-order moment. In this work, we introduce explicit\nformulae for the computation of the second-order statistic on the target\nnumber. The proposed concept of regional variance quantifies the level of\nconfidence on target number estimates in arbitrary regions and facilitates\ninformation-based decisions. We provide algorithms for its computation for the\nProbability Hypothesis Density (PHD) and the Cardinalized Probability\nHypothesis Density (CPHD) filters. We demonstrate the behaviour of the regional\nstatistics through simulation examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 16:27:38 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Delande", "Emmanuel", ""], ["Uney", "Murat", ""], ["Houssineau", "Jeremie", ""], ["Clark", "Daniel", ""]]}, {"id": "1310.3073", "submitter": "Teresa Scholz", "authors": "Teresa Scholz, Vitor V. Lopes, Ana Estanqueiro", "title": "A cyclic time-dependent Markov process to model daily patterns in wind\n  turbine power production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind energy is becoming a top contributor to the renewable energy mix, which\nraises potential reliability issues for the grid due to the fluctuating nature\nof its source. To achieve adequate reserve commitment and to promote market\nparticipation, it is necessary to provide models that can capture daily\npatterns in wind power production. This paper presents a cyclic inhomogeneous\nMarkov process, which is based on a three-dimensional state-space (wind power,\nspeed and direction). Each time-dependent transition probability is expressed\nas a Bernstein polynomial. The model parameters are estimated by solving a\nconstrained optimization problem: The objective function combines two maximum\nlikelihood estimators, one to ensure that the Markov process long-term behavior\nreproduces the data accurately and another to capture daily fluctuations. A\nconvex formulation for the overall optimization problem is presented and its\napplicability demonstrated through the analysis of a case-study. The proposed\nmodel is capable of reproducing the diurnal patterns of a three-year dataset\ncollected from a wind turbine located in a mountainous region in Portugal. In\naddition, it is shown how to compute persistence statistics directly from the\nMarkov process transition matrices. Based on the case-study, the power\nproduction persistence through the daily cycle is analysed and discussed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 10:13:43 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Scholz", "Teresa", ""], ["Lopes", "Vitor V.", ""], ["Estanqueiro", "Ana", ""]]}, {"id": "1310.3197", "submitter": "Yaniv Erlich", "authors": "Yaniv Erlich and Arvind Narayanan", "title": "Routes for breaching and protecting genetic privacy", "comments": "Draft for comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are entering the era of ubiquitous genetic information for research,\nclinical care, and personal curiosity. Sharing these datasets is vital for\nrapid progress in understanding the genetic basis of human diseases. However,\none growing concern is the ability to protect the genetic privacy of the data\noriginators. Here, we technically map threats to genetic privacy and discuss\npotential mitigation strategies for privacy-preserving dissemination of genetic\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 17:02:54 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Erlich", "Yaniv", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1310.3223", "submitter": "Fang Han", "authors": "Fang Han, Han Liu, Brian Caffo", "title": "Sparse Median Graphs Estimation in a High Dimensional Semiparametric\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript a unified framework for conducting inference on complex\naggregated data in high dimensional settings is proposed. The data are assumed\nto be a collection of multiple non-Gaussian realizations with underlying\nundirected graphical structures. Utilizing the concept of median graphs in\nsummarizing the commonality across these graphical structures, a novel\nsemiparametric approach to modeling such complex aggregated data is provided\nalong with robust estimation of the median graph, which is assumed to be\nsparse. The estimator is proved to be consistent in graph recovery and an upper\nbound on the rate of convergence is given. Experiments on both synthetic and\nreal datasets are conducted to illustrate the empirical usefulness of the\nproposed models and methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 17:58:19 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Han", "Fang", ""], ["Liu", "Han", ""], ["Caffo", "Brian", ""]]}, {"id": "1310.3454", "submitter": "Aravindh Krishnamoorthy", "authors": "Aravindh Krishnamoorthy", "title": "Linear Extended Whitening Filters", "comments": "3pp, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a class of linear whitening filters termed linear\nextended whitening filters (EWFs) which are whitening filters that have\ndesirable secondary properties and can be used for simplifying algorithms, or\nachieving desired side-effects on given secondary matrices, random vectors or\nrandom processes. Further, we present an application of EWFs for simplification\nof QR decomposition based ML detection algorithm in Wireless Communication.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 08:00:27 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Krishnamoorthy", "Aravindh", ""]]}, {"id": "1310.3465", "submitter": "Martin Voracek", "authors": "Martin Voracek", "title": "No effects of androgen receptor gene CAG and GGC repeat polymorphisms on\n  digit ratio (2D:4D): Meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: A series of meta-analyses assessed whether differentially\nefficacious variants (CAG and GGC repeat-length polymorphisms) of the human\nandrogen receptor gene are associated with digit ratio (2D:4D), a widely\ninvestigated putative pointer to prenatal androgen action. Methods: Extensive\nliterature search strategies identified a maximum of 16 samples (total N =\n2157) eligible for meta-analysis. Results: In contrast to a small-sample (N =\n50) initial report, widely cited affirmatively in the literature, meta-analysis\nof the entire retrievable evidence base did not support associations between\nandrogen receptor gene efficacy and 2D:4D. Conclusions: These meta-analytical\nnil findings, along with several further suggestive strands of evidence\nconsistent with these, undermine one validity claim for 2D:4D as a\nretrospective pointer to prenatal testosterone action.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 10:35:26 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Voracek", "Martin", ""]]}, {"id": "1310.3607", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann, Sruthi Moorthy and Zifan Shi", "title": "Predicting college basketball match outcomes using machine learning\n  techniques: some results and lessons learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing work on predicting NCAAB matches has been developed in a\nstatistical context. Trusting the capabilities of ML techniques, particularly\nclassification learners, to uncover the importance of features and learn their\nrelationships, we evaluated a number of different paradigms on this task. In\nthis paper, we summarize our work, pointing out that attributes seem to be more\nimportant than models, and that there seems to be an upper limit to predictive\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 09:42:54 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Zimmermann", "Albrecht", ""], ["Moorthy", "Sruthi", ""], ["Shi", "Zifan", ""]]}, {"id": "1310.3863", "submitter": "Giovanni Montana", "authors": "Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech,\n  Christoforos Anagnostopoulos, Giovanni Montana", "title": "Estimating Time-varying Brain Connectivity Networks from Functional MRI\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the functional architecture of the brain in terms of networks\nis becoming increasingly common. In most fMRI applications functional networks\nare assumed to be stationary, resulting in a single network estimated for the\nentire time course. However recent results suggest that the connectivity\nbetween brain regions is highly non-stationary even at rest. As a result, there\nis a need for new brain imaging methodologies that comprehensively account for\nthe dynamic (i.e., non-stationary) nature of the fMRI data. In this work we\npropose the Smooth Incremental Graphical Lasso Estimation (SINGLE) algorithm\nwhich estimates dynamic brain networks from fMRI data. We apply the SINGLE\nalgorithm to functional MRI data from 24 healthy patients performing a\nchoice-response task to demonstrate the dynamic changes in network structure\nthat accompany a simple but attentionally demanding cognitive task. Using graph\ntheoretic measures we show that the Right Inferior Frontal Gyrus, frequently\nreported as playing an important role in cognitive control, dynamically changes\nwith the task. Our results suggest that the Right Inferior Frontal Gyrus plays\na fundamental role in the attention and executive function during cognitively\ndemanding tasks and may play a key role in regulating the balance between other\nbrain regions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 21:37:55 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 08:50:01 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Hellyer", "Peter", ""], ["Sharp", "David", ""], ["Leech", "Robert", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1310.3946", "submitter": "Behrooz Makki", "authors": "Behrooz Makki, Alexandre Graell i Amat, Thomas Eriksson", "title": "On Noisy ARQ in Block-Fading Channels", "comments": "Accepted for publication on IEEE Transactions on Vehicular Technology", "journal-ref": null, "doi": "10.1109/TVT.2013.2276371", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming noisy feedback channels, this paper investigates the data\ntransmission efficiency and robustness of different automatic repeat request\n(ARQ) schemes using adaptive power allocation. Considering different\nblock-fading channel assumptions, the long-term throughput, the delay-limited\nthroughput, the outage probability and the feedback load of different ARQ\nprotocols are studied. A closed-form expression for the power-limited\nthroughput optimization problem is obtained which is valid for different ARQ\nprotocols and feedback channel conditions. Furthermore, the paper presents\nnumerical investigations on the robustness of different ARQ protocols to\nfeedback errors. It is shown that many analytical assertions about the ARQ\nprotocols are valid both when the channel remains fixed during all\nretransmission rounds and when it changes in each round (in)dependently. As\ndemonstrated, optimal power allocation is crucial for the performance of noisy\nARQ schemes when the goal is to minimize the outage probability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 08:07:54 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Makki", "Behrooz", ""], ["Amat", "Alexandre Graell i", ""], ["Eriksson", "Thomas", ""]]}, {"id": "1310.3970", "submitter": "Behrooz Makki", "authors": "Behrooz Makki, Alexandre Graell i Amat, Thomas Eriksson", "title": "Green Communication via Power-optimized HARQ Protocols", "comments": "Accepted for publication on IEEE Transactions on Vehicular Technology", "journal-ref": null, "doi": "10.1109/TVT.2013.2274287", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, efficient use of energy has become an essential research topic for\ngreen communication. This paper studies the effect of optimal power controllers\non the performance of delay-sensitive communication setups utilizing hybrid\nautomatic repeat request (HARQ). The results are obtained for repetition time\ndiversity (RTD) and incremental redundancy (INR) HARQ protocols. In all cases,\nthe optimal power allocation, minimizing the outage-limited average\ntransmission power, is obtained under both continuous and bursting\ncommunication models. Also, we investigate the system throughput in different\nconditions. The results indicate that the power efficiency is increased\nsubstantially, if adaptive power allocation is utilized. For instance, assume\nRayleigh-fading channel, a maximum of two (re)transmission rounds with rates\n$\\{1,\\frac{1}{2}\\}$ nats-per-channel-use and an outage probability constraint\n${10}^{-3}$. Then, compared to uniform power allocation, optimal power\nallocation in RTD reduces the average power by 9 and 11 dB in the bursting and\ncontinuous communication models, respectively. In INR, these values are\nobtained to be 8 and 9 dB, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 08:58:27 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Makki", "Behrooz", ""], ["Amat", "Alexandre Graell i", ""], ["Eriksson", "Thomas", ""]]}, {"id": "1310.3975", "submitter": "Behrooz Makki", "authors": "Behrooz Makki, Alexandre Graell i Amat, Thomas Eriksson", "title": "HARQ Feedback in Spectrum Sharing Networks", "comments": "Published in IEEE Communications Letters", "journal-ref": "\"HARQ Feedback in Spectrum Sharing Networks,\" 16(9), pp.\n  1337-1340, Sept. 2012", "doi": "10.1109/LCOMM.2012.070512.112003", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter studies the throughput and the outage probability of spectrum\nsharing networks utilizing hybrid automatic repeat request (HARQ) feedback. We\nfocus on the repetition time diversity and the incremental redundancy HARQ\nprotocols where the results are obtained for both continuous and bursting\ncommunication models. The channel data transmission efficiency is investigated\nin the presence of both secondary user peak transmission power and primary user\nreceived interference power constraints. Finally, we evaluate the effect of\nsecondary-primary channel state information imperfection on the performance of\nthe secondary channel. Simulation results show that, while the throughput is\nnot necessarily increased by HARQ, substantial outage probability reduction is\nachieved in all conditions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 09:09:39 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Makki", "Behrooz", ""], ["Amat", "Alexandre Graell i", ""], ["Eriksson", "Thomas", ""]]}, {"id": "1310.4444", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini", "title": "A structural analysis on Gravity of Trade: on removing distance from the\n  model", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gravity Model is the workhorse for empirical studies in International\nEconomies for its empirical power and it is commonly used in explaining the\ntrade flow between countries; it relies on a function that relates the trade\nwith the masses of the two countries and the distance (as a proxy of the\ntrasport costs) between them. However, the notion that using of distance\nfunctions in conventional interaction models effectively captures spatial\ndependence in international flows has long been challenged. It has been\nrecently fully recognized that a spatial interaction effect exists essentially\ndue to the spatial spillover and the third country effect. This motivates the\nintroduction of the spatial autoregressive components in the so-called spatial\ngravity model of trade. A so-called weight matrix is used in order to define\nthe set of the spatial neighbors and it is traditionally based on the inverse\nof the distance. Two issues follow from this standard procedure: the first\nregards the biasness of the distance if it is used as a proxy of the transport\ncosts in a panel data, the second is related to the collinearity emerging if we\nuse distance twice. So, several attempt were made in the recent literature\nhaving the scope of remove the distance. We propose a theoretically consistent\nprocedure based on Anderson, Van Wincoop derivation model, and some ad-hoc\ntests, relating to this attempt. The empirical results based on a 22-years\npanel of OECD countries are conforting, and they allow us to estimate the model\nwithout the distance, if properly replaced by a set of fixed effects. This\narticle, in addition, fits in the dispute about how to estimate the\nmultilateral resistance terms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 16:51:11 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Metulini", "Rodolfo", ""]]}, {"id": "1310.4461", "submitter": "Aaron Clauset", "authors": "Sears Merritt and Aaron Clauset", "title": "Scoring dynamics across professional team sports: tempo, balance and\n  predictability", "comments": "18 pages, 8 figures, 4 tables, 2 appendices", "journal-ref": "EPJ Data Science 3, 4 (2014)", "doi": "10.1140/epjds29", "report-no": null, "categories": "stat.AP cs.CY physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite growing interest in quantifying and modeling the scoring dynamics\nwithin professional sports games, relative little is known about what patterns\nor principles, if any, cut across different sports. Using a comprehensive data\nset of scoring events in nearly a dozen consecutive seasons of college and\nprofessional (American) football, professional hockey, and professional\nbasketball, we identify several common patterns in scoring dynamics. Across\nthese sports, scoring tempo---when scoring events occur---closely follows a\ncommon Poisson process, with a sport-specific rate. Similarly, scoring\nbalance---how often a team wins an event---follows a common Bernoulli process,\nwith a parameter that effectively varies with the size of the lead. Combining\nthese processes within a generative model of gameplay, we find they both\nreproduce the observed dynamics in all four sports and accurately predict game\noutcomes. These results demonstrate common dynamical patterns underlying\nwithin-game scoring dynamics across professional team sports, and suggest\nspecific mechanisms for driving them. We close with a brief discussion of the\nimplications of our results for several popular hypotheses about sports\ndynamics.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 17:44:35 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 21:27:29 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Merritt", "Sears", ""], ["Clauset", "Aaron", ""]]}, {"id": "1310.4667", "submitter": "Anna Helga Jonsdottir", "authors": "Anna Helga Jonsdottir, Gunnar Stefansson", "title": "Enhanced Learning with Web-Assisted Education", "comments": "Presented at the Joint Statistical Meeting - JSM 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An educational system, the tutor-web (http://tutor-web.net), has been\ndeveloped and used for educational research. The system is accessible and free\nto use for anyone having access to the Web. It is based on open source software\nand the teaching material is licensed under the Creative Commons\nAttribution-ShareAlike License. The system has been used for computer-assisted\neducation in statistics and mathematics. It offers a unique way to structure\nand link together teaching material and includes interactive quizzes with the\nprimary purpose of increasing learning rather than mere evaluation.\n  The system was used in a course on basic statistics in 2011. Three types of\ndata were gathered during the course. A randomized crossover experiment was\nconducted to assess the possible difference in learning (measured by repeated\nexams) between students using the system and students doing regular homework.\nThe difference between the groups was not found to be significant. Responses to\nquiz questions were collected and analysed with item response theory type\nmodels. These analysis were used to improve the item banks. Finally, the\nstudents answered an in-class survey regarding their experience using the\ntutor-web. The responses of the students gave clear indications of student\npreferences.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 12:01:39 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Jonsdottir", "Anna Helga", ""], ["Stefansson", "Gunnar", ""]]}, {"id": "1310.4792", "submitter": "Barbara Engelhardt", "authors": "Chuan Gao, Christopher D Brown, Barbara E Engelhardt", "title": "A latent factor model with a mixture of sparse and dense factors to\n  model gene expression data with confounding effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important problem in genome science is to determine sets of co-regulated\ngenes based on measurements of gene expression levels across samples, where the\nquantification of expression levels includes substantial technical and\nbiological noise. To address this problem, we developed a Bayesian sparse\nlatent factor model that uses a three parameter beta prior to flexibly model\nshrinkage in the loading matrix. By applying three layers of shrinkage to the\nloading matrix (global, factor-specific, and element-wise), this model has\nnon-parametric properties in that it estimates the appropriate number of\nfactors from the data. We added a two-component mixture to model each factor\nloading as being generated from either a sparse or a dense mixture component;\nthis allows dense factors that capture confounding noise, and sparse factors\nthat capture local gene interactions. We developed two statistics to quantify\nthe stability of the recovered matrices for both sparse and dense matrices. We\ntested our model on simulated data and found that we successfully recovered the\ntrue latent structure as compared to related models. We applied our model to a\nlarge gene expression study and found that we recovered known covariates and\nsmall groups of co-regulated genes. We validated these gene subsets by testing\nfor associations between genotype data and these latent factors, and we found a\nsubstantial number of biologically important genetic regulators for the\nrecovered gene subsets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 18:38:16 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Gao", "Chuan", ""], ["Brown", "Christopher D", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1310.5103", "submitter": "Mu Zhu", "authors": "Wanhua Su, Yan Yuan, Mu Zhu", "title": "Threshold-free Evaluation of Medical Tests for Classification and\n  Prediction: Average Precision versus Area Under the ROC Curve", "comments": "The first two authors contributed equally to this paper, and should\n  be regarded as co-first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating medical tests or biomarkers for disease classification, the\narea under the receiver-operating characteristic (ROC) curve is a widely used\nperformance metric that does not require us to commit to a specific decision\nthreshold. For the same type of evaluations, a different metric known as the\naverage precision (AP) is used much more widely in the information retrieval\nliterature. We study both metrics in some depths in order to elucidate their\ndifference and relationship. More specifically, we explain mathematically why\nthe AP may be more appropriate if the earlier part of the ROC curve is of\ninterest. We also address practical matters, deriving an expression for the\nasymptotic variance of the AP, as well as providing real-world examples\nconcerning the evaluation of protein biomarkers for prostate cancer and the\nassessment of digital versus film mammography for breast cancer screening.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 17:28:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Su", "Wanhua", ""], ["Yuan", "Yan", ""], ["Zhu", "Mu", ""]]}, {"id": "1310.5169", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "On the graph-theoretical interpretation of Pearson correlations in a\n  multivariate process and a novel partial correlation measure", "comments": "20 oages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dependencies of the lagged (Pearson) correlation function on the\ncoefficients of multivariate autoregressive models are interpreted in the\nframework of time series graphs. Time series graphs are related to the concept\nof Granger causality and encode the conditional independence structure of a\nmultivariate process. The authors show that the complex dependencies of the\nPearson correlation coefficient complicate an interpretation and propose a\nnovel partial correlation measure with a straightforward graph-theoretical\ninterpretation. The novel measure has the additional advantage that its\nsampling distribution is not affected by serial dependencies like that of the\nPearson correlation coefficient. In an application to climatological time\nseries the potential of the novel measure is demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 21:34:01 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "1310.5177", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer, Philip J. Marshall, Matthew W. Auger, Tommaso Treu,\n  Aaron A. Dutton, Matteo Barnab\\`e", "title": "The SWELLS Survey. VI. hierarchical inference of the initial mass\n  functions of bulges and discs", "comments": "Accepted for publication in MNRAS. 15 pages, 8 figures. Code\n  available at https://github.com/eggplantbren/SWELLS_Hierarchical", "journal-ref": null, "doi": "10.1093/mnras/stt2026", "report-no": null, "categories": "astro-ph.IM astro-ph.GA physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-standing assumption that the stellar initial mass function (IMF) is\nuniversal has recently been challenged by a number of observations. Several\nstudies have shown that a \"heavy\" IMF (e.g., with a Salpeter-like abundance of\nlow mass stars and thus normalisation) is preferred for massive early-type\ngalaxies, while this IMF is inconsistent with the properties of less massive,\nlater-type galaxies. These discoveries motivate the hypothesis that the IMF may\nvary (possibly very slightly) across galaxies and across components of\nindividual galaxies (e.g. bulges vs discs). In this paper we use a sample of 19\nlate-type strong gravitational lenses from the SWELLS survey to investigate the\nIMFs of the bulges and discs in late-type galaxies. We perform a joint analysis\nof the galaxies' total masses (constrained by strong gravitational lensing) and\nstellar masses (constrained by optical and near-infrared colours in the context\nof a stellar population synthesis [SPS] model, up to an IMF normalisation\nparameter). Using minimal assumptions apart from the physical constraint that\nthe total stellar mass within any aperture must be less than the total mass\nwithin the aperture, we find that the bulges of the galaxies cannot have IMFs\nheavier (i.e. implying high mass per unit luminosity) than Salpeter, while the\ndisc IMFs are not well constrained by this data set. We also discuss the\nnecessity for hierarchical modelling when combining incomplete information\nabout multiple astronomical objects. This modelling approach allows us to place\nupper limits on the size of any departures from universality. More data,\nincluding spatially resolved kinematics (as in paper V) and stellar population\ndiagnostics over a range of bulge and disc masses, are needed to robustly\nquantify how the IMF varies within galaxies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 22:26:51 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Marshall", "Philip J.", ""], ["Auger", "Matthew W.", ""], ["Treu", "Tommaso", ""], ["Dutton", "Aaron A.", ""], ["Barnab\u00e8", "Matteo", ""]]}, {"id": "1310.5524", "submitter": "Jonathan Heydari", "authors": "Jonathan Heydari, Conor Lawless, David A. Lydall and Darren J.\n  Wilkinson", "title": "Fast Bayesian parameter estimation for stochastic logistic growth models", "comments": "24 pages, 5 figures and 2 tables", "journal-ref": null, "doi": "10.1016/j.biosystems.2014.05.002", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transition density of a stochastic, logistic population growth model with\nmultiplicative intrinsic noise is analytically intractable. Inferring model\nparameter values by fitting such stochastic differential equation (SDE) models\nto data therefore requires relatively slow numerical simulation. Where such\nsimulation is prohibitively slow, an alternative is to use model approximations\nwhich do have an analytically tractable transition density, enabling fast\ninference. We introduce two such approximations, with either multiplicative or\nadditive intrinsic noise, each derived from the linear noise approximation of\nthe logistic growth SDE. After Bayesian inference we find that our fast LNA\nmodels, using Kalman filter recursion for computation of marginal likelihoods,\ngive similar posterior distributions to slow arbitrarily exact models. We also\ndemonstrate that simulations from our LNA models better describe the\ncharacteristics of the stochastic logistic growth models than a related\napproach. Finally, we demonstrate that our LNA model with additive intrinsic\nnoise and measurement error best describes an example set of longitudinal\nobservations of microbial population size taken from a typical, genome-wide\nscreening experiment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 12:32:11 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 15:03:10 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Heydari", "Jonathan", ""], ["Lawless", "Conor", ""], ["Lydall", "David A.", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1310.6719", "submitter": "Sujeet Patole", "authors": "Sujeet Patole and Murat Torlak", "title": "Two Dimensional Array Imaging with Beam Steered Data", "comments": null, "journal-ref": "IEEE Transactions on Image Processing Dec. 2013 (Volume:22, Issue:\n  12, Page(s):5181 - 5189 )", "doi": "10.1109/TIP.2013.2282115", "report-no": "ISSN: 1057-7149", "categories": "cs.CV cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses different approaches used for millimeter wave imaging of\ntwo-dimensional objects. Imaging of a two dimensional object requires reflected\nwave data to be collected across two distinct dimensions. In this paper, we\npropose a reconstruction method that uses narrowband waveforms along with two\ndimensional beam steering. The beam is steered in azimuthal and elevation\ndirection, which forms the two distinct dimensions required for the\nreconstruction. The Reconstruction technique uses inverse Fourier transform\nalong with amplitude and phase correction factors. In addition, this\nreconstruction technique does not require interpolation of the data in either\nwavenumber or spatial domain. Use of the two dimensional beam steering offers\nbetter performance in the presence of noise compared with the existing methods,\nsuch as switched array imaging system. Effects of RF impairments such as\nquantization of the phase of beam steering weights and timing jitter which add\nto phase noise, are analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 19:33:50 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 23:01:54 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Patole", "Sujeet", ""], ["Torlak", "Murat", ""]]}, {"id": "1310.6904", "submitter": "Emil B. Iversen", "authors": "Emil B. Iversen, Juan M. Morales, Jan K. M{\\o}ller, Henrik Madsen", "title": "Probabilistic Forecasts of Solar Irradiance by Stochastic Differential\n  Equations", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts of renewable energy production provide users with\nvaluable information about the uncertainty associated with the expected\ngeneration. Current state-of-the-art forecasts for solar irradiance have\nfocused on producing reliable \\emph{point} forecasts. The additional\ninformation included in probabilistic forecasts may be paramount for decision\nmakers to efficiently make use of this uncertain and variable generation. In\nthis paper, a stochastic differential equation (SDE) framework for modeling the\nuncertainty associated with the solar irradiance point forecast is proposed.\nThis modeling approach allows for characterizing both the interdependence\nstructure of prediction errors of short-term solar irradiance and their\npredictive distribution. A series of different SDE models are fitted to a\ntraining set and subsequently evaluated on a one-year test set. The final model\nproposed is defined on a bounded and time-varying state space with zero\nprobability almost surely of events outside this space.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 13:14:08 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Iversen", "Emil B.", ""], ["Morales", "Juan M.", ""], ["M\u00f8ller", "Jan K.", ""], ["Madsen", "Henrik", ""]]}, {"id": "1310.7033", "submitter": "Yue Wang", "authors": "Niya Wang, Eric P. Hoffman, Robert Clarke, Zhen Zhang, David M.\n  Herrington, Ie-Ming Shih, Douglas A. Levine, Guoqiang Yu, Jianhua Xuan and\n  Yue Wang", "title": "A feasible roadmap for unsupervised deconvolution of two-source mixed\n  gene expressions", "comments": "5 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue heterogeneity is a major confounding factor in studying individual\npopulations that cannot be resolved directly by global profiling. Experimental\nsolutions to mitigate tissue heterogeneity are expensive, time consuming,\ninapplicable to existing data, and may alter the original gene expression\npatterns. Here we ask whether it is possible to deconvolute two-source mixed\nexpressions (estimating both proportions and cell-specific profiles) from two\nor more heterogeneous samples without requiring any prior knowledge. Supported\nby a well-grounded mathematical framework, we argue that both constituent\nproportions and cell-specific expressions can be estimated in a completely\nunsupervised mode when cell-specific marker genes exist, which do not have to\nbe known a priori, for each of constituent cell types. We demonstrate the\nperformance of unsupervised deconvolution on both simulation and real gene\nexpression data, together with perspective discussions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 20:10:08 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Wang", "Niya", ""], ["Hoffman", "Eric P.", ""], ["Clarke", "Robert", ""], ["Zhang", "Zhen", ""], ["Herrington", "David M.", ""], ["Shih", "Ie-Ming", ""], ["Levine", "Douglas A.", ""], ["Yu", "Guoqiang", ""], ["Xuan", "Jianhua", ""], ["Wang", "Yue", ""]]}, {"id": "1310.7148", "submitter": "Jonathan Azose", "authors": "Jonathan J. Azose and Adrian E. Raftery", "title": "Bayesian Probabilistic Projection of International Migration Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for obtaining joint probabilistic projections of\nmigration rates for all countries, broken down by age and sex. Joint\ntrajectories for all countries are constrained to satisfy the requirement of\nzero global net migration. We evaluate our model using out-of-sample validation\nand compare point projections to the projected migration rates from a\npersistence model similar to the method used in the United Nations' World\nPopulation Prospects, and also to a state of the art gravity model. We also\nresolve an apparently paradoxical discrepancy between growth trends in the\nproportion of the world population migrating and the average absolute migration\nrate across countries.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2013 21:35:44 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Azose", "Jonathan J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1310.7278", "submitter": "Yichen Qin", "authors": "Yichen Qin and Carey E. Priebe", "title": "Robust Hypothesis Testing via Lq-Likelihood", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a robust hypothesis testing procedure: the\nLq-likelihood-ratio-type test (LqRT). By deriving the asymptotic distribution\nof this test statistic, the authors demonstrate its robustness both\nanalytically and numerically, and they investigate the properties of both its\ninfluence function and its breakdown point. A proposed method to select the\ntuning parameter q offers a good efficiency/robustness trade-off, compared with\nthe traditional likelihood ratio test (LRT) and other robust tests. A\nsimulation and real data analysis provides further evidence of the advantages\nof the proposed LqRT method. In particular, for the special case of testing the\nlocation parameter in the presence of gross error contamination, the LqRT\ndominates the Wilcoxon-Mann-Whitney test and the sign test at various levels of\ncontamination.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 23:49:41 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 23:28:18 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 01:41:23 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Qin", "Yichen", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1310.7467", "submitter": "Katharine Turner", "authors": "Andrew Robinson and Katharine Turner", "title": "Hypothesis Testing for Topological Data Analysis", "comments": "14 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology is a vital tool for topological data analysis. Previous\nwork has developed some statistical estimators for characteristics of\ncollections of persistence diagrams. However, tools that provide statistical\ninference for observations that are persistence diagrams are limited.\nSpecifically, there is a need for tests that can assess the strength of\nevidence against a claim that two samples arise from the same population or\nprocess. We propose the use of randomization-style null hypothesis significance\ntests (NHST) for these situations. The test is based on a loss function that\ncomprises pairwise distances between the elements of each sample and all the\nelements in the other sample. We use this method to analyze a range of\nsimulated and experimental data. Through these examples we experimentally\nexplore the power of the p-values. Our results show that the\nrandomization-style NHST based on pairwise distances can distinguish between\nsamples from different processes, which suggests that its use for hypothesis\ntests upon persistence diagrams is reasonable. We demonstrate its application\non a real dataset of fMRI data of patients with ADHD.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 15:49:46 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 15:42:46 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Robinson", "Andrew", ""], ["Turner", "Katharine", ""]]}, {"id": "1310.7505", "submitter": "Peter Klimek", "authors": "Peter Klimek, Alexandra Kautzky-Willer, Anna Chmiel, Irmgard\n  Schiller-Fr\\\"uwirth, Stefan Thurner", "title": "Quantifying age- and gender-related diabetes comorbidity risks using\n  nation-wide big claims data", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently emerging \"big data\" techniques are reshaping medical science into a\ndata science. Medical claims data allow assessing an entire nation's health\nstate in a quantitative way, in particular with regard to the occurrences and\nconsequences of chronic and pandemic diseases like diabetes.\n  We develop a quantitative, statistical approach to test for associations\nbetween the incidence of type 1 or type 2 diabetes and any possible other\ndisease as provided by the ICD10 diagnosis codes using a complete set of\nAustrian inpatient data. With a new co-occurrence analysis the relative risks\nfor each possible comorbidity are studied as a function of patient age and\ngender, a temporal analysis investigates whether the onset of diabetes\ntypically precedes or follows the onset of the other disease. The samples is\nalways of maximal size, i.e. contains all patients with that comorbidity within\nthe country. The present study is an equivalent of almost 40,000 studies, all\nwith maximum patient number available. Out of more than thousand possible\nassociations, 123 comorbid diseases for type 1 or type 2 diabetes are\nidentified at high significance levels.\n  Well known diabetic comorbidities are recovered, such as retinopathies,\nhypertension, chronic kidney diseases, etc. This validates the method.\nAdditionally, a number of comorbidities are identified which have only been\nrecognized to a lesser extent, for example epilepsy, sepsis, or mental\ndisorders. The temporal evolution, age, and gender-dependence of these\ncomorbidities are discussed. The new statistical-network methodology developed\nhere can be readily applied to other chronic diseases.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 17:33:22 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Klimek", "Peter", ""], ["Kautzky-Willer", "Alexandra", ""], ["Chmiel", "Anna", ""], ["Schiller-Fr\u00fcwirth", "Irmgard", ""], ["Thurner", "Stefan", ""]]}, {"id": "1310.7714", "submitter": "Sourabh Bhattacharya", "authors": "Sabyasachi Mukhopadhyay and Sourabh Bhattacharya", "title": "An Improved Bayesian Semiparametric Model for Palaeoclimate\n  Reconstruction: Cross-validation Based Model Assessment", "comments": "Approximately this version will appear in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fossil-based palaeoclimate reconstruction is an important area of ecological\nscience that has gained momentum in the backdrop of the global climate change\ndebate. The hierarchical Bayesian paradigm provides an interesting platform for\nstudying such important scientific issue. However, our cross-validation based\nassessment of the existing Bayesian hierarchical models with respect to two\nmodern proxy data sets based on chironomid and pollen, respectively, revealed\nthat the models are inadequate for the data sets.\n  In this paper, we model the species assemblages (compositional data) by the\nzero-inflated multinomial distribution, while modelling the species response\nfunctions using Dirichlet process based Gaussian mixtures. This modelling\nstrategy yielded significantly improved performances, and a formal Bayesian\ntest of model adequacy, developed recently, showed that our new model is\nadequate for both the modern data sets. Furthermore, combining together the\nzero-inflated assumption, Importance Resampling Markov Chain Monte Carlo\n(IRMCMC) and the recently developed Transformation-based Markov Chain Monte\nCarlo (TMCMC), we develop a powerful and efficient computational methodology.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 09:11:16 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 15:08:37 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1310.7815", "submitter": "Ludger Evers", "authors": "A. W. Bowman, L. Evers, D. Molinari, W. R. Jones and M. J. Spence", "title": "Efficient and automatic methods for flexible regression on\n  spatiotemporal data, with applications to groundwater monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting statistical models to spatiotemporal data requires finding the right\nbalance between imposing smoothness and following the data. In the context of\np-splines, we propose a Bayesian framework for choosing the smoothing parameter\nwhich allows the construction of fully-automatic data-driven methods for\nfitting flexible models to spatiotemporal data. A computationally efficient\nimplementation, exploiting the sparsity of the arising design and penalty\nmatrices, is proposed. The findings are illustrated using a simulation and two\nexamples, all concerned with the modelling of contaminants in groundwater,\nwhich suggest that the proposed strategy is more stable that competing\nstrategies based on the use of criteria such as GCV and AIC.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 14:26:42 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Bowman", "A. W.", ""], ["Evers", "L.", ""], ["Molinari", "D.", ""], ["Jones", "W. R.", ""], ["Spence", "M. J.", ""]]}, {"id": "1310.7850", "submitter": "Roy Dong", "authors": "Roy Dong and Lillian Ratliff and Henrik Ohlsson and S. Shankar Sastry", "title": "Fundamental Limits of Nonintrusive Load Monitoring", "comments": "Submitted to the 3rd ACM International Conference on High Confidence\n  Networked Systems (HiCoNS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provided an arbitrary nonintrusive load monitoring (NILM) algorithm, we seek\nbounds on the probability of distinguishing between scenarios, given an\naggregate power consumption signal. We introduce a framework for studying a\ngeneral NILM algorithm, and analyze the theory in the general case. Then, we\nspecialize to the case where the error is Gaussian. In both cases, we are able\nto derive upper bounds on the probability of distinguishing scenarios. Finally,\nwe apply the results to real data to derive bounds on the probability of\ndistinguishing between scenarios as a function of the measurement noise, the\nsampling rate, and the device usage.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 15:46:57 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Dong", "Roy", ""], ["Ratliff", "Lillian", ""], ["Ohlsson", "Henrik", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1310.7918", "submitter": "L\\'aszl\\'o Varga", "authors": "L\\'aszl\\'o Varga, P\\'al Rakonczai, Andr\\'as Zempl\\'eni", "title": "Applications of threshold models and the weighted bootstrap for\n  Hungarian precipitation data", "comments": "10 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents applications of the peaks-over threshold methodology for\nboth the univariate and the recently introduced bivariate case, combined with a\nnovel bootstrap approach. We compare the proposed bootstrap methods to the more\ntraditional profile likelihood.\n  We have investigated 63 years of the European Climate Assessment daily\nprecipitation data for five Hungarian grid points, first separately for the\nsummer and winter months, then aiming at the detection of possible changes by\ninvestigating 20 years moving windows. We show that significant changes can be\nobserved both in the univariate and the bivariate cases, the most recent period\nbeing the most dangerous, as the return levels here are the highest. We\nillustrate these effects by bivariate coverage regions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 18:45:20 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Varga", "L\u00e1szl\u00f3", ""], ["Rakonczai", "P\u00e1l", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1310.8158", "submitter": "Ludger Evers", "authors": "Wayne R. Jones, Michael J. Spence, Adrian W. Bowman, Ludger Evers,\n  Daniel A. Molinari", "title": "GWSDAT - GroundWater Spatiotemporal Data Analysis Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodic monitoring of groundwater quality at industrial and commercial sites\ngenerates large volumes of spatiotemporal concentration data. Data modelling is\ntypically restricted to either the analysis of monotonic trends in individual\nwells, or independent fitting of spatial concentration distributions (e.g.\nKriging) to separate monitoring time periods. Neither of these techniques\nsatisfactorily elucidate the interaction between spatial and temporal\ncomponents of the data. Potential negative consequences include an incomplete\nunderstanding of groundwater plume dynamics, which can lead to the selection of\ninappropriate remedial strategies. The GroundWater Spatiotemporal Data Analysis\nTool (GWSDAT) is a user friendly, open source, decision support tool that has\nbeen developed to address these issues. Uniquely, GWSDAT applies a\nspatiotemporal model for a more coherent and smooth interpretation of the\ninteraction in spatial and time-series components of groundwater solute\nconcentrations. GWSDAT has been designed to work with standard groundwater\nmonitoring data sets, and has no special data requirements. Data entry is via a\nstandardised Microsoft Excel input template. The underlying statistical\nmodelling and graphical output are generated using R. This paper describes in\ndetail the various plotting options available and how the graphical user\ninterface can be used for rapid, rigorous and interactive trend analysis with\nfacilitated report generation. GWSDAT has been used extensively in the\nassessment of soil and groundwater conditions at Shell's downstream assets and\nthe discussion section describes the benefits of its applied use. These include\nrapid interpretation of complex data sets, early identification of new spills,\ndetection of off-site plume migration and simplified preparation of groundwater\nmonitoring reports - all of which facilitate expedited risk assessment and\nremediation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 14:07:44 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Jones", "Wayne R.", ""], ["Spence", "Michael J.", ""], ["Bowman", "Adrian W.", ""], ["Evers", "Ludger", ""], ["Molinari", "Daniel A.", ""]]}, {"id": "1310.8185", "submitter": "Andrzej Jarynowski", "authors": "Amdrzej Jarynowski, Andrzej Buda", "title": "Dynamics of popstar record sales on phonographic market -- stochastic\n  model", "comments": "Summer Solstice 2013 International Conference on Discrete Models of\n  Complex Systems, Warsaw, Poland", "journal-ref": "Acta Physica Polonica B (PS) No 2, Vol. 7 2014", "doi": "10.5506/APhysPolBSupp.7.317", "report-no": null, "categories": "stat.AP cs.SY math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate weekly record sales of the world's most popular 30 artists\n(2003-2013). Time series of sales have non-trivial kind of memory\n(anticorrelations, strong seasonality and constant autocorrelation decay within\n120 weeks). Amount of artists record sales are usually the highest in the first\nweek after premiere of their brand new records and then decrease to fluctuate\naround zero till next album release. We model such a behavior by discrete\nmean-reverting geometric jump diffusion (MRGJD) and Markov regime switching\nmechanism (MRS) between the base and the promotion regimes. We can built up the\nevidence through such a toy model that quantifies linear and nonlinear\ndynamical components (with stationary and nonstationary parameters set), and\nmeasure local divergence of the system with collective behavior phenomena. We\nfind special kind of disagreement between model and data for Christmas time due\nto unusual shopping behavior. Analogies to earthquakes, product life-cycles,\nand energy markets will also be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 15:05:37 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Jarynowski", "Amdrzej", ""], ["Buda", "Andrzej", ""]]}, {"id": "1310.8236", "submitter": "Gunnar Stefansson", "authors": "Anna Helga Jonsdottir and Gunnar Stefansson", "title": "Design and analysis of experiments linking on-line drilling methods to\n  improvements in knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An on-line drilling system, the tutor-web, has been developed and used for\nteaching mathematics and statistics. The system was used in a basic course in\ncalculus including 182 students. The students were requested to answer quiz\nquestions in the tutor-web and therefore monitored continuously during the\nsemester. Data available are grades on a status exam conducted in the beginning\nof the course, a final grade and data gathered in the tutor-web system. A\nclassification of the students is proposed using the data gathered in the\nsystem; a Good student should be able to solve a problem quickly and get it\nright, the \"diligent\" hard-working Learner may take longer to get the right\nanswer, a guessing (Poor) student will not take long to get the wrong answer\nand the remaining (Unclassified) apparent non-learning students take long to\nget the wrong answer, resulting in a simple classification GLUP. The (Poor)\nstudents were found to show the least improvement, defined as the change in\ngrade from the status to the final exams, while the Learners were found to\nimprove the most. The results are used to demonstrate how further experiments\nare needed and can be designed as well as to indicate how a system needs to be\nfurther developed to accommodate such experiments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 17:21:26 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Jonsdottir", "Anna Helga", ""], ["Stefansson", "Gunnar", ""]]}, {"id": "1310.8341", "submitter": "Yevgeniy Kovchegov", "authors": "Anatoly Yambartsev, Michael Perlin, Yevgeniy Kovchegov, Natalia\n  Shulzhenko, Karina L. Mine, Xiaoxi Dong, Andrey Morgun", "title": "Unexpected links reflect the noise in networks", "comments": null, "journal-ref": null, "doi": "10.1186/s13062-016-0155-0", "report-no": null, "categories": "q-bio.MN q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene covariation networks are commonly used to study biological processes.\nThe inference of gene covariation networks from observational data can be\nchallenging, especially considering the large number of players involved and\nthe small number of biological replicates available for analysis. We propose a\nnew statistical method for estimating the number of erroneous edges in\nreconstructed networks that strongly enhances commonly used inference\napproaches. This method is based on a special relationship between sign of\ncorrelation (positive/negative) and directionality (up/down) of gene\nregulation, and allows for the identification and removal of approximately half\nof all erroneous edges. Using the mathematical model of Bayesian networks and\npositive correlation inequalities we establish a mathematical foundation for\nour method. Analyzing existing biological datasets, we find a strong\ncorrelation between the results of our method and false discovery rate (FDR).\nFurthermore, simulation analysis demonstrates that our method provides a more\naccurate estimate of network error than FDR.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 22:49:43 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 18:58:07 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2015 01:27:43 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yambartsev", "Anatoly", ""], ["Perlin", "Michael", ""], ["Kovchegov", "Yevgeniy", ""], ["Shulzhenko", "Natalia", ""], ["Mine", "Karina L.", ""], ["Dong", "Xiaoxi", ""], ["Morgun", "Andrey", ""]]}, {"id": "1310.8527", "submitter": "Regla Somoza", "authors": "R. D. Somoza, E. S. Pereira, E. M. L. Novo and C. D. Renn\\'o", "title": "A water level relationship between consecutive gauge stations along\n  Solim\\~oes/Amazonas main channel: a wavelet approach", "comments": null, "journal-ref": "WIT Transactions on Ecology and The Environment, 2013, Vol 178, p\n  53-62", "doi": "10.2495/WS130051", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Gauge stations are distributed along the Solim\\~oes/Amazonas main channel to\nmonitor water level changes over time. Those measurements help quantify both\nthe water movement and its variability from one gauge station to the next\ndownstream. The objective of this study is to detect changes in the water level\nrelationship between consecutive gauge stations along the Solim\\~oes/Amazonas\nmain channel, since 1980. To carry out the analyses, data spanning from 1980 to\n2010 from three consecutive gauges (Tefe, Manaus and Obidos) were used to\ncompute standardized daily anomalies. In particular for infra-annual periods it\nwas possible to detect changes for the water level variability along the\nSolim\\~oes/Amazonas main channel, by applying the Morlet Wavelet Transformation\n(WT) and Wavelet Cross Coherence (WCC) methods. It was possible to quantify the\nwaves amplitude for the WT infra-annual scaled-period and were quite similar to\nthe three gauge stations denoting that the water level variability are related\nto the same hydrological forcing functions. Changes in the WCC was detected for\nthe Manaus-Obidos river stretch and this characteristic might be associated\nwith land cover changes in the floodplains. The next steps of this research,\nwill be to test this hypotheses by integrating land cover changes into the\nfloodplain with hydrological modelling simulations throughout the time-series.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 14:38:21 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Somoza", "R. D.", ""], ["Pereira", "E. S.", ""], ["Novo", "E. M. L.", ""], ["Renn\u00f3", "C. D.", ""]]}, {"id": "1310.8574", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy, Michael Habeck, Bernhard Sch\\\"olkopf", "title": "Spatial statistics, image analysis and percolation theory", "comments": null, "journal-ref": "The Joint Statistical Meetings Proceedings (2011), Time Series and\n  Network Section, American Statistical Association, pp. 5571 - 5581", "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method for detection of signals and reconstruction of\nimages in the presence of random noise. The method uses results from\npercolation theory. We specifically address the problem of detection of\nmultiple objects of unknown shapes in the case of nonparametric noise. The\nnoise density is unknown and can be heavy-tailed. The objects of interest have\nunknown varying intensities. No boundary shape constraints are imposed on the\nobjects, only a set of weak bulk conditions is required. We view the object\ndetection problem as a multiple hypothesis testing for discrete statistical\ninverse problems. We present an algorithm that allows to detect greyscale\nobjects of various shapes in noisy images. We prove results on consistency and\nalgorithmic complexity of our procedures. Applications to cryo-electron\nmicroscopy are presented.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 16:26:09 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Habeck", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1310.8604", "submitter": "Matias Leppisaari", "authors": "Matias Leppisaari", "title": "Modeling catastrophic deaths using EVT with a microsimulation approach\n  to reinsurance pricing", "comments": "32 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a marked Poisson process (MPP) model for life catastrophe risk was\nproposed in [6]. We provide a justification and further support for the model\nby considering more general Poisson point processes in the context of extreme\nvalue theory (EVT), and basing the choice of model on statistical tests and\nmodel comparisons. A case study examining accidental deaths in the Finnish\npopulation is provided.\n  We further extend the applicability of the catastrophe risk model by\nconsidering small and big accidents separately; the resulting combined MPP\nmodel can flexibly capture the whole range of accidental death counts. Using\nthe proposed model, we present a simulation framework for pricing (life)\ncatastrophe reinsurance, based on modeling the underlying policies at\nindividual contract level. The accidents are first simulated at population\nlevel, and their effect on a specific insurance company is then determined by\nexplicitly simulating the resulting insured deaths. The proposed\nmicrosimulation approach can potentially lead to more accurate results than the\ntraditional methods, and to a better view of risk, as it can make use of all\nthe information available to the re/insurer and can explicitly accommodate even\ncomplex re/insurance terms and product features. As an example we price several\nexcess reinsurance contracts. The proposed simulation model is also suitable\nfor solvency assessment.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 17:26:19 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Leppisaari", "Matias", ""]]}]